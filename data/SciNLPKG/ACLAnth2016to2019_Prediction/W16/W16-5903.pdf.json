{"title": [{"text": "A Joint Model of Rhetorical Discourse Structure and Summarization", "labels": [], "entities": [{"text": "Rhetorical Discourse Structure and Summarization", "start_pos": 17, "end_pos": 65, "type": "TASK", "confidence": 0.7574549078941345}]}], "abstractContent": [{"text": "In Rhetorical Structure Theory, discourse units participate in asymmetric relationships, with one element acting as the nucleus and the other as the satellite.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.9109929402669271}]}, {"text": "In the resulting tree-like nuclearity structure, the importance of each discourse unit can be measured by the number of relations in which it acts as the nucleus or as the satellite.", "labels": [], "entities": []}, {"text": "Existing approaches to automatically parsing such structures suffer from two problems: they employ local inference techniques that do not capture document-level structural regularities, and they rely on annotated training data, which is expensive to obtain at the discourse level.", "labels": [], "entities": []}, {"text": "We investigate the SampleRank structure learning algorithm as a potential solution to both problems.", "labels": [], "entities": [{"text": "SampleRank structure learning", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6598378419876099}]}, {"text": "SampleRank allows us to incorporate arbitrary document-level features in a global stochastic inference algorithm.", "labels": [], "entities": []}, {"text": "Furthermore, it enables the training of a joint model of discourse structure and summarization, which can be learned from document-level summaries alone, without discourse-level supervision.", "labels": [], "entities": [{"text": "summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9721885919570923}]}, {"text": "We obtain mixed results in the fully supervised case, and negative results for the joint model of discourse structure and summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9730885624885559}]}], "introductionContent": [{"text": "Rhetorical structure theory (RST) is a hierarchical model of document-level organization, in which segments of text are linked in binary or multi-way discourse relations (  with 1A, and then the combined unit 1A:B at the nucleus of its relationship with 1C.", "labels": [], "entities": [{"text": "Rhetorical structure theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8393361320098242}]}, {"text": "In any given discourse relation, the nucleus is more likely to relevant to a summary of the document, and its sentiment is more likely to be relevant to the overall document-level polarity).", "labels": [], "entities": []}, {"text": "Thus, recovering this nuclearity structure is a key task for discourse parsing, with important practical applications.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7707215249538422}]}, {"text": "All known RST discourse parsers take one of two approximations, which are well known in structure prediction.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.939646045366923}, {"text": "structure prediction", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.8937399983406067}]}, {"text": "In dynamic programming approaches to discourse parsing, the feature space is locally restricted, allowing only features of discourse units that are sequentially adjacent (, or adjacent in the discourse parse ().", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7204103171825409}]}, {"text": "This makes exact inference possible, but at the cost of ignoring aspects of document structure that maybe relevant for identify-ing the correct parse.", "labels": [], "entities": []}, {"text": "For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features.", "labels": [], "entities": []}, {"text": "Alternatively, transition-based methods construct the discourse parse through a series of local decisions, typically driven by a classifier.", "labels": [], "entities": []}, {"text": "While the classifier is free to examine any aspect of the document or the existing partial parse, the accuracy of such methods maybe limited by search errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9987300038337708}]}, {"text": "A second limitation of existing discourse parsers relates to the amount of available training data.", "labels": [], "entities": []}, {"text": "Because discourse is a high-level linguistic phenomenon, relatively large amounts of text must be annotated to produce each training instance.", "labels": [], "entities": []}, {"text": "In RST, the smallest possible components of each discourse relation are elementary discourse units (EDUs), which correspond roughly to clauses.", "labels": [], "entities": [{"text": "RST", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9587768316268921}]}, {"text": "A relatively long news article might feature only a few dozen discourse relations, yet it still requires considerable time for the annotator to read and understand.", "labels": [], "entities": []}, {"text": "This suggests that it will be inherently difficult to train accurate discourse parsers using standard supervised learning techniques.", "labels": [], "entities": []}, {"text": "This paper proposes to solve both problems using SampleRank, a structure learning algorithm).", "labels": [], "entities": []}, {"text": "SampleRank uses stochastic search to explore the space of possible outputs, updating its model after each sample.", "labels": [], "entities": []}, {"text": "It imposes no limitations on the feature set; given an appropriate sampling distribution, it is capable of exploring the entire space of output configurations (in the limit).", "labels": [], "entities": []}, {"text": "Furthermore, SampleRank can be trained using indirect supervision, which provides a potential solution to the problem of limited training data for discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7230215966701508}]}, {"text": "Because discourse nuclearity structures are closely linked to other documentlabeling tasks -such as summarization and sentiment analysis -it is in principle possible to use labels from those tasks as a supervision signal for discourse parsing itself.", "labels": [], "entities": [{"text": "discourse nuclearity", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7225304245948792}, {"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9864461421966553}, {"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.818058431148529}, {"text": "discourse parsing", "start_pos": 225, "end_pos": 242, "type": "TASK", "confidence": 0.7218772321939468}]}, {"text": "To do this, we link discourse structure and summarization using a constraint proposed by.", "labels": [], "entities": [{"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9852269887924194}]}, {"text": "SampleRank then explores the joint space of extractive summaries and discourse parses, scoring the summaries against automatically-obtained reference summaries, while simultaneously learning to produce discourse parses that are compatible with high-scoring summaries.", "labels": [], "entities": []}, {"text": "At this stage, we have obtained only mixed empirical results with the application of SampleRank to RST discourse parsing: SampleRank offers improvements on one metric for RST parsing in the supervised learning scenario, but it does not improve over a summarization baseline in the indirect supervision scenario.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.927848756313324}, {"text": "RST parsing", "start_pos": 171, "end_pos": 182, "type": "TASK", "confidence": 0.9837574064731598}]}, {"text": "Nonetheless, we hope the ideas presented here will inspire further research in stochastic structure prediction for automated discourse structure analysis.", "labels": [], "entities": [{"text": "stochastic structure prediction", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.7108800808588663}, {"text": "automated discourse structure analysis", "start_pos": 115, "end_pos": 153, "type": "TASK", "confidence": 0.6767801344394684}]}], "datasetContent": [{"text": "We evaluate the supervised model from \u00a7 2 on the RST parsing task, and the indirectly-supervised model \u00a7 3 on summarization.", "labels": [], "entities": [{"text": "RST parsing task", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9296231865882874}, {"text": "summarization", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9560815691947937}]}, {"text": "The supervised model is evaluated on supervised task of discourse parsing on RST-DT dataset).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7101328372955322}, {"text": "RST-DT dataset", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.7707386910915375}]}, {"text": "The RST Discourse Treebank (RST-DT) consists of 385 documents, with 347 for train and 38 for testing in the standard split.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT)", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.849873830874761}]}, {"text": "We only focus on nuclearity and span prediction tasks.", "labels": [], "entities": [{"text": "nuclearity and span prediction", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.8400899916887283}]}, {"text": "We use the same F1 score on span and nuclearity as our evaluation metrics defined in the section 2.3.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.975673109292984}, {"text": "span", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9532288908958435}, {"text": "nuclearity", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9419373869895935}]}, {"text": "We compare our SampleRank approach with several competitive parsers from the literature: HILDA, a bottom-up classification-driven parser; DPLP), a shift-reduce parser that uses representation learning; and a condition random field (CRF) based parser with post-editing operations and a rich array of features.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.7033339738845825}]}, {"text": "SampleRank is competitive on the span metric, outperforming all systems except for the CRF approach, which employs rich linguistic features including syntax and entity transitions.", "labels": [], "entities": []}, {"text": "On the nuclearity metric, the SampleRank-based parser does somewhat worse than these prior efforts.", "labels": [], "entities": [{"text": "nuclearity", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9554353952407837}]}], "tableCaptions": [{"text": " Table 1: Evaluation of RST discourse parsing", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.9166857202847799}]}, {"text": " Table 2: Evaluation of joint summarization and discourse parsing algorithm", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7083084881305695}]}]}