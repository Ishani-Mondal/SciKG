{"title": [{"text": "Aligning Texts and Knowledge Bases with Semantic Sentence Simplification", "labels": [], "entities": [{"text": "Aligning Texts and Knowledge Bases", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8355812907218934}]}], "abstractContent": [{"text": "Finding the natural language equivalent of structured data is both a challenging and promising task.", "labels": [], "entities": []}, {"text": "In particular, an efficient alignment of knowledge bases with texts would benefit many applications, including natural language generation, information retrieval and text simplification.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.680115282535553}, {"text": "information retrieval", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.8286029398441315}, {"text": "text simplification", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7871173620223999}]}, {"text": "In this paper, we present an approach to build a dataset of triples aligned with equivalent sentences written in natural language.", "labels": [], "entities": []}, {"text": "Our approach consists of three main steps.", "labels": [], "entities": []}, {"text": "First, target sentences are annotated automatically with knowledge base (KB) concepts and instances.", "labels": [], "entities": []}, {"text": "The triples linking these elements in the KB are extracted as candidate facts to be aligned with the annotated sentence.", "labels": [], "entities": []}, {"text": "Second, we use tex-tual mentions referring to the subject and object of these facts to semantically simplify the target sentence via crowdsourc-ing.", "labels": [], "entities": []}, {"text": "Third, the sentences provided by different contributors are post-processed to keep only the most relevant simplifications for the alignment with KB facts.", "labels": [], "entities": []}, {"text": "We present different filtering methods, and share the constructed datasets in the public domain.", "labels": [], "entities": []}, {"text": "These datasets contain 1,050 sentences aligned with 1,885 triples.", "labels": [], "entities": []}, {"text": "They can be used to train natural language generators as well as semantic or contextual text simplifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large part of the information on the Web is contained in databases and is not suited to be directly accessed by human users.", "labels": [], "entities": []}, {"text": "A proper exploitation of these data requires relevant visualization techniques which may range from simple tabular presentation with meaningful queries, to graph generation and textual description.", "labels": [], "entities": [{"text": "graph generation", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7235620766878128}]}, {"text": "This last type of visualization is particularly interesting as it produces an additional raw resource that can be read by both computational agents (e.g. search engines) and human users.", "labels": [], "entities": []}, {"text": "From this perspective, the ability to generate high quality text from knowledge and data bases could be a game changer.", "labels": [], "entities": []}, {"text": "In the Natural language Processing community, this task is known as Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "Natural language Processing", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.6225171089172363}, {"text": "Natural Language Generation (NLG)", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.772674431403478}]}, {"text": "Efficient NLG solutions would allow displaying the content of knowledge and data bases to lay users; generating explanations, descriptions and summaries from ontologies and linked open data ; or guiding the user in formulating knowledge-base queries.", "labels": [], "entities": []}, {"text": "However, one strong and persistent limitation to the development of adequate NLG solutions for the semantic web is the lack of appropriate datasets on which to train NLG models.", "labels": [], "entities": []}, {"text": "The difficulty is that the semantic data available in knowledge and data bases need to be aligned with the corresponding text.", "labels": [], "entities": []}, {"text": "Unfortunately, this alignment task is far from straightforward.", "labels": [], "entities": [{"text": "alignment", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.9722287058830261}]}, {"text": "In fact, both human beings and machines perform poorly on it.", "labels": [], "entities": []}, {"text": "Nonetheless, there has been much work on datato-text generation and different strategies have been used to create the data-to-text corpora that are required for learning and testing.", "labels": [], "entities": [{"text": "datato-text generation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7280283719301224}]}, {"text": "Two main such strategies can be identified.", "labels": [], "entities": []}, {"text": "One strategy consists in creating a small, domain-specific corpus where data and text are manually aligned by a small group of experts (often the researchers who work on developing the NLG system).", "labels": [], "entities": []}, {"text": "Typically, such corpora are domain specific and of relatively small size while their linguistic variability is often restricted.", "labels": [], "entities": []}, {"text": "A second strategy consists in automatically building a large data-to-text corpus in which the alignment between data and text is much looser.", "labels": [], "entities": []}, {"text": "For instance, extracted a corpus consisting of 728,321 biography articles from English Wikipedia and created a data-to-text corpus by simply associating the infobox of each article with its introduction section.", "labels": [], "entities": []}, {"text": "The resulting dataset has a vocabulary of 403k words but there is no guarantee that the text actually matches the content of the infobox.", "labels": [], "entities": []}, {"text": "In this paper, we explore a middle-ground approach and introduce anew methodology for semi-automatically building large, high quality data-to-text corpora.", "labels": [], "entities": []}, {"text": "More precisely, our approach relies on a semantic sentence simplification method which allows transforming existing corpora into sentences aligned with KB facts.", "labels": [], "entities": []}, {"text": "Contrary to manual methods, our approach does not rely on having a small group of experts to identify alignments between text and data.", "labels": [], "entities": []}, {"text": "Instead, this task is performed (i) by multiple, independent contributors through a crowdsourcing platform, and (ii) by an automatic scoring of the quality of the contributions, which enables faster and more reliable data creation process.", "labels": [], "entities": []}, {"text": "Our approach also departs from the fully automatic approaches (e.g., ) in that it ensures a systematic alignment between text and data.", "labels": [], "entities": []}, {"text": "In the following section we present work related to corpus generation for NLG.", "labels": [], "entities": [{"text": "corpus generation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7530899047851562}]}, {"text": "In section 3 we describe our approach.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments, evaluations, and the statistics on the initial corpora and the generated (aligned) datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the first experiments, we build two datasets of natural language sentences aligned with KB facts.", "labels": [], "entities": []}, {"text": "Our first dataset is built by aligning all astronaut pages on Wikipedia 2 (Wiki) with triples from DBpedia 3 . The main motivation behind the choice of this corpus is to have both general and specific relations.", "labels": [], "entities": []}, {"text": "We used KODA as described in section 3.1.1 to obtain initial annotations.", "labels": [], "entities": [{"text": "KODA", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.6847966313362122}]}, {"text": "Our second dataset is built by aligning the medical encyclopedia part of Medline Plus 4 (MLP) with triples extracted with SemRep.", "labels": [], "entities": [{"text": "Medline Plus 4 (MLP)", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.8752564688523611}]}, {"text": "The motivation behind the selection of this corpus is twofold: a) to experiment with a domain-specific corpus, and b) to test the simplification when the triples are extracted from the text itself.", "labels": [], "entities": []}, {"text": "We used CrowdFlower 5 as a crowdsourcing platform.", "labels": [], "entities": []}, {"text": "We submitted 600 annotated sentences for the Wiki corpus and 450 sentences for the MLP corpus.", "labels": [], "entities": [{"text": "Wiki corpus", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9526275098323822}, {"text": "MLP corpus", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9696325063705444}]}, {"text": "We implemented several methods to select the best simplification among the 15 contributions for each sentence (cf. section 3.3).", "labels": [], "entities": []}, {"text": "To evaluate these methods we randomly selected 90 initial sentences from each dataset, then extracted the best simplification according to each of the 4 scoring metrics.", "labels": [], "entities": []}, {"text": "The authors then rated each simplification from 1 to 5, with 1 indicating a very bad simplification, and 5 indicating an excellent simplification.", "labels": [], "entities": []}, {"text": "One of the authors prepared the evaluation tables, anonymized the method names and did not participate in the evaluation.", "labels": [], "entities": []}, {"text": "The remaining 6 authors shared the 180 sentences and performed the ratings.: Statistics on the Aligned Dataset ating the selection methods we selected the most relevant simplification for each sentence in the dataset according to \u03be (i.e., in-cluster scoring), and generated the final datasets that link the best simplification to the facts associated with its original sentence.", "labels": [], "entities": [{"text": "Aligned Dataset", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.6558376252651215}]}, {"text": "presents the final statistics on the aligned datasets.", "labels": [], "entities": []}, {"text": "Both datasets are made available online .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic Statistics on Initial Corpora", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of S3 Selection Methods (av- erage rating)", "labels": [], "entities": [{"text": "av- erage rating", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.8686425089836121}]}, {"text": " Table 3: Statistics on the Aligned Dataset", "labels": [], "entities": [{"text": "Aligned Dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.7814933657646179}]}, {"text": " Table 4: Top 10 predicates", "labels": [], "entities": [{"text": "predicates", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.7760313749313354}]}, {"text": " Table 5: Number of participants and contributors'  ratings (on a 1 to 5 scale)", "labels": [], "entities": []}]}