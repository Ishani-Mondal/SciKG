{"title": [{"text": "CIST System for CL-SciSumm 2016 Shared Task", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces the methods and experiments applied in CIST system participating in the CL-SciSumm 2016 Shared Task at BIRNDL 2016.", "labels": [], "entities": [{"text": "CL-SciSumm 2016 Shared Task at BIRNDL 2016", "start_pos": 94, "end_pos": 136, "type": "DATASET", "confidence": 0.7682944706508091}]}, {"text": "We have participated in the TAC 2014 Biomedical Summariza-tion Track, so we develop the system based on previous work.", "labels": [], "entities": [{"text": "TAC 2014 Biomedical Summariza-tion Track", "start_pos": 28, "end_pos": 68, "type": "DATASET", "confidence": 0.7160391211509705}]}, {"text": "This time the domain is Computational Linguistics (CL).", "labels": [], "entities": []}, {"text": "The training corpus contains 20 topics from Training-Set-2016 and Development-Set-Apr8 published by CL-SciSumm 2016.", "labels": [], "entities": []}, {"text": "As to Task 1A and 1B, we mainly use rule-based methods with various features of lexicons and similarities; meanwhile we also have tried the machine learning method of SVM.", "labels": [], "entities": []}, {"text": "As to Task 2, hLDA topic model is adopted for content modeling, which provides us knowledge about sentence clustering (subtopic) and word distributions (abstractiveness) for summarization.", "labels": [], "entities": [{"text": "content modeling", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7242172807455063}, {"text": "summarization", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9665338397026062}]}, {"text": "We then combine hLDA knowledge with several other classical features using different weights and proportions to evaluate the sentences in the Reference Paper from its cited text spans.", "labels": [], "entities": []}, {"text": "Finally we extract the representative sentences to generate a summary within 250 words.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid development of Computational Linguistics (CL), the scientific literature of this domain has grown into a rich, complex, and continually expanding resource.", "labels": [], "entities": [{"text": "Computational Linguistics (CL)", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6502672791481018}]}, {"text": "Literature surveys and review articles in CL do help readers to gain a gist of the state-of-the-art in research fora topic.", "labels": [], "entities": []}, {"text": "However, literature survey writing is labor-intensive and a literature survey is not always available for every topic of interest.", "labels": [], "entities": [{"text": "literature survey writing", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.5982849101225535}]}, {"text": "What are needed, are resources which can automate the synthesis and updating of text summarization of CL research papers.", "labels": [], "entities": [{"text": "synthesis and updating of text summarization of CL research papers", "start_pos": 54, "end_pos": 120, "type": "TASK", "confidence": 0.6765914797782898}]}, {"text": "The CL-SciSumm 2016 (The 2nd Computational Linguistics Scientific Document Summarization Shared Task) has highlighted the challenges and relevance of the scientific summarization problem.", "labels": [], "entities": [{"text": "Computational Linguistics Scientific Document Summarization Shared Task)", "start_pos": 29, "end_pos": 101, "type": "TASK", "confidence": 0.7011729180812836}, {"text": "summarization", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.7433760166168213}]}, {"text": "In this paper, we describe our strategies, methods and experiments applied for CL-SciSumm 2016.", "labels": [], "entities": [{"text": "CL-SciSumm 2016", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.8338161408901215}]}, {"text": "As to Task 1A, we firstly use different combination methods and strategies based on various feature rules of different lexicons and similarities to identify the spans of text (cited text spans) in the RP (Reference Paper).", "labels": [], "entities": [{"text": "RP (Reference Paper)", "start_pos": 201, "end_pos": 221, "type": "DATASET", "confidence": 0.6038182437419891}]}, {"text": "Then we also have tried the machine learning method of SVM (Support Vector Machine).", "labels": [], "entities": []}, {"text": "As to Task 1B, we also use feature rules as a basis.", "labels": [], "entities": []}, {"text": "Besides, SVM is used to judge which facet the cited text span belongs to.", "labels": [], "entities": []}, {"text": "A voting method is also used to integrate different candidate results.", "labels": [], "entities": []}, {"text": "And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling.", "labels": [], "entities": [{"text": "document content modeling", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6827125052611033}]}, {"text": "The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization.", "labels": [], "entities": [{"text": "latent sentence clustering", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.672209640343984}, {"text": "summarization", "start_pos": 172, "end_pos": 185, "type": "TASK", "confidence": 0.9853375554084778}]}, {"text": "Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The performance of LDA", "labels": [], "entities": [{"text": "LDA", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6091950535774231}]}, {"text": " Table 2. As we can see  that Jaccard similarity performs the best.  Feature  F-measure  Lexicon 1  0.01793  Lexicon 2  0.05131  Lexicon 3  0.28437  Idf similarity  0.10589  Jaccard similarity  0.11167  Idf context similarity  0.07344  Jaccard context similarity  0.07042  Word vector  0.07771  Doc vector  0.05231", "labels": [], "entities": [{"text": "Idf similarity  0.10589  Jaccard similarity  0.11167  Idf context similarity  0.07344  Jaccard context similarity  0.07042  Word vector  0.07771  Doc vector  0.05231", "start_pos": 149, "end_pos": 314, "type": "METRIC", "confidence": 0.8218525499105453}]}, {"text": " Table 2. The performance of single feature", "labels": [], "entities": []}, {"text": " Table 3. The parameters of V1.0", "labels": [], "entities": [{"text": "V1.0", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6051546931266785}]}, {"text": " Table 4. The parameters of V2.0", "labels": [], "entities": [{"text": "V2.0", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6784282326698303}]}, {"text": " Table 7. In Run5, we used the SVM Method. And  the accuracy we got in training set is 80.59% as a closed testing.  Method  P  R  Fs  Voting 1.0  0.07627  0.18881  0.10865  Voting 2.0  0.08898  0.22028  0.12676  Jaccard Focused  0.09675  0.23951  0.13783  Jaccard Cascade  0.0911  0.22552  0.12978", "labels": [], "entities": [{"text": "Run5", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9172806143760681}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994468092918396}, {"text": "Jaccard Focused  0.09675  0.23951  0.13783  Jaccard Cascade  0.0911  0.22552  0.12978", "start_pos": 212, "end_pos": 297, "type": "DATASET", "confidence": 0.8691062033176422}]}, {"text": " Table 7. Task 1A results of training dataset", "labels": [], "entities": []}, {"text": " Table 9. Task 1B results of training dataset", "labels": [], "entities": []}, {"text": " Table 10. Task 1B results of testing dataset", "labels": [], "entities": []}, {"text": " Table 11. The ROUGE values for training dataset", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9864272475242615}]}, {"text": " Table 12. ROUGE values of different sentence choosing method for level feature", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9897370934486389}]}, {"text": " Table 13. ROUGE values of selected answer for testing data.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9955202341079712}]}]}