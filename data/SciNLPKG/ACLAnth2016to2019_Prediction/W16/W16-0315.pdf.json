{"title": [{"text": "Classifying ReachOut posts with a radial basis function SVM", "labels": [], "entities": [{"text": "SVM", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.5598233938217163}]}], "abstractContent": [{"text": "The ReachOut clinical psychology shared task challenge addresses the problem of providing an automatic triage for posts to a support forum for people with a history of mental health issues.", "labels": [], "entities": []}, {"text": "Posts are classified into green, amber, red and crisis.", "labels": [], "entities": []}, {"text": "The non-green categories correspond to increasing levels of urgency for some form of intervention.", "labels": [], "entities": []}, {"text": "The Thomson Reuters submissions arose from an idea about self-training and ensemble learning.", "labels": [], "entities": [{"text": "Thomson Reuters submissions", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9416129390398661}]}, {"text": "The available labeled training set is small (947 examples) and the class distribution unbalanced.", "labels": [], "entities": []}, {"text": "It was therefore hoped to develop a method that would make use of the larger dataset of unlabeled posts provided by the organisers.", "labels": [], "entities": []}, {"text": "This did notwork, but the performance of a radial basis function SVM intended as a baseline was relatively good.", "labels": [], "entities": []}, {"text": "Therefore, the report focuses on the latter, aiming to understand the reasons for its performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "The ReachOut clinical psychology shared task challenge addresses the problem of providing an automatic triage for posts to a support forum for people with a history of mental health issues.", "labels": [], "entities": []}, {"text": "Posts are classified into green,amber,red and crisis.", "labels": [], "entities": []}, {"text": "The non-green categories correspond to increasing levels of urgency for some form of intervention, and can be regarded as positive.", "labels": [], "entities": []}, {"text": "Green means \"all clear\", no need for intervention.", "labels": [], "entities": []}, {"text": "includes manuallycreated examples of posts from each class.", "labels": [], "entities": []}, {"text": "1 1 These are made-up examples, for reasons of patient confidentiality.", "labels": [], "entities": []}, {"text": "They are also much shorter than typical posts.", "labels": [], "entities": []}, {"text": "Class Example green sitting in my armchair listening to the birds amber Not over that old friendship.", "labels": [], "entities": []}, {"text": "red What's the point of talking to anyone?", "labels": [], "entities": []}, {"text": "The entry from Thomson Reuters was planned to be a system in which an ensemble of base classifiers is followed by a final system combination step in order to provide a final answer.", "labels": [], "entities": [{"text": "Thomson Reuters", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.9340947866439819}]}, {"text": "But this did not pan out, so we report results on a baseline classifier.", "labels": [], "entities": []}, {"text": "All of the machine learning was done using scikit-learn).", "labels": [], "entities": []}, {"text": "The first step, shared between all runs, was to split the labeled data into a training partition of 625 examples (Train) and two development sets (Dev_test1 and Dev_test2) of 161 examples each.", "labels": [], "entities": []}, {"text": "There were two development sets only because of the plan to do system combination.", "labels": [], "entities": []}, {"text": "This turns out to have been fortunate.", "labels": [], "entities": []}, {"text": "All data sets were first transformed into Pandas (McKinney, 2010) data-frames for convenient onward processing.", "labels": [], "entities": [{"text": "Pandas (McKinney, 2010) data-frames", "start_pos": 42, "end_pos": 77, "type": "DATASET", "confidence": 0.8073723912239075}]}, {"text": "When the test set became available, it was similarly transformed into the test data-frame (Test).", "labels": [], "entities": []}, {"text": "The first submitted run was an RBF SVM, intended as a strong baseline.", "labels": [], "entities": [{"text": "RBF SVM", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8805452287197113}]}, {"text": "This run achieved a better score than any of the more elaborate approaches, and, together with subsequent analysis, sheds some light on the nature of the task and the evaluation metrics used.", "labels": [], "entities": []}, {"text": ", with a radial basis function kernel.", "labels": [], "entities": []}, {"text": "scikit-learn provides a grid search function that uses stratified cross-validation to tune the classifier parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "Class distributions We have four datasets: the two sets of development data, the main training set and the official test set distributed by the organisers.", "labels": [], "entities": []}, {"text": "shows the class distributions for the three evaluation sets and the training set are different.", "labels": [], "entities": []}, {"text": "In particular, the final test set used for official scoring has only one instance of the crisis category, when one might expect around ten.", "labels": [], "entities": []}, {"text": "Of course, none of the teams knew this at submission time.", "labels": [], "entities": []}, {"text": "The class distributions are always imbalanced, but it is a surprise to seethe extreme imbalance in the final test set.", "labels": [], "entities": []}, {"text": "Evaluation metrics The main evaluation metric used for the competition is a macro-averaged F1-score restricted to amber, red and crisis.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9443045854568481}]}, {"text": "This is very sensitive to the unbalanced class distributions, since it weights all three positive classes equally.", "labels": [], "entities": []}, {"text": "A classifier that correctly hits the one positive example for crisis will achieve a large gain in score relative to one that does not.", "labels": [], "entities": []}, {"text": "Microaveraged F1, which simply counts true positives, false positives and false negatives overall the pos-itive classes, might have proven a more stable target.", "labels": [], "entities": [{"text": "Microaveraged", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.5725303888320923}, {"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.7915021181106567}]}, {"text": "An alternative is the multi-class Matthews correlation coefficient).", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 43, "end_pos": 66, "type": "METRIC", "confidence": 0.9332749247550964}]}, {"text": "Or, since the labels are really ordinal, similar to a Likert scale, quadratic weighted kappa (Vaughn and Justice, 2015) could be used.", "labels": [], "entities": []}, {"text": "Class weights Preliminary explorations revealed that the classifier was producing results that overrepresented the 'green' category.", "labels": [], "entities": []}, {"text": "To rectify this, the grid search was re-done using a non-uniform class weight vector of 1 for 'green' and 20 for 'crisis','red' and 'amber'.", "labels": [], "entities": []}, {"text": "The effect of this was to increase by a factor of 20 the effective classification penalty for the three positive classes.", "labels": [], "entities": [{"text": "effective classification penalty", "start_pos": 57, "end_pos": 89, "type": "METRIC", "confidence": 0.5776482125123342}]}, {"text": "The grid search used for the final submission set \u03b3=0.01, C at 15 logarithmically spaced locations between 1 and 1000 inclusive, all vocabulary size limits in {10, 30, 100, 300, 1000, 3000, 10000} and assumed that author type, kudos and first in thread were always relevant and should always be used.", "labels": [], "entities": []}, {"text": "The scoring metric used for this grid search was mean accuracy.", "labels": [], "entities": [{"text": "grid search", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.8267420530319214}, {"text": "mean", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.955262303352356}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.7559350728988647}]}, {"text": "The optimal parameters for this setting were estimated to be: C=51.79, |V |=3000.", "labels": [], "entities": []}, {"text": "The role of luck in feature selection This classifier is perfect on the training set, suggesting overfitting (see section 4 fora deeper dive into this point).", "labels": [], "entities": [{"text": "luck", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9422500133514404}, {"text": "feature selection", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8685412406921387}]}, {"text": "Classification reports for the two development sets are shown in table 3.", "labels": [], "entities": []}, {"text": "After submission a more complete grid search was conducted allowing for the possibility of excluding the author type, kudos and first in thread features.", "labels": [], "entities": []}, {"text": "All but kudos were excluded.", "labels": [], "entities": []}, {"text": "Comparing using Dev_test1 the second classifier would have been chosen, but using Dev_test2 we would have chosen the original.", "labels": [], "entities": [{"text": "Dev_test1", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.8688556750615438}]}, {"text": "The major reason for this difference is that the second classifier happened to correctly classify one of the 8 examples for crisis in Dev_test1, but missed all the five examples of that class in Dev_test2.", "labels": [], "entities": [{"text": "Dev_test1", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.8496177593866984}, {"text": "Dev_test2", "start_pos": 195, "end_pos": 204, "type": "DATASET", "confidence": 0.8749877413113912}]}, {"text": "In fact, on the actual test set, the first classifier is better.", "labels": [], "entities": []}, {"text": "The choice to tune on Dev_test1 was arbitrary, and fortunate.", "labels": [], "entities": [{"text": "Dev_test1", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.9333412647247314}]}, {"text": "The choice not to consider turning off the metadata features was a pure accident.", "labels": [], "entities": []}, {"text": "Tuning via grid search is challenging in the face of small training sets and unbalanced class distributions, and in   this case would have led the classifier astray.", "labels": [], "entities": []}, {"text": "Once optimal parameters had been selected, the classifier was re-trained using on the concatenation of Train, Dev_test1 and Dev_test2, and predictions were generated for Test.", "labels": [], "entities": []}, {"text": "contains classification reports for the classweighted version that was submitted and a nonweighted version that was prepared after submission.", "labels": [], "entities": []}, {"text": "The source of the improved official score achieved by the class-weighted version is a larger F-score on the red category, at the expense of a smaller score on the green category, which is not one of the positive categories averaged in the official scoring metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9982182383537292}]}], "tableCaptions": [{"text": " Table 2: Class distribution for training, development and test", "labels": [], "entities": []}, {"text": " Table 3:  Classification reports for Dev test1 and", "labels": [], "entities": [{"text": "Dev test1", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.897189199924469}]}, {"text": " Table 4: Classification reports for Test with and without class", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8864551186561584}]}]}