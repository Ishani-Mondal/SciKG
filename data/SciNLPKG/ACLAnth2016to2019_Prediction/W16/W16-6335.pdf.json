{"title": [{"text": "A New Feature Selection Technique Combined with ELM Feature Space for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.787097305059433}]}], "abstractContent": [{"text": "The aim of text classification is to classify the text documents into a set of pre-defined categories.", "labels": [], "entities": [{"text": "text classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7524416148662567}]}, {"text": "But the complexity of natural languages, high dimensional feature space and low quality of feature selection become the main problem for text classification process.", "labels": [], "entities": [{"text": "text classification", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.8441793322563171}]}, {"text": "Hence, in order strengthen the classification technique, selection of important features, and consequently removing the unimportant ones is the need of the day.", "labels": [], "entities": [{"text": "classification", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.960338294506073}]}, {"text": "The Paper proposes an approach called Commonality-Rarity Score Computation (CRSC) for selecting top features of a corpus and highlights the importance of ML-ELM feature space in the domain of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7288483381271362}]}, {"text": "Experimental results on two benchmark datasets signify the prominence of the proposed approach compared to other established approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increase in number of documents on the Web, it has become increasingly important to reduce the noisy and redundant features which can reduce the training time and hence increase the performance of the classifier during text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.8207457661628723}]}, {"text": "Large number of features, produces feature vector with very high dimensionality and hence, different methods to reduce the dimension can be used such as Singular Value Decomposition (SVD), Wavelet Analysis (, Principle Component Analysis (PCA)() etc.", "labels": [], "entities": [{"text": "Principle Component Analysis (PCA)()", "start_pos": 209, "end_pos": 245, "type": "TASK", "confidence": 0.7349632481733958}]}, {"text": "The algorithms used for feature selection are broadly classified into three categories: filters, wrapper and embedded methods.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7479462325572968}]}, {"text": "Filter methods use the properties of the dataset to select the features without using any specific algorithm, and hence preferred over wrapper methods.", "labels": [], "entities": []}, {"text": "Most filter methods give a ranking of the best features rather than one single set of best features.", "labels": [], "entities": []}, {"text": "Wrapper methods use a predecided learning algorithm i.e. a classifier to evaluate the features and hence computationally expensive).", "labels": [], "entities": []}, {"text": "Also, they have a higher possibility of overfitting than filter methods.", "labels": [], "entities": []}, {"text": "Hence, large scale problems like text categorization mostly do not use wrapper methods.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7912939488887787}]}, {"text": "Embedded methods tend to combine the advantages of both the aforementioned methods.", "labels": [], "entities": []}, {"text": "The computational complexity of the embedded methods, thus, lies in between that of the filters and the wrappers.", "labels": [], "entities": []}, {"text": "Ample research work has already been done in this domain ()( Selection of a good classifier plays a vital role in the text classification process.", "labels": [], "entities": [{"text": "text classification process", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.8529229362805685}]}, {"text": "Many of the traditional classifiers have their own limitations while solving any complex problems.", "labels": [], "entities": []}, {"text": "On the other hand, Extreme Learning Machine (ELM) is able to approximate any complex non-linear mappings directly from the training samples ().", "labels": [], "entities": []}, {"text": "Hence, ELM has a better universal approximation capability than conventional neural networks based classifiers.", "labels": [], "entities": []}, {"text": "Also, quick learning speed, ability to manage huge volume of data, requirement of less human intervention, good generalization capability, easy implementation etc. are some of the salient features which make ELM more popular compared to other traditional classifiers.", "labels": [], "entities": []}, {"text": "Recently developed Multilayer ELM which is based on the architecture of deep learning is an extension of ELM and have more than one hidden layer.", "labels": [], "entities": []}, {"text": "285 In this paper, we propose an approach for feature selection called Commonality-Rarity Score Computation (CRSC) by means of three parameters (Alpha (measures weighted commonality), Beta (measures extent of occurrence of a term) and Gamma (average weight of term per document)), computes the score of a term in order to rank them based on their relevance.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7069584131240845}]}, {"text": "The top m% features are selected for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8529905378818512}]}, {"text": "The proposed approach is compared with traditional feature selections techniques such as Chi-Square (, Bi-normal separation (BNS), Information Gain (IG)) and GINI (.", "labels": [], "entities": []}, {"text": "Empirical results on 20-Newsgroups and Reuters datasets show the effectiveness of the proposed approach compared to other feature selection techniques.", "labels": [], "entities": [{"text": "Reuters datasets", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.8705068826675415}, {"text": "feature selection", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7061300277709961}]}, {"text": "The paper is outlined as follows: Section 2 discussed the architecture of ELM, ML-ELM and ML-ELM extended feature space.", "labels": [], "entities": []}, {"text": "The proposed approach is described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 covers the experimental work and finally, the paper is concluded in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "20-Newsgroups 1 and Reuters 2 datasets are used for experimental purpose.", "labels": [], "entities": [{"text": "Reuters 2 datasets", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.8920752008756002}]}, {"text": "The classifiers which are used for comparison purpose are Support Vector Machine (LinearSVC), Decision Tree (DT), SVM linear kernel (LinearSVM), Gaussian Naive Bayes (GNB), Random Forest (RF), Nearest Centroid (NC), Adaboost, Multinomial Naive-Bayes (M-NB) and ELM.", "labels": [], "entities": []}, {"text": "In all the tables bold indicates the highest F-measure obtained by CRSC using the corresponding classifier.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9979039430618286}, {"text": "CRSC", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9400333762168884}]}, {"text": "The algorithm was tested on hidden layer nodes of different size both for ELM and ML-ELM and the best results are obtained when the number of nodes of hidden layer are more than the nodes in the input layer.", "labels": [], "entities": []}, {"text": "In the k-means clustering, k (the number of clusters) was set as 8 (decided empirically) for both the datasets.", "labels": [], "entities": []}, {"text": "The following parameters are used to measure the performance.", "labels": [], "entities": []}, {"text": "1 http://qwone.com/\u223cjason/20Newsgroups/ 2 www.daviddlewis.com/resources/testcollections/reuters/ Precision (P): F-Measure (F): It combines both precision and recall and can be defined as follows:  20-Newsgroups is a very popular machine learning dataset generally used for text classification and having 7 different categories.", "labels": [], "entities": [{"text": "F-Measure (F)", "start_pos": 112, "end_pos": 125, "type": "METRIC", "confidence": 0.895557701587677}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9986888766288757}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9945333003997803}, {"text": "text classification", "start_pos": 273, "end_pos": 292, "type": "TASK", "confidence": 0.7963248491287231}]}, {"text": "For experimental purpose, approximately 11300 documents are used for training and 7500 for testing.", "labels": [], "entities": []}, {"text": "The results can be summarized as follows: -top 1% features: CRSC using ML-ELM and Multinomial naive-bayes has obtained the best results.", "labels": [], "entities": [{"text": "CRSC", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.6756391525268555}]}, {"text": "Classifier wise, ML-ELM generates the maximum average F-measure for CRSC.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9933384656906128}]}, {"text": "-top 5% features: ML-ELM and LinearSVM generate the best results.", "labels": [], "entities": []}, {"text": "Classifier wise, maximum average F-measure is obtained using ML-ELM.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9957834482192993}, {"text": "ML-ELM", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7180164456367493}]}, {"text": "-top 10% features: CRSC has obtained best results using ML-ELM and Random Forest.", "labels": [], "entities": [{"text": "CRSC", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.5489660501480103}, {"text": "ML-ELM", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.7465691566467285}]}, {"text": "Classifier wise CRSC obtained the highest average F-measure of 0.9602 using ML-ELM.", "labels": [], "entities": [{"text": "CRSC", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.8307371735572815}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9982950091362}, {"text": "ML-ELM", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.7271711230278015}]}, {"text": "-top 1% features: CRSC using Adaboost has obtained the best results.", "labels": [], "entities": []}, {"text": "Classifier wise, LinearSVM generates the maximum average Fmeasure for CRSC).", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9921523928642273}]}, {"text": "-top 5% features: Adaboost and ML-ELM generate the best results.", "labels": [], "entities": []}, {"text": "Classifier wise, maximum average F-measure is obtained using ML-ELM.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9957834482192993}, {"text": "ML-ELM", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7180164456367493}]}, {"text": "-top 10% features: CRSC has obtained the best results using ML-ELM and Adaboost.", "labels": [], "entities": [{"text": "CRSC", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.6428199410438538}]}, {"text": "Classifier wise CRSC obtained the highest average F-measure of 0.9598 using ML-ELM)..", "labels": [], "entities": [{"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.998011589050293}, {"text": "ML-ELM", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.565959632396698}]}, {"text": "It is evident from all the results that ML-ELM outperforms other well known classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-measure on top 1% features (20-NG)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9899647831916809}]}, {"text": " Table 2: F-measure on top 5% features (20-NG)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9912376403808594}]}, {"text": " Table 3: F-measure on top 10% features (20-NG)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9894054532051086}]}, {"text": " Table 4: F-measure on top 1% features (Reuters)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9852421879768372}, {"text": "Reuters)", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9520713686943054}]}, {"text": " Table 5: F-measure on top 5% features (Reuters)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9807658195495605}, {"text": "Reuters)", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9500537812709808}]}, {"text": " Table 6: F-measure on top 10% features (Reuters)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9770370721817017}, {"text": "Reuters)", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9489785432815552}]}, {"text": " Table 7: F-measure comparisons on CRSC", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9034901857376099}, {"text": "CRSC", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7533903121948242}]}]}