{"title": [], "abstractContent": [{"text": "We introduce SimpleNets: a resource-light solution to the sentence-level Quality Estimation task of WMT16 that combines Recurrent Neural Networks, word embedding models, and the principle of compo-sitionality.", "labels": [], "entities": [{"text": "sentence-level Quality Estimation task", "start_pos": 58, "end_pos": 96, "type": "TASK", "confidence": 0.6447421759366989}, {"text": "WMT16", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.6652379035949707}]}, {"text": "The SimpleNets systems explore the idea that the quality of a translation can be derived from the quality of its n-grams.", "labels": [], "entities": []}, {"text": "This approach has been successfully employed in Text Simplification quality assessment in the past.", "labels": [], "entities": [{"text": "Text Simplification quality assessment", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.8875059932470322}]}, {"text": "Our experiments show that, surprisingly, our models can learn more about a translation's quality by focusing on the original sentence, rather than on the translation itself.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation (QE)", "start_pos": 12, "end_pos": 55, "type": "TASK", "confidence": 0.8366594101701464}]}, {"text": "The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations.", "labels": [], "entities": [{"text": "QE", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.8960266709327698}, {"text": "Machine Translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8470627665519714}]}, {"text": "Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency, selecting high quality translations, translation re-ranking), and visual assistance for manual translation revision).", "labels": [], "entities": [{"text": "translation re-ranking", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8332071304321289}, {"text": "manual translation revision", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.6576159596443176}]}, {"text": "QE can be performed in various ways in order to suit different purposes.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7072328329086304}]}, {"text": "The most widely addressed form of this task is sentence-level QE.", "labels": [], "entities": [{"text": "sentence-level QE", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.47706449031829834}]}, {"text": "Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations.", "labels": [], "entities": []}, {"text": "As quality labels, previous work uses either real valued scores estimated by humans, which require fora given QE system to address the task as a regression problem, or likert scale discrete values, which allow for the task to be addressed as either a regression or a classification problem.", "labels": [], "entities": []}, {"text": "Sentence-level QE has been covered by shared tasks organised by WMT since 2012, with subsequent years covering also word and documentlevel tasks.", "labels": [], "entities": [{"text": "Sentence-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7681439518928528}, {"text": "WMT", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.948161780834198}]}, {"text": "Recent advances in Distributional Semantics have been showing promising results in the context of QE strategies for different prediction levels.", "labels": [], "entities": [{"text": "QE", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9076808094978333}]}, {"text": "An example of that are modern word embedding architectures, such as the CBOW and Skip-Gram models introduced by, which have been used as features in some of the best ranking systems in the sentence and word-level QE shared tasks of WMT15 (.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8828101754188538}, {"text": "WMT15", "start_pos": 232, "end_pos": 237, "type": "DATASET", "confidence": 0.8567930459976196}]}, {"text": "Word embeddings are not only versatile, but also cheap to produce, making for both reliable and cost-effective QE solutions.", "labels": [], "entities": [{"text": "QE", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9520527124404907}]}, {"text": "Neural Networks have also been successfully employed in QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9027581810951233}]}, {"text": "The FBK-UPV-UEdin () and HDCL () systems are good examples of that.", "labels": [], "entities": [{"text": "FBK-UPV-UEdin", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9223272800445557}]}, {"text": "They achieved 1st and 2nd places in the word-level QE tasks of WMT14 and WMT15, respectively, outperforming strategies that resort to much more resource-heavy features.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8755881190299988}, {"text": "WMT15", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.8273608684539795}]}, {"text": "Another successful example are neural Language Models for sentence-level QE (.", "labels": [], "entities": []}, {"text": "We were notable to find, however, any examples of sentence-level QE systems that combine word embedding models and Neural Networks.", "labels": [], "entities": []}, {"text": "In this paper, we present our efforts in doing so.", "labels": [], "entities": []}, {"text": "We introduce SimpleNets: the resource-light and language agnostic sentence-level QE systems submitted to WMT16 that exploit the principle of compositionality for QE.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.9430257081985474}]}, {"text": "In the Sections that follow, we describe the sentence-level QE task of WMT16, introduce the approach used by the SimpleNets systems, and present the results obtained.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.8663390874862671}]}], "datasetContent": [{"text": "SimpleNets are two systems submitted to the sentence-level QE task of WMT16.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.7982776165008545}]}, {"text": "In this task, participants were challenged to predict real-valued quality scores in 0,100 of sentences translated from English into German.", "labels": [], "entities": []}, {"text": "The translations were produced by an in-house phrase-based Statistical Machine Translation system, and were then postedited by professional translators.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6230760117371877}]}, {"text": "The real-valued quality scores are HTER () values that represent the post-editing effort spent on each given translation.", "labels": [], "entities": [{"text": "HTER", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9312487840652466}]}, {"text": "The task organisers provided three datasets: \u2022 Training: Contains 12,000 translation instances accompanied by their respective postedits and HTER values.", "labels": [], "entities": [{"text": "HTER", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.7388893365859985}]}, {"text": "\u2022 Development: Contains 1,000 translation instances accompanied by their respective postedits and HTER values.", "labels": [], "entities": []}, {"text": "\u2022 Test: Contains 2,000 translation instances only, without their respective post-edits or HTER values.", "labels": [], "entities": [{"text": "HTER", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.770207941532135}]}, {"text": "Each instance is composed by the original sentence in English along with its translation in German.", "labels": [], "entities": []}, {"text": "HTER scores were capped to 100.", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5490532070398331}]}, {"text": "The organisers also provided 17 baseline feature values extracted using QuEst++ ( ) for each dataset.", "labels": [], "entities": [{"text": "organisers", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.831520140171051}]}, {"text": "To assess the efficacy of our SimpleNets, we train them over the training set provided by the organizers, which contain 12,000 instances.", "labels": [], "entities": []}, {"text": "In order to select the architecture to be used by the LSTM networks of our SimpleNets, we resort to the technique used in, in which each aspect of a Neural Network is determined through parameter optimisation over the development set.", "labels": [], "entities": []}, {"text": "The optimisation metric used is Pearson correlation, since it is the main evaluation metric adopted by the WMT16 task.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 32, "end_pos": 51, "type": "METRIC", "confidence": 0.8428154289722443}, {"text": "WMT16 task", "start_pos": 107, "end_pos": 117, "type": "TASK", "confidence": 0.4934772849082947}]}, {"text": "The aspects of the architecture considered and the values tested for each one of them are: 1.", "labels": [], "entities": []}, {"text": "Number of hidden layers: 1 to 5 in steps of 1. 2. Hidden layer size: 100 to 500 in steps of 100.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence-level QE scores of systems submitted to the WMT16 task", "labels": [], "entities": [{"text": "WMT16", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.830418050289154}]}, {"text": " Table 2: N-grams with highest and lowest HTER scores, as predicted by SimpleNets-TGT", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.8623501658439636}, {"text": "SimpleNets-TGT", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.672680139541626}]}, {"text": " Table 3: N-grams with highest and lowest HTER scores, as predicted by SimpleNets-SRC", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.8573701977729797}, {"text": "SimpleNets-SRC", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.6819795370101929}]}]}