{"title": [{"text": "Posterior regularization for Joint Modelling of Multiple Structured Prediction Tasks with Soft Constraints", "labels": [], "entities": [{"text": "Posterior regularization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5762030184268951}, {"text": "Joint Modelling of Multiple Structured Prediction Tasks", "start_pos": 29, "end_pos": 84, "type": "TASK", "confidence": 0.7786546775272915}]}], "abstractContent": [{"text": "We propose a multi-task learning objective for training joint structured prediction models when no jointly annotated data is available.", "labels": [], "entities": []}, {"text": "We use conditional random fields as the joint predictive model and train their parameters by optimizing the marginal likelihood of all available annotations, with additional posterior constraints on the distributions of the latent variables imposed to enforce agreement.", "labels": [], "entities": []}, {"text": "Experiments on named entity recognition and part-of-speech tagging show that the proposed model outperforms independent task estimation , and the posterior constraints provide a useful mechanism for incorporating domain-specific knowledge.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6331476867198944}, {"text": "part-of-speech tagging", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7214359045028687}]}], "introductionContent": [], "datasetContent": [{"text": "We performed experiments on jointly modelling two tasks: 1) Named Entity Recognition(NER) and 2) Part of Speech (PoS) tagging.", "labels": [], "entities": [{"text": "Named Entity Recognition(NER)", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.6958042780558268}, {"text": "Part of Speech (PoS) tagging", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.561240438904081}]}, {"text": "For NER, we follow the standard convention of 'B-I-O tagging' where 'B' and 'I' help identify segments of named entities and 'O' identifies the words that are not named entities.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9015100598335266}]}, {"text": "For PoS, we used the 'Universal' PoS tagset, which is largely invariant across several languages).", "labels": [], "entities": [{"text": "PoS tagset", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.7102116346359253}]}, {"text": "The tagset for the two tasks was: Since, we wish to study the effect of the size of training data, we used the standard English ConLL dataset) for both NER and PoS tagging models and artificially impoverished the data by randomly sampling disjoint task.", "labels": [], "entities": [{"text": "English ConLL dataset", "start_pos": 120, "end_pos": 141, "type": "DATASET", "confidence": 0.6106448670228323}]}, {"text": "For training all of the CRF models (single, supervised joint, latent joint, and posterior regularized joint), we used a standard set of indicator features derivable from the input sequence.", "labels": [], "entities": []}, {"text": "For obtaining informative constraints, we used the statistics from a large Spanish NER dataset.", "labels": [], "entities": [{"text": "Spanish NER dataset", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7316176891326904}]}, {"text": "We specifically chose this setting to gauge the ease and efficacy of language invariant relationships between NER and PoS tagging tasks.", "labels": [], "entities": [{"text": "NER and PoS tagging tasks", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.681754219532013}]}, {"text": "Specifically, we focused on the expected proportions of the joint labels and the joint edges in the training corpus.", "labels": [], "entities": []}, {"text": "We also used the performance on our development set to identify a small pool of constraintswhich are listed in.", "labels": [], "entities": []}, {"text": "It should be noted that depending upon the specific data and task settings many other kinds of informative constraints, that also condition on observed sequence x can be easily incorporated as long as their computation decomposes along the cliques of our joint models.", "labels": [], "entities": []}, {"text": "For numerical stability, the constraints in table 2 were scaled to be in the same range by scaling \u03c6.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of the different training datasets.", "labels": [], "entities": []}, {"text": " Table 3: Performance on NER and Part of Speech tagging. 'P', 'R', 'F1' stand for Precision, Recall and F1 score for Named Entity", "labels": [], "entities": [{"text": "Speech tagging", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.6718018501996994}, {"text": "P', 'R', 'F1'", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.6882602393627166}, {"text": "Precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9549317955970764}, {"text": "Recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.8509721159934998}, {"text": "F1 score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9883319437503815}]}]}