{"title": [{"text": "Know2Look: Commonsense Knowledge for Visual Search Sreyasi Nag Chowdhury", "labels": [], "entities": []}], "abstractContent": [{"text": "With the rise in popularity of social media, images accompanied by contextual text form a huge section of the web.", "labels": [], "entities": []}, {"text": "However, search and retrieval of documents are still largely dependent on solely textual cues.", "labels": [], "entities": []}, {"text": "Although visual cues have started to gain focus, the imperfection in object/scene detection do not lead to significantly improved results.", "labels": [], "entities": [{"text": "object/scene detection", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.641485907137394}]}, {"text": "We hypothesize that the use of background commonsense knowledge on query terms can significantly aid in retrieval of documents with associated images.", "labels": [], "entities": []}, {"text": "To this end we deploy three different modalities-text, visual cues, and common-sense knowledge pertaining to the query-as a recipe for efficient search and retrieval.", "labels": [], "entities": []}], "introductionContent": [{"text": "Motivation: Image retrieval by querying visual contents has been on the agenda of the database, information retrieval, multimedia, and computer vision communities for decades (.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7501366436481476}, {"text": "information retrieval", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.7181945145130157}]}, {"text": "Search engines like Baidu, Bing or Google perform reasonably well on this task, but crucially rely on textual cues that accompany an image: tags, caption, URL string, adjacent text etc.", "labels": [], "entities": []}, {"text": "In recent years, deep learning has led to a boost in the quality of visual object recognition in images with fine-grained object labels (Simonyan and Zisserman, 2014;.", "labels": [], "entities": [{"text": "visual object recognition", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.797216554482778}]}, {"text": "Methods like LSDA () are trained on more than 15,000 classes of ImageNet () (which are mostly leaflevel synsets of WordNet), and annotate newly seen images with class labels for bound- ing boxes of objects.", "labels": [], "entities": []}, {"text": "For the image in, for example, object labels traffic light, car, person, bicycle and bus have been recognized making it easily retrievable for queries with these concepts.", "labels": [], "entities": []}, {"text": "However, these labels come with uncertainty.", "labels": [], "entities": []}, {"text": "For the image in, there is much higher noise in its visual object labels; so querying by visual labels would notwork here.", "labels": [], "entities": []}, {"text": "Opportunity and Challenge: These limitations of text-based search, on one hand, and visual-object search, on the other hand, suggest combining the cues from text and vision for more effective retrieval.", "labels": [], "entities": []}, {"text": "Although each side of this combined feature space is incomplete and noisy, the hope is that the \"environment friendly traffic\" \"downsides of mountaineering\" \"street-side soulful music\" Figure 2: Sample queries containing abstract concepts and expected results of image retrieval.", "labels": [], "entities": []}, {"text": "combination can improve retrieval quality.", "labels": [], "entities": []}, {"text": "Unfortunately, images that show more sophisticated scenes, or emotions evoked on the viewer are still out of reach.", "labels": [], "entities": []}, {"text": "shows three examples, along with query formulations that would likely consider these sample images as relevant results.", "labels": [], "entities": []}, {"text": "These answers would best be retrieved by queries with abstract words (e.g. \"environment friendly\") or activity words (e.g. \"traffic\") rather than words that directly correspond to visual objects (e.g. \"car\" or \"bike\").", "labels": [], "entities": []}, {"text": "So there is a vocabulary gap, or even concept mismatch, between what users want and express in queries and the visual and textual cues that come directly with an image.", "labels": [], "entities": []}, {"text": "This is the key problem addressed in this paper.", "labels": [], "entities": []}, {"text": "Approach and Contribution: To bridge the concepts and vocabulary between user queries and image features, we propose an approach that harnesses commonsense knowledge (CSK).", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9290158152580261}]}, {"text": "Recent advances in automatic knowledge acquisition have produced large collections of CSK: physical (e.g. color or shape) as well as abstract (e.g. abilities) properties of everyday objects (e.g. bike, bird, sofa, etc.)", "labels": [], "entities": [{"text": "automatic knowledge acquisition", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6505356033643087}, {"text": "CSK: physical (e.g. color or shape) as well as abstract (e.g. abilities) properties of everyday objects (e.g. bike, bird, sofa, etc.)", "start_pos": 86, "end_pos": 219, "type": "Description", "confidence": 0.776788579300046}]}, {"text": "(), subclass and partwhole relations between objects (Tandon et al.,), activities and their participants, and more.", "labels": [], "entities": []}, {"text": "This kind of knowledge allows us to establish relationships between our example queries and observable objects or activities in the image.", "labels": [], "entities": []}, {"text": "For example, the following CSK triples establish relationships between 'backpack', 'tourist' and 'travel map': (backpacks, are carried by, tourists), (tourists, use, travel maps).", "labels": [], "entities": []}, {"text": "This allows for retrieval of images with generic queries like \"travel with backpack\".", "labels": [], "entities": []}, {"text": "This idea is worked out into a query expansion model where we leverage a CSK knowledge base for automatically generating additional query words.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.724966287612915}, {"text": "CSK knowledge base", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.8677075107892355}]}, {"text": "Our model unifies three kinds of features: textual features from the page context of an image, visual features obtained from recognizing fine-grained object classes in an image, and CSK features in the form of additional properties of the concepts referred to by query words.", "labels": [], "entities": []}, {"text": "The weighing of the different features is crucial for query-result ranking.", "labels": [], "entities": [{"text": "query-result ranking", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8165127635002136}]}, {"text": "To this end, we have devised a method based on statistical language models.", "labels": [], "entities": []}, {"text": "The paper's contribution can be characterized as follows.", "labels": [], "entities": []}, {"text": "We present the first model for incorporating CSK into image retrieval.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7622868418693542}]}, {"text": "We develop a fullfledged system architecture for this purpose, along with a query processor and an answer-ranking component.", "labels": [], "entities": []}, {"text": "Our system Know2Look, uses commonsense knowledge to look for images relevant to a query by looking at the components of the images in greater detail.", "labels": [], "entities": []}, {"text": "We further discuss experiments that compare our approach to state-of-the-art image search in various configurations.", "labels": [], "entities": [{"text": "image search", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.7551017999649048}]}, {"text": "Our approach substantially improves the query result quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purpose of demonstration we choose a topical domain -Tourism.", "labels": [], "entities": []}, {"text": "Our CSK knowledge base and image dataset obey this constraint.", "labels": [], "entities": [{"text": "CSK knowledge base", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7745163639386495}]}, {"text": "CSK acquisition through OpenIE: We consider a slice of Wikipedia pertaining to the domain tourism as the text corpus to extract CSK from.", "labels": [], "entities": [{"text": "CSK acquisition", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6733456999063492}]}, {"text": "Nouns from the Wikipedia article titled 'Tourism'(seed document) constitute our basic language model.", "labels": [], "entities": [{"text": "Tourism'(seed document)", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.7839855968952179}]}, {"text": "We collect articles by traversing the Wiki Category hierarchy tree while pruning out those with substantial topic drift.", "labels": [], "entities": [{"text": "Wiki Category hierarchy tree", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.7992046028375626}]}, {"text": "The Jaccard Distance (Equation 6) of a document from the seed document is used as a metric for pruning.", "labels": [], "entities": [{"text": "Jaccard Distance (Equation 6)", "start_pos": 4, "end_pos": 33, "type": "METRIC", "confidence": 0.8745386600494385}]}, {"text": "where, In Equation 7, acquired Wikipedia articles d i are compared to the seed document D; f (d , w) is the frequency of occurrence of word win document d . For simplicity only articles with Jaccard distance of 1 from the seed document are pruned out.", "labels": [], "entities": []}, {"text": "The corpus of domain-specific pages thus collected constitute ~5000 Wikipedia articles.", "labels": [], "entities": []}, {"text": "The OpenIE tool) run against our corpus produces around 1 million noisy SPO triples.", "labels": [], "entities": []}, {"text": "After filtering with our basic language model we have ~22,000 moderately clean assertions.", "labels": [], "entities": []}, {"text": "Image Dataset: For the purpose of experiments we construct our own image dataset.", "labels": [], "entities": [{"text": "Image Dataset", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7900379002094269}]}, {"text": "~50,000 images with descriptions are collected from the following datasets:), Pascal Sentences ( , SBU Captioned Photo Dataset (), and MSCOCO ().", "labels": [], "entities": [{"text": "SBU Captioned Photo Dataset", "start_pos": 99, "end_pos": 126, "type": "DATASET", "confidence": 0.6828320324420929}]}, {"text": "The images are collected by comparing their textual descriptions with our basic language model for Tourism.", "labels": [], "entities": []}, {"text": "An existing object detection algorithm -LSDA) -is used for object detection in the images.", "labels": [], "entities": [{"text": "object detection", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7218473106622696}, {"text": "object detection", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7827326059341431}]}, {"text": "The detected object classes are based on the 7000 leaf nodes of ImageNet ().", "labels": [], "entities": []}, {"text": "We also expand these classes by adding their super-classes or hypernyms with the same confidence score.", "labels": [], "entities": []}, {"text": "Query Benchmark: We construct a benchmark of 20 queries from co-occurring Flickr tags from the YFCC100M dataset (.", "labels": [], "entities": [{"text": "YFCC100M dataset", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.9725442230701447}]}, {"text": "This benchmark is shown in.", "labels": [], "entities": []}, {"text": "Each query consists of two keywords that have appeared together with high frequency as user tags in Flickr images.", "labels": [], "entities": [{"text": "Flickr images", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9142843782901764}]}, {"text": "Baseline Google search results on our image dataset form the baseline for the evaluation of Know2Look.", "labels": [], "entities": []}, {"text": "We consider the results in two settings -search only on original image caption (Vanilla Google), and on image captions along with detected object classes (Extended Google).", "labels": [], "entities": [{"text": "Vanilla Google)", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9403182069460551}]}, {"text": "The later is done to aid", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of Know2Look with baselines", "labels": [], "entities": [{"text": "Know2Look", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.9213289618492126}]}]}