{"title": [{"text": "Find the word that does not belong: A Framework for an Intrinsic Evaluation of Word Vector Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew framework for an intrinsic evaluation of word vector representations based on the outlier detection task.", "labels": [], "entities": [{"text": "outlier detection task", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8134184877077738}]}, {"text": "This task is intended to test the capability of vector space models to create semantic clusters in the space.", "labels": [], "entities": []}, {"text": "We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the task is extremely high compared to the standard word similarity task, and state-of-the-art word embedding models, whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector Space Models have been successfully used on many NLP tasks such as automatic thesaurus generation), word similarity) and clustering), query expansion (, information extraction (), semantic role labeling, spelling correction, and Word Sense Disambiguation.", "labels": [], "entities": [{"text": "automatic thesaurus generation", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.7033979098002116}, {"text": "query expansion", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.7862458825111389}, {"text": "information extraction", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7979929745197296}, {"text": "semantic role labeling", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.7171181241671244}, {"text": "spelling correction", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.8281631171703339}, {"text": "Word Sense Disambiguation", "start_pos": 236, "end_pos": 261, "type": "TASK", "confidence": 0.6127312878767649}]}, {"text": "These models are in the main based on the distributional hypothesis of claiming that words that occur in the same contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been) and have proved beneficial on key NLP applications such as syntactic parsing, Machine Translation (, and Question Answering ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.7948571443557739}, {"text": "Machine Translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.8572153747081757}, {"text": "Question Answering", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.8552169799804688}]}, {"text": "Word similarity, which numerically measures the extent to which two words are similar, is generally viewed as the most direct intrinsic evaluation of these word vector representations (.", "labels": [], "entities": []}, {"text": "Given a gold standard of human-assigned scores, the usual evaluation procedure consists of calculating the correlation between these human similarity scores and scores calculated by the system.", "labels": [], "entities": []}, {"text": "While word similarity has been shown to bean interesting task for measuring the semantic coherence of a vector space model, it suffers from various problems.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7777514159679413}]}, {"text": "First, the human inter-annotator agreement of standard datasets has been shown to be relatively too low for it to be considered a reliable evaluation benchmark (.", "labels": [], "entities": []}, {"text": "In fact, many systems have already surpassed the human inter-annotator agreement upper bound inmost of the standard word similarity datasets (.", "labels": [], "entities": []}, {"text": "Another drawback of the word similarity evaluation benchmark is its simplicity, as words are simply viewed as points in the vector space.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7679566939671835}]}, {"text": "Other interesting properties of vector space models are not directly addressed in the task.", "labels": [], "entities": []}, {"text": "As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items).", "labels": [], "entities": [{"text": "outlier detection task", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7957181334495544}]}, {"text": "As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting language understanding property of vector space models not fully addressed to date, and this is their ability to create semantic clusters in the vector space, with potential applications to various NLP tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7111669927835464}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Outlier Position Percentage (OPP) and  Accuracy (Acc.) of different word embedding  models on the 8-8-8 outlier detection dataset.", "labels": [], "entities": [{"text": "Outlier Position Percentage (OPP)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.766713966925939}, {"text": "Accuracy (Acc.)", "start_pos": 49, "end_pos": 64, "type": "METRIC", "confidence": 0.9399767220020294}]}]}