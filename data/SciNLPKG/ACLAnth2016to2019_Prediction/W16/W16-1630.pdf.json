{"title": [{"text": "Neural Associative Memory for Dual-Sequence Modeling", "labels": [], "entities": [{"text": "Neural Associative Memory", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8682703375816345}, {"text": "Dual-Sequence Modeling", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.740856409072876}]}], "abstractContent": [{"text": "Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks.", "labels": [], "entities": []}, {"text": "Recent advances in building end-to-end neural ar-chitectures have been highly successful in solving such tasks.", "labels": [], "entities": []}, {"text": "In this work we propose anew architecture for dual-sequence modeling that is based on associative memory.", "labels": [], "entities": []}, {"text": "We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN).", "labels": [], "entities": []}, {"text": "This architecture is extended to the Dual AM-RNN which operates on two AMs at once.", "labels": [], "entities": []}, {"text": "Our models achieve very competitive results on textual en-tailment.", "labels": [], "entities": []}, {"text": "A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs.", "labels": [], "entities": []}, {"text": "However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (), recognizing textual entailment (, auto-encoding ( , syntactical parsing ( or document-level question answering ( . We might even argue that most, if not all, NLP problems can (at least partially) be modeled by this paradigm ().", "labels": [], "entities": [{"text": "Dual-sequence modeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7640171349048615}, {"text": "sequence-tosequence modeling", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7230821400880814}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.815654456615448}, {"text": "recognizing textual entailment", "start_pos": 177, "end_pos": 207, "type": "TASK", "confidence": 0.7926244934399923}, {"text": "syntactical parsing", "start_pos": 229, "end_pos": 248, "type": "TASK", "confidence": 0.7287023365497589}, {"text": "document-level question answering", "start_pos": 254, "end_pos": 287, "type": "TASK", "confidence": 0.5940811336040497}]}, {"text": "These models operate on two distinct sequences, the source and the target sequence.", "labels": [], "entities": []}, {"text": "Some tasks require the generation of the target based on the source (sequence-to-sequence modeling), e.g., machine translation, whereas other tasks involve making predictions about a given source and target sequence (dual-sequence modeling), e.g., recognizing textual entailment.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7778094410896301}, {"text": "recognizing textual entailment", "start_pos": 248, "end_pos": 278, "type": "TASK", "confidence": 0.8543194135030111}]}, {"text": "Existing state-of-the-art, end-to-end differentiable models for both tasks exploit the same architectural ideas.", "labels": [], "entities": []}, {"text": "The ability of such models to carry information overlong distances is a key enabling factor for their performance.", "labels": [], "entities": []}, {"text": "Typically this can be achieved by employing recurrent neural networks (RNN) that convey information overtime through an internal memory state.", "labels": [], "entities": []}, {"text": "Most famous is the LSTM) that accumulates information at every time step additively into its memory state, which avoids the problem of vanishing gradients that hindered previous RNN architectures from learning long range dependencies.", "labels": [], "entities": []}, {"text": "For example, connected two LSTMs conditionally for machine translation where the memory state after processing the source was used as initialization for the memory state of the target LSTM.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8041376173496246}]}, {"text": "This very simple architecture achieved competitive results compared to existing, very elaborate and feature-rich models.", "labels": [], "entities": []}, {"text": "However, learning the inherent long range dependencies between source and target requires extensive training on large datasets.", "labels": [], "entities": []}, {"text": "proposed an architecture that resolved this issue by allowing the model to attend overall positions in the source sentence when predicting the target sentence, which enabled the model to automatically learn alignments of words and phrases of the source with the target sentence.", "labels": [], "entities": []}, {"text": "The important difference is that previous long range dependencies could be bridged directly via attention.", "labels": [], "entities": []}, {"text": "However, this architecture requires a larger number of operations that scales with the product of the lengths of the source-and target sequence and a memory that scales with the length of the source sequence.", "labels": [], "entities": []}, {"text": "In this work we introduce a novel architecture for dual-sequence modeling that is based on associative memories (AM).", "labels": [], "entities": [{"text": "dual-sequence modeling", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7512756884098053}]}, {"text": "AMs are fixed sized memory arrays used to read and write content via an associated keys.", "labels": [], "entities": []}, {"text": "Holographic Reduced Representations (HRR)) enable the robust and efficient retrieval of previously written content from redundant memory arrays.", "labels": [], "entities": [{"text": "Holographic Reduced Representations (HRR))", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7429654399553934}]}, {"text": "Our approach is inspired by the works of who recently demonstrated the benefits of exchanging the memory cell of an LSTM with an associative memory on various sequence modeling tasks.", "labels": [], "entities": []}, {"text": "In contrast to their architecture which directly adapts the LSTM architecture we propose an augmentation to generic RNNs (AMRNNs, \u00a73.2).", "labels": [], "entities": [{"text": "AMRNNs", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.8140268921852112}]}, {"text": "Similar in spirit to Neural Turing Machines () we decouple the AM from the RNN and restrict the interaction with the AM to read and write operations which we believe to be important.", "labels": [], "entities": []}, {"text": "Based on this architecture we derive the Dual AM-RNN ( \u00a74) that operates on two associative memories simultaneously for dual-sequence modeling.", "labels": [], "entities": []}, {"text": "We conduct experiments on the task of recognizing textual entailment ( \u00a75).", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.8315411607424418}]}, {"text": "Our results and qualitative analysis demonstrate that AMs can be used to bridge long range dependencies similar to the attention mechanism while preserving the computational benefits of conveying information through a single, fixed-size memory state.", "labels": [], "entities": []}, {"text": "Finally, an initial inspection into sequence-to-sequence modeling with Dual AM-RNNs shows that there are open problems that need to be resolved to make this approach applicable to these kinds of tasks.", "labels": [], "entities": []}, {"text": "A TensorFlow () implementation of (Dual)-AM RNNs can be found at https://github.com/ dirkweissenborn/dual_am_rnn.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracies of different RNN-based architectures on SNLI dataset. We also report the respec- tive hidden dimension H and number of parameters |\u03b8 \u2212E | for each architecture without taking word  embeddings E into account.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9477965831756592}, {"text": "SNLI dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8491226732730865}, {"text": "respec- tive hidden dimension H", "start_pos": 94, "end_pos": 125, "type": "METRIC", "confidence": 0.7050934284925461}]}]}