{"title": [{"text": "Towards Semantic-based Hybrid Machine Translation between Bulgarian and English", "labels": [], "entities": [{"text": "Semantic-based Hybrid Machine Translation", "start_pos": 8, "end_pos": 49, "type": "TASK", "confidence": 0.6022506952285767}]}], "abstractContent": [{"text": "The paper focuses on the creation of a semantic-based hybrid Machine Translation system between Bulgarian and En-glish in the domain of Information Technology.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7248954176902771}]}, {"text": "The preprocessing strategies are presented.", "labels": [], "entities": []}, {"text": "A method for the substitution of English word forms with the synsets or Bulgarian representative lemmas is discussed.", "labels": [], "entities": []}, {"text": "Finally, the creation of a factored model in the Moses system is described.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we present first results from the implementation of a hybrid machine translation system between Bulgarian and English (en\u2194bg) using Word Sense Annotation (WSD) of the source language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7651872932910919}]}, {"text": "There is an existing line of research that aims to combine the advantages of competing approaches to machine translation in a hybrid framework.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7299436777830124}]}, {"text": "summarized several different architectures of hybrid systems using SMT and RBMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9145123958587646}]}, {"text": "Some widely explored ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9418050646781921}]}, {"text": "In our case, after WSD, we use the alignment between Bulgarian and English WordNets for the generation of rules for word substitution.", "labels": [], "entities": [{"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.6422556042671204}, {"text": "word substitution", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7621389627456665}]}, {"text": "On the basis of these rules we generate a corpus with factors, on which the Moses system () is trained: word forms, lemmas and POS tags as factors.", "labels": [], "entities": []}, {"text": "Although we have not improved the baseline, the substitution rules have proven helpful.", "labels": [], "entities": []}, {"text": "We plan to generate more sophisticated rules in our future work.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: in the next section the related work is presented.", "labels": [], "entities": []}, {"text": "Section 3 describes the workflow of the Parallel Corpora Processing.", "labels": [], "entities": []}, {"text": "Section 4 outlines the experiments that have been conducted.", "labels": [], "entities": []}, {"text": "Section 5 introduces some discussion on the results and concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three experiments have been performed: using synset IDs returned by the WSD software (ExpA); using representative target language lemmas for the synsets returned by the WSD software (ExpB); using representative target lemmas where the WSD software is run on domain-adapted wordnets, extended with domain gazetteers and terms (ExpC).", "labels": [], "entities": []}, {"text": "The experiments for en\u2192bg were performed through these steps: (1) annotation of the English text with the IXA pipeline, including tokenization, sentence splitting, part-of-speech tagging and word sense annotation with UKB; (2) substitution of the English word form with the synset (in ExpA) or Bulgarian representative lemma (in ExpB and ExpC); and (3) creating a factored model in the Moses system.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7239838093519211}, {"text": "part-of-speech tagging", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.7117594480514526}, {"text": "UKB", "start_pos": 218, "end_pos": 221, "type": "DATASET", "confidence": 0.9090525507926941}, {"text": "ExpB", "start_pos": 329, "end_pos": 333, "type": "DATASET", "confidence": 0.9255122542381287}, {"text": "ExpC", "start_pos": 338, "end_pos": 342, "type": "DATASET", "confidence": 0.7403427362442017}]}, {"text": "In the direction bg\u2192en we performed similar processing.", "labels": [], "entities": []}, {"text": "Additionally, we provided part-of-speech tags 3 (PoS) from the pipeline, as well as the source language lemma, as factors for Moses.", "labels": [], "entities": []}, {"text": "The PoS factor is important for Bulgarian, since Bulgarian is a morphologically rich language.", "labels": [], "entities": [{"text": "PoS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9864602088928223}]}, {"text": "Here is an example for the procedure we performed with respect to the training, testing and tuning tasks: English sentence: This is real progress ...", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU and NIST results of en\u2194bg experiments with concepts on Batch3 (Batch3a for en\u2192bg and Batch3q", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9941918253898621}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9344674944877625}]}]}