{"title": [{"text": "Modeling verbal inflection for English to German SMT", "labels": [], "entities": [{"text": "Modeling verbal inflection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8274030486742655}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7572080492973328}]}], "abstractContent": [{"text": "German verbal inflection is frequently wrong in standard statistical machine translation approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6444704631964365}]}, {"text": "German verbs agree with subjects in person and number , and they bear information about mood and tense.", "labels": [], "entities": []}, {"text": "For subject-verb agreement, we parse German MT output to identify subject-verb pairs and ensure that the verb agrees with the subject.", "labels": [], "entities": [{"text": "German MT output", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.6364629566669464}]}, {"text": "We show that this approach improves subject-verb agreement.", "labels": [], "entities": []}, {"text": "We model tense/mood translation from English to German by means of a statistical classification model.", "labels": [], "entities": [{"text": "tense/mood translation from English to German", "start_pos": 9, "end_pos": 54, "type": "TASK", "confidence": 0.7632370367646217}]}, {"text": "Although our model shows good results on well-formed data, it does not systematically improve tense and mood in MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9951074123382568}]}, {"text": "Reasons include the need for discourse knowledge, dependency on the domain, and stylistic variety in how tense/mood is translated.", "labels": [], "entities": []}, {"text": "We present a thorough analysis of these problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation of English into German faces two main problems involving verbs: (i) correct placement of the verbs, and (ii) generation of the appropriate inflection for the verb.", "labels": [], "entities": [{"text": "machine translation of English into German", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.8282674252986908}]}, {"text": "The position of verbs in German and English differs greatly and often large-range reorderings are needed to place the German verbs in the correct positions.", "labels": [], "entities": []}, {"text": "showed that the preordering approach applied on Englishto-German SMT overcomes large problems with both missing and misplaced verbs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8315973877906799}]}, {"text": "proposed an approach for handling inflectional problems in English to German SMT, focusing on the problems of sparsity caused by nominal inflection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.627963125705719}]}, {"text": "However, they do not handle the verbs, ensuring neither that verbs appear in the correct position (which is a problem due to the highly divergent word order of English and German), nor that verbs are correctly inflected (problematic due to the richer system of verbal inflection in German).", "labels": [], "entities": []}, {"text": "In many cases, verbs do not match their subjects (in person and number) which makes understanding of translations difficult.", "labels": [], "entities": [{"text": "understanding of translations", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.8666888475418091}]}, {"text": "In addition to person and number, the German verbal inflection also includes information about tense and mood.", "labels": [], "entities": []}, {"text": "If these are wrong (i.e. do not correspond to the tense/mood in the source), very important information, such as point of time and modality of an action/state expressed by the verb, is incorrect.", "labels": [], "entities": []}, {"text": "This can lead to false understanding of the overall sentence.", "labels": [], "entities": []}, {"text": "In this paper, we reimplement the nominal inflection modeling for translation to German presented by and combine it with the reordering of the source data.", "labels": [], "entities": [{"text": "translation to German", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.8857692877451578}]}, {"text": "Ina novel extension, we present a method for correction of the agreement errors, and an approach for modeling the translation of tense and mood from English into German.", "labels": [], "entities": [{"text": "translation of tense and mood from English", "start_pos": 114, "end_pos": 156, "type": "TASK", "confidence": 0.880303578717368}]}, {"text": "While the subject-verb agreement problems are dealt with successfully, modeling of tense/mood translation is problematic due to many reasons which we will analyze in detail.", "labels": [], "entities": [{"text": "tense/mood translation", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.6424707844853401}]}, {"text": "In Section 2, we give an overview of the processing pipeline for handling verbal inflection.", "labels": [], "entities": [{"text": "handling verbal inflection", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.7632447083791097}]}, {"text": "The method for handling subject-verb agreement errors is described in Section 3, while modeling of tense/mood translation is presented in Section 4.", "labels": [], "entities": [{"text": "tense/mood translation", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.6323715820908546}]}, {"text": "The impact of the proposed methods for modeling verbal inflection on the quality of the MT output is shown in Section 5.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9920623302459717}]}, {"text": "An extensive discussion of the problems related to modeling tense/mood is given in Section 6.", "labels": [], "entities": []}, {"text": "Finally, future work is presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although both maximum entropy, as well as CRF models trained on the same data using the same feature set perform equally well, CRF performs better for certain labels as shown in.", "labels": [], "entities": []}, {"text": "We further evaluate the CRF model on test sets from different domains (cf.).", "labels": [], "entities": []}, {"text": "Note that the test sets are well-formed sentences taken from the corpora we work with.", "labels": [], "entities": []}, {"text": "We contrast evaluation results gained on well-formed test data to those obtained for noisy MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 91, "end_pos": 100, "type": "TASK", "confidence": 0.9193834066390991}]}, {"text": "The evaluation on the well-formed data is given in F 1 -scores while the MT output is evaluated with BLEU.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9680342078208923}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.8992181420326233}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.998375415802002}]}, {"text": "The row mostFreqTense is considered to be a baseline: the verbs are annotated with tense which is the most frequent German tense given a specific English tense (cf.   The difference in performance gained on test sets from different domains (although small) raises the question whether the classifier is solely to be trained on in-domain data.", "labels": [], "entities": []}, {"text": "Since we work with MT output of the news test set, we would have to train the classifier only on the news data.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.6833505630493164}, {"text": "news test set", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.7762101888656616}]}, {"text": "Due to the corpus size (272k sentences), we get into sparsity problems since many lexical features are used.", "labels": [], "entities": []}, {"text": "A further reason for using additional (outof-domain) training data are low-frequent labels which then get more training instances.", "labels": [], "entities": []}, {"text": "In summary, the evaluation indicates that a single classifier leads to different results when applied on data from different domains.", "labels": [], "entities": []}, {"text": "Furthermore, the initial experiments showed that having better results on the clean data does not necessarily lead to better results for the noisy MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.949622392654419}]}, {"text": "The baseline SMT system is applied on a news test set from WMT 2015.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9714118838310242}, {"text": "news test set from WMT 2015", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.8411202629407247}]}, {"text": "The baseline MT output we aim at correcting is surprisingly good.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9767554402351379}]}, {"text": "The stem-and surface-based comparison of the verbs in the baseline with the reference revealed that 82% of the verbs in the baseline are already correctly inflected.", "labels": [], "entities": []}, {"text": "This quite high number though takes only 21% of the verbs in the baseline into account: nearly 80% of the verbs in the baseline do not match the reference, i.e. the lexical choice (the lemma) of the verbs differs from the reference.", "labels": [], "entities": []}, {"text": "Our verbal inflection correcting system changes 242 (6%) of the verbs output by the baseline SMT system.", "labels": [], "entities": [{"text": "verbal inflection correcting", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6839726368586222}, {"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9830507636070251}]}, {"text": "Given the strong baseline we work with, we would in fact do worse if we changed more (i.e. already correctly inflected) verbs.", "labels": [], "entities": []}, {"text": "Considering the fact that most of the finite verbs do not match the reference and are thus not considered with automatic metrics such es BLEU (cf. Section 5.2.1), we also carried out a human evaluation which is presented in Section 5.2.2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9979109168052673}]}, {"text": "In, the BLEU scores () of the MT output with predicted verbal inflection are presented.: BLEU scores of MT outputs with corrected verbal inflection.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9992676377296448}, {"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9235454797744751}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9991480112075806}, {"text": "MT outputs", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.8813033699989319}]}, {"text": "70 sentence pairs consisting of the baseline MT output and MT output with corrected verbal inflection with respect to tense and mood were evaluated by four human evaluators.", "labels": [], "entities": []}, {"text": "The evaluators annotated the better translation alternative with 1, the worse one with 2.", "labels": [], "entities": []}, {"text": "For each of the translations, the majority vote (most frequent annotation) was computed.", "labels": [], "entities": []}, {"text": "The counts of the human votes are given in: Results of human evaluation.", "labels": [], "entities": []}, {"text": "1 = better, 2 = worse, 3 = don't know, nA = no majority vote.", "labels": [], "entities": []}, {"text": "Human evaluators prefer the choice of tense (expressed in verbal inflection) made by the baseline.", "labels": [], "entities": []}, {"text": "Only a third of the alternatives with verbal inflection handling are considered to be better than the baseline.", "labels": [], "entities": [{"text": "verbal inflection handling", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.67518150806427}]}, {"text": "An interesting fact is that the annotator agreement in terms of Kappa was only 0.33 which means that the annotators often disagreed which translation alternative was better.", "labels": [], "entities": []}, {"text": "In, a few example MT outputs are shown in which the verbal inflection is correct, while the baseline is incorrect.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.892236739397049}]}, {"text": "The VI translation of SRC1 shows corrected agreement between the plural subject Kl\u00e4ger/claimants and the finite verb legten/presented.", "labels": [], "entities": [{"text": "SRC1", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.6377177238464355}]}, {"text": "The translations of SCR2 and SRC3 show the corrected tense.", "labels": [], "entities": [{"text": "SRC3", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8476933240890503}]}, {"text": "In SRC2, the English verb in past tense is in VI also translated as past tense.", "labels": [], "entities": []}, {"text": "In SRC3, the German translation of the subordinate clause should be past subjunctive as generated by VI.: Example of MT outputs with improved (upper part) and incorrect verbal inflection (lower part).", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 117, "end_pos": 127, "type": "TASK", "confidence": 0.8878185451030731}]}, {"text": "SRC denotes the source sentences, the baseline translations are indicated with BL, while the translations with verbal inflection handling are indicated with VI.", "labels": [], "entities": [{"text": "BL", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.993595540523529}]}, {"text": "The VI translation of intended in SRC4 retains the tense in the source sentences.", "labels": [], "entities": [{"text": "SRC4", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6848695874214172}]}, {"text": "The human evaluators, however, prefer the baseline translation, which switches to present tense.", "labels": [], "entities": []}, {"text": "German has two past tenses: the baseline translation of have rung in SRC5 is perfect (habe geklingelt), while the VI translation is pluperfect (hatte geklingelt).", "labels": [], "entities": [{"text": "SRC5", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9431468844413757}]}, {"text": "Even fora human, it is hard to decide which of the translations is better.", "labels": [], "entities": []}, {"text": "The translation of SRC6 shows a problem with English modal verbs such as could which expose functional ambiguity.", "labels": [], "entities": [{"text": "translation of SRC6", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6276787519454956}]}, {"text": "As subjunctive, could almost always translates into subjunctive German modal k\u00f6nnte.", "labels": [], "entities": []}, {"text": "Thus the model always predicts konjunktiv II given English modals for which the past indicative form equals to the subjunctive form.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Subject-verb distances in German texts.", "labels": [], "entities": [{"text": "Subject-verb distances", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.860624223947525}]}, {"text": " Table 3: Distribution of the tense/mood labels in  the German corpora (given in percentage).", "labels": [], "entities": []}, {"text": " Table 5: Performance of a CRF vs. maximum  entropy classifier gained for a test set containing  5,000 sentence from the news corpus.", "labels": [], "entities": []}, {"text": " Table 6: Classifier evaluation using different fea- tures and different test sets. Each of the clean data  test sets contain 5,000 sentences. Clean data sets  are evaluated in terms of F 1 scores, while the MT  output is evaluated with BLEU.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9542973637580872}, {"text": "BLEU", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.998105525970459}]}, {"text": " Table 7: BLEU scores of MT outputs with cor- rected verbal inflection.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992414712905884}, {"text": "MT outputs", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.8853592872619629}]}, {"text": " Table 8: Results of human evaluation. 1 = better,  2 = worse, 3 = don't know, nA = no majority vote.", "labels": [], "entities": []}]}