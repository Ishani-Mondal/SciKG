{"title": [], "abstractContent": [{"text": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model.", "labels": [], "entities": []}, {"text": "We evaluate our approach on tag induction.", "labels": [], "entities": [{"text": "tag induction", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.899956464767456}]}, {"text": "Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic graphical models are among the most important tools available to the NLP community.", "labels": [], "entities": []}, {"text": "In particular, the ability to train generative models using Expectation-Maximization (EM), Variational Inference (VI), and sampling methods like MCMC has enabled the development of unsupervised systems for tag and grammar induction, alignment, topic models and more.", "labels": [], "entities": [{"text": "tag and grammar induction", "start_pos": 206, "end_pos": 231, "type": "TASK", "confidence": 0.6055347546935081}]}, {"text": "These latent variable models discover hidden structure in text which aligns to known linguistic phenomena and whose clusters are easily identifiable.", "labels": [], "entities": []}, {"text": "Recently, much of supervised NLP has found great success by augmenting or replacing context, features, and word representations with embeddings derived from Deep Neural Networks.", "labels": [], "entities": []}, {"text": "These models allow for learning highly expressive non-convex functions by simply backpropagating prediction errors.", "labels": [], "entities": []}, {"text": "Inspired by , who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in * This research was carried out while all authors were at the Information Sciences Institute.", "labels": [], "entities": []}, {"text": "unsupervised settings, simple neural network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference.", "labels": [], "entities": []}, {"text": "In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood.", "labels": [], "entities": []}, {"text": "The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data.", "labels": [], "entities": []}, {"text": "Using features in unsupervised learning has been a fruitful enterprise () and attempts to combine HMMs and Neural Networks date back to 1991.", "labels": [], "entities": []}, {"text": "Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7149034589529037}]}, {"text": "Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed.", "labels": [], "entities": []}, {"text": "Common to this work and ours is the use of neural networks to produce potentials.", "labels": [], "entities": []}, {"text": "The approach presented here is easily applied to other latent variable models where inference is tractable and are typically trained with EM.", "labels": [], "entities": []}, {"text": "We believe there are three important strengths: 1.", "labels": [], "entities": []}, {"text": "Using a neural network to produce model probabilities allows for seamless integration of additional context not easily represented by conditioning variables in a traditional model.", "labels": [], "entities": []}, {"text": "2. Gradient based training trivially allows for multiple objectives in the same loss function.", "labels": [], "entities": []}, {"text": "3. Rich model representations do not saturate as quickly and can therefore utilize large quantities of unlabeled text.", "labels": [], "entities": []}, {"text": "Our focus in this preliminary work is to present a generative neural approach to HMMs and demonstrate how this framework lends itself to modularity (e.g. the easy inclusion of morphological information via Convolutional Neural Networks \u00a75), and the addition of extra conditioning context (e.g. using an RNN to model the sentence \u00a76).", "labels": [], "entities": []}, {"text": "Our approach will be demonstrated and evaluated on the simple task of part-of-speech tag induction.", "labels": [], "entities": [{"text": "part-of-speech tag induction", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.7969464858373007}]}, {"text": "Future work, should investigate the second and third proposed strengths.", "labels": [], "entities": []}], "datasetContent": [{"text": "Once a model is trained, the one best latent sequence is extracted for every sentence and evaluated on three metrics.", "labels": [], "entities": []}, {"text": "Many-to-One (M-1) Many-to-one computes the most common true part-of-speech tag for each cluster.", "labels": [], "entities": []}, {"text": "It then computes tagging accuracy as if the cluster were replaced with that tag.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9906448125839233}]}, {"text": "This metric is easily gamed by introducing a large number of clusters.", "labels": [], "entities": []}, {"text": "One-to-One (1-1) One-to-One performs the same computation as Many-to-One but only one cluster is allowed to be assigned to a given tag.", "labels": [], "entities": []}, {"text": "This prevents the gaming of M-1.", "labels": [], "entities": [{"text": "M-1", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8959633111953735}]}, {"text": "V-Measure (VM) V-Measure is an F-measure which trades off conditional entropy between the clusters and gold tags.", "labels": [], "entities": []}, {"text": "found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English Penn Treebank results with 45 induced clusters.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.954718828201294}]}, {"text": " Table 3: Exploring different configurations of NHMM", "labels": [], "entities": [{"text": "NHMM", "start_pos": 48, "end_pos": 52, "type": "TASK", "confidence": 0.4758022129535675}]}]}