{"title": [], "abstractContent": [{"text": "In this paper, we investigate the impact of context for the paraphrase ranking task, comparing and quantifying results for multi-word expressions and single words.", "labels": [], "entities": [{"text": "paraphrase ranking task", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.9337413907051086}]}, {"text": "We focus on systematic integration of existing paraphrase resources to produce paraphrase candidates and later ask human annotators to judge paraphrasability in context.", "labels": [], "entities": []}, {"text": "We first conduct a paraphrase-scoring annotation task with and without context for targets that are i) single-and multi-word expressions ii) verbs and nouns.", "labels": [], "entities": []}, {"text": "We quantify how differently annotators score paraphrases when context information is provided.", "labels": [], "entities": []}, {"text": "Furthermore, we report on experiments with automatic paraphrase ranking.", "labels": [], "entities": [{"text": "paraphrase ranking", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7938860356807709}]}, {"text": "If we regard the problem as a binary classification task, we obtain an F1-score of 81.56% and 79.87% for multi-word expressions and single words resp.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.999601423740387}]}, {"text": "Approaching the problem as a learning-to-rank task, we attain MAP scores up to 87.14% and 91.58% for multi-word expressions and single words resp.", "labels": [], "entities": [{"text": "MAP", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9280882477760315}]}, {"text": "using LambdaMART, thus yielding high-quality contextualized paraphrased selection.", "labels": [], "entities": []}, {"text": "Further, we provide the first dataset with paraphrase judgments for multi-word targets in context.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work, we examine the influence of context for paraphrasing of multi-word expressions (MWEs).", "labels": [], "entities": [{"text": "paraphrasing of multi-word expressions (MWEs)", "start_pos": 54, "end_pos": 99, "type": "TASK", "confidence": 0.7642407757895333}]}, {"text": "Paraphrases are alternative ways of writing texts while conveying the same information (.", "labels": [], "entities": []}, {"text": "There are several applications where an automatic text paraphrasing is desired such as text shortening), text simplification, machine translation (), or textual entailment.", "labels": [], "entities": [{"text": "text paraphrasing", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7174300402402878}, {"text": "text shortening", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7607879340648651}, {"text": "text simplification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8223604559898376}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7413413226604462}, {"text": "textual entailment", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7578954696655273}]}, {"text": "Over the last decade, a large number of paraphrase resources have been released including PPDB, which is the largest in size.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.7494130730628967}]}, {"text": "However, PPDB provides only paraphrases without context.", "labels": [], "entities": []}, {"text": "This hampers the usage of such a resource in applications.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the research question on how we can automatically rank paraphrase candidates from abundantly available paraphrase resources.", "labels": [], "entities": []}, {"text": "Most existing work on paraphrases focuses on lexical-, phrase-, sentenceand document level ().", "labels": [], "entities": []}, {"text": "We primarily focus on contextualization of paraphrases based on existing paraphrase resources.", "labels": [], "entities": []}, {"text": "Furthermore, we target multi-worded paraphrases, since single-word replacements are covered well in lexical substitution datasets, such as (.", "labels": [], "entities": []}, {"text": "While these datasets contain multi-word substitution candidates, the substitution targets are strictly single words.", "labels": [], "entities": []}, {"text": "Multi-word expressions are prevalent in text, constituting roughly as many entries as single words in a speaker's lexicon (), and are important fora number of NLP applications.", "labels": [], "entities": []}, {"text": "For example, the work by shows that detection of multiword expressions improves the F-score of a word sense disambiguation task by 5 percent.", "labels": [], "entities": [{"text": "F-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9985098242759705}, {"text": "word sense disambiguation task", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.7516588270664215}]}, {"text": "In this paper, we experiment with both MWE and single words and investigate the difficulty of the paraphrasing task for single words vs. MWEs, using the same contextual features.", "labels": [], "entities": []}, {"text": "Our work, centered in assessing the effect of context for paraphrase ranking of humans and its automatic prediction, includes the following steps: 1) systematic combination of existing paraphrase 1 resources to produce paraphrase candidates for single-and multi-word expressions, 2) collection of dataset for paraphrase ranking/selection annotation task using crowdsourcing, and 3) investigating different machine learning approaches for an automatic paraphrase ranking.", "labels": [], "entities": [{"text": "paraphrase ranking", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8815377652645111}, {"text": "paraphrase ranking", "start_pos": 309, "end_pos": 327, "type": "TASK", "confidence": 0.8412365317344666}, {"text": "paraphrase ranking", "start_pos": 451, "end_pos": 469, "type": "TASK", "confidence": 0.7848888337612152}]}], "datasetContent": [{"text": "In this subsection, we present the processes carried out to collect datasets for the paraphrase ranking task.", "labels": [], "entities": [{"text": "paraphrase ranking task", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.9602618217468262}]}, {"text": "This includes selection of documents, identification of target paraphrases, and generation of candidate paraphrases from existing resources.", "labels": [], "entities": []}, {"text": "We use 2.8k essay sentences from the ANC 2 and BAWE corpora for the annotation task.", "labels": [], "entities": [{"text": "ANC 2", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.7250005602836609}, {"text": "BAWE corpora", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.6903137713670731}]}, {"text": "Target detection and candidate generation: In order to explore the impact of contexts for paraphrasing, the first step is to determine possible targets for paraphrasing, as shown in.", "labels": [], "entities": [{"text": "Target detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7752256691455841}, {"text": "candidate generation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7309285998344421}]}, {"text": "As a matter of fact, every word or MWE in a sentence can be a target for paraphrasing.", "labels": [], "entities": []}, {"text": "When prototyping the annotation setup, we found that five paraphrase targets area reasonable amount to be completed in a single Human Intelligence Task (HIT), a single and self-contained unit of task to be completed and submitted by an annotator to receive a reward in a return 3 . We select targets that have at least five candidates in our combined paraphrase resources.", "labels": [], "entities": []}, {"text": "The paraphrase resources (S) for candidates generations are composed of collections from PPDB (), WordNet and JoBimText distributional thesaurus (DT -only for single words).", "labels": [], "entities": [{"text": "PPDB", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9212976694107056}, {"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9647497534751892}]}, {"text": "For MWE paraphrase targets, we have used different MWE resources.", "labels": [], "entities": [{"text": "MWE paraphrase", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.862831711769104}]}, {"text": "A total of 79,349 MWE are collected from WordNet, STREUSLE) 4 , Wiki50 (Vincze et al., 2011) and the MWE project () . We consider MWEs from this resources to be a paraphrase target when it is possible to generate paraphrase candidates from our paraphrase resources (S).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9826546311378479}, {"text": "Wiki50", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9439911246299744}]}, {"text": "Candidates paraphrases fora target (both single and MWE) are generated as follows.", "labels": [], "entities": []}, {"text": "For each paraphrase target, we retrieve candidates from the resources (S).", "labels": [], "entities": []}, {"text": "When more than five candidates are collected: 1) for single words, we select the top candidates that bear different meanings in context using the automatic sense induction API by, 2) for MWEs we select candidates that are collected from multiple resources in S.", "labels": [], "entities": []}, {"text": "We present five candidates for the workers to select the suitable candidates in context.", "labels": [], "entities": []}, {"text": "We also allow workers to provide their own alternative candidates when they found that none of the provided candidates are suitable in the current context.", "labels": [], "entities": []}, {"text": "shows the Amazon Mechanical Turk user interface for the paraphrase candidate selection task.", "labels": [], "entities": [{"text": "paraphrase candidate selection task", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.8173590302467346}]}, {"text": "We discuss the different statistics and quality of annotations obtained in Section 5.2.", "labels": [], "entities": []}, {"text": "Now we discuss the different experimental results using the K-Nearest Neighbors (kNN) 11 from the scikit-learn 12 machine leaning framework (binary classification setup) and the LambdaMART learning to rank algorithm from the RankLib (learning to rank setup).", "labels": [], "entities": []}, {"text": "We have used 5-fold cross validation on 17k data points (2k MWEs and 15k single) from the crowdsourcing annotation task for both approaches.", "labels": [], "entities": []}, {"text": "The cross-validation is conducted in away that there is no target overlap in in each split, so that our model is forced to learn a delexicalized function that can apply to all targets where substitution candidates are available, cf. (. As evaluation metrics, precision, recall, and Fscore are used for the first setup.", "labels": [], "entities": [{"text": "precision", "start_pos": 259, "end_pos": 268, "type": "METRIC", "confidence": 0.9996525049209595}, {"text": "recall", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.999510645866394}, {"text": "Fscore", "start_pos": 282, "end_pos": 288, "type": "METRIC", "confidence": 0.9997977614402771}]}, {"text": "For the second setup we use P@1, Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG).", "labels": [], "entities": [{"text": "P@1", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.938529372215271}, {"text": "Mean Average Precision (MAP)", "start_pos": 33, "end_pos": 61, "type": "METRIC", "confidence": 0.9672774871190389}, {"text": "Normalized Discounted Cumulative Gain (NDCG)", "start_pos": 67, "end_pos": 111, "type": "METRIC", "confidence": 0.7384984961577824}]}, {"text": "P@1 measures the percentage of correct paraphrases at rank 1, thus gives the percentage of how often the best-ranked paraphrase is judged as correct.", "labels": [], "entities": [{"text": "P@1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9608158270517985}]}, {"text": "MAP provides a single-figure measure of quality across recall levels.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6287346482276917}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9925368428230286}]}, {"text": "NDCG is a ranking score that compares the optimal ranking to the system ranking, taking into account situations where many resp.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8330000638961792}]}, {"text": "very few candidates are relevant ().", "labels": [], "entities": []}, {"text": "In the following subsections, we will discuss the performance of the two machine learning setups.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation of human judgment  with PPDB2 default rankings. The column MWE  shows the result of only MWEs and the column  Single shows the result of only single words.", "labels": [], "entities": []}, {"text": " Table 2: Binary classification vs. learning-to-rank  results on baseline and 8 top-performing feature  combinations.", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7895401418209076}]}, {"text": " Table  2. The baseline ranking given by F 0 is con- sistently lower than our context-aware classi- fiers. The best scores are attained with all fea- tures enabled (P@1=89.72, NDCG@5=88.82 and  MAP=91.58 for single words vs. P@1=84.69,  NDCG@5=77.54 and MAP=86.21 for MWEs).  A more detailed analysis between the ranking  of single-worded targets and multi-worded para- phrases will be discussed in Section 5.3.", "labels": [], "entities": [{"text": "F 0", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9452667534351349}, {"text": "MAP", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9621715545654297}, {"text": "MAP", "start_pos": 254, "end_pos": 257, "type": "METRIC", "confidence": 0.9931689500808716}]}, {"text": " Table 4: LambdaMART ranking scores", "labels": [], "entities": []}]}