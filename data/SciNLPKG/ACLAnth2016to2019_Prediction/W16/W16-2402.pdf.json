{"title": [{"text": "Adaptive Importance Sampling from Probabilistic Tree Automata", "labels": [], "entities": [{"text": "Adaptive Importance Sampling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8892260591189066}]}], "abstractContent": [{"text": "We present a general importance sampling technique for approximating expected values based on samples from probabilistic finite tree automata.", "labels": [], "entities": []}, {"text": "The algorithm uses the samples it produces to adapt rule probabilities for the automaton in order to improve sample quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "For different choices of G and f we obtain e.g. (: Feature Expectations for conditional random fields.", "labels": [], "entities": []}, {"text": "Kullback-Leibler Divergence to compare the predictions of different probability models.", "labels": [], "entities": []}, {"text": "Expected Loss for minimum risk decoding.", "labels": [], "entities": [{"text": "Expected Loss", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8270747363567352}]}, {"text": "Log-Likelihood for predicting the next word in language models.", "labels": [], "entities": [{"text": "predicting the next word", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8724104911088943}]}, {"text": "Gradients of those quantities for optimization.", "labels": [], "entities": []}, {"text": "Exact computation of these values is feasible if LA is small or additional assumptions can be made, e.g. if the expected value is defined via semiring operations on the automaton defining LA (.", "labels": [], "entities": [{"text": "LA", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9892170429229736}]}, {"text": "give an exact semiring solution if two key assumptions can be made.", "labels": [], "entities": []}, {"text": "First the definition of the probability G(T ) must decompose into smaller derivation steps along the rules of A.", "labels": [], "entities": []}, {"text": "Second the number of rules of A cannot be too large, as they must all be visited.", "labels": [], "entities": []}, {"text": "The first assumption is violated when e.g. non-local features are used to define probabilities or when probabilities are defined by recurrent neural nets that use hidden states derived from whole subtrees.", "labels": [], "entities": []}, {"text": "The second assumption is violated when e.g. tree automata are used to represent parse charts for combinatorially complex objects like in graph parsing (.", "labels": [], "entities": [{"text": "graph parsing", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.7837038040161133}]}, {"text": "When semiring techniques are not applicable, it is necessary to use approximation techniques.", "labels": [], "entities": []}, {"text": "One popular technique is the use of Monte Carlo methods, i.e. sampling.", "labels": [], "entities": []}, {"text": "It is often based on Markov Chain Monte Carlo () or Particle Monte Carlo) approaches and requires minimal knowledge about the expected value being approximated.", "labels": [], "entities": []}, {"text": "In this work we develop an importance sampler based on pRTAs which can be used to approximate expected values in settings where exact solutions are infeasible.", "labels": [], "entities": []}, {"text": "One can efficiently sample from a pRTA making it a suitable tool for generating proposals for importance sampling, as we show in Section 3.2.", "labels": [], "entities": [{"text": "importance sampling", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7883014380931854}]}, {"text": "Good performance of importance sampling requires choosing rule probabilities for the pRTA to closely approximate the target distribution.", "labels": [], "entities": []}, {"text": "One can attempt to derive rule probabilities that achieve this by analyzing the target distribution a priori or by using a proposal that is known to be a good fit).", "labels": [], "entities": []}, {"text": "We present a technique for self-adaption in Section 4 that allows the sampler to learn a good proposal distribution on its own.", "labels": [], "entities": []}, {"text": "Following recent advances in adaptive importance sampling () our technique picks the best possible rule probabilities for the pRTA according to an appropriate quality measure.", "labels": [], "entities": []}, {"text": "Both the generation of proposals from a pRTA and the adaption procedure we propose allow fora lazy implementation that only needs to visit states seen when drawing a sample.", "labels": [], "entities": []}, {"text": "Therefore our algorithm can deal with very large automata.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created an implementation of our approach in order to investigate its behavior.", "labels": [], "entities": []}, {"text": "It, along with all evaluation data and documentation, is available via https://bitbucket.org/tclup/alto/ wiki/AdaptiveImportanceSampling as part of Alto, an Interpreted Regular Tree Grammar () toolkit.", "labels": [], "entities": []}, {"text": "We encourage readers to use the sampler in their own experiments.", "labels": [], "entities": []}, {"text": "In this section we will evaluate the ability of our algorithm to learn good proposal weights in an artificial data experiment that specifically focuses on this aspect.", "labels": [], "entities": []}, {"text": "This will show whether the theoretical guarantees correspond to practical benefits.", "labels": [], "entities": []}, {"text": "In future work we will evaluate the algorithm in End-to-End NLP experiments to see how it interacts with a larger tool chain.", "labels": [], "entities": []}, {"text": "We tested our approach by computing a constant expected value E G(T ).", "labels": [], "entities": [{"text": "expected value E G(T )", "start_pos": 47, "end_pos": 69, "type": "METRIC", "confidence": 0.7615532875061035}]}, {"text": "The ratio 1 * G(T ) PA \u03b8 (T ) be- comes 1 if PA \u03b8 perfectly matches G and a single sample would suffice for an exact estimate.", "labels": [], "entities": []}, {"text": "Therefore any error in the estimate is directly due to a mismatch between G and PA \u03b8 . We defined G through a pRTA with the same structure as the automaton we are sampling from.", "labels": [], "entities": []}, {"text": "Therefore it is possible for PA \u03b8 to exactly match G.", "labels": [], "entities": [{"text": "PA \u03b8", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9553313255310059}]}, {"text": "If we were told the correct parameters for \u03b8 then we would obtain a perfect sample in the first step and a good sample should be generated if the SGD steps successfully move \u03b8 from 0 to the values defining G.", "labels": [], "entities": []}, {"text": "The parameters for G were randomly chosen so that the resulting probabilities for rules given their left hand sides where distributed according to asymmetric Dirichlet Distribution.", "labels": [], "entities": []}, {"text": "This is a good model for probability distributions that describe natural language processes).", "labels": [], "entities": []}, {"text": "The symmetric Dirichlet Distribution is parametrized by a concentration parameter \u03b3.", "labels": [], "entities": []}, {"text": "The rule weights become more likely to be concentrated on a few of the rules for each left hand side as \u03b3 goes toward 0.", "labels": [], "entities": []}, {"text": "Therefore many trees will be improbable according to G.", "labels": [], "entities": []}, {"text": "We obtain a complete evaluation problem by giving the structure of the automata used.", "labels": [], "entities": []}, {"text": "As stated, we use the same automaton to specify G and A \u03b8 save for \u03b8 which we initialize to be 0 for all entries.", "labels": [], "entities": []}, {"text": "We chose a structure similar to a CKY parse chart for the underlying rules and states.", "labels": [], "entities": []}, {"text": "Given a length parameter l \u2208 N we added to the automaton all states of the form i, j with 0 \u2264 i < j \u2264 l.", "labels": [], "entities": []}, {"text": "The automaton has all rules of the form i, i + 1 \u2192 i() and all rules i, j \u2192 * (i, h, h, j) with 0 \u2264 i < h < j \u2264 land * an arbitrary functor.", "labels": [], "entities": []}, {"text": "Therefore the parameters for our evaluation problems are land \u03b3.", "labels": [], "entities": []}, {"text": "A central concern in making SGD based algorithms efficient is the choice of \u03b1 (n) . We use an adaptive strategy for configuring the learning rates ().", "labels": [], "entities": [{"text": "SGD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9594491720199585}]}, {"text": "These schemes usually have convergence proofs that require much stricter conditions than \"vanilla\" SGD and we therefore cannot claim that \u03b8 (n) will converge with these approaches, but in practice they often perform well.", "labels": [], "entities": []}, {"text": "Concretely, we use the technique for setting learning rates that was introduced by which uses \u03b1 and divides it by the sum of all the gradient estimates seen so far to obtain the learning rate for each dimension.", "labels": [], "entities": []}, {"text": "\u03b1 is fixed ahead of time -we chose 0.5 and we found that values between 1.0 and 0.1 could be used interchangeably to obtain the best performance.", "labels": [], "entities": []}, {"text": "We set the regularization parameter \u03bb in our objective o (\u03b8) to 100 for comparatively weak regularization.", "labels": [], "entities": []}, {"text": "Note the tendency towards underestimation in all experiments.", "labels": [], "entities": []}, {"text": "This indicates that the algorithm proposes many trees with low probability under G and has to adapt in order to find more likely trees.", "labels": [], "entities": []}, {"text": "For l = 20 and \u03b3 = 0.5 and k = 500 the sampler converges in 40 iterations.", "labels": [], "entities": []}, {"text": "Note that performance after four steps with k = 500 is better than with one step of k = 2000.", "labels": [], "entities": []}, {"text": "This shows that adapting parameters provides a benefit over simply increasing the sample size.", "labels": [], "entities": []}, {"text": "With \u03b3 = 0.1, l = 20 and k = 500 the samples improve much more slowly.", "labels": [], "entities": []}, {"text": "This is to be expected as there are more than 1 billion trees in LA and only very few of them will be assigned large probabilities by the more peaky rule probabilities.", "labels": [], "entities": []}, {"text": "Therefore the algorithm has to randomly find these few trees to produce a good estimate both for the evaluated value and for the gradient used in adaptation.", "labels": [], "entities": []}, {"text": "When k = 2000 there are faster improvements as the algorithm has a better gradient estimate.", "labels": [], "entities": []}, {"text": "Convergence is also slower when l is increased to 30 as the number of trees to consider rises and the amount of parameters in \u03b8 grows in the order of O(l 3 ).", "labels": [], "entities": [{"text": "Convergence", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9536004662513733}, {"text": "O", "start_pos": 150, "end_pos": 151, "type": "METRIC", "confidence": 0.9792990684509277}]}, {"text": "Convergence speed again increases if we set the sample size k = 2000.", "labels": [], "entities": [{"text": "Convergence speed", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.9189970791339874}]}, {"text": "Overall we can see that the adaption steps improve the quality of our importance sampler and lead to a simple, yet versatile algorithm for approximating expected values.", "labels": [], "entities": []}], "tableCaptions": []}