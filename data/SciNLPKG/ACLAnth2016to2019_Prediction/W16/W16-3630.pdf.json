{"title": [{"text": "Real-Time Understanding of Complex Discriminative Scene Descriptions", "labels": [], "entities": [{"text": "Real-Time Understanding of Complex Discriminative Scene Descriptions", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.8170015130724225}]}], "abstractContent": [{"text": "Real-world scenes typically have complex structure, and utterances about them consequently do as well.", "labels": [], "entities": []}, {"text": "We devise and evaluate a model that processes descriptions of complex configurations of geometric shapes and can identify the described scenes among a set of candidates, including similar distractors.", "labels": [], "entities": []}, {"text": "The model works with raw images of scenes, and by design can work word-byword in-crementally.", "labels": [], "entities": []}, {"text": "Hence, it can be used in highly-responsive interactive and situated settings.", "labels": [], "entities": []}, {"text": "Using a corpus of descriptions from game-play between human subjects (who found this to be a challenging task), we show that reconstruction of description structure in our system contributes to task success and supports the performance of the word-based model of grounded semantics that we use.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present and evaluate a language processing pipeline that enables an automated system to detect and understand complex referential language about visual objects depicted on a screen.", "labels": [], "entities": []}, {"text": "This is an important practical capability for present and future interactive spoken dialogue systems.", "labels": [], "entities": []}, {"text": "There is a trend toward increasing deployment of spoken dialogue systems for smartphones, tablets, automobiles, TVs, and other settings where information and options are presented on-screen along with an interactive speech channel in which visual items can be discussed).", "labels": [], "entities": []}, {"text": "Similarly, for future systems such as smartphones, quadcopters, or selfdriving cars that are equipped with cameras, users * The work was done while at Bielefeld University.", "labels": [], "entities": []}, {"text": "may wish to discuss objects visible to the system in camera images or video streams.", "labels": [], "entities": []}, {"text": "A challenge in enabling such capabilities fora broad range of applications is that human speakers draw on a diverse set of perceptual and language skills to communicate about objects in situated visual contexts.", "labels": [], "entities": []}, {"text": "Consider the example in, drawn from the corpus of RDG-Pento games (discussed further in Section 2).", "labels": [], "entities": []}, {"text": "In this example, a human in the director role describes the visual scene highlighted in red (the target image) to another human in the matcher role.", "labels": [], "entities": []}, {"text": "The scene description is provided in one continuous stream of speech, but it includes three functional segments each providing different referential information: [this one is kind of a uh a blue T] [and a wooden w sort of ] [the T is kind of malformed].", "labels": [], "entities": []}, {"text": "The first and third of these three segments refer to the object at the top left of the target image, while the middle segment refers to the object at bottom right.", "labels": [], "entities": []}, {"text": "An ability to detect the individual segments of language that carry information about individual referents is an important part of deciphering a scene description like this.", "labels": [], "entities": []}, {"text": "Beyond detection, actually understanding these referential segments in context seems to require perceptual knowledge of vocabulary for colors, shapes, materials and hedged descriptions like kind of a blue T.", "labels": [], "entities": []}, {"text": "In other game scenarios, it's important to understand plural references like two brown crosses and relational expressions like this one has the Lon top of the T.", "labels": [], "entities": []}, {"text": "A variety of vocabulary knowledge is needed, as different speakers may describe individual objects in very different ways (the object described as kind of a blue T may also be called a blue odd-shaped piece or a facebook).", "labels": [], "entities": []}, {"text": "When many scenes are described by the same pair of speakers, the pair tends to entrain or align to each other's vocabulary, for example by settling on facebook as a shorthand description for this ob-ject type.", "labels": [], "entities": []}, {"text": "Finally, to understand a full scene description, the matcher needs to combine all the evidence from multiple referential segments involving a group of objects to identify the target image.", "labels": [], "entities": []}, {"text": "In this paper, we define and evaluate a language processing pipeline that allows many of these perceptual and language skills to be integrated into an automated system for understanding complex scene descriptions.", "labels": [], "entities": [{"text": "understanding complex scene descriptions", "start_pos": 172, "end_pos": 212, "type": "TASK", "confidence": 0.7490261197090149}]}, {"text": "We take the challenging visual reference game RDG-Pento, shown in, as our testbed, and we evaluate both human-human and automated system performance in a corpus study.", "labels": [], "entities": []}, {"text": "No prior work we are aware of has put forth techniques for grounded understanding of the kinds of noisy, complex, spoken descriptions of visual scenes that can occur in such interactive dialogue settings.", "labels": [], "entities": []}, {"text": "This work describes and evaluates an initial approach to this complex problem, and it demonstrates the critical importance of segmentation and entrainment to achieving strong understanding performance.", "labels": [], "entities": []}, {"text": "This approach extends the prior work () that assumed either that referential language from users has been pre-segmented, or that visual scenes are given not as raw images but as clean semantic representations, or that visual scenes are simple enough to be described with a one-off referring expression or caption.", "labels": [], "entities": []}, {"text": "Our work makes none of these assumptions.", "labels": [], "entities": []}, {"text": "Our automated pipeline, discussed in Section 3, includes components for learning perceptually grounded word meanings, segmenting a stream of speech, identifying the type of referential language in each speech segment, resolving the references in each type of segment, and aggregating evidence across segments to select the most likely target image.", "labels": [], "entities": [{"text": "learning perceptually grounded word meanings", "start_pos": 72, "end_pos": 116, "type": "TASK", "confidence": 0.6281262159347534}]}, {"text": "Our technical approach enables all of these components to be trained in a supervised manner from annotated, in-domain, human-human reference data.", "labels": [], "entities": []}, {"text": "Our quantitative evaluation, presented in Section 4, looks at the performance of the individual components as well as the overall pipeline, and quantifies the strong importance of segmentation, segment type identification, and speaker-specific vocabulary entrainment for improving performance in this task.", "labels": [], "entities": [{"text": "segment type identification", "start_pos": 194, "end_pos": 221, "type": "TASK", "confidence": 0.722813089688619}, {"text": "speaker-specific vocabulary entrainment", "start_pos": 227, "end_pos": 266, "type": "TASK", "confidence": 0.5769341091314951}]}], "datasetContent": [{"text": "We first evaluate the segmenter and segment type classifier as individual modules.", "labels": [], "entities": []}, {"text": "We then evaluate the entire processing pipeline and explore the impact of several factors on pipeline performance.", "labels": [], "entities": []}, {"text": "Task & Data We used the annotated RDGPento data to perform a \"hold-one-dialogue-pairout\" cross-validation of the segmenter.", "labels": [], "entities": [{"text": "RDGPento data", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8075237274169922}]}, {"text": "The task is to segment each speaker's speech for each target image by tagging each word using the tags SEG and NOSEG.", "labels": [], "entities": [{"text": "SEG", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9819757342338562}, {"text": "NOSEG", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.6455327868461609}]}, {"text": "The SEG tag here indicates the last word in the current segment.", "labels": [], "entities": [{"text": "SEG", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9196142554283142}]}, {"text": "gives an example of the tagging.", "labels": [], "entities": []}, {"text": "Task & Data We used the annotated RDGPento data to perform a hold-one-pair-out crossvalidation of the segment type classifier, training a 237 SVM classifier to predict labels SIN, MUL, REL, and OT using the features described in Section 3.2.", "labels": [], "entities": [{"text": "RDGPento data", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.770018994808197}, {"text": "REL", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.8715325593948364}]}, {"text": "Results The results are given in.", "labels": [], "entities": []}, {"text": "We also report the percentage of segments that have each label in the corpus.", "labels": [], "entities": []}, {"text": "The segment type classifier performs well on most of the class labels.", "labels": [], "entities": []}, {"text": "Of slight concern is the low-frequency MUL label.", "labels": [], "entities": []}, {"text": "One factor here is that people use number words like two not just to refer to multiple objects, but also to describe individual objects, e.g., the two red crosses (a MUL segment) vs. the one with two sides (a SIN segment).", "labels": [], "entities": []}, {"text": "We evaluated our pipeline under varied conditions to understand how well it works when segmentation is not performed at all, when the segmentation and type classifier modules produce perfect output (using oracle annotations), and when entrainment to a specific speaker is possible.", "labels": [], "entities": []}, {"text": "We evaluate our pipeline on the accuracy of the task of image retrieval given a scene description from our data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994974136352539}, {"text": "image retrieval", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7701148688793182}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We can observe that object retrieval is by itself a non-trivial problem for our WAC model, especially as the number of objects increases.", "labels": [], "entities": [{"text": "object retrieval", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.779256284236908}]}, {"text": "This is somewhat by design in that the multiple objects present within an image are often selected to be fairly similar in their properties, and multiple objects may match ambiguous SIN segments such as the T or the plus sign.", "labels": [], "entities": []}, {"text": "We speculate that we could gain here from factoring in positional information implicit in description strategies such as going from top left to bottom right in describing the objects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Segment type classifier performance", "labels": [], "entities": [{"text": "Segment type classifier", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9311572909355164}]}, {"text": " Table 4: Image retrieval accuracies for five ver- sions of the pipeline and three baselines.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7192858457565308}]}]}