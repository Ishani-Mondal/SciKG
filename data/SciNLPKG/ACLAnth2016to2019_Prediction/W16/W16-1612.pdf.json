{"title": [{"text": "Mapping Unseen Words to Task-Trained Embedding Spaces", "labels": [], "entities": [{"text": "Mapping Unseen Words to Task-Trained Embedding Spaces", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7791127392223903}]}], "abstractContent": [{"text": "We consider the supervised training setting in which we learn task-specific word embeddings.", "labels": [], "entities": []}, {"text": "We assume that we start with initial embeddings learned from unla-belled data and update them to learn task-specific embeddings for words in the supervised training data.", "labels": [], "entities": []}, {"text": "However, for new words in the test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors.", "labels": [], "entities": []}, {"text": "We address this by learning a neural network to map from initial em-beddings to the task-specific embedding space, via a multi-loss objective function.", "labels": [], "entities": []}, {"text": "The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.833323061466217}, {"text": "sentiment analysis", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.9517918229103088}]}], "introductionContent": [{"text": "Performance on NLP tasks drops significantly when moving from training sets to held-out data (.", "labels": [], "entities": []}, {"text": "One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in anew domain.", "labels": [], "entities": []}, {"text": "We refer to such out-of-trainingvocabulary (OOTV) words as unseen words.", "labels": [], "entities": []}, {"text": "NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7980990707874298}]}, {"text": "Word embeddings can counter the effects of limited training data ().", "labels": [], "entities": []}, {"text": "While the effectiveness of pretrained embeddings can be heavily task-dependent (), there is a great deal of work on updating embeddings during supervised training to make them more task-specific ().", "labels": [], "entities": []}, {"text": "These tasktrained embeddings have shown encouraging results but raise some concerns: (1) the updated embeddings of infrequent words are prone to overfitting, and (2) many words in the test data are not contained in the training data at all.", "labels": [], "entities": []}, {"text": "In the latter case, attest time, systems either use a single, generic embedding for all unseen words or use their initial embeddings (typically derived from unlabelled data)).", "labels": [], "entities": []}, {"text": "Neither choice is ideal: A single unknown embedding conflates many words, while the initial embeddings maybe in a space that is not comparable to the trained embedding space.", "labels": [], "entities": []}, {"text": "In this paper, we address both concerns by learning to map from the initial embedding space to the task-trained space.", "labels": [], "entities": []}, {"text": "We train a neural network mapping function that takes initial word embeddings and maps them to task-specific embeddings that are trained for the given task, via a multi-loss objective function.", "labels": [], "entities": []}, {"text": "We tune the mapper's hyperparameters to optimize performance on each domain of interest, thereby achieving some of the benefits of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.737997978925705}]}, {"text": "We demonstrate significant improvements in dependency parsing across several domains and for the downstream task of dependency-based sentiment analysis using the model of.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8684886693954468}, {"text": "dependency-based sentiment analysis", "start_pos": 116, "end_pos": 151, "type": "TASK", "confidence": 0.7704948782920837}]}], "datasetContent": [{"text": "We consider a number of datasets with varying rates of OOTV words.", "labels": [], "entities": []}, {"text": "We define the OOTV rate (or, equivalently, the unseen rate) of a dataset as the percentage of the vocabulary (types) of words occurring in the set that were not seen in training.", "labels": [], "entities": [{"text": "OOTV rate", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9568818211555481}, {"text": "unseen rate)", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9243555068969727}]}], "tableCaptions": [{"text": " Table 2: Improvements on Stanford Sentiment Treebank test  set using our parser with the Dependency Tree-LSTM.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank test  set", "start_pos": 26, "end_pos": 63, "type": "DATASET", "confidence": 0.931081235408783}]}, {"text": " Table 3: Average Web Treebank development UAS at differ- ent threshold settings.", "labels": [], "entities": [{"text": "Web Treebank development", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.7278170883655548}, {"text": "UAS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.49650195240974426}]}, {"text": " Table 2. We improve upon the  original accuracies in both binary and fine-grained  classification.", "labels": [], "entities": []}, {"text": " Table 4: Comparison to k-nearest neighbor matching of  Tafforeau et al. (2015).", "labels": [], "entities": []}]}