{"title": [{"text": "How Do I Look? Publicity Mining From Distributed Keyword Representation of Socially Infused News Articles", "labels": [], "entities": [{"text": "Publicity Mining From Distributed Keyword Representation of Socially Infused News", "start_pos": 15, "end_pos": 96, "type": "TASK", "confidence": 0.7316076904535294}]}], "abstractContent": [{"text": "Previous work on opinion mining and sentiment analysis mainly concerns product, movie, or literature reviews; few applied this technique to analyze the publicity of person.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7628214061260223}, {"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9323428273200989}]}, {"text": "We present a novel document model-ing method that utilizes embeddings of emotion keywords to perform reader's emotion classification, and calculates a publicity score that serves as a quantifiable measure for the publicity of a person of interest.", "labels": [], "entities": [{"text": "reader's emotion classification", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.6335962265729904}]}, {"text": "Experiments are conducted on two Chinese corpora that in total consists of over forty thousand users' emotional response after reading news articles.", "labels": [], "entities": []}, {"text": "Results demonstrate that the proposed method can outperform state-of-the-art reader-emotion classification methods, and provide a substantial ground for publicity score estimation for candidates of political elections.", "labels": [], "entities": [{"text": "reader-emotion classification", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.7825112342834473}, {"text": "publicity score estimation", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.6570132970809937}]}, {"text": "We believe it is a promising direction for mining the publicity of a person from online social and news media that can be useful for propaganda and other purposes.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Internet has grown into a powerful medium for information dispersion and social interaction, on which one can easily share experiences and emotions instantly.", "labels": [], "entities": [{"text": "information dispersion", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7552433609962463}]}, {"text": "It has become a popular source for sentiment analysis and opinion mining, e.g., movie reviews (), product reviews (), and other subjects.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.97364941239357}, {"text": "opinion mining", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7443530857563019}]}, {"text": "Moreover, human feelings can be quickly identified through automatic emotion classification, as these emotions reflect an individual's feelings and experiences toward certain subject matters.", "labels": [], "entities": [{"text": "automatic emotion classification", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7565552592277527}]}, {"text": "Emotion classification aims to predict the emotion categories (e.g., happy, angry, or worried) to which the given text belongs (.", "labels": [], "entities": [{"text": "Emotion classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9304527342319489}]}, {"text": "There are two aspects of emotions regarding apiece of text, namely, the writer's and the reader's emotion.", "labels": [], "entities": []}, {"text": "The former consists of the emotions expressed by the author, while the latter refers to the emotions that the readers of the text may possess after reading the text.", "labels": [], "entities": []}, {"text": "Recognition of reader-emotion is different from that of writeremotion and maybe even more complicated ().", "labels": [], "entities": [{"text": "Recognition of reader-emotion", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8277555108070374}]}, {"text": "In particular, writers can directly express their emotions through sentiment words; in contrast, reader-emotions possess a more complex nature, as even common words can evoke different types of reader-emotions depending on personal experiences and knowledge of the readers (.", "labels": [], "entities": []}, {"text": "For instance, a news article with the title \"The price of crude oil will rise 0.5% next week\" is just objectively reporting an event without any emotion, but it may invoke emotions like angry or worried in its readers.", "labels": [], "entities": []}, {"text": "In addition, it is possible that more sponsorship opportunities can be obtained from companies or manufacturers if the articles describing a certain product are able to promote greater emotional resonance in the readers.", "labels": [], "entities": []}, {"text": "As online commerce becomes more and more prominent nowadays, a growing amount of customers rely on online reviews to determine their purchases.", "labels": [], "entities": []}, {"text": "Meanwhile, news organizations observe increasing traffic on their online websites as opposed to paper-based publications.", "labels": [], "entities": []}, {"text": "We believe that reader's emotion analysis has a great potential in all domains and applications.", "labels": [], "entities": [{"text": "reader's emotion analysis", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.6782976537942886}]}, {"text": "In light of the above rationale, in this work we attempt to capture the perception of readers toward public figures through recognizing reader's emotion from news articles.", "labels": [], "entities": []}, {"text": "We propose a distributed emotion keyword vector (DEKV) representation for reader-emotion classification, from which we derive a novel method for publicity mining.", "labels": [], "entities": [{"text": "reader-emotion classification", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.8405905067920685}, {"text": "publicity mining", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.8950860798358917}]}, {"text": "It is a practice of monitoring the public opinion toward a certain human subject at a given period of time.", "labels": [], "entities": []}, {"text": "Experiments show that DEKV outperforms other text categorization and reader-emotion classification methods; in turn, these results can be used to conduct publicity mining for propaganda and other public relations purposes.", "labels": [], "entities": [{"text": "reader-emotion classification", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.7695455849170685}, {"text": "publicity mining", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.758741945028305}]}], "datasetContent": [{"text": "We conduct two experiments to test the effectiveness of DEKV.", "labels": [], "entities": [{"text": "DEKV", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.6832072138786316}]}, {"text": "The goal of the first one is detecting the reader-emotion of a news article, and the second one is inferring the publicity of famous public figures.", "labels": [], "entities": [{"text": "detecting the reader-emotion of a news article", "start_pos": 29, "end_pos": 75, "type": "TASK", "confidence": 0.8080160192080906}]}, {"text": "Details are explained in the following sections.", "labels": [], "entities": []}, {"text": "We use a corpus containing 47,285 Chinese news articles 1 for evaluation.", "labels": [], "entities": []}, {"text": "It is a very suitable testbed because it contains a socially infused feature of community voting.", "labels": [], "entities": [{"text": "community voting", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7107602059841156}]}, {"text": "In particular, a reader of a news article can cast a vote expressing his or her feelings after reading this article with the emotion categories include angry, worried, boring, happy, odd, depressing, warm, and informative.", "labels": [], "entities": []}, {"text": "Furthermore, only those with a clear statistical distinction between the highest vote and others determined by a t-test with 95% confidence level are included to ensure the validity of our experiments.", "labels": [], "entities": [{"text": "validity", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.974254846572876}]}, {"text": "The dataset is divided into training and test sets, containing 11,681 and 35,604 articles, respectively.", "labels": [], "entities": []}, {"text": "Detail statistics of the corpus is listed in Table 1.", "labels": [], "entities": [{"text": "the corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.7163203358650208}]}, {"text": "Note that the evaluation excludes informative for it is not considered as an emotion ().", "labels": [], "entities": []}, {"text": "DEKV is based on embeddings learned from the training set using CBOW with default settings in the toolkit, and LLR for keywords in each emotion category as article is represented as a weighted average of keywords and classified by linear SVM (.", "labels": [], "entities": []}, {"text": "Different combinations of the dimension in embeddings and number of keywords are tested, and the best one (500-dimension embeddings with 2,000 keywords/emotion) is compared with other methods described below.", "labels": [], "entities": []}, {"text": "First, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes () is used as baseline (denoted as NB).", "labels": [], "entities": []}, {"text": "Next, we include LDA () as document representation and an SVM classifier (denoted as LDA).", "labels": [], "entities": []}, {"text": "To examine the effect of our keyword extraction approach, an emotion keyword-based model that represents each article as a sparse vector and uses SVM as its classifier, denoted as KW, is also compared.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.776774138212204}]}, {"text": "In addition, we implement a method (denoted as CF) in () that uses extensive features including bigrams, words, metadata, and emotion category words.", "labels": [], "entities": []}, {"text": "To inspect the effect of weighting, we also use the average of keyword vectors trained using the same parameters as DEKV, denote as mean.", "labels": [], "entities": [{"text": "DEKV", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8765304684638977}]}, {"text": "Details of the implementations of these methods are as follows.", "labels": [], "entities": []}, {"text": "We employ CKIP ( for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6865394115447998}]}, {"text": "The dictionary required by Na\u00a8\u0131veNa\u00a8\u0131ve Bayes and LDA is constructed by removing stop words according to a Chinese stop word list provided by, and retaining tokens that makeup 90% of the accumulated frequency.", "labels": [], "entities": []}, {"text": "In other words, the dictionary can cover up to 90% of the tokens in the corpus.", "labels": [], "entities": []}, {"text": "As for unseen events, we use Laplace smoothing in Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, and an LDA toolkit is used to perform the detection of LDA.", "labels": [], "entities": []}, {"text": "Regarding the CF, the words output by the segmentation tool are used.", "labels": [], "entities": []}, {"text": "The information related to news reporter, news category, location of the news event, time (hour of publication) and news agency are treated as the metadata features.", "labels": [], "entities": []}, {"text": "The extracted emotion keywords are used in place of the emotion category words, since the emotion categories was not released in (.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of these systems, we adopt the accuracy measures used by; macro-average (avg M ) and micro-average (avg \u00b5 ) are selected to compute the average performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9990813732147217}, {"text": "micro-average (avg \u00b5 )", "start_pos": 115, "end_pos": 137, "type": "METRIC", "confidence": 0.8964212894439697}]}, {"text": "lists performances of all methods.", "labels": [], "entities": []}, {"text": "As a baseline, the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier is a keyword statistics-based system which can only accomplish a mediocre performance.", "labels": [], "entities": []}, {"text": "Since it only considers surface word weightings, it is difficult to represent inter-word relations.", "labels": [], "entities": []}, {"text": "The overall accuracy of the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier is 56.13%, with the emotion \"Warm\" only achieving 15.09% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994977712631226}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9967706203460693}]}, {"text": "On the contrary, the LDA yields a macro average accuracy of 74.12%, indicating its ability to select important topics for some emotion categories.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.657355010509491}]}, {"text": "However, KW is more effective in finding representative keywords using LLR as weights, obtaining 80.79% accuracy overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9992879033088684}]}, {"text": "Furthermore, it exhibits a more evenly distributed performance among categories than LDA.", "labels": [], "entities": []}, {"text": "Next, CF achieves an overall accuracy of 85.69%, which maybe attributed to its extensive feature engineering.", "labels": [], "entities": [{"text": "CF", "start_pos": 6, "end_pos": 8, "type": "DATASET", "confidence": 0.7859636545181274}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9995749592781067}]}, {"text": "It also obtains the highest accuracy for the category boring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.999488115310669}, {"text": "category boring", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.6366311311721802}]}, {"text": "Finally, when comparing mean and DEKV, it is clear that using a simple average of embeddings is inferior to weighting by LLR.", "labels": [], "entities": [{"text": "mean", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.964286744594574}, {"text": "DEKV", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.8187684416770935}]}, {"text": "DEKV obtains the best macro average accuracy of 89.21%, and six out of seven best per-category accuracy.", "labels": [], "entities": [{"text": "DEKV", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8134745955467224}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9341628551483154}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9848330616950989}]}, {"text": "For the purpose of our next task, we combine finegrained emotions happy, warm, odd into \"positive\", and angry, boring, depressing, worried into \"negative\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of the reader-emotion dataset.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of accuracies from five reader-emotion", "labels": [], "entities": []}]}