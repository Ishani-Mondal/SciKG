{"title": [{"text": "Large-Scale Acquisition of Commonsense Knowledge via a Quiz Game on a Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "Commonsense knowledge is essential for fully understanding language in many situations.", "labels": [], "entities": []}, {"text": "We acquire large-scale commonsense knowledge from humans using a game with a purpose (GWAP) developed on a smartphone spoken dialogue system.", "labels": [], "entities": []}, {"text": "We transform the manual knowledge acquisition process into an enjoyable quiz game and have collected over 150,000 unique common-sense facts by gathering the data of more than 70,000 players over eight months.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7411862015724182}]}, {"text": "In this paper, we present a simple method for maintaining the quality of acquired knowledge and an empirical analysis of the knowledge acquisition process.", "labels": [], "entities": [{"text": "knowledge acquisition process", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.7785447438557943}]}, {"text": "To the best of our knowledge, this is the first work to collect large-scale knowledge via a GWAP on a widely-used spoken dialogue system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-scale knowledge is an essential resource in many natural language processing (NLP) applications.", "labels": [], "entities": []}, {"text": "There have long been efforts devoted to collecting commonsense knowledge, i.e., general knowledge that every person knows (.", "labels": [], "entities": []}, {"text": "We rely on such prior knowledge to understand languages.", "labels": [], "entities": []}, {"text": "For example, consider the sentence \"She went to get strawberries.\"", "labels": [], "entities": []}, {"text": "A human might think she went to the refrigerator in the kitchen or a supermarket in the neighborhood.", "labels": [], "entities": []}, {"text": "Computers, however, do not know that strawberries would be stored in refrigerators.", "labels": [], "entities": []}, {"text": "This paper presents a methodology for acquiring largescale commonsense knowledge from humans.", "labels": [], "entities": []}, {"text": "Early work on commonsense knowledge acquisition includes the Cyc project, where a small group of human annotators organized resources.", "labels": [], "entities": [{"text": "commonsense knowledge acquisition", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.8513826926549276}]}, {"text": "Manually curated resources are of high quality but require significant cost and time to build.", "labels": [], "entities": []}, {"text": "Thus, several studies have automatically constructed knowledge bases on existing resources such as semi-structured or unstructured texts (for example,).", "labels": [], "entities": []}, {"text": "However, commonsense knowledge is so clear for every person that it is often omitted in a text).", "labels": [], "entities": []}, {"text": "For instance, we rarely state in a text that strawberries are stored in refrigerators.", "labels": [], "entities": []}, {"text": "Rather, we often talk about a major production region for strawberries.", "labels": [], "entities": []}, {"text": "Therefore, manual effort is still required to build commonsense knowledge bases.", "labels": [], "entities": []}, {"text": "To reduce the cost of manual knowledge acquisition, some studies explored the use of crowdsourcing, a process that requests various tasks of non-expert workers on the Internet.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7164988368749619}]}, {"text": "The Open Mind Common Sense (OMCS) project () recruited volunteers on the Internet and constructed ConceptNet, a large collection of commonsense knowledge such as (cake, AtLocation, supermarket).", "labels": [], "entities": []}, {"text": "Whereas participants in the OMCS projects entered the commonsense knowledge in Web forms, some studies have transformed the knowledge acquisition process into a type of enjoyable game, called games with a purpose (GWAP).", "labels": [], "entities": [{"text": "knowledge acquisition process", "start_pos": 124, "end_pos": 153, "type": "TASK", "confidence": 0.7807692090670267}]}, {"text": "The advantage of a GWAP is that it is more attractive to humans than the standard annotation processes and is able to collect accurate resources as aside effect of their enjoyment of the games.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use crowdsourced judgments as the gold standard for evaluating the collected knowledge.", "labels": [], "entities": []}, {"text": "We recruited crowd workers on Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.8990947008132935}]}, {"text": "Crowdsourcing 9 and evaluated 6,669 facts that were collected from multiple players.", "labels": [], "entities": []}, {"text": "The facts were sampled from the data collected up to the end of February 2016.", "labels": [], "entities": []}, {"text": "The workers answered whether a given fact was corrector not.", "labels": [], "entities": []}, {"text": "We requested the judgments of five workers for each fact and aggregated them using the multi-class minimax entropy algorithm ().", "labels": [], "entities": []}, {"text": "The facts that were labeled as true consisted of 54% of all the facts.", "labels": [], "entities": []}, {"text": "This is lower than those of previous studies because our game suffered from ASR errors.", "labels": [], "entities": [{"text": "ASR errors", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.7220213711261749}]}, {"text": "To obtain correct knowledge from such noisy collected facts, we needed to aggregate them and estimate confidence scores for each fact.", "labels": [], "entities": [{"text": "confidence scores", "start_pos": 102, "end_pos": 119, "type": "METRIC", "confidence": 0.9616283178329468}]}, {"text": "In this analysis, we validate the performance of the scoring method explained in Section 3.4 in terms of ROC-AUC, performing 3-fold cross validation on the evaluation set.", "labels": [], "entities": [{"text": "ROC-AUC", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9547863602638245}]}, {"text": "To calculate a weight based on hint distances, we determined g by doing a grid search over {2, 4, 8, 16}, searching for the values that: ROC-AUC of estimated confidence scores.", "labels": [], "entities": [{"text": "ROC-AUC", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9969018697738647}]}, {"text": "Note that the number of evaluated facts is different before and after ASR error reduction using the method explained in Section 3.3 (6,669 and 5,669 facts, respectively).", "labels": [], "entities": [{"text": "ASR error reduction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7717672288417816}]}], "tableCaptions": [{"text": " Table 2: Data collected by the quiz game from December 2015 to August 2016.", "labels": [], "entities": []}, {"text": " Table 3: ROC-AUC of estimated confidence scores. Note that the number of evaluated facts is different before and after ASR  error reduction using the method explained in Section 3.3 (6,669 and 5,669 facts, respectively).", "labels": [], "entities": [{"text": "ROC-AUC", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9921000599861145}, {"text": "ASR  error reduction", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.6788521707057953}]}, {"text": " Table 5: Examples of collected facts.", "labels": [], "entities": []}]}