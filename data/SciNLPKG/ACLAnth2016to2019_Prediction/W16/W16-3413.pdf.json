{"title": [{"text": "Semantic Textual Similarity in Quality Estimation", "labels": [], "entities": [{"text": "Semantic Textual Similarity in Quality Estimation", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6663530419270197}]}], "abstractContent": [{"text": "Quality Estimation (QE) predicts the quality of machine translation output without the need fora reference translation.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7566619396209717}, {"text": "machine translation output", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7578830818335215}]}, {"text": "This quality can be defined differently based on the task at hand.", "labels": [], "entities": []}, {"text": "In an attempt to focus further on the adequacy and informativeness of translations, we integrate features of semantic similarity into QuEst, a framework for QE feature extraction.", "labels": [], "entities": [{"text": "QE feature extraction", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.7665356794993082}]}, {"text": "By using methods previously employed in Semantic Textual Similarity (STS) tasks, we use semantically similar sentences and their quality scores as features to estimate the quality of machine translated sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) tasks", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.7933743936674935}]}, {"text": "Preliminary experiments show that finding semantically similar sentences for some datasets is difficult and time-consuming.", "labels": [], "entities": []}, {"text": "Therefore, we opt to start from the assumption that we already have access to semantically similar sentences.", "labels": [], "entities": []}, {"text": "Our results show that this method can improve the prediction of machine translation quality for semantically similar sentences.", "labels": [], "entities": [{"text": "prediction of machine translation", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6495705842971802}]}], "introductionContent": [{"text": "Machine Translation Quality Estimation (MTQE) has been gaining increasing interest in Machine Translation (MT) output assessment, as it can be used to measure different aspects of correctness.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation (MTQE)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8481910909925189}, {"text": "Machine Translation (MT) output assessment", "start_pos": 86, "end_pos": 128, "type": "TASK", "confidence": 0.86666339635849}]}, {"text": "Furthermore, Quality Estimation (QE) tools forego the need fora reference translation and instead predict the quality of the output based on the source.", "labels": [], "entities": []}, {"text": "In this paper, we address the use of semantic correctness in QE by integrating STS measures into the process, without relying on a reference translation.", "labels": [], "entities": []}, {"text": "We propose a set of features that compares MT output to a semantically similar sentence, that has already been assessed, using monolingual STS tools to measure the semantic proximity of the sentence in relation to the second sentence.", "labels": [], "entities": [{"text": "MT output", "start_pos": 43, "end_pos": 52, "type": "TASK", "confidence": 0.8999185860157013}]}, {"text": "The rest of this paper is organised as follows: Section 2 features the state of the art in QE and the context for our research.", "labels": [], "entities": [{"text": "QE", "start_pos": 91, "end_pos": 93, "type": "DATASET", "confidence": 0.6808446049690247}]}, {"text": "Section 3 introduces our approach to integrating semantic information into QE.", "labels": [], "entities": []}, {"text": "Section 4 details our experimental set-up, including the tools we use for our experiments.", "labels": [], "entities": []}, {"text": "Section 5 explains our experiments, details our new STS features and summarises the results we observe when adding these features to QuEst.", "labels": [], "entities": []}, {"text": "Finally, Section 6 presents our concluding remarks and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we start with a brief introduction to the QuEst framework, followed by a description of the settings for the experiments described in this paper.", "labels": [], "entities": [{"text": "QuEst framework", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8004743158817291}]}, {"text": "As mentioned earlier, all our datasets focus on MTQE for English\u2192French MT output.", "labels": [], "entities": []}, {"text": "In all our experiments we have a set of machine translated sentences A for which we need a QE and a set of sentences B, semantically similar to the set of sentences A and for which we have some type of evaluation score available.", "labels": [], "entities": [{"text": "QE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.979282557964325}]}, {"text": "In early experiments, we attempted to use freely available datasets used in previous workshops on machine translation (WMT2012 and WMT2013) for the translation task and within the news domain ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7483057677745819}, {"text": "WMT2012", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9639559984207153}, {"text": "WMT2013", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.818381130695343}, {"text": "translation task", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.9239719212055206}]}, {"text": "The WMT datasets have two main advantages: first, they allow us to compare our system with previous systems for QE and render our experiments replicable.", "labels": [], "entities": [{"text": "WMT datasets", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.780672013759613}]}, {"text": "Second, they have manual evaluations that are available with the machine translations.", "labels": [], "entities": []}, {"text": "Each sentence in the WMT dataset comes with a score between 1 and 5, provided by human annotators.", "labels": [], "entities": [{"text": "WMT dataset", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.8225084543228149}]}, {"text": "However, this method proved to be too time-consuming, as it often required scoring thousands of sentences before finding two that were similar.", "labels": [], "entities": []}, {"text": "The first obstacle we faced in testing our approach with these datasets was the collection of similar sentences against which to compare and evaluate.", "labels": [], "entities": []}, {"text": "We automatically searched large parallel corpora for sentences that yielded high similarity scores.", "labels": [], "entities": []}, {"text": "These corpora included the Europarl corpus (, the Acquis Communautaire () and previous WMT data (from 2012 and 2013).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9950264990329742}, {"text": "Acquis Communautaire", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8453845381736755}, {"text": "WMT data", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.8610823452472687}]}, {"text": "Furthermore, the STS system we use (see Section 4.2) returned many false-positives.", "labels": [], "entities": []}, {"text": "Some sentences which appeared similar to the STS system were actually too different to be usable.", "labels": [], "entities": []}, {"text": "This led to noisy data and unusable results.", "labels": [], "entities": []}, {"text": "The scarcity of semantically similar sentences and the computational cost of finding these sentences, lead us to look into alternate datasets, preferably those with semantic similarity built into the corpus: the DGT-TM and the SICK dataset.", "labels": [], "entities": [{"text": "DGT-TM", "start_pos": 212, "end_pos": 218, "type": "DATASET", "confidence": 0.9401722550392151}, {"text": "SICK dataset", "start_pos": 227, "end_pos": 239, "type": "DATASET", "confidence": 0.8192013204097748}]}, {"text": "All our experiments have the same set-up.", "labels": [], "entities": []}, {"text": "In all cases, we used 500 randomly selected sentences for testing, and the remaining sentences in the respective data-set for training QuEst.", "labels": [], "entities": []}, {"text": "We automatically search large parallel corpora for sentences that yield high similarity scores using the STS system described in section 4.2.", "labels": [], "entities": [{"text": "STS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.8961926698684692}]}, {"text": "We attempt to predict the quality scores of the individual sentences, using the STS features described above, added to QuEst's 17 baseline features.", "labels": [], "entities": [{"text": "STS", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9344851970672607}]}, {"text": "We compare our results to both the QuEst baseline (cf. Section 4.1). and the majority class baseline . We also test our STS-related features separately, without the baseline features, and compare them to the system with the combined system (STS+baseline).", "labels": [], "entities": [{"text": "QuEst baseline", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.823963463306427}]}, {"text": "We use the Mean Absolute Error (MAE) to evaluate the prediction rate of our systems.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 11, "end_pos": 36, "type": "METRIC", "confidence": 0.9223801891009012}]}, {"text": "MAE measures the average magnitude of the errors on the test set, without considering their direction.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6239437460899353}]}, {"text": "Therefore, it is ideal for measuring the accuracy for continuous variables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993909597396851}]}, {"text": "MAE is calculated as per Equation 1.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9548354148864746}, {"text": "Equation", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8994060158729553}]}, {"text": "The Mean Absolute Error calculated using the mean rating in the training set as a projected score for every sentence in the test set.", "labels": [], "entities": [{"text": "Mean Absolute Error", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8282783428827921}]}, {"text": "where n is the number of instances in the test set, xi is the score predicted by the system, and y is the observed score.", "labels": [], "entities": []}, {"text": "In our experiments, we use S-BLEU scores as the observed score.", "labels": [], "entities": []}, {"text": "In order to further test the suitability of our approach for semantically similar sentences, we use the SICK dataset for further experiments.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.8504125475883484}]}, {"text": "SICK (Sentences Involving Compositional Knowledge) is a dataset specifically designed for compositional distributional semantics.", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge)", "start_pos": 6, "end_pos": 50, "type": "TASK", "confidence": 0.6532746076583862}]}, {"text": "It includes a large number of English sentence pairs that are rich in lexical, syntactic and semantic phenomena.", "labels": [], "entities": []}, {"text": "The SICK dataset is generated from existing datasets based on images and video descriptions, and each sentence pair is annotated for relatedness (similarity) and entailment by means of crowd-sourcing techniques).", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7887492775917053}]}, {"text": "This means that we did not need to use the STS tool to annotate the sentences.", "labels": [], "entities": [{"text": "STS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.912059485912323}]}, {"text": "The similarity score is a score between 1 and 5, further described in.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9796358048915863}]}, {"text": "As these scores are obtained by averaging several separate annotations by distinct evaluators, they are continuous, rather than discrete.", "labels": [], "entities": []}, {"text": "As SICK already provides us with sentence pairs of variable similarity, it cuts out the need to search extensively for similar sentences.", "labels": [], "entities": []}, {"text": "Furthermore, the crowd-sourced similarity scores act as a gold standard that eliminates the uncertainty introduced by the automatic STS tool.", "labels": [], "entities": []}, {"text": "This dataset lacks a reliable reference translation to compare against, however..", "labels": [], "entities": []}, {"text": "The resulting dataset consists of 5,000 semantically similar sentence pairs and their French machine translations.", "labels": [], "entities": []}, {"text": "Of this set, 4,500 are used to train an SVM regression model in the same manner as described in Section 5.1.", "labels": [], "entities": []}, {"text": "The remaining 500 sentences are used for testing.", "labels": [], "entities": []}, {"text": "As the SICK dataset is monolingual and therefore lacking in a reference translation, we opted to use a back-translation (into English) as a reference instead of a French translation for these results.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7904751896858215}]}, {"text": "A back-translation is a translation of a translated text back into the original language.", "labels": [], "entities": []}, {"text": "Back-translations are usually used to compare translations with the original text for quality and accuracy, and can help to evaluate equivalence of meaning between the source and target texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9979357719421387}]}, {"text": "In machine translation contexts, they can be used to create a pseudo-source that can be compared against the original source.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7208174169063568}]}, {"text": "used this back-translation as a feature in QE with some success.", "labels": [], "entities": [{"text": "QE", "start_pos": 43, "end_pos": 45, "type": "DATASET", "confidence": 0.7840359210968018}]}, {"text": "They compared the back-translation to the original source using fuzzy match scoring and used the result to estimate the quality of the translation.", "labels": [], "entities": []}, {"text": "The intuition here is that the closer the back translation is to the original source, the better the translation is in the first place.", "labels": [], "entities": []}, {"text": "Following this idea, we use the S-BLEU scores of the back-translations as standins for the MT quality scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9428954720497131}]}, {"text": "We use the MT system described in Section 3.3 for the back-translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.770686149597168}]}, {"text": "shows a sample sentence from the resulting dataset, including the original English sentence pairs and each sentence's MT output.", "labels": [], "entities": []}, {"text": "The crowd-sourced STS score for this sentence pair is 4, indicating that only minor details differ.", "labels": [], "entities": [{"text": "STS score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9727109372615814}]}, {"text": "Results: Results on the SICK datasets are summarised in.", "labels": [], "entities": [{"text": "SICK datasets", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7902979552745819}]}, {"text": "The lowest error rate (MAE) is observed for the system that combined our STS-based features with QuEst's baseline features) just as in the DGT-TM experiments.", "labels": [], "entities": [{"text": "error rate (MAE)", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.9131815910339356}]}, {"text": "We observe that even the STS features on their own outperformed QuEst in this environment.", "labels": [], "entities": []}, {"text": "The cherry-picked examples in are from the SICK dataset, and show that a high STS score between the source sentences can contribute to a high prediction accuracy.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8751515448093414}, {"text": "STS score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.975440263748169}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9433350563049316}]}, {"text": "In both examples, the predicted score for Sentence A is close to the actual observed score.", "labels": [], "entities": [{"text": "Sentence", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.9462644457817078}]}, {"text": "Furthermore, when we filtered the test set for the SICK experiments for sentences with high similarity (4+), we observed an even higher drop in MAE, as demonstrated in.", "labels": [], "entities": [{"text": "MAE", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9974700212478638}]}, {"text": "This suggests that our experiments perform especially well if we select for sentences with high similarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Predicting the S-BLEU scores for DGT-TM -Mean Absolute Error", "labels": [], "entities": [{"text": "Predicting", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.922502338886261}, {"text": "DGT-TM -Mean Absolute Error", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.723833829164505}]}, {"text": " Table 7. SICK Sample Prediction", "labels": [], "entities": [{"text": "SICK Sample Prediction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7468782464663187}]}]}