{"title": [{"text": "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description", "labels": [], "entities": [{"text": "Multimodal Machine Translation", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6659945150216421}, {"text": "Crosslingual Image Description", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6099106172720591}]}], "abstractContent": [{"text": "This paper introduces and summarises the findings of anew shared task at the intersection of Natural Language Processing and Computer Vision: the generation of image descriptions in a target language, given an image and/or one or more descriptions in a different (source) language.", "labels": [], "entities": []}, {"text": "This challenge was organised along with the Conference on Machine Translation (WMT16), and called for system submissions for two task variants: (i) a translation task, in which a source language image description needs to be translated to a target language, (optionally) with additional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image.", "labels": [], "entities": [{"text": "Machine Translation (WMT16)", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.8291455030441284}, {"text": "description generation task", "start_pos": 333, "end_pos": 360, "type": "TASK", "confidence": 0.8022514184316}]}, {"text": "In this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams.", "labels": [], "entities": [{"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.9270163774490356}, {"text": "image description task", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8926709493001302}]}], "introductionContent": [{"text": "In recent years, significant research has been done to address problems that require joint modelling of language and vision.", "labels": [], "entities": []}, {"text": "Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (, image retrieval based on textual and visual cues, visual question answering (), among many others (see) for more examples).", "labels": [], "entities": [{"text": "image description generation", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.7531630396842957}, {"text": "video captioning", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.7580670118331909}, {"text": "image retrieval based on textual and visual cues", "start_pos": 168, "end_pos": 216, "type": "TASK", "confidence": 0.7882239930331707}, {"text": "visual question answering", "start_pos": 218, "end_pos": 243, "type": "TASK", "confidence": 0.6064702073733012}]}, {"text": "With very few exceptions;), these applications are inherently monolingual and existing work explore mostly English data.", "labels": [], "entities": []}, {"text": "In an attempt to push this interdisciplinary field to incorporate a multilingual component, we propose the first shared task on two new applications: Multimodal Machine Translation and Crosslingual Image Description.", "labels": [], "entities": [{"text": "Multimodal Machine Translation", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.7318060795466105}, {"text": "Crosslingual Image Description", "start_pos": 185, "end_pos": 215, "type": "TASK", "confidence": 0.7499339779218038}]}, {"text": "Generally speaking, this shared task targets the generation of image descriptions in a target language, given an image and one or more descriptions in a different (source) language.", "labels": [], "entities": []}, {"text": "More specifically, the task can be addressed from two perspectives:", "labels": [], "entities": []}], "datasetContent": [{"text": "We created anew dataset for the shared task by extending the Flickr30K dataset () into another language.", "labels": [], "entities": [{"text": "Flickr30K dataset", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9572636783123016}]}, {"text": "The Multi30K dataset) contains two types of multilingual data: a corpus of English sentences translated into German (used for Task 1), and a corpus of independently collected English and German sentences (used for Task 2).", "labels": [], "entities": [{"text": "Multi30K dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8586177229881287}]}, {"text": "For the translation corpus, one sentence (of five) was chosen for professional translation such that the final dataset is a combination of short, medium, and long length sentences.", "labels": [], "entities": []}, {"text": "The second corpus consists of crowdsourced descriptions gathered from Crowdflower, 2 where each worker generated an independent description of the image.", "labels": [], "entities": [{"text": "Crowdflower, 2", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9353224833806356}]}, {"text": "We used a translation of the original instructions used to gather the English sentences, in order to ensure as much similarity across the German and English descriptions as possible.", "labels": [], "entities": [{"text": "similarity", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9738571643829346}]}, {"text": "presents an overview of the data available for each task.", "labels": [], "entities": []}, {"text": "The images are publicly available 3 but to en-courage participation we released two types of features extracted from the images.", "labels": [], "entities": []}, {"text": "The use of such features was not mandatory, and participants could also extract image features from the original images in the Flickr30K dataset using their own algorithms.", "labels": [], "entities": [{"text": "Flickr30K dataset", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.9828631281852722}]}, {"text": "We released features extracted from the VGG-19 Convolutional Neural Network (CNN), as described in, from the FC 7 (relu7) and CONV 5,4 layers.", "labels": [], "entities": [{"text": "VGG-19 Convolutional Neural Network (CNN)", "start_pos": 40, "end_pos": 81, "type": "DATASET", "confidence": 0.9138642208916801}, {"text": "FC 7 (relu7) and CONV 5,4 layers", "start_pos": 109, "end_pos": 141, "type": "DATASET", "confidence": 0.8655919035275778}]}, {"text": "We extracted these image features using Caffe RC2 4 with the matlab features reference code from NeuralTalk.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus-level statistics about the translation and the description data over 31,014 images.", "labels": [], "entities": []}, {"text": " Table 4: Official results for the WMT16 Crosslingual Image Description task. The baseline results are  underlined. Systems with grey background indicate use of resources that fall outside the constraints  provided for the shared task. The winning submission, indicated by a \u2022, is significantly different from all  other submissions based on Meteor scores. Submissions marked with a * are not significantly different  compared to the baseline (2 GroundedTranslation C).", "labels": [], "entities": [{"text": "WMT16 Crosslingual Image Description task", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.6679703712463378}]}]}