{"title": [{"text": "Human-like Natural Language Generation Using Monte Carlo Tree Search", "labels": [], "entities": [{"text": "Human-like Natural Language Generation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5712630450725555}]}], "abstractContent": [{"text": "We propose a method of probabilistic natural language generation observing both a syntactic structure and an input of situational content.", "labels": [], "entities": [{"text": "probabilistic natural language generation", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.7866412848234177}]}, {"text": "We employed Monte Carlo Tree Search for this nontrivial search problem, employing context-free grammar rules as search operators and evaluating numerous putative generations from these two aspects using logistic regression and n-gram language model.", "labels": [], "entities": []}, {"text": "Through several experiments, we confirmed that our method can effectively generate sentences with various words and phrasings.", "labels": [], "entities": []}], "introductionContent": [{"text": "People unconsciously produce utterances in daily life according to different situations.", "labels": [], "entities": []}, {"text": "When a person encounters a situation in which a dog eats apiece of bread, he or she retrieves appropriate words and creates a natural sentence, retaining the dependent relationships among the words in proper order, to describe the situation.", "labels": [], "entities": []}, {"text": "This ability of natural language generation (NLG) from situations will become essential for robotics and conversational agents in the future.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8229659696420034}]}, {"text": "However, this problem is intrinsically difficult because it is hard to encode what to say into a sentence while ensuring its syntactic correctness.", "labels": [], "entities": []}, {"text": "We propose to use Monte Carlo tree search (MCTS) (), a stochastic search algorithm for decision processes, to find an optimal solution in the decision space.", "labels": [], "entities": [{"text": "Monte Carlo tree search (MCTS)", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.6847324456487384}]}, {"text": "We build a search tree of possible syntactic trees to generate a sentence, by selecting proper rules through numerous random simulations of possible yields.", "labels": [], "entities": []}, {"text": "2 NLG with MCTS simulations 2.1 MCTS MCTS combines random simulation and best-first search in its search process ().", "labels": [], "entities": []}, {"text": "It has been successfully applied as an algorithm for playing Go game and similar planning problems.", "labels": [], "entities": [{"text": "Go game", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.873683750629425}]}, {"text": "In fact, both Go game and NLG share the same characteristic: their outputs can be evaluated only when their process reaches the last state.", "labels": [], "entities": []}, {"text": "Therefore, we think that the process of NLG can be represented in MCTS simulations.", "labels": [], "entities": []}, {"text": "MCTS uses the upper confidence bounds one (UCB1) value to determine the next move from a viewpoint of multi-armed bandit problem: Here, vi is the winning rate of candidate i, C is an adjustment coefficient, N is the total number of simulations, and n i is the number of visits to the candidate i.", "labels": [], "entities": [{"text": "MCTS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8341018557548523}, {"text": "upper confidence bounds one (UCB1)", "start_pos": 14, "end_pos": 48, "type": "METRIC", "confidence": 0.7242782286235264}, {"text": "adjustment coefficient", "start_pos": 183, "end_pos": 205, "type": "METRIC", "confidence": 0.9598768055438995}]}, {"text": "The first term of equation corresponds to exploitation and the second term corresponds to exploration in simulation, achieving a balanced search between the two factors ().", "labels": [], "entities": []}], "datasetContent": [{"text": "For this purpose, we use logistic regression with partial syntactic trees of a sentence as its features for identifying whether it is natural or not.", "labels": [], "entities": []}, {"text": "illustrates the procedure of building a classifier for structure evaluation.", "labels": [], "entities": []}, {"text": "We used the Brown corpus 1 and extracted 4,661 sentences consisting of three to seven words other than punctuation marks.", "labels": [], "entities": [{"text": "Brown corpus 1", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9821534554163615}]}, {"text": "Those extracted sentences were parsed using the Stanford parser 2 , and a set of CFGs was created based on its result.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9079228639602661}]}, {"text": "The CFGs contained 7,220 grammar rules and 5,867 terminal symbols.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9536818265914917}]}, {"text": "As the training data for the classifier, we regard syntactic subtrees of sentences in the Brown corpus as the positive examples, and subtrees of the sentences generated from random simulation of CFGs as negative examples.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.8739890158176422}]}, {"text": "As we see in, we have prepared 46,610 syntactically incorrect sentences as negative examples -the reason why there are ten times as many negative examples as positive examples is that it is highly possible that more syntactically incorrect than correct sentences can be generated using MCTS simulations with CFGs.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 308, "end_pos": 312, "type": "DATASET", "confidence": 0.9245480298995972}]}, {"text": "We use FREQuent Tree miner (FREQT) 3 to extract syntactic subtrees of sentences (.", "labels": [], "entities": [{"text": "FREQuent Tree miner (FREQT) 3", "start_pos": 7, "end_pos": 36, "type": "DATASET", "confidence": 0.8298397830554417}]}, {"text": "From all of the subtrees obtained, we use the subtrees from which only the terminal symbols have been removed as the features for the classifier, because of our exclusive focus on syntactic structure with nonterminal symbols.", "labels": [], "entities": []}, {"text": "We call the probability of the output from this classifier as Syntactic Probability (SP) (see,).", "labels": [], "entities": []}, {"text": "We evaluated the classifier with 10-fold cross validation and obtained 98% accuracy for the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996135830879211}]}, {"text": "To evaluate the word sequence in a generated sentence, we conducted an experiment to compare the accuracy of evaluation between two kinds of the ngram based scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9993165731430054}]}, {"text": "One is the score calculating the perplexity of trigrams with Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We call this score 'PP'.", "labels": [], "entities": [{"text": "PP", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.5997750759124756}]}, {"text": "The other is the score called Acceptability proposed in, which measures the acceptability of a sentence for an English native speaker.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9954765439033508}]}, {"text": "In this study, we use the Acceptability (AP) below fora sentence s: 3 http://chasen.org/\u02dctaku/software/freqt/ As an n-gram language model p(s), we use trigrams with Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Acceptability (AP)", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.9631324708461761}]}, {"text": "In (2), p uni (s) denotes the probability with a unigram distribution and (2) measures a relative fluency per word as compared to baseline probability p uni .  In this section, we conducted experiments with two cases where we evaluate only syntactic structure of a generated sentence, and evaluate both syntactic structure and n-gram language model characteristic of a generated sentence.", "labels": [], "entities": []}, {"text": "We use the same linguistic resources mentioned in section 4.", "labels": [], "entities": []}, {"text": "In the experiment, we dealt with three cases where the content fora generated sentence has the words, either \"dog, run\", \"dog, eat, bread\", or \"boy, play, basketball\".", "labels": [], "entities": []}, {"text": "As for lexical selection, we put a constraint that a word to generate must have positive bigram counts from the preceding word in the Wiki-5 statistics.", "labels": [], "entities": [{"text": "lexical selection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7120920121669769}]}, {"text": "Setting this constraint avoids unlikely words in advance to achieve more appropriate lexical selection within MTCS framework.", "labels": [], "entities": [{"text": "MTCS framework", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.7491396367549896}]}, {"text": "Furthermore, as for the constraints on the number of simulations and on the length of a generated sentence, they are the same settings mentioned in section 6.1 and 6.3, respectively.", "labels": [], "entities": []}, {"text": "shows an example of generated sentences from different situations.", "labels": [], "entities": []}, {"text": "Comparing AP with PP, when AP is used, a wide variety of words are selected.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8980804085731506}]}, {"text": "As a concrete example, in the case where the words \"dog\", \"eat\" and \"bread\" are specified, when PP was used for evaluation of the n-gram language model, the word \"every\" was selected as an adjective many times.", "labels": [], "entities": []}, {"text": "In contrast, when AP was used, words such as \"another\", \"neither\" and \"all\" were selected.(a) shows the trends in the average values of AP and SP whenever the root node is updated in MCTS simulations with an example of generating a sentence with the specified words {boy, play, basketball}.", "labels": [], "entities": [{"text": "AP", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9707923531532288}]}, {"text": "We see that the value of SP is approximately 0.1 initially and then converges around 0.99 as exploration deepens.", "labels": [], "entities": [{"text": "SP", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9842096567153931}]}, {"text": "As for AP, we have not observed any clear convergence in the exploration process, however, at the initial stage of exploration we have observed instead that generated sentences do not satisfy the generation constraints, e.g., whose length is too short or too long, therefore, the values of more than 100 or less than 20 have been observed. and shows the values of AP of the initial and final 1,000 simulations, respectively.", "labels": [], "entities": [{"text": "AP", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.8992034792900085}, {"text": "AP", "start_pos": 364, "end_pos": 366, "type": "METRIC", "confidence": 0.9962401390075684}]}, {"text": "From these figures, we see that AP converges to a particular value.", "labels": [], "entities": [{"text": "AP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9925758838653564}]}, {"text": "First, we focus on only syntactic structure, and conducted generation experiments to evaluate it.", "labels": [], "entities": []}, {"text": "As the evaluation score fora generated sentence, we employ only 'SP'.", "labels": [], "entities": [{"text": "SP", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9162524342536926}]}, {"text": "Looking at the above sentences, we see that they are syntactically correct -they have syntactic structure of either SVO or SV.", "labels": [], "entities": []}, {"text": "The scores of them are approximately 0.99, therefore, we see that correct syntactic structure are apparently generated based on the classifier.", "labels": [], "entities": []}, {"text": "Next, we conducted an experiment based on both evaluation criteria for syntactic structure and an ngram language model.", "labels": [], "entities": []}, {"text": "The 'win' or 'lose' is decided as explained in section 3.3.", "labels": [], "entities": []}, {"text": "Furthermore, in order to confirm that we can generate sentences of various lengths, we introduce a constraint on sentence length: if a generated sentence has a length shorter than the predefined length, the simulation result is regarded as a lose.", "labels": [], "entities": []}, {"text": "Moreover, as mentioned in section 3.2, we used PP and AP to compare the results evaluated by them.", "labels": [], "entities": [{"text": "AP", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9878355860710144}]}, {"text": "Here, because lower perplexity is better, the simulation result is regarded as a win when the score is less than the average of those of other candidate nodes.", "labels": [], "entities": []}, {"text": "From the results shown in, we see that syntactically correct sentences are generated.", "labels": [], "entities": []}, {"text": "In the case of using AP, we also see that low frequency words were selected in generating sentences, and a sentence is generated without any influence from word frequency.", "labels": [], "entities": []}, {"text": "On the other hand, we have confirmed that when a generated sentence is evaluated by PP, the sentence is influenced more byword fre-   quency than when AP is used.", "labels": [], "entities": []}, {"text": "In this experiment, we aim to generate sentences with specific words as given situational information.", "labels": [], "entities": []}, {"text": "For the output of situation (a) in \"every dog runs her cat\", we have observed the sentences that resulted in \"lose\" during the generation in Table 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: How to determine the final decision", "labels": [], "entities": []}, {"text": " Table 3: Generated sentences based on the evaluation for only", "labels": [], "entities": []}, {"text": " Table 2: Generation with AP and PP", "labels": [], "entities": [{"text": "AP", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9592161178588867}]}, {"text": " Table 5: Sentences that resulted in \"lose\" to generate every dog", "labels": [], "entities": []}, {"text": " Table 4: NLG with situational information. Situation of (a) = {dog,run}, (b) = {dog,eat,bread}, (c) = {boy,play,basketball}.", "labels": [], "entities": []}]}