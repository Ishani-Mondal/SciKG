{"title": [{"text": "Distributed Vector Representations for Unsupervised Automatic Short Answer Grading", "labels": [], "entities": [{"text": "Distributed Vector Representations", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8234134316444397}, {"text": "Unsupervised Automatic Short Answer Grading", "start_pos": 39, "end_pos": 82, "type": "TASK", "confidence": 0.6128757894039154}]}], "abstractContent": [{"text": "We address the problem of automatic short answer grading, evaluating a collection of approaches inspired by recent advances in distributional text representations.", "labels": [], "entities": [{"text": "automatic short answer grading", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.5473950058221817}, {"text": "distributional text representations", "start_pos": 127, "end_pos": 162, "type": "TASK", "confidence": 0.6208259363969167}]}, {"text": "In addition, we propose an un-supervised approach for determining text similarity using one-to-many alignment of word vectors.", "labels": [], "entities": [{"text": "determining text similarity", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7032713294029236}]}, {"text": "We evaluate the proposed technique across two datasets from different domains, namely, computer science and English reading comprehension, that additionally vary between high-school level and undergraduate students.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that the proposed technique often outperforms other compositional distributional semantics approaches as well as vector space methods such as latent semantic analysis.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6540520588556925}]}, {"text": "When combined with a scoring scheme, the proposed technique provides a powerful tool for tackling the complex problem of short answer grading.", "labels": [], "entities": [{"text": "short answer grading", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.583057185014089}]}, {"text": "We also discuss a number of other key points worthy of consideration in preparing viable, easy-to-deploy automatic short-answer grading systems for the real-world.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grading is an important task in schools and colleges in order to assess students' understanding and guide teachers in providing instructive feedback.", "labels": [], "entities": [{"text": "Grading", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7722626328468323}]}, {"text": "However, answer grading is tedious work and the prevalence of Computer Assisted Assessment has been limited to recognition questions with constrained answers such as multiple choice questions.", "labels": [], "entities": [{"text": "answer grading", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.934271514415741}, {"text": "Computer Assisted Assessment", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6811460057894388}]}, {"text": "In this paper, we delve into the topic of automatic assessment of students' constructed responses.", "labels": [], "entities": []}, {"text": "In particular, we consider short answers which area few words or a few sentences long, including everything in between fill-in-the-gap and essay-type answers (.", "labels": [], "entities": []}, {"text": "Automatic short answer grading (ASAG) involves scoring a student answer given an instructorprovided model (reference) answer.", "labels": [], "entities": [{"text": "Automatic short answer grading (ASAG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7355934211186}]}, {"text": "Scoring schemes may optionally be provided to indicate the relative importance of different parts of the model answer.", "labels": [], "entities": []}, {"text": "This is a complex natural language understanding task owing to linguistic variations (the same answer could be articulated in different ways), the subjective nature of assessment (multiple possible correct answers or no correct answer) and lack of consistency inhuman rating.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7979492545127869}]}, {"text": "For example, in, both student answers are correct, but this may not be apparent to a computer system.", "labels": [], "entities": []}, {"text": "In this paper, we employ distributed vector representation of words () for unsupervised ASAG, a task where graded student answers are not provided as training data (although a reference answer is still available).", "labels": [], "entities": [{"text": "ASAG", "start_pos": 88, "end_pos": 92, "type": "TASK", "confidence": 0.9064703583717346}]}, {"text": "This has not yet been systematically explored even though such word embeddings have proven useful in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6410288512706757}]}, {"text": "(However, there has been work using embeddings for supervised ASAG, and neural networks for essay grading.)", "labels": [], "entities": [{"text": "ASAG", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.8421252369880676}]}, {"text": "We conduct an empirical study to compare the proposed method against various other vector aggregation including naive vector addition, Word Mover's Distance (WMD) () and paragraph vectors () using two datasets that come from two different domains.", "labels": [], "entities": []}, {"text": "The first is the undergraduate computer science dataset used in, while the second is a high-school English reading comprehension task which we present and intend to share with the community for future research.", "labels": [], "entities": []}, {"text": "An important feature of the latter dataset is the Question What is the unexpected fact stated by the writer?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Spearman's \u03c1 of the ASAG systems on the CSDataset of Mohler et. al. (2011). There were 87  questions in total, and so only a representative sampling is presented, along with the overall correlation  across all 87 questions. Undefined numbers are indicated by und. (the correlation is undefined when the  variance along any dimension is zero).", "labels": [], "entities": [{"text": "CSDataset of Mohler et. al. (2011)", "start_pos": 50, "end_pos": 84, "type": "DATASET", "confidence": 0.9299184150165982}]}, {"text": " Table 3: Overall and question-wise performance of methods on the reading comprehension dataset. Gold  indicates whether the gold standard is the marking scheme or model answer.", "labels": [], "entities": []}]}