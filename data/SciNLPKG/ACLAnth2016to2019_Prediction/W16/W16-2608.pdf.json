{"title": [{"text": "UdS-(retrain|distributional|surface): Improving POS Tagging for OOV Words in German CMC and Web Data", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7703897655010223}]}], "abstractContent": [{"text": "We present in this paper our three system submissions for the POS tagging subtask of the Empirist Shared Task: Our baseline system UdS-retrain extends a standard training dataset with in-domain training data; UdS-distributional and UdS-surface add two different ways of handling OOV words on top of the baseline system by using either dis-tributional information or a combination of surface similarity and language model information.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7707769870758057}]}, {"text": "We reach the best performance using the distributional model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is a fundamental subtask in many linguistic tool-chains that provides necessary information for subsequent analysis steps such as lemmatization or syntactic parsing.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5795607089996337}, {"text": "syntactic parsing", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7338660061359406}]}, {"text": "Most recent approaches to POS tagging use statistical techniques and can provide excellent results -as long as the tagger is applied to the same kind of text it has been trained on.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9577319920063019}]}, {"text": "When applied out-ofdomain, results tend to be significantly worse.", "labels": [], "entities": []}, {"text": "This problem is particularly pronounced in the case of data from the domain of computer-mediated communication (CMC) such as posts in Internet fora or micro-posts from Twitter.", "labels": [], "entities": []}, {"text": "POS taggers are usually trained on newspaper articles or other edited texts from professional writers, while CMC data often deviates on the lexical, orthographic (e.g., spelling errors, non-capitalization of German nouns) and grammatical level (e.g., sentences without subjects) and contains phenomena such as emoticons or action words that are not covered by standard POS tagsets (.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6961955577135086}]}, {"text": "This paper describes our contribution to the EmpiriST 2015 Shared Task \"Automatic Linguistic Annotation of Computer-Mediated Communication/Social Media\" where we participated in the subtask of adapting POS taggers to German CMC and Web data.", "labels": [], "entities": [{"text": "EmpiriST 2015 Shared Task", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.8933603316545486}, {"text": "Automatic Linguistic Annotation of Computer-Mediated Communication/Social Media", "start_pos": 72, "end_pos": 151, "type": "TASK", "confidence": 0.7677330275376638}]}, {"text": "All three of our submitted systems are at least partially based on a previous tagging system, that we developed in the BMBF funded project \"Analyse und Instrumentarien zur Beobachtung des Schreibgebrauchs im Deutschen.\"", "labels": [], "entities": [{"text": "BMBF", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.8958635926246643}]}, {"text": "We have shown that out-of-vocabulary (OOV) words are particularly problematic when a standard tagger is applied to out-of-domain CMC data.", "labels": [], "entities": []}, {"text": "Therefore, our previous system focuses on OOV words in two ways: First, tagger accuracy can be improved substantially by adding relatively small amounts of manually annotated in-domain (CMC) data to a standard training set ().", "labels": [], "entities": [{"text": "tagger", "start_pos": 72, "end_pos": 78, "type": "TASK", "confidence": 0.9761339426040649}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.932312548160553}]}, {"text": "This method is used in our retrain system that we consider as a baseline.", "labels": [], "entities": []}, {"text": "A further, smaller but still significant improvement can be obtained by using an additional component based on distributional models () that predicts possible POS tags of words which are still OOV under the retrained model.", "labels": [], "entities": []}, {"text": "For the shared task, we modify our system in two ways: First, the annotation guidelines underlying the training data used in our previous work differ in some details from the guidelines of the shared task.", "labels": [], "entities": []}, {"text": "We re-annotate our previous training data to match the new annotation guidelines and use it in addition to the training data provided by the shared task.", "labels": [], "entities": []}, {"text": "Second, we experiment with two different components for predicting POS tags of OOV words.", "labels": [], "entities": []}, {"text": "These experiments resulted in three individual systems: UdS-retrain uses different versions of additional in-domain training data to retrain a POS tagger and constitutes the basis tagger for the other two systems.", "labels": [], "entities": []}, {"text": "UdS-distributional adds a component to predict the POS tag for OOV words based on distributional information similar to; UdS-surface uses a combination of surface similarity and language model perplexity to normalize OOV words in a preprocessing step.", "labels": [], "entities": [{"text": "POS", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8804661631584167}]}, {"text": "Almost all of our system configurations outperform a baseline trained on the TIGER corpus () alone on both datasets (with the exception of surface run 1 on Web); the improvement is especially pronounced on the CMC subcorpus.", "labels": [], "entities": [{"text": "TIGER corpus", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.7750304639339447}, {"text": "CMC subcorpus", "start_pos": 210, "end_pos": 223, "type": "DATASET", "confidence": 0.9258276522159576}]}, {"text": "We achieve the best results on both corpora with the distributional system (87.33% on CMC and 93.55% on Web).", "labels": [], "entities": [{"text": "CMC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9116932153701782}]}, {"text": "An oracle experiment shows that the different models do not subsume each other and perform differently so that there might be room for further benefits through model combinations.", "labels": [], "entities": []}, {"text": "The plan for the paper continues as follows: We give a short overview of our previous work in Section 2 and describe the various data and tagsets used in our experiments in Section 3.", "labels": [], "entities": []}, {"text": "We describe the architecture of our three systems in Section 4 and provide our results in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 provides additional analyses and experiments to better understand our results.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "One interesting question is how different our individual systems really are: Do they subsume each other, or are there opportunities for improvements by combining them?", "labels": [], "entities": []}, {"text": "To address this question, we evaluate as an oracle condition how good a combined tagger would be.", "labels": [], "entities": []}, {"text": "To this end, we evaluate a condition where we take everything as correct that is correctly done by at least one configuration of one of our systems.", "labels": [], "entities": []}, {"text": "This evaluation is thus an upper bound of what an optimal combination of all our apporaches might be able to reach.", "labels": [], "entities": []}, {"text": "We do that within individual systems and across all three systems (see).", "labels": [], "entities": []}, {"text": "We also evaluate for how many tokens all systems get it right (all correct in the table).", "labels": [], "entities": []}, {"text": "We can see that we only profit slightly from combining different variations fora single system, and -as expected -more substantially from combining the three models corresponding to three different approaches.", "labels": [], "entities": []}, {"text": "The all correct evaluation shows that even the system with the worst performance (surface-1) is better than only those cases that all systems have correct, i.e. even this system contributes something and is not subsumed by the others.", "labels": [], "entities": []}, {"text": "In order to understand the remaining problems better, we looked at the remaining hard cases, i.e., tokens that none of our system configurations were able to tag correctly.", "labels": [], "entities": []}, {"text": "show the most frequent mistaggings and the confusions for those POS tags that occur at least 10 times in a dataset.", "labels": [], "entities": [{"text": "confusions", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9799304008483887}]}, {"text": "We can see that we especially struggle with the new adverb derivates; we assume that to be because of their low frequencies, and because the lexical items appear often with the ADV tag in TIGER.", "labels": [], "entities": []}, {"text": "Other hard cases are more typical POS confusion phenomena such as NN vs. NE, ADJD vs. ADV, VVINF vs. VVFIN etc.", "labels": [], "entities": [{"text": "POS confusion", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7659679651260376}]}, {"text": "All of our submitted systems use the Schreibgebrauch data in someway.", "labels": [], "entities": [{"text": "Schreibgebrauch data", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.8593644797801971}]}, {"text": "We have observed in previous work that adding this data improved performance, compared to a model trained on newspaper data, by a large amount Therefore, we want to check, in the next experiment, what our results would be if we had used only the in-domain training data provided by the shared task for each subcorpus.", "labels": [], "entities": []}, {"text": "We see in table 8 that the CMC subcorpus profited substantially from the additional Schreibgebrauch corpus (up to +2.96%); for Web, however, the performance did not change.", "labels": [], "entities": []}, {"text": "We attribute that to the domain differences between Web and CMC.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets used in our models", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results split into OOV and IV words according to the TIGER baseline.", "labels": [], "entities": [{"text": "OOV", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9741520881652832}, {"text": "TIGER", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8773338198661804}]}, {"text": " Table 5: Results for an oracle condition experiment.  In parentheses is the performance of the best run  that contributed to the oracle experiment and the  worst run for the all correct condition.", "labels": [], "entities": []}, {"text": " Table 6: Most frequent mistagged gold standard  tags for CMC. We show the frequency of the  mistagged word compared to the overall occurrence  of that word. Misstagging numbers are higher, as  they refer to the sum of misstaggings by all nine  tagging models.", "labels": [], "entities": [{"text": "Misstagging", "start_pos": 158, "end_pos": 169, "type": "METRIC", "confidence": 0.9510274529457092}]}, {"text": " Table 7: Most frequent mistagged gold standard  tags for Web", "labels": [], "entities": []}, {"text": " Table 8: Results for versions of our systems that  have been trained without our additionally anno- tated training data.", "labels": [], "entities": []}]}