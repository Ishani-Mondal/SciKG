{"title": [{"text": "Sparsifying Word Representations for Deep Unordered Sentence Modeling", "labels": [], "entities": [{"text": "Sparsifying Word Representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6702244877815247}, {"text": "Deep Unordered Sentence Modeling", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.5554480031132698}]}], "abstractContent": [{"text": "Sparsity often leads to efficient and inter-pretable representations for data.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an architecture to infer the appropriate sparsity pattern for the word embeddings while learning the sentence composition in a deep network.", "labels": [], "entities": []}, {"text": "The proposed approach produces competitive results in sentiment and topic classification tasks with high degree of sparsity.", "labels": [], "entities": [{"text": "sentiment and topic classification tasks", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.8846916675567627}]}, {"text": "It is computationally cheaper to compute sparse word representations than existing approaches.", "labels": [], "entities": []}, {"text": "The imposed sparsity is directly controlled by the task considered and leads to more interpretability.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent surge in representation learning has resulted in remarkable advances in a variety of applications including computer vision and speech processing.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.9753493666648865}, {"text": "speech processing", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7304020822048187}]}, {"text": "In the context of natural language processing, much effort has been focused on constructing vector space representations for words through neural language models () and designing appropriate composition functions to apply word embeddings for modeling sentences or documents.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.679060697555542}]}, {"text": "By design, the goal of neural word embedding approaches is to build dense vector representations that capture syntactic and semantic similarities in data (e.g., beautiful, and attractive have similar meanings, as opposed to ugly, and repulsive), that the classic categorical representation of words as indices of a vocabulary fails to capture.", "labels": [], "entities": []}, {"text": "The composition function based on these embeddings can be either unordered (e.g. average of the word representations) or syntactic, wherein the word order is explicity modeled.", "labels": [], "entities": []}, {"text": "While the former class of approaches results in simple architectures that are easily scalable, the latter can provide richer models with much severe computational complexity during training.", "labels": [], "entities": []}, {"text": "Furthermore, the input word vectors are often fine-tuned during the training phase to improve the sentence (or document) classification performance.", "labels": [], "entities": [{"text": "sentence (or document) classification", "start_pos": 98, "end_pos": 135, "type": "TASK", "confidence": 0.693488876024882}]}, {"text": "However, this can lead to severe overfitting and hence regularization strategies such as word-dropout are used and in other cases the original word vectors are augmented to the input as a static channel.", "labels": [], "entities": []}, {"text": "Alternately, approaches that build word representations using different forms of regularization inspired by the linguistic study of word meanings have been effective in modeling sentences.", "labels": [], "entities": []}, {"text": "For example, sparsity regularization can be been used to construct distributed representations) that capture some of the crucial lexical semantics largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy).", "labels": [], "entities": [{"text": "sparsity regularization", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7884218096733093}]}, {"text": "Instead of employing sparsity to regularize word embeddings, we propose to infer appropriate sparsity patterns for pre-learned word vectors: Sparsity has been commonly used to regularize word embeddings in order to effectively govern the relationship between word dimensions and provide interpretable representations.", "labels": [], "entities": []}, {"text": "Examples include the approaches in ( ) (left-top) and ) (left-bottom).", "labels": [], "entities": []}, {"text": "In contrary, we propose to infer sparsity patterns for pre-learned word embeddings in order to preserve only the key semantics required for the sentence classification task (right).", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.7797117233276367}]}, {"text": "to improve the discrimination of sentence representations.", "labels": [], "entities": []}, {"text": "In particular, we consider a unordered composition setting, similar to (, wherein the sentence representation is obtained as the average of the words.", "labels": [], "entities": []}, {"text": "Intuitively, sparsity is imposed to govern the relationship between word dimensions to capture only the semantics crucial to the particular task considered.", "labels": [], "entities": []}, {"text": "For example, in a sentiment analysis task, opposite relationships between adjectives such as beautiful and ugly are more important than gender relationships such as king and queen.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.9558931589126587}]}, {"text": "Surprisingly, without any additional regularization such as worddropout or static channel of word vectors, the proposed approach produces competitive results in sentiment and topic classification tasks with high degree of sparsity.", "labels": [], "entities": [{"text": "sentiment and topic classification tasks", "start_pos": 161, "end_pos": 201, "type": "TASK", "confidence": 0.8159218192100525}]}, {"text": "While it is cheaper to compute sparse word representations than existing approaches ( ) the imposed sparsity is not merely based on the semantics of the space of words, but directly controlled by the task considered.", "labels": [], "entities": []}, {"text": "Furthermore, by automatically learning sparsity masks that preserve only the semantic relationships appropriate for the task at hand, the resulting sentence models are highly discriminative.", "labels": [], "entities": []}, {"text": "For example, in Figures 6(a) and 6(b), we show the sentence representations (word averaging) obtained using the original Glove word embeddings and the proposed ap- proach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed approaches using a set of commonly used text classification datasets both at the sentence level and the document level.", "labels": [], "entities": []}, {"text": "We report the performance of the proposed architecture with respect to the classification task pertaining to each dataset.", "labels": [], "entities": []}, {"text": "This is followed up by investigation of the properties of the sparsified word vectors.", "labels": [], "entities": []}, {"text": "For all classification performance comparisons we used the vanilla DAN with word-dropout regularization, and the proposed DAN + sparsity mask and DAN + binary mask variants.", "labels": [], "entities": []}, {"text": "Datasets: \u2022 IMDB (document level): This dataset () consists of 50,000 labeled instances of movie reviews taken form the movie review site, IMDB.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.9279347658157349}]}, {"text": "Each review can be made up of several sentences and is labeled as either positive or negative.", "labels": [], "entities": []}, {"text": "The dataset also provides a balanced split of 25,000 instances for training and 25,000 instances for testing.", "labels": [], "entities": []}, {"text": "\u2022 SST-fine (sentence level): This sentence level dataset was created by) and extended by).", "labels": [], "entities": []}, {"text": "The sentences are taken from movie review site, Rotten Tomatoes (RT).", "labels": [], "entities": [{"text": "Rotten Tomatoes (RT)", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8611740291118621}]}, {"text": "In our experiments, we use the fine-grain labels for the classification task.", "labels": [], "entities": [{"text": "classification", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.9719110727310181}]}, {"text": "The dataset provides three set for training, validation and testing with each containing and, respectively.", "labels": [], "entities": []}, {"text": "Each row belongs to different dimension.", "labels": [], "entities": []}, {"text": "Original blinddate, micro-device, bible-study, greenfingers, fever-pitched, bogosian, darabont, navona, 66-day, murri Masked screenplay, cinematic, entertaining, fascinating, movie, daughter, he, micro-device, secret, discovers: Demonstration of the discriminative power of sparsified word embeddings -Words with largest`1 largest`largest`1 -norm in the SUBJ dataset.", "labels": [], "entities": [{"text": "SUBJ dataset", "start_pos": 354, "end_pos": 366, "type": "DATASET", "confidence": 0.9151398837566376}]}, {"text": "The words colored in blue occur most commonly found in sentences from the subjective class while words marked in red occur commonly in objective sentences.", "labels": [], "entities": []}, {"text": "eral existing syntactic approaches also utilize the phrase level labels by augmenting them to the training set.", "labels": [], "entities": []}, {"text": "However, we evaluate the three DAN architectures without the phraselevel labels.", "labels": [], "entities": []}, {"text": "\u2022 SUBJ (sentence level): This dataset called as the Subjective dataset () involves classifying a sentence as either being subjective or objective.", "labels": [], "entities": []}, {"text": "This dataset provides 10,000 instances in total and contain separate validation/test set.", "labels": [], "entities": []}, {"text": "\u2022 Reuters (document level): This dataset comprises of 11228 newswires from Reuters.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9356338977813721}]}, {"text": "The task is to classify the newswires into one of the given 46 topics.", "labels": [], "entities": []}, {"text": "There is no standard train/test split for this dataset.", "labels": [], "entities": []}, {"text": "The classification performance on these datasets is reported in table 1.", "labels": [], "entities": []}, {"text": "As it can be observed, the sparsified word vectors ourperform the conventional word embeddings with the DAN architecture and perform competitively with respect to state-of-the-art syntactic methods.", "labels": [], "entities": []}, {"text": "Investigating the properties of the masked work vectors and comparing them to original work vectors can shed some light on the behavior of the sparsification procedure.", "labels": [], "entities": []}, {"text": "shows the mean`1mean`mean`1 -norm of each dimension of the word vector across all the words in the vocabulary for the SST sentiment classification dataset.", "labels": [], "entities": [{"text": "mean`1 -norm", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.8319005370140076}, {"text": "SST sentiment classification", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.9423015912373861}]}, {"text": "The dimensions are ordered by their`1their`their`1 -norm in the original word vector space.", "labels": [], "entities": []}, {"text": "The general behavior remains the same, however with an overall reduction in norm that can be attributed to the sparsity in the masked word vectors.", "labels": [], "entities": []}, {"text": "Similar analysis can be performed with respect to words instead of each word vector dimensions.", "labels": [], "entities": []}, {"text": "In, the blue line corresponds t\u00f2t\u00f2 1 -norm of the top 500 words ordered by the norm.", "labels": [], "entities": []}, {"text": "The norm for the same words in the masked space is shown in red, which indicates that the mask is word-specific and can tune the entries as suited for the task in hand.", "labels": [], "entities": []}, {"text": "Analysis of task-specific mask: To understand the effect of the task-specific mask, we study the similarity of words and compare them in the original word vector space and the sparsified word vector spaces.", "labels": [], "entities": []}, {"text": "shows a couple of example neighborhoods of words in these spaces.", "labels": [], "entities": []}, {"text": "Subjectively, we can see that the word vector semantic space is modified such that word neighborhoods that are more important for the task are preserved and enhanced.", "labels": [], "entities": []}, {"text": "The top words are obtained by sorting the absolute value of the words along each of those dimensions.", "labels": [], "entities": []}, {"text": "Since, there is a direct correspondence between original and masked word vector dimensions, we can directly compare them.", "labels": [], "entities": []}, {"text": "The examples in show that mask improves the semantic consistency and hence improves interpretation of individual dimensions.", "labels": [], "entities": []}, {"text": "Similar analysis is carried our for the SUBJ dataset and the results are reported in Finally, we use the SUBJ dataset to demonstrate the discriminative power of sparsified word embeddings in sentence classification.", "labels": [], "entities": [{"text": "SUBJ dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.875741571187973}, {"text": "SUBJ dataset", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8982433676719666}, {"text": "sentence classification", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.7271642088890076}]}, {"text": "The words with the largest`1largest`largest`1 -norm in the masked vector space in reveal that the sparsity mask identifies a set of words crucial for discriminating the two classes.", "labels": [], "entities": []}, {"text": "Finally, we consider an example sentence in each of the two classes and show the averag\u00e8 1 norms for words in the sentences in.", "labels": [], "entities": [{"text": "averag\u00e8 1 norms", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9522443215052286}]}, {"text": "As it can be observed, words such as emotional and material are crucial to identifying the subjective nature of the sentence while words such as 125 \ud97b\udf59 year which has prominence in the original word vector space has no relevance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence classification performance of the proposed approach in comparison to other meth- ods. In addition to outperforming the deep averaging architecture, our approach achieves competitive  performances in comparison to state-of-the-art syntactic sentence classification methods.", "labels": [], "entities": [{"text": "Sentence classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9401006400585175}, {"text": "sentence classification", "start_pos": 259, "end_pos": 282, "type": "TASK", "confidence": 0.771948516368866}]}]}