{"title": [{"text": "Corpus and dictionary development for classifiers/quantifiers towards French-Japanese machine translation", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.654777005314827}]}], "abstractContent": [{"text": "Although quantifiers/classifiers expressions occur frequently in everyday communications or written documents, there is no description for them in classical bilingual paper dictionaries, nor in machine-readable dictionaries.", "labels": [], "entities": []}, {"text": "The paper describes a corpus and dictionary development for quantifiers/classifiers, and their usage in the framework of French-Japanese machine translation (MT).", "labels": [], "entities": [{"text": "French-Japanese machine translation (MT)", "start_pos": 121, "end_pos": 161, "type": "TASK", "confidence": 0.7849976172049841}]}, {"text": "They often cause problems of lexical ambiguity and of set phrase recognition during analysis , in particular fora long-distance language pair like French and Japanese.", "labels": [], "entities": [{"text": "phrase recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7351148873567581}]}, {"text": "For the development of a dictionary aiming at ambiguity resolution for expressions including quantifiers and classifiers which maybe ambiguous with common nouns, we have annotated our corpus with UWs (interlin-gual lexemes) of UNL (Universal Networking Language) found on the UNL-jp dictionary.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.82130366563797}, {"text": "UNL-jp dictionary", "start_pos": 276, "end_pos": 293, "type": "DATASET", "confidence": 0.9732341468334198}]}, {"text": "The extraction of potential classifiers/quantifiers from corpus is made by UNLexplorer web service.", "labels": [], "entities": [{"text": "UNLexplorer web service", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.9557151794433594}]}], "introductionContent": [{"text": "Recent Machine Translation (MT) evaluation tends to be conducted based on (1) Automatic evaluation metrics use reference translations for each segment such as BLEU, NIST, METEOR (;).", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 7, "end_pos": 42, "type": "TASK", "confidence": 0.8712154428164164}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9975256323814392}, {"text": "NIST", "start_pos": 165, "end_pos": 169, "type": "DATASET", "confidence": 0.8411549925804138}, {"text": "METEOR", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9159407019615173}]}, {"text": "This shows frequent efforts for MT, by measuring a similarity or a distance between a translation hypothesis and its post-editions.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.993232250213623}]}, {"text": "Basic operations used for post-editions are substitution, deletion, and insertion of words or phrases in a sentence, whatever the MT system is.", "labels": [], "entities": []}, {"text": "(2) Subjective measures are based on human judgements of \"intelligibility\", \"fidelity\", \"adequacy\" and \"fluency\" of MT outputs.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 116, "end_pos": 126, "type": "TASK", "confidence": 0.8972832262516022}]}, {"text": "These methods are really suitable for evaluating the progress of MT systems, but they do not contribute directly to improve the quality of MT outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9896822571754456}, {"text": "MT outputs", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.9128638207912445}]}, {"text": "Here we focus on lexical ambiguities, which are considered as a main cause of the degradation of the quality in MT for spoken or written sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9847020506858826}]}, {"text": "Several types of ambiguity appear on each phase of MT for different types of documents.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9877614974975586}]}, {"text": "We have categorized ambiguity problems according to the levels of MT analysis and to the MT contexts in which they are encountered, and we have proposed a formal ambiguity representation as well as guidelines for ambiguity labelling to build an ambiguity database . In fact, according to our studies of ambiguities, 14% of analysis errors 2 are due to polysemous words.", "labels": [], "entities": [{"text": "MT analysis", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9821538031101227}]}, {"text": "Also, (G.) say the most frequent necessary post-edition in their French corpus translation into English is to correct articles like \u00ables\u00bb, \u00able\u00bb, \u00abdu\u00bb, etc., and the next one concerns lexical transfer errors of polysemous words.", "labels": [], "entities": []}, {"text": "In addition, when polysemous words are used in their abstract or figurative meaning where they could be classifier or quantifier, translation results produced by current This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "We have done research on ambiguity analysis from the lexical, semantic and contextual points of view since 1996.", "labels": [], "entities": [{"text": "ambiguity analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.749369889497757}]}, {"text": "Ambiguities have been defined, categorized, and formalized as objects in an ambiguity database, and we have used this theoretical background to label ambiguities in Japanese-English interpreted dialogues, collected for the development of a speech translation system at ATR in Japan.)", "labels": [], "entities": [{"text": "speech translation", "start_pos": 240, "end_pos": 258, "type": "TASK", "confidence": 0.7317279875278473}]}, {"text": "The ambiguity analysis includes assignment of speech acts, although generally speaking speech act ambiguity isn't taken account of, so the percentage is important.", "labels": [], "entities": [{"text": "assignment of speech acts", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.863837942481041}]}, {"text": "MT systems are not at all good.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9594274163246155}]}, {"text": "Even measure words like cm, km, kg, etc.", "labels": [], "entities": []}, {"text": "Example: cm \u2192 centim\u00e8tre, congr\u00e9gation de la mission, co\u00fbt marginal, etc.", "labels": [], "entities": []}, {"text": "The following example shows that \u00abpinc\u00e9e (pinch, , tsumami) \u00bb in a quantifier phrase appears inform of \u00abune pinc\u00e9e de\u00bb, and is used in its figurative meaning.", "labels": [], "entities": []}, {"text": "When one looks at the translation outputs produced by commercial MT systems, it's not hard to deduce there is alack of phraseology studies and polysemy disambiguation method for the word \u00abpinc\u00e9e\u00bb 3 . For the treatment of the classifier/quantifier expressions, at first, we must know whether a word or an expression in a document is the classifier/quantifier or not, and which kind of information is necessary to handle it in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9587172269821167}, {"text": "MT", "start_pos": 425, "end_pos": 427, "type": "TASK", "confidence": 0.6466882824897766}]}, {"text": "Example: Ajoutez une pinc\u00e9e de sel.", "labels": [], "entities": [{"text": "Ajoutez", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9680976867675781}]}, {"text": "(Add a pinch of salt.)", "labels": [], "entities": []}, {"text": "\u2192 4 Sections 1 & 2 discuss the problems encountered in the processing of classifiers and quantifiers arising for meaning determination in the source language and from the structural differences between a language pair in the framework of MT.", "labels": [], "entities": [{"text": "meaning determination", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7349624037742615}, {"text": "MT", "start_pos": 238, "end_pos": 240, "type": "TASK", "confidence": 0.9106864333152771}]}, {"text": "Section 3 describes morpho-syntactic problems between two languages for quantifier/classifier expressions.", "labels": [], "entities": []}, {"text": "In Section 4, the difficulty of quantifiers/classifiers extraction is described.", "labels": [], "entities": [{"text": "quantifiers/classifiers extraction", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.5978023707866669}]}, {"text": "In Section 5, we propose a solution using a dictionary, edited from collected documents, themselves annotated with semantic UNL (hyper)graphs, presented as a parallel corpus, and give somme details about a small French-Japanese dictionary for quantifiers/classifiers, built for MT experimentation with an UNL system 5 .", "labels": [], "entities": [{"text": "MT", "start_pos": 278, "end_pos": 280, "type": "TASK", "confidence": 0.9278680086135864}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: KWIC of \"pointe\" from Sketch Engine  doc#357  qui marque le d\u00e9clin d\u00e9finitif de  cette", "labels": [], "entities": [{"text": "KWIC", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9190352559089661}]}]}