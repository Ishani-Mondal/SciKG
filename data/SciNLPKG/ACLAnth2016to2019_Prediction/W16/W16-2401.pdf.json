{"title": [{"text": "Equivalences between Ranked and Unranked Weighted Tree Automata via Binarization", "labels": [], "entities": []}], "abstractContent": [{"text": "Encoding unranked trees to binary trees, henceforth called binarization, is an important method to deal with unranked trees.", "labels": [], "entities": []}, {"text": "For each of three binarizations we show that weighted (ranked) tree au-tomata together with the binarization are equivalent to weighted unranked tree au-tomata; even in the probabilistic case.", "labels": [], "entities": []}, {"text": "This allows to easily adapt training methods for weighted (ranked) tree automata to weighted unranked tree automata.", "labels": [], "entities": []}], "introductionContent": [{"text": "When dealing with trees, tree grammars, and formal languages of trees, a restriction to binary trees is often beneficial, e.g. for improved generalization when generating grammars from treebanks, or for reduced parsing complexity.", "labels": [], "entities": []}, {"text": "A binarization maps any (unranked) tree to a binary tree such that the original tree can be reconstructed from the result.", "labels": [], "entities": []}, {"text": "In this paper we investigate three different binarizations inspired by).", "labels": [], "entities": []}, {"text": "For each of these binarizations we show that a weighted unranked tree language is recognizable by a weighted unranked tree automaton (wuta) iff the binarization of the language is recognizable by a weighted tree automaton (wta).", "labels": [], "entities": []}, {"text": "This even holds with restriction to probabilistic automata.", "labels": [], "entities": []}, {"text": "To support this result we show that for any R \u22650 -weighted finite state automaton with the sum of weights of all words being 1, there is an equivalent probabilistic finite state automaton.", "labels": [], "entities": []}, {"text": "This implies that the class of weighted string languages recognizable by probabilistic finite state automata is closed under reversal.", "labels": [], "entities": []}, {"text": "These results suggest that by adding binarization to training methods for wta we effectively get training methods for wuta.", "labels": [], "entities": []}, {"text": "Alterations to the weights and the state behaviour while training then carryover to wuta.", "labels": [], "entities": []}, {"text": "This also gives an explanation for why the performance of the training by is rather independent from the used binarization.", "labels": [], "entities": []}, {"text": "give an overview over wta, and introduced wuta.", "labels": [], "entities": []}, {"text": "For the unweighted case binarizations (also called encodings) were investigated by, e.g.,).", "labels": [], "entities": []}, {"text": "Their first-child-next-sibling encoding is similar to our left-branching binarization.", "labels": [], "entities": []}, {"text": "Their extension encoding is also used to define stepwise tree automata (.", "labels": [], "entities": []}, {"text": "A stepwise tree automaton is defined like a (ranked) tree automaton.", "labels": [], "entities": []}, {"text": "It accepts an unranked tree, if it accepts the extension encoding of the tree while the automaton is interpreted as a (ranked) tree automaton..2 and 6.2) extend this connection to the weighted case and show that weighted stepwise tree automata and wuta are equally powerful.", "labels": [], "entities": []}, {"text": "Our results for weighted and for probabilistic finite state automata area special case of renormalization of weighted or of probabilistic contextfree grammars (.", "labels": [], "entities": []}, {"text": "We provide alternative proofs for this special case.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}