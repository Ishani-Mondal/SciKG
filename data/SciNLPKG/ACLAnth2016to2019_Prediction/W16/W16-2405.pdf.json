{"title": [{"text": "Learning Transducer Models for Morphological Analysis from Example Inflections", "labels": [], "entities": [{"text": "Morphological Analysis", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7611048519611359}]}], "abstractContent": [{"text": "In this paper, we present a method to convert morphological inflection tables into un-weighted and weighted finite transducers that perform parsing and generation.", "labels": [], "entities": []}, {"text": "These transducers model the inflectional behavior of morphological paradigms induced from examples and can map inflected forms of previously unseen word forms into their lemmas and give morphosyntactic descriptions of them.", "labels": [], "entities": []}, {"text": "The system is evaluated on several languages with data collected from the Wiktionary.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wide-coverage morphological parsers that return lemmas and morphosyntactic descriptions (MSDs) of arbitrary word forms are fundamental for achieving strong performance of many downstream tasks in NLP (.", "labels": [], "entities": []}, {"text": "This is particularly true for languages that exhibit rich inflectional and derivational morphology.", "labels": [], "entities": []}, {"text": "Finite-state transducers are the standard technology for addressing this issue, but constructing them often requires not only significant commitment of resources but also demands linguistic expertise from the developers.", "labels": [], "entities": []}, {"text": "Access to large numbers of example inflections organized into inflection tables in resources such as the Wiktionary promises to offer a less laborious route to constructing robust large-scale analyzers.", "labels": [], "entities": []}, {"text": "Learning morphological generalizations from such example data has been the focus of much recent research, particularly in the domain of morphologically complex languages (.", "labels": [], "entities": [{"text": "Learning morphological generalizations", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6934420466423035}]}, {"text": "In this paper we present a tool for automatic generation of both probabilistic and non-probabilistic morphological analyzers that can be represented as unweighted and weighted transducers.", "labels": [], "entities": []}, {"text": "The assumption is that we have access to a collection of example word forms together with corresponding MSDs.", "labels": [], "entities": []}, {"text": "We present two systems: one that is designed to be high-recall and operates with unweighted automata, the purpose of which is to return all linguistically plausible analyses for an unknown word form; the second is an addition to the first in that the word shapes are modeled with a generative probabilistic model that can be implemented as a weighted transducer that produces a ranking of the plausible analyses.", "labels": [], "entities": []}, {"text": "The analyzers are constructed with standard finite state tools and are designed to operate similarly to a hand-constructed morphophonological analyzer extended with a 'guesser' module to handle unknown word forms.", "labels": [], "entities": []}, {"text": "The system takes as input sets of lemmatized words annotated with an MSD, all grouped into inflection tables-such as can be found in, for example, the Wiktionary.", "labels": [], "entities": []}, {"text": "The output is a morphological analyzer either as an unweighted (in the non-probabilistic case) or a weighted model (in the probabilistic case).", "labels": [], "entities": []}, {"text": "For the non-probabilistic case we use the Xerox regular expression formalism (), which we compile into a transducer with the open-source finite-state toolkit foma and for the weighted case we have used the Kleene toolkit.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the systems, we used the data set published by, which includes full inflection tables fora large number of lemmas in German (nouns and verbs), Spanish (verbs), and Finnish (nouns+adjectives and verbs).", "labels": [], "entities": []}, {"text": "That source also provides a division into train/dev/test splits, with 200 tables in dev and test, respectively.", "labels": [], "entities": []}, {"text": "We then evaluated the ability of our systems to provide a correct lemmatization and MSD of each word form in the held-out tables, testing separately on each part of speech.", "labels": [], "entities": [{"text": "MSD", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9179401397705078}]}, {"text": "For the unweighted analyzer, we use the three-part setup as described above.", "labels": [], "entities": []}, {"text": "For the weighted case, we produce a single highest scoring analysis.", "labels": [], "entities": []}, {"text": "The train/dev/test sets are entirely disjoint and share no tables.", "labels": [], "entities": []}, {"text": "We trained the models by inspecting all the word forms and corresponding MSDs, organizing them into tables, learning the paradigms, and the generating weighted and unweighted transducers as described above.", "labels": [], "entities": []}, {"text": "These transducers were then run on the test data to provide lemmatization and analyses of the unseen word forms.", "labels": [], "entities": []}, {"text": "summarizes the number of inflection tables seen during training, together with the final number of paradigms learned.", "labels": [], "entities": []}, {"text": "shows the statistics in the held-out data.", "labels": [], "entities": []}, {"text": "Because we focus on the recall figures of the analyzers, we also calculated an \"inherent ambiguity\" measure of the test data.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9957044720649719}]}, {"text": "This is the average number of different MSDs that are given for each word form.", "labels": [], "entities": []}, {"text": "This ambiguity may arise as follows: the Spanish verb tenga, for example, can be either the first person singular present subjunctive of tener 'to have' or the third person singular present subjunctive.", "labels": [], "entities": []}, {"text": "Such ambiguity shows that there exist cases where returning multiple analyses is warranted, given that we do not have any sentence context to determine the correct choice.", "labels": [], "entities": []}, {"text": "For the weighted case, sometimes the system returns multiple equally scoring parses.", "labels": [], "entities": []}, {"text": "This is due to the fact that the language model only operates over the variables, and, in many languages multi-   LM x1 LM x2.", "labels": [], "entities": []}, {"text": "in Spanish with weights (in effect the negative log probability), the inferred variable division, the lemmatization, and MSDs.", "labels": [], "entities": []}, {"text": "Lemmas and parts of the analysis that are correct are given in boldface.", "labels": [], "entities": [{"text": "Lemmas", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9514967799186707}]}, {"text": "Note that several paradigms can produce an entirely correct parse fora single form such as this one, even though the paradigms would differ in other forms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Statistics on the D&DN13 train+dev sets.  Paradigms is the corresponding number of in- duced paradigm functions.", "labels": [], "entities": [{"text": "D&DN13 train+dev sets", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.9378045541899545}]}, {"text": " Table 5: Statistics on the D&DN13 test set. Amb.  is the average number of lemma-MSD pairs per  unique word form (wf).", "labels": [], "entities": [{"text": "D&DN13 test set", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8472916126251221}, {"text": "Amb.", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9972354769706726}]}, {"text": " Table 6: Evaluation of the weighted model (all  figures represent the recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9989486336708069}]}, {"text": " Table 7: Weighted parsing example: top-10 ranked parses for the word form compraste 'buy PAST'", "labels": [], "entities": [{"text": "Weighted parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.5149699151515961}]}]}