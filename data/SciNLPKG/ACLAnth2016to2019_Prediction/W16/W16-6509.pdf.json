{"title": [{"text": "Building a learner corpus for Russian *", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we describe an open learner corpus of Russian.", "labels": [], "entities": []}, {"text": "The Russian Learner Corpus (RLC) is the first corpus with clear distinction between foreign language learners and heritage speakers.", "labels": [], "entities": [{"text": "Russian Learner Corpus (RLC)", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.8060899525880814}]}, {"text": "We discuss the structure of the corpus, its development and the annotation principles.", "labels": [], "entities": []}, {"text": "This paper describes the platform of the RLC which combines online tools for text uploading, processing, error annotation and corpus search.", "labels": [], "entities": [{"text": "RLC", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7244395613670349}, {"text": "corpus search", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.783492386341095}]}], "introductionContent": [{"text": "Designing learner corpora has become a rapidly developing branch of corpus linguistics, which is accounted for by obvious reasons -both research and practical.", "labels": [], "entities": [{"text": "corpus linguistics", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.733941912651062}]}, {"text": "As annotated collections of texts produced by non-native speakers of a certain language, learner corpora open up new horizons in areas, such as quantitative studies in second language acquisition, contrastive interlanguage analysis, etc., and the urge for well-organized error classification and frequency based error analysis can hardly be overestimated for language teaching.", "labels": [], "entities": [{"text": "contrastive interlanguage analysis", "start_pos": 197, "end_pos": 231, "type": "TASK", "confidence": 0.6353617111841837}]}, {"text": "Moreover, computerized learner data serve as training and test data sets for various NLP tasks, such as native language identification task ( and L2 (second language) speakers; 3.", "labels": [], "entities": [{"text": "native language identification task", "start_pos": 104, "end_pos": 139, "type": "TASK", "confidence": 0.6933590397238731}]}, {"text": "It is built on an integrated multifunctional platform that provides a single interface for uploading, annotating and search.", "labels": [], "entities": []}, {"text": "Russian Learner Corpus is an international project carried out by the Linguistic Laboratory for Corpus Technologies at the Higher School of Economics in close collaboration with experts from more than 10 countries (see \"Our partners\" at http://www.webcorpora.net/RLC).", "labels": [], "entities": [{"text": "Russian Learner Corpus", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.7941890358924866}]}, {"text": "The corpus currently comprises more than 730000 tokens.", "labels": [], "entities": []}, {"text": "56 percent of the data is produced by L2 learners of Russian, 44 percent -by heritage speakers of Russian, who are college/university-age students at the proficiency level of intermediate and higher.", "labels": [], "entities": []}, {"text": "The first version of the RLC contained only texts from American English-dominant speakers of Russian.", "labels": [], "entities": []}, {"text": "The number of dominant languages has by far grown to eight.", "labels": [], "entities": []}, {"text": "Three of them are at the moment scarcely presented in the corpus, however, more data on them and two more languages are being prepared for upload.", "labels": [], "entities": []}, {"text": "A valuable part of the RLC is a large longitudinal subcorpus of academic writing called RULEC collected by Olessya Kisselev and Anna Alsufieva.", "labels": [], "entities": [{"text": "RLC", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8456363081932068}]}, {"text": "All the respondents signed a special consent form and their names are anonymized in the corpus.", "labels": [], "entities": []}, {"text": "In the longitudinal RULEC the speakers were assigned fake names so that the user could easily trace the progress of each student.", "labels": [], "entities": []}, {"text": "Other respondents are assigned a unique students code.", "labels": [], "entities": []}, {"text": "In Section 2 we give an overview of similar projects developed for the Russian language.", "labels": [], "entities": []}, {"text": "Section 3 describes corpus data and metainformation provided to each text.", "labels": [], "entities": []}, {"text": "Section 4 presents annotation principles, and Section 5 focuses on characteristics of the corpus platform.", "labels": [], "entities": []}, {"text": "In Section 6 we will make some concluding remarks and discuss our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The error annotation is performed by students of linguistics and supervised by our team.", "labels": [], "entities": []}, {"text": "Currently, the RLC annotation tool does not allow two annotators to work on the same texts without seeing the annotation decisions of each other.", "labels": [], "entities": [{"text": "RLC annotation", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.702383279800415}]}, {"text": "Therefore, we designed an offline inter-annotator agreement experiment in order to evaluate the consistency of annotation and to reveal ambiguous tags and/or inconsistencies in annotation guidelines.", "labels": [], "entities": [{"text": "consistency", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9755065441131592}]}, {"text": "The experiment was conducted on the sample consisted of 50 texts (8547 tokens in total) written by English and German L2 students.", "labels": [], "entities": []}, {"text": "The annotation was made in files retrieved from the corpus.", "labels": [], "entities": []}, {"text": "These have the following format: every word was presented on a separate line consisting of 6 columns -sentence number in the database, word, number of words in sentence, error tags, error correction, and annotator code.", "labels": [], "entities": []}, {"text": "Each text was annotated by two annotators (6 pairs in total).", "labels": [], "entities": []}, {"text": "Before tagging each participant received 5 trial texts which were checked by supervisors.", "labels": [], "entities": []}, {"text": "The most common mistakes were discussed with the annotators and outlined in the annotation guidelines.", "labels": [], "entities": []}, {"text": "Afterwards the experimental sample was annotated, the tag mismatches were counted and Cohens kappa coefficient was calculated.", "labels": [], "entities": [{"text": "Cohens kappa coefficient", "start_pos": 86, "end_pos": 110, "type": "METRIC", "confidence": 0.6551172137260437}]}, {"text": "We assume that relatively low agreement (the highest score was obtained for syntactic (0.317) and spelling (0.249) errors, while the lowest coefficient of 0.185 was achieved for errors in constructions) was primarily caused by the lack of more detailed annotation guidelines.", "labels": [], "entities": [{"text": "agreement", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9966574907302856}]}, {"text": "Although the current guidelines list the definition of all the tags and illustrate them with corresponding examples, difficult or ambiguous cases have not been outlined yet.", "labels": [], "entities": []}, {"text": "Thus, the annotators made typical errors and did not distinguish between close error types, such as lexical errors and errors in constructions or spelling and inflectional errors.", "labels": [], "entities": []}, {"text": "Moreover, since the experiment was performed outside the corpus platform, the annotators had to accommodate to anew data format and workflow, which might also serve as a source for inconsistent annotation.", "labels": [], "entities": []}, {"text": "Therefore, an extensive annotation training might help to increase the intercoder agreement score.", "labels": [], "entities": []}, {"text": "Having analyzed discrepancies in annotation, we decided to elaborate new annotation guidelines in order to improve the annotator agreement rate.", "labels": [], "entities": []}, {"text": "We believe that this will lead to better results in the next session of our inter-annotator agreement experiments.", "labels": [], "entities": []}], "tableCaptions": []}