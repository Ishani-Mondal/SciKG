{"title": [{"text": "Character-Aware Neural Networks for Arabic Named Entity Recognition for Social Media", "labels": [], "entities": [{"text": "Arabic Named Entity Recognition", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6535155400633812}]}], "abstractContent": [{"text": "Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organisation.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organisation", "start_pos": 0, "end_pos": 155, "type": "Description", "confidence": 0.7422653657418711}]}, {"text": "For Arabic language, recognizing named entities is a challenging task because of the complexity and the unique characteristics of this language.", "labels": [], "entities": []}, {"text": "In addition, most of the previous work focuses on Modern Standard Arabic (MSA), however, recognizing named entities in social media is becoming more interesting these days.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6990081568559011}, {"text": "recognizing named entities in social media", "start_pos": 89, "end_pos": 131, "type": "TASK", "confidence": 0.8504262963930765}]}, {"text": "Dialectal Arabic (DA) and MSA are both used in social media, which is deemed as another challenging task.", "labels": [], "entities": []}, {"text": "Most state-of-the-art Arabic NER systems count heavily on hand-crafted engineering features and lexicons which is time consuming.", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8996532559394836}]}, {"text": "In this paper, we introduce a novel neural network architecture which benefits both from character-and word-level representations automatically, by using combination of bidirectional Long Short-Term Memory (LSTM) and Conditional Random Field (CRF), eliminating the need for most feature engineering.", "labels": [], "entities": []}, {"text": "Moreover, our model relies on unsupervised word representations learned from unannotated corpora.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our model achieves state-of-the-art performance on publicly available benchmark for Arabic NER for social media and surpassing the previous system by a large margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the task of tagging, labeling or identifying atomic items in the text with predefined set of named entity categories such as Person, Location, Organization, etc. from large corpora ( . Recently, named entity recognition has gained an important role in Natural Language Processing (NLP) because it can have an impact on other NLP applications.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) is the task of tagging, labeling or identifying atomic items in the text with predefined set of named entity categories such as Person, Location, Organization, etc. from large corpora", "start_pos": 0, "end_pos": 214, "type": "Description", "confidence": 0.7755381908172216}, {"text": "named entity recognition", "start_pos": 229, "end_pos": 253, "type": "TASK", "confidence": 0.6938612659772238}]}, {"text": "In Question Answering (QA), showed that using NER system in their QA model improves its performance and questions contain 85% Named Entities.", "labels": [], "entities": [{"text": "Question Answering (QA", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8231430798768997}]}, {"text": "showed that adding NER system in their Text Clustering system enhanced its performance and allowed them to outperform the existing state-of-the-art system.", "labels": [], "entities": []}, {"text": "clarified that using named entity recognition in Machine Translation (MT) can help the system to improve the translation task.", "labels": [], "entities": [{"text": "named entity recognition in Machine Translation (MT)", "start_pos": 21, "end_pos": 73, "type": "TASK", "confidence": 0.7594328787591722}]}, {"text": "prompted that using NER improve Information Retrieval (IR) performance.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.8020478188991547}]}, {"text": "In addition, NER could be used in various NLP systems to improve their performance such as semantic parsers, part of speech taggers, document and news searching.", "labels": [], "entities": [{"text": "part of speech taggers", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.6358109787106514}, {"text": "document and news searching", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.602434940636158}]}, {"text": "Current state-of-the-art systems perform very well on recognizing Arabic named entities such as Person, Location, or Organization () for MSA texts.", "labels": [], "entities": []}, {"text": "However, there is relatively less interest on recognizing named entities in social media like Twitter, movies, TV shows.", "labels": [], "entities": []}, {"text": "In this paper, we focus on recognizing Arabic named entities in Twitter.", "labels": [], "entities": []}, {"text": "Lately, Arabic language was the fastest growing language on Twitter, and in 2012, it was the 6th most used language on Twitter.", "labels": [], "entities": []}, {"text": "This rapid increase in online social media has encouraged researchers in many fields to analyze its content for many purposes such as opinion mining, event detection, and others.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.7841413915157318}, {"text": "event detection", "start_pos": 150, "end_pos": 165, "type": "TASK", "confidence": 0.8005020022392273}]}, {"text": "Since NER plays a vital role in many NLP applications, any of these applications focused on dealing with Twitter content, needs to use NER system to improve its performance and deals with Twitter specific challenges.", "labels": [], "entities": []}, {"text": "As consequences, Arabic is very complex language compared to European languages.", "labels": [], "entities": []}, {"text": "On the one hand, it has both complex and rich morphology as well as ambiguity.", "labels": [], "entities": []}, {"text": "On the other hand, there are no capital letters and Arabic texts are written without diacritics.", "labels": [], "entities": []}, {"text": "Therefore, building Arabic NLP applications and especially NER is very intriguing.", "labels": [], "entities": [{"text": "NER", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9470158815383911}]}, {"text": "Most of Arabic NER systems use three approaches: rule-based, machine learning and hybrid approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8244265913963318}]}, {"text": "In this paper, we use Deep Neural Networks (DNN) because they are extremely powerful machine learning models that have attained great success in vast applications such as image classification () and speech recognition ().", "labels": [], "entities": [{"text": "image classification", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.7609653770923615}, {"text": "speech recognition", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.7955266237258911}]}, {"text": "Furthermore, DNNs can achieve state-of-the-art in many NLP applications for instance Machine Translation () and sentiment analysis.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8316147923469543}, {"text": "sentiment analysis", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.9649135768413544}]}, {"text": "These powerful models can use backpropagation algorithm for training.", "labels": [], "entities": []}, {"text": "In order to process variable length input, recurrent neural networks (RNNs) are the best solution (.", "labels": [], "entities": []}, {"text": "In recent years, RNNs are widely used and achieved state-of-the-art in several NLP tasks such as language modeling), machine translation () and speech recognition.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7201279550790787}, {"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8432253301143646}, {"text": "speech recognition", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8421834409236908}]}, {"text": "We use Long Short-Term Memory (LSTM), which is one kind of RNNs with complex cells).", "labels": [], "entities": []}, {"text": "With its forget gate, LSTM allows highly non-trivial long-distance dependencies to be easily learned.", "labels": [], "entities": []}, {"text": "For sequential labeling tasks, it has been shown that using a bi-directional LSTM model is preferred to LSTM model because it can capture infinite amount of context on both sides fora sentence by eliminating the main problem of limited context in feed-forward neural networks (.", "labels": [], "entities": [{"text": "sequential labeling tasks", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.761970599492391}]}, {"text": "In fact, to build a system for Arabic NER for social media, we propose a model based on bidirectional LSTM networks with Conditional Random Field (CRF) layer on the top of the networks.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.7040374875068665}]}, {"text": "Most of the existing Arabic NER systems rely on handcrafted engineering features which is time consuming and the use of large gazetteers to improve the accuracy.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.7271733283996582}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9962995648384094}]}, {"text": "In this paper, we investigate the impact of using character-level and word embedding features as inputs for bidirectional LSTM network with CRF on the top of the networks as contextual feature on Arabic NER performance for social media.", "labels": [], "entities": []}, {"text": "Experimental results show that we are able to obtain state-of-the-art performance on Twitter dataset without using any large gazetteers and lots of handcrafted engineering features.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9379385709762573}]}, {"text": "The main contributions of this paper are the following: \u2022 Study the impact of bidirectional LSTM on sequence tagging like NER on Arabic Twitter texts; \u2022 The effectiveness of using character-level for morphologically rich languages (Arabic as an example) and also show that using word representations improve the system performance; \u2022 Investigate the use of CRF on the top of bidirectional LSTM to capture contextual features in Arabic Twitter texts; \u2022 We get state-of-the-art results and outperform the existing systems on publicly available dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation was performed on the Twitter dataset developed by. gives an overview of this dataset.", "labels": [], "entities": [{"text": "Twitter dataset developed", "start_pos": 32, "end_pos": 57, "type": "DATASET", "confidence": 0.887320319811503}]}, {"text": "Before training starts, we split the dataset into sentences by replacing \".\" and \",\" by spaces.", "labels": [], "entities": []}, {"text": "Every digit in the dataset is replaced by zero.", "labels": [], "entities": []}, {"text": "We use the training and test datasets developed by) in order to test our model.", "labels": [], "entities": []}, {"text": "This dataset was tagged with three types of named entities: location, person and organisation.", "labels": [], "entities": []}, {"text": "The training dataset contains tweets randomly selected from the period of May 3-12, 2012.", "labels": [], "entities": []}, {"text": "The testing data contains tweets that were randomly selected between November 23, 2011 and November 27, 2011.", "labels": [], "entities": []}, {"text": "We mention that these two datasets were annotated using the Linguistics Data Consortium ACE tagging guidelines.", "labels": [], "entities": [{"text": "Linguistics Data Consortium ACE tagging guidelines", "start_pos": 60, "end_pos": 110, "type": "DATASET", "confidence": 0.8346562534570694}]}, {"text": "As we will see in the experimental results, this dataset was used in and (Zirikly and Diab, 2015) for testing.", "labels": [], "entities": []}, {"text": "We run many experiments representing the combination of different models and architectures to understand their influence on Arabic NER system for social media.", "labels": [], "entities": [{"text": "Arabic NER system", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.7285025119781494}]}, {"text": "We explored the impact of using CRF as contextual features, pretrained word embeddings and character-level embeddings.", "labels": [], "entities": []}, {"text": "Experiments show that the marvelous improvement in the overall system performance was observed with the use of pretrained word embeddings (indicated by \"word emb\" in) which gives us an improvement by 9.57 points in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9785516858100891}]}, {"text": "Adding CRF layer provides us an improvement of 5.34 points in F1 score.", "labels": [], "entities": [{"text": "CRF", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.6975862979888916}, {"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9744980037212372}]}, {"text": "Using character-level embeddings (indicated by \"char\" in) improve our system performance by 3.47 points in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9733831286430359}]}, {"text": "We compare our system with two other models.", "labels": [], "entities": []}, {"text": "The best score reported on this task was obtained by.", "labels": [], "entities": []}, {"text": "Their system uses large gazetteers, and a semi-supervised method.", "labels": [], "entities": []}, {"text": "They got 65.2 points in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9537871778011322}]}, {"text": "The same Twitter dataset was used by) to test their model, which used a lot of handcrafted engineering features and gazetteers.", "labels": [], "entities": []}, {"text": "They obtained F1 score of 59.59.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9597989618778229}]}, {"text": "Our model outperforms these two models without using any large gazetteers, and with the use of minimal features combined with bidirectional LSTMs.", "labels": [], "entities": []}, {"text": "shows our results on Arabic NER for social media in comparison with these two systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.5224319100379944}]}, {"text": "On the one hand, as far as we know, we are the first to explore the impact of character-level embeddings, pretrained word embeddings and contextual features (CRF) to develop a system for Arabic NER for social media.", "labels": [], "entities": []}, {"text": "Using character-level embeddings allow our model to learn interesting morphological and orthographic features instead of hand-engineering them.", "labels": [], "entities": []}, {"text": "On the other hand, we are the first to use Arabic pretrained word embeddings developed by) to initialize our word vectors for Arabic NER for social media and explore their impact on our system performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results with different choice of word embeddings.", "labels": [], "entities": []}, {"text": " Table 2: Twitter Evaluation data statistics.", "labels": [], "entities": [{"text": "Twitter Evaluation data statistics", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.9118027985095978}]}, {"text": " Table 3: Twitter dataset results with our models", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9127532243728638}]}, {"text": " Table 4: Comparison of our system with two other models on Twitter dataset", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9221320450305939}]}]}