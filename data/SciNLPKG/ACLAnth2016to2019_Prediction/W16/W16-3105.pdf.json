{"title": [{"text": "HPI Question Answering System in BioASQ 2016", "labels": [], "entities": [{"text": "HPI Question Answering", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8454758922259012}]}], "abstractContent": [{"text": "Question answering (QA) systems are crucial when searching for exact answers for natural language questions in the biomed-ical domain.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8935195446014405}]}, {"text": "Answers to many of such questions can be extracted from the 26 millions biomedical publications currently included in MEDLINE when relying on appropriate natural language processing (NLP) tools.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.8206270933151245}]}, {"text": "In this work we describe our participation in the task 4b of the BioASQ challenge using two QA systems that we developed for biomedicine.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.5487671792507172}]}, {"text": "Preliminary results show that our systems achieved first and second positions in the snippet retrieval sub-task and for the generation of ideal answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The deluge of scientific publication in biomedicine requires tools for processing and searching precise information in real time.", "labels": [], "entities": []}, {"text": "Question answering (QA) comes as an alternative to standard search engines system, e.g. PubMed , and provides precise and short answers for questions in natural language.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9324877262115479}]}, {"text": "One of the advantages of QA systems is that the user does not need to be proficient in formulating queries in away that the system can understand.", "labels": [], "entities": []}, {"text": "Instead, a user may simply enter a question as they would pose it to another person and receive a answer in return.", "labels": [], "entities": []}, {"text": "Thus, no formal training is required to use QA systems.", "labels": [], "entities": []}, {"text": "QA is one of the more complex applications of natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.8002055982748667}]}, {"text": "This is usually achieved through a three-steps architecture: (1) the users question 1 http://www.ncbi.nlm.nih.gov/pubmed must be processed so that a query can be generated; (2) this query is then used to find all relevant text passages from a large document collection; and (3) finally, the system generates the exact answer to the users question and/or a summary of the facts from these passages.", "labels": [], "entities": []}, {"text": "Some QA systems already exist for the biomedical domain.", "labels": [], "entities": []}, {"text": "However, none of them are capable of answering questions in real time, in part due to the large collections of documents involved in the task.", "labels": [], "entities": []}, {"text": "We describe our participation in the fourth edition of the BioASQ challenge 2 (), a community-based shared task which aims to evaluate the current solutions fora variety of QA sub-tasks.", "labels": [], "entities": [{"text": "BioASQ challenge 2", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.5869061450163523}]}, {"text": "We submitted runs from two QA systems which were specifically developed for the biomedical domain.", "labels": [], "entities": []}, {"text": "One of the system (HPI1) successfully participated in the previous editions of the BioASQ challenge (Neves, 2015) and our second system (HPI2) is described in this work.", "labels": [], "entities": [{"text": "BioASQ challenge (Neves, 2015)", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.5666047079222543}]}, {"text": "We relied on existing NLP functionality from a inmemory database (IMDB) and we extend it with new procedures tailored specifically to QA.", "labels": [], "entities": []}, {"text": "We participated in the task 4b (Biomedical Semantic QA) which is split in two phases: (a) phase A: concept mapping and document, passage and RDF triples retrieval; and (b) phase B: exact and ideal (short summary) answers.", "labels": [], "entities": [{"text": "Biomedical Semantic QA)", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.620770052075386}, {"text": "concept mapping", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7268986701965332}, {"text": "RDF triples retrieval", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.6555363337198893}]}, {"text": "The next section presents a short description of our the HPI2 system, followed by the preliminary results that we obtained in the challenge and a short discussion about our performance and methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Currently, only preliminary results are available for some of the tasks of the BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.604244589805603}]}, {"text": "More details on the results can be found in the BioASQ website . We present in this section a discussion on the preliminary results that we obtained in the BioASQ challenge, on the limitation of our methods and improvements for future versions of our QA system.: Preliminary results in the BioASQ task 4b.", "labels": [], "entities": [{"text": "BioASQ task 4b", "start_pos": 290, "end_pos": 304, "type": "DATASET", "confidence": 0.6276674667994181}]}, {"text": "Scores for concepts, documents and snippets are in terms of MAP (Mean Average Precision).", "labels": [], "entities": [{"text": "MAP", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9926337599754333}, {"text": "Mean Average Precision)", "start_pos": 65, "end_pos": 88, "type": "METRIC", "confidence": 0.9532027989625931}]}, {"text": "\"na\" indicated that results are still not available for this task, while \"-\" indicated that we did not submit any run for the task.", "labels": [], "entities": []}, {"text": "The values inside parameters indicate our current rank and the total number of submissions for the task.", "labels": [], "entities": []}, {"text": "Curiously, although the strategy used for the document retrieval is exactly the same one used for the snippet retrieval, we obtained much better results for the later, in term of position in the ranking, also in previous editions of the BioASQ challenge.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6576565206050873}, {"text": "BioASQ challenge", "start_pos": 237, "end_pos": 253, "type": "DATASET", "confidence": 0.6982601284980774}]}, {"text": "As gold-standard and not available, we can only try to guess the reasons for our performance.", "labels": [], "entities": []}, {"text": "When comparing our two systems, HPI2 performed much worse than HPI1, which proves that we still have to need to be improved to deal with large document collections, while HPI1 rely on up to 200 previously retrieved from PubMed.", "labels": [], "entities": [{"text": "HPI1", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.931706964969635}, {"text": "PubMed", "start_pos": 220, "end_pos": 226, "type": "DATASET", "confidence": 0.9467687606811523}]}, {"text": "Our system HPI1 performed well again and it a good candidate for obtaining first and second position in the challenge.", "labels": [], "entities": [{"text": "HPI1", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8884689807891846}]}, {"text": "This proves that the IMDB could effectively match the keywords in the queries to the documents and rank the sentences.", "labels": [], "entities": []}, {"text": "However, we see much room for improvement in our approach as named-entities are still not being used in this component, a step which can certainly improve both document and passage retrieval.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7758727967739105}]}, {"text": "Our results for ideal answers, i.e., short summaries, provided by system HPI2 also obtained either first or second positions in the all of the batches, when considering results by teams, instead of each individual run.", "labels": [], "entities": [{"text": "HPI2", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9452463984489441}]}], "tableCaptions": [{"text": " Table 1. More details on  the results can be found in the BioASQ web site", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.7955768704414368}]}]}