{"title": [{"text": "Data61-CSIRO systems at the CLPsych 2016 Shared Task", "labels": [], "entities": [{"text": "Data61-CSIRO systems at the CLPsych 2016 Shared Task", "start_pos": 0, "end_pos": 52, "type": "DATASET", "confidence": 0.7567894533276558}]}], "abstractContent": [{"text": "This paper describes the Data61-CSIRO text classification systems submitted as part of the CLPsych 2016 shared task.", "labels": [], "entities": [{"text": "Data61-CSIRO text classification", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.756746510664622}, {"text": "CLPsych 2016 shared task", "start_pos": 91, "end_pos": 115, "type": "DATASET", "confidence": 0.825628474354744}]}, {"text": "The aim of the shared task is to develop automated systems that can help mental health professionals with the process of triaging posts with ideations of depression and/or self-harm.", "labels": [], "entities": []}, {"text": "We structured our participation in the CLPsych 2016 shared task in order to focus on different facets of modelling online forum discussions: (i) vector space representations; (ii) different text gran-ularities; and (iii) fine-versus coarse-grained labels indicating concern.", "labels": [], "entities": [{"text": "CLPsych 2016 shared task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8488344997167587}]}, {"text": "We achieved an F1-score of 0.42 using an ensemble classification approach that predicts fine-grained labels of concern.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996868371963501}, {"text": "ensemble classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7171233594417572}]}, {"text": "This was the best score obtained by any submitted system in the 2016 shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of the shared task is to research and develop automatic systems that can help mental health professionals with the process of triaging posts with ideations of depression and/or self-harm.", "labels": [], "entities": []}, {"text": "We structured our participation in the CLPsych 2016 shared task in order to focus on different facets of modelling online forum discussions: (i) vector space representations (TF-IDF vs. embeddings); (ii) different text granularities (e.g., sentences vs posts); and (iii) fineversus coarse-grained (FG and CG respectively) labels indicating concern.", "labels": [], "entities": []}, {"text": "(i) For our exploration of vector space representations, we explored the traditional TF-IDF feature representation that has been widely applied to NLP.", "labels": [], "entities": []}, {"text": "We also investigated the use of post embeddings, which have recently attracted much attention as feature vectors for representing text (.", "labels": [], "entities": []}, {"text": "Here, as in other related work), the post embeddings are learned from the unlabelled data as features for supervised classifiers.", "labels": [], "entities": []}, {"text": "(ii) Our exploration of text granularity focuses on classifiers for sentences as well as posts.", "labels": [], "entities": []}, {"text": "For the sentence-level classifiers, a post is split into sentences as the basic unit of annotation using a sentence segmenter.", "labels": [], "entities": []}, {"text": "(iii) To explore the granularity of labels indicating concern, we note that the data includes a set of 12 FG labels representing factors that assist in deciding on whether a post is concerning or not.", "labels": [], "entities": []}, {"text": "These are in addition to 4 CG labels.", "labels": [], "entities": []}, {"text": "We trained 6 single classifiers based on different combinations of vector space features, text granularities and label sets.", "labels": [], "entities": []}, {"text": "We also explored ensemble classifiers (based on these 6 single classifiers), as this is away of combining the strengths of the single classifiers.", "labels": [], "entities": []}, {"text": "We used one of two ensemble methods: majority voting and probability scores over labels.", "labels": [], "entities": []}, {"text": "We submitted five different systems as submissions to the shared task.", "labels": [], "entities": []}, {"text": "Two of them were based on single classifiers, whereas the remaining three systems used ensemble-based classifiers.", "labels": [], "entities": []}, {"text": "We achieved an F1-score of 0.42 using an ensemble classification approach that predicts FG labels of concern.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.999671459197998}, {"text": "ensemble classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7251909375190735}, {"text": "FG", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.6599879860877991}]}, {"text": "This was the best score obtained by any submitted system in the 2016 shared task.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: Section 2 briefly discusses the data of the shared task.", "labels": [], "entities": []}, {"text": "Section 3 presents the details of the systems we sub-mitted.", "labels": [], "entities": []}, {"text": "Section 4 then shows experimental results.", "labels": [], "entities": []}, {"text": "Finally, we summarise our findings in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present two evaluation results: the cross-validation results and the final test results.", "labels": [], "entities": []}, {"text": "We performed 5-fold cross-validation on the training set (947 labelled posts).", "labels": [], "entities": []}, {"text": "We also report the shared task evaluation scores for the five systems on the test set of 214 posts.", "labels": [], "entities": []}, {"text": "These are shown in where scores are computed for three labels: Amber, Red and Crisis (but not Green), since this is the official evaluation metric in the shared task.", "labels": [], "entities": []}, {"text": "We observe that two of the ensemble systems (Ensb-6classifiers-mv and Ensb-3classifiers-12labels-prob) show higher F1-scores than the others in the cross-validation experiments.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9977704286575317}]}, {"text": "In particular, Ensb-3classifiers-12labels-prob performs best both in the cross-validation experiment (0.37) and the main competition (0.42).", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, the first system, Posttfidf-4labels, gave us an F1-score of 0.39 on the test data, while its F1-score was the lowest in the cross-validation experiment.", "labels": [], "entities": [{"text": "Posttfidf-4labels", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.7901538014411926}, {"text": "F1-score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9996123909950256}, {"text": "F1-score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993639588356018}]}, {"text": "This result indicates that good performance is possible on the test dataset using a \"textbook\" TF-IDF classifier but further investigation is required to understand why the official test result differs from our cross-validation result.", "labels": [], "entities": []}, {"text": "shows the superior performance of the Ensb-3classifiers-12labels-prob, with respect to the other systems in terms of F1 and accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9997445940971375}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9992963075637817}]}, {"text": "It achieved the highest accuracy (0.85) for the three labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9993792772293091}]}, {"text": "Furthermore, it is a robust system for identifying the non-concerning label, Green.", "labels": [], "entities": []}, {"text": "It is interesting to see that the F1-score was im-  proved by performing the hard classification task of 12 labels compared to 4-label classification.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9988875985145569}]}, {"text": "We compare the performance of the Ensb-3classifiers-4labels-prob and Ensb-3classifiers-12labels-prob systems on the test data per label, as shown in to shed light on why the 12-labelling system has superior performance.", "labels": [], "entities": []}, {"text": "Both systems were unable to detect any Crisis-labelled posts.", "labels": [], "entities": []}, {"text": "A notable difference between the two systems is that the Ensb-3classifiers-12labels-prob system produces significantly higher recall (0.63) than the Ensb-3classifiers-4labels-prob system (0.33).", "labels": [], "entities": [{"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.999504566192627}]}, {"text": "In addition, the Ensb-3classifiers-12labels-prob system has a higher precision for finding Amber posts.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9982573390007019}]}, {"text": "These results consequently led to overall better F1 as shown in, and suggest that identifying Green and Amber posts fora user-in-the-loop scenario maybe one way to help moderators save time in triaging posts.", "labels": [], "entities": [{"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9997527003288269}]}], "tableCaptions": [{"text": " Table 1: CG and FG label sets. Their frequencies represent the", "labels": [], "entities": []}, {"text": " Table 2: F1 results for 5-fold cross-validation on training data", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9971616268157959}]}, {"text": " Table 3: Results for the test set. The filter decides whether the", "labels": [], "entities": []}, {"text": " Table 4: Comparison results on the test dataset in terms of pre-", "labels": [], "entities": []}]}