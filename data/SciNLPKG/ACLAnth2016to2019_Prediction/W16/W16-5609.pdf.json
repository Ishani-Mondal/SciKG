{"title": [{"text": "Identifying Stance by Analyzing Political Discourse on Twitter", "labels": [], "entities": [{"text": "Identifying Stance", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9724764823913574}]}], "abstractContent": [{"text": "Politicians often use Twitter to express their beliefs, stances on current political issues, and reactions concerning national and international events.", "labels": [], "entities": []}, {"text": "Since politicians are scrutinized for what they choose or neglect to say, they craft their statements carefully.", "labels": [], "entities": []}, {"text": "Thus despite the limited length of tweets, their content is highly indicative of a politician's stances.", "labels": [], "entities": []}, {"text": "We present a weakly supervised method for understanding the stances held by politicians, on a wide array of issues, by analyzing how issues are framed in their tweets and their temporal activity patterns.", "labels": [], "entities": []}, {"text": "We combine these components into a global model which collectively infers the most likely stance and agreement patterns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently the popularity of traditional media outlets such as television and printed press has decreased, causing politicians to turn their attention to social media outlets, which allow them to directly access the public, express their beliefs, and react to current events.", "labels": [], "entities": []}, {"text": "This trend emerged during the 2008 U.S. presidential election campaign and has since moved to the mainstream -in the 2016 campaign, all candidates employ social media platforms.", "labels": [], "entities": []}, {"text": "One of the most notable examples of this trend is the microblogging outlet Twitter, which unlike its predecessors, requires candidates to compress their ideas, political stances, and reactions into 140 character long tweets.", "labels": [], "entities": []}, {"text": "As a result, candidates have to cleverly choose how to frame controversial issues, as well as react to events and each other (.", "labels": [], "entities": []}, {"text": "In this work we present a novel approach for modeling the microblogging activity of presidential candidates and other prominent politicians.", "labels": [], "entities": []}, {"text": "We look into two aspects of the problem, stance prediction over a wide array of issues, as well as agreement and disagreement patterns between politicians over these issues.", "labels": [], "entities": [{"text": "stance prediction", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.9052638113498688}]}, {"text": "While the two aspects are related, we argue they capture different information, as identifying agreement patterns reveals alliances and rivalries between candidates, across and inside their party.", "labels": [], "entities": []}, {"text": "We show that understanding the political discourse on microblogs requires modeling both the content of posted messages as well as the social context in which they are generated, and suggest a joint model capturing both aspects.", "labels": [], "entities": []}, {"text": "Converse to other works predicting stance per individual tweet, we use the overall Twitter behavior to predict a politician's stance on an issue.", "labels": [], "entities": []}, {"text": "We argue that these settings are better suited for the political arena on Twitter.", "labels": [], "entities": []}, {"text": "Given the limit of 140 characters, the stance relevance of a tweet is not independent of the social context in which it was generated.", "labels": [], "entities": []}, {"text": "In an extreme case, even the lack of Twitter activity on certain topics can be indicative of a stance.", "labels": [], "entities": []}, {"text": "Additionally, framing issues in order to create bias towards their stance is a tool often used by politicians to contextualize the discussion ().", "labels": [], "entities": []}, {"text": "Previous works exploring framing analyze text in traditional settings, such as congressional speeches or newspaper articles.", "labels": [], "entities": []}, {"text": "To apply framing analysis to Twitter data, we allow tweets to hold multiple frames when necessary, as we find that on average many tweets are relevant to two frames per issue.", "labels": [], "entities": [{"text": "framing analysis", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.9306369423866272}]}, {"text": "For example, consider the issue of gun control.", "labels": [], "entities": [{"text": "gun control", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.8106215596199036}]}, {"text": "shows three issue-related tweets by three politicians.", "labels": [], "entities": []}, {"text": "To correctly identify the stance taken by each of the politicians, our model must combine three aspects.", "labels": [], "entities": []}, {"text": "First, the relevance of these tweets to the question can be identified using issue indicators (marked in green).", "labels": [], "entities": []}, {"text": "Second, the similarity between the stances taken by two of the three politicians can be identified by observing how the issue is framed (marked in yellow).", "labels": [], "entities": []}, {"text": "In this example, tweets (1) and (3) frame the issue of gun control as a matter of safety, while (2) frames it as an issue related to personal freedom, thus revealing the agreement and disagreement patterns between them.", "labels": [], "entities": [{"text": "gun control", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.6888195723295212}]}, {"text": "Finally, we note the strong negative sentiment of tweet (1).", "labels": [], "entities": []}, {"text": "Notice that each aspect individually might not contain sufficient information for correct classification, but combining all three, by propagating the stance bias (derived from analyzing the negative sentiment of (1)) to politicians likely to hold similar or opposing views (derived from frame analysis), leads to a more reliable prediction.", "labels": [], "entities": [{"text": "correct classification", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.4915694445371628}]}, {"text": "Given the dynamic nature of this domain, we design our approach to use minimal supervision and naturally adapt to new issues.", "labels": [], "entities": []}, {"text": "Our model builds on several weakly supervised local learners that use a small seed set of issue and frame indicators to characterize the stance of tweets (based on lexical heuristics) and framing dimensions) and activity statistics which capture temporally similar patterns between politicians' Twitter activity.", "labels": [], "entities": []}, {"text": "Our final model represents agreement and stance bias by combining these weak models into a weakly supervised joint model through Probabilistic Soft Logic (PSL), a recently introduced probabilistic modeling framework (.", "labels": [], "entities": []}, {"text": "PSL combines these aspects declaratively by specifying high level rules over a relational representation of the politicians' activities (as shown in), which is further compiled into a graphical model called a hinge-loss Markov random field (, and used to make predictions about stance and agreement between politicians.", "labels": [], "entities": []}, {"text": "We analyze the Twitter activity of 32 prominent U.S. politicians, some of which were candidates for the U.S. 2016 presidential election.", "labels": [], "entities": []}, {"text": "We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision.", "labels": [], "entities": [{"text": "global modeling", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7361266314983368}]}], "datasetContent": [{"text": "Experimental Settings: As described in Section 4, the data generated from the local models is used as input to the PSL models.", "labels": [], "entities": []}, {"text": "Stances collected in Section 3 are used as the ground truth for evaluation of the results of the PSL models.", "labels": [], "entities": []}, {"text": "We initialize Model 1, as described in Section 4.4, using political party affiliation knowledge.", "labels": [], "entities": []}, {"text": "Model 2 builds upon Model 1 by adding the results of the issue and sentiment analysis local models.", "labels": [], "entities": []}, {"text": "Model 3 combines all previous models with higher level Twitter activities: tweet agreement, temporal activity, and frames.", "labels": [], "entities": []}, {"text": "We implement our PSL models to have an initial bias that candidates do not share a stance and are against an issue.", "labels": [], "entities": []}, {"text": "Experimental Results By Issue: presents the results of using our three proposed PSL models.", "labels": [], "entities": []}, {"text": "Local Baseline (LB) refers to using only the weak local models for prediction with no additional information about party affiliation.", "labels": [], "entities": [{"text": "Local Baseline (LB)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5373082280158996}, {"text": "prediction", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9629690051078796}]}, {"text": "We observe that for prediction of stance (PRO) LB performs better than random chance in 11 of 16 issues; for prediction of agreement (SAMESTANCE I ), LB performs much lower overall, with only 5 of 16 issues predicted above chance.", "labels": [], "entities": [{"text": "prediction of stance (PRO", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7851624965667725}, {"text": "prediction of agreement (SAMESTANCE I )", "start_pos": 109, "end_pos": 148, "type": "METRIC", "confidence": 0.7443641935076032}]}, {"text": "Using Model 1 (M1), we improve stance prediction accuracy for 11 of the issues and agreement accuracy for all issues.", "labels": [], "entities": [{"text": "stance prediction", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8114190399646759}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9496148824691772}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.5436925292015076}]}, {"text": "Model 2 (M2) further improves the stance and agreement predictions for an additional 8 and 10 issues, respectively.", "labels": [], "entities": []}, {"text": "Model 3 (M3) increases the stance prediction accuracy of M2 for 4 issues and the agreement accuracy for 9 issues.", "labels": [], "entities": [{"text": "stance prediction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8134329617023468}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8504244685173035}, {"text": "agreement accuracy", "start_pos": 81, "end_pos": 99, "type": "METRIC", "confidence": 0.683603823184967}]}, {"text": "The final agreement predictions of M3 are significantly improved over the initial LB for all issues.", "labels": [], "entities": [{"text": "M3", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.5575495362281799}]}, {"text": "The final stance predictions of M3 are improved overall issues except Guns, Iran, and TPP.", "labels": [], "entities": [{"text": "M3", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.654066801071167}, {"text": "Guns", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9643959403038025}, {"text": "Iran", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7733232975006104}, {"text": "TPP", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8355453610420227}]}, {"text": "For Guns, the stance prediction remains the same throughout all models, meaning additional party information does not boost the initial predictions determined from Twitter behaviors.", "labels": [], "entities": []}, {"text": "For Iran, the addition of M1 and M2 lower the accuracy, but the behavioral features from M3 are able to restore it to the original prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996829032897949}]}, {"text": "For TPP, this trend is likely due to the fact that all models incorporate party information and the issue of TPP is the most heavily divided within and across parties, with 8 Republicans and 4 Democrats in support of TPP and 8 Republicans and 12 Democrats opposed.", "labels": [], "entities": [{"text": "TPP", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5488523244857788}, {"text": "TPP", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.8049402832984924}, {"text": "TPP", "start_pos": 217, "end_pos": 220, "type": "DATASET", "confidence": 0.844393253326416}]}, {"text": "Even in cases where M1 and/or M2 lowered the initial baseline result (e.g. stance for Religion or agreement for Environment), the final prediction by M3 is still higher than that of the baseline.", "labels": [], "entities": []}, {"text": "Framing and Temporal Information: As shown in, performance for some issues does not improve in Model 3.", "labels": [], "entities": []}, {"text": "Upon investigation, we found that for all issues, except Abortion which improves in agreement, one or both of the top frames for the party are the same across party lines.", "labels": [], "entities": [{"text": "Abortion", "start_pos": 57, "end_pos": 65, "type": "TASK", "confidence": 0.9427638649940491}]}, {"text": "For example, for ACA both Republicans and Democrats have the Economic and Health and Safety frames as their top two frames.", "labels": [], "entities": []}, {"text": "For TPP, both parties share the Economic frame.", "labels": [], "entities": [{"text": "TPP", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5605713725090027}]}, {"text": "In addition to similar framing overlap, the Twitter timeline for ACA also exhibits overlap, as shown in.", "labels": [], "entities": [{"text": "Twitter timeline for ACA", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.683960311114788}]}, {"text": "This figure highlights one week before and after the Supreme Court ruling (seen as the peak of activity, 6/25/2015) to uphold the ACA.", "labels": [], "entities": [{"text": "6/25/2015", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.8780346274375915}, {"text": "uphold the ACA", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.4938688774903615}]}, {"text": "Conversely, Abortion, which shares no frames between parties (Democrats frame Abortion with Constitutionality and Health and Safety frames; Repub-    licans use Economic and Capacity and Resources frames), exhibits a timeline with greater fluctuation.", "labels": [], "entities": [{"text": "Abortion", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.8980289697647095}]}, {"text": "The peak of(b) is 8/3/2015, which is the day that the budget was passed to include funding for Planned Parenthood.", "labels": [], "entities": [{"text": "8/3/2015", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.8666933298110961}]}, {"text": "Overall both parties have different patterns over this time range, allowing M3 to increase agreement prediction accuracy by 1.61%.", "labels": [], "entities": [{"text": "agreement prediction", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7412280440330505}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9689899682998657}]}], "tableCaptions": [{"text": " Table 4: Stance and Agreement Accuracy by Issue. LB uses weak local models, M1 represents party line agreement,", "labels": [], "entities": []}]}