{"title": [{"text": "Characterizing Text Difficulty with Word Frequencies", "labels": [], "entities": [{"text": "Characterizing Text Difficulty", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8881890376408895}]}], "abstractContent": [{"text": "Natural language processing (NLP) method-ologies have been widely adopted for readabil-ity assessment and greatly enhanced predic-tive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9441490173339844}]}, {"text": "In the present study, we study a well-established feature, the frequency of a word in common language use, and systematically explore how such a word-level feature is best used to characterize the reading levels of texts, a text-level classification problem.", "labels": [], "entities": [{"text": "text-level classification", "start_pos": 224, "end_pos": 249, "type": "TASK", "confidence": 0.6896904408931732}]}, {"text": "While traditionally such word-level features are simply averaged for all words of given text, we show that a richer representation leads to significantly better predictive models.", "labels": [], "entities": []}, {"text": "A basic approach adding a feature for the standard deviation already shows clear gains, and two more complex options systematically integrating more frequency information are explored: (i) encoding separate means for the words of a text according to which frequency band of the language they occur in, and (ii) encoding the mean of each cluster of words obtained by agglomerative hierarchical clustering of the words in the text based on their frequency.", "labels": [], "entities": []}, {"text": "The former organizes frequency around general language characteristics , whereas the latter aims to lose as little information as possible about the distribution of word frequencies in a given text.", "labels": [], "entities": []}, {"text": "To investigate the generalizability of the results, we compare cross-validation experiments within a corpus with cross-corpus experiments testing on the Common Core State Standards reference texts.", "labels": [], "entities": [{"text": "Common Core State Standards reference texts", "start_pos": 153, "end_pos": 196, "type": "DATASET", "confidence": 0.9698554078737894}]}, {"text": "We also contrast two different frequency norms and compare frequency with a measure of contextual diversity.", "labels": [], "entities": []}, {"text": "* Xiaobin Chen is also affiliated with the South China University of Technology, where he holds a lecturer position.", "labels": [], "entities": [{"text": "South China University of Technology", "start_pos": 43, "end_pos": 79, "type": "DATASET", "confidence": 0.7408553183078765}]}], "introductionContent": [{"text": "Although readability research has gone through a history of more than one hundred years, the use of Natural Language Processing (NLP) technology in readability research is a recent phenomenon.", "labels": [], "entities": []}, {"text": "It has greatly improved the predictive accuracy by enabling a multi-dimensional characterization of a text's reading level).", "labels": [], "entities": [{"text": "predictive", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9603071212768555}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9570104479789734}]}, {"text": "For example,  showed that 46 lexical and syntactic features mostly inspired by complexity measures Second Language Acquisition research support a classification accuracy of 91.3% on WeeklyReader, a collection of texts targeting children in four age groups commonly used in such readability research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9298639893531799}, {"text": "WeeklyReader", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.9492735862731934}]}, {"text": "The readability of a text is determined by the combination of all text aspects that affects the reader's understanding, reading speed, and level of interest in the text.", "labels": [], "entities": []}, {"text": "Recent studies explore lexical, morphological, semantic, psycholinguistic, syntactic, and cognitive features for determining the reading levels of texts ().", "labels": [], "entities": []}, {"text": "Among all these elements, the semantic variable of word difficulty has traditionally been found to account for the greatest percentage of readability variance (.", "labels": [], "entities": []}, {"text": "Word difficulty is often associated with word frequency given that the amount of exposure of a reader to the word is believed to be the major predictor of word knowledge).", "labels": [], "entities": []}, {"text": "In the present study, we zoom in to the question how word frequency can best be used to characterize the readability of a text.", "labels": [], "entities": [{"text": "characterize the readability of a text", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.7908908824125925}]}, {"text": "We experimented with three different methods of using frequency as a word-level feature to inform our predictions of readability at the text-level.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before turning to the three experiments carried out, let us introduce the resources and the general procedure used.", "labels": [], "entities": []}, {"text": "As source of the frequency and CD 1 information, we used the SUBTLEXus (Brysbaert and New, 2009) and the SUBTLEXuk) resources.", "labels": [], "entities": [{"text": "SUBTLEXus (Brysbaert and New, 2009)", "start_pos": 61, "end_pos": 96, "type": "DATASET", "confidence": 0.8408389911055565}]}, {"text": "We ran all experiments with two distinct frequency resources to be able to study the impact of the choice of resource.", "labels": [], "entities": []}, {"text": "As corpus for exploring the approach and 10-fold cross validation testing we used the leveled text corpus WeeBit).", "labels": [], "entities": [{"text": "WeeBit", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.785047173500061}]}, {"text": "For independent crosscorpus testing, we trained on WeeBit and tested on the exemplar texts from Appendix B of the Common Core State Standards.", "labels": [], "entities": [{"text": "WeeBit", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9776207804679871}, {"text": "Common Core State Standards", "start_pos": 114, "end_pos": 141, "type": "DATASET", "confidence": 0.9285721927881241}]}, {"text": "For machine learning, we used the basic k-nearest neighbor algorithm implemented in the R package class given that in our initial exploration it turned out to perform on a par or better than other commonly used algorithms such as Support Vector Machine or Decision Trees.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8066937327384949}]}, {"text": "The following basic procedure was followed for each of the experiments carried out: 1.", "labels": [], "entities": []}, {"text": "Tokenize corpus texts with CoreNLP Tokenizer (), which had also been used to compose the SUBTLEX frequency lists.", "labels": [], "entities": [{"text": "CoreNLP Tokenizer", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.8202401697635651}]}, {"text": "2. Characterize each text using frequency features.", "labels": [], "entities": []}, {"text": "The nature of the features differs across the three studies, for which details are given in the following sections.", "labels": [], "entities": []}, {"text": "3. Train classification models on the WeeBit corpus i) in a 10-fold Cross-Validation (CV) setup or ii) using the full corpus when the Common Core data was used as test.", "labels": [], "entities": [{"text": "Train classification", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.6997096538543701}, {"text": "WeeBit corpus", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9863739907741547}, {"text": "Common Core data", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.830587108929952}]}, {"text": "The K-nearest neighbors algorithm of the R package class was used for model construction and testing.", "labels": [], "entities": [{"text": "model construction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7250753343105316}]}, {"text": "4. Apply the trained model to the test folds or test corpus to assess model performance.", "labels": [], "entities": []}, {"text": "5. Report results in terms of Spearman's correlation coefficient (\u03c1) to allow comparison of CV and cross-corpus results.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient (\u03c1)", "start_pos": 30, "end_pos": 68, "type": "METRIC", "confidence": 0.8438104433672768}]}, {"text": "We report both 10-fold CV performance on WeeBit and the test performance on Common Core as references for model fit and generalizability.", "labels": [], "entities": [{"text": "WeeBit", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.965107798576355}, {"text": "Common Core", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9730275869369507}]}, {"text": "The KNN algorithm results in different models when the parameter K is set differently.", "labels": [], "entities": []}, {"text": "The parameter K for each model was decided automatically by testing K from one up to the square root of the number of texts used for training and choosing the value that resulted in the best performing model.", "labels": [], "entities": []}, {"text": "In this paper, we report the performance of the best models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the WeeBit corpus", "labels": [], "entities": [{"text": "WeeBit corpus", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.955520510673523}]}, {"text": " Table 2: 10-fold CV results for models without/with SD", "labels": [], "entities": []}, {"text": " Table 3: Common Core test results without/with SD", "labels": [], "entities": [{"text": "Common Core test", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7948509653409322}]}, {"text": " Table 4: Best-performing models from clustering experiment", "labels": [], "entities": []}]}