{"title": [{"text": "Building a bagpipe with a bag and a pipe: Exploring Conceptual Combination in Vision *", "labels": [], "entities": []}], "abstractContent": [{"text": "This preliminary study investigates whether, and to what extent, conceptual combination is conveyed by vision.", "labels": [], "entities": []}, {"text": "Working with noun-noun compounds we show that, for some cases, the composed visual vector built with a simple additive model is effective in approximating the visual vector representing the complex concept.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conceptual combination is the cognitive process by which two or more existing concepts are combined to form new complex concepts).", "labels": [], "entities": [{"text": "Conceptual combination", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7890659868717194}]}, {"text": "From a linguistic perspective, this mechanism can be observed in the formation and lexicalization of compound words (eg. boathouse, swordfish, headmaster, etc.), a widespread and very productive linguistic device that is usually defined in literature as the result of the composition of two (or more) existing and free-standing words (Lieber and\u0160tekauerand\u02c7and\u0160tekauer, 2009).", "labels": [], "entities": []}, {"text": "Within both perspectives, scholars agree that the composition of concepts/words is something more than a simple addition ().", "labels": [], "entities": []}, {"text": "However, additive models turned out to be effective in language, where they have been successfully applied to distributional semantic vectors (Paperno and Baroni, to appear).", "labels": [], "entities": []}, {"text": "Based on these previous findings, the present work addresses the issue of whether, and to what extent, conceptual combination can be described * We are grateful to Marco Baroni and Aur\u00e9lie Herbelot for the valuable advice and feedback.", "labels": [], "entities": []}, {"text": "This project has received funding from ERC 2011 Starting Independent Research Grant n.", "labels": [], "entities": [{"text": "ERC 2011 Starting Independent Research Grant n", "start_pos": 39, "end_pos": 85, "type": "DATASET", "confidence": 0.9048127957752773}]}, {"text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in our research.", "labels": [], "entities": []}, {"text": "in vision as the result of adding together two single concepts.", "labels": [], "entities": []}, {"text": "That is, can the visual representation of clipboard be obtained by using the visual representations of a clip and aboard as shown in?", "labels": [], "entities": []}, {"text": "In order to investigate this issue, we experiment with visual features that are extracted from images representing concrete and imageable concepts.", "labels": [], "entities": []}, {"text": "More precisely, we use nounnoun compounds for which ratings of imageability are available.", "labels": [], "entities": []}, {"text": "The rationale for choosing NNcompounds is that composition should take advantage from dealing with concepts for which clear, well-defined visual representations are available, as it is the case of nouns (representing objects).", "labels": [], "entities": []}, {"text": "In particular, we test whether a simple additive model can be applied to vision in a similar fashion to how it has been done for language).", "labels": [], "entities": []}, {"text": "We show that for some NN-compounds the visual representation of the whole can be obtained by simply summing up its parts.", "labels": [], "entities": []}, {"text": "We also discuss cases where the model fails and provide conjectures for more suitable approaches.", "labels": [], "entities": []}, {"text": "Since, to our knowledge, no datasets of images labeled with NN-compounds are currently available, we manually build and make available a preliminary dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our hypothesis, we used the publicly available dataset by.", "labels": [], "entities": []}, {"text": "It contains 629 English compounds for which human ratings on overall imageability (ie., a variable measuring the extent to which a compound word evokes a nonverbal image besides a verbal representation) are available.", "labels": [], "entities": []}, {"text": "We relied on this measure for carrying out a first filtering of the data, based on the assumption that the more imageable a compound, the clearer and better-defined its visual representation.", "labels": [], "entities": []}, {"text": "As a first step, we selected the most imageable items in the list by retaining only the ones with an average score of at least 5 points in a scale ranging from 1 (e.g., whatnot: 1.04) to 7 (e.g., watermelon: 6.95).", "labels": [], "entities": []}, {"text": "From this subset, including 240 items, one of the authors further selected only genuine noun-noun combinations, so that items like outfit or handout were discarded.", "labels": [], "entities": []}, {"text": "We then queried each compound and its constituent nouns in Google images and we selected only those items for which every object in the tuple (eg. airplane, air, and plane) had a relatively good visual representation by looking at the top 25 images.", "labels": [], "entities": []}, {"text": "This step, in particular, was aimed at discarding the surprisingly numerous cases for which only noisy images (ie., representing brands, products, or containing signs) were available.", "labels": [], "entities": []}, {"text": "From the resulting dataset, containing 115 items, we manually selected those that we considered as compositional in vision.", "labels": [], "entities": []}, {"text": "As a criterion, only NN-combinations that can be seen as resulting from either combining an object with a background (e.g., airplane: a plane is somehow superimposed in the air background) or concatenating two objects (e.g., clipboard) were selected.", "labels": [], "entities": []}, {"text": "Such a criterion is consistent with our purpose, that is finding those cases where visual composition works.", "labels": [], "entities": []}, {"text": "The rationale is that there should be composition when both the constituent concepts are present in the visual representation of the composed one.", "labels": [], "entities": []}, {"text": "Two authors separately carried out the selection procedure, and the few cases for which there was disagreement were resolved by discussion.", "labels": [], "entities": [{"text": "selection", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.975482702255249}]}, {"text": "In total, 38 items were selected and included in what we will heceforth refer to as compositional group.", "labels": [], "entities": []}, {"text": "Interestingly, the two visual criteria followed by the annotators turned out to partly reflect the kind of semantic relation implicitly tying the two nouns.", "labels": [], "entities": []}, {"text": "In particular, most of the selected items hold either a noun2 HAS noun1 (eg., clipboard) or a noun2 LOCATED noun1 (eg., cupcake) relation according to.", "labels": [], "entities": [{"text": "HAS", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8288810849189758}]}, {"text": "In addition, 12 other compounds (eg., sunflower, footstool, rattlesnake, etc.) were randomly selected from the 115-item subset.", "labels": [], "entities": []}, {"text": "We will heceforth refer to this set as the control group, whereas we will refer to the concatenation of the two sets (38+12=50 items) as the full group.", "labels": [], "entities": []}, {"text": "For each compound in the full group, we manually searched images representing it and each of its constituents nouns in Google images.", "labels": [], "entities": []}, {"text": "One good image, possibly showing the most prototypical representation of that concept according to the au-thors' experience, was selected.", "labels": [], "entities": []}, {"text": "In total, 79 images for N-constituents plus 50 images for NNcompounds (129 in total) images were included in our dataset.", "labels": [], "entities": []}, {"text": "To evaluate the compositionality of each NNcompound, we measure the extent to which the composed vector is similar to the corresponding observed one, ie. the vector directly extracted from either texts or the selected image.", "labels": [], "entities": []}, {"text": "Hence, first of all we use the standard Cosine similarity measure.", "labels": [], "entities": [{"text": "Cosine similarity measure", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.6722226937611898}]}, {"text": "The higher the similarity, the better the composition.", "labels": [], "entities": [{"text": "similarity", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9781768321990967}]}, {"text": "It could be the case that the composed vector is however less similar to the observed one than it is the closest N-constituent.", "labels": [], "entities": []}, {"text": "Thus, similarity by its own is not informative of whether the composition function has provided additional information compared to that conveyed by the closest single noun.", "labels": [], "entities": []}, {"text": "In order to take into account this issue, we also compute the similarity between the composed vector and both its Nconstituents.", "labels": [], "entities": []}, {"text": "We lower the similarity between the composed and the observed vector by subtracting the similarity between the observed vector and the noun that is closest to it (we call this measure CompInf o, since it is informative of the effectiveness of the composition).", "labels": [], "entities": [{"text": "CompInf o", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9067619740962982}]}, {"text": "When the composition operation maps the composed vector closer to the observed vector compared to its constituents in the semantic space, the composition provides more information.", "labels": [], "entities": []}, {"text": "In particular, when CompInf o is positive (ie., greater than 0), the composition is considered to be effective.", "labels": [], "entities": []}, {"text": "To further evaluate the compositionality of the nominal compound, we test the effectiveness of the composed vector in the retrieval task.", "labels": [], "entities": []}, {"text": "The reason is to double-check the distictiveness of the composed vector with respect to all the objects (ie., 79 N-constituents plus 50 NN-compounds) in the semantic space.", "labels": [], "entities": []}, {"text": "Using the composed vector as query, we are interested in knowing the rank of the corresponding observed vector.", "labels": [], "entities": []}, {"text": "Since for each query there is only one correct item in the whole semantic space, the most informative retrieval measure is Recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9294757843017578}]}, {"text": "Hence, we evaluate compositionality by Rec@k.", "labels": [], "entities": []}, {"text": "Since we have already scrutinized the role of the N-constituents with the previous measure, in the retrieval of a NN-compound both its N-constituents are removed from the semantic space.", "labels": [], "entities": []}, {"text": "The same evaluation is conducted for both vision and language, thus providing away to directly compare the two modalities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Compositionality evaluation in Vision and Language.", "labels": [], "entities": [{"text": "Compositionality evaluation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8711259663105011}]}]}