{"title": [{"text": "\"A Distorted Skull Lies in the Bottom Center...\" Identifying Paintings from Text Descriptions", "labels": [], "entities": [{"text": "Identifying Paintings from Text Descriptions", "start_pos": 49, "end_pos": 93, "type": "TASK", "confidence": 0.8947480082511902}]}], "abstractContent": [{"text": "Most question answering systems use symbolic or text information.", "labels": [], "entities": [{"text": "question answering", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.880592554807663}]}, {"text": "We present a dataset fora task that requires understanding descriptions of visual themes and their layout: identifying paintings from their descriptions.", "labels": [], "entities": []}, {"text": "We annotate paintings with contour data, align regions with entity mentions from an ontology, and associate image regions with text spans from descriptions.", "labels": [], "entities": []}, {"text": "A simple embedding-based method applied to text-to-image coreferences achieves state-of-the-art results on our task when paired with bipartite matching.", "labels": [], "entities": [{"text": "text-to-image coreferences", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.661758229136467}]}, {"text": "The task is made all the more difficult by scarcity of training data.", "labels": [], "entities": []}, {"text": "1 Knowledge from Images Question answering is a standard NLP task that typically requires gathering information from knowledge sources such as raw text, ontologies, and databases.", "labels": [], "entities": [{"text": "1 Knowledge from Images Question answering", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.577958732843399}]}, {"text": "Recently, vision and language have been amalgamated into an exciting and difficult task: using images to ask or answer questions.", "labels": [], "entities": []}, {"text": "While humans can easily answer complex questions using knowledge gleaned from images, visual question answering (VQA) is difficult for computers.", "labels": [], "entities": [{"text": "visual question answering (VQA)", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7398816645145416}]}, {"text": "Humans excel at this task because they abstract key concepts away from the minutiae of visual representations , but computers often fail to synthesise prior knowledge with confusing visual representations.", "labels": [], "entities": []}, {"text": "We present anew instance of visual question answering: can a computer identify an artistic work given only a textual description?", "labels": [], "entities": [{"text": "visual question answering", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7374890645345052}]}, {"text": "Our dataset contains images of paintings, tapestries, and sculptures covering centuries of artistic movements from dozens of countries.", "labels": [], "entities": []}, {"text": "Since these images are of cultural importance , we have access to many redundant descriptions of the same works, allowing us to create a natural-istic but inexpensive dataset.", "labels": [], "entities": []}, {"text": "Due to the complex and oblique nature of questions about paintings, their visual complexity, and the relatively small data size, prior approaches used for VQA over natural images are infeasible for our task.", "labels": [], "entities": []}, {"text": "We formalise the task in Section 3, where we also present a preliminary system (ARTMATCH) and compare with it a data-driven text baseline to illustrate the usefulness and versatility of our method (Section 4).", "labels": [], "entities": [{"text": "ARTMATCH", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.871224045753479}]}, {"text": "Finally, in Section 5 we compare our task and system to previous work that combines NLP and vision.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of our new question answering dataset", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7949213087558746}]}, {"text": " Table 2: Individual metrics of classes and features detected by", "labels": [], "entities": []}, {"text": " Table 3: Our system vs the blind baseline. DAN is trained on", "labels": [], "entities": [{"text": "DAN", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.5017901659011841}]}]}