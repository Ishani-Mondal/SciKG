{"title": [], "abstractContent": [{"text": "Neural Machine Translation (NMT) systems , introduced only in 2013, have achieved state of the art results in many MT tasks.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7813328802585602}, {"text": "MT tasks", "start_pos": 115, "end_pos": 123, "type": "TASK", "confidence": 0.9392313361167908}]}, {"text": "MetaMind's submissions to WMT '16 seek to push the state of the art in one such task, English\u2192German news-domain translation.", "labels": [], "entities": [{"text": "WMT '16", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.8517111738522848}, {"text": "English\u2192German news-domain translation", "start_pos": 86, "end_pos": 124, "type": "TASK", "confidence": 0.586317378282547}]}, {"text": "We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture.", "labels": [], "entities": [{"text": "subword splitting", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7821495234966278}, {"text": "neural translation", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8008669018745422}]}], "introductionContent": [{"text": "The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.8347233335177103}]}, {"text": "In that time, researchers have explored architectures ranging from convolutional neural networks to recurrent neural networks () to attentional models ( and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs.", "labels": [], "entities": []}, {"text": "NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015 () and subsequent improvements have been reported since then.", "labels": [], "entities": [{"text": "WMT English\u2192German news-domain task", "start_pos": 62, "end_pos": 97, "type": "DATASET", "confidence": 0.791729748249054}]}, {"text": "The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8134037554264069}]}, {"text": "This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively.", "labels": [], "entities": []}, {"text": "Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (.", "labels": [], "entities": []}, {"text": "The encoder and decoder in these models typically consist of one-layer ( ) or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs.", "labels": [], "entities": []}, {"text": "The attention mechanism in our four-layer model is what Luong (2015) describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1.", "labels": [], "entities": []}, {"text": "Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs.", "labels": [], "entities": []}, {"text": "Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) maybe unable to output words found only in the test corpus.", "labels": [], "entities": []}, {"text": "There are three main techniques for achieving fully open-ended decoder output.", "labels": [], "entities": []}, {"text": "Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary ( or they may conduct sentence tokenization at the level of individual characters () or subword units such as morphemes ().", "labels": [], "entities": []}, {"text": "The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (.", "labels": [], "entities": [{"text": "subword splitting", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.753361314535141}]}, {"text": "Another focus of recent research has been ways of using monolingual corpus data, available in much larger quantities, to augment the limited parallel corpora used to train translation models.", "labels": [], "entities": []}, {"text": "One way to accomplish this is to train a separate monolingual language model on a large corpus of the target language, then use this language model as an additional input to the decoder or for re-ranking output translations).", "labels": [], "entities": []}, {"text": "More recently, Sennrich (2015b) introduced the concept of augmentation through back-translation, where an entirely separate translation model is trained on a parallel corpus from the target language to the source language.", "labels": [], "entities": []}, {"text": "This backwards translation model is then used to machine-translate a monolingual corpus from the target language into the source language, producing a pseudo-parallel corpus to augment the original parallel training corpus.", "labels": [], "entities": []}, {"text": "We extend this back-translation method by translating a very large monolingual German corpus into English, then concatenating a unique subset of this augmentation corpus to the original parallel corpus for each training epoch.", "labels": [], "entities": []}], "datasetContent": [{"text": "Initial tokenization and preprocessing of the WMT 2016 English\u2192German news translation dataset was performed using the standard scripts provided with Moses (.", "labels": [], "entities": [{"text": "WMT 2016 English\u2192German news translation dataset", "start_pos": 46, "end_pos": 94, "type": "DATASET", "confidence": 0.9287048652768135}]}, {"text": "Two further processing steps were used to create the subwordbased training dataset.", "labels": [], "entities": []}, {"text": "First, capitalized characters were replaced with a sequence of a capitalization control character (a Unicode private-use character) and the corresponding lowercase character, in order to allow the subword splitting algorithm to treat capitalized words as either inherently capitalized or capitalized versions of lowercase words.", "labels": [], "entities": [{"text": "subword splitting", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.754546195268631}]}, {"text": "Without this step, much of the limited output softmax capacity is taken up with capitalized variants of common lowercase words; performing this transformation also allows us to forego \"truecasing,\" which removes sentence-initial capitalization in a lossy and sometimes unhelpful way.", "labels": [], "entities": []}, {"text": "Second, the capitalization-transformed training corpus for each language is ingested by a Morfessor 2.0 instance configured to use a balance between corpus and vocabulary entropy that produces a vocabulary of approximately 50,000 subword units.", "labels": [], "entities": []}, {"text": "For all experiments, we used using plain stochastic gradient descent with learning rate 0.7, gradient clipping at magnitude 5.0, dropout of 0.2, and learning rate decay of 50% per epoch after 8 epochs.", "labels": [], "entities": [{"text": "learning rate 0.7", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.9538204073905945}, {"text": "learning rate decay", "start_pos": 149, "end_pos": 168, "type": "METRIC", "confidence": 0.91904749472936}]}, {"text": "Following Sennrich (2015b), we first trained a non-Y-LSTM model in the reverse direction (German\u2192English) on the full WMT '16 training corpus (4.4 million sentences).", "labels": [], "entities": [{"text": "WMT '16 training corpus", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.8988293528556823}]}, {"text": "This model was then used simultaneously on 8 GPUs (with abeam search width of 4 for speed purposes) to translate 45 million sentences of the 2014 monolingual German news crawl into English.", "labels": [], "entities": [{"text": "translate 45 million sentences of the 2014 monolingual German news crawl", "start_pos": 103, "end_pos": 175, "type": "TASK", "confidence": 0.6387997757304799}]}, {"text": "A full copy of the original training corpus was then concatenated with a unique subset of this augmentation corpus to create anew training corpus for each epoch from 1 to 10; the corpus for epoch 1 was then repeated as epoch 11 et cetera.", "labels": [], "entities": []}, {"text": "For metamind-single, we trained a non-Y-LSTM model using these augmented corpora, with data-parallel synchronous SGD across four GPUs enabling a batch size of 384 and training speed of about 2,500 subword units per second.", "labels": [], "entities": []}, {"text": "The run submitted as metamind-single uses a single snapshot of this model after 12 total training epochs.", "labels": [], "entities": []}, {"text": "For metamind-ylstm, we trained a Y-LSTM model using the same corpora, with data-parallel synchronous SGD across four GPUs enabling a batch size of 320 and training speed of about 1,500 subword units per second.", "labels": [], "entities": []}, {"text": "The run submitted as metamind-ylstm uses a single snapshot of this model after 9 total training epochs.", "labels": [], "entities": []}, {"text": "The run submitted as metamind-ensemble uses an equally-weighted ensemble of three snapshots of the metamind-single model (after 10, 11, and 12 epochs) and a single snapshot of the metamind-ylstm model after 9 total training epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU results on the official WMT 2016 test set. Only our main ensemble was entered into the  human ranking process, coming in second place behind U. Edinburgh.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993438124656677}, {"text": "WMT 2016 test set", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9085642695426941}]}]}