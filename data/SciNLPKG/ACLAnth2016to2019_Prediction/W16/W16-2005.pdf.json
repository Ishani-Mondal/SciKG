{"title": [{"text": "Morphological Reinflection via Discriminative String Transduction", "labels": [], "entities": [{"text": "Morphological Reinflection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7511928081512451}, {"text": "Discriminative String Transduction", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6187979280948639}]}], "abstractContent": [{"text": "We describe our approach and experiments in the context of the SIGMOR-PHON 2016 Shared Task on Morphological Reinflection.", "labels": [], "entities": [{"text": "SIGMOR-PHON 2016 Shared Task on Morphological Reinflection", "start_pos": 63, "end_pos": 121, "type": "TASK", "confidence": 0.597877813237054}]}, {"text": "The results show that the methods of Nicolai et al.", "labels": [], "entities": []}, {"text": "(2015) perform well on typologically diverse languages.", "labels": [], "entities": []}, {"text": "We also discuss language-specific heuris-tics and errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many languages have complex morphology with dozens of different word-forms for any given lemma.", "labels": [], "entities": []}, {"text": "It is often beneficial to reduce the data sparsity introduced by morphological variation in order to improve the applicability of methods that rely on textual regularity.", "labels": [], "entities": []}, {"text": "The task of inflection generation (Task 1) is to produce an inflected form given a lemma and desired inflection, which is specified as an abstract tag.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.8262582123279572}]}, {"text": "The task of labelled reinflection (Task 2) replaces the input lemma with a morphologically-tagged inflected form.", "labels": [], "entities": []}, {"text": "Finally, the task of unlabelled reinflection (Task 3) differs from Task 2 in that the input lacks the inflection tag.", "labels": [], "entities": []}, {"text": "In this paper, we describe our system as participants in the SIGMORPHON 2016 Shared Task on Morphological Reinflection (.", "labels": [], "entities": [{"text": "SIGMORPHON 2016 Shared Task on Morphological Reinflection", "start_pos": 61, "end_pos": 118, "type": "TASK", "confidence": 0.6066065302916935}]}, {"text": "Our approach is based on discriminative string transduction performed with a modified version of the DIRECTL+ program ().", "labels": [], "entities": []}, {"text": "We perform Task 1 using the inflection generation approach of, which we refer to as the lemma-to-word model.", "labels": [], "entities": []}, {"text": "We also derive a reverse word-to-lemma (lemmatization) model from the Task 1 data.", "labels": [], "entities": []}, {"text": "We perform Task 3 by composing the word-to-lemma and lemma-to-word models.", "labels": [], "entities": []}, {"text": "We reduce Task 2 to Task 3 by simply ignoring the input inflection tag.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our transduction models are trained on the pairs of word-forms and their lemmas.", "labels": [], "entities": []}, {"text": "The wordto-lemma models (Section 2.2), are trained on the Task 1 training dataset, which contains goldstandard lemmas.", "labels": [], "entities": [{"text": "Task 1 training dataset", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.5861249193549156}]}, {"text": "These models are then employed in Tasks 2 and 3 for lemmatizing the source word-forms.", "labels": [], "entities": []}, {"text": "The lemma-to-word models (Section 2.4) are derived from the training data of all three tasks, observing the Track 1 stipulations (Section 2.5).", "labels": [], "entities": []}, {"text": "For example, the lemma-to-word models employed in Task 2 are trained on a combination of the gold-standard lemmas from Task 1, as well as the lemmas generated by the wordto-lemma models from the source word-forms in Task 2.", "labels": [], "entities": []}, {"text": "Our development experiments showed that this kind of self-training approach can improve the overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9984768033027649}]}], "tableCaptions": [{"text": " Table 1.  The Task 1 results are broken down by part-of- speech. Because of an ambiguity in the initial  shared task instructions, all development models  were trained on a union of the data from all three  tasks.", "labels": [], "entities": []}, {"text": " Table 1: Word accuracy on the development sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9625638127326965}]}, {"text": " Table 2: Word accuracy on the test sets. 3", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.936730682849884}]}]}