{"title": [{"text": "Chinese-to-Japanese Patent Machine Translation based on Syntactic Pre-ordering for WAT 2016", "labels": [], "entities": [{"text": "Chinese-to-Japanese Patent Machine Translation", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.5610386729240417}, {"text": "WAT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.7494252920150757}]}], "abstractContent": [{"text": "This paper presents our Chinese-to-Japanese patent machine translation system for WAT 2016 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures.", "labels": [], "entities": [{"text": "Chinese-to-Japanese patent machine translation", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.6107625886797905}, {"text": "WAT 2016 (Group ID: ntt)", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.6272407323122025}]}, {"text": "Chinese words are reordered by a learning-to-rank model based on pairwise classification to obtain word order close to Japanese.", "labels": [], "entities": []}, {"text": "In this year's system, two different machine translation methods are compared: traditional phrase-based statistical machine translation and recent sequence-to-sequence neural machine translation with an attention mechanism.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7701449990272522}, {"text": "phrase-based statistical machine translation", "start_pos": 91, "end_pos": 135, "type": "TASK", "confidence": 0.5918051972985268}, {"text": "sequence-to-sequence neural machine translation", "start_pos": 147, "end_pos": 194, "type": "TASK", "confidence": 0.6298976689577103}]}, {"text": "Our pre-ordering showed a significant improvement over the phrase-based baseline, but, in contrast, it degraded the neural machine translation baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Patent documents, which are well-structured written texts that describe the technical details of inventions, are expected to have almost no semantic ambiguities caused by indirect or rhetorical expressions.", "labels": [], "entities": []}, {"text": "Therefore, they are good candidates for literal translation, which most machine translation (MT) approaches aim to do.", "labels": [], "entities": [{"text": "literal translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.9422470927238464}, {"text": "machine translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8477849721908569}]}, {"text": "One technical challenge for patent machine translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for word order in distant languages.", "labels": [], "entities": [{"text": "patent machine translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6732571025689443}, {"text": "MT reordering", "start_pos": 163, "end_pos": 176, "type": "TASK", "confidence": 0.982188493013382}]}, {"text": "Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation.", "labels": [], "entities": []}, {"text": "In the WAT 2016 evaluation campaign (, we participated in a Chinese-to-Japanese patent translation task and tackled long distance reordering by syntactic pre-ordering based on Chinese dependency structures, as in our last year's system . We also use a recent neural MT as the following MT implementation for comparison with a traditional phrase-based statistical MT.", "labels": [], "entities": [{"text": "Chinese-to-Japanese patent translation task", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.7262421324849129}]}, {"text": "Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering with hand-written rules or a learning-to-rank model; and the following MT component (phrase-based MT or neural MT).", "labels": [], "entities": [{"text": "Chinese syntactic analysis", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6355049908161163}, {"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6914713233709335}, {"text": "part-of-speech (POS) tagging", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.6651617765426636}, {"text": "dependency parsing", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7058402001857758}, {"text": "MT", "start_pos": 292, "end_pos": 294, "type": "TASK", "confidence": 0.935949444770813}]}, {"text": "This paper describes our system's details and discusses our evaluation results.", "labels": [], "entities": []}, {"text": "shows a brief workflow of our Chinese-to-Japanese MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9510434865951538}]}, {"text": "Its basic architecture is standard with syntactic pre-ordering.", "labels": [], "entities": []}, {"text": "Input sentences are first applied to word segmentation and POS tagging, parsed into dependency trees, reordered using pre-ordering rules or a pre-ordering model, and finally translated into Japanese by MT.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7517038285732269}, {"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.7922193109989166}, {"text": "MT", "start_pos": 202, "end_pos": 204, "type": "DATASET", "confidence": 0.823404848575592}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official evaluation results in JPO Adequacy, Pairwise Crowdsourcing Evaluation scores (Pair- wise), BLEU, RIBES, and AMFM. Automatic evaluation scores are based on JUMAN Japanese word  segmentation. Scores in bold are best in the same group.  \u2020: Systems used external resources.", "labels": [], "entities": [{"text": "JPO", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.767419159412384}, {"text": "Adequacy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.47920098900794983}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9983435869216919}, {"text": "RIBES", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.9949329495429993}, {"text": "AMFM", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.8755524158477783}, {"text": "JUMAN Japanese word  segmentation", "start_pos": 174, "end_pos": 207, "type": "TASK", "confidence": 0.7032295167446136}]}, {"text": " Table 2: Source-side test set perplexities on dev, devtest, and test sets by word 5-gram language models  of Chinese and pre-ordered Chinese. The vocabulary size is 172,108.", "labels": [], "entities": []}]}