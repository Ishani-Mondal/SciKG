{"title": [], "abstractContent": [{"text": "We describe the submissions of the University of Sheffield (USFD) for the phrase-level Quality Estimation (QE) shared task of WMT16.", "labels": [], "entities": [{"text": "phrase-level Quality Estimation (QE) shared task of WMT16", "start_pos": 74, "end_pos": 131, "type": "TASK", "confidence": 0.555988734960556}]}, {"text": "We test two different approaches for phrase-level QE: (i) we enrich the provided set of baseline features with information about the context of the phrases, and (ii) we exploit predictions at other granularity levels (word and sentence).", "labels": [], "entities": [{"text": "phrase-level QE", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.5591495931148529}]}, {"text": "These approaches perform closely in terms of multiplication of F 1-scores (primary evaluation metric), but are considerably different in terms of the F 1-scores for individual classes.", "labels": [], "entities": [{"text": "F 1-scores", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9374779760837555}]}], "introductionContent": [{"text": "Quality Estimation (QE) of Machine Translation (MT) is the task of determining the quality of an automatically translated text without comparing it to a reference translation.", "labels": [], "entities": [{"text": "Quality Estimation (QE) of Machine Translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7733097645369443}]}, {"text": "This task has received more attention recently because of the widespread use of MT systems and the need to evaluate their performance on the fly.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9718512296676636}]}, {"text": "The problem has been modelled to estimate the quality of translations at the word, sentence and document levels (.", "labels": [], "entities": []}, {"text": "Word-level QE can be particularly useful for post-editing of machinetranslated texts: if we know the erroneous words in a sentence, we can highlight them to attract posteditor's attention, which should improve both productivity and final translation quality.", "labels": [], "entities": []}, {"text": "However, the choice of words in an automatically translated sentence is motivated by the context, so MT errors are also context-dependent.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.990140974521637}]}, {"text": "Moreover, as it has been shown in), errors in multiple adjacent words can be caused by a single incorrect decision -e.g. an incorrect lexical choice can result in errors in all its syntactic dependants.", "labels": [], "entities": []}, {"text": "The task of estimating quality at the phrase level aims to address these limitations of word-level models for improved prediction performance.", "labels": [], "entities": []}, {"text": "The first effort to estimate the quality of translated n-grams (instead of individual words) was described in (, but there the multi-word nature of predictions was motivated by the architecture of the MT system used in the experiment: an interactive MT system which did not translate entire sentences, but rather predicted the next n word translations in a sentence.", "labels": [], "entities": []}, {"text": "An approach was designed to estimate the confidence of the MT system about the prediction and was aimed at improving translation prediction quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9772790670394897}, {"text": "translation prediction", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.9709189832210541}]}, {"text": "The phrase-level QE in its current formulation -estimation of the quality of phrases in a pretranslated sentence using external features of these phrases -was first addressed in the work of, where the authors segmented automatically translated sentences into phrases, labelled these phrases based on wordlevel labels and trained several phrase-level QE models using different feature sets and machine learning algorithms.", "labels": [], "entities": []}, {"text": "The baseline phrase-level QE system used in this shared task was based on the results in ( . This year's Conference on Statistical Machine Translation (WMT16) includes a shared task on phrase-level QE (QE Task 2p) for the first time.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT16)", "start_pos": 119, "end_pos": 158, "type": "TASK", "confidence": 0.750815749168396}]}, {"text": "This task uses the same training and test data as the one used for the word-level QE task (QE Task 2): the set of English sentences, their automatic translations into German and their manual post-editions performed by professional translators.", "labels": [], "entities": []}, {"text": "The data belongs to the IT domain.", "labels": [], "entities": [{"text": "IT domain", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.9527325332164764}]}, {"text": "The training set contains 12,000 sentences, development and test sets -1,000 and 2,000 sentences, respectively.", "labels": [], "entities": []}, {"text": "For model training and evaluation, the words are la-belled as \"BAD\" or \"OK\" based on labelling generated with the TERcom tool 1 : if an edit operation (substitution or insertion) was applied to a word, it is labelled as \"BAD\"; contrarily, if the word was left unchanged, it is considered \"OK\".", "labels": [], "entities": [{"text": "BAD", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9957090616226196}, {"text": "BAD", "start_pos": 221, "end_pos": 224, "type": "METRIC", "confidence": 0.9907518625259399}]}, {"text": "For the phrase-level task, the data was segmented also into phrases.", "labels": [], "entities": []}, {"text": "The segmentation was given by the decoder that produced the automatic translations.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9741474986076355}]}, {"text": "The segments are labelled at the phrase level using the word-level labels: a phrase is labelled as \"OK\" if it contains only words labelled as \"OK\"; if one or more words in a phrase are \"BAD\"', the phrase is \"BAD\" itself.", "labels": [], "entities": [{"text": "BAD", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.922738254070282}, {"text": "BAD", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.910550057888031}]}, {"text": "The predictions are done at the phrase level, but evaluated at the word level: for the evaluation phrase-level labels are unrolled back to their word-level versions (i.e. if a threeword phrase is labelled as \"BAD\", it is equivalent to three \"BAD\" word-level labels).", "labels": [], "entities": [{"text": "BAD", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.8932772874832153}]}, {"text": "The baseline phrase-level features provided by the organisers of the task are black-box features that were originally used for sentence-level quality estimation and extracted using the QuEst toolkit 2 ( . While this feature set considers many aspects of sentence quality (mostly the ones that do not depend on internal MT system information and do not require language-specific resources), it has an important limitation when applied to phrases.", "labels": [], "entities": [{"text": "sentence-level quality estimation", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.6305770178635915}]}, {"text": "Namely, it does not take into account the context of the phrase, i.e. words and phrases in the sentence, either before or after the phrase of interest.", "labels": [], "entities": []}, {"text": "In order to advance upon the baseline results, we enhanced the baseline feature set with contextual information for phrases.", "labels": [], "entities": []}, {"text": "Another approach we experimented with is the use of predictions made by QE models at other levels of granularity: word level and sentence level.", "labels": [], "entities": []}, {"text": "The motivation here is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, we use a wider range of features which are unavailable at the phrase level.", "labels": [], "entities": []}, {"text": "On the other hand, the use of word-level and sentence-level predictions can help mitigate the uncertainty of phraselevel scores: there, a phrase is labelled as \"BAD\" if it has any number of \"BAD\" words, so \"BAD\" phrases can be of very different quality.", "labels": [], "entities": []}, {"text": "We believe that information on the quality of individual words and the overall quality of a sentence can be complementary for phrase-level quality prediction.", "labels": [], "entities": [{"text": "phrase-level quality prediction", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.7577571074167887}]}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "We describe our context-based QE strategy in Section 2.", "labels": [], "entities": [{"text": "QE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8393853902816772}]}, {"text": "In Section 3 we explain our approach to build phrase-level QE models using predictions of other levels.", "labels": [], "entities": []}, {"text": "Section 4 reports the final results, while Section 5 outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F 1 -multiplied scores of models trained on  baseline and extended feature sets using different  optimisation algorithms for CRFSuite.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9798779189586639}]}, {"text": " Table 2: Performance of our official submissions  on the test set.", "labels": [], "entities": []}, {"text": " Table 3: Performance for combinations of models  on the test set.", "labels": [], "entities": []}]}