{"title": [], "abstractContent": [{"text": "The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns.", "labels": [], "entities": []}, {"text": "Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3.", "labels": [], "entities": []}, {"text": "Lately, model-based analysis of corpus annotations have proven better at all three tasks.", "labels": [], "entities": []}, {"text": "But there has been relatively little work comparing them on the same datasets.", "labels": [], "entities": []}, {"text": "This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items.", "labels": [], "entities": []}, {"text": "We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization , and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9362525939941406}, {"text": "annotator characterization", "start_pos": 113, "end_pos": 139, "type": "TASK", "confidence": 0.6588759124279022}]}, {"text": "We conclude with guidelines for model selection , application, and implementation.", "labels": [], "entities": [{"text": "model selection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6573509126901627}]}], "introductionContent": [{"text": "The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen's \u03ba ().", "labels": [], "entities": []}, {"text": "However, aggregation by majority vote implicitly assumes equal expertise among the annotators.", "labels": [], "entities": []}, {"text": "This assumption, though, has been repeatedly shown to be false in annotation practice ().", "labels": [], "entities": []}, {"text": "Chanceadjusted coefficients of agreement also have many shortcomings-for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction).", "labels": [], "entities": []}, {"text": "Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing.", "labels": [], "entities": []}, {"text": "Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9972878694534302}]}, {"text": "They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication.", "labels": [], "entities": []}, {"text": "But even though a large number of such models has been proposed, inter alia), it is not immediately obvious to potential users how these models differ or, in fact, how they should be applied at all.", "labels": [], "entities": []}, {"text": "To our knowledge, the literature comparing models of annotation is limited, focused exclusively on synthetic data) or using publicly available implementations that constrain the experiments almost exclusively to binary annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "The models of annotation discussed in this paper find their application in multiple tasks: to label items, characterize the annotators, or flag especially difficult items.", "labels": [], "entities": []}, {"text": "This section lays out the metrics used in the evaluation of each of these tasks.", "labels": [], "entities": []}, {"text": "PD 5892 43161 294 4 1 5 7 7 9 57 1 4 13 147 51 3395: General statistics (I items, N observations, J annotators, K classes) together with summary statistics for the number of annotators per item (J/I) and the number of items per annotator (I/J) (i.e., Min, 1st Quartile, Median, Mean, 3rd Quartile, and Max).", "labels": [], "entities": [{"text": "PD 5892 43161 294", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9213923811912537}]}, {"text": "We evaluate on a collection of datasets reflecting a variety of use-cases and conditions: binary vs. multi-class classification; small vs. large number of annotators; sparse vs. abundant number of items per annotator / annotators per item; and varying degrees of annotator quality (statistics presented in).", "labels": [], "entities": []}, {"text": "Three of the datasets-WSD, RTE, and TEMP, created by-are widely used in the literature on annotation models).", "labels": [], "entities": [{"text": "RTE", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.43558236956596375}, {"text": "TEMP", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.8773489594459534}]}, {"text": "In addition, we include the Phrase Detectives 1.0 (PD) corpus, which differs in a number of key ways from the datasets: It has a much larger number of items and annotations, greater sparsity, and a much greater likelihood of spamming due to its collection via a game-with-a-purpose setting.", "labels": [], "entities": [{"text": "Phrase Detectives 1.0 (PD)", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7972619831562042}]}, {"text": "This dataset is also less artificial than the datasets in, which were created with the express purpose of testing crowdsourcing.", "labels": [], "entities": []}, {"text": "The data consist of anaphoric annotations, which we reduce to four general classes (DN/DO = discourse new/old, PR = property, and NR = non-referring).", "labels": [], "entities": []}, {"text": "To ensure similarity with the Snow et al. datasets, we also limit the coders to one annotation per item (discarded data were mostly redundant annotations).", "labels": [], "entities": [{"text": "Snow et al. datasets", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.6716466844081879}]}, {"text": "Furthermore, this corpus allows us to evaluate on meta-data not usually available in traditional crowdsourcing platforms, namely, information about confessed spammers and good, established players.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: General statistics (I items, N observations, J  annotators, K classes) together with summary statis- tics for the number of annotators per item (J/I) and  the number of items per annotator (I/J) (i.e., Min, 1st  Quartile, Median, Mean, 3rd Quartile, and Max).", "labels": [], "entities": []}, {"text": " Table 3: PD dataset results against the gold standard. * indicates that significance holds after Bonferroni correction.", "labels": [], "entities": [{"text": "PD dataset", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8893372416496277}, {"text": "significance", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9917663931846619}]}, {"text": " Table 4: Results against the gold (\u00b5 = Micro; M = Macro).", "labels": [], "entities": [{"text": "\u00b5 = Micro; M = Macro)", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.772664025425911}]}, {"text": " Table 6: Correlation between gold and estimated accu- racy of annotators. The last two columns refer to the  list of known spammers and non-spammers in PD.", "labels": [], "entities": []}, {"text": " Table 8: Spammer analysis example. D&S provides a  confusion matrix; MACE shows the spamming prefer- ence and the credibility.", "labels": [], "entities": [{"text": "Spammer analysis", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8835272192955017}, {"text": "MACE", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.8599129319190979}]}, {"text": " Table 9: A non-spammer analysis example. D&S pro- vides a confusion matrix; MACE shows the spamming  preference and the credibility.", "labels": [], "entities": [{"text": "MACE", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.911725640296936}]}]}