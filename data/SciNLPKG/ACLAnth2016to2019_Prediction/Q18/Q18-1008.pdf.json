{"title": [{"text": "Evaluating the Stability of Embedding-based Word Similarities", "labels": [], "entities": [{"text": "Stability of Embedding-based Word Similarities", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.7331510305404663}]}], "abstractContent": [{"text": "Word embeddings are increasingly being used as a tool to study word associations in specific corpora.", "labels": [], "entities": []}, {"text": "However, it is unclear whether such embeddings reflect enduring properties of language or if they are sensitive to inconsequential variations in the source documents.", "labels": [], "entities": []}, {"text": "We find that nearest-neighbor distances are highly sensitive to small changes in the training corpus fora variety of algorithms.", "labels": [], "entities": []}, {"text": "For all methods, including specific documents in the training set can result in substantial variations.", "labels": [], "entities": []}, {"text": "We show that these effects are more prominent for smaller training corpora.", "labels": [], "entities": []}, {"text": "We recommend that users never rely on single embedding models for distance calculations, but rather average over multiple bootstrap samples, especially for small corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings area popular technique in natural language processing (NLP) in which the words in a vocabulary are mapped to low-dimensional vectors.", "labels": [], "entities": [{"text": "Word embeddings", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6612008661031723}, {"text": "natural language processing (NLP)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7882325251897176}]}, {"text": "Embedding models are easily trained-several implementations are publicly available-and relationships between the embedding vectors, often measured via cosine similarity, can be used to reveal latent semantic relationships between pairs of words.", "labels": [], "entities": []}, {"text": "Word embeddings are increasingly being used by researchers in unexpected ways and have become popular in fields such as digital humanities and computational social science (.", "labels": [], "entities": []}, {"text": "Embedding-based analyses of semantic similarity can be a robust and valuable tool, but we find that standard methods dramatically under-represent the variability of these measurements.", "labels": [], "entities": []}, {"text": "Embedding algorithms are much more sensitive than they appear to factors such as the presence of specific documents, the size of the documents, the size of the corpus, and even seeds for random number generators.", "labels": [], "entities": []}, {"text": "If users do not account for this variability, their conclusions are likely to be invalid.", "labels": [], "entities": []}, {"text": "Fortunately, we also find that simply averaging over multiple bootstrap samples is sufficient to produce stable, reliable results in all cases tested.", "labels": [], "entities": []}, {"text": "NLP research in word embeddings has so far focused on a downstream-centered use case, where the end goal is not the embeddings themselves but performance on a more complicated task.", "labels": [], "entities": []}, {"text": "For example, word embeddings are often used as the bottom layer in neural network architectures for NLP.", "labels": [], "entities": []}, {"text": "The embeddings' training corpus, which is selected to be as large as possible, is only of interest insofar as it generalizes to the downstream training corpus.", "labels": [], "entities": []}, {"text": "In contrast, other researchers take a corpuscentered approach and use relationships between embeddings as direct evidence about the language and culture of the authors of a training corpus (.", "labels": [], "entities": []}, {"text": "Embeddings are used as if they were simulations of a survey asking subjects to free-associate words from query terms.", "labels": [], "entities": []}, {"text": "Unlike the downstream-centered approach, the corpus-centered approach is based on direct human analysis of nearest neighbors to embedding vectors, and the training corpus is not simply an off-the-shelf convenience but rather the central object of study.", "labels": [], "entities": []}, {"text": "Corpus-centered Big corpus Small corpus, difficult or impossible to expand Source is not important Source is the object of study Only vectors are important Specific, fine-grained comparisons are important Embeddings are used in downstream tasks Embeddings are used to learn about the mental model of word association for the authors of the corpus: Comparison of downstream-centered and corpuscentered approaches to word embeddings.", "labels": [], "entities": []}, {"text": "While word embeddings may appear to measure properties of language, they in fact only measure properties of a curated corpus, which could suffer from several problems.", "labels": [], "entities": []}, {"text": "The training corpus is merely a sample of the authors' language model).", "labels": [], "entities": []}, {"text": "Sources could be missing or over-represented, typos and other lexical variations could be present, and, as noted by, \"Many datasets are most naturally arranged in away where successive examples are highly correlated.\"", "labels": [], "entities": []}, {"text": "Furthermore, embeddings can vary considerably across random initializations, making lists of \"most similar words\" unstable.", "labels": [], "entities": []}, {"text": "We hypothesize that training on small and potentially idiosyncratic corpora can exacerbate these problems and lead to highly variable estimates of word similarity.", "labels": [], "entities": []}, {"text": "Such small corpora are common in digital humanities and computational social science, and it is often impossible to mitigate these problems simply by expanding the corpus.", "labels": [], "entities": []}, {"text": "For example, we cannot create more 18th Century English books or change their topical focus.", "labels": [], "entities": []}, {"text": "We explore causes of this variability, which range from the fundamental stochastic nature of certain algorithms to more troubling sensitivities to properties of the corpus, such as the presence or absence of specific documents.", "labels": [], "entities": []}, {"text": "We focus on the training corpus as a source of variation, viewing it as a fragile artifact curated by often arbitrary decisions.", "labels": [], "entities": []}, {"text": "We examine four different algorithms and six datasets, and we manipulate the corpus by shuffling the order of the documents and taking bootstrap samples of the documents.", "labels": [], "entities": []}, {"text": "Finally, we examine the effects of these manipulations on the cosine similarities between embeddings.", "labels": [], "entities": []}, {"text": "We find that there is considerable variability in embeddings that may not be obvious to users of these methods.", "labels": [], "entities": []}, {"text": "Rankings of most similar words are not reliable, and both ordering and membership in such lists are liable to change significantly.", "labels": [], "entities": []}, {"text": "Some uncertainty is expected, and there is no clear criterion for \"acceptable\" levels of variance, but we argue that the amount of variation we observe is sufficient to call the whole method into question.", "labels": [], "entities": []}, {"text": "For example, we find cases in which there is zero set overlap in \"top 10\" lists for the same query word across bootstrap samples.", "labels": [], "entities": []}, {"text": "Smaller corpora and larger document sizes increase this variation.", "labels": [], "entities": []}, {"text": "Our goal is to provide methods to quantify this variability, and to account for this variability, we recommend that as the size of a corpus gets smaller, cosine similarities should be averaged over many bootstrap samples.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of the number of documents, number of unique words (after removing words that appear fewer  than 20 times), vocabulary density (the ratio of unique words to the total number of words), and the average number of  words per document for each corpus.", "labels": [], "entities": []}, {"text": " Table 4: The most similar words with their means and standard deviations for the cosine similarities between the query  word marijuana and its 10 nearest neighbors (highest mean cosine similarity in the FIXED setting. Embeddings are  learned from documents segmented by sentence.", "labels": [], "entities": [{"text": "FIXED setting", "start_pos": 204, "end_pos": 217, "type": "DATASET", "confidence": 0.8410681486129761}]}]}