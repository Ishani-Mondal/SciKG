{"title": [], "abstractContent": [{"text": "Video captioning has attracted an increasing amount of interest, due in part to its potential for improved accessibility and information retrieval.", "labels": [], "entities": [{"text": "Video captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7331957817077637}, {"text": "information retrieval", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7711376845836639}]}, {"text": "While existing methods rely on different kinds of visual features and model archi-tectures, they do not make full use of pertinent semantic cues.", "labels": [], "entities": []}, {"text": "We present a unified and exten-sible framework to jointly leverage multiple sorts of visual features and semantic attributes.", "labels": [], "entities": []}, {"text": "Our novel architecture builds on LSTMs with two multi-faceted attention layers.", "labels": [], "entities": []}, {"text": "These first learn to automatically select the most salient visual features or semantic attributes, and then yield overall representations for the input and output of the sentence generation component via custom feature scaling operations.", "labels": [], "entities": []}, {"text": "Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms previous work and performs robustly even in the presence of added noise to the features and attributes.", "labels": [], "entities": [{"text": "MSR-VTT datasets", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9063462615013123}]}], "introductionContent": [{"text": "The task of automatically generating captions for videos has been receiving an increasing amount of attention.", "labels": [], "entities": [{"text": "automatically generating captions for videos", "start_pos": 12, "end_pos": 56, "type": "TASK", "confidence": 0.7727548837661743}]}, {"text": "On YouTube, for example, every single minute, hundreds of hours of video content are uploaded.", "labels": [], "entities": []}, {"text": "Obviously, there is noway a person could sit and binge-watch these overwhelming amounts of video, so new techniques to search and quickly understand them are highly sought.", "labels": [], "entities": []}, {"text": "Generating captions, i.e., short natural language descriptions, for videos is an important technique to address this chalVideo: 301 frames a monkey is playing with a dog.", "labels": [], "entities": []}, {"text": "Semantic Attributes: \"Monkey\", \"Animal\", \"Dog\", \"Playing\", \"Pulling\", \"Grass\"...", "labels": [], "entities": []}, {"text": "lenge, while also greatly improving their accessibility for blind and visually impaired users.", "labels": [], "entities": []}], "datasetContent": [{"text": "MSVD: We evaluate our video captioning models on the Microsoft Research Video Description Corpus (.", "labels": [], "entities": [{"text": "MSVD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9059283137321472}, {"text": "video captioning", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7729949951171875}, {"text": "Microsoft Research Video Description Corpus", "start_pos": 53, "end_pos": 96, "type": "DATASET", "confidence": 0.9224869012832642}]}, {"text": "MSVD consists of 1,970 video clips downloaded from YouTube that typically depict a single activity.", "labels": [], "entities": [{"text": "MSVD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9181103110313416}]}, {"text": "Each video clip is annotated with multiple human-written descriptions in several languages.", "labels": [], "entities": []}, {"text": "We only use the English descriptions, about 41 descriptions per video.", "labels": [], "entities": []}, {"text": "In total, the dataset consists of 80,839 video-description pairs.", "labels": [], "entities": []}, {"text": "Each description on average contains about 8 words.", "labels": [], "entities": []}, {"text": "We use 1,200 videos for training, 100 videos for validation, and 670 videos for testing, as provided by previous work.", "labels": [], "entities": []}, {"text": "MSR-VTT: We also evaluate on the MSR Videoto-Text (MSR-VTT) dataset ( , a recent large-scale video benchmark for video captioning.", "labels": [], "entities": [{"text": "MSR Videoto-Text (MSR-VTT) dataset", "start_pos": 33, "end_pos": 67, "type": "DATASET", "confidence": 0.8090362449487051}, {"text": "video captioning", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7032980471849442}]}, {"text": "MSR-VTT provides 10,000 web video clips.", "labels": [], "entities": [{"text": "MSR-VTT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9704645276069641}]}, {"text": "Each video is annotated with about 20 sentences.", "labels": [], "entities": []}, {"text": "Thus, we have 200,000 video-caption pairs in total.", "labels": [], "entities": []}, {"text": "Our video captioning models are trained and hyperparameters are selected using the official training and validation set, which consists of 6,513 and 497 video clips, respectively.", "labels": [], "entities": []}, {"text": "The models are evaluated using the test set of 2,990 video clips.", "labels": [], "entities": []}, {"text": "We rely on three standard metrics, BLEU (), METEOR (Banerjee and, and CIDEr ( ) to evaluate our methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9989050626754761}, {"text": "METEOR", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9852912425994873}, {"text": "CIDEr", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.865423321723938}]}, {"text": "These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work.", "labels": [], "entities": [{"text": "image and video captioning tasks", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7171810030937195}]}, {"text": "We use the Microsoft COCO evaluation toolkit , which is widely used in previous work, to compute the metric scores.", "labels": [], "entities": [{"text": "Microsoft COCO evaluation toolkit", "start_pos": 11, "end_pos": 44, "type": "DATASET", "confidence": 0.7810061573982239}]}, {"text": "Across all three metrics, higher scores indicate that the generated captions are assessed as being closer to captions authored by humans.", "labels": [], "entities": []}, {"text": "Based on our hyperparameter optimization on the validation set, the number of hidden units in the first multi-facted attention layer and in the LSTM are both set to 512.", "labels": [], "entities": []}, {"text": "The activation function of the LSTM is tanh and the activation functions of both multifaceted attention layers are linear.", "labels": [], "entities": []}, {"text": "The dropout rates of both of the multi-facted attention layers are set to 0.5.", "labels": [], "entities": [{"text": "dropout rates", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9512006938457489}]}, {"text": "We use pretrained 300-dimensional GloVe () vectors as our word embedding matrix.", "labels": [], "entities": []}, {"text": "We rely on the RMSPROP algorithm) to update parameters for better convergence, with a learning rate of 10 \u22124 . The beam size during sentence generation is set to 5.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7720882296562195}]}, {"text": "Our system is implemented using the Theano () framework.", "labels": [], "entities": [{"text": "Theano () framework", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9246575236320496}]}], "tableCaptions": [{"text": " Table 1: Comparison with existing results on MSVD, where (-) indicates unknown scores.", "labels": [], "entities": [{"text": "MSVD", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.517276406288147}]}, {"text": " Table 2: Results on MSVD.", "labels": [], "entities": [{"text": "MSVD", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.49282708764076233}]}, {"text": " Table 3: Results on MSR-VTT.", "labels": [], "entities": [{"text": "MSR-VTT", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.7944629788398743}]}]}