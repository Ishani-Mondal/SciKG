{"title": [{"text": "Low-Rank RNN Adaptation for Context-Aware Language Modeling", "labels": [], "entities": [{"text": "Context-Aware Language Modeling", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6723854939142863}]}], "abstractContent": [{"text": "A context-aware language model uses location , user and/or domain metadata (context) to adapt its predictions.", "labels": [], "entities": []}, {"text": "In neural language models, context information is typically represented as an embedding and it is given to the RNN as an additional input, which has been shown to be useful in many applications.", "labels": [], "entities": []}, {"text": "We introduce a more powerful mechanism for using context to adapt an RNN by letting the context vector control a low-rank transformation of the recurrent layer weight matrix.", "labels": [], "entities": []}, {"text": "Experiments show that allowing a greater fraction of the model parameters to be adjusted has benefits in terms of perplexity and classification for several different types of context.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many language modeling applications, the speech or text is associated with some metadata or contextual information.", "labels": [], "entities": []}, {"text": "For example, in speech recognition, if a user is speaking to a personal assistant then the system might know the time of day or the identity of the task that the user is trying to accomplish.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7499734461307526}]}, {"text": "If the user takes a picture of a sign to translate it with their smartphone, the system would have contextual information related to the geographic location and the user's preferred language.", "labels": [], "entities": []}, {"text": "The context-aware language model targets these types of applications with a model that can adapt its predictions based on the provided contextual information.", "labels": [], "entities": []}, {"text": "There has been much work on using context information to adapt language models.", "labels": [], "entities": []}, {"text": "Here, we are interested in contexts described by metadata (vs. word history or related documents) and in neural network approaches due to their flexibility for representing diverse types of contexts.", "labels": [], "entities": []}, {"text": "Specifically, we focus on recurrent neural networks (RNNs) due to their widespread use.", "labels": [], "entities": []}, {"text": "The standard approach to adapt an RNN language model is to concatenate the context representation with the word embedding at the input to the RNN (.", "labels": [], "entities": []}, {"text": "Optionally, the context embedding is also concatenated with the output from the recurrent layer to adapt the softmax layer.", "labels": [], "entities": []}, {"text": "This basic strategy has been adopted for various types of adaptation such as for LM personalization (), adapting to television show genres, and adapting to long range dependencies in a document ( , etc.", "labels": [], "entities": [{"text": "LM personalization", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9246595203876495}, {"text": "adapting to television show genres", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.7955557942390442}]}, {"text": "We propose a more powerful mechanism for using a context vector, which we call the FactorCell.", "labels": [], "entities": []}, {"text": "Rather than simply using context as an additional input, it is used to control a factored (low-rank) transformation of the recurrent layer weight matrix.", "labels": [], "entities": []}, {"text": "The motivation is that allowing a greater fraction of the model parameters to be adjusted in response to the input context will produce a model that is more adaptable and responsive to that context.", "labels": [], "entities": []}, {"text": "We evaluate the resulting models in terms of context-dependent perplexity and context classification accuracy on six tasks reflecting different types of context variables, comparing to baselines that represent the most popular methods for using context in neural models.", "labels": [], "entities": [{"text": "context classification", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7010887712240219}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9074236154556274}]}, {"text": "We choose tasks where context is specified by metadata, rather than text samples as used in many prior studies.", "labels": [], "entities": []}, {"text": "The combination of experiments on a variety of data sources provides strong evidence for the utility of the FactorCell model, but the results show that it can be useful to consider more than just perplexity in training a language model.", "labels": [], "entities": []}, {"text": "The remainder proceeds as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the FactorCell model and show how it differs mathematically from alternative approaches.", "labels": [], "entities": []}, {"text": "Next, Section 3 describes the six datasets used to probe the performance of different models.", "labels": [], "entities": []}, {"text": "Experiments and analyses contrasting perplexity and classification results fora variety of context variables are provided in Section 4, demonstrating consistent improvements in both criteria for the FactorCell model but also confirming that perplexity is not correlated with classification performance for all models.", "labels": [], "entities": []}, {"text": "Analyses explore the effectiveness of the model for characterizing high-dimensional context spaces.", "labels": [], "entities": []}, {"text": "The model is compared to related work in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8843196332454681}]}, {"text": "Finally, Section 6 summarizes contributions and open questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiments is to show that the FactorCell model can deliver improved performance over current approaches for multiple language model applications and a variety of types of contexts.", "labels": [], "entities": []}, {"text": "Specifically, results are reported for contextconditioned perplexity and generative model text classification accuracy, using contexts that capture a range of phenomena and dimensionalities.", "labels": [], "entities": [{"text": "generative model text classification", "start_pos": 73, "end_pos": 109, "type": "TASK", "confidence": 0.918284460902214}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.8163778185844421}]}, {"text": "Test set perplexity is the most widely accepted method for evaluating language models, both for use in recognition/translation applications and generation.", "labels": [], "entities": [{"text": "recognition/translation", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.8494165539741516}]}, {"text": "It has the advantage that it is easy to measure and is widely used as a criteria for model fit, but the limitation that it is not directly matched to most tasks that language models are directly used for.", "labels": [], "entities": []}, {"text": "Text classification using the model in a generative classifier is a simple application of Bayes rule: where w 1:T is the text sequence, p(\u03c9) is the class prior, which we assume to be uniform.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8375421166419983}]}, {"text": "Classification accuracy provides additional information about the power of a model, even if it is not being designed explicitly for text classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8809468150138855}, {"text": "text classification", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7718251645565033}]}, {"text": "Further, it allows us to be able to directly compare our model perfor-mance against previously published text classification benchmarks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7271881550550461}]}, {"text": "Note that the use of classification accuracy for evaluation here involves counting errors associated with applying the generative model to independent test samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.6891722083091736}]}, {"text": "This differs from the accuracy criterion used for evaluating context-sensitive language models for text generation based on a separate discriminative classifier trained on generated text).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9991154074668884}, {"text": "text generation", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.74395951628685}]}, {"text": "We discuss this further in Section 5.", "labels": [], "entities": []}, {"text": "The experiments compare the FactorCell model (equations 4 and 6) to two popular alternatives, which we refer to as ConcatCell (equations 2 and 6) and SoftmaxBias (equation 6).", "labels": [], "entities": []}, {"text": "As noted earlier, the SoftmaxBias method is a simplification of the ConcatCell model, which is in turn a simplification of the FactorCell model.", "labels": [], "entities": []}, {"text": "The SoftmaxBias method impacts only the output layer and thus only unigram statistics.", "labels": [], "entities": []}, {"text": "Since bag-of-word models provide strong baselines in many text classification tasks, we hypothesize that the SoftmaxBias model will capture much of the relative improvement over the unadapted model for word-based tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7824870049953461}]}, {"text": "However, in small vocabulary character-based models, the unigram distribution is unlikely to carry much information about the context, so adapting the recurrent layer should become more important in characterlevel models.", "labels": [], "entities": []}, {"text": "We expect that performance gains will be greatest for the FactorCell model for sources that have sufficient structure and data to support learning the extra degrees of freedom.", "labels": [], "entities": []}, {"text": "Another possible baseline would use models independently trained on the subset of data for each context.", "labels": [], "entities": []}, {"text": "This is the \"independent component\" case in).", "labels": [], "entities": []}, {"text": "This will fail when a context variable takes on many values (or continuous values) or when training data is limited, because it makes poor use of the training data, as shown in that study.", "labels": [], "entities": []}, {"text": "While we do have some datasets where this approach is plausible, we feel that its limitations have been clearly established.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics: Dataset size in words (* or characters) of Train, Dev and Test sets, vocabulary size, number  of training documents, and context variables.", "labels": [], "entities": []}, {"text": " Table 2: Selected hyperparameters for each dataset. When a range is listed it means that a different values were  selected for the FactorCell, ConcatCell, SoftmaxBias or Unadapted models.", "labels": [], "entities": [{"text": "FactorCell", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.8985555171966553}]}, {"text": " Table 3: Perplexity and classification accuracy on the test set for the four word-based datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9800707697868347}]}, {"text": " Table 4. The FactorCell model has the  lowest perplexity and the highest accuracy for both  datasets. Again, the FactorCell model clearly im- proves on the ConcatCell as measured by classifi- cation accuracy. Consistent with our hypothesis,  adapting the softmax bias is not effective for these  small vocabulary character-based tasks. The Soft- maxBias model has small perplexity improvements  (< 1%) and low classification accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990647435188293}, {"text": "classification accuracies", "start_pos": 411, "end_pos": 436, "type": "METRIC", "confidence": 0.7307423949241638}]}, {"text": " Table 4: Perplexity and classification accuracies for the  EuroTwitter and GeoTwitter datasets.", "labels": [], "entities": [{"text": "EuroTwitter", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9920632243156433}, {"text": "GeoTwitter datasets", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.8695663511753082}]}, {"text": " Table 5: The top boosted words in the Softmax bias layer for different context settings in a FactorCell model.", "labels": [], "entities": []}]}