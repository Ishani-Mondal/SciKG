{"title": [{"text": "Conversation Modeling on Reddit Using a Graph-Structured LSTM", "labels": [], "entities": [{"text": "Conversation Modeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.804139256477356}]}], "abstractContent": [{"text": "This paper presents a novel approach for mod-eling threaded discussions on social media using a graph-structured bidirectional LSTM (long-short term memory) which represents both hierarchical and temporal conversation structure.", "labels": [], "entities": []}, {"text": "In experiments with a task of predicting popularity of comments in Reddit discussions, the proposed model outperforms a node-independent architecture for different sets of input features.", "labels": [], "entities": [{"text": "predicting popularity of comments in Reddit discussions", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.7608640789985657}]}, {"text": "Analyses show a benefit to the model over the full course of the discussion , improving detection in both early and late stages.", "labels": [], "entities": []}, {"text": "Further, the use of language cues with the bidirectional tree state updates helps with identifying controversial comments.", "labels": [], "entities": [{"text": "identifying controversial comments", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.8335892160733541}]}], "introductionContent": [{"text": "Social media provides a convenient and widely used platform for discussions among users.", "labels": [], "entities": []}, {"text": "When the comment-response links are preserved, those conversations can be represented in a tree structure where comments represent nodes, the root is the original post, and each new reply to a previous comment is added as a child of that comment.", "labels": [], "entities": []}, {"text": "Some examples of popular services with tree-like structures include Facebook, Reddit, Quora, and StackExchange.", "labels": [], "entities": []}, {"text": "shows an example conversation on Reddit, where bigger nodes indicate higher upvoting of a comment.", "labels": [], "entities": []}, {"text": "In services like Twitter, tweets and their retweets can also be viewed as forming a tree structure.", "labels": [], "entities": []}, {"text": "When time stamps are available with a contribution, the nodes of the tree can be ordered and annotated with that information.", "labels": [], "entities": []}, {"text": "The tree structure is useful for seeing how a discussion unfolds into different subtopics and showing differences in the level of activity in different branches of the discussion.", "labels": [], "entities": []}, {"text": "Predicting popularity of comments in social media is a task of growing interest.", "labels": [], "entities": [{"text": "Predicting popularity of comments in social media", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8934840389660427}]}, {"text": "Popularity has been defined in terms of the volume of the response, but when the social media platform has a mechanism for readers to like or dislike comments (or, upvote/downvote), then the difference in positive/negative votes provides a more informative score for popularity prediction.", "labels": [], "entities": [{"text": "popularity prediction", "start_pos": 267, "end_pos": 288, "type": "TASK", "confidence": 0.6772438436746597}]}, {"text": "This definition of popularity, which has also been called community endorsement, is the task of interest in our work on tree-structured modeling of discussions.", "labels": [], "entities": []}, {"text": "Previous studies found that the time when the comment/post was published has a big impact on its popularity ().", "labels": [], "entities": []}, {"text": "In addition, the number of immediate responses can be predictive of the popularity, but some comments with a high number of replies can be either controversial or have a highly negative score.", "labels": [], "entities": []}, {"text": "Language should be extremely important for distinguishing these cases.", "labels": [], "entities": []}, {"text": "Indeed, community style matching is shown to be correlated to comment popularity in Reddit (.", "labels": [], "entities": [{"text": "community style matching", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6289355258146921}]}, {"text": "However, learning useful language cues can be difficult due to the low frequency of these events and the dominance of time, topic and other factors.", "labels": [], "entities": []}, {"text": "Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (.", "labels": [], "entities": []}, {"text": "In this study, we have no such constraints, but attempt to use the tree structure to capture the flow of information in order to better model the context in which a comment is submitted, including both the history it responds to as well as the subsequent response to that comment.", "labels": [], "entities": []}, {"text": "To capture discussion dynamics, we introduce a novel approach to modeling the discussion using a bidirectional graph-structured LSTM, where each comment in the tree corresponds to a single LSTM unit.", "labels": [], "entities": []}, {"text": "In one direction, we capture the prior history of contributions leading up to anode, and in the other, we characterize the response to that comment.", "labels": [], "entities": []}, {"text": "Motivated by prior findings that both response structure and timing are important in predicting popularity, the LSTM units include both hierachical and temporal components to the update, which distinguishes this work from prior treestructured LSTM models.", "labels": [], "entities": []}, {"text": "We assess the utility of the model in experiments on popularity prediction with Reddit discussions, comparing to a neural network baseline that treats comments independently but leverages information about the graph context and timing of the comment.", "labels": [], "entities": [{"text": "popularity prediction", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7495622932910919}]}, {"text": "We analyze the results to show that the graph LSTM provides a useful summary representation of the language context of the comment.", "labels": [], "entities": []}, {"text": "As in, but unlike other work, our model makes use of the full discussion thread in predicting popularity.", "labels": [], "entities": []}, {"text": "While knowledge of the full discussion is only useful for posthoc analysis of past discussions, it is reasonable to consider initial responses to a comment, particularly given that many responses occur within minutes of someone posting a comment.", "labels": [], "entities": []}, {"text": "Comments are often popular because of witty analogies made, which requires knowledge of the world beyond what is captured in current models.", "labels": [], "entities": []}, {"text": "Responses to these comments, as well as to controversial comments, can improve popularity prediction.", "labels": [], "entities": [{"text": "popularity prediction", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7269304692745209}]}, {"text": "Responses of others clearly influence the likelihood of someone to like or dislike a comment, but also whether they even read a comment.", "labels": [], "entities": []}, {"text": "By introducing a forward-backward treestructured model, we provide a mechanism for leveraging early responses in predicting popularity, as well as a framework for better understanding the relative importance of these responses.", "labels": [], "entities": [{"text": "predicting popularity", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.8508919775485992}]}, {"text": "The main contributions of this paper include: a novel approach for representing tree-structured language processes (e.g., social media discussions) with LSTMs; evaluation of the model on the popularity prediction task using Reddit discussions; and analysis of the performance gains, particularly with respect to the role of language context.", "labels": [], "entities": [{"text": "popularity prediction task", "start_pos": 191, "end_pos": 217, "type": "TASK", "confidence": 0.7682365675767263}]}], "datasetContent": [{"text": "Reddit karma has a Zipfian distribution, highly skewed toward the low-karma comments.", "labels": [], "entities": []}, {"text": "Since the rare high karma comments are of greatest interest in popularity prediction, proposes a task of predicting quantized karma (using a nonlinear head-tail break rule for binning) with evaluation using a macro average of the F1 scores for predicting whether a comment exceeds each different level.", "labels": [], "entities": [{"text": "popularity prediction", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7707556784152985}, {"text": "F1", "start_pos": 230, "end_pos": 232, "type": "METRIC", "confidence": 0.997767448425293}]}, {"text": "Experiments reported here use this framework.", "labels": [], "entities": []}, {"text": "Specifically, all the comments with karma lower than 1 are assigned to level 0, and each subsequent level corresponds to karma less than or equal to the median karma in the rest of the comments based on the training data statistics.", "labels": [], "entities": []}, {"text": "Each subreddit has 8 quantized karma levels based on its karma distribution.", "labels": [], "entities": []}, {"text": "There are 7 binary subtasks (does the comment have karma at level j or higher for j = 1, . .", "labels": [], "entities": []}, {"text": ", 7), and the scoring metric is the macro average of F 1(j).", "labels": [], "entities": [{"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9791760444641113}]}, {"text": "For tuning hyperparameters and as a stopping criterion, we use a linearly weighted average of F1 scores to increase the weight on high karma comments, which gives slightly better performance for the high karma cases but has only a small effect on the macro average.", "labels": [], "entities": [{"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9941775798797607}]}], "tableCaptions": [{"text": " Table 2: Precision and recall of the pruning classifier and  percentage of comments pruned.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9941792488098145}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9977185726165771}]}, {"text": " Table 3: Average F1 score of karma level prediction for  node-independent (indep) vs. graph-structured (graph)  models with and without text features; interp corresponds  to an interpolation of the graph-structured model with- out text and the node-independent model with text; and  graph(f) corresponds to a graph-structured model con- tains forward direction only.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.980279803276062}, {"text": "karma level prediction", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8929178516070048}]}, {"text": " Table 4: Example comments and karma level predictions: reference, no text, graph(f), graph.", "labels": [], "entities": []}]}