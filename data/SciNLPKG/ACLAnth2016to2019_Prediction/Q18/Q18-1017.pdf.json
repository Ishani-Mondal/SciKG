{"title": [{"text": "Scheduled Multi-Task Learning: From Syntax to Translation", "labels": [], "entities": [{"text": "Translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.5651332139968872}]}], "abstractContent": [{"text": "Neural encoder-decoder models of machine translation have achieved impressive results, while learning linguistic knowledge of both the source and target languages in an implicit end-to-end manner.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.770668089389801}]}, {"text": "We propose a framework in which our model begins learning syntax and translation interleaved, gradually putting more focus on translation.", "labels": [], "entities": []}, {"text": "Using this approach, we achieve considerable improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a low-resource (WIT German to English) setup.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9790722131729126}, {"text": "WMT14 English to German", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.8436314761638641}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT)) has recently become the stateof-the-art approach to machine translation (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7812121013800303}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7760948836803436}]}, {"text": "One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input, rather than focusing on local information.", "labels": [], "entities": []}, {"text": "Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology ( or syntax (.", "labels": [], "entities": []}, {"text": "Nonetheless, as shown in  and, systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations.", "labels": [], "entities": []}, {"text": "Providing explicit linguistic information (Dyer et al., * Work carried out during summer internship at IBM Research.;) has proven to be beneficial, achieving higher results in language modeling and machine translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7269823551177979}, {"text": "machine translation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.8153988420963287}]}, {"text": "Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that lookalike.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8766466498374939}]}, {"text": "The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others.", "labels": [], "entities": []}, {"text": "For example, tasks like multiword expression detection and part-of-speech tagging have been found very useful for others like combinatory categorical grammar (CCG) parsing, chunking and super-sense tagging.", "labels": [], "entities": [{"text": "multiword expression detection", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7870791157086691}, {"text": "part-of-speech tagging", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.704221323132515}, {"text": "combinatory categorical grammar (CCG) parsing", "start_pos": 126, "end_pos": 171, "type": "TASK", "confidence": 0.6440271053995404}, {"text": "super-sense tagging", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7803590893745422}]}, {"text": "In order to perform accurate translations, we proceed by analogy to humans.", "labels": [], "entities": [{"text": "translations", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.8317535519599915}]}, {"text": "It is desirable to acquire a deep understanding of the languages; and, once this is acquired it is possible to learn how to translate gradually and with experience (including revisiting and re-learning some aspects of the languages).", "labels": [], "entities": []}, {"text": "We propose a similar strategy by introducing the concept of Scheduled Multi-Task Learning (Section 4) in which we propose to interleave the different tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose to learn the structure of language (through syntactic parsing and part-ofspeech tagging) with a multi-task learning strategy with the intentions of improving the performance of tasks like machine translation that use that structure and make generalizations.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.6913764327764511}, {"text": "machine translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.7618833780288696}]}, {"text": "We achieve considerable improvements in terms of BLEU score on a relatively large parallel corpus (WMT14 English to Ger-man) and a low-resource (WIT German to English) setup.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9776085019111633}, {"text": "WMT14 English to Ger-man", "start_pos": 99, "end_pos": 123, "type": "DATASET", "confidence": 0.8006351888179779}]}, {"text": "Our different scheduling strategies show interesting differences in performance both in the lowresource and standard setups.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of our models fora low-resource setting and a standard setting.", "labels": [], "entities": []}, {"text": "Translation performances are reported in case-sensitive BLEU ().", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.930230975151062}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9811820387840271}]}, {"text": "We report translation quality using tokenized 1 BLEU comparable with existing Neural Machine Translation papers.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9641838669776917}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9595469832420349}, {"text": "Neural Machine Translation", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7084745367368063}]}, {"text": "Our experiments are centered around the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9347907304763794}]}, {"text": "We aim to determine whether other syntactically oriented tasks can improve translation and vice versa.", "labels": [], "entities": []}, {"text": "Each task is presented in a sequence to sequence manner (as described in Section 3).", "labels": [], "entities": []}, {"text": "A single sequence to sequence with attention model is used to solve all tasks (all the parameters are shared between the different tasks).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scheduled Multi-Task learning results for IWSLT German to English translation.", "labels": [], "entities": [{"text": "IWSLT German to English translation", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.7153663396835327}]}, {"text": " Table 3: METEOR results for our best scoring sys- tems in comparison with BLEU scores. Fragmenta- tion refers to the fragmentation penalty.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9913902282714844}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9981561303138733}, {"text": "Fragmenta- tion", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.8901827534039816}]}, {"text": " Table 4: Scheduled Multi-Task learning results for WMT14 English to German translation.", "labels": [], "entities": [{"text": "WMT14 English to German translation", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.7094478011131287}]}, {"text": " Table 5: Example from our standard English to German translation (WMT).", "labels": [], "entities": []}]}