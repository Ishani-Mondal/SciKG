{"title": [{"text": "Attentive Convolution: Equipping CNNs with RNN-style Attention Mechanisms", "labels": [], "entities": []}], "abstractContent": [{"text": "In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms.", "labels": [], "entities": []}, {"text": "We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into con-volution).", "labels": [], "entities": []}, {"text": "Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text t x.", "labels": [], "entities": []}, {"text": "In this work, we propose an attentive convolution network, ATTCONV.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.6439788341522217}]}, {"text": "It extends the context scope of the convolution operation, deriving higher-level features fora word not only from local context, but also from information extracted from nonlocal context by the attention mechanism commonly used in RNNs.", "labels": [], "entities": []}, {"text": "This nonlocal context can come (i) from parts of the input text t x that are distant or (ii) from extra (i.e., external) contexts t y.", "labels": [], "entities": []}, {"text": "Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.7080479860305786}, {"text": "sentence representation learning", "start_pos": 195, "end_pos": 227, "type": "TASK", "confidence": 0.7824214200178782}]}, {"text": "In particular, attentive convo-lution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) has benefited greatly from the resurgence of deep neural networks (DNNs), thanks to their high performance with less need of engineered features.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7784600257873535}]}, {"text": "A DNN typically is composed of a stack of non-linear trans-formation layers, each generating a hidden representation for the input by projecting the output of a preceding layer into anew space.", "labels": [], "entities": []}, {"text": "To date, building a single and static representation to express an input across diverse problems is far from satisfactory.", "labels": [], "entities": []}, {"text": "Instead, it is preferable that the representation of the input vary in different application scenarios.", "labels": [], "entities": []}, {"text": "In response, attention mechanisms) have been proposed to dynamically focus on parts of the input that are expected to be more specific to the problem.", "labels": [], "entities": []}, {"text": "They are mostly implemented based on fine-grained alignments between two pieces of objects, each emitting a dynamic soft-selection to the components of the other, so that the selected elements dominate in the output hidden representation.", "labels": [], "entities": []}, {"text": "Attention-based DNNs have demonstrated good performance on many tasks.", "labels": [], "entities": []}, {"text": "Convolutional neural networks (CNNs;) and recurrent neural networks (RNNs; Elman, 1990) are two important types of DNNs.", "labels": [], "entities": []}, {"text": "Most work on attention has been done for RNNs.", "labels": [], "entities": [{"text": "attention", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.9800400733947754}]}, {"text": "Attention-based RNNs typically take three types of inputs to make a decision at the current step: (i) the current input state, (ii) a representation of local context (computed unidirectionally or bidirectionally; Rockt\u00e4schel et al.), and (iii) the attention-weighted sum of hidden states corresponding to nonlocal context (e.g., the hidden states of the encoder in neural machine translation; Bahdanau et al.).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 365, "end_pos": 391, "type": "TASK", "confidence": 0.7628040909767151}]}, {"text": "An important question, therefore, is whether CNNs can benefit from such an attention mechanism as well, and how.", "labels": [], "entities": []}, {"text": "This is our technical motivation.", "labels": [], "entities": []}, {"text": "Our second motivation is natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.662259578704834}]}, {"text": "In generic sentence modeling without extra context, CNNs learn sentence representations by composing word representations that are conditioned on a local context window.", "labels": [], "entities": [{"text": "generic sentence modeling", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6736101806163788}]}, {"text": "We believe that attentive convolution is needed 687 premise, modeled as context t y Plant cells have structures that animal cells lack.", "labels": [], "entities": []}, {"text": "0 Animal cells do not have cell walls.", "labels": [], "entities": []}, {"text": "1 The cell wall is not a freestanding structure.", "labels": [], "entities": []}, {"text": "0 Plant cells possess a cell wall, animals never.", "labels": [], "entities": []}, {"text": "1: Examples of four premises for the hypothesis t x = \"A cell wall is not present in animal cells.\" in SCITAIL data set.", "labels": [], "entities": [{"text": "SCITAIL data set", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.9218120177586874}]}, {"text": "Right column (hypothesis's label): \"1\" means true, \"0\" otherwise.", "labels": [], "entities": []}, {"text": "for some natural language understanding tasks that are essentially sentence modeling within contexts.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.6833035747210184}, {"text": "sentence modeling within contexts", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8040955290198326}]}, {"text": "Examples: textual entailment (is a hypothesis true given a premise as the single context?; Dagan et al.) and claim verification (is a claim correct given extracted evidence snippets from a text corpus as the context?; Thorne et al.).", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7348602712154388}]}, {"text": "Consider the SCITAIL () textual entailment examples in; here, the input text t x is the hypothesis and each premise is a context text t y . And consider the illustration of claim verification in; here, the input text t x is the claim and t y can consist of multiple pieces of context.", "labels": [], "entities": []}, {"text": "In both cases, we would like the representation oft x to be context-specific.", "labels": [], "entities": []}, {"text": "In this work, we propose attentive convolution networks, ATTCONV, to model a sentence (i.e., t x ) either in intra-context (where t y = t x ) or extracontext (where t y = t x and t y can have many pieces) scenarios.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.8731111288070679}]}, {"text": "In the intra-context case (sentiment analysis, for example), ATTCONV extends the local context window of standard CNNs to cover the entire input text t x . In the extra-context case, ATTCONV extends the local context window to cover accompanying contexts t y . For a convolution operation over a window int x such as (left context , word, right context ), we first compare the representation of word with all hidden states in the context t y to obtain an attentive context representation att context , then convolution filters derive a higher-level representation for word, denoted as word new , by integrating word with three pieces of context: left context , right context , and att context . We interpret this attentive convolution in two perspectives.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9245046675205231}]}, {"text": "hidden states of cross-text aligned terms, with local context.", "labels": [], "entities": []}, {"text": "We apply ATTCONV to three sentence modeling tasks with variable-size context: a largescale Yelp sentiment classification task () (intra-context, i.e., no additional context), SCITAIL textual entailment ( (single extra-context), and claim verification () (multiple extra-contexts).", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7151663452386856}, {"text": "largescale Yelp sentiment classification", "start_pos": 80, "end_pos": 120, "type": "TASK", "confidence": 0.5386544279754162}, {"text": "SCITAIL textual entailment", "start_pos": 175, "end_pos": 201, "type": "TASK", "confidence": 0.6238484382629395}]}, {"text": "ATTCONV outperforms competitive DNNs with and without attention and achieves state-of-the-art on the three tasks.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5406011343002319}]}, {"text": "Overall, we make the following contributions: \u2022 This is the first work that equips convolution filters with the attention mechanism commonly used in RNNs.", "labels": [], "entities": []}, {"text": "\u2022 We distinguish and build flexible modulesattention source, attention focus, and attention beneficiary-to greatly advance the expressivity of attention mechanisms in CNNs.", "labels": [], "entities": []}, {"text": "\u2022 ATTCONV provides anew way to broaden the originally constrained scope of filters in conventional CNNs.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.8110202550888062}]}, {"text": "Broader and richer context comes from either external context (i.e., t y ) or the sentence itself (i.e., t x ).", "labels": [], "entities": []}, {"text": "\u2022 ATTCONV shows its flexibility and effectiveness in sentence modeling with variablesize context.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.8036985993385315}, {"text": "sentence modeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7075574994087219}]}], "datasetContent": [{"text": "We evaluate ATTCONV on sentence modeling in three scenarios: (i) Zero-context, that is, intracontext; the same input sentence acts as t x as well as t y ; (ii) Single-context, that is, textual entailment-hypothesis modeling with a single premise as the extra-context; and (iii) Multiplecontext, namely, claim verification-claim modeling with multiple extra-contexts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: System comparison of sentiment analysis on  Yelp. Significant improvements over state of the art are  marked with  *  (test of equal proportions, p < 0.05).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9527914524078369}, {"text": "Yelp", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9471485018730164}]}, {"text": " Table 4: Statistics of SCITAIL data set.", "labels": [], "entities": [{"text": "SCITAIL data set", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.8468168179194132}]}, {"text": " Table 5: ATTCONV vs. baselines on SCITAIL.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8817775249481201}]}, {"text": " Table 6: Error cases of ATTCONV in SCITAIL. \". . . \": truncated text. \"G/P\": gold/predicted label.", "labels": [], "entities": [{"text": "ATTCONV", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.900704026222229}]}, {"text": " Table 7: Performance comparison on SNLI test. En- semble systems are not included.", "labels": [], "entities": [{"text": "SNLI test", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.793887734413147}]}, {"text": " Table 8: Statistics of claims in the FEVER data set.", "labels": [], "entities": [{"text": "FEVER data set", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8307221929232279}]}, {"text": " Table 9: Performance on dev and test of FEVER. In  \"gold evi.\" scenario, ALL SUBSET are the same.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9964259266853333}, {"text": "ALL", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9895601272583008}, {"text": "SUBSET", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.49160563945770264}]}]}