{"title": [{"text": "Whodunnit? Crime Drama as a Case for Natural Language Understanding", "labels": [], "entities": [{"text": "Whodunnit? Crime Drama", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.686243548989296}]}], "abstractContent": [{"text": "In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it.", "labels": [], "entities": [{"text": "CSI: Crime Scene Investigation", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.6338272094726562}, {"text": "approximating real-world natural language understanding", "start_pos": 138, "end_pos": 193, "type": "TASK", "confidence": 0.742402446269989}]}, {"text": "We propose to treat crime drama as anew inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed.", "labels": [], "entities": []}, {"text": "We develop anew dataset 1 based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data.", "labels": [], "entities": [{"text": "perpetrator identification", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7126141786575317}, {"text": "sequence labeling", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.6858072280883789}]}, {"text": "Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of neural networks in a variety of applications ( and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities.", "labels": [], "entities": [{"text": "machine understanding of natural language", "start_pos": 144, "end_pos": 185, "type": "TASK", "confidence": 0.8010253131389617}]}, {"text": "The problem has assumed several guises in the literature such as reading comprehension (, recognizing textual entailment (, and notably question answering based on text (), images (, or video (.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7100326120853424}, {"text": "question answering based on text", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.8319190263748169}]}, {"text": "In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.6557625929514567}]}, {"text": "For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being able to answer a question, or make inferences.", "labels": [], "entities": [{"text": "understanding", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9627180099487305}]}, {"text": "They are typically exposed to non-conversational texts or still images when focusing on the visual modality, ignoring the fact that understanding is situated in time and space and involves interactions between speakers.", "labels": [], "entities": []}, {"text": "In this work we relax some of these simplifications by advocating anew task for natural language understanding which is multi-modal, exhibits spoken conversation, and is incremental, i.e., unfolds sequentially in time.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.6653755108515421}]}, {"text": "Specifically, we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation can be used to approximate real-world natural language understanding and the complex inferences associated with it.", "labels": [], "entities": []}, {"text": "CSI revolves around a team of forensic investigators trained to solve criminal cases by scouring the crime scene, collecting irrefutable evidence, and finding the missing pieces that solve the mystery.", "labels": [], "entities": [{"text": "CSI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.828029453754425}]}, {"text": "Each episode poses the same \"whodunnit\" question and naturally provides the answer when the perpetrator is revealed.", "labels": [], "entities": []}, {"text": "Speculation about the identity of the perpetrator is an integral part of watching CSI and an incremental process: viewers revise their hypotheses based on new evidence gathered around the suspect/s or on new inferences which they make as the episode evolves.", "labels": [], "entities": [{"text": "Speculation about the identity of the perpetrator", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7911770003182548}]}, {"text": "We formalize the task of identifying the perpetrator in a crime series as a sequence labeling problem.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7028857469558716}]}, {"text": "Like humans watching an episode, we assume the model is presented with a sequence of inputs comprising information from different modalities such as text, video, or audio (see Section 4 for details).", "labels": [], "entities": []}, {"text": "The model predicts for each input whether the perpetrator is mentioned or not.", "labels": [], "entities": []}, {"text": "Our formulation generalizes over episodes and crime series.", "labels": [], "entities": []}, {"text": "It is not specific to the identity and number of persons committing the crime as well as the type of police drama under consideration.", "labels": [], "entities": []}, {"text": "Advantageously, it is incremental, we can track model predictions from the beginning of the episode and examine its behavior, e.g., how often it changes its mind, whether it is consistent in its predictions, and when the perpetrator is identified.", "labels": [], "entities": []}, {"text": "We develop anew dataset based on 39 CSI episodes which contains goldstandard perpetrator mentions as well as viewers' guesses about the perpetrator while each episode unfolds.", "labels": [], "entities": []}, {"text": "The sequential nature of the inference task lends itself naturally to recurrent network modeling.", "labels": [], "entities": [{"text": "recurrent network modeling", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7133434216181437}]}, {"text": "We adopt a generic architecture which combines a one-directional long-short term memory network) with a softmax output layer over binary labels indicating whether the perpetrator is mentioned.", "labels": [], "entities": []}, {"text": "Based on this architecture, we investigate the following questions: 1.", "labels": [], "entities": []}, {"text": "What type of knowledge is necessary for performing the perpetrator inference task?", "labels": [], "entities": []}, {"text": "Is the textual modality sufficient or do other modalities (i.e., visual and auditory input) also play a role?", "labels": [], "entities": []}, {"text": "2. What type of inference strategy is appropriate?", "labels": [], "entities": []}, {"text": "In other words, does access to past information matter for making accurate inferences?", "labels": [], "entities": []}, {"text": "3. To what extent does model behavior simulate humans?", "labels": [], "entities": []}, {"text": "Does performance improve overtime and how much of an episode does the model need to process in order to make accurate guesses?", "labels": [], "entities": [{"text": "overtime", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9910757541656494}]}, {"text": "Experimental results on our new dataset reveal that multi-modal representations are essential for the task at hand boding well with real-world natural language understanding.", "labels": [], "entities": []}, {"text": "We also show that an incremental inference strategy is key to guessing the perpetrator accurately although the model tends to be less consistent compared to humans.", "labels": [], "entities": []}, {"text": "In the remainder, we first discuss related work (Section 2), then present our dataset (Section 3) and formalize the modeling problem (Section 4).", "labels": [], "entities": []}, {"text": "We describe our experiments in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we make use of episodes of the U.S. TV show \"Crime Scene Investigation Las Vegas\" (henceforth CSI), one of the most successful crime series ever made.", "labels": [], "entities": [{"text": "Crime Scene Investigation Las Vegas\" (henceforth CSI)", "start_pos": 59, "end_pos": 112, "type": "TASK", "confidence": 0.5782553583383561}]}, {"text": "Fifteen seasons with a total of 337 episodes were produced over the course of fifteen years.", "labels": [], "entities": []}, {"text": "CSI is a procedural crime series, it follows a team of investigators employed by the Las Vegas Police Department as they collect and evaluate ev-: Statistics on the CSI data set.", "labels": [], "entities": [{"text": "CSI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8912999033927917}, {"text": "CSI data set", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9095090826352438}]}, {"text": "The type of crime was identified by our annotators via a multiple-choice questionnaire (which included the option \"other\").", "labels": [], "entities": []}, {"text": "Note that accidents may also involve perpetrators.", "labels": [], "entities": []}, {"text": "idence to solve murders, combining forensic police work with the investigation of suspects.", "labels": [], "entities": []}, {"text": "We paired official CSI videos (from seasons 1-5) with screenplays which we downloaded from a website hosting TV show transcripts.", "labels": [], "entities": []}, {"text": "Our dataset comprises 39 CSI episodes, each approximately 43 minutes long.", "labels": [], "entities": []}, {"text": "Episodes follow a regular plot, they begin with the display of a crime (typically without revealing the perpetrator) or a crime scene.", "labels": [], "entities": []}, {"text": "A team of five recurring police investigators attempt to reconstruct the crime and find the perpetrator.", "labels": [], "entities": []}, {"text": "During the investigation, multiple (innocent) suspects emerge, while the crime is often committed by a single person, who is eventually identified and convicted.", "labels": [], "entities": []}, {"text": "Some CSI episodes may feature two or more unrelated cases.", "labels": [], "entities": []}, {"text": "At the beginning of the episode the CSI team is split and each investigator is assigned a single case.", "labels": [], "entities": []}, {"text": "The episode then alternates between scenes covering each case, and the stories typically do not overlap.", "labels": [], "entities": []}, {"text": "displays a small excerpt from a CSI screenplay.", "labels": [], "entities": []}, {"text": "Readers unfamiliar with script writing conventions should note that scripts typically consist of scenes, which have headings indicating where the scene is shot (e.g., inside someone's house).", "labels": [], "entities": []}, {"text": "Character cues preface the lines the actors speak (see boldface in), and scene descriptions explain what the camera sees (see second and fifth panel in).", "labels": [], "entities": []}, {"text": "Screenplays were further synchronized with the video using closed captions which are time-stamped and provided in the form of subtitles as part of the video data.", "labels": [], "entities": []}, {"text": "The alignment between screenplay and closed captions is non-trivial, since the latter only contain dialogue, omitting speaker information or scene descriptions.", "labels": [], "entities": []}, {"text": "We first used dynamic time warping (DTW;) to approximately align closed captions with the dialogue in the scripts.", "labels": [], "entities": []}, {"text": "And then heuristically time-stamped remaining elements of the screenplay (e.g., scene descriptions), allocating them to time spans between spoken utterances.", "labels": [], "entities": []}, {"text": "shows some descriptive statistics on our dataset, featuring the number of cases per episode, its length (in terms of number of sentences), the type of crime, among other information.", "labels": [], "entities": []}, {"text": "The data was further annotated, with two goals in mind.", "labels": [], "entities": []}, {"text": "Firstly, in order to capture the characteristics of the human inference process, we recorded how participants incrementally update their beliefs about the perpetrator.", "labels": [], "entities": []}, {"text": "Secondly, we collected goldstandard labels indicating whether the perpetrator is mentioned.", "labels": [], "entities": []}, {"text": "Specifically, while a participant watches an episode, we record their guesses about who the perpetrator is (Section 3.1).", "labels": [], "entities": []}, {"text": "Once the episode is finished and the perpetrator is revealed, the same participant annotates entities in the screenplay referring to the true perpetrator (Section 3.2).", "labels": [], "entities": []}, {"text": "In our experiments we investigate what type of knowledge and strategy are necessary for identifying the perpetrator in a CSI episode.", "labels": [], "entities": [{"text": "identifying the perpetrator in a CSI episode", "start_pos": 88, "end_pos": 132, "type": "TASK", "confidence": 0.6999301569802421}]}, {"text": "In order to shed light on the former question we compare variants of our model with access to information from different modalities.", "labels": [], "entities": []}, {"text": "We examine different inference strategies by comparing the LSTM to three baselines.", "labels": [], "entities": []}, {"text": "The first one lacks the ability to flexibly fuse multi-modal information (a CRF), while the second one does not have a notion of history, classifying inputs independently (a multilayer perceptron).", "labels": [], "entities": []}, {"text": "Our third baseline is a rule-base system that neither uses multi-modal inputs nor has a notion of history.", "labels": [], "entities": []}, {"text": "We also compare the LSTM to humans watching CSI.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.6742682456970215}]}, {"text": "Before we report our results, we describe our setup and comparison models in more detail.", "labels": [], "entities": []}, {"text": "Our CSI data consists of 39 episodes giving rise to 59 cases (see).", "labels": [], "entities": [{"text": "CSI data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9195398092269897}]}, {"text": "The model was trained on 53 cases using cross-validation (five splits with 47/6 training/test cases).", "labels": [], "entities": []}, {"text": "The remaining 6 cases were used as truly held-out test data for final evaluation.", "labels": [], "entities": []}, {"text": "We trained our model using ADAM with stochastic gradient-descent and mini-batches of six episodes.", "labels": [], "entities": []}, {"text": "Weights were initialized randomly, except for word embeddings which were initialized with pre-trained 50-dimensional GloVe vectors, and fine-tuned during training.", "labels": [], "entities": []}, {"text": "We trained our networks for 100 epochs and report the best result obtained during training.", "labels": [], "entities": []}, {"text": "All results are averages of five runs of the network.", "labels": [], "entities": []}, {"text": "Parameters were optimized using two cross-validation splits.", "labels": [], "entities": []}, {"text": "The sentence convolution layer has three filters of sizes 3, 4, 5 each of which after convolution returns 75-dimensional output.", "labels": [], "entities": []}, {"text": "The final sentence representation x sis obtained by concatenating the output of the three filters and is of dimension 225.", "labels": [], "entities": []}, {"text": "We set the size of the hidden representation of merged crossmodal inputs x h to 300.", "labels": [], "entities": []}, {"text": "The LSTM has one layer with 128 nodes.", "labels": [], "entities": []}, {"text": "We set the learning rate to 0.001 and apply dropout with probability of 0.5.", "labels": [], "entities": []}, {"text": "We compared model output against the gold standard of perpetrator mentions which we collected as part of our annotation effort (second pass).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the CSI data set. The type of crime  was identified by our annotators via a multiple-choice  questionnaire (which included the option \"other\"). Note  that accidents may also involve perpetrators.", "labels": [], "entities": [{"text": "CSI data set", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9868848323822021}]}, {"text": " Table 2: Precision (pr) recall (re) and f1 for detecting the  minority class (perpetrator mentioned) for humans (bot- tom) and various systems. We report results with cross- validation (center) and on a held-out data set (right) using  the textual (T) visual (V), and auditory (A) modalities.", "labels": [], "entities": [{"text": "Precision (pr) recall (re)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8746711909770966}]}, {"text": " Table 3: Sentence ID in the script where  the LSTM and Humans predict the true  perpetrator for the first time. We show  the earliest (min) latest (max) and av- erage (avg) prediction time over 30 test  episodes (five cross-validation splits).", "labels": [], "entities": [{"text": "earliest (min) latest (max) and av- erage (avg) prediction time", "start_pos": 126, "end_pos": 189, "type": "METRIC", "confidence": 0.7047684543273028}]}, {"text": " Table 4: Excerpts of CSI episodes together with model predictions. Model confidence (p(l = 1)) is illustrated in red,  with darker shades corresponding to higher confidence. True perpetrator mentions are highlighted in blue. Top: a  conversation involving the true perpetrator. Bottom: a conversation with a suspect who is not the perpetrator.", "labels": [], "entities": []}]}