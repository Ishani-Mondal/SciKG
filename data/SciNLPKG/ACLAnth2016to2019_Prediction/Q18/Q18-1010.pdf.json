{"title": [{"text": "Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision", "labels": [], "entities": [{"text": "Learning Representations", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7047269642353058}]}], "abstractContent": [{"text": "Spatial understanding is crucial in many real-world problems, yet little progress has been made towards building representations that capture spatial knowledge.", "labels": [], "entities": [{"text": "Spatial understanding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9497661292552948}]}, {"text": "Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given object-relationship-object instances (e.g., \"cat under chair\") and a simple neural network model that learns the task from annotated images.", "labels": [], "entities": []}, {"text": "We show that the model succeeds in this task and, furthermore, that it is capable of predicting correct spatial arrangements for unseen objects if either CNN features or word embed-dings of the objects are provided.", "labels": [], "entities": []}, {"text": "The differences between visual and linguistic features are discussed.", "labels": [], "entities": []}, {"text": "Next, to evaluate the spatial representations learned in the previous task, we introduce a task and a dataset consisting in a set of crowdsourced human ratings of spatial similarity for object pairs.", "labels": [], "entities": []}, {"text": "We find that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if we update them when training the model that predicts spatial arrangements of objects.", "labels": [], "entities": []}, {"text": "Overall, this paper paves the way towards building distributed spatial representations , contributing to the understanding of spatial expressions in language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing spatial knowledge is instrumental in any task involving text-to-scene conversion such as robot understanding of natural language commands () or a number of robot navigation tasks.", "labels": [], "entities": [{"text": "text-to-scene conversion", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.749769777059555}, {"text": "robot understanding of natural language commands", "start_pos": 102, "end_pos": 150, "type": "TASK", "confidence": 0.8757466375827789}, {"text": "robot navigation", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7134886384010315}]}, {"text": "Despite recent advances in building specialized representations in domains such as sentiment analysis), semantic similarity/relatedness () or dependency parsing (), little progress has been made towards building distributed representations (a.k.a. embeddings) specialized in spatial knowledge.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9284696877002716}, {"text": "dependency parsing", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7158640772104263}]}, {"text": "Intuitively, one may reasonably expect that the more attributes two objects share (e.g., size, functionality, etc.), the more likely they are to exhibit similar spatial arrangements with respect to other objects.", "labels": [], "entities": []}, {"text": "Leveraging this intuition, we foresee that visual and linguistic representations can be spatially informative about unseen objects as they encode features/attributes of objects (.", "labels": [], "entities": []}, {"text": "For instance, without having ever seen an \"elephant\" before, but only a \"horse\", one would probably devise the \"elephant\" carrying the \"human\" than otherwise, just by considering their size attribute.", "labels": [], "entities": []}, {"text": "Similarly, one can infer that a \"tablet\" and a \"book\" will show similar spatial patterns (usually on a table, in someone's hands, etc.) although they barely show any visual resemblance-yet they are similar in size and functionality.", "labels": [], "entities": []}, {"text": "In this paper we systematically study how informative visual and linguistic features-in the form of convolutional neural network (CNN) features and word embeddings-are about the spatial behavior of objects.", "labels": [], "entities": []}, {"text": "An important goal of this work is to learn distributed representations specialized in spatial knowledge.", "labels": [], "entities": []}, {"text": "As a vehicle to learn spatial representations, we leverage the task of predicting the 2D spatial arrangement for two objects under a relationship expressed by either a preposition (e.g., \"below\" or \"on\") or a verb (e.g., \"riding\", \"jumping\", etc.).", "labels": [], "entities": []}, {"text": "For that, we make use of images where both objects are annotated with bounding boxes.", "labels": [], "entities": []}, {"text": "For instance, in an image depicting (horse, jumping, fence) we reasonably expect to find the \"horse\" above the \"fence\".", "labels": [], "entities": []}, {"text": "To learn the task, we employ a feed forward network that represents objects as continuous (spatial) features in an embedding layer and guides the learning with a distance-based supervision on the objects' coordinates.", "labels": [], "entities": []}, {"text": "We show that the model fares well in this task and that by informing it with either word embeddings or CNN features it is able to output accurate predictions about unseen objects, e.g., predicting the spatial arrangement of (man, riding, bike) without having ever been exposed to a \"bike\" before.", "labels": [], "entities": [{"text": "predicting the spatial arrangement of (man, riding, bike)", "start_pos": 186, "end_pos": 243, "type": "TASK", "confidence": 0.7772368490695953}]}, {"text": "This result suggests that the semantic and visual knowledge carried by the visual and linguistic features correlates to a certain extent with the spatial properties of words, thus providing predictive power for unseen objects.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the spatial representations learned in the previous task, we introduce a task consisting in a set of 1,016 human ratings of spatial similarity between object pairs.", "labels": [], "entities": []}, {"text": "It is thus desirable for spatial representations that \"spatially similar\" objects (i.e., objects that are arranged spatially similar inmost situations and relative to other objects) have similar embeddings.", "labels": [], "entities": []}, {"text": "In these ratings we show, first, that both CNN features and word embeddings are good predictors of human judgments, and second, that these vectors can be further specialized in spatial knowledge if we update them by backpropagation when learning the model in the task of predicting spatial arrangements of objects.", "labels": [], "entities": [{"text": "predicting spatial arrangements of objects", "start_pos": 271, "end_pos": 313, "type": "TASK", "confidence": 0.8671257138252259}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "2 we review related research.", "labels": [], "entities": []}, {"text": "3 we describe two spatial tasks and a model.", "labels": [], "entities": []}, {"text": "4 we describe our experimental setup.", "labels": [], "entities": []}, {"text": "5 we present and discuss our results.", "labels": [], "entities": []}, {"text": "6 we summarize our contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the experimental settings employed in the tasks and the model.", "labels": [], "entities": []}, {"text": "In the Prediction task, we consider the following subsets of Visual Genome (Sect.", "labels": [], "entities": [{"text": "Prediction task", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.92775958776474}]}, {"text": "4.1) for evaluation purposes: (i) Original set: a test split from the original data which contains instances unseen at training time.", "labels": [], "entities": []}, {"text": "That is, the test combinations (S, R, O) might have been seen at training time, yet in different instances (e.g., in different images).", "labels": [], "entities": []}, {"text": "This set contains a large number of noisy combinations such as (people, walk, funny) or (metal, white, chandelier).", "labels": [], "entities": []}, {"text": "(ii) Unseen Words set: We randomly select a list of 25 objects (e.g., \"wheel\", \"camera\", \"elephant\", etc.) among the 100 most frequent objects in Visual Genome.", "labels": [], "entities": []}, {"text": "We choose them among the most frequent ones in order to avoid meaningless objects such as \"gate 2\", \"number 40\" or \"2:10 pm\" which are not infrequent in Visual Genome.", "labels": [], "entities": []}, {"text": "We then take all instances of combinations that contain any of these words, yielding \u223c 123K instances.", "labels": [], "entities": []}, {"text": "For example, since \"cap\" is in our list, (girl, wears, cap) is included in this set.", "labels": [], "entities": []}, {"text": "When we enforce \"unseen\" conditions, we remove all these instances from the training set, using them only for testing.", "labels": [], "entities": []}, {"text": "Besides statistical significance, it is worth mentioning that the INI methods consistently outperformed both their RND counterparts and NU-1H in each of the 10 folds (not shown here) by a steadily large margin.", "labels": [], "entities": []}, {"text": "In fact, results are markedly stable across folds, in part due to the large size of the training and test sets (> 0.9M and > 120K examples respectively).", "labels": [], "entities": []}, {"text": "Additionally, to ensure that \"unseen\" results are not dependent on our particular list of objects, we repeated the experiment with two additional lists of randomly selected objects, obtaining very similar results.", "labels": [], "entities": []}, {"text": "Remarkably, the INI methods experience only a small performance drop under unseen conditions (Tab.", "labels": [], "entities": []}, {"text": "3, left) compared to when we allow them to train with these words (Tab.", "labels": [], "entities": []}, {"text": "3, right), and this difference might be partially attributed to the reduction of the training data under \"unseen\" conditions, whereat least 10% of the training data are left out.", "labels": [], "entities": []}, {"text": "Altogether, these results on unseen words show that semantic and visual similarities between concepts, as encoded byword and visual embeddings, can be leveraged by the model in order to predict spatial knowledge about unseen words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of our collected similarity ratings.", "labels": [], "entities": []}, {"text": " Table 2: Results in the Original test set (Sect. 4.2). Bold- face indicates best performance within the corresponding  block of methods (lang above, and vis below).", "labels": [], "entities": [{"text": "Original test set", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7128130296866099}, {"text": "Bold- face", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9217801094055176}]}, {"text": " Table 3: Results in the Unseen Words set (Sect. 4.2). Left table: results of enforcing \"unseen\" conditions, i.e., leaving  out all words of the Unseen Words set from our training data. Right table: the models are evaluated in the same  set but we allow them to train with the words from this set. Asterisks (  *  ) in an INI method indicate significantly  better performance (p < 0.05) than its RND counterpart (i.e., U-INI emb type is compared against U-RND, and NU- INI emb type against NU-RND). Diamonds ( ) indicate significantly better performance than NU-1H.", "labels": [], "entities": []}, {"text": " Table 4: Spearman correlations between model predic- tions and human ratings. Standard errors across folds  are shown for the methods that involve learning (second  block). Columns correspond to the word pairs for which  both embeddings (vis and lang) are available (V&L)  and those for which only the linguistic embeddings are  available (LANG). Asterisk (  *  ) indicates significant im- provement (p < 0.05) of a U-INI method of the second  block (U-INI vis and U-INI lang ) over its corresponding  untrained embedding (i.e., VGG-128 or GloVe respec- tively) from the first block.", "labels": [], "entities": [{"text": "LANG", "start_pos": 341, "end_pos": 345, "type": "METRIC", "confidence": 0.9254857301712036}, {"text": "VGG-128", "start_pos": 530, "end_pos": 537, "type": "DATASET", "confidence": 0.948961615562439}]}]}