{"title": [{"text": "Do latent tree learning models identify meaningful structure in sentences?", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work on the problem of latent tree learning has made it possible to train neu-ral networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time.", "labels": [], "entities": []}, {"text": "Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers.", "labels": [], "entities": [{"text": "sentence understanding tasks", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.8200188676516215}]}, {"text": "This paper aims to investigate what these latent tree learning models learn.", "labels": [], "entities": []}, {"text": "We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification , (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.7362285703420639}, {"text": "Penn Treebank (PTB) parses", "start_pos": 316, "end_pos": 342, "type": "DATASET", "confidence": 0.953552226225535}]}], "introductionContent": [{"text": "Tree-structured recursive neural networks ()-which build a vector representation fora sentence by incrementally computing representations for each node in its parse tree-have been proven to be effective at sentence understanding tasks like sentiment analysis, textual entailment (, and translation (.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.7269299775362015}, {"text": "sentiment analysis", "start_pos": 240, "end_pos": 258, "type": "TASK", "confidence": 0.9305089712142944}, {"text": "textual entailment", "start_pos": 260, "end_pos": 278, "type": "TASK", "confidence": 0.7592954933643341}, {"text": "translation", "start_pos": 286, "end_pos": 297, "type": "TASK", "confidence": 0.9761297106742859}]}, {"text": "Some variants of these models can also be trained to produce * Now at eBay, Inc.", "labels": [], "entities": []}, {"text": "parse trees that they then consume.", "labels": [], "entities": []}, {"text": "Recent work on latent tree learning () has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 299, "end_pos": 322, "type": "TASK", "confidence": 0.7243928760290146}]}, {"text": "These models are designed to learn grammars-strategies for assigning trees to sentences-that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB;).", "labels": [], "entities": [{"text": "sentence understanding task", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.8037823637326559}, {"text": "Penn Treebank (PTB;)", "start_pos": 235, "end_pos": 255, "type": "DATASET", "confidence": 0.9690053105354309}]}, {"text": "Latent tree learning models have shown striking success at sentence understanding, reliably performing better on sentiment analysis and textual entailment than do comparable TreeRNN models which use parses assigned by conventional parsers, and setting the state of the art among sentence-encoding models for textual entailment.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7917479574680328}, {"text": "sentiment analysis", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8284958302974701}]}, {"text": "However, none of the work in latent tree learning to date has included any substantial evaluation of the quality of the trees induced by these models, leaving open an important question which this paper aims to answer: Do these models owe their success to consistent, principled latent grammars?", "labels": [], "entities": []}, {"text": "If they do, these grammars maybe worthy objects of study for researchers in syntax and semantics.", "labels": [], "entities": []}, {"text": "If they do not, understanding why the models succeed without such syntax could lead to new insights into the use of TreeRNNs and into sentence understanding more broadly.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7712775766849518}]}, {"text": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has long been clear that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units, or constituents.", "labels": [], "entities": [{"text": "language understanding and generation", "start_pos": 124, "end_pos": 161, "type": "TASK", "confidence": 0.7600532472133636}]}, {"text": "This is well illustrated by structurally ambiguous sentences like the one below, repeated from Sag (1991) a.o.:", "labels": [], "entities": [{"text": "Sag (1991) a.o", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.6684225499629974}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test set results. Our implementations of  SPINN and RL-SPINN differ only in how they are  trained to parse, and our implementations of SPINN- NC and ST-Gumbel also differ only in how they are  trained to parse. SPINN-PI-NT includes no tracking  or parsing component and instead uses externally  provided Stanford Parser trees, random trees (Rand.)  or balanced trees (Bal.), as described in Section 3.", "labels": [], "entities": []}, {"text": " Table 2: Dev. Acc. shows MultiNLI development set  accuracy across the five runs (expressed as mean,  standard deviation, and maximum). Self F1 shows  how well each of the five models agrees in its pars- ing decisions with the other four.", "labels": [], "entities": [{"text": "Dev. Acc.", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7357410490512848}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.6644523739814758}]}, {"text": " Table 3: Results on the full Wall Street Journal section of the Penn Treebank. The F1 columns represent  overall unlabeled binary F1. The Accuracy columns represent the fraction of ground truth constituents of a  given type that correspond to constituents in the model parses. Italics mark results that are worse than the  random baseline.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 30, "end_pos": 78, "type": "DATASET", "confidence": 0.9460698515176773}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9894847273826599}, {"text": "Accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9950836896896362}]}, {"text": " Table 4: F1 wrt. shows F1 scores on the MultiNLI development set with respect to strictly right-and left- branching trees and with respect to the Stanford Parser trees supplied with the corpus. The F1 results from", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9013974070549011}, {"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9974006414413452}, {"text": "MultiNLI development set", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9659176270167033}, {"text": "F1", "start_pos": 199, "end_pos": 201, "type": "METRIC", "confidence": 0.9859963059425354}]}]}