{"title": [{"text": "Finding Convincing Arguments Using Scalable Bayesian Preference Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification.", "labels": [], "entities": [{"text": "pairwise classification", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.6248959451913834}]}, {"text": "Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments.", "labels": [], "entities": []}, {"text": "One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning.", "labels": [], "entities": [{"text": "Gaussian process (GP) preference learning", "start_pos": 103, "end_pos": 144, "type": "TASK", "confidence": 0.655028624194009}]}, {"text": "We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data.", "labels": [], "entities": [{"text": "predict argument convincingness", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8071827491124471}]}, {"text": "We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains.", "labels": [], "entities": []}, {"text": "While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.", "labels": [], "entities": [{"text": "predicting argument convincingness", "start_pos": 165, "end_pos": 199, "type": "TASK", "confidence": 0.8089622656504313}]}], "introductionContent": [{"text": "Arguments are intended to persuade the audience of a particular point of view and are an important way for humans to reason about controversial topics ().", "labels": [], "entities": []}, {"text": "The amount of argumentative text on any chosen subject can, howTopic: \"William Farquhar ought to be honoured as the rightful founder of Singapore\".", "labels": [], "entities": []}, {"text": "Stance: \"No, it is Raffles!\"", "labels": [], "entities": []}, {"text": "Argument 1: HE HAS A BOSS(RAFFLES) HE HAS TO FOLLOW HIM AND NOT GO ABOUT DOING ANYTHING ELSE...", "labels": [], "entities": [{"text": "HE HAS A BOSS", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.712868332862854}, {"text": "RAFFLES", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9016320705413818}, {"text": "HE HAS TO FOLLOW HIM", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.6901286363601684}, {"text": "NOT", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9759635925292969}, {"text": "GO", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.8598308563232422}, {"text": "ABOUT DOING ANYTHING ELSE", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.7817344218492508}]}, {"text": "Argument 2: Raffles conceived a town plan to remodel Singapore into a modern city.", "labels": [], "entities": []}, {"text": "The plan consisted of separate areas for different...", "labels": [], "entities": []}, {"text": "Crowdsourced labels: {2 1, 1 2, 2 1} ever, overwhelm a reader.", "labels": [], "entities": []}, {"text": "Consider the scale of historical text archives or social media platforms with millions of users.", "labels": [], "entities": []}, {"text": "Automated methods could help readers overcome this challenge by identifying highquality, persuasive arguments from both sides of a debate.", "labels": [], "entities": []}, {"text": "Theoretical approaches for assessing argument quality have proved difficult to apply to everyday arguments (.", "labels": [], "entities": []}, {"text": "Empirical machine learning approaches instead train models using example judgments of arguments, such as those shown in.", "labels": [], "entities": []}, {"text": "Previous approaches to obtaining such judgments include training annotators to assign scores from 1-6 (, asking annotators for simple binary or three-class categories (, and aggregating binary votes from multiple people ().", "labels": [], "entities": []}, {"text": "However, these approaches are limited by the cost of training annotators, a highly restricted set of categories, or the need for multiple annotators per document.", "labels": [], "entities": []}, {"text": "An alternative way to judge arguments is to compare them against one another.", "labels": [], "entities": []}, {"text": "When comparing the arguments in, we may judge that argument 1 is less convincing due to its writing style, whereas argument 2 presents evidence in the form of historical events.", "labels": [], "entities": []}, {"text": "Pairwise comparisons such as this are known to place less cognitive burden on human annotators than choosing a numerical rating and allow fine-grained sorting of items that is not possible with categorical labels.", "labels": [], "entities": []}, {"text": "Unlike numerical ratings, pairwise comparisons are not affected by different annotators' biases toward high, low or middling values, or an individual's bias changing overtime.", "labels": [], "entities": []}, {"text": "In practice, we face a data acquisition bottleneck when encountering new domains or audiences.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7332485914230347}]}, {"text": "For example, neural network methods typically require datasets with many thousands of hand-labeled examples to perform well ().", "labels": [], "entities": []}, {"text": "One solution is to employ multiple non-specialist annotators at low cost (crowdsourcing), but this requires quality control techniques to account for errors.", "labels": [], "entities": []}, {"text": "Another source of data are the actions of users of a software application, which can be interpreted as pairwise judgments).", "labels": [], "entities": []}, {"text": "For example, when a user clicks on an argument in a list it can be interpreted as a preference for the selected argument over more highly-ranked arguments.", "labels": [], "entities": []}, {"text": "However, the resulting pairwise labels are likely to be a very noisy indication of preference.", "labels": [], "entities": []}, {"text": "In this paper, we develop a Bayesian approach to learn from noisy pairwise preferences based on Gaussian process preference learning (GPPL) (Chu and).", "labels": [], "entities": []}, {"text": "We model argument convincingness as a function of textual features, including word embeddings, and develop an inference method for GPPL that scales to realistic dataset sizes using stochastic variational inference (SVI)).", "labels": [], "entities": []}, {"text": "Using datasets provided by, we show that our method outperforms the previous state-of-the-art for ranking arguments by convincingness and identifying the most convincing argument in a pair.", "labels": [], "entities": []}, {"text": "Further experiments show that our approach is particularly advantageous with small, noisy datasets, and in an active learning set-up.", "labels": [], "entities": []}, {"text": "Our software is publicly available . The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work on argumentation, then Section 3 motivates the use of Bayesian methods by discussing their successful applications in NLP.", "labels": [], "entities": []}, {"text": "In Section 4, we review preference learning methods and then Section 5 describes our scalable Gaussian process-based approach.", "labels": [], "entities": []}, {"text": "Section 6 presents our evaluation, comparing our method to the state-of-the art and testing with noisy data and active learning.", "labels": [], "entities": []}, {"text": "Finally, we present conclusions and future work.", "labels": [], "entities": []}, {"text": "demonstrated that an audience's personality and prior stance affect an argument's persuasiveness, but they were unable to predict belief change to a high degree of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9931294322013855}]}, {"text": "Related work has shown how persuasiveness is also affected by the sequence of arguments in a discussion (, but this work focuses on predicting salience of an argument given the state of the debate, rather than the qualities of arguments.", "labels": [], "entities": []}, {"text": "recently showed that relative comparisons of argument convincingness correlate with theory-derived quality ratings.", "labels": [], "entities": []}, {"text": "Habernal and Gurevych (2016) established datasets containing crowdsourced pairwise judgments of convincingness for arguments taken from online discussions.", "labels": [], "entities": []}, {"text": "Errors in the crowdsourced data were handled by determining gold labels using the MACE algorithm ().", "labels": [], "entities": []}, {"text": "The gold labels were then used to train SVM and bi-directional long short-term memory (BiLSTM) classifiers to predict pairwise labels for new arguments.", "labels": [], "entities": []}, {"text": "The gold labels were also used to construct a directed graph of convincingness, which was input to PageRank to produce scores for each argument.", "labels": [], "entities": [{"text": "PageRank", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9439930319786072}]}, {"text": "These scores were then used to train SVM and BiLSTM regression models.", "labels": [], "entities": []}, {"text": "A drawback of such pipeline approaches is that they are prone to error propagation, and consensus algorithms, such as MACE, require multiple crowdsourced labels for each argument pair, which increases annotation costs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first use toy datasets to illustrate the behavior of several different methods (described below).", "labels": [], "entities": []}, {"text": "Then, we analyze the scalability and performance of our approach on datasets provided by, which contain pairwise labels for arguments taken from online discussion forums.", "labels": [], "entities": []}, {"text": "The labels can have a value of 0, meaning the annotator found the second argument in the pair more convincing, 1 if the annotator was undecided, or 2 if the first argument was more convincing.", "labels": [], "entities": []}, {"text": "To test different scenarios, different pre-processing steps were used to produce the three UKPConvArg* datasets shown in.", "labels": [], "entities": [{"text": "UKPConvArg* datasets", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.9540712436040243}]}, {"text": "UKPConvArgStrict and UKPConvArgRank were cleaned to remove disagreements between annotators, hence can be considered to be noise-free.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9863866567611694}, {"text": "UKPConvArgRank", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.971031129360199}]}, {"text": "UKPConvArgCrowdSample is used to evaluate performance with noisy crowdsourced data including conflicts and undecided labels, and to test the suitability of our method for active learning to address the cold-start problem in domains with no labeled data.", "labels": [], "entities": [{"text": "UKPConvArgCrowdSample", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9799121022224426}]}, {"text": "For these datasets, we perform 32-fold cross validation, where each fold corresponds to one of 16 controversial topics, and one of two stances for that topic.", "labels": [], "entities": []}, {"text": "To illustrate some key differences between GPPL, SVM and PageRank, we simulate four scenarios, each of which contains arguments labeled arg0 to arg4.", "labels": [], "entities": []}, {"text": "In each scenario, we generate a set of pairwise preference labels according to the graphs shown in.", "labels": [], "entities": []}, {"text": "Each scenario is repeated 25 times: in each repeat, we select arguments at random from one fold of UKPConvArgStrict then associate the mean GloVe embeddings for these arguments with the labels arg0 to arg4.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9878537654876709}]}, {"text": "We train GPPL, PageRank and the SVM classifier on the preference pairs shown in each graph and predict ranks and pairwise labels for arguments arg0 to arg4.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9129108190536499}]}, {"text": "In the \"no cycle\" scenario, arg0 is preferred to both arg1 and arg2, which is reflected in the scores predicted by PageRank and GPPL in.", "labels": [], "entities": [{"text": "arg0", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9892557859420776}, {"text": "PageRank", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9645131230354309}, {"text": "GPPL", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9334994554519653}]}, {"text": "However, arg3 and arg4 are not connected to the rest of the graph, and PageRank and GPPL score them differently.", "labels": [], "entities": [{"text": "arg3", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9242225289344788}, {"text": "PageRank", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9306061863899231}, {"text": "GPPL", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8793451189994812}]}, {"text": "shows how GPPL provides less confident classifications for pairs that were not yet observed, e.g. arg2 arg4, in contrast with the dis- .72 .73 .74 .75 .76 .77 .78 .79 . .60 .62 .64 .66 .68 .70 .72 . crete classifications of the SVM.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8929509520530701}]}, {"text": "The next scenario shows a \"single cycle\" in the preference graph.", "labels": [], "entities": []}, {"text": "Both PageRank and GPPL produce equal values for the arguments in the cycle (arg0, arg1, arg2).", "labels": [], "entities": [{"text": "PageRank", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.9378978610038757}, {"text": "GPPL", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.9065381288528442}]}, {"text": "PageRank assigns lower scores to both arg3 and arg4 than the arguments in the cycle, while GPPL more intuitively gives a higher score to arg3, which was preferred to arg4.", "labels": [], "entities": [{"text": "PageRank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9032295346260071}, {"text": "GPPL", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9328107237815857}]}, {"text": "SVM predicts that arg0 and arg1 are preferred over arg3, although arg0 and arg1 are in a cycle so there is no reason to prefer them.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7729475498199463}]}, {"text": "GPPL, in contrast, weakly predicts that arg3 is preferred.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9378572702407837}, {"text": "arg3", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.988068163394928}]}, {"text": "The \"double cycle\" scenario contains two paths from arg2 to arg0, via arg1 or arg3, and one conflicting preference arg2 arg0.", "labels": [], "entities": []}, {"text": "GPPL scores the arguments as if the single conflicting preference, arg2 arg0, is less important than the two parallel paths from arg2 to arg0.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9033938646316528}]}, {"text": "In contrast, PageRank gives high scores to both arg0 and arg2.", "labels": [], "entities": [{"text": "PageRank", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9341238737106323}, {"text": "arg0", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9920132756233215}, {"text": "arg2", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8925194144248962}]}, {"text": "The classifications by GPPL and SVM are similar, but GPPL produces more uncertain predictions than in the first scenario due to the conflict.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9361207485198975}, {"text": "SVM", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8354752063751221}, {"text": "GPPL", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8512113690376282}]}, {"text": "Finally, shows the addition of 9 undecided labels to the \"no cycle\" scenario, indicated by undirected edges in, to simulate multiple annotators viewing the pair without being able to choose the most convincing argument.", "labels": [], "entities": []}, {"text": "The SVM and PageRank are unaffected as they cannot be trained using the undecided labels.", "labels": [], "entities": [{"text": "SVM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8879656791687012}, {"text": "PageRank", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9101972579956055}]}, {"text": "However, the GPPL classifications are less confident and the difference in GPPL scores between arg0 and the other arguments decreases, since GPPL gives the edge from arg2 to arg0 less weight.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.6562227010726929}, {"text": "arg0", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9673352837562561}, {"text": "GPPL", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.7909484505653381}]}, {"text": "In conclusion, GPPL appears to resolve conflicts in the preference graphs more intuitively than PageRank, which was designed to rank web pages by importance rather than preference.", "labels": [], "entities": []}, {"text": "In contrast to SVM, GPPL is able to account for cycles and undecided labels to soften its predictions.", "labels": [], "entities": []}, {"text": "We analyze empirically the scalability of the proposed SVI method for GPPL using the UKPConvArgStrict dataset.", "labels": [], "entities": [{"text": "UKPConvArgStrict dataset", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.9965400993824005}]}, {"text": "shows the effect of varying the number of inducing points, M , on the overall runtime and accuracy of the method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9995373487472534}]}, {"text": "The accuracy increases quickly with M , and flattens out, suggesting there is little benefit to increasing M further on this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995998740196228}, {"text": "M", "start_pos": 36, "end_pos": 37, "type": "METRIC", "confidence": 0.9573798179626465}, {"text": "M", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9974084496498108}]}, {"text": "The runtimes increase with M , and are much longer with 32, 310 features than with 300 features.", "labels": [], "entities": [{"text": "M", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.8530665040016174}]}, {"text": "The difference is due to the cost of computing the kernel, which is linear in M , With only 300 features, the runtime appears polynomial, reflecting the O(M 3 ) term in the inference procedure.", "labels": [], "entities": []}, {"text": "We tested GPPL with both the SVI algorithm, with M = 100 and P n = 200, and variational inference without inducing points or stochastic updates (labeled \"no SVI\") with different sizes of training dataset subsampled from UKPConvArgStrict.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8083397746086121}, {"text": "UKPConvArgStrict", "start_pos": 220, "end_pos": 236, "type": "DATASET", "confidence": 0.9925664663314819}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For GPPL with SVI, the runtime increases very little with dataset size, while the runtime with \"no SVI\" increases polynomially with training set size (both N and P ).", "labels": [], "entities": []}, {"text": "At N = 100, the number of inducing points is M = N but the SVI algorithm is still faster due to the stochastic updates with P n = 200 P pairs.", "labels": [], "entities": []}, {"text": "shows the effect of the number of features, D, on runtimes.", "labels": [], "entities": []}, {"text": "Runtimes for GPPL increase by a large amount with D = 32, 310, because the SVI method computes the kernel matrix, K mm , with computational complexity O(D).", "labels": [], "entities": [{"text": "D", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9912432432174683}]}, {"text": "While Dis small, other costs dominate.", "labels": [], "entities": [{"text": "Dis", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9717075228691101}]}, {"text": "We show runtimes using the MLII optimization procedure with GPPL in.", "labels": [], "entities": [{"text": "MLII optimization", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6833420544862747}, {"text": "GPPL", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9400941729545593}]}, {"text": "Owing to the long computation times required, the procedure was limited to a maximum of 25 iterations and did not terminate in fewer than 25 in any of the test runs.", "labels": [], "entities": []}, {"text": "This creates a similar pattern to (approximately multiples of 50).", "labels": [], "entities": []}, {"text": "We include runtimes for SVM and BiLSTM in Figures 5a and 5c to show their runtime patterns, but note that the runtimes reflect differences in implementations and system hardware.", "labels": [], "entities": []}, {"text": "Both SVM and GPPL were run on an Intel i7 quad-core desktop.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8731335401535034}]}, {"text": "For SVM we used LibSVM version 3.2, which could be sped up if probability estimates were not required.", "labels": [], "entities": []}, {"text": "BiLSTM was run with Theano 0.7 2 on an Nvidia Tesla P100 GPU.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8751342296600342}, {"text": "Theano 0.7", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9802677631378174}, {"text": "Nvidia Tesla P100 GPU", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.8765606135129929}]}, {"text": "We can see in that the runtime for BiLSTM does not appear to increase due to the number of features, while that of SVM increases sharply with 32, 310 features.", "labels": [], "entities": []}, {"text": "In, we observe the SVM runtimes increase polynomially with training set size.", "labels": [], "entities": []}, {"text": "We compare classification performance on UKPConvArgStrict and ranking performance on UKPConvArgRank.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.986375093460083}, {"text": "UKPConvArgRank", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.9922106266021729}]}, {"text": "The results in show that when using ling features, GPPL produces similar accuracy and improves the area under the ROC curve (AUC) by .02 and cross entropy error (CEE) by .01.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.6656545400619507}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993269443511963}, {"text": "ROC curve (AUC)", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.9641728162765503}, {"text": "cross entropy error (CEE)", "start_pos": 141, "end_pos": 166, "type": "METRIC", "confidence": 0.9169787267843882}]}, {"text": "AUC quantifies how well the predicted probabilities separate the classes, while CEE quantifies the usefulness of the probabilities output by each method.", "labels": [], "entities": [{"text": "AUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4412398636341095}, {"text": "CEE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.97519850730896}]}, {"text": "Much larger improvements can be seen in the ranking metrics.", "labels": [], "entities": []}, {"text": "When GPPL is run with GloVe, it performs worse than BiLSTM for classification but improves the ranking metrics.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.8949148654937744}]}, {"text": "Using a combination of features improves all methods, suggesting that embeddings and linguistic features contain complementary information.", "labels": [], "entities": []}, {"text": "This improvement is statistically significant (p .01 using two-tailed Wilcoxon signed-rank test) for SVM with all metrics except accuracy, for BiLSTM with AUC only, and for GPPL medi. with Pearson correlation only.", "labels": [], "entities": [{"text": "SVM", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.8355149626731873}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.99904865026474}, {"text": "AUC", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.810089111328125}, {"text": "GPPL medi.", "start_pos": 173, "end_pos": 183, "type": "DATASET", "confidence": 0.8607231378555298}, {"text": "Pearson correlation", "start_pos": 189, "end_pos": 208, "type": "METRIC", "confidence": 0.9310042858123779}]}, {"text": "Optimizing the length-scale using MLII improves classification accuracy by 1% over the median heuristic, and significantly improves accuracy (p = .043) and AUC (p = .013) over the previous stateof-the-art, SVM ling.", "labels": [], "entities": [{"text": "classification", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9290947914123535}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.958322286605835}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.999291181564331}, {"text": "AUC", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9990954399108887}]}, {"text": "However, the cost of these im-  provements is that each fold required around 2 hours to compute instead of approximately 10 minutes on the same machine (Intel i7 quad-core desktop) using the median heuristic.", "labels": [], "entities": []}, {"text": "The differences in all ranking metrics between GPPL opt. and SVM ling + GloVe are statistically significant, with p = .029 for Pearson's rand p .01 for both Spearman's \u03c1 and Kendall's \u03c4 . GPC produces the best results on the classification task (p < .01 for all metrics compared to all other methods), indicating the benefits of a Bayesian approach over SVM and BiLSTM.", "labels": [], "entities": [{"text": "GPC", "start_pos": 188, "end_pos": 191, "type": "DATASET", "confidence": 0.9376028776168823}]}, {"text": "However, unlike GPPL, GPC cannot be used to rank the arguments.", "labels": [], "entities": [{"text": "GPC", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.8587722182273865}]}, {"text": "The results also show that PL+SVR does not reach the same performance as GPPL, suggesting that GPPL may benefit from the Bayesian integration of a GP with the preference likelihood.", "labels": [], "entities": []}, {"text": "We use UKPConvArgCrowdSample to introduce noisy data and conflicting pairwise labels to both the classification and regression tasks, to test the hypothesis that GPPL would best handle unreliable crowdsourced data.", "labels": [], "entities": [{"text": "UKPConvArgCrowdSample", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.9827865362167358}]}, {"text": "The evaluation uses gold labels from UKPConvArgStrict and UKPConvArgRank for the test set.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.9822521209716797}, {"text": "UKPConvArgRank", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9786730408668518}]}, {"text": "The results in show that all methods perform worse compared to Experiment 3 due to the presence of errors in the pairwise labels.", "labels": [], "entities": []}, {"text": "Here, GPPL produces the best classification accuracy and cross-entropy error (significant with p .01 compared to all other methods except accuracy compared to GP+SVR, for which p = .045), while GPC has the highest AUC (p .01 compared to all except GP+SVR, which was not significant).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9680766463279724}, {"text": "cross-entropy error", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.6925526261329651}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9974278807640076}, {"text": "GP+SVR", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.855461319287618}, {"text": "GPC", "start_pos": 194, "end_pos": 197, "type": "DATASET", "confidence": 0.9360412359237671}, {"text": "AUC", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9916286468505859}, {"text": "GP+SVR", "start_pos": 248, "end_pos": 254, "type": "DATASET", "confidence": 0.8897777795791626}]}, {"text": "Compared to UKPConvArgStrict, the classification performance of GPC, SVM and BiLSTM decreased more than that of GPPL.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.9475467801094055}, {"text": "GPC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.916828453540802}, {"text": "BiLSTM", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.8722688555717468}, {"text": "GPPL", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9303954243659973}]}, {"text": "These methods lack a mechanism to resolve conflicts in the preference graph, unlike GPPL and PL+SVR, which handle conflicts through the preference likelihood.", "labels": [], "entities": []}, {"text": "PL+SVR again performs worse than GPPL on classification metrics, although its ranking performance is comparable.", "labels": [], "entities": [{"text": "PL+SVR", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.5184618135293325}]}, {"text": "For ranking, GPPL again outperforms SVM and BiLSTM in all metrics (significant with p .01 in all cases except for SVM with Pearson's correlation).", "labels": [], "entities": [{"text": "GPPL", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.7822306752204895}, {"text": "BiLSTM", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9545519351959229}, {"text": "Pearson's correlation", "start_pos": 123, "end_pos": 144, "type": "METRIC", "confidence": 0.583542118469874}]}, {"text": "In this experiment, we hypothesized that GPPL provides more meaningful confidence estimates than SVM or BiLSTM, which can be used to facilitate active learning in scenarios where labeled training data is expensive or initially unavailable.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.6839687824249268}, {"text": "BiLSTM", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.7978707551956177}]}, {"text": "To test this hypothesis, we simulate an active learning scenario, in which an agent iteratively learns a model for each fold.", "labels": [], "entities": []}, {"text": "Initially, 2 pairs are chosen at random, then used to train the classifier.", "labels": [], "entities": []}, {"text": "The agent then performs uncertainty sampling to select the 2 pairs with the least confident classifications.", "labels": [], "entities": []}, {"text": "The labels for these pairs are then added to the training set and used to re-train the model.", "labels": [], "entities": []}, {"text": "We repeated the process until 400 labels had been sampled.", "labels": [], "entities": []}, {"text": "The result is plotted in, showing that GPPL reaches a mean accuracy of 70% with only 100 labels, while SVM and BiLSTM do not reach the same performance given 400 labels.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6812731623649597}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9769838452339172}, {"text": "BiLSTM", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.5411786437034607}]}, {"text": "After 100 labels, the performance of BiLSTM decreases.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.49605345726013184}]}, {"text": "It has previously been shown) that uncertainty sampling sometimes causes accuracy to decrease.", "labels": [], "entities": [{"text": "uncertainty sampling", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7760838568210602}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9989601373672485}]}, {"text": "If the model overfits to a small dataset, it can mis-classify some data points with high confidence so that they are not selected and corrected by uncertainty sampling.", "labels": [], "entities": []}, {"text": "The larger number of parameters in the BiLSTM may make it may more prone to overfitting with small datasets than SVM or GPPL.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.7764108777046204}, {"text": "GPPL", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.9013634920120239}]}, {"text": "The Bayesian approach of GPPL aims to further reduce overfitting by accounting for parameter uncertainty.", "labels": [], "entities": []}, {"text": "The results suggest that GPPL maybe more suitable than the alternatives in cold-start scenarios with small amounts of labeled data.", "labels": [], "entities": [{"text": "GPPL", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.782470703125}]}], "tableCaptions": [{"text": " Table 1: Summary of datasets, showing the different steps used to produce each Internet argument dataset.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison on UKPConvArgStrict and UKPConvArgRank datasets.", "labels": [], "entities": [{"text": "UKPConvArgStrict", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9833394289016724}, {"text": "UKPConvArgRank datasets", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.9764646291732788}]}, {"text": " Table 3: Performance comparison on UKPConvA- rgCrowdSample using ling+GloVe features.", "labels": [], "entities": [{"text": "UKPConvA- rgCrowdSample", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.903672436873118}]}, {"text": " Table 4: Normalized length-scales for linguistic features  learned using MLII. Shows mean values over folds with  > 3% improvement. Includes all values < 0.99, except  for POS n-grams (only smallest 5 of 18 shown).", "labels": [], "entities": []}]}