{"title": [{"text": "Joint Semantic Synthesis and Morphological Analysis of the Derived Word", "labels": [], "entities": [{"text": "Morphological Analysis of the Derived Word", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.7671104868253072}]}], "abstractContent": [{"text": "Much like sentences are composed of words, words themselves are composed of smaller units.", "labels": [], "entities": []}, {"text": "For example, the English word questionably can be analyzed as question+able+ly.", "labels": [], "entities": []}, {"text": "However, this structural decomposition of the word does not directly give us a semantic representation of the word's meaning.", "labels": [], "entities": []}, {"text": "Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments.", "labels": [], "entities": [{"text": "word formation", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7466756701469421}]}, {"text": "Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes.", "labels": [], "entities": []}, {"text": "We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data.", "labels": [], "entities": [{"text": "English CELEX data", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8466701110204061}, {"text": "German DErivBase (Zeller et al., 2013) data", "start_pos": 55, "end_pos": 98, "type": "DATASET", "confidence": 0.7518072187900543}]}, {"text": "We show that jointly modeling semantics increases both segmentation accuracy and morpheme F 1 by between 3% and 5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9811309576034546}, {"text": "morpheme F 1", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.701824943224589}]}, {"text": "Additionally, we investigate different models of vector composition , showing that recurrent neural networks yield an improvement oversimple additive models.", "labels": [], "entities": []}, {"text": "Finally, we study the degree to which the representations correspond to a linguist's notion of morphological productivity.", "labels": [], "entities": []}], "introductionContent": [{"text": "In most languages, words decompose further into smaller units, termed morphemes.", "labels": [], "entities": []}, {"text": "For example, the English word questionably can be analyzed as question+able+ly.", "labels": [], "entities": []}, {"text": "This structural decomposition of the word, however, by itself is not a semantic representation of the word's meaning; we further require an account of how to synthesize the meaning from the decomposition.", "labels": [], "entities": []}, {"text": "Fortunately, words-just like phrases-to a large extent obey the principle of compositionality: the semantics of the word can be systematically derived from the meaning of its parts.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel joint probabilistic model of word formation that captures both structural decomposition of a word w into its constituent segments and the synthesis of w's meaning from the meaning of those segments.", "labels": [], "entities": [{"text": "word formation", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7422009408473969}]}, {"text": "Morphological segmentation is a structured prediction task that seeks to break a word up into its constituent morphemes.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8833813369274139}]}, {"text": "The output segmentation has been shown to aid a diverse set of applications, such as automatic speech recognition (), keyword spotting (), machine translation () and parsing.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6038537820180258}, {"text": "keyword spotting", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7759769856929779}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8283971548080444}, {"text": "parsing", "start_pos": 166, "end_pos": 173, "type": "TASK", "confidence": 0.9660439491271973}]}, {"text": "In contrast to much of this prior work, we focus on supervised segmentation, i.e., we provide the model with gold segmentations during training time.", "labels": [], "entities": []}, {"text": "Instead of sur-1 There are many different linguistic and computational theories for interpreting the structural decomposition of a word.", "labels": [], "entities": [{"text": "interpreting the structural decomposition of a word", "start_pos": 84, "end_pos": 135, "type": "TASK", "confidence": 0.8767322472163609}]}, {"text": "For example, un-often signifies negation and its effect on semantics can then be modeled by theories based on logic.", "labels": [], "entities": []}, {"text": "This work addresses the question of structural decomposition and semantic synthesis in the general framework of distributional semantics.", "labels": [], "entities": [{"text": "structural decomposition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7367057502269745}, {"text": "semantic synthesis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7562608420848846}]}, {"text": "Morphological research in theoretical and computational linguistics often focuses on noncompositional or less compositional phenomena-simply because compositional derivation poses fewer interesting research problems.", "labels": [], "entities": []}, {"text": "It is also true that-just as many frequent multiword units are not completely compositional-many frequent derivations (e.g., refusal, fitness) are not completely compositional.", "labels": [], "entities": []}, {"text": "An indication that nonlexicalized derivations are usually compositional is the fact that standard dictionaries like OUP editors (2010) list derivational affixes with their compositional meaning, without a hedge that they can also occur as part of only partially compositional forms.", "labels": [], "entities": []}, {"text": "See also face segmentation, our model performs canonical segmentation (, i.e., it allows the induction of orthographic changes together with the segmentation, which is not typical.", "labels": [], "entities": [{"text": "face segmentation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.779207319021225}]}, {"text": "For the example questionably, our model can restore the deleted characters le, yielding the canonical segments question, able and ly.", "labels": [], "entities": []}, {"text": "In this work, our primary contribution lies in the integration of continuous semantic vectors into supervised morphological segmentationwe present a joint model of morphological analysis and semantic synthesis at the word-level.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.7649118602275848}, {"text": "semantic synthesis", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7418020963668823}]}, {"text": "We experimentally investigate three novel aspects of our model.", "labels": [], "entities": []}, {"text": "\u2022 First, we show that jointly modeling continuous representations of the semantics of morphemes and words allows us to improve morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7326045036315918}]}, {"text": "On the English portion of CELEX (, we achieve a 5 point improvement in segmentation accuracy and a 3 point improvement in morpheme F 1 . On the German DErivBase dataset we achieve a 3 point improvement in segmentation accuracy and a 3 point improvement in morpheme F 1 . \u2022 Second, we explore improved models of vector composition for synthesizing word meaning.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.582679808139801}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.898669421672821}, {"text": "German DErivBase dataset", "start_pos": 144, "end_pos": 168, "type": "DATASET", "confidence": 0.7489820818106333}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.8067259192466736}]}, {"text": "We find a recurrent neural network improves over previously proposed additive models.", "labels": [], "entities": []}, {"text": "Moreover, we find that more syntactically oriented vectors ( are better suited for morphology than bag-ofword (BOW) models.", "labels": [], "entities": []}, {"text": "\u2022 Finally, we explore the productivity of English derivational affixes in the context of distributional semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on English and German derivational morphology.", "labels": [], "entities": [{"text": "English and German derivational morphology", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.574240791797638}]}, {"text": "We analyze our joint model's ability to segment words into their canonical morphemes as well as its ability to compositionally derive vectors for new words.", "labels": [], "entities": []}, {"text": "Finally, we explore the relationship between distributional semantics and morphological productivity.", "labels": [], "entities": []}, {"text": "For English, we use the pretrained vectors of for all experiments.", "labels": [], "entities": []}, {"text": "For German, we train word2vec skip-gram vectors on the German Wikipedia.", "labels": [], "entities": []}, {"text": "We first describe our English dataset, the subset of the English portion of the CELEX lexical database ( that was selected by; the dataset contains 10,000 forms.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.757280558347702}, {"text": "CELEX lexical database", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9519813060760498}]}, {"text": "This allows for comparison with previously proposed methods.", "labels": [], "entities": []}, {"text": "(i) make the two-morpheme assumption: every word is composed of exactly two morphemes.", "labels": [], "entities": []}, {"text": "In general, this is not true, so we further segment all complex words in the corpus.", "labels": [], "entities": []}, {"text": "For example, friendless+ness is further segmented into friend+less+ness.", "labels": [], "entities": []}, {"text": "To nevertheless allow for fair comparison, we provide versions of our experiments with and without the two-morpheme assumption where appropriate.", "labels": [], "entities": []}, {"text": "(ii) only provide a single train/test split.", "labels": [], "entities": []}, {"text": "As we require a held-out development set for hyperparameter tuning, we randomly allocate a portion of the training data to select the hyperparameters and then retrain the model using these parameters on the original train split.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7603603303432465}]}, {"text": "We also report 10-fold cross validation results in addition tos train/test split.", "labels": [], "entities": []}, {"text": "Our German dataset is taken from and is described in.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9484250545501709}]}, {"text": "It, again, consists of 10,000 derivational forms.", "labels": [], "entities": []}, {"text": "We report results on 10-fold cross validation.", "labels": [], "entities": []}, {"text": "For our first experiment, we test whether jointly modeling the continuous representations allows us to segment words more accurately.", "labels": [], "entities": []}, {"text": "We assume that we are given an embedding for the target word.", "labels": [], "entities": []}, {"text": "We estimate the model p(v, s, l, u | w) as described in \u00a74 with L 2 regularization \u03bb||\u03b8|| 2 2 . To evaluate, we decode the distribution p(s, l, u | v, w).", "labels": [], "entities": []}, {"text": "We perform approximate MAP inference with importance sampling-taking the sample with the highest score.: Vector approximation (measured by mean cosine similarity) both with (\"oracle\") and without (\"joint\", \"char\") gold morphology.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9685502052307129}]}, {"text": "Surprisingly, joint models are close in performance to models with gold morphology.", "labels": [], "entities": []}, {"text": "In these experiments, we use the RNN with the dependency vectors, the combination of which performs best on vector approximation in \u00a76.2.", "labels": [], "entities": [{"text": "vector approximation", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7147307693958282}]}, {"text": "We follow the experimental design of.", "labels": [], "entities": []}, {"text": "We compare against two baselines (marked \"Baseline\" in): (i) a \"Semi-CRF\" segmenter that cannot account for orthographic changes and (ii) the full \"Joint\" model of.", "labels": [], "entities": []}, {"text": "We additionally consider an \"Oracle\" setting, where we give the model the gold underlying orthographic form (\"UR\") at both training and test time.", "labels": [], "entities": []}, {"text": "This gives us insight into the performance of the transduction factor of our model, i.e., how much could we benefit from a richer model.", "labels": [], "entities": []}, {"text": "Our hyperparameters are (i) the regularization coefficient \u03bb and (ii) \u03c3 2 , the variance of the Gaussian factor.", "labels": [], "entities": []}, {"text": "We use grid search to tune them: \u03bb \u2208 {0.0, 10 1 , 10 2 , 10 3 , 10 4 , 10 5 }, \u03c3 2 \u2208 {0.25, 0.5, 0.75, 1.0}.", "labels": [], "entities": []}, {"text": "We use three metrics to evaluate segmentation accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9749987125396729}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8750057816505432}]}, {"text": "Note that the evaluation of canonical segmentation is hard since a system may return a sequence of morphemes whose concatenation is not the same length as the concatenation of the gold morphemes.", "labels": [], "entities": []}, {"text": "This rules out metrics for surface segmentation like border F 1 (, which require the strings to be of the same length.", "labels": [], "entities": [{"text": "surface segmentation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8031762838363647}, {"text": "border F 1", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.6474871834119161}]}, {"text": "We now define the metrics.", "labels": [], "entities": []}, {"text": "(i) Segmentation accuracy measures whether every single canonical morpheme in the returned sequence is correct.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9413098692893982}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9394619464874268}]}, {"text": "It is inflexible: closer answers are penalized the same as i.e., a model without the Gaussian factor that scores vectors.", "labels": [], "entities": []}, {"text": "(ii) Morpheme F 1 (van den) takes the predicted sequence of canonical morphemes, turns it into a set, computes precision and recall in the standard way and based on that then computes F 1 . This metric gives credit if some of the canonical morphemes were correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9954057931900024}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9971578121185303}, {"text": "F 1", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9741081595420837}]}, {"text": "(iii) Levenshtein distance joins the canonical segments with a special symbol # into a single string and computes the Levenshtein distance between predicted and gold strings.", "labels": [], "entities": []}, {"text": "show that jointly modeling semantic coherence improves our ability to analyze words.", "labels": [], "entities": []}, {"text": "For test, our proposed joint model (\"This Work\") outperforms the baseline supervised canonical segmenter, which is state-of-the-art for the task, by .05 (resp. .03) on accuracy and .03 (resp. .03) on F 1 for English (resp. German).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.999339759349823}]}, {"text": "We also find that when we give the joint model an oracle UR the vectors generally help less: .01 (resp. .02) on accuracy and .01 (resp. .03) on F 1 for English (resp. German).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9993495345115662}]}, {"text": "This indicates that the chief boon the vector composition factor provides lies in selection of an appropriate UR.", "labels": [], "entities": []}, {"text": "Moreover, the up to .15 difference in English between systems with and without the oracle UR suggests that reversing orthographic changes is a particularly difficult part of the task, at least for English.", "labels": [], "entities": []}, {"text": "We adopt the experimental design of.", "labels": [], "entities": []}, {"text": "Its aim is to approximate a vector of a derivationally complex word using a learned model of composition.", "labels": [], "entities": []}, {"text": "As assume a gold morphological analysis, we compare two settings: (i) oracle morphological analysis and (ii) inferred morphological analysis.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, (ii) is a novel experimental condition that no previous work has addressed.", "labels": [], "entities": []}, {"text": "We consider four composition models (See Table 1).", "labels": [], "entities": []}, {"text": "(i) stem, using just the stem vector.", "labels": [], "entities": []}, {"text": "This baseline tells us what happens if we make the incorrect assumption that derivation behaves like inflection and is not meaning-changing.", "labels": [], "entities": []}, {"text": "(ii) add, a purely additive model.", "labels": [], "entities": []}, {"text": "This is arguably the simplest way of combining the vectors of the morphemes.", "labels": [], "entities": []}, {"text": "(iii) LDS, a linear dynamical system.", "labels": [], "entities": []}, {"text": "This is arguably the simplest sequence model.", "labels": [], "entities": []}, {"text": "rent neural networks are currently the most widely used nonlinear sequence model and simple RNNs are the simplest such models.", "labels": [], "entities": []}, {"text": "Part of the motivation for considering a richer class of models lies in our removal of the twomorpheme assumption.", "labels": [], "entities": []}, {"text": "Indeed, it is unclear that the wadd and fulladd models () are useful models in the general case of multimorphemic words-the weights are tied by position, i.e., the first morpheme's vector (be it a prefix or stem) is always multiplied by the same matrix..", "labels": [], "entities": []}, {"text": "This dataset enforces that all words are composed of exactly two morphemes.", "labels": [], "entities": []}, {"text": "Thus, a word like unquestionably is segmented as un+questionably, without further decomposition.", "labels": [], "entities": []}, {"text": "The vectors employed by are high-dimensional count vectors derived from lemmatized and POS tagged text with a before-and-after window of size 2.", "labels": [], "entities": []}, {"text": "They then apply pointwise mutual information (PMI) weighting and dimensionality reduction by non-negative matrix factorization.", "labels": [], "entities": [{"text": "mutual information (PMI) weighting", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.6780791829029719}]}, {"text": "In contrast, we employ WORD2VEC (), a model that is also interpretable as the factorization of a PMI matrix ().", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.772688627243042}]}, {"text": "We consider three WORD2VEC models: two bag-of-word (BOW) models with before-and-after windows of size 2 and 5 and DEPs (), a dependency-based model whose context is derived from dependency parses rather than BOW.", "labels": [], "entities": []}, {"text": "In general, the results indicate that the key to better vector approximation is not a richer model of composition, but rather lies in the vectors themselves.", "labels": [], "entities": [{"text": "vector approximation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.761907696723938}]}, {"text": "We find that our best model, the RNN, only marginally edges out the LDS.", "labels": [], "entities": []}, {"text": "Additionally, looking at the \"all\" column and the DEPs vectors, the simple additive model is only \u2264.02 lower than LDS.", "labels": [], "entities": []}, {"text": "In comparison, we observe large differences between the vectors.", "labels": [], "entities": []}, {"text": "The RNN+DEPs model is .23 better than the BOW5 models (.81 vs. .58), .14 better than the BOW2 models (.81 vs. .67) and .25 better than Lazaridou et al.'s best model (.81 vs. .56).", "labels": [], "entities": [{"text": "BOW5", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.901643693447113}, {"text": "BOW2", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8954399824142456}]}, {"text": "A wider context for BOW (5 instead of 2) yields worse results.", "labels": [], "entities": [{"text": "BOW", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9834085702896118}]}, {"text": "This suggests that syntactic information or at least positional information is necessary for improved models of morpheme composition.", "labels": [], "entities": []}, {"text": "The test vectors are annotated for relatedness, which is a proxy for semantic coherence.", "labels": [], "entities": []}, {"text": "HR (highrelatedness) words were judged to be more compositional than LR (low-relatedness) words.", "labels": [], "entities": []}, {"text": "As a further strong baseline, we consider a retrofitting) approach based on characterlevel recurrent neural networks.", "labels": [], "entities": []}, {"text": "Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model-empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos, parsing ( and language modeling 42.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.8299881517887115}, {"text": "language modeling 42", "start_pos": 233, "end_pos": 253, "type": "TASK", "confidence": 0.7492794096469879}]}, {"text": "To the best of our knowledge, character-level retrofitting is a novel approach.", "labels": [], "entities": [{"text": "character-level retrofitting", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8027573525905609}]}, {"text": "Given a vector v fora word form w, we seek a function to minimize the following objective where h N is the final hidden state of a recurrent neural architecture, i.e., where \u03c3 is a non-linearity and w i is the i th character in w, h i\u22121 is the previous hidden state and A and B are matrices.", "labels": [], "entities": []}, {"text": "While we have defined the architecture fora vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs) as well as deep variants.", "labels": [], "entities": []}, {"text": "Importantly, this model has no knowledge of morphology-it can only rely on representations it extracts from the characters.", "labels": [], "entities": []}, {"text": "This gives us a clear ablation on the benefit of adding structured morphological knowledge.", "labels": [], "entities": []}, {"text": "We optimize the depth and the size of the hidden units on development data using a coarse-grained grid search.", "labels": [], "entities": [{"text": "depth", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9646387696266174}]}, {"text": "We found a depth of 2 and hidden units of size 100 (in both LSTM and GRU) performed best.", "labels": [], "entities": [{"text": "GRU", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.6368778347969055}]}, {"text": "We trained all models for 100 iterations of Adam ( with L 2 regularization with regularization coefficient 0.01.", "labels": [], "entities": []}, {"text": "shows that the two character-level models (\"c-GRU\" and \"c-LSTM\") perform much worse than our models.", "labels": [], "entities": []}, {"text": "This indicates that supervised morphological analysis produces higher-quality vector representations than \"knowledge-poor\" characterlevel models.", "labels": [], "entities": []}, {"text": "However, we note that these character-level models have fewer parameters than our morpheme-level models-there are many more morphemes in a languages than characters.", "labels": [], "entities": []}, {"text": "In general, the twomorpheme assumption is incorrect.", "labels": [], "entities": []}, {"text": "We consider an expanded setting of's task, in which we fully decompose the word, e.g., unquestionably \u2192un+question+able+ly.", "labels": [], "entities": []}, {"text": "These results are reported in (top block, \"oracle\").", "labels": [], "entities": []}, {"text": "We report mean cosine similarity.", "labels": [], "entities": [{"text": "mean cosine similarity", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.5066180129845937}]}, {"text": "Standard deviations s for 10-fold cross-validation (not shown) are small (\u2264 .012) with two exceptions: s = .044 for the DEPs-joint-stem results.", "labels": [], "entities": []}, {"text": "The multi-morphemic results mirror those of the bi-morphemic setting of.", "labels": [], "entities": []}, {"text": "(i) RNN+DEPs attains an average cosine similarity of around .80 for English.", "labels": [], "entities": [{"text": "RNN+DEPs", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6545393466949463}, {"text": "cosine similarity", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.6353708505630493}]}, {"text": "Numbers for German are lower, around .70.", "labels": [], "entities": []}, {"text": "(ii) The RNN only marginally edges out LDS for English and is slightly worse for German.", "labels": [], "entities": [{"text": "RNN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.6166050434112549}, {"text": "LDS", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9540851712226868}]}, {"text": "Again, this is not surprising as we are modeling short sequences.", "labels": [], "entities": []}, {"text": "(iii) Certain embeddings lend themselves more naturally to derivational compositionality: BOW2 is better than BOW5, DEPs is the clear winner.", "labels": [], "entities": [{"text": "BOW2", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9258242249488831}]}, {"text": "The final setting we consider is the vector approximation task without gold morphology.", "labels": [], "entities": [{"text": "vector approximation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.838125079870224}]}, {"text": "In this case, we rely on the full joint model p(v, s, l, u | w).", "labels": [], "entities": []}, {"text": "At evaluation, we are interested in the marginal distribution p(v | w) = s,l,u p(v, s, l, u | w).", "labels": [], "entities": []}, {"text": "We then use importance sampling to approximate the mean of this marginal distribution as the predicted embedding, i.e., where w (i) are the importance weights defined in Equation 8 and l (i) and s (i) are the i th sampled labeling and segmentation, respectively.", "labels": [], "entities": []}, {"text": "Surprisingly, (joint) shows that relying on the inferred morphology does not drastically affect the results.", "labels": [], "entities": []}, {"text": "Indeed, we are often within .01 of the result with gold morphology.", "labels": [], "entities": []}, {"text": "Our method can be viewed as a retrofitting procedure, so this result is useful: it indicates that joint semantic synthesis and morphological analysis produces high-quality vectors.", "labels": [], "entities": [{"text": "semantic synthesis", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7076341807842255}]}, {"text": "We now delve into the relation between distributional semantics and morphological productivity.", "labels": [], "entities": []}, {"text": "The extent to which jointly modeling semantics aids morphological analysis will be determined by the inherent compositionality of the words within the vector space.", "labels": [], "entities": []}, {"text": "We breakdown our results on the vector approximation task with gold morphology using the On the left end of we see extremely productive suffixes.", "labels": [], "entities": [{"text": "vector approximation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8184735178947449}]}, {"text": "The affix ize is used productively with relatively obscure words in the sciences, e.g., Rao-Blackwellize.", "labels": [], "entities": [{"text": "Rao-Blackwellize", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.956549346446991}]}, {"text": "Likewise, the affix ness can be applied to almost any adjective without restriction, e.g., Poissonness 'degree to which data have a Poisson distribution'.", "labels": [], "entities": [{"text": "affix ness", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.8937284648418427}, {"text": "Poissonness 'degree", "start_pos": 91, "end_pos": 110, "type": "METRIC", "confidence": 0.8684839010238647}]}, {"text": "On the right end, we find -ment, -er and re-.", "labels": [], "entities": []}, {"text": "The affix -ment is borderline productive-modern English tends to form novel nominalizations with ness or ity.", "labels": [], "entities": []}, {"text": "More interesting are re-and er, both of which are very productive in English.", "labels": [], "entities": [{"text": "re-and er", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8086776733398438}]}, {"text": "For er, many of the words bringing down the average are simply non-compositional.", "labels": [], "entities": []}, {"text": "For example, homer 'homerun in baseball' is not derived from home+er-this is an error in data.", "labels": [], "entities": []}, {"text": "We also see examples like cutter.", "labels": [], "entities": [{"text": "cutter", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.8425624370574951}]}, {"text": "It has a compositional reading (e.g., \"box cutter\"), but also frequently occurs in the non-compositional meaning 'type of boat'.", "labels": [], "entities": []}, {"text": "Finally, proper nouns like Homer and Turner end in er and in our experiments we computed vectors for lowercased words.", "labels": [], "entities": []}, {"text": "The affix re-similarly has a large number of non-compositional cases, e.g., remove, relocate, remark.", "labels": [], "entities": []}, {"text": "Indeed, to get the compositional reading of remove, the first syllable (rather than the second) is typically stressed to emphasize the prefix.", "labels": [], "entities": []}, {"text": "We finally note several limitations of this experiment.", "labels": [], "entities": []}, {"text": "(i) The ability of our models-even the recurrent neural network-to model transformations between vectors is limited.", "labels": [], "entities": []}, {"text": "(ii) Our vectors are far from perfect; e.g., sparseness in the training data affects quality and some of the words in our corpus are rare.", "labels": [], "entities": []}, {"text": "(iii) Semantic coherence is not the only criterion for productivity.", "labels": [], "entities": []}, {"text": "An example is -th in English.", "labels": [], "entities": []}, {"text": "As noted earlier, it is compositional in a word like warmth, but it cannot be used to form new words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Vector approximation (measured by mean co- sine similarity) both with (\"oracle\") and without (\"joint\",  \"char\") gold morphology. Surprisingly, joint models are  close in performance to models with gold morphology.", "labels": [], "entities": []}, {"text": " Table 4: Vector approximation (measured by mean cosine  similarity) with gold morphology on the train/test split of  Lazaridou et al. (2013). HR/LR = high/low-relatedness  words. See Lazaridou et al. (2013) for details.", "labels": [], "entities": [{"text": "HR/LR", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.8726804653803507}]}]}