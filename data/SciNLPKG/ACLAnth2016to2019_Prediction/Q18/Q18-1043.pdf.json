{"title": [{"text": "Exploring Neural Methods for Parsing Discourse Representation Structures", "labels": [], "entities": [{"text": "Parsing Discourse Representation Structures", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.8780385404825211}]}], "abstractContent": [{"text": "Neural methods have had several recent successes in semantic parsing, though they have yet to face the challenge of producing meaning representations based on formal semantics.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7800313532352448}]}, {"text": "We present a sequence-to-sequence neural semantic parser that is able to produce Discourse Representation Structures (DRSs) for English sentences with high accuracy, outperforming traditional DRS parsers.", "labels": [], "entities": [{"text": "sequence-to-sequence neural semantic parser", "start_pos": 13, "end_pos": 56, "type": "TASK", "confidence": 0.6733605787158012}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.996035635471344}]}, {"text": "To facilitate the learning of the output, we represent DRSs as a sequence of flat clauses and introduce a method to verify that produced DRSs are well-formed and interpretable.", "labels": [], "entities": []}, {"text": "We compare models using characters and words as input and see (somewhat surprisingly) that the former performs better than the latter.", "labels": [], "entities": []}, {"text": "We show that eliminating variable names from the output using De Bruijn indices increases parser performance.", "labels": [], "entities": []}, {"text": "Adding silver training data boosts performance even further.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8275631368160248}]}, {"text": "Semantic parsing used to be the domain of symbolic and statistical approaches.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8113459348678589}]}, {"text": "Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.8340585430463155}]}, {"text": "These include code generation (, question answering ( and Abstract Meaning Representation parsing (.", "labels": [], "entities": [{"text": "code generation", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7735379040241241}, {"text": "question answering", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9032312333583832}, {"text": "Abstract Meaning Representation parsing", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.7839584648609161}]}, {"text": "Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (.", "labels": [], "entities": []}, {"text": "These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations.", "labels": [], "entities": []}, {"text": "This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a wellstudied formalism developed informal semantics, dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (see).", "labels": [], "entities": [{"text": "parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT)", "start_pos": 25, "end_pos": 125, "type": "TASK", "confidence": 0.8343317588170369}]}, {"text": "DRSs are recursive structures and thus form a challenge for sequence-tosequence models because they need to generate a well-formed structure and not something that looks like one but is not interpretable.", "labels": [], "entities": []}, {"text": "The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR;).", "labels": [], "entities": [{"text": "mapping sentences to an Abstract Meaning Representation (AMR", "start_pos": 88, "end_pos": 148, "type": "TASK", "confidence": 0.7025209102365706}]}, {"text": "But there are notable differences between DRS and AMR.", "labels": [], "entities": [{"text": "DRS", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6865744590759277}, {"text": "AMR", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7699007391929626}]}, {"text": "Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation.", "labels": [], "entities": []}, {"text": "Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning.", "labels": [], "entities": []}, {"text": "DRS parsing was attempted in the 1980s for small fragments of English (.", "labels": [], "entities": [{"text": "DRS parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7274819612503052}]}], "datasetContent": [{"text": "A DRS parser is evaluated by comparing its output DRS to a gold standard DRS using the Counter tool.", "labels": [], "entities": []}, {"text": "Counter calculates an F-score over matching clauses.", "labels": [], "entities": [{"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9987610578536987}]}, {"text": "Because variable names are meaningless, obtaining the matching clauses essentially is a search for the best variable mapping between two DRSs.", "labels": [], "entities": []}, {"text": "Counter tries to find this mapping by performing a hill-climbing search with a predefined number of restarts to avoid getting stuck in a local optimum, which is similar to the evaluation system SMATCH for AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 205, "end_pos": 216, "type": "TASK", "confidence": 0.8593293726444244}]}, {"text": "Counter generalizes over WordNet synsets (i.e., a system is not penalized for predicting a word sense that is in the same synset as the gold standard word sense).", "labels": [], "entities": []}, {"text": "To calculate whether there is a significant difference between two systems, we perform approximate randomization) with \u03b1 = 0.05, R = 1,000, and F (model 1 ) > F (model 2 ) as test statistics for each individual DRS pair.", "labels": [], "entities": []}, {"text": "This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training.", "labels": [], "entities": []}, {"text": "Because semantic annotation is a difficult and time-consuming task, gold standard data sets are usually relatively small.", "labels": [], "entities": [{"text": "semantic annotation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8542964160442352}]}, {"text": "This means that semantic parsers (and data-hungry neural methods in particular) can often benefit from more training data.", "labels": [], "entities": []}, {"text": "Some examples in semantic parsing are data recombination (, paraphrasing, or exploiting machinegenerated output (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7452077567577362}, {"text": "data recombination", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.714451253414154}]}, {"text": "However, before we do any experiments using extra training data, we want to be sure that we can still ben-: F1-score and percentage of ill-formed DRSs on the test set, for the experiments with the PMB-released silver data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9995061159133911}, {"text": "PMB-released silver data", "start_pos": 197, "end_pos": 221, "type": "DATASET", "confidence": 0.9253276387850443}]}, {"text": "The scores without using an ensemble are an average of five runs of the model.", "labels": [], "entities": []}, {"text": "efit from more gold training data.", "labels": [], "entities": []}, {"text": "For both the character level and word level we plot the learning curve, adding 500 training instances at a time, in.", "labels": [], "entities": []}, {"text": "For both models the F-score clearly still improves when using more training instances, which shows that there is at least the potential for additional data to improve the score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9976286292076111}]}, {"text": "For DRSs, the PMB-2.1.0 release already contains a large set of silver standard data (71,308 instances), containing DRSs that are only partially manually corrected.", "labels": [], "entities": [{"text": "PMB-2.1.0 release", "start_pos": 14, "end_pos": 31, "type": "DATASET", "confidence": 0.9172860682010651}]}, {"text": "We then train a model on both the gold and silver standard data, making no distinction between them during training.", "labels": [], "entities": []}, {"text": "After training we take the last model and restart the training on only the gold data, in a similar process as described in and van.", "labels": [], "entities": []}, {"text": "In general, restarting the training to fine-tune the weights of the model is a common technique in NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.8754140734672546}]}, {"text": "We are aware that there are many methods to obtain and utilize additional data.", "labels": [], "entities": []}, {"text": "However, our main aim is not to find the optimal method for DRS parsing, but to demonstrate that using additional data is indeed beneficial for neural DRS parsing.", "labels": [], "entities": [{"text": "DRS parsing", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9462984800338745}, {"text": "neural DRS parsing", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.6336225171883901}]}, {"text": "Because we are not further fine-tuning our model, we will show results on the test set in this section.", "labels": [], "entities": []}, {"text": "shows the results of adding the silver data.", "labels": [], "entities": [{"text": "silver", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9483649730682373}]}, {"text": "This results in a large increase in performance, for both the character-and word-level models.", "labels": [], "entities": []}, {"text": "We are still reliant on manually annotated data, however, because without the gold data (so training on only the silver data), we score even lower than our baseline model (68.4 and 68.1 for the char and word parser).", "labels": [], "entities": []}, {"text": "Similarly, we are reliant on the fine-tuning procedure, as we also score below our baseline models without it (71.6 and 71.0 for the char and word parsers, respectively).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents, sentences, and to- kens for the English part of PMB release 2.1.0.  Note that the number of tokens is based on the  PMB tokenization, treating multiword expressions  as a single token.", "labels": [], "entities": [{"text": "PMB release 2.1.0", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.714320182800293}]}, {"text": " Table 2: Counts of relevant semantic phenomena  for PMB release 2.1.0. 3 These phenomena are  described and further discussed in  \u00a76.3.", "labels": [], "entities": [{"text": "PMB release 2.1.0.", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.6501025954882304}]}, {"text": " Table 4: Evaluating different input represen- tations. The percentage of ill-formed DRSs is  denoted by % ill.", "labels": [], "entities": []}, {"text": " Table 5: Results of the 10-fold CV experiments re- garding tokenization, variable rewriting, and cas- ing; bs/mos means that we use no tokenization for  the character-level parser, while we use Moses for  the word-level parser.", "labels": [], "entities": []}, {"text": " Table 6: F1-score and percentage of ill-formed  DRSs on the test set, for the experiments with the  PMB-released silver data. The scores without us- ing an ensemble are an average of five runs of the  model.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993744492530823}, {"text": "PMB-released silver data", "start_pos": 101, "end_pos": 125, "type": "DATASET", "confidence": 0.9318764011065165}]}, {"text": " Table 8: Test set results of our best neural models  compared to two baseline models and two parsers.", "labels": [], "entities": []}, {"text": " Table 9: F-scores of fine-grained evaluation on the  test set of the three semantic parsers.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9956187605857849}]}, {"text": " Table 10: Manual evaluation of the output of the  three semantic parsers on several semantic phe- nomena. Reported numbers are accuracies.", "labels": [], "entities": []}]}