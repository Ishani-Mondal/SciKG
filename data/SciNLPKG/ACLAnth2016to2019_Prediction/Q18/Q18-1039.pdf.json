{"title": [{"text": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification", "labels": [], "entities": [{"text": "Adversarial Deep Averaging Networks", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8020160794258118}, {"text": "Cross-Lingual Sentiment Classification", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.8020704587300619}]}], "abstractContent": [{"text": "In recent years great success has been achieved in sentiment classification for En-glish, thanks in part to the availability of copious annotated resources.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9738104343414307}]}, {"text": "Unfortunately, most languages do not enjoy such an abundance of labeled data.", "labels": [], "entities": []}, {"text": "To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN 1) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exist.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.9615919291973114}]}, {"text": "ADAN has two discriminative branches: a sentiment classifier and an adversarial language dis-criminator.", "labels": [], "entities": [{"text": "ADAN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6575774550437927}, {"text": "sentiment classifier", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7783640027046204}]}, {"text": "Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages.", "labels": [], "entities": []}, {"text": "Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.", "labels": [], "entities": [{"text": "Chinese and Arabic sentiment classification", "start_pos": 15, "end_pos": 58, "type": "TASK", "confidence": 0.6831069350242615}, {"text": "ADAN", "start_pos": 76, "end_pos": 80, "type": "TASK", "confidence": 0.7451492547988892}]}], "introductionContent": [{"text": "Many state-of-the-art models for sentiment classification () are supervised learning approaches that rely on the availability of an adequate amount of labeled training data.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.9666838049888611}]}, {"text": "For a few resource-rich languages, including English, such labeled data are indeed available.", "labels": [], "entities": []}, {"text": "For the vast majority of languages, however, it is the norm that only a limited amount of annotated text exists.", "labels": [], "entities": []}, {"text": "Worse still, many low-resource languages have no labeled data at all.", "labels": [], "entities": []}, {"text": "To aid the creation of sentiment classification systems in such low-resource languages, an active research direction is cross-lingual sentiment classification (CLSC), in which the abundant resources of a source language (likely English, denoted as SOURCE) are leveraged to produce sentiment classifiers fora target language (TARGET).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8907716870307922}, {"text": "cross-lingual sentiment classification (CLSC)", "start_pos": 120, "end_pos": 165, "type": "TASK", "confidence": 0.8291952709356943}]}, {"text": "In general, CLSC methods make use of generalpurpose bilingual resources-such as hand-crafted bilingual lexica or parallel corpora-to alleviate or eliminate the need for task-specific TARGET annotations.", "labels": [], "entities": []}, {"text": "In particular, the bilingual resource of choice for the majority of previous CLSC models is a full-fledged Machine Translation (MT) system, a component that is expensive to obtain.", "labels": [], "entities": [{"text": "full-fledged Machine Translation (MT)", "start_pos": 94, "end_pos": 131, "type": "TASK", "confidence": 0.8213312824567159}]}, {"text": "In this work, we propose a language-adversarial training approach that does not need a highly engineered MT system, and requires orders of magnitude less in terms of the size of a parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.950329601764679}]}, {"text": "Specifically, we propose an Adversarial Deep Averaging Network (ADAN) that leverages a set of bilingual word embeddings (BWEs;) trained on bitexts, in order to eliminate the need for labeled TARGET training data.", "labels": [], "entities": []}, {"text": "We introduce the ADAN model in \u00a72, and in \u00a73 evaluate ADAN using English as the SOURCE with two TARGET choices: Chinese and Arabic.", "labels": [], "entities": [{"text": "TARGET", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9916341304779053}]}, {"text": "ADAN is first compared to two baseline systems: (i) one trained only on labeled SOURCE data, relying on BWEs for cross-lingual generalization; and (ii) a domain adaptation method) that views the two languages simply as two distinct domains.", "labels": [], "entities": []}, {"text": "We then validate ADAN against two state-of-the-art CLSC methods: (iii) an approach that uses a powerful MT system, and (iv) the crosslingual \"distillation\" approach of that makes direct use of a parallel corpus (see.", "labels": [], "entities": []}, {"text": "In all cases, we find that ADAN achieves statistically significantly better results.", "labels": [], "entities": [{"text": "ADAN", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.7843744158744812}]}, {"text": "We further investigate the semi-supervised setting, where a small amount of annotated TARGET data exists, and show that ADAN continues to outperform the alternatives given the same amount of TARGET supervision ( \u00a73.3.1).", "labels": [], "entities": [{"text": "ADAN", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.7404959797859192}]}, {"text": "We provide an analysis and visualization of ADAN ( \u00a73.3.2), shedding light on how our approach manages to achieve its strong cross-lingual performance.", "labels": [], "entities": []}, {"text": "Additionally, we study the bilingual resource that ADAN depends on, the BWEs, and demonstrate that ADAN's performance is robust with respect to the choice of BWEs.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.5783641934394836}]}, {"text": "Furthermore, even without the pre-trained BWEs (i.e., using random initialized embeddings), ADAN outperforms all but the stateof-the-art MT-based and distillation systems ( \u00a73.3.3).", "labels": [], "entities": [{"text": "BWEs", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.859640896320343}, {"text": "ADAN", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.6152517199516296}, {"text": "MT-based", "start_pos": 137, "end_pos": 145, "type": "TASK", "confidence": 0.94161456823349}]}, {"text": "This makes ADAN the first CLSC model that outperforms BWE-based baseline systems without relying on any bilingual resources.", "labels": [], "entities": []}, {"text": "A final methodological contribution distinguishes ADAN from previous adversarial networks for text classification (: ADAN minimizes the Wasserstein distance ( ) between the feature distributions of SOURCE and TARGET ( \u00a72.2), which yields better performance and smoother training than the standard training method with a Gradient Reversal Layer (; see \u00a73.3.5).", "labels": [], "entities": [{"text": "text classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7481569647789001}, {"text": "TARGET", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.7883034944534302}]}], "datasetContent": [{"text": "To demonstrate the effectiveness of our model, we experiment on Chinese and Arabic sentiment classification, using English as SOURCE for both.", "labels": [], "entities": [{"text": "Arabic sentiment classification", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.655971884727478}]}, {"text": "For all data used in experiments, tokenization is done using Stanford CoreNLP ().", "labels": [], "entities": [{"text": "tokenization", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9744656682014465}, {"text": "Stanford CoreNLP", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.934842050075531}]}], "tableCaptions": [{"text": " Table 1: ADAN performance for Chinese (5-cls) and Arabic (3-cls) sentiment classification without using labeled  TARGET data. All systems but the CLD ones use BWE to map SOURCE and TARGET words into the same space.  CLD-based CLTC represents cross-lingual text classification methods based on cross-lingual distillation (", "labels": [], "entities": [{"text": "ADAN", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7327640056610107}, {"text": "sentiment classification", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.8651397526264191}, {"text": "BWE", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9804619550704956}, {"text": "cross-lingual text classification", "start_pos": 243, "end_pos": 276, "type": "TASK", "confidence": 0.6570993562539419}]}, {"text": " Table 2: Model performance on Chinese with various  BWE initializations.", "labels": [], "entities": []}, {"text": " Table 3: Performance and speed for various feature  extractor architectures on Chinese.", "labels": [], "entities": [{"text": "speed", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9830971956253052}]}]}