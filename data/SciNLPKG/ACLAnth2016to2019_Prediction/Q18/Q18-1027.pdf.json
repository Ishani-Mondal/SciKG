{"title": [{"text": "Polite Dialogue Generation Without Parallel Data", "labels": [], "entities": [{"text": "Polite Dialogue Generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.768854022026062}]}], "abstractContent": [{"text": "Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguis-tically accurate.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7059226036071777}]}, {"text": "Moreover, parallel datasets for regular-to-stylistic pairs are usually unavailable.", "labels": [], "entities": []}, {"text": "We present three weakly-supervised models that can generate diverse, polite (or rude) dialogue responses without parallel data.", "labels": [], "entities": []}, {"text": "Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances.", "labels": [], "entities": []}, {"text": "Our label-fine-tuning (LFT) model prepends to each source sequence a politeness-score scaled label (pre-dicted by our state-of-the-art politeness classi-fier) during training, and attest time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score.", "labels": [], "entities": []}, {"text": "Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response.", "labels": [], "entities": [{"text": "politeness generation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.8405425250530243}]}, {"text": "We also present two retrieval-based, polite dialogue model baselines.", "labels": [], "entities": []}, {"text": "Human evaluation validates that while the Fusion and the retrieval-based models achieve politeness with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating stylistic, personality-based language is crucial to developing engaging, convincing, and trustworthy conversational agents, for their effective application in intelligent tutoring, home assistance, online reservations/purchasing, healthcare, etc.", "labels": [], "entities": []}, {"text": "Most current chatbots and conversational models lack any such style, which can be asocial issue because human users might learn biased styles from such interactions, e.g., kids learning to be rude because the dialogue system encourages short, curt responses, and also does not itself use politeness to set an example.", "labels": [], "entities": []}, {"text": "In this work, we focus on the important and diverse paralinguistic style axis of politeness vs. rudeness.", "labels": [], "entities": []}, {"text": "Generating stylistic dialogue responses is a substantially challenging task because the generated response needs to be syntactically and semantically fluent, contextually-relevant to the conversation, as well as convey accurate paralinguistic features.", "labels": [], "entities": []}, {"text": "This is further complicated by the fact that content and style are only available in separate unpaired datasets, as opposed to translation-type parallel datasets containing regular-to-stylistic text pairs.", "labels": [], "entities": []}, {"text": "Hence, we need indirectly-supervised models that can incorporate style into the generated response in absence of parallel data (i.e., where the training data for the conversation, versus style components, comes from two different datasets or domains), while still maintaining conversation relevance.", "labels": [], "entities": []}, {"text": "In this work, we present three such weaklysupervised models 2 that can generate diverse, natural, and contextually-relevant polite (and rude) di-alogue responses, using data from separate style and dialogue domains: the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al.,) with Wikipedia and Stack Exchange requests, and the MovieTriples Dialogue Corpus (  with IMSDB movie scripts, respectively.", "labels": [], "entities": [{"text": "MovieTriples Dialogue Corpus", "start_pos": 333, "end_pos": 361, "type": "DATASET", "confidence": 0.782024880250295}]}, {"text": "Each of our three models is based on a state-of-the-art politeness classifier and a sequence-to-sequence dialogue model.", "labels": [], "entities": []}, {"text": "The first model (Fusion) employs a late fusion technique to merge the response generation decoder of the dialogue model with a language model trained on polite utterances chosen by the politeness classifier.", "labels": [], "entities": []}, {"text": "The second label-fine-tuning (LFT) model prepends to the input utterance a single politeness label whose embedding is continuously scaled by the politeness score of the target sequence during training.", "labels": [], "entities": []}, {"text": "This score is determined by feeding the corresponding ground-truth target sequence to our politeness classifier.", "labels": [], "entities": []}, {"text": "During test time, we show that the LFT model is able to control the politeness level of generated responses by simply scaling the label's embedding by the continuous target politeness score of our choice.", "labels": [], "entities": []}, {"text": "Our third reinforcement-based model (Polite-RL) encourages politeness generation by using the continuous-scale politeness score of the decoder-sampled sentence as a reward (via mixedobjective policy gradient methods), i.e., polite utterances are encouraged with positive reward, and rude ones discouraged with negative reward.", "labels": [], "entities": [{"text": "politeness generation", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7797837257385254}]}, {"text": "Hence, our models only need a style classifier (without parallel data) to automatically influence and encourage continuous-scale stylistic language generation in a complex dialogue setup, which also requires maintaining relevance to conversational context.", "labels": [], "entities": [{"text": "continuous-scale stylistic language generation", "start_pos": 112, "end_pos": 158, "type": "TASK", "confidence": 0.723517969250679}]}, {"text": "Each of these models requires minimal changes to the architecture of either the underlying sequence-to-sequence (Seq2seq) dialogue base model or the style classifier, and hence can modularly update the architecture with the latest stateof-the-art dialogue models or style classifiers (and for diverse styles).", "labels": [], "entities": []}, {"text": "In addition, we also employ two retrieval-based models, where we output the response which has the highest match with the input context from a set of classifier-picked polite responses or manually-picked generic polite utterof the LFT model were added to the Feb 1, 2018 resubmission based on reviewer discussions. ances.", "labels": [], "entities": []}, {"text": "These two retrieval models serve as parallel investigations on the performance of our three proposed generative models above.", "labels": [], "entities": []}, {"text": "We conducted multiple human evaluations (for style and dialogue quality) on Amazon Mechanical Turk (MTurk)) for all three models plus the base sequence-to-sequence dialogue model and the retrieval-based models, and show that while the Fusion and the two retrieval models increase the politeness level of responses at the cost of poorer dialogue quality, both our LFT and Polite-RL models can successfully produce polite responses (capturing several politeness strategies discussed by), without sacrificing dialogue coherence and relevance compared to the base Seq2seq model (hence better balance between politeness and dialogue quality).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk))", "start_pos": 76, "end_pos": 107, "type": "DATASET", "confidence": 0.9019474983215332}]}, {"text": "We also compare the output dialogue politeness levels of the continuous LFT model for three different politeness levels.", "labels": [], "entities": []}, {"text": "Finally, we present several detailed qualitative and quantitative analyses, including positive and negative output examples, automatic metric results on output responses, classifier error analysis, and visualization of the RL rewards.", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed above, we propose models that can deal with style data coming from an unpaired, nonparallel domain, different from the domain of the dialogue dataset.", "labels": [], "entities": []}, {"text": "For our style (politeness) domain, we use the Stanford Politeness Corpus (DanescuNiculescu-Mizil et al., 2013), which contains a collection of requests from Wikipedia (WIKI) editor's talk pages and the Stack Exchange (SE) questionanswering communities.", "labels": [], "entities": [{"text": "Stanford Politeness Corpus (DanescuNiculescu-Mizil et al., 2013)", "start_pos": 46, "end_pos": 110, "type": "DATASET", "confidence": 0.8609580159187317}]}, {"text": "Based on scores from human annotators, these requests are labeled with either Polite or Rude, with each class equally consisting of 1,089 requests for the Wikipedia domain and 1,651 requests for the Stack Exchange domain.", "labels": [], "entities": [{"text": "Rude", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.8099492788314819}, {"text": "Stack Exchange domain", "start_pos": 199, "end_pos": 220, "type": "DATASET", "confidence": 0.7391647497812907}]}, {"text": "For the content (dialogue) domain, we use the popular MovieTriples dialogue corpus ( , which contains 245K conversations extracted from IMSDB movie scripts in X-Y-X triplet-utterance format, where X and Y correspond to two movie characters (and the model's task is to generate the last response).", "labels": [], "entities": [{"text": "MovieTriples dialogue corpus", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.8905297517776489}]}, {"text": "Human To evaluate our models' ability to generate polite responses without sacrificing dialogue quality, we conducted several comprehensive human evaluation studies on Amazon Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 168, "end_pos": 198, "type": "DATASET", "confidence": 0.92221999168396}]}, {"text": "Specifically, we compare the three stylistic models w.r.t. the base model on both dialogue quality (i.e., context relevance and coherence) and politeness level.", "labels": [], "entities": []}, {"text": "For this, we randomly sampled 300 contexts covering all types of conversations and their generated responses from the Seq2seq base model, the three stylistic models, and the retrievalbased models.", "labels": [], "entities": []}, {"text": "For each source input, the six responses are randomly shuffled to anonymize model identities.", "labels": [], "entities": []}, {"text": "Each response was then annotated by two human evaluators that were located in the US, had an approval rate greater than 98%, and had at least 10, 000 approved HITs (Human Intelligence Tasks) on record (to prevent those who had just started using MTurk and hence unconditionally enjoyed a high acceptance rate.).", "labels": [], "entities": []}, {"text": "All our human evaluations are performed by two annotators (for both dialogue quality and politeness level) in order to calculate inter-rater agreement, for which we employ Cohens Kappa \u03ba, a score that measures the level of inter-rater agreement between two annotators on a classification problem (Artstein and Poe- We opted for dialogue quality rather than several separated, fine-grained metrics such as relevance, specificity, informativeness because found that little additional information was provided by adding in more metrics on top of overall dialogue quality, and it also confused MTurkers in many scenarios.", "labels": [], "entities": [{"text": "Cohens Kappa \u03ba", "start_pos": 172, "end_pos": 186, "type": "METRIC", "confidence": 0.709689865509669}]}, {"text": "We had similar observations in our initial human study on MTurk.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7323559522628784}]}, {"text": "For both dialogue quality and politeness evaluations, the human raters were shown the conversation context (input) and the six shuffled responses (from the six models).", "labels": [], "entities": []}, {"text": "Clear instructions (closely following those from) corresponding to each score were shown in the interface.", "labels": [], "entities": []}, {"text": "More specifically, we asked the annotators to first read the context and each of the generated/retrieved responses, and assign a score to each response.", "labels": [], "entities": []}, {"text": "They then scored each response on a fivepoint Likert scale (Likert, 1932) (for both politeness and dialogue quality), hence providing absolute measurements but in an overall comparative (relative) setting.", "labels": [], "entities": []}, {"text": "We explicitly stated that it is possible for them to find some conversation disconnected or lacking context, and encouraged them to make the best guess when in doubt.", "labels": [], "entities": []}, {"text": "Using similar instructions (and a 300-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively.", "labels": [], "entities": []}, {"text": "Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; () as an approximation of dialogue quality as used by some previous work).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 275, "end_pos": 279, "type": "METRIC", "confidence": 0.9969788789749146}]}, {"text": "Note that we choose to report BLEU scores not to draw any immediate conclusion (  found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses 'Polite', 'Slightly Polite', 'Neutral', 'Slightly Rude', 'Rude'; and our dialogue quality study uses 'Very good', 'Good', 'Acceptable', 'Poor', and 'Very poor', instead of the abstract scores 1-5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.991679847240448}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9951441287994385}, {"text": "Acceptable", "start_pos": 486, "end_pos": 496, "type": "METRIC", "confidence": 0.975104570388794}]}, {"text": "Note that we did not adopt pairwise comparisons because first, it will create several independent sets of pairwise results (15 sets in our case), which also raises the cost substantially, and secondly, pairwise comparison does not tell us \"by how much\" a response is better/equal/worse than the other.", "labels": [], "entities": []}, {"text": "In contrast, our absolute scores can help future research compare more directly to our results.", "labels": [], "entities": []}, {"text": "We will release our detailed instructions and MTurk interfaces, plus our annotation scores on our webpage.", "labels": [], "entities": []}, {"text": "We also compute the politeness classifier's scores as an approximation of politeness level.", "labels": [], "entities": []}, {"text": "5.2, we also use some automatic evaluation metrics to complement and verify the MTurk human study results.", "labels": [], "entities": [{"text": "MTurk human study", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.5867308576901754}]}, {"text": "In, we present the average politeness classifier and BLEU-4 scores of responses from each model.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9996341466903687}]}, {"text": "First, we can see that our politeness classifier agrees reasonably well with the human politeness judgments in, since both identify the Retrieval-based models and LFT as the most polite, followed by Polite-RL and Fusion in descending order.", "labels": [], "entities": []}, {"text": "We quantified this 'agreement' concretely, and found high correlation between the six human Politeness scores Politeness column) and the six automatic classifier scores (  ), their relative system-ranking still roughly agrees with that of human judgments -we found reasonably high correlation between human Dialogue Quality and BLEU (based on the six scores in Quality column and BLEU-4 column): Pearson correlation is 0.793 (stat.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 328, "end_pos": 332, "type": "METRIC", "confidence": 0.9978654980659485}, {"text": "BLEU-4", "start_pos": 380, "end_pos": 386, "type": "METRIC", "confidence": 0.9662902355194092}, {"text": "Pearson correlation", "start_pos": 396, "end_pos": 415, "type": "METRIC", "confidence": 0.9747613072395325}]}, {"text": "significance p = 0.0597), and Spearman's rank-order correlation is 0.771 (p = 0.0724).", "labels": [], "entities": [{"text": "rank-order correlation", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.746830552816391}]}, {"text": "Hence, overall, the automatic metric evaluation again shows that without politeness training, the base dialogue model produces neutral responses on average (0.49 score), while the retrieval-based models and all three proposed generative models improve on politeness score.", "labels": [], "entities": []}, {"text": "Also, the BLEU scores show, similar to the human study results in, that among the three proposed models, the Fusion model sacrifices the most dialog quality to become more polite, whereas the LFT and RL models main-   tain comparable quality with improved politeness over the base model (Seq2seq).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992433786392212}]}, {"text": "For the retrieval models, we again see that their politeness levels are better than LFT and RL models, but with a corresponding loss in dialogue quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Politeness classification accuracies. Top results  are boldfaced.", "labels": [], "entities": [{"text": "Politeness classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8689621388912201}, {"text": "accuracies", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.7453632354736328}]}, {"text": " Table 2: PPL, WER results computed on {U 1 , U 2 , U 3 }  and PPL@L, WER@L computed on {U 3 } conditioned  on {U 1 , U 2 }. Lower is better for all metrics. Top results  are boldfaced.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.986214816570282}]}, {"text": " Table 3: MTurk human evaluation results on politeness  level and dialogue quality (as well as the absolute value  difference between the two, to show balance) of the Re- trieval Models, Seq2seq and the three proposed genera- tive models (avg. of two annotators is shown here). Top  results are boldfaced.", "labels": [], "entities": [{"text": "MTurk human evaluation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7958236734072367}]}, {"text": " Table 4 Discrete-Continuous-LFT Discrete-LFT  Polite  3.70  3.52  Neutral  3.15  3.09  Rude  1.19  2.93", "labels": [], "entities": []}, {"text": " Table 5: Automatic metrics: avg. politeness and BLEU-4  scores for the two Retrieval models, Seq2seq and three  proposed models. Also, the politeness score of Neutral- LFT and Rude-LFT are 0.48, 0.25, resp. Top results are  boldfaced.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996007084846497}]}, {"text": " Table 6: Analysis of politeness classifier: sampled re- sponses from MovieTriples dialogue corpus (without  cherry-picking) and their politeness score. The double  line separates polite and rude classified responses.", "labels": [], "entities": [{"text": "MovieTriples dialogue corpus", "start_pos": 70, "end_pos": 98, "type": "DATASET", "confidence": 0.9245697657267252}]}]}