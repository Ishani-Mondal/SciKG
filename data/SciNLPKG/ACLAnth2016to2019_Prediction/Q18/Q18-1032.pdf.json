{"title": [{"text": "Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction", "labels": [], "entities": [{"text": "Word-Level Prediction", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.6778409481048584}]}], "abstractContent": [{"text": "Neural architectures are prominent in the construction of language models (LMs).", "labels": [], "entities": []}, {"text": "However , word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7911302447319031}]}, {"text": "Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks.", "labels": [], "entities": [{"text": "next-word prediction in language modeling tasks", "start_pos": 153, "end_pos": 200, "type": "TASK", "confidence": 0.7370438824097315}]}, {"text": "Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios.", "labels": [], "entities": []}, {"text": "In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community , while considering subword-level information.", "labels": [], "entities": []}, {"text": "The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.7029059330622355}, {"text": "word-level prediction", "start_pos": 199, "end_pos": 220, "type": "TASK", "confidence": 0.7922950983047485}]}, {"text": "We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages.", "labels": [], "entities": []}, {"text": "Our code and data sets are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language Modeling (LM) is a key NLP task, serving as an important component for applications that require some form of text generation, such as machine translation (, speech recognition (, dialogue generation (, or summarisation (.", "labels": [], "entities": [{"text": "Language Modeling (LM)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8376979351043701}, {"text": "text generation", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.7382237911224365}, {"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7715826332569122}, {"text": "speech recognition", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7457281053066254}, {"text": "dialogue generation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.7906493842601776}, {"text": "summarisation", "start_pos": 215, "end_pos": 228, "type": "TASK", "confidence": 0.9863691329956055}]}, {"text": "A traditional recurrent neural network (RNN) LM setup operates on a limited closed vocabulary of words (.", "labels": [], "entities": []}, {"text": "The limitation arises due to the model learning parameters exclusive to single words.", "labels": [], "entities": []}, {"text": "A standard training procedure for neural LMs gradually modifies the parameters based on contextual/distributional information: each occurrence of a word token in training data contributes to the estimate of a word vector (i.e., model parameters) assigned to this word type.", "labels": [], "entities": []}, {"text": "Low-frequency words therefore often have incorrect estimates, not having moved far from their random initialisation.", "labels": [], "entities": []}, {"text": "A common strategy for dealing with this issue is to simply exclude the low-quality parameters from the model (i.e., to replace them with the <unk> placeholder), leading to only a subset of the vocabulary being represented by the model.", "labels": [], "entities": []}, {"text": "This limited vocabulary assumption enables the model to bypass the problem of unreliable word estimates for low-frequency and unseen words, but it does not resolve it.", "labels": [], "entities": []}, {"text": "The assumption is far from ideal, partly due to the Zipfian nature of each language, and its limitation is even more pronounced for morphologically-rich languages (MRLs): these languages inherently generate a plethora of words by their morphological systems.", "labels": [], "entities": []}, {"text": "As a consequence, there will be a large number of words for which a standard RNN LM cannot guarantee a reliable word estimate.", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.7725202739238739}]}, {"text": "Since gradual parameter estimation based on contextual information is not feasible for rare phenomena in the full vocabulary setup, it is of crucial importance to construct and enable techniques that can obtain these parameters in alternative ways.", "labels": [], "entities": []}, {"text": "One solution is to draw information from additional sources, such as characters and character sequences.", "labels": [], "entities": []}, {"text": "As a consequence, such character-aware models should facilitate LM word-level prediction in a real-life LM setup which deals with a large amount of low-frequency or unseen words.", "labels": [], "entities": [{"text": "LM word-level prediction", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8811012705167135}]}, {"text": "Efforts into this direction have yielded exciting results, primarily on the input side of neural LMs.", "labels": [], "entities": []}, {"text": "A standard RNN LM architecture relies on two word representation matrices learned during training for its input and next-word prediction.", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.8580455780029297}]}, {"text": "This effectively means that there are two sets of per-word specific parameters that need to be trained.", "labels": [], "entities": []}, {"text": "Recent work shows that it is possible to generate a word representation on-the-fly based on its constituent characters, thereby effectively solving the problem for the parameter set on the input side of the model (.", "labels": [], "entities": []}, {"text": "However, it is not straightforward how to advance these ideas to the output side of the model, as this second set of word-specific parameters is directly responsible for the next-word prediction: it has to encode a much wider range of information, such as topical and semantic knowledge about words, which cannot be easily obtained from its characters alone (.", "labels": [], "entities": []}, {"text": "While one solution is to directly output characters instead of words, a recent work from suggests that such purely character-based architectures, which do not reserve parameters for information specific to single words, cannot attain state-ofthe-art LM performance on word-level prediction.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 268, "end_pos": 289, "type": "TASK", "confidence": 0.7137032449245453}]}, {"text": "In this work, we combine the two worlds and propose a novel LM approach which relies on both wordlevel (i.e., contextual) and subword-level knowledge.", "labels": [], "entities": []}, {"text": "In addition to training word-specific parameters for word-level prediction using a regular LM objective, our method encourages the parameters to also reflect subword-level patterns by injecting knowledge about morphology.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7771416306495667}]}, {"text": "This information is extracted in an unsupervised manner based on already available information in convolutional filters from earlier network layers.", "labels": [], "entities": []}, {"text": "The proposed method leads to large improvements in perplexity across a wide spectrum of languages: in Korean on our LM benchmarks.", "labels": [], "entities": []}, {"text": "We also show that the gains extend to another multilingual LM evaluation set, compiled recently for 7 languages by.", "labels": [], "entities": []}, {"text": "We conduct a systematic LM study on 50 typologically diverse languages, sampled to represent a variety of morphological systems.", "labels": [], "entities": []}, {"text": "We discuss the implications of typological diversity on the LM task, both theoretically in Section 2, and empirically in Section 7; we find a clear correspondence between performance of state-of-the art LMs and structural linguistic properties.", "labels": [], "entities": [{"text": "LM task", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9127624332904816}]}, {"text": "Further, the consistent perplexity gains across the large sample of languages suggest wide applicability of our novel method.", "labels": [], "entities": []}, {"text": "Finally, this article can also be read as a comprehensive multilingual analysis of current LM architectures on a set of languages which is much larger than the ones used in recent LM work.", "labels": [], "entities": []}, {"text": "We hope that this article with its new datasets, methodology and models, all available online at http://people.ds.cam.", "labels": [], "entities": []}, {"text": "ac.uk/dsg40/lmmrl.html, will pave the way for true multilingual research in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7219877690076828}]}], "datasetContent": [{"text": "Datasets We use the Polyglot Wikipedia (Al-Rfou et al., 2013) for all available languages except for Japanese, Chinese, and Thai, and add these and further languages using Wikipedia dumps.", "labels": [], "entities": [{"text": "Polyglot Wikipedia", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9503111243247986}]}, {"text": "The Wiki dumps were cleaned and preprocessed by the Polyglot tokeniser.", "labels": [], "entities": [{"text": "Wiki dumps", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8786077201366425}, {"text": "Polyglot tokeniser", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9649545550346375}]}, {"text": "We construct similarly-sized datasets by extracting 46K sentences for each language from the beginning of each dump, filtered to contain only full sentences, and split into train (40K), validation (3K), and test (3K).", "labels": [], "entities": []}, {"text": "The final list of languages along with standard language codes (ISO 639-1 standard, used throughout the paper) and statistics on vocabulary and token counts are provided in.", "labels": [], "entities": []}, {"text": "Evaluation Setup We report perplexity scores (Jurafsky and Martin, 2017, Chapter 4.2.1) using the full vocabulary of the respective LM dataset.", "labels": [], "entities": [{"text": "LM dataset", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.7211030274629593}]}, {"text": "This means that we explicitly decide to retain also infrequent words in the modeled data.", "labels": [], "entities": []}, {"text": "Replacing infrequent words by a placeholder token <unk> is a standard technique in LM to obtain equal vocabulary sizes across different datasets.", "labels": [], "entities": []}, {"text": "Motivated by the observation that infrequent words constitute a significant part of the vocabulary in MRLs, and that vocabulary sizes naturally differ between languages, we have decided to avoid the <unk> placeholder for low-frequency words, and run all models on the full vocabulary (.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 102, "end_pos": 106, "type": "TASK", "confidence": 0.9085468649864197}]}, {"text": "We believe that this full-vocabulary setup offers additional insight into the standard LM techniques, leading to evaluation which pinpoints crucial limitations of current word-based models with regard to morphologically-rich languages.", "labels": [], "entities": []}, {"text": "In our setup the vocabulary contains all words occurring at least once in the training set.", "labels": [], "entities": []}, {"text": "To ensure a fair comparison between all neural models, words occurring only in the test set are mapped to a random vector with the same technique for all neural models, as described next.", "labels": [], "entities": []}, {"text": "since it never occurs in the training data.", "labels": [], "entities": []}, {"text": "Therefore, we sample a random <unk> vector attest time from the same part of the space as the trained vectors, using a normal distribution with the mean and the variance of M wand the same fixed random seed for all models.", "labels": [], "entities": []}, {"text": "We employ this methodology for all neural LM models, and thereby ensure that results are comparable.", "labels": [], "entities": []}, {"text": "Training Setup and Parameters We reproduce the standard LM setup of and parameter choices of, with batches of 20 and a sequence length of 35, where one step corresponds to one token.", "labels": [], "entities": []}, {"text": "The maximum word length is chosen dynamically based on the longest word in the corpus.", "labels": [], "entities": []}, {"text": "The corpus is processed continuously, and the RNN hidden state resets occur at the beginning of each epoch.", "labels": [], "entities": []}, {"text": "Parameters are optimised with stochastic gradient descent.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9480375647544861}]}, {"text": "The gradient is averaged over the batch size and sequence length.", "labels": [], "entities": []}, {"text": "We then scale the averaged gradient by the sequence length (=35) and clip to 5.0 for more stable training.", "labels": [], "entities": [{"text": "clip", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9913836717605591}]}, {"text": "The learning rate is 1.0, decayed by 0.5 after each epoch if the validation perplexity does not improve.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.962897002696991}]}, {"text": "We train all models for 15 epochs on the smaller corpora, and for 30 on the large ones, which is typically sufficient for model convergence.", "labels": [], "entities": [{"text": "model convergence", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.6979939043521881}]}, {"text": "Our AP fine-tuning method operates on the whole M w space, but we only allow words more frequent than 5 as cue words x w (see Section 5 again), while there are no restrictions on x p and x n . 3 Our preliminary analysis on the influence of the number of nearest neighbours in M c shows that this parameter has only a moderate effect on the final LM scores.", "labels": [], "entities": []}, {"text": "We thus fix it to 3 positive and 3 negative samples for each x w without any tuning.", "labels": [], "entities": []}, {"text": "AP is optimised with Adagrad (Duchi et al., 2011) and a learning rate of 0.05, the gradients are clipped to \u00b12. 4 A full summary of all hyper-parameters and their values is provided in.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9763525128364563}, {"text": "Adagrad (Duchi et al., 2011)", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.854749783873558}, {"text": "learning rate", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.973826676607132}]}, {"text": "As mentioned, the traditional LM setup is to use words both on the input and on the output side) relying on n-gram word sequences.", "labels": [], "entities": []}, {"text": "We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5-grams with extended Kneser-Ney smoothing (KN5) ( . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages.", "labels": [], "entities": []}, {"text": "Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (, i.a.).", "labels": [], "entities": []}, {"text": "These LMs map a sequence of input words to embedding vectors using a look-up matrix.", "labels": [], "entities": []}, {"text": "The embeddings are passed to the LSTM as input, and the model is trained in an autoregressive fashion to predict the next word from the pre-defined vocabulary given the current context.", "labels": [], "entities": []}, {"text": "As a strong baseline from this LM family, we train a standard LSTM LM (LSTM-Word) relying on the setup from (see).", "labels": [], "entities": []}, {"text": "Finally, a recent strand of LM work uses characters on the input side while retaining word-level prediction on the output side.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.6638582199811935}]}, {"text": "A representative architecture from this group, also serving as the basis in our work (Section 3), is Char CNN-LSTM (.", "labels": [], "entities": [{"text": "Char CNN-LSTM", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.6959726810455322}]}, {"text": "All neural models operate on exactly the same vocabulary and treat out-of-vocabulary (OOV) words in exactly the same way.", "labels": [], "entities": []}, {"text": "As mentioned, we include KN5 as a strong (non-neural) baseline to give perspective on how this more traditional model performs across 50 typologically diverse languages.", "labels": [], "entities": []}, {"text": "We have selected the setup for the KN5 model to be as close as possible to that of neural LMs, However, due to the different nature of the models, we note that the results between KN5 and other models are not comparable.", "labels": [], "entities": []}, {"text": "In KN5 discounts are added for low-frequency words, and unseen words attest time are regarded as outliers and assigned low probability estimates.", "labels": [], "entities": []}, {"text": "In contrast, for all neural models we sample unseen word vectors to lie in the space of trained vectors (see before).", "labels": [], "entities": []}, {"text": "We find the latter setup to better reflect our intuition that especially in MRLs unseen words are not outliers but often arise due to morphological complexity.", "labels": [], "entities": [{"text": "MRLs unseen words", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8795438607533773}]}], "tableCaptions": [{"text": " Table 4: Test perplexities for 50 languages (ISO 639-1 codes sorted alphabetically) in the full-vocabulary  prediction LM setup; Left: Basic statistics of our evaluation data. Middle: Results with the Baseline LMs.", "labels": [], "entities": []}, {"text": " Table 5.  Significance tests show p-values < 1 \u22123 for all combi- nations of models and independent variables, demon- strating all of them are good performance predictors.  Our main finding indicates that linguistic categories  and data statistics both correlate well (\u2248 0.35 and  \u2248 0.82, respectively) with the performance of lan- guage models.", "labels": [], "entities": []}, {"text": " Table 5: Correlations between model performance and language typology as well as with corpus statistics  (type/token ratio and new word types in test data). All variables are good performance predictions.", "labels": [], "entities": []}, {"text": " Table 6: Results on the larger MWC data set (Kawakami et al., 2017) and on a subset of the Europarl (EP)  corpus. Improvements with +AP are not dependent on corpus size, but rather they strongly correlate with the  type/token ratio of the corpus.", "labels": [], "entities": [{"text": "MWC data set", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8339914282162985}, {"text": "Europarl (EP)  corpus", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.9478892922401428}]}, {"text": " Table 7: Comparison of type/token ratios in the cor- pora used for evaluation. The ratio is not dependent  only on the corpus size but also on the language and  domain of the corpus.", "labels": [], "entities": []}, {"text": " Table 8: Results on German with data sets of comparable size and increasing type/token ratio.", "labels": [], "entities": []}, {"text": " Table 8. The  AP method is especially helpful for corpora with high  type/token ratios.", "labels": [], "entities": [{"text": "AP", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.6578454971313477}]}]}