{"title": [{"text": "Surface Statistics of an Unknown Language Indicate How to Parse It", "labels": [], "entities": [{"text": "Parse It", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.8298183381557465}]}], "abstractContent": [{"text": "We introduce a novel framework for delex-icalized dependency parsing in anew language.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7143538147211075}]}, {"text": "We show that useful features of the target language can be extracted automatically from an unparsed corpus, which consists only of gold part-of-speech (POS) sequences.", "labels": [], "entities": []}, {"text": "Providing these features to our neural parser enables it to parse sequences like those in the corpus.", "labels": [], "entities": []}, {"text": "Strikingly, our system has no supervision in the target language.", "labels": [], "entities": []}, {"text": "Rather, it is a multilingual system that is trained end-to-end on a variety of other languages, so it learns a feature extractor that works well.", "labels": [], "entities": []}, {"text": "We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 105, "end_pos": 112, "type": "TASK", "confidence": 0.9692463278770447}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9745324850082397}]}, {"text": "(2) Including thousands of synthetic languages in the training yields further improvement.", "labels": [], "entities": []}, {"text": "(3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work's interpretable typolog-ical features that require parsed corpora or expert categorization of the language.", "labels": [], "entities": []}, {"text": "Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past \"grammar induction\" work that does not use training languages (Naseem et al., 2010).", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing is one of the core natural language processing tasks.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9056456089019775}, {"text": "natural language processing", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6389452914396921}]}, {"text": "It aims to parse a given sentence into its dependency tree: a directed graph of labeled syntactic relations between words.", "labels": [], "entities": []}, {"text": "Supervised dependency parsers-which are trained using a \"treebank\" of known parses in the target language-have been very successful.", "labels": [], "entities": []}, {"text": "By contrast, the progress of unsupervised dependency parsers has been slow, and they have apparently not been used in any downstream NLP systems).", "labels": [], "entities": []}, {"text": "An unsupervised parser does not have access to a treebank, but only to a corpus of unparsed sentences in the target language.", "labels": [], "entities": []}, {"text": "Unsupervised parsing has been studied for decades.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.8824326992034912}]}, {"text": "The most common approach is grammar induction).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8535690903663635}]}, {"text": "Grammar induction induces an explicit grammar from the unparsed corpus, such as a probabilistic context-free grammar (PCFG), and uses that to parse sentences of the language.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7050254642963409}]}, {"text": "This approach has encountered two major difficulties: \u2022 Search error: Most formulations of grammar induction involve optimizing a highly non-convex objective function such as likelihood.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7911114692687988}]}, {"text": "The optimization is typically NP-hard, and approximate local search methods tend to get stuck in local optima.", "labels": [], "entities": []}, {"text": "\u2022 Model error: Likelihood does not correlate well with parsing accuracy anyway (Smith, 2006,.2).", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9634175896644592}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.8053951263427734}]}, {"text": "Likelihood optimization seeks latent trees that help to predict the observed sentences, but these unsupervised trees may use a non-standard syntactic analysis or even be optimized to predict nonsyntactic properties such as topic.", "labels": [], "entities": [{"text": "Likelihood optimization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.936680018901825}]}, {"text": "We seek a standard syntactic analysis-what Smith (2006) calls the MATCHLINGUIST task.", "labels": [], "entities": [{"text": "MATCHLINGUIST", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.6818940043449402}]}, {"text": "We address both difficulties by using a supervised learning framework-one whose objective function is easier to optimize and explicitly tries to match linguists' standard syntactic analyses.", "labels": [], "entities": []}, {"text": "Our approach is inspired by, who use an unparsed but tagged corpus to predict the fine-grained syntactic typology of a language.", "labels": [], "entities": []}, {"text": "For example, they may predict that about 70% of the direct objects fall to the right of the verb.", "labels": [], "entities": []}, {"text": "Their system is trained on a large number of (unparsed corpus, true typology) pairs, each representing a different language.", "labels": [], "entities": []}, {"text": "With this training, it can generalize to predict typology from the unparsed corpus of anew language.", "labels": [], "entities": []}, {"text": "Our approach is similar except that we predict parses rather than just a typology.", "labels": [], "entities": []}, {"text": "In both cases, the system is trained to optimize a task-specific quality measure.", "labels": [], "entities": []}, {"text": "The system's parameterization can be chosen to simplify optimization (strikingly, the training objective could even be made convex by using a conditional random field architecture) and/or to incorporate linguistically motivated features.", "labels": [], "entities": []}, {"text": "The positive results of demonstrate that there are indeed surface clues to syntactic structure in the input corpus, at least if it is POS-tagged (as in their work and ours).", "labels": [], "entities": []}, {"text": "However, their method only found global typological information: it did not establish which 70% of the direct objects fell to the right of their verbs, let alone identify which nouns were in fact direct objects of which verbs.", "labels": [], "entities": []}, {"text": "That requires a token-level analysis of each sentence, which we undertake in this paper.", "labels": [], "entities": []}, {"text": "Again, the basic idea is that instead of predicting interpretable typological properties of a language as did, we will predict a language-specific version of the scoring function that a parser uses to choose among various actions or substructures.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all previous sections, we evaluated on the 16 languages in the training set by cross-validation.", "labels": [], "entities": []}, {"text": "For the final test, we combine all the 20 treebanks and train the system with the hyperparameters given in \u00a79.5, then test on the 15 unseen test languages.", "labels": [], "entities": []}, {"text": "displays results on these 15 test languages (top) as well as the cross-validation results on the 16 languages (bottom).", "labels": [], "entities": []}, {"text": "We see that we improve significantly over baseline on almost every language.", "labels": [], "entities": []}, {"text": "Indeed, on the test languages, +T(u) improves both UAS and LAS by > 3.5 percentage points on average.", "labels": [], "entities": [{"text": "T", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9752878546714783}, {"text": "UAS", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.5808548927307129}, {"text": "LAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8948785662651062}]}, {"text": "The improvement grows to > 5.6 if we augment the training data as well (+GD, meaning +T(u)+GD).", "labels": [], "entities": [{"text": "GD", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9424546360969543}, {"text": "T(u)+GD)", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.7352128227551779}]}, {"text": "One disappointment concerns the added benefit on the LAS of +GD over just +T(u): while this data augmentation helped significantly on nearly everyone of the 16 development languages, it produced less consistent improvements on the test languages and hurt some of them.", "labels": [], "entities": [{"text": "LAS", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9937784075737}]}, {"text": "We suspect that this is because we tuned the hyperparameters to maximize UAS, not LAS ( \u00a79.2).", "labels": [], "entities": [{"text": "UAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.8866747617721558}, {"text": "LAS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9904775619506836}]}, {"text": "As a result, while the average benefit across our 15 test languages was fairly large, this sample was not large enough to establish that it was significantly greater from 0, that is, that future test languages would also see an improvement from data augmentation.", "labels": [], "entities": []}, {"text": "We also notice that there seems to be a small difference between the pattern of results on development versus test languages.", "labels": [], "entities": []}, {"text": "This may simply reflect overfitting to the development languages, but we also note that the test languages (chosen by  Wang and Eisner) tended to have considerably smaller unparsed corpora u, so there maybe a domain mismatch problem.", "labels": [], "entities": []}, {"text": "To ameliorate this problem, one could include training examples with versions of u that are truncated to lengths seen in test data (cf.).", "labels": [], "entities": []}, {"text": "One could also include the size |u| explicitly in T(u).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average parsing results over 16 languages,  computed by 5-fold cross-validation. We compare  training on real languages only (the UD column) ver- sus augmenting with synthetic languages at \u03b2 = 0.2  (the +GD column). Baseline is the ablated system that  omits T(u) ( \u00a79.2). SST is the single-source transfer  approach ( \u00a75.2). H and N use only hand-engineered  features or neural features, while H;N defines \u03c0(u)  to concatenate both ( \u00a76.1) and H+N is the product-of- experts model ( \u00a77). T D and T W that incorporate oracle  knowledge of the target-language syntax ( \u00a79.4). For  each comparison between UD and +GD, we boldface  the better (higher) result, or both if they are not signifi- cantly different (paired permutation test over languages  with p < 0.05). In each column, we star the best result  as well as all results that are not significantly worse.", "labels": [], "entities": []}, {"text": " Table 1.) The x-axis gives the  average accuracy of the trained RDRPOSTagger. The  y-axis gives the average parsing performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992864727973938}, {"text": "parsing", "start_pos": 109, "end_pos": 116, "type": "TASK", "confidence": 0.9543920755386353}]}]}