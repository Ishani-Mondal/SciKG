{"title": [], "abstractContent": [{"text": "Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities , and their relations across a full document.", "labels": [], "entities": [{"text": "Reading comprehension (RC)-", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7847489595413208}, {"text": "information retrieval-requires integrating information and reasoning about events, entities , and their relations across a full document", "start_pos": 42, "end_pos": 178, "type": "Description", "confidence": 0.7102523081832461}]}, {"text": "Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9040654003620148}]}, {"text": "However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC.", "labels": [], "entities": []}, {"text": "To encourage progress on deeper comprehension of language, we present anew dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts.", "labels": [], "entities": []}, {"text": "These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.", "labels": [], "entities": []}, {"text": "We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here.", "labels": [], "entities": []}, {"text": "We provide an analysis of the dataset and the challenges it presents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding seeks to create models that read and comprehend text.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6811129152774811}]}, {"text": "A common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read, akin to how reading comprehension is tested in children when they are learning to read.", "labels": [], "entities": []}, {"text": "After reading a document, a reader usually cannot reproduce", "labels": [], "entities": []}], "datasetContent": [{"text": "There area large number of datasets and associated tasks available for the training and evaluation of read-1 http://deepmind.com/publications 2 For example, new names and words maybe coined by the author (e.g. \"muggle\" in Harry Potter novels) but the reader need only appeal to the book itself to understand the meaning of these concepts, and their place in the narrative.", "labels": [], "entities": []}, {"text": "This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension, and is in part tested as part of the question answering tasks we present.", "labels": [], "entities": [{"text": "question answering", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7117983549833298}]}, {"text": "We summarize the key features of a collection of popular recent datasets in.", "labels": [], "entities": []}, {"text": "In this section, we briefly discuss the nature and limitations of these datasets and their associated tasks.", "labels": [], "entities": []}, {"text": "MCTest () is a collection of short stories, each with multiple questions.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8991923928260803}]}, {"text": "Each such question has set of possible answers, one of which is labelled as correct.", "labels": [], "entities": []}, {"text": "While this could be used as a QA task, the MCTest corpus is in fact intended as an answer selection corpus.", "labels": [], "entities": [{"text": "MCTest corpus", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.873990535736084}, {"text": "answer selection corpus", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8172071178754171}]}, {"text": "The data is human generated, and the answers can be phrases or sentences.", "labels": [], "entities": []}, {"text": "The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size.", "labels": [], "entities": []}, {"text": "In contrast, CNN/Daily Mail (), Children's Book Test (CBT) (, and BookTest ( ) each provide large amounts of question-answer pairs.", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.8201119601726532}, {"text": "Children's Book Test (CBT)", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.8522210121154785}, {"text": "BookTest", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9055312871932983}]}, {"text": "Questions are Cloze-form (predict the missing word) and are produced from either short abstractive summaries (CNN/Daily Mail) or from the next sentence in the document the context was taken from (CBT and BookTest).", "labels": [], "entities": [{"text": "CNN/Daily Mail)", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.9215801358222961}, {"text": "CBT", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.9078693985939026}, {"text": "BookTest", "start_pos": 204, "end_pos": 212, "type": "DATASET", "confidence": 0.5547682642936707}]}, {"text": "The tasks associated with these datasets are all selecting an answer from a set of options, which is explicitly provided for CBT and BookTest, and is implicit for CNN/Daily Mail, as the answers are always entities from the document.", "labels": [], "entities": [{"text": "CBT", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9288919568061829}, {"text": "BookTest", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.8574631810188293}, {"text": "CNN/Daily Mail", "start_pos": 163, "end_pos": 177, "type": "DATASET", "confidence": 0.8950363397598267}]}, {"text": "This significantly favors models that operate by pointing to a particular token (or type).", "labels": [], "entities": []}, {"text": "Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) ( , exploit precisely this bias in the data.", "labels": [], "entities": [{"text": "Attention Sum Reader (AS Reader", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6520936240752538}]}, {"text": "However, these models are inappropriate for answers requiring synthesis of anew answer.", "labels": [], "entities": []}, {"text": "This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.8933091163635254}]}, {"text": "Stanford Question Answering Dataset (SQuAD) () and NewsQA () offer a different challenge.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 0, "end_pos": 43, "type": "DATASET", "confidence": 0.8550485713141305}, {"text": "NewsQA", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9617905020713806}]}, {"text": "A large number of questions and answers are provided fora set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words from the document.", "labels": [], "entities": []}, {"text": "Although the answers are not just single word/entity answers, many plausible questions for assessing RC cannot be asked because no document span would contain its answer.", "labels": [], "entities": [{"text": "assessing RC", "start_pos": 91, "end_pos": 103, "type": "TASK", "confidence": 0.5969692170619965}]}, {"text": "While they provide a large number of questions, these are from a relatively small number of documents, which are themselves fairly short, thereby limiting the lexical and topical diversity of models trained on this data.", "labels": [], "entities": []}, {"text": "While the answers are multi-word phrases, the spans are generally short and rarely cross sentence boundaries.", "labels": [], "entities": []}, {"text": "Simple models scoring and/or extracting candidate spans conditioned on the question and superficial signal from the rest of the document do well, e.g.,.", "labels": [], "entities": []}, {"text": "These models will not trivially generalize to problems where the answers are not spans in the document, supervision for spans is not provided, or several discontinuous spans are needed to generate a correct answer.", "labels": [], "entities": []}, {"text": "This restricts the scalability and applicability of models doing well on SQuAD or NewsQA to more complex problems.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.6892070174217224}]}, {"text": "MS MARCO dataset (Nguyen et al., 2016) presents a bolder challenge: questions are paired with sets of snippets (\"context passages\") that contain the information necessary to answer the question and answers are free-form human generated text.", "labels": [], "entities": [{"text": "MS MARCO dataset", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8972723285357157}]}, {"text": "However, as no restriction was placed on annotators to prevent them from copying answers from source documents, many answers are in fact verbatim copies of short spans from the context passages.", "labels": [], "entities": []}, {"text": "Models that do well on SQuAD (e.g.,,), extracting spans or pointing, do well here too, and the same concerns about the general applicability of solutions to this particular dataset to larger reading comprehension problems apply here also, as above.", "labels": [], "entities": []}, {"text": "SearchQA () is a recent dataset in which the context for each question is a set of documents retrieved by a search engine using the question as the query.", "labels": [], "entities": []}, {"text": "However, in contrast with previous datasets neither questions nor answers were produced by annotating the context documents, but rather the context documents were retrieved after collecting pre-existing question-answer pairs.", "labels": [], "entities": []}, {"text": "As such, it is not open to same annotation bias as the datasets discussed above.", "labels": [], "entities": []}, {"text": "However, upon examining answers in the Jeopardy data used to construct this dataset, one finds that 80% of answers are bigrams or unigrams, and 99% are 5 tokens or fewer.", "labels": [], "entities": [{"text": "Jeopardy data", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9392867684364319}]}, {"text": "Of a sample of 100 answers, 72% are named entities, all are short noun-phrases.", "labels": [], "entities": []}, {"text": "We see several limitations of the scope and depth of the RC problems in existing datasets.", "labels": [], "entities": []}, {"text": "First, several datasets are small (MCTest) or not overly naturalistic (bAbI ().", "labels": [], "entities": [{"text": "bAbI", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8796631693840027}]}, {"text": "Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering).", "labels": [], "entities": []}, {"text": "This, we suspect, is largely an artifact of the question generation methodology, in which annotators have created questions from a context document, or where context documents that explicitly answer a question are iden-319 tified using a search engine.", "labels": [], "entities": [{"text": "question generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7621260285377502}]}, {"text": "Although the factoid-like Jeopardy questions of SearchQA also appear to favor questions answerable with local context.", "labels": [], "entities": []}, {"text": "Finally, we see further evidence of the superficiality of the questions in the architectures that have evolved to solve them, which tend to exploit span selection based on representations derived from local context and the query ().", "labels": [], "entities": []}, {"text": "In this section, we introduce our new dataset, NarrativeQA, which addresses many of the limitations identified in existing datasets.", "labels": [], "entities": []}, {"text": "In this section, we describe the data preparation methodology we used, and the experimental results on the summary-reading task as well as the full story task.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7539381086826324}]}], "tableCaptions": [{"text": " Table 2: NarrativeQA dataset statistics.", "labels": [], "entities": []}, {"text": " Table 3: Frequency of first  token of the question in the  training set.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9788683652877808}]}, {"text": " Table 4: Question categories  on a sample of 300 questions  from the validation set.", "labels": [], "entities": []}, {"text": " Table 5: Experiments on summaries. Higher is better for all metrics. Sections 4.1 and 4.2 explain the IR and neural  models, respectively.", "labels": [], "entities": []}, {"text": " Table 6: Experiments on full stories. Each chunk contains 200 tokens. Higher is better for all metrics. Sections 4.1  and 4.2 explain the IR and neural models, respectively. Note that the human scores are based on answering questions  given summaries, same as in", "labels": [], "entities": []}]}