{"title": [{"text": "Learning to Remember Translation History with a Continuous Cache", "labels": [], "entities": [{"text": "Learning to Remember Translation History", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6865271091461181}]}], "abstractContent": [{"text": "Existing neural machine translation (NMT) models generally translate sentences in isolation , missing the opportunity to take advantage of document-level information.", "labels": [], "entities": []}, {"text": "In this work, we propose to augment NMT models with a very lightweight cache-like memory network, which stores recent hidden representations as translation history.", "labels": [], "entities": []}, {"text": "The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt overtime.", "labels": [], "entities": []}, {"text": "Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has advanced the state of the art in recent years (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.847756584485372}]}, {"text": "However, existing models generally treat documents as a list of independent sentence pairs and ignore cross-sentence information, which leads to translation inconsistency and ambiguity arising from a single source sentence.", "labels": [], "entities": []}, {"text": "There have been few recent attempts to model cross-sentence context for NMT: use a hierarchical RNN to summarize the previous K source sentences, while use an additional set of an encoder and attention model to dynamically select part of the previous source sentence.", "labels": [], "entities": []}, {"text": "While these approaches have proven their ability to represent cross-sentence context, they generate the context from discrete lexicons, thus would cause errors propagated from generated translations.", "labels": [], "entities": []}, {"text": "Accordingly, they only take into account source sentences but fail to make use of target-side information.", "labels": [], "entities": []}, {"text": "Another potential limitation is that they are computationally expensive, which limits the scale of cross-sentence context.", "labels": [], "entities": []}, {"text": "In this work, we propose a very light-weight alternative that can both cover large-scale cross-sentence context as well as exploit bilingual translation history.", "labels": [], "entities": []}, {"text": "Our work is inspired by recent successes of memory-augmented neural networks on multiple NLP tasks (, especially the efficient cache-like memory networks for language modeling (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7222196608781815}]}, {"text": "Specifically, the proposed approach augments NMT models with a continuous cache (CACHE), which stores recent hidden representations as history context.", "labels": [], "entities": []}, {"text": "By minimizing the computation burden of the cache-like memory, we are able to use larger memory and scale to longer translation history.", "labels": [], "entities": []}, {"text": "Since we leverage internal representations instead of output words, our approach is more robust to the error propagation problem, and thus can incorporate useful target-side context.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.702877551317215}]}, {"text": "Experimental results show that the proposed approach significantly and consistently improves translation performance over a strong NMT baseline on multiple domains with different topics and styles.", "labels": [], "entities": [{"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9539896845817566}]}, {"text": "We found the introduced cache is able to remember translation patterns at different levels of matching and granularity, ranging from exactly matched lexi-cal patterns to fuzzily matched patterns, from wordlevel patterns to phrase-level patterns.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of sentences (|S|) and words (|W |). K stands for thousands and M for millions.", "labels": [], "entities": []}, {"text": " Table 2: Translation performances of different cache  sizes on the tuning sets.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9742745161056519}]}, {"text": " Table 3: Effect of the cache overwrite mechanism  for slots that correspond to the same target word.", "labels": [], "entities": []}, {"text": " Table 4: Translation qualities on multiple domains. \"*\" indicates statistically significant difference (p <  0.01) from\"BASE\" , and \"\" denotes relative improvement over \"BASE\".", "labels": [], "entities": [{"text": "BASE", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.8939817547798157}, {"text": "BASE", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.8813596367835999}]}, {"text": " Table 5: Model complexity. \"Speed\" is measured in  words/second for both training and testing. We em- ploy a beam search with beam being 10 for testing.", "labels": [], "entities": [{"text": "Speed\"", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9340440034866333}]}, {"text": " Table 6: Comparison of shallow fusion (i.e., words  as cache values) and deep fusion (i.e., continuous  vectors as cache values) in the News domain.", "labels": [], "entities": []}]}