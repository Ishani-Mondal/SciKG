{"title": [{"text": "Modeling Past and Future for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6964555581410726}]}], "abstractContent": [{"text": "Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a novel mechanism that separates the source information into two parts: translated PAST contents and untranslated FUTURE contents, which are modeled by two additional recurrent layers.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.899647057056427}]}, {"text": "The PAST and FUTURE contents are fed to both the attention model and the de-coder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9669608473777771}, {"text": "Neural Machine Translation (NMT)", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.8272585173447927}]}, {"text": "Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks.", "labels": [], "entities": [{"text": "English-German translation tasks", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.7509106794993082}]}, {"text": "Specifically , the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) generally adopts an encoder-decoder framework, where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8768922289212545}]}, {"text": "During translation, the decoder implicitly serves several functionalities at the same time: 1.", "labels": [], "entities": []}, {"text": "Building a language model over the target sentence for translation fluency (LM).", "labels": [], "entities": [{"text": "translation fluency (LM)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.9200377583503723}]}, {"text": "2. Acquiring the most relevant source-side information to generate the current target word (PRESENT).", "labels": [], "entities": [{"text": "PRESENT", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8943743705749512}]}, {"text": "3. Maintaining what parts in the source have been translated (PAST) and what parts have not (FUTURE).", "labels": [], "entities": [{"text": "Maintaining", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9828837513923645}, {"text": "PAST", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9907750487327576}, {"text": "FUTURE", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9980692267417908}]}, {"text": "However, it maybe difficult fora single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously.", "labels": [], "entities": []}, {"text": "A recent successful extension of NMT models is the attention mechanism (, which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state.", "labels": [], "entities": []}, {"text": "In this sense, the attention mechanism separates the PRESENT functionality from the decoder RNN, achieving significant performance improvement.", "labels": [], "entities": []}, {"text": "In addition to PRESENT, we address the importance of modeling PAST and FUTURE contents in machine translation.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9333575367927551}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7288030385971069}]}, {"text": "The PAST contents indicate translated information, whereas the FUTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (.", "labels": [], "entities": [{"text": "PAST", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8760421276092529}, {"text": "FUTURE", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9939022064208984}]}, {"text": "Ideally, PAST grows and FUTURE declines during the translation process.", "labels": [], "entities": [{"text": "PAST", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9208856225013733}, {"text": "FUTURE", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.998981773853302}, {"text": "translation process", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8936642110347748}]}, {"text": "However, it maybe difficult fora single RNN to explicitly model the above processes.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural machine translation system that explicitly models PAST and FUTURE contents with two additional RNN layers.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7626111706097921}, {"text": "FUTURE", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9137595891952515}]}, {"text": "The RNN modeling the PAST contents (called PAST layer) starts from scratch and accumulates the in-formation that is being translated at each decoding step (i.e., the PRESENT information yielded by attention).", "labels": [], "entities": []}, {"text": "The RNN modeling the FUTURE contents (called FUTURE layer) begins with holistic source summarization, and subtracts the PRESENT information at each step.", "labels": [], "entities": []}, {"text": "The two processes are guided by proposed auxiliary objectives.", "labels": [], "entities": []}, {"text": "Intuitively, the RNN state of the PAST layer corresponds to source contents that have been translated at a particular step, and the RNN state of the FUTURE layer corresponds to source contents of untranslated words.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.7141627073287964}]}, {"text": "At each decoding step, PAST and FUTURE together provide a full summarization of the source information.", "labels": [], "entities": [{"text": "PAST", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.931655764579773}, {"text": "FUTURE", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9933862090110779}]}, {"text": "We then feed the PAST and FUTURE information to both the attention model and decoder states.", "labels": [], "entities": [{"text": "PAST", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.5438434481620789}, {"text": "FUTURE", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9851685762405396}]}, {"text": "In this way, our proposed mechanism not only provides coverage information for the attention model, but also gives a holistic view of the source information at each time.", "labels": [], "entities": []}, {"text": "We conducted experiments on Chinese-English, German-English, and English-German benchmarks.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed mechanism yields 2.7, 1.7, and 1.1 improvements of BLEU scores in three tasks, respectively.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9786223769187927}]}, {"text": "In addition, it obtains an alignment error rate of 35.90%, significantly lower than the baseline (39.73%) and the coverage model (38.73%) by.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.8785990277926127}, {"text": "coverage", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9866132140159607}]}, {"text": "We observe that in traditional attention-based NMT, most errors occur due to over-and under-translation, which is probably because the decoder RNN fails to keep track of what has been translated and what has not.", "labels": [], "entities": []}, {"text": "Our model can alleviate such problems by explicitly modeling PAST and FUTURE contents.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7466700077056885}]}], "datasetContent": [{"text": "We conduct experiments on ChineseEnglish (Zh-En), German-English (De-En), and English-German (En-De) translation tasks.", "labels": [], "entities": [{"text": "English-German (En-De) translation tasks", "start_pos": 78, "end_pos": 118, "type": "TASK", "confidence": 0.7187910477320353}]}, {"text": "For Zh-En, the training set consists of 1.6m sentence pairs, which are extracted from the LDC corpora 3 . The NIST 2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets.", "labels": [], "entities": [{"text": "NIST 2003 (MT03) dataset", "start_pos": 110, "end_pos": 134, "type": "DATASET", "confidence": 0.9351856211821238}, {"text": "NIST 2002 (MT02)", "start_pos": 163, "end_pos": 179, "type": "DATASET", "confidence": 0.9110609292984009}, {"text": "MT06) datasets", "start_pos": 213, "end_pos": 227, "type": "DATASET", "confidence": 0.8630454341570536}]}, {"text": "We also evaluate the alignment performance on the standard benchmark of, which contains 900 manually aligned sentence pairs.", "labels": [], "entities": []}, {"text": "We measure the alignment quality with the alignment error rate.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.8996828397115072}]}, {"text": "For De-En and En-De, we conduct experiments on the WMT17 ( corpus.", "labels": [], "entities": [{"text": "WMT17 ( corpus.", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.8831626623868942}]}, {"text": "The dataset consists of 5.6M sentence pairs.", "labels": [], "entities": []}, {"text": "We use newstest2016 as our development set, and newstest2017 as our testset.", "labels": [], "entities": []}, {"text": "We follow to segment both German and English words into subwords using byte-pair encoding (.", "labels": [], "entities": []}, {"text": "We measure the translation quality with BLEU scores ().", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9516048431396484}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9993566870689392}]}, {"text": "We use the multi-bleu script for Zh-En 4 , and the multi-bleu-detok script for De-En and En-De), implementing a baseline translation system, RNNSEARCH.", "labels": [], "entities": []}, {"text": "For Zh-En, we limit the vocabulary size to 30K.", "labels": [], "entities": []}, {"text": "For De-En and En-De, the number of joint BPE operations is 90,000.", "labels": [], "entities": []}, {"text": "We use the total BPE vocabulary for each side.", "labels": [], "entities": [{"text": "BPE vocabulary", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.5984172970056534}]}, {"text": "We tie the weights of the target-side embeddings and the output weight matrix for De-En.", "labels": [], "entities": []}, {"text": "All out-of-vocabulary words are mapped to a special token UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8171497583389282}]}, {"text": "We train each model with sentences lengths of up to 50 words in the training data.", "labels": [], "entities": []}, {"text": "The dimension of word embeddings is 512, and all hidden sizes are 1024.", "labels": [], "entities": []}, {"text": "In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De.", "labels": [], "entities": [{"text": "ZhEn", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8479253649711609}]}, {"text": "We set the beam size to 12 in testing.", "labels": [], "entities": []}, {"text": "We shuffle the training corpus after each epoch.", "labels": [], "entities": []}, {"text": "We use Adam () with annealing as our optimization algorithm.", "labels": [], "entities": []}, {"text": "We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease.", "labels": [], "entities": []}, {"text": "For the proposed model, we use the same setting as the baseline model.", "labels": [], "entities": []}, {"text": "The FUTURE and PAST layer sizes are 1024.", "labels": [], "entities": [{"text": "FUTURE", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9043074250221252}, {"text": "PAST", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.8483965396881104}]}, {"text": "We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated.", "labels": [], "entities": []}, {"text": "Model parameters shared with the baseline are initialized by the baseline model.", "labels": [], "entities": []}, {"text": "Following, we conduct subjective evaluations to validate the benefit of modeling the PAST and the FUTURE.", "labels": [], "entities": [{"text": "PAST", "start_pos": 85, "end_pos": 89, "type": "TASK", "confidence": 0.456870436668396}, {"text": "FUTURE", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.8826050758361816}]}, {"text": "Four human evaluators are asked to evaluate the translations of 100 source sentences, which are randomly sampled from the testsets without knowing from which system the translation is selected.", "labels": [], "entities": []}, {"text": "For the BASE system, 1.7% of the source words are over-translated and 8.8% are under-translated.", "labels": [], "entities": [{"text": "BASE", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.4365037679672241}]}, {"text": "Our proposed model alleviates these problems by explicitly modeling the dynamic: Results of De-En and En-De \"synthetic data\" denotes additional 10M monolingual sentences, which is not used in our work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evidence shows that attention-based NMT  fails to make full use of source information, thus los- ing the holistic picture of source contents.", "labels": [], "entities": []}, {"text": " Table 2: Case-insensitive BLEU on Chinese-English Translation. \"LOSS\" means applying loss functions  for FUTURE layer (FRNN) and PAST layer (PRNN).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9464324116706848}, {"text": "LOSS", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9930157661437988}, {"text": "FUTURE", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9649598002433777}]}, {"text": " Table 5: Results of De-En and En-De \"synthetic data\" denotes additional 10M monolingual sentences,  which is not used in our work.", "labels": [], "entities": []}, {"text": " Table 6: Statistics of parameters, training and testing  speeds (sentences per second).", "labels": [], "entities": []}, {"text": " Table 7: Contributions of loss functions from param- eter training (\"Train\") and reranking of candidates in  testing (\"Test\").", "labels": [], "entities": []}, {"text": " Table 8: Influence of initialization of FRNN  layer (GRU-i)", "labels": [], "entities": []}]}