{"title": [], "abstractContent": [{"text": "In this work, we propose anew language mod-eling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models.", "labels": [], "entities": []}, {"text": "These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters.", "labels": [], "entities": []}, {"text": "This approach allows us to seamlessly incorporate linguistic intuitions-including polysemy and the existence of multi-word lexical items-into our language model.", "labels": [], "entities": []}, {"text": "Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chi-nese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network models have recently contributed towards a great amount of progress in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6355392237504324}]}, {"text": "These models typically share a common backbone: recurrent neural networks (RNN), which have proven themselves to be capable of tackling a variety of core natural language processing tasks.", "labels": [], "entities": []}, {"text": "One such task is language modeling, in which we estimate a probability distribution over sequences of tokens that corresponds to observed sentences ( \u00a72).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7599374353885651}]}, {"text": "Neural language models, particularly models conditioned on a particular input, have many applications including in machine translation (, abstractive summarization (, and speech processing (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8372251987457275}, {"text": "abstractive summarization", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.6483953893184662}, {"text": "speech processing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7020261436700821}]}, {"text": "Similarly, state-of-the-art language models are almost universally based on RNNs, particularly long short-term memory (LSTM) networks.", "labels": [], "entities": []}, {"text": "While powerful, LSTM language models usually do not explicitly model many commonly-accepted linguistic phenomena.", "labels": [], "entities": []}, {"text": "As a result, standard models lack linguistically informed inductive biases, potentially limiting their accuracy, particularly in lowdata scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9967731833457947}]}, {"text": "In this work, we present a novel modification to the standard LSTM language modeling framework that allows us to incorporate some varieties of these linguistic intuitions seamlessly: neural lattice language models ( \u00a73.1).", "labels": [], "entities": [{"text": "LSTM language modeling", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7213434378306071}]}, {"text": "Neural lattice language models define a lattice over possible paths through a sentence, and maximize the marginal probability overall paths that lead to generating the reference sentence, as shown in.", "labels": [], "entities": []}, {"text": "Depending on how we define these paths, we can incorporate different assumptions about how language should be modeled.", "labels": [], "entities": []}, {"text": "In the particular instantiations of neural lattice language models covered by this paper, we focus on two properties of language that could potentially be of use in language modeling: the existence of multiword lexical units) ( \u00a74.1) and poly-semy) ( \u00a74.2).", "labels": [], "entities": []}, {"text": "Neural lattice language models allow the model to incorporate these aspects in an end-to-end fashion by simply adjusting the structure of the underlying lattices.", "labels": [], "entities": []}, {"text": "We run experiments to explore whether these modifications improve the performance of the model ( \u00a75).", "labels": [], "entities": []}, {"text": "Additionally, we provide qualitative visualizations of the model to attempt to understand what types of multi-token phrases and polysemous embeddings have been learned.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare a baseline LSTM model, dense lattices of size 1, 2, and 3, and a multilattice with 2 and 3 embeddings per word.", "labels": [], "entities": []}, {"text": "The implementation of our networks was done in DyNet ( . All LSTMs had 2 layers, each with a hidden dimension of 200.", "labels": [], "entities": []}, {"text": "Variational dropout ( of .2 was used on the Chinese experiments, but hurt performance on the English data, so it was not used.", "labels": [], "entities": [{"text": "Variational dropout", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9608104825019836}, {"text": "English data", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.67996646463871}]}, {"text": "The 10,000 word embeddings each had dimension 256.", "labels": [], "entities": []}, {"text": "For lattice models, chunk vocabularies were selected by taking the 10,000 words in the vocabulary and adding the most common 10,000 n-grams with 1 < n \u2264 L.", "labels": [], "entities": []}, {"text": "The weights on the final layer of the network were tied with the input embeddings, as done by and.", "labels": [], "entities": []}, {"text": "In all lattice models, hidden states were computed using a weighted expectation ( \u00a73.3.3) unless mentioned otherwise.", "labels": [], "entities": []}, {"text": "In multi-embedding models, em-535  bedding sizes were decreased so as to maintain the same total number of parameters.", "labels": [], "entities": []}, {"text": "All models were trained using the Adam optimizer with a learning rate of .01 on a NVIDIA K80 GPU.", "labels": [], "entities": []}, {"text": "The results can be seen in.", "labels": [], "entities": []}, {"text": "In the multi-token phrase experiments, many additional parameters are accrued by the BiLSTM encoder and sub-LSTM predictive model, making them not strictly comparable to the baseline.", "labels": [], "entities": []}, {"text": "To account for this, we include results for L = 1, which, like the baseline LSTM approach, fails to leverage multi-token phrases, but includes the same number of parameters as L = 2 and L = 3.", "labels": [], "entities": []}, {"text": "In both the English and Chinese experiments, we seethe same trend: increasing the maximum lattice size decreases the perplexity, and for L = 2 and above, the neural lattice language model outperforms the baseline.", "labels": [], "entities": []}, {"text": "Similarly, increasing the number of embeddings per word decreases the perplexity, and for E = 2 and above, the multiple-embedding model outperforms the baseline.", "labels": [], "entities": []}, {"text": "We compare the various hidden-state calculation approaches discussed in Section 3.3 on the English data using a lattice of size L = 2 and dropout of .2.", "labels": [], "entities": [{"text": "English data", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8774165213108063}]}, {"text": "These results can be seen in.", "labels": [], "entities": []}, {"text": "For all hidden state calculation techniques, the neural lattice language models outperform the LSTM baseline.", "labels": [], "entities": []}, {"text": "The ancestral sampling technique used by is worse than the others, which we found to be due to it getting stuck in a local minimum which represents almost everything as unigrams.", "labels": [], "entities": []}, {"text": "There is only a small difference between the perplexities of the other techniques.", "labels": [], "entities": []}, {"text": "To verify that our findings scale to state-of-theart language models, we also compared a baseline model, dense lattices of size 1 and 2, and a multilattice with 2 embeddings per word on the full bytepair encoded Billion Word Corpus.", "labels": [], "entities": [{"text": "bytepair encoded Billion Word Corpus", "start_pos": 195, "end_pos": 231, "type": "DATASET", "confidence": 0.6595678448677063}]}, {"text": "In this set of experiments, we take the full Billion Word Corpus, and apply byte-pair encoding as described by to construct a vocabulary of 10,000 sub-word tokens.", "labels": [], "entities": [{"text": "Billion Word Corpus", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8008596301078796}]}, {"text": "Our model consists of three LSTM layers, each with 1500 hidden units.", "labels": [], "entities": []}, {"text": "We train the model fora single epoch over the corpus, using the Adam optimizer with learning rate .0001 on a P100 GPU.", "labels": [], "entities": []}, {"text": "We use a batch size of 40, and variational dropout of 0.1.", "labels": [], "entities": [{"text": "variational dropout", "start_pos": 31, "end_pos": 50, "type": "METRIC", "confidence": 0.9485515654087067}]}, {"text": "The 10,000 sub-word embeddings each had dimension 600.", "labels": [], "entities": []}, {"text": "For lattice models, chunk vocabularies were selected by taking the 10,000 sub-words in the vocabulary and adding the most common 10,000 n-grams with 1 < n \u2264 L.", "labels": [], "entities": []}, {"text": "The weights on the final layer of the network were tied with the input embeddings, as done by.", "labels": [], "entities": []}, {"text": "In all lattice models, hidden states were computed using weighted expectation ( \u00a73.3.3).", "labels": [], "entities": []}, {"text": "In multi-embedding models, embedding sizes were decreased so as to maintain the same total number of parameters.", "labels": [], "entities": []}, {"text": "Results of these experiments are in.", "labels": [], "entities": []}, {"text": "The performance of the baseline model is roughly on par with that of state-of-the-art models on this database; differences can be explained by model size and hyperparameter tuning.", "labels": [], "entities": []}, {"text": "The results show the same trend as the results of our main experiments, indicating that the performance gains shown by our smaller neural lattice language models generalize to the much larger datasets used in state-of-the-art systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on English language modeling task", "labels": [], "entities": [{"text": "English language modeling", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6951943635940552}]}, {"text": " Table 2: Results on Chinese language modeling task", "labels": [], "entities": [{"text": "Chinese language modeling", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.650304764509201}]}, {"text": " Table 3: Hidden state calculation comparison results  Model  Valid. Perp. Test Perp.  Baseline  64.18  60.67  Direct ( \u00a73.3.1)  59.74  55.98  Monte Carlo ( \u00a73.3.2)  62.97  59.08  Marginalization ( \u00a73.3.3)  58.62  55.06  GS Interpolation ( \u00a73.3.4)  59.19  55.73", "labels": [], "entities": []}, {"text": " Table 5: Results on large-scale Billion Word Corpus  Model  Valid. Test  Sec./  Perp. Perp. Batch  Baseline  54.1  37.7  .45  Multi-Token (L = 1)  54.2  37.4  .82  Multi-Token (L = 2)  53.9  36.4  4.85  Multi-Emb (E = 2)  53.8  35.2  2.53", "labels": [], "entities": []}, {"text": " Table 6: Vocabulary size comparison", "labels": [], "entities": [{"text": "Vocabulary size", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9302020967006683}]}]}