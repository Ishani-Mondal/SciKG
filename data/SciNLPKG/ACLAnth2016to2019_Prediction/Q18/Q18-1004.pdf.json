{"title": [{"text": "Representation Learning for Grounded Spatial Reasoning", "labels": [], "entities": [{"text": "Representation Learning for Grounded Spatial Reasoning", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7692813078562418}]}], "abstractContent": [{"text": "The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment.", "labels": [], "entities": []}, {"text": "We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards.", "labels": [], "entities": []}, {"text": "The proposed model learns a representation of the world steered by instruction text.", "labels": [], "entities": []}, {"text": "This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions.", "labels": [], "entities": []}, {"text": "We train our model with reinforcement learning using a variant of generalized value iteration.", "labels": [], "entities": []}, {"text": "The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding spatial references in natural language is essential for successful human-robot communication and autonomous navigation.", "labels": [], "entities": [{"text": "Understanding spatial references in natural language", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8101443350315094}, {"text": "autonomous navigation", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7833648920059204}]}, {"text": "This problem is challenging because interpretation of spatial references is highly context-dependent.", "labels": [], "entities": [{"text": "interpretation of spatial references", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.8383192718029022}]}, {"text": "For instance, the instruction \"Reach the cell above the westernmost rock\" translates into different goal locations in the two environments shown in.", "labels": [], "entities": []}, {"text": "Therefore, to enable generalization to new, unseen worlds, the model must jointly reason over the instruction text and environment configuration.", "labels": [], "entities": []}, {"text": "Moreover, the richness and flexibility in verbalizing spatial references further complicates interpretation of such instructions.", "labels": [], "entities": []}, {"text": "Code and dataset are available at https://github.", "labels": [], "entities": []}, {"text": "com/JannerM/spatial-reasoning Reach the cell above the westernmost rock In this paper, we explore the problem of spatial reasoning in the context of interactive worlds.", "labels": [], "entities": [{"text": "JannerM", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.946965217590332}]}, {"text": "Specifically, we assume access to a simulated environment, in which an agent can take actions to interact with the world and is rewarded for reaching the location specified by the language instruction.", "labels": [], "entities": []}, {"text": "This feedback is the only source of supervision the model uses for interpreting spatial references.", "labels": [], "entities": [{"text": "interpreting spatial references", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.9065462946891785}]}, {"text": "The key modeling task here is to induce a representation that closely ties environment observations and linguistic expressions.", "labels": [], "entities": []}, {"text": "In prior work, this issue was addressed by learning representations for each modality and then combining them, for instance, with concatenation (.", "labels": [], "entities": []}, {"text": "While this approach captures high-level correspondences between instructions and maps, it does not encode de-49 tailed, lower-level mappings between specific positions on the map and their descriptions.", "labels": [], "entities": []}, {"text": "As our experiments demonstrate, combining the language and environment representations in a spatially localized manner yields significant performance gains on the task.", "labels": [], "entities": []}, {"text": "To this end, our model uses the instruction text to drive the learning of the environment representation.", "labels": [], "entities": []}, {"text": "We start by converting the instruction text into a realvalued vector using a recurrent neural network with LSTM cells.", "labels": [], "entities": []}, {"text": "Using this vector as a kernel in a convolution operation, we obtain an instruction-conditioned representation of the state.", "labels": [], "entities": []}, {"text": "This allows the model to reason about immediate local neighborhoods in references such as \"two cells to the left of the triangle\".", "labels": [], "entities": []}, {"text": "We further augment this design to handle global references that involve information concerning the entire map (e.g. \"the westernmost rock\").", "labels": [], "entities": []}, {"text": "This is achieved by predicting a global value map using an additional component of the instruction representation.", "labels": [], "entities": []}, {"text": "The entire model is trained with reinforcement learning using the environmental reward signal as feedback.", "labels": [], "entities": []}, {"text": "We conducted our experiments using a 2D virtual world as shown in.", "labels": [], "entities": []}, {"text": "Overall, we created over 3,300 tasks across 200 maps, with instructions sourced from Mechanical Turk.", "labels": [], "entities": []}, {"text": "We compare our model against two state-of-the-art systems adapted for our task ().", "labels": [], "entities": []}, {"text": "The key findings of our experiments are threefold.", "labels": [], "entities": []}, {"text": "First, our model can more precisely interpret instructions than baseline models and find the goal location, yielding a 45% reduction in Manhattan distance error over the closest competitor.", "labels": [], "entities": [{"text": "Manhattan distance error", "start_pos": 136, "end_pos": 160, "type": "METRIC", "confidence": 0.9300477902094523}]}, {"text": "Second, the model can robustly generalize across new, unseen map layouts.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that factorizing the instruction representation enables the model to sustain high performance when handling both local and global references.", "labels": [], "entities": []}], "datasetContent": [{"text": "Puddle world navigation data In order to study generalization across a wide variety of environmental conditions and linguistic inputs, we develop an extension of the puddle world reinforcement learning benchmark.", "labels": [], "entities": [{"text": "puddle world reinforcement learning", "start_pos": 166, "end_pos": 201, "type": "TASK", "confidence": 0.7985605299472809}]}, {"text": "States in a 10 \u00d7 10 grid are first filled with either grass or water cells, such that the grass forms one connected component.", "labels": [], "entities": []}, {"text": "We then populate the grass region with six unique objects which appear only once per map (triangle, star, diamond, circle, heart, and spade) and four non-unique objects (rock, tree, horse, and house) which can appear any number of times on a given map.", "labels": [], "entities": []}, {"text": "See for an example visualization.", "labels": [], "entities": []}, {"text": "We also evaluate our model on the ISI Language Grounding dataset), which contains human-annotated instructions describing how to arrange blocks identified by numbers and logos.", "labels": [], "entities": [{"text": "ISI Language Grounding dataset", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.8021539449691772}]}, {"text": "Although it does not contain variable environment maps as in our dataset, it has a larger action space and vocabulary.", "labels": [], "entities": []}, {"text": "The caveat is that the task as posed in the original dataset is not compatible with our model.", "labels": [], "entities": []}, {"text": "For a policy to be derived from a value map with the same dimension as the state observation, it is implicitly assumed that there is a single controllable agent, whereas the ISI set allows multiple blocks to be moved.", "labels": [], "entities": []}, {"text": "We therefore modify the ISI setup using an oracle to determine which block is given agency during each step.", "labels": [], "entities": []}, {"text": "This allows us to The MSE of the value map prediction as a function of a subgoal's ordering in an overall task.", "labels": [], "entities": [{"text": "value map prediction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.6588607827822367}]}, {"text": "The model performs better on subgoals later in a task despite the subgoals being treated completely independently during both training and testing.", "labels": [], "entities": []}, {"text": "retain the linguistic variability of the dataset while overcoming the mismatch in task setup.", "labels": [], "entities": []}, {"text": "The states are discretized to a 13 \u00d7 13 map and the instructions are lemmatized.", "labels": [], "entities": []}, {"text": "Performance on the modified ISI dataset is reported in and representative visualizations are shown in.", "labels": [], "entities": [{"text": "ISI dataset", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.7958641052246094}]}, {"text": "Our model outperforms both baselines by a greater margin in policy quality than on our own dataset.", "labels": [], "entities": []}, {"text": "also use this dataset and report results in part by determining the minimum distance between an agent and a goal during an evaluation lasting N steps.", "labels": [], "entities": []}, {"text": "This evaluation metric is therefore dependent on this timeout parameter N . Because we discretized the state space so as to be able to represent it as a grid of embeddings, the notion of a single step has been changed and direct comparison limited to N steps is ill-defined.", "labels": [], "entities": []}, {"text": "Hence, due to modifica-6 When a model is available and the states are not overwhelmingly high-dimensional, policy quality is a useful metric that is independent of this type of parameter.", "labels": [], "entities": []}, {"text": "As such, it is our tions in the task setup, we cannot compare directly to the results in.", "labels": [], "entities": []}, {"text": "Understanding grounding evaluation An interesting finding in our analysis was that the difficulty of the language interpretation task is a function of the stage in task execution).", "labels": [], "entities": [{"text": "language interpretation task", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.7894873122374216}]}, {"text": "In the ISI Language Grounding set (, each individual instruction (describing whereto move a particular block) is a subgoal in a larger task (such as constructing a circle with all of the blocks).", "labels": [], "entities": [{"text": "ISI Language Grounding", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.7004485527674357}]}, {"text": "The value maps predicted for subgoals occurring later in a task are more accurate than those occurring early in the task.", "labels": [], "entities": []}, {"text": "It is likely that the language plays a less crucial role in specifying the subgoal position in the final steps of a task.", "labels": [], "entities": []}, {"text": "As shown in(a), it maybe possible to narrow down candidate subgoal positions just by looking at a nearly-constructed highdefault metric here.", "labels": [], "entities": []}, {"text": "However, estimating policy quality for environments substantially larger than those investigated here is a challenge in itself.", "labels": [], "entities": [{"text": "estimating policy quality", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.8364718357721964}]}, {"text": "In contrast, this would not be possible early in a task because most of the blocks will be randomly positioned.", "labels": [], "entities": []}, {"text": "This finding is consistent with a result from, who reported that strategy game manuals were useful early in the game but became less essential further into play.", "labels": [], "entities": []}, {"text": "It appears to be part of a larger trend that the marginal benefit of language in such grounding tasks can vary predictably between individual instructions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of models trained via reinforcement learning on a held-out set of environments and  instructions. Policy quality is the true expected normalized reward and distance denotes the Manhattan  distance from goal location prediction to true goal position. We show results from training on the local and  global instructions both separately and jointly.", "labels": [], "entities": []}, {"text": " Table 3: Performance on a test set of environments and instructions after supervised training. Lower is  better for MSE and Manhattan distance; higher is better for policy quality. The gradient basis significantly  improves the reconstruction error and goal localization of our model on global instructions, and expectedly  does not affect its performance on local instructions.", "labels": [], "entities": []}, {"text": " Table 4: The performance of our model and two  baselines on the ISI Language Grounding dataset  (", "labels": [], "entities": [{"text": "ISI Language Grounding dataset", "start_pos": 65, "end_pos": 95, "type": "DATASET", "confidence": 0.6773852556943893}]}]}