{"title": [{"text": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents", "labels": [], "entities": [{"text": "Multi-hop Reading Comprehension Across Documents", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.7828665375709534}]}], "abstractContent": [{"text": "Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document.", "labels": [], "entities": []}, {"text": "Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability.", "labels": [], "entities": []}, {"text": "We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods.", "labels": [], "entities": [{"text": "text understanding across multiple documents", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.8388506650924683}]}, {"text": "In our task, a model learns to seek and combine evidence-effectively performing multi-hop, alias multi-step, inference.", "labels": [], "entities": []}, {"text": "We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents.", "labels": [], "entities": []}, {"text": "Two datasets from different domains are induced, 1 and we identify potential pitfalls and devise circum-vention strategies.", "labels": [], "entities": []}, {"text": "We evaluate two previously proposed competitive models and find that one can integrate information across documents.", "labels": [], "entities": []}, {"text": "However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance.", "labels": [], "entities": []}, {"text": "While the models outperform several strong baselines, their best accuracy reaches 54.5% on an annotated test set, compared to human performance at 85.0%, leaving ample room for improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9995195865631104}]}], "introductionContent": [{"text": "Devising computer systems capable of answering questions about knowledge described using text has Available at http://qangaroo.cs.ucl.ac.uk The Hanging Gardens, in, also known as Pherozeshah Mehta Gardens, are terraced gardens \u2026 They provide sunset views over the [Arabian Sea] \u2026 Mumbai (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra.", "labels": [], "entities": []}, {"text": "It is the most populous city in India \u2026 Q: (Hanging gardens of Mumbai, country, ?) Options: {Iran, India, Pakistan, Somalia, \u2026} The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India \u2026: A sample from the WIKIHOP dataset where it is necessary to combine information spread across multiple documents to infer the correct answer.", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 337, "end_pos": 352, "type": "DATASET", "confidence": 0.9635836780071259}]}, {"text": "been a longstanding challenge in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.7697111368179321}]}, {"text": "Contemporary end-to-end Reading Comprehension (RC) methods can learn to extract the correct answer span within a given text and approach human-level performance ().", "labels": [], "entities": [{"text": "end-to-end Reading Comprehension (RC)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.6864107052485148}]}, {"text": "However, for existing datasets, relevant information is often concentrated locally within a single sentence, emphasizing the role of locating, matching, and aligning information between query and support text.", "labels": [], "entities": []}, {"text": "For example,  observed that a simple binary word-in-query indicator feature boosted the relative accuracy of a baseline model by 27.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9728465676307678}]}, {"text": "We argue that, in order to further the ability of machine comprehension methods to extract knowledge from text, we must move beyond a scenario where relevant information is coherently and explicitly stated within a single document.", "labels": [], "entities": []}, {"text": "Methods with this capability would aid Information Extraction (IE) applications, such as discovering drug-drug interac-tions () by connecting protein interactions reported across different publications.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8457254052162171}]}, {"text": "They would also benefit search and Question Answering (QA) applications () where the required information cannot be found in a single location.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8231078803539276}]}, {"text": "shows an example from WIKIPEDIA, where the goal is to identify the country property of the Hanging Gardens of Mumbai.", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8679764866828918}, {"text": "country property of the Hanging Gardens of Mumbai", "start_pos": 67, "end_pos": 116, "type": "DATASET", "confidence": 0.7610842622816563}]}, {"text": "This cannot be inferred solely from the article about them without additional background knowledge, as the answer is not stated explicitly.", "labels": [], "entities": []}, {"text": "However, several of the linked articles mention the correct answer India (and other countries), but cover different topics (e.g. Mumbai, Arabian Sea, etc.).", "labels": [], "entities": []}, {"text": "Finding the answer requires multi-hop reasoning: figuring out that the Hanging Gardens are located in Mumbai, and then, from a second document, that Mumbai is a city in India.", "labels": [], "entities": []}, {"text": "We define a novel RC task in which a model should learn to answer queries by combining evidence stated across documents.", "labels": [], "entities": []}, {"text": "We introduce a methodology to induce datasets for this task and derive two datasets.", "labels": [], "entities": []}, {"text": "The first, WIKIHOP, uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity cannot be located in the entity's article.", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9202006459236145}]}, {"text": "In the second dataset, MEDHOP, the goal is to establish drug-drug interactions based on scientific findings about drugs and proteins and their interactions, found across multiple MEDLINE abstracts.", "labels": [], "entities": [{"text": "MEDHOP", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7864419221878052}]}, {"text": "For both datasets we draw upon existing Knowledge Bases (KBs), WIKIDATA and DRUG-BANK, as ground truth, utilizing distant supervision () to induce the data -similar to and.", "labels": [], "entities": [{"text": "DRUG-BANK", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.7609419226646423}]}, {"text": "We establish that for 74.1% and 68.0% of the samples, the answer can be inferred from the given documents by a human annotator.", "labels": [], "entities": []}, {"text": "Still, constructing multi-document datasets is challenging; we encounter and prescribe remedies for several pitfalls associated with their assembly -for example, spurious co-locations of answers and specific documents.", "labels": [], "entities": []}, {"text": "For both datasets we then establish several strong baselines and evaluate the performance of two previously proposed competitive RC models (.", "labels": [], "entities": []}, {"text": "We find that one can integrate information across documents, but neither excels at selecting relevant information from a larger documents set, as their accuracy increases significantly when given only documents guaranteed to be relevant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.997461199760437}]}, {"text": "The best model reaches 54.5% on an annotated test set, compared to human performance at 85.0%, indicating ample room for improvement.", "labels": [], "entities": []}, {"text": "In summary, our key contributions are as follows: Firstly, proposing a cross-document multi-step RC task, as well as a general dataset induction strategy.", "labels": [], "entities": [{"text": "cross-document multi-step RC task", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.6354837566614151}]}, {"text": "Secondly, assembling two datasets from different domains and identifying dataset construction pitfalls and remedies.", "labels": [], "entities": []}, {"text": "Thirdly, establishing multiple baselines, including two recently proposed RC models, as well as analysing model behaviour in detail through ablation studies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will now formally define the multi-hop RC task, and a generic methodology to construct multi-hop RC datasets.", "labels": [], "entities": []}, {"text": "Later, in Sections 3 and 4 we will demonstrate how this method is applied in practice by creating datasets for two different domains.", "labels": [], "entities": []}, {"text": "Task Formalization A model is given a query q, a set of supporting documents Sq , and a set of candidate answers C q -all of which are mentioned in Sq . The goal is to identify the correct answer a * \u2208 C q by drawing on the support documents Sq . Queries could potentially have several true answers when not constrained to rely on a specific set of support documents -e.g., queries about the parent of a certain individual.", "labels": [], "entities": []}, {"text": "However, in our setup each sample has only one true answer among C q and Sq . Note that even though we will utilize background information during dataset assembly, such information will not be available to a model: the document set will be provided in random order and without any metadata.", "labels": [], "entities": []}, {"text": "While certainly beneficial, this would distract from our goal of fostering end-to-end RC methods that infer facts by combining separate facts stated in text.", "labels": [], "entities": []}, {"text": "Dataset Assembly We assume that there exists a document corpus D, together with a KB containing fact triples (s, r, o) -with subject entity s, relation r, and object entity o.", "labels": [], "entities": []}, {"text": "For example, one such fact could be (Hanging Gardens of Mumbai, country, India).", "labels": [], "entities": [{"text": "Hanging Gardens of Mumbai, country", "start_pos": 37, "end_pos": 71, "type": "DATASET", "confidence": 0.8204084932804108}]}, {"text": "We start with individual KB facts and transform them into query-answer pairs by leaving the object slot empty, i.e. q = (s, r, ?) and a * = o.", "labels": [], "entities": []}, {"text": "Next, we define a directed bipartite graph, where vertices on one side correspond to documents in D, and vertices on the other side are entities from the KB -see for an example.", "labels": [], "entities": []}, {"text": "A document node dis connected to an entity e if e is mentioned ind, though there maybe further constraints when defining the graph connectivity.", "labels": [], "entities": []}, {"text": "For a given (q, a * ) pair, the candidates C q and support documents Sq \u2286 Dare identified by traversing the bipartite graph using breadth-first search; the documents visited will become the support documents Sq . As the traversal starting point, we use the node belonging to the subject entity s of the query q.", "labels": [], "entities": []}, {"text": "As traversal end points, we use the set of all entity nodes that are type-consistent answers to q.", "labels": [], "entities": []}, {"text": "2 Note that whenever there is another fact (s, r, o ) in the KB, i.e. a fact producing the same q but with a different a * , we will not include o into the set of end points for this sample.", "labels": [], "entities": []}, {"text": "This ensures that precisely one of the end points corresponds to a correct answer to q.", "labels": [], "entities": []}, {"text": "When traversing the graph starting at s, several of the end points will be visited, though generally not all; those visited define the candidate set C q . If however the correct answer a * is not among them we discard the entire (q, a * ) pair.", "labels": [], "entities": []}, {"text": "The documents visited to reach the end points will define the support document set Sq . That is, Sq comprises chains of documents leading not only from the query subject to the correct answer candidate, but also to type-consistent false answer candidates.", "labels": [], "entities": []}, {"text": "With this methodology, relevant textual evidence for (q, a * ) will be spread across documents along the chain connecting sand a * -ensuring that multihop reasoning goes beyond resolving co-reference within a single document.", "labels": [], "entities": []}, {"text": "Note that including other type-consistent candidates alongside a * as end points in the graph traversal -and thus into the support documents -renders the task considerably more challenging.", "labels": [], "entities": []}, {"text": "Models could otherwise identify a * in the documents by simply relying on type-consistency heuristics.", "labels": [], "entities": []}, {"text": "It is worth pointing out that by introducing alternative candidates we counterbalance a type-consistency bias, in contrast to and who instead rely on entity masking.", "labels": [], "entities": [{"text": "entity masking", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.7128757387399673}]}, {"text": "To determine entities which are type-consistent fora query q, we consider all entities which are observed as object in a fact with r as relation type -including the correct answer.", "labels": [], "entities": []}, {"text": "Dataset creation is always fraught with the risk of inducing unintended errors and biases Candidate Frequency Imbalance A first observation is that there is a significant bias in the answer distribution of WIKIREADING.", "labels": [], "entities": [{"text": "Dataset creation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6490674763917923}, {"text": "Candidate Frequency Imbalance", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6108465691407522}, {"text": "WIKIREADING", "start_pos": 206, "end_pos": 217, "type": "DATASET", "confidence": 0.6376116275787354}]}, {"text": "For example, in the majority of the samples the property country has the United States of America as the answer.", "labels": [], "entities": []}, {"text": "A simple majority class baseline would thus prove successful, but would tell us little about multi-hop reasoning.", "labels": [], "entities": []}, {"text": "To combat this issue, we subsampled the dataset to ensure that samples of anyone particular answer candidate makeup no more than 0.1% of the dataset, and omitted articles about the United States.", "labels": [], "entities": []}, {"text": "Document-Answer Correlations A problem unique to our multi-document setting is the possibility of spurious correlations between candidates and documents induced by the graph traversal method.", "labels": [], "entities": [{"text": "Document-Answer Correlations", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7080551087856293}]}, {"text": "In fact, if we were not to address this issue, a model designed to exploit these regularities could achieve 74.6% accuracy (detailed in Section 6).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.998968243598938}]}, {"text": "Concretely, we observed that certain documents frequently co-occur with the correct answer, independently of the query.", "labels": [], "entities": []}, {"text": "For example, if the article about London is present in Sq , the answer is likely to be the United Kingdom, independent of the query type or entity in question.", "labels": [], "entities": [{"text": "Sq", "start_pos": 55, "end_pos": 57, "type": "DATASET", "confidence": 0.7917264103889465}]}, {"text": "We designed a statistic to measure this effect and then used it to sub-sample the dataset.", "labels": [], "entities": []}, {"text": "The statistic counts how often a candidate c is observed as the correct answer when a certain document is present in Sq across training set samples.", "labels": [], "entities": []}, {"text": "More formally, fora given document d and answer candidate c, let cooccurrence(d, c) denote the total count of how often d co-occurs with c in a sample where c is also the correct answer.", "labels": [], "entities": []}, {"text": "We use this statistic to filter the dataset, by discarding samples with at least one document-candidate pair (d, c) for which cooccurrence(d, c) > 20.", "labels": [], "entities": []}, {"text": "This section describes experiments on WIKIHOP and MEDHOP with the goal of establishing the performance of several baseline models, including recent neural RC models.", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.837830126285553}, {"text": "MEDHOP", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.7632638812065125}]}, {"text": "We empirically demonstrate the importance of mitigating dataset biases, probe whether multi-step behavior is beneficial for solving the task, and investigate if RC models can learn to perform lexical abstraction.", "labels": [], "entities": []}, {"text": "Training will be conducted on the respective training sets, and evaluation on both the full test set and validated portion (Section 5.3) allowing fora comparison between the two.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 shows statistics on the number of candi- dates and documents per sample on the respective  training sets. For MEDHOP, the majority of sam- ples have 9 candidates, due to the way documents  are selected up until a maximum of 64 documents is", "labels": [], "entities": [{"text": "MEDHOP", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.7295240163803101}]}, {"text": " Table 2: Candidates and documents per sample and doc- ument length statistics. WH: WIKIHOP; MH: MEDHOP.", "labels": [], "entities": [{"text": "doc- ument length", "start_pos": 50, "end_pos": 67, "type": "METRIC", "confidence": 0.7820923179388046}, {"text": "WH", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.8832952976226807}, {"text": "WIKIHOP", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.322244256734848}, {"text": "MH", "start_pos": 93, "end_pos": 95, "type": "DATASET", "confidence": 0.6494939923286438}, {"text": "MEDHOP", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.733372688293457}]}, {"text": " Table 3: Qualitiative analysis of WIKIHOP samples.", "labels": [], "entities": [{"text": "Qualitiative analysis", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8550531268119812}, {"text": "WIKIHOP samples", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8300096094608307}]}, {"text": " Table 4: Accuracy comparison for simple baseline mod- els on WIKIHOP before and after filtering.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959718585014343}, {"text": "WIKIHOP", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8933846950531006}]}, {"text": " Table 5: Test accuracies for the WIKIHOP and MEDHOP datasets, both in standard (unmasked) and masked setup.  Columns marked with asterisk are for the validated portion of the dataset.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.8330186009407043}, {"text": "WIKIHOP", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9231247305870056}, {"text": "MEDHOP datasets", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.7385760694742203}]}, {"text": " Table 6: Test accuracy comparison when only using documents leading to the correct answer (gold chain). Columns  with asterisk hold results for the validated samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.977242112159729}]}, {"text": " Table 7: Test accuracy (masked) when only documents  containing answer candidates are given (rem).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9648894667625427}]}]}