{"title": [{"text": "Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings", "labels": [], "entities": [{"text": "Unsupervised Word Mapping", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6067667504151663}]}], "abstractContent": [{"text": "Most existing methods for automatic bilingual dictionary induction rely on prior alignments between the source and target languages, such as parallel corpora or seed dictionaries.", "labels": [], "entities": [{"text": "automatic bilingual dictionary induction", "start_pos": 26, "end_pos": 66, "type": "TASK", "confidence": 0.5729528814554214}]}, {"text": "For many language pairs, such supervised alignments are not readily available.", "labels": [], "entities": []}, {"text": "We propose an unsupervised approach for learning a bilingual dictionary fora pair of languages given their independently-learned monolingual word em-beddings.", "labels": [], "entities": []}, {"text": "The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other.", "labels": [], "entities": []}, {"text": "We show empirically that the performance of bilingual correspondents that are learned using our proposed unsupervised method is comparable to that of using supervised bilingual correspondents from a seed dictionary.", "labels": [], "entities": []}], "introductionContent": [{"text": "The working hypothesis in distributional semantics is that the meaning of a word can be inferred by its distribution, or co-occurrence, around other words.", "labels": [], "entities": []}, {"text": "The validity of this hypothesis is most evident in the performance of distributed vector representations of words, i.e word embeddings, that are automatically induced from large text corpora ().", "labels": [], "entities": []}, {"text": "The qualitative nature of these embeddings can be demonstrated through empirical evidence of regularities that reflect certain semantic relationships.", "labels": [], "entities": []}, {"text": "Words in the vector space are generally clustered by meaning, and the distances between words and clusters reflect semantic or syntactic relationships, which makes it possible to perform arithmetic on word vectors for analogical reasoning and semantic composition).", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 243, "end_pos": 263, "type": "TASK", "confidence": 0.7545635402202606}]}, {"text": "For example, in a vector space V where f = V (''f rance\"), p = V (''paris\"), and g = V (''germany\"), the distance f \u2212 p reflects the country-capital relationship, and g + f \u2212 p results in a vector closest to V (''berlin\").", "labels": [], "entities": []}, {"text": "Named entities and inflectional morphemes are particularly amenable to vector arithmetic, while derivational morphology, polysemy, and other nuanced semantic categories result in lower performance in analogy questions).", "labels": [], "entities": [{"text": "vector arithmetic", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7050367742776871}]}, {"text": "The extent of these semantic and syntactic regularities is difficult to assess intrinsically, and the performance in analogical reasoning can be partially attributed to the clustering of the words in question.", "labels": [], "entities": []}, {"text": "If meaning is encoded in the relative distances among word vectors, then the structure within vector spaces should be consistent across different languages given that the datasets used to build them express similar content.", "labels": [], "entities": []}, {"text": "In, a simulation study showed that similarity in word co-occurrence patterns within unrelated German and English texts is correlated with the number of corresponding word positions in the monolingual cooccurrence matrices.", "labels": [], "entities": []}, {"text": "More recently, showed that a linear projection can be learned to transform word embeddings from one language into the vector space of another using a medium-size seed dictionary, which demonstrates that the multilingual vector spaces are at least related by a linear transform.", "labels": [], "entities": []}, {"text": "This makes it possible to align word embeddings of different languages in order to be directly comparable within the same seman-tic space.", "labels": [], "entities": []}, {"text": "Such cross-lingual word embeddings can be used to expand dictionaries or to learn languageindependent classifiers.", "labels": [], "entities": []}, {"text": "A number of methods have been proposed recently for learning cross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries, sentence-level alignment using parallel corpora (, or document alignment using crosslingual topic models (.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7487812042236328}, {"text": "sentence-level alignment", "start_pos": 192, "end_pos": 216, "type": "TASK", "confidence": 0.7058310508728027}, {"text": "document alignment", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.7011378109455109}]}, {"text": "Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (.", "labels": [], "entities": []}, {"text": "In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (.", "labels": [], "entities": []}, {"text": "However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no prior knowledge whatsoever is valuable.", "labels": [], "entities": []}, {"text": "Intuitively, if the monolingual corpora express similar aspects of the world, there should be enough structure within the vector space of each language to recover the mappings in a completely unsupervised manner.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach for learning a transformation between monolingual word embeddings without the use of prior alignments.", "labels": [], "entities": []}, {"text": "We show empirically that we can recover mappings with high accuracy in two language pairs: a close language pair, French-English; and a distant language pair, Arabic-English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9950405955314636}]}, {"text": "The proposed method relies on the consistent regularities within monolingual vector spaces of different languages.", "labels": [], "entities": []}, {"text": "We extract initial mappings using spectral embeddings that encode the local geometry around each word, and we use these tentative pairs to seed a greedy algorithm which minimizes the differences in global pair-wise distances among word vectors.", "labels": [], "entities": []}, {"text": "The retrieved mappings are then used to fit a linear projection matrix to transform word embeddings from the source to the target language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with two language pairs: FrenchEnglish, and Arabic-English.", "labels": [], "entities": [{"text": "FrenchEnglish", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.907112181186676}]}, {"text": "French shares similar orthography and word roots with English, but for evaluating the generality of the approach, we don't utilize these similarities in any form.", "labels": [], "entities": []}, {"text": "Arabic, on the other hand, is a dissimilar language with more limited resources, and it is noisier at the word level due to clitic affixation that is challenging to tokenize.", "labels": [], "entities": []}, {"text": "This makes it a suitable test-case fora realistic lowresource language.", "labels": [], "entities": []}, {"text": "For each of the datasets described above, we generated 100-dimensional word embeddings using the subword skip-gram model (.", "labels": [], "entities": []}, {"text": "We extracted the most frequent 2K words from the source and target languages and their embeddings for the iterative mapping (IM) method.", "labels": [], "entities": []}, {"text": "The loss function Lin equation 2 was used to guide the tuning of model parameters.", "labels": [], "entities": []}, {"text": "We tuned k = for the spectral initialization, and due to randomness in IM, we repeated each experiment 10 times and used the mapping that resulted in the smallest loss.", "labels": [], "entities": [{"text": "spectral initialization", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6627247780561447}]}, {"text": "For the final linear transformation T , we used the most frequent 50K words in both source and target languages, and we used the hubness reduction method described in with c=5000.", "labels": [], "entities": []}, {"text": "We extracted dictionary pairs from the Multilingual WordNet) where the source words are within the top 15K words in all datasets.", "labels": [], "entities": []}, {"text": "From these pairs, we extracted a random sample of 2K unique (source, target) pairs for training the supervised method, and the remaining source words and all their translations were used for testing.", "labels": [], "entities": []}, {"text": "This resulted in a total of 977 French words and 473 Arabic words for evaluation.", "labels": [], "entities": []}, {"text": "Using the (source, target) pairs extracted using IM with spectral initialization (IM-SI), we fit a linear projection matrix from the source to the target embeddings to compare the results with supervised linear transformation.", "labels": [], "entities": []}, {"text": "We also compare with a baseline of random initialization of the IM method (IMRand).", "labels": [], "entities": []}, {"text": "We evaluate the linear transformations on the different datasets in by reporting the precision of mapping each test word to a correct translation within its k nearest neighbors, fork \u2208 {1, 5, 10, 20, 50, 100}.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9996343851089478}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "While the initial spectral embeddings didn't always recover the correct correspondences (see), these tentative pairs helped initialize the IM algorithm in the right direction for better global convergence.", "labels": [], "entities": [{"text": "IM", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.9255340099334717}]}, {"text": "As shown in, initializing IM with random pairs resulted in poor performance while spectral initialization helped converge to plausible (b) Precision at k by noise level 'parking', 'three-story', 'six-story', 'mall', 'five-story': Examples of correct and incorrect transformations at k = 5 for Arabic-English using the unsupervised IM-SI mappings to fit a linear projection matrix. mappings.", "labels": [], "entities": [{"text": "Precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9468067288398743}]}, {"text": "In fact, the use of spectral initialization in combination with IM to seed the transformation resulted in a precision close to the supervised baseline as seen in. shows the performance of transforming Arabic word embeddings using the various models.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9973983764648438}]}, {"text": "The supervised baseline results are lower than the French-English case, which is partly due to the low coverage of WordNet translations for Arabic (see).", "labels": [], "entities": []}, {"text": "Nevertheless, we managed to recover accurate mappings and linear transformations that perform comparably to the supervised baseline.", "labels": [], "entities": []}, {"text": "shows some examples of correct and incorrect transformations at k = 5 on Arabic test words.", "labels": [], "entities": []}, {"text": "Observe that even in the case of incorrect matches, the k nearest neighbors are related to the target words in meaning.", "labels": [], "entities": []}, {"text": "For example, all five nearest neighbors of the word ('\u202b'/'\ufe91\ufee8\ufe8e\ufef3\ufe94\u202cbuilding'), are building-related, such as 'tower', 'parking', 'three-story', and 'mall'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: A random sample of word mappings from  French to English using IM with spectral initialization.  These pairs are later used to fit a linear projection matrix  between the source and target embeddings. Correct map- pings are indicated in italics", "labels": [], "entities": []}, {"text": " Table 5: Examples of correct and incorrect transforma- tions at k = 5 for Arabic-English using the unsupervised  IM-SI mappings to fit a linear projection matrix.", "labels": [], "entities": []}]}