{"title": [{"text": "Unsupervised Grammar Induction with Depth-bounded PCFG", "labels": [], "entities": [{"text": "PCFG", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.6828141808509827}]}], "abstractContent": [{"text": "There has been recent interest in applying cognitively-or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016).", "labels": [], "entities": []}, {"text": "This work extends this depth-bounding approach to probabilistic context-free grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depth-bounding.", "labels": [], "entities": [{"text": "context-free grammar induction", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6450082461039225}]}, {"text": "Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy.", "labels": [], "entities": [{"text": "grammar acquisition from transcribed child-directed speech and newswire text", "start_pos": 26, "end_pos": 102, "type": "TASK", "confidence": 0.7616056005160013}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.8913345336914062}]}, {"text": "Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar acquisition or grammar induction) has been of interest to linguists and cognitive scientists for decades.", "labels": [], "entities": [{"text": "Grammar acquisition or grammar induction)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8022165497144064}]}, {"text": "This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (, or as apiece of evidence) about the Distributional Hypothesis against the poverty of the stimulus.", "labels": [], "entities": []}, {"text": "Unfortunately, previous attempts at inducing unbounded context-free grammars) converged to weak modes of a very multimodal distribution of grammars.", "labels": [], "entities": []}, {"text": "There has been recent interest in applying cognitively-or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models. and in particular report benefits for depth bounds on grammar acquisition using hierarchical sequence models, but either without the capacity to learn full grammar rules (e.g. that a noun phrase may consist of a noun phrase followed by a prepositional phrase), or with a very large parameter space that may offset the gains of depth-bounding.", "labels": [], "entities": [{"text": "grammar acquisition", "start_pos": 215, "end_pos": 234, "type": "TASK", "confidence": 0.720944732427597}]}, {"text": "This work extends the depth-bounding approach to directly induce probabilistic context-free grammars, which have a smaller parameter space than hierarchical sequence models, and therefore arguably make better use of the space reductions of depthbounding.", "labels": [], "entities": []}, {"text": "This approach employs a procedure for deriving a sequence model from a PCFG (van, developed in the context of a supervised learning model, and adapts it to an unsupervised setting.", "labels": [], "entities": [{"text": "PCFG (van", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.8566310803095499}]}, {"text": "Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy.", "labels": [], "entities": [{"text": "grammar acquisition from transcribed child-directed speech and newswire text", "start_pos": 26, "end_pos": 102, "type": "TASK", "confidence": 0.7616060309939914}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.8913345336914062}]}, {"text": "Moreover, grammars acquired from this model demonstrate a consistent use of category labels, as shown in a noun phrase discovery task, something which has not been demonstrated by other acquisition models.", "labels": [], "entities": [{"text": "noun phrase discovery task", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.7776714861392975}]}], "datasetContent": [{"text": "The DB-PCFG model described in Section 4 is evaluated first on synthetic data to determine whether it can reliably learn a recursive grammar from data with a known optimum solution, and to determine the hyper-parameter value for \u03b2 for doing so.", "labels": [], "entities": []}, {"text": "Two experiments on natural data are then carried out.", "labels": [], "entities": []}, {"text": "First, the model is run on natural data from the Adam and Eve parts of the CHILDES corpus) to compare with other grammar induction systems on a human-like acquisition task.", "labels": [], "entities": [{"text": "CHILDES corpus", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.930489182472229}]}, {"text": "Then data from the Wall Street Journal section of the Penn Treebank () is used for further comparison in a domain for which competing systems are optimized.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 19, "end_pos": 67, "type": "DATASET", "confidence": 0.9407019838690758}]}, {"text": "The competing systems include UPPARSE , CCL , BMMM+DMV with undirected dependency features and UHHMM.", "labels": [], "entities": [{"text": "UPPARSE", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.6818860769271851}, {"text": "UHHMM", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.8440618515014648}]}, {"text": "For the natural language datasets, the variously parametrized DB-PCFG systems are first validated on a development set, and the optimal system is then run until convergence with the chosen hyperparameters on the test set.", "labels": [], "entities": []}, {"text": "In development experiments, the log-likelihood of the dataset plateaus usually after 500 iterations.", "labels": [], "entities": []}, {"text": "The system is therefore run at least 500 iterations in all test set experiments, with one iteration being a full cycle of Gibbs sampling.", "labels": [], "entities": []}, {"text": "The system is then checked to see whether the loglikelihood has plateaued, and halted if it has.", "labels": [], "entities": []}, {"text": "The DB-PCFG model assigns trees sampled from conditional posteriors to all sentences in a dataset in every iteration as part of the inference.", "labels": [], "entities": []}, {"text": "The system is further allowed to run at least 250 iterations after convergence and proposed parses are chosen from the iteration with the greatest log-likelihood after convergence.", "labels": [], "entities": []}, {"text": "However, once the system reaches convergence, the evaluation scores of parses from different iterations post-convergence appear to differ very little.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance scores of unlabeled parse eval- uation of different systems on synthetic data.", "labels": [], "entities": []}, {"text": " Table 2: PARSEVAL results of different hyperparameter  settings for the DB-PCFG system on the Adam dataset.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9262474179267883}, {"text": "Adam dataset", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9468974471092224}]}, {"text": " Table 3: PARSEVAL scores on Eve dataset for all com- peting systems. These are unlabeled precision, recall and  F1 scores on constituent trees without punctuation. Both  the right-branching baseline and the best performing sys- tem are in bold. (**: p < 0.0001, permutation test)", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9900772571563721}, {"text": "Eve dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9466349184513092}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9940682649612427}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9937905669212341}, {"text": "F1", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9947493672370911}]}, {"text": " Table 4: Performances of different systems for noun  phrase recall and aggregated F1 scores on the Eve dataset.", "labels": [], "entities": [{"text": "noun  phrase recall", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7153758009274801}, {"text": "F1 scores", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9079515635967255}, {"text": "Eve dataset", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9006536602973938}]}, {"text": " Table 5: PARSEVAL scores for all competing systems on WSJ10 and WSJ20 test sets. These are unlabeled precision,  recall and F1 scores on constituent trees without punctuation (**: p <0.0001, permutation test).", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9541743993759155}, {"text": "WSJ10", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9826704859733582}, {"text": "WSJ20 test sets", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9253171881039938}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9940998554229736}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9923795461654663}, {"text": "F1", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9819929599761963}]}, {"text": " Table 6: Published PARSEVAL results for competing systems. Please see text for details as the systems are trained  and evaluated differently.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.4463655352592468}]}]}