{"title": [{"text": "Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results", "labels": [], "entities": [{"text": "Question Answering Research", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.7679897447427114}]}], "abstractContent": [{"text": "\"Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field\" (Pfeiffer and Hoffmann, 2009).", "labels": [], "entities": [{"text": "reliability", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9743586182594299}]}, {"text": "As we know, deep learning is very popular and the ability to reproduce results is an important part of science.", "labels": [], "entities": []}, {"text": "There is growing concern within the deep learning community about the reproducibility of results that are presented.", "labels": [], "entities": []}, {"text": "In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the re-producibility of those results.", "labels": [], "entities": []}, {"text": "Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough.", "labels": [], "entities": []}, {"text": "Source code without a reproducible environment does not mean anything at all.", "labels": [], "entities": []}, {"text": "In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent \"reproducibility crisis\") in various scientific fields (particularly Psychology and Social Sciences) indicates that some introspection is needed in all fields, particularly those that are experimental by nature.", "labels": [], "entities": [{"text": "introspection", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.9435575604438782}]}, {"text": "The efforts of Collberg's repeatability studies highlight the state of affairs within the computer systems research community.", "labels": [], "entities": []}, {"text": "Other fields have also begun to push for more stringent presentation of 1 http://reproducibility.cs.arizona.edu results, for example, the information retrieval community has been aware for sometime of the issues surrounding weak baselines ( and more recently reproducibility (.", "labels": [], "entities": [{"text": "information retrieval community", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.8107598026593527}]}, {"text": "The issue of reproducibility in the deep-learning community has also started to become a growing concern, with the need for replicable and reproducible results being included in a list of challenges for the ACL.", "labels": [], "entities": []}, {"text": "In reinforcement learning, showed that there area number of effects that would change the results obtained by published authors and call for more rigorous testing, and reporting, of state-of-the-art methods.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8908364176750183}]}, {"text": "There is also an ongoing project by OpenAI to provide baselines in reinforcement learning that are reproduced from published descriptions, but even they admit that their scores are only \"roughly on par with the scores in published papers.\"", "labels": [], "entities": []}, {"text": "2 Reimers and Gurevych (2017) investigated over 50,000 combinations of hyper-parameter settings, such as word embedding sources and the optimizer across five different NLP tasks and found that these settings have a significant impact on both the variability, and the relative effectiveness of models.", "labels": [], "entities": []}, {"text": "In this paper we present a number of controllable environment settings that often go unreported, and illustrate that these are factors that can cause irreproducibility of results as presented in the literature.", "labels": [], "entities": []}, {"text": "These environmental factors have an effect on the effectiveness of neural networks due to the nonconvexity of the optimization surface, meaning that even minor changes in computation can lead the network to fall into one of a multitude of local minima.", "labels": [], "entities": []}, {"text": "Because these effect sizes are comparable to the largest incremental improvements that have been reported brings into question those improvements, and associated claims of progress.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to limit the scope of this paper, we specifically focus our efforts on a single natural language processing task-answer selection within question answering-elaborated upon in Section 2.1.", "labels": [], "entities": [{"text": "natural language processing task-answer selection within question answering-elaborated", "start_pos": 89, "end_pos": 175, "type": "TASK", "confidence": 0.7782304361462593}]}, {"text": "We also further limit our discussion to look at how these environmental effects manifest in a single implementation of a single model, described in Section 2.2.", "labels": [], "entities": []}, {"text": "These restrictions, however, do not mean that our results are only applicable to this model on this task, rather our discussion generalizes to all neural network based research.", "labels": [], "entities": []}, {"text": "To isolate the effect that each environmental factor has all other settings related to the network are fixed; that is, the hyper-parameters are static across all experiments, and only the environmental variable of interest is manipulated.", "labels": [], "entities": []}, {"text": "Along with each of the presented factors we include suggestions on how to respond to these in order to best ensure that the work, as presented, is reproducible.", "labels": [], "entities": []}, {"text": "The experiments reported in this paper are all performed against the TrecQA dataset that was first released by and further elab-  orated upon by, as well as the WikiQA dataset released by.", "labels": [], "entities": [{"text": "TrecQA dataset", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9450265467166901}, {"text": "WikiQA dataset", "start_pos": 161, "end_pos": 175, "type": "DATASET", "confidence": 0.9304094612598419}]}, {"text": "Both datasets consists of pre-determined training, development and test sets.", "labels": [], "entities": []}, {"text": "For each question, each candidate answer is labelled positive if it contains the answer to the question, otherwise negative.", "labels": [], "entities": []}, {"text": "The ratios of these labels and size of the splits for both datasets are shown in.", "labels": [], "entities": []}, {"text": "The TrecQA dataset has further diverged into two versions, named RAW and CLEAN.", "labels": [], "entities": [{"text": "TrecQA dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9294984042644501}, {"text": "RAW", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.4805315136909485}]}, {"text": "The CLEAN version has removed those questions that had no positive labelled answers.", "labels": [], "entities": [{"text": "CLEAN version", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8830238878726959}]}, {"text": "Results on these two variants are not directly comparable to each other ( , and experiments in this paper are performed against the RAW variant.", "labels": [], "entities": [{"text": "RAW variant", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.8352133333683014}]}, {"text": "Similar manipulation of the WikiQA dataset has also been performed, although no analysis of the comparability of the results has been conducted.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9739228188991547}]}], "tableCaptions": [{"text": " Table 2: State-of-the-art (replicated from ACL Wiki  (ACL, 2017)) results on the TrecQA dataset versions, an- notated with improvement over prior state-of-the-art re- sults and a simple baseline.", "labels": [], "entities": [{"text": "ACL Wiki  (ACL, 2017))", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9298500759260995}, {"text": "TrecQA dataset", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9371849894523621}]}, {"text": " Table 3: State-of-the-art (gathered by manual inspection)  results on the WikiQA dataset, annotated with improve- ment over prior state-of-the-art results.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9705187380313873}]}, {"text": " Table 4: Effect of the version of the model being used on  model results. Only versions that modified the py files  are included. A * indicates that the model at that change- set does not run under the created Docker environment,  and results are taken from a native host, and a + indicates  that the results from this version are themselves not repro- ducible, changing between runs. Version 1f894ba does  not complete due to a bug. A  \u2021 indicates that the result  was statistically significantly different at the p < 0.01  level, and  \u2020 at the p < 0.05 level, compared to cf0e269.", "labels": [], "entities": []}, {"text": " Table 5: Effect of the version of PyTorch being used on  model results. Version 0.1.8 and earlier would not run  the sample model due to API changes. A  \u2020 indicates  that the results are statistically significantly different to  0.1.12 at the p < 0.05 level.", "labels": [], "entities": []}, {"text": " Table 6: Effect of changing math library and architec- ture on model results, using PyTorch 0.1.12. None of  the results are statistically significantly different to the i7- 6800K.", "labels": [], "entities": [{"text": "PyTorch 0.1.12", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.771332710981369}]}, {"text": " Table 7: Effect of number of threads on model results  using MKL-backed PyTorch v0.1.12 on an Intel i7- 6800K processor. None of the results are statistically sig- nificantly different to a single thread.", "labels": [], "entities": []}, {"text": " Table 8: Effect of the computation hardware on model results. None of the results are statistically significantly different  when compared to the Intel i7-6800K.", "labels": [], "entities": []}, {"text": " Table 9: Kendall's \u03c4 and Spearman's \u03c1 based on the rank- ings of model effectiveness on different seeds across met- rics and training computation backend. All values are sta- tistically significant at the p < 0.01 level.", "labels": [], "entities": [{"text": "rank- ings", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.7366149226824442}]}]}