{"title": [{"text": "Probabilistic Verb Selection for Data-to-Text Generation", "labels": [], "entities": [{"text": "Data-to-Text Generation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7154458165168762}]}], "abstractContent": [{"text": "In data-to-text Natural Language Generation (NLG) systems, computers need to find the right words to describe phenomena seen in the data.", "labels": [], "entities": [{"text": "data-to-text Natural Language Generation (NLG)", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.7878630757331848}]}, {"text": "This paper focuses on the problem of choosing appropriate verbs to express the direction and magnitude of a percentage change (e.g., in stock prices).", "labels": [], "entities": []}, {"text": "Rather than simply using the same verbs again and again, we present a principled data-driven approach to this problem based on Shannon's noisy-channel model so as to bring variation and naturalness into the generated text.", "labels": [], "entities": []}, {"text": "Our experiments on three large-scale real-world news corpora demonstrate that the proposed probabilistic model can be learned to accurately imitate human authors' pattern of usage around verbs, outperforming the state-of-the-art method significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is a fundamental task in Artificial Intelligence (AI).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7616342157125473}]}, {"text": "It aims to automatically turn structured data into prose -the opposite of the better-known field of Natural Language Processing (NLP) that transforms raw text into structured data (e.g., a logical form or a knowledge base)).", "labels": [], "entities": []}, {"text": "Being dubbed \"algorithmic authors\" or \"robot journalists\", NLG systems have attracted a lot of attention in recent years, thanks to the rise of big data.", "labels": [], "entities": []}, {"text": "The use of NLG in financial services has been growing very fast.", "labels": [], "entities": []}, {"text": "One particularly important NLG problem for summarizing financial or business data is to automatically generate textual descriptions of trends between two data points (such as stock prices).", "labels": [], "entities": [{"text": "summarizing financial or business", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.9049516469240189}]}, {"text": "In this paper, we elect to use relative percentages rather than absolute numbers to describe the change from one data point to another.", "labels": [], "entities": []}, {"text": "This is because an absolute number might be considered small in one case but large in another, depending on the unit and the context.", "labels": [], "entities": []}, {"text": "For example, 1000 British pounds are worth much more than 1000 Japanese yen; arise of 100 US dollars in car price might be negligible but the same amount of increase in bike price would be significant.", "labels": [], "entities": [{"text": "arise", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9948410391807556}]}, {"text": "Given two data points (e.g., on a stock chart), the percentage change can always be calculated easily.", "labels": [], "entities": []}, {"text": "The challenge is to select the appropriate verb for any percentage change.", "labels": [], "entities": []}, {"text": "For example, in newspapers, we often see headlines like \"Apple's stock had jumped 34% this year in anticipation of the next iPhone . .", "labels": [], "entities": []}, {"text": "\" and \"Microsoft's profit climbed 28% with shift to Web-based software . .", "labels": [], "entities": []}, {"text": "\". The journalists writing such news stories use descriptive language such as the verbs like jump and climb to express the direction and magnitude of a percentage change.", "labels": [], "entities": []}, {"text": "It is of course possible to simply keep using the same neutral verbs, e.g., increase and decrease for upward and downward changes respectively, again and again, as inmost existing datato-text NLG systems.", "labels": [], "entities": []}, {"text": "However, the generated text would sound much more natural if computers could use a variety of verbs suitable in the context like human authors do.", "labels": [], "entities": []}, {"text": "Expressions of percentage changes are readily available in many natural language text datasets and 511 can be easily extracted.", "labels": [], "entities": []}, {"text": "Therefore computers should be able to learn from such expressions how people decide which verbs to use for what kind of percentage changes.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of verb selection for data-to-text NLG through a principled data-driven approach.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7118212431669235}]}, {"text": "Specifically, we show how to employ Bayesian reasoning to train a probabilistic model for verb selection based on large-scale realworld news corpora, and demonstrate its advantages over existing verb selection methods.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7609362006187439}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the related work in literature.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the dataset used for our investigation.", "labels": [], "entities": []}, {"text": "In Section 4, we present our probabilistic model for verb selection in detail.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7540243864059448}]}, {"text": "In Section 5, we conduct experimental evaluation.", "labels": [], "entities": []}, {"text": "In Section 6, we discuss possible extensions to the proposed approach.", "labels": [], "entities": []}, {"text": "In Section 7, we draw conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The end users' perception of a verb selection algorithm's quality depends on not only how accurately the chosen verbs reflect the corresponding percentage changes but also how diverse the chosen verbs are, which are two largely orthogonal dimensions for evaluation.", "labels": [], "entities": []}, {"text": "Accuracy: The easiest way to assess the accuracy of an NLG method or system is to compare the texts generated by computers and the texts written by humans for the same input data, using an automatic metric such as BLEU ().", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9924865961074829}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9977543950080872}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9983568787574768}]}, {"text": "For our task of verb selection, we decide to use the metric MRR that stands for mean reciprocal rank) and can be calculated as follows: where } is the set of test examples, and rank(w i ) refers to the rank position of w i -the verb really used by the human author to describe the percentage change xi -in the list of predicted verbs ranked in the descending order of their probabilities of correctness given by the model.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.713070809841156}, {"text": "MRR", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9545255303382874}]}, {"text": "The MRR metric is most widely used for the evaluation of automatic question answering which is similar to automatic verb selection in the following sense: they both aim to output just one suitable response (answer or verb) to any given input (question or percentage change).", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8365083932876587}, {"text": "question answering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7278925180435181}]}, {"text": "Through 5-fold cross-validation (, we have got the MRR scores of our proposed model (see Section 4) and the two baseline models (see Section 5.1) which are shown in.", "labels": [], "entities": [{"text": "MRR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.892237663269043}]}, {"text": "The models were trained/tested separately on each dataset (see Section 3).", "labels": [], "entities": []}, {"text": "In each round of 5-fold crossvalidation, 20% of the data would become the test set; in the remaining 80% of the data, randomly selected 60% would be the training set and the other 20% would be the development set if parameter tuning is needed (otherwise the whole 80% would be used for training).", "labels": [], "entities": []}, {"text": "The parameter \u03bb of our model controls the strength of smoothing over the prior probability (see Section 4.1) and thus dictates the trade-off between accuracy and diversity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9991382360458374}]}, {"text": "If we focus on the accuracy  only and ignore the diversity, the optimal value of \u03bb should just be 1 (i.e., no smoothing).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9995751976966858}]}, {"text": "In order to strike a healthy balance between accuracy and diversity, we carried out a line search for the value of \u03bb from 0 to 1 with step size 0.05 using the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9991984963417053}]}, {"text": "It turned out that the smoothing effect upon diversity would only become noticeable when \u03bb \u2264 0.1, so we further conducted a line search from 0 to 0.1 with step size 0.01, and found that using \u03bb = 0.05 consistently yield a good performance on different corpora.", "labels": [], "entities": []}, {"text": "Actually, this phenomenon should not be very surprising, given the Zipfian distribution of verbs which is highly skewed (see).", "labels": [], "entities": []}, {"text": "Our observation in the experiments still indicate that smoothing with a none-zero \u03bb worked better than setting \u03bb = 0.", "labels": [], "entities": []}, {"text": "That is to say, it would not be wise to go to extremes to ignore the prior entirely which would unnecessarily harm the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.99842369556427}]}, {"text": "An alternative smoothing solution for mitigating the severe skewness of the empirical prior that we also considered is to make the smoothed prior probability proportional to the logarithm of the raw prior probability, but we did not take that route as (i) we could not find a good principled interpretation for such a trick and; (ii) using a small \u03bb value like 0.05 seemed to work sufficiently well.", "labels": [], "entities": []}, {"text": "It will be shown later that sampling verbs from the posterior probability distribution rather than just using the one with the maximum probability would help to alleviate the problem of prior skewness and thus prevent verb selection from being dominated by the most popular verbs.", "labels": [], "entities": []}, {"text": "It can be observed from the experimental results that smoothing (see Section 4.1) does reduce the accuracy of verb selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9986982345581055}, {"text": "verb selection", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7124927639961243}]}, {"text": "The MRR scores with \u03bb = 0.05 are lower than those with \u03bb = 1.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9375849366188049}]}, {"text": "Nevertheless, as we shall soon see, strong smoothing is crucially important for achieving a good level of diversity.", "labels": [], "entities": []}, {"text": "Furthermore, there seemed to belittle performance difference between the usage of the KDE technique or the Beta distribution to fit the likelihood function in our approach.", "labels": [], "entities": []}, {"text": "This suggests that the latter is preferable because it is as effective as the former but much more efficient.", "labels": [], "entities": []}, {"text": "Therefore, in the remaining part of this paper, we shall focus on this specific version of our model (with \u03bb = 0.05, Beta) even though it may not be the most accurate.", "labels": [], "entities": []}, {"text": "The MRR scores achieved by our approach are around 0.4 -0.8 which implies that, on average, the first or the second verb selected by our approach would be the \"correct\" verb used by human authors.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9202051162719727}]}, {"text": "Across all the three corpora, our proposed probabilistic model, whether it is smoothed or not, whether it uses the KDE technique or the Beta distribution, outperforms the Thomson Reuters baseline by a large margin in terms of MRR.", "labels": [], "entities": [{"text": "Thomson Reuters baseline", "start_pos": 171, "end_pos": 195, "type": "DATASET", "confidence": 0.8789787888526917}, {"text": "MRR", "start_pos": 226, "end_pos": 229, "type": "METRIC", "confidence": 0.8711681365966797}]}, {"text": "According to the Wilcoxon signed-rank test, the performance improvements brought by our approach over the Thomson Reuters baseline are statistically significant with the (two-sided) p-value 0.0001 on the two English corpora and = 0.0027 on the Chinese corpus.", "labels": [], "entities": [{"text": "Thomson Reuters baseline", "start_pos": 106, "end_pos": 130, "type": "DATASET", "confidence": 0.9476269682248434}, {"text": "Chinese corpus", "start_pos": 244, "end_pos": 258, "type": "DATASET", "confidence": 0.8378770053386688}]}, {"text": "With respect to the Neural Network baseline, on all the three corpora, its accuracy is slightly better than that of our smoothed model (\u03bb = 0.05) though it still could not beat our original unsmoothed model (\u03bb = 1).", "labels": [], "entities": [{"text": "Neural Network baseline", "start_pos": 20, "end_pos": 43, "type": "DATASET", "confidence": 0.7936102151870728}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.99973064661026}]}, {"text": "The major problem with the Neural Network baseline is that, similar to the probabilistic model without smoothing, its verb choices would concentrate on the most frequent ones and thus have very poor diversity.", "labels": [], "entities": []}, {"text": "A prominent advantage of our proposed probabilistic model, in comparison with discriminative learning algorithms such as the Neural Network baseline, is that we are able to explicitly control the trade-off between accuracy and diversity by adjusting the strength of smoothing.", "labels": [], "entities": [{"text": "Neural Network baseline", "start_pos": 125, "end_pos": 148, "type": "DATASET", "confidence": 0.7093331317106882}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9981573224067688}]}, {"text": "It is worth emphasizing that the accuracy of a verb selection method only reflects its ability to imitate how writers (journalists) use verbs, but this is not necessarily the same as how readers interpret the verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9988231062889099}]}, {"text": "Usually the ultimate goal of an NLG system is to successfully communicate information to readers.", "labels": [], "entities": []}, {"text": "Previous research in NLG and psychology suggests that there is wide variation in how different people interpret verbs and words in general, which is probably much larger in the general population than amongst journalists.", "labels": [], "entities": []}, {"text": "Specifically, the MRR metric would probably underestimate the effectiveness of a verb selection method, since a verb different from the one really used by the writer is not necessarily a less appropriate choice for the corresponding percentage change from the reader's perspective.", "labels": [], "entities": [{"text": "MRR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6301977634429932}]}, {"text": "Diversity: Other than the accuracy of reproducing the verb choices made by human authors, verb selection methods could also be automatically evaluated in terms of diversity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9975062012672424}]}, {"text": "Following, we borrow the diversity measures from ecology to quantitatively analyze the diversity of verb choices: each specific verb is considered as a particular species.", "labels": [], "entities": []}, {"text": "When measuring the biological diversity of a habitant, it is important to consider not only the number of distinct species present but also the relative abundance of each species.", "labels": [], "entities": []}, {"text": "In the literature of ecology, the former is called richness and the latter is called evenness.", "labels": [], "entities": []}, {"text": "Here we utilize the well-known Inverse Simpson Index aka Simpson's Reciprocal Index which takes both richness and evenness into account: R is the total number of distinct species (i.e., richness), and pi is the the proportion of the individuals belonging to the i-th species relative to the entire population.", "labels": [], "entities": [{"text": "Inverse Simpson Index aka Simpson's Reciprocal Index", "start_pos": 31, "end_pos": 83, "type": "DATASET", "confidence": 0.554544523358345}]}, {"text": "The evenness is given by the value of diversity normalized to the range between 0 and 1, so it can be calculated as D/R. shows the diversity scores of verb choices made by our approach and the Thomson Reuters baseline for 450 randomly sampled percentage changes (see Section 5.4).", "labels": [], "entities": [{"text": "D/R.", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.8847975134849548}, {"text": "Thomson Reuters baseline", "start_pos": 193, "end_pos": 217, "type": "DATASET", "confidence": 0.9133401314417521}]}, {"text": "Overall, in terms of diversity, our approach would lose to Thomson Reuters.", "labels": [], "entities": [{"text": "Thomson Reuters", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9470402300357819}]}, {"text": "The Neural Network baseline is omitted here because its diversity scores were very low.", "labels": [], "entities": [{"text": "Neural Network baseline", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.6809511085351309}]}, {"text": "Discussion: show the confusion matrices of our approach (\u03bb = 0.05, Beta) on the WSJ corpus as (row-normalized) heatmaps: in the former we choose the verb with the highest posterior probability (argmax) while in the latter we sample the verb from the posterior probability distribution (see Section 4).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.979909211397171}]}, {"text": "The argmax way would be dominated by a few verbs (e.g., \"rise\", \"soar\", \"fall\", and \"plummet\").", "labels": [], "entities": []}, {"text": "In contrast, random sampling would lead to a much wider variety of verbs.", "labels": [], "entities": []}, {"text": "The experimental results of all verb selection methods reported in this paper are generated by the sampling strategy, if not indicated otherwise.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.6980428546667099}]}, {"text": "It can be seen from that the verbs \"soar\" and \"plunge\" are the easiest to be predicted.", "labels": [], "entities": []}, {"text": "Generally speaking, the prediction of verbs is relatively more accurate for bigger percentage changes, whether upwards or downwards.", "labels": [], "entities": []}, {"text": "This is probably because there are fewer verbs available to describe such radical percentage changes (see) and thus the model faces less uncertainty.", "labels": [], "entities": []}, {"text": "Most misclassification (confusion) happens when a verb is incorrectly predicted to be the most frequent one (\"rise\" or \"fall\").", "labels": [], "entities": []}, {"text": "The two aspects, accuracy and diversity, are both important for the task of verb selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993658661842346}, {"text": "diversity", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9693292379379272}, {"text": "verb selection", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7210795879364014}]}, {"text": "Although we have shown that automatic evaluation could be car-: The results of human evaluation, where the p-values are given by the sign test (two-sided).", "labels": [], "entities": []}, {"text": "ried out for either accuracy or diversity alone, there is no obvious way to assess the overall effectiveness of a verb selection method using machines only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9991459846496582}]}, {"text": "The ultimate judgment on the quality of verb selection would have to come from human assessors.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7129416018724442}]}, {"text": "To manually compare our approach (the version with \u03bb = 0.05, Beta) with a baseline method (Thomson Reuters or Neural Network), we conduct a questionnaire survey with 450 multiple-choice questions.", "labels": [], "entities": [{"text": "Thomson Reuters or Neural Network", "start_pos": 91, "end_pos": 124, "type": "DATASET", "confidence": 0.8686693906784058}]}, {"text": "In each question, a respondent would see a pair of generated sentences describing the same percentage change with the verbs selected by two different methods respectively and need to judge which one sounds better than the other (or it is hard to tell).", "labels": [], "entities": []}, {"text": "For example, a respondent could be shown the following pair of generated sentences: (1) Net profit declines 3% (2) Net profit plummets 3% and then they were supposed to choose one of the three following options as their answer: [a] Sentence (1) sounds better.", "labels": [], "entities": []}, {"text": "[b] Sentence (2) sounds better.", "labels": [], "entities": []}, {"text": "[c] They are equally good.", "labels": [], "entities": []}, {"text": "The respondents would be blinded to whether the first verb or the second verb was provided by our proposed method, as their appearing order would have been randomized in advance.", "labels": [], "entities": []}, {"text": "The questionnaire survey system withheld the information about the source of each verb until the answers from all respondents had been collected, and then it would count how many times the verb selected by our proposed method was deemed better than (>), worse than (<), or as good as (\u2248) the verb selected by the baseline method.", "labels": [], "entities": []}, {"text": "For each corpus, we produced 150 different questions, of which half were about upward verbs and half were about downward verbs.", "labels": [], "entities": []}, {"text": "As we have explained above, each question compares a pair of generated sentences describing the same percentage change with different verbs.", "labels": [], "entities": []}, {"text": "The sentence generation process is the same as that used by . The subjects were randomly picked from the most popular ones in the corpus (e.g., \"gross domestic product\"), and the percentage changes (as the objects) were randomly sampled from the corpus as well.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.718244805932045}]}, {"text": "Each of the two verb selection methods, in comparison, would provide one verb (as the predicate) for describing that specific percentage change.", "labels": [], "entities": []}, {"text": "Note that in this sentence generation process, a pair of sentences would be retained only if the verbs selected by the two methods were different, as it would be meaningless to compare two identical sentences.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7897451221942902}]}, {"text": "A total of 15 college-educated people participated in the questionnaire survey.", "labels": [], "entities": []}, {"text": "They are all bilingual, i.e., native or fluent speakers of both English and Chinese.", "labels": [], "entities": []}, {"text": "Each person was given 30 questions: 10 questions (including 5 upward and 5 downward ones) from each corpus.", "labels": [], "entities": []}, {"text": "We (the authors of this paper) were excluded from participating in the questionnaire survey to avoid any conscious or unconscious bias.", "labels": [], "entities": []}, {"text": "The results of human evaluation are shown in.", "labels": [], "entities": []}, {"text": "Altogether, respondents prefer the verb selected by our approach 234/450=52% of times, as opposed to 186/450=41% for the Thomson Reuters baseline; respondents prefer the verb selected by our approach 290/450=64% of times, as opposed to 144/450=32% for the Neural Network baseline.", "labels": [], "entities": [{"text": "Thomson Reuters baseline", "start_pos": 121, "end_pos": 145, "type": "DATASET", "confidence": 0.943453868230184}, {"text": "Neural Network baseline", "start_pos": 256, "end_pos": 279, "type": "DATASET", "confidence": 0.8454035719235738}]}, {"text": "According to the sign test, our approach works significantly better than the two baseline methods, Thomson Reuters and Neural Network: overall the (two-sided) p-values are less than 0.05.", "labels": [], "entities": []}, {"text": "Discussion: Our approach exhibits more superiority over the Thomson Reuters baseline on the English datasets than on the Chinese dataset.", "labels": [], "entities": [{"text": "Thomson Reuters baseline", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.9584048787752787}, {"text": "English datasets", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9098900854587555}, {"text": "Chinese dataset", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.7836036682128906}]}, {"text": "Since the Chinese dataset is bigger than the Reuters dataset, though smaller than the WSJ dataset, the performance difference is not caused by corpus size but due to language characteristics.", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8874305188655853}, {"text": "Reuters dataset", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9847534894943237}, {"text": "WSJ dataset", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.989684522151947}]}, {"text": "Remember that for Chinese we are actually predicting adverb+verb combinations (see Section 3.3).", "labels": [], "entities": [{"text": "predicting adverb+verb combinations", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.8620420455932617}]}, {"text": "Retrospective manual inspection of the experimental results suggests that users seem to have relatively higher expectations of diversity for Chinese adverbs than for English verbs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The descriptive statistics of percentage changes (in %) for each verb, in the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.960131824016571}]}, {"text": " Table 2: The accuracy of verb selection measured by MRR (mean\u00b1std) via 5-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995895028114319}, {"text": "verb selection", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.6886726319789886}, {"text": "MRR", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9979100823402405}]}, {"text": " Table 3: The diversity of verb selection measured by the Inverse Simpson Index.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6873092204332352}, {"text": "Inverse Simpson Index", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.5960556368033091}]}, {"text": " Table 4: The results of human evaluation, where the p-values are given by the sign test (two-sided).", "labels": [], "entities": []}]}