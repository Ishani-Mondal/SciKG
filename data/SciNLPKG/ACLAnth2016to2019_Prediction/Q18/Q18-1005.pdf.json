{"title": [], "abstractContent": [{"text": "In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations.", "labels": [], "entities": []}, {"text": "Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.", "labels": [], "entities": []}, {"text": "Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases.", "labels": [], "entities": []}, {"text": "Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both inter-pretable and meaningful.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling), summarization), sentiment analysis (, question answering (, and machine translation.", "labels": [], "entities": [{"text": "Document modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8932549357414246}, {"text": "topic labeling", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7095712423324585}, {"text": "summarization", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.9900097250938416}, {"text": "sentiment analysis", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.9640559554100037}, {"text": "question answering", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.8929303586483002}, {"text": "machine translation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7951992750167847}]}, {"text": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (.", "labels": [], "entities": []}, {"text": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST;, graphs), entity transitions (, or combinations thereof (.", "labels": [], "entities": []}, {"text": "The availability of discourse annotated corpora) has led to the development of off-the-shelf discourse parsers (e.g.,, and the common use of trees as representations of document structure.", "labels": [], "entities": []}, {"text": "For example, improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.", "labels": [], "entities": [{"text": "document-level sentiment analysis", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.8295837044715881}]}, {"text": "Linguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers).", "labels": [], "entities": []}, {"text": "Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7084808945655823}]}, {"text": "Moreover, despite recent advances in discourse processing, the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance.", "labels": [], "entities": []}, {"text": "It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations.", "labels": [], "entities": []}, {"text": "The main idea is to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (.", "labels": [], "entities": []}, {"text": "further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism () which acknowledges that sentences are differentially important in different contexts.", "labels": [], "entities": []}, {"text": "Their model learns to pay more or less attention to individual sentences when constructing the representation of the document.", "labels": [], "entities": []}, {"text": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (.", "labels": [], "entities": []}, {"text": "introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees.", "labels": [], "entities": []}, {"text": "Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables.", "labels": [], "entities": []}, {"text": "They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency tree.", "labels": [], "entities": []}, {"text": "Without recourse to an external parser, their model learns meaningful task-specific dependency structures, achieving competitive results in several sentence-level tasks.", "labels": [], "entities": []}, {"text": "However, for document modeling, this approach has two drawbacks.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8282471299171448}]}, {"text": "Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis ().", "labels": [], "entities": [{"text": "documentlevel discourse analysis", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.5778644581635793}]}, {"text": "As illustrated in, the tree structure of a document can be flexible and the dependency edges may cross.", "labels": [], "entities": []}, {"text": "Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew model for representing documents while automatically learning richer structural dependencies.", "labels": [], "entities": []}, {"text": "Using a variant of, our model implicitly considers non-projective depen- Figure 1: The document is analyzed in the style of Rhetorical Structure Theory (, and represented as a dependency tree following the conversion algorithm of. dency tree structures.", "labels": [], "entities": []}, {"text": "We keep each step of the learning process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser.", "labels": [], "entities": []}, {"text": "The inside-outside model of and our model both have a O(n 3 ) worst case complexity.", "labels": [], "entities": [{"text": "O(n 3 ) worst case complexity", "start_pos": 54, "end_pos": 83, "type": "METRIC", "confidence": 0.8570567741990089}]}, {"text": "However, major operations in our approach can be parallelized efficiently on GPU computing hardware.", "labels": [], "entities": []}, {"text": "Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.745648205280304}]}, {"text": "Advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word order.", "labels": [], "entities": []}, {"text": "Our contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experiments for evaluating the performance of our model.", "labels": [], "entities": []}, {"text": "Since sentence representations constitute the basic building blocks of our document model, we first evaluate the performance of structured attention on a sentence-level task, namely natural language inference.", "labels": [], "entities": []}, {"text": "We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language.", "labels": [], "entities": []}, {"text": "Our code is available at https://github.com/ nlpyang/structured.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracy on the SNLI dataset and number of parameters \u03b8 (excluding embeddings). Wherever  available we also provide the size of the recurrent unit.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.998871386051178}, {"text": "SNLI dataset", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.8645943999290466}]}, {"text": " Table 2: Comparison of speed of different models  on the SNLI testset. The unit of measurement is  seconds per instance. All results were obtained on  a GeForce GTX TITAN X (Pascal) GPU.", "labels": [], "entities": [{"text": "SNLI testset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.827134519815445}]}, {"text": " Table 3: Dataset statistics; #class is the number  of classes per dataset, #docs denotes the number  of documents; #s/d and #w/d represent the average  number of sentences and words per document.", "labels": [], "entities": []}, {"text": " Table 4: Test accuracy on four datasets and number of parameters \u03b8 (excluding embeddings). Regarding  feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ  movies from Brychc\u0131n and Habernal (2013), and Debates from", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994001388549805}, {"text": "Yelp", "start_pos": 152, "end_pos": 156, "type": "DATASET", "confidence": 0.9482802152633667}, {"text": "IMDB", "start_pos": 161, "end_pos": 165, "type": "DATASET", "confidence": 0.7696979641914368}, {"text": "CZ  movies from Brychc\u0131n and Habernal (2013)", "start_pos": 205, "end_pos": 249, "type": "DATASET", "confidence": 0.8480561441845365}]}, {"text": " Table 5: Descriptive statistics for dependency trees  produced by our model and the Stanford parser  (Manning et al., 2014) on the SNLI test set.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.910412053267161}]}, {"text": " Table 6:  Descriptive statistics for induced  document-level dependency trees across datasets.", "labels": [], "entities": []}]}