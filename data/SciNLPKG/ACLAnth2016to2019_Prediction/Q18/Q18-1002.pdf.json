{"title": [{"text": "Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8091369271278381}]}], "abstractContent": [{"text": "We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL).", "labels": [], "entities": [{"text": "fine-grained sentiment analysis", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6832609673341116}, {"text": "multiple instance learning (MIL)", "start_pos": 80, "end_pos": 112, "type": "TASK", "confidence": 0.6448901494344076}]}, {"text": "Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments , i.e. sentences or elementary discourse units (EDUs), without segment-level supervision.", "labels": [], "entities": [{"text": "document sentiment labels", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7132967710494995}, {"text": "predict the sentiment of text segments , i.e. sentences or elementary discourse units (EDUs)", "start_pos": 72, "end_pos": 164, "type": "TASK", "confidence": 0.5828865580260754}]}, {"text": "We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and anew dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MIL-style sentiment models like ours.", "labels": [], "entities": [{"text": "MIL-style sentiment", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.8953909575939178}]}, {"text": "Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.", "labels": [], "entities": [{"text": "EDU-level opinion extraction", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.6878482302029928}]}], "introductionContent": [{"text": "Sentiment analysis has become a fundamental area of research in Natural Language Processing thanks to the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9599782526493073}]}, {"text": "A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions.", "labels": [], "entities": []}, {"text": "The success of neural networks in a variety of applications () and the availability of large amounts of labeled data have led to an increased focus on sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.9583006203174591}]}, {"text": "Supervised models are typically trained on documents, sentences, or phrases; ] I had a very mixed experience at The Stand.", "labels": [], "entities": [{"text": "The Stand", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.9631563723087311}]}, {"text": "The burger and fries were good.", "labels": [], "entities": []}, {"text": "The chocolate shake was divine: rich and creamy.", "labels": [], "entities": []}, {"text": "It took us at least 30 minutes to order when there were only four cars in front of us.", "labels": [], "entities": []}, {"text": "We complained about the wait and got a half-hearted apology.", "labels": [], "entities": []}, {"text": "I would go back because the food is good, but my only hesitation is the wait.", "labels": [], "entities": [{"text": "wait", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9588485360145569}]}], "datasetContent": [{"text": "In this section we describe the data used to assess the performance of our model.", "labels": [], "entities": []}, {"text": "We also give details on model training and comparison systems.: SPOT dataset: numbers of documents and segments with polarity annotations.", "labels": [], "entities": [{"text": "SPOT dataset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.7155588567256927}]}, {"text": "Our models were trained on two large-scale sentiment classification collections.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8659600615501404}]}, {"text": "The Yelp'13 corpus was introduced in and contains customer reviews of local businesses, each associated with human ratings on a scale from 1 (negative) to 5 (positive).", "labels": [], "entities": [{"text": "Yelp'13 corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9716455340385437}]}, {"text": "The IMDB corpus of movie reviews was obtained from; each review is associated with user ratings ranging from 1 to 10.", "labels": [], "entities": [{"text": "IMDB corpus of movie reviews", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9488022685050964}]}, {"text": "Both datasets are split into training (80%), validation (10%) and test (10%) sets.", "labels": [], "entities": []}, {"text": "A summary of statistics for each collection is provided in.", "labels": [], "entities": []}, {"text": "In order to evaluate model performance on the segment level, we constructed anew dataset named SPOT (as a shorthand for Segment POlariTy) by annotating documents from the Yelp'13 and IMDB collections.", "labels": [], "entities": [{"text": "Yelp'13 and IMDB collections", "start_pos": 171, "end_pos": 199, "type": "DATASET", "confidence": 0.7995624840259552}]}, {"text": "Specifically, we sampled reviews from each collection such that all document-level classes are represented uniformly, and the document lengths are representative of the respective corpus.", "labels": [], "entities": []}, {"text": "Documents were segmented into sentences and EDUs, resulting in two segment-level datasets per collection.", "labels": [], "entities": []}, {"text": "Each review was presented to three Amazon Mechanical Turk (AMT) annotators who were asked to judge the sentiment conveyed by each segment (i.e., sentence or EDU) as negative, neutral, or pos- itive.", "labels": [], "entities": []}, {"text": "We assigned labels using a majority vote or a fourth annotator in the rare cases of no agreement (< 5%).", "labels": [], "entities": []}, {"text": "shows the distribution of segment labels for each document-level class.", "labels": [], "entities": []}, {"text": "As expected, documents with positive labels contain a larger number of positive segments compared to documents with negative labels and vice versa.", "labels": [], "entities": []}, {"text": "Neutral segments are distributed in an approximately uniform manner across document classes.", "labels": [], "entities": []}, {"text": "Interestingly, the proportion of neutral EDUs is significantly higher compared to neutral sentences.", "labels": [], "entities": []}, {"text": "The observation reinforces our argument in favor of EDU segmentation, as it suggests that a sentence with positive or negative overall polarity may still contain neutral EDUs.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.88133504986763}]}, {"text": "Discarding neutral EDUs, could therefore lead to more concise opinion extraction compared to relying on entire sentences.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7116058021783829}]}, {"text": "We further experimented on two collections introduced by which also originate from the YELP'13 and IMDB datasets.", "labels": [], "entities": [{"text": "YELP'13", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9301912784576416}, {"text": "IMDB datasets", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.8459386229515076}]}, {"text": "Each collection consists of 1,000 randomly sampled sentences annotated with binary sentiment labels.", "labels": [], "entities": []}, {"text": "We trained MILNET and HIERNET using Adadelta and 5 words with 100 feature maps per window size, resulting in 300-dimensional segment vectors.", "labels": [], "entities": []}, {"text": "The GRU hidden vector dimensions for each direction were set to 50 and the attention vector dimensionality to 100.", "labels": [], "entities": [{"text": "GRU hidden vector dimensions", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.8109226524829865}, {"text": "attention vector dimensionality", "start_pos": 75, "end_pos": 106, "type": "METRIC", "confidence": 0.8307521740595499}]}, {"text": "We used L2-normalization and dropout to regularize the softmax classifiers and additional dropout on the internal GRU connections.", "labels": [], "entities": []}, {"text": "Real-valued polarity scores produced by the two models are mapped to discrete labels using two appropriate thresholds t 1 , t 2 \u2208 [\u22121, 1], so that a segment sis classified as negative if polarity(s) < t 1 , positive if polarity(s) > t 2 or neutral otherwise.", "labels": [], "entities": []}, {"text": "To evaluate performance, we use macro-averaged F1 which is unaffected by class imbalance.", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.8761702179908752}]}, {"text": "We select optimal thresholds using 10-fold cross-validation and report mean scores across folds.", "labels": [], "entities": []}, {"text": "The fully-supervised convolutional segment classifier (Seg-CNN) uses the same window size and feature map configuration as our segment encoder.", "labels": [], "entities": []}, {"text": "Seg-CNN was trained on SPOT using segment labels directly and 10-fold cross-validation (identical folds as in our main models).", "labels": [], "entities": [{"text": "Seg-CNN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8978890180587769}, {"text": "SPOT", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.8801921606063843}]}, {"text": "Seg-CNN is not directly comparable to MILNET (or HIERNET) due to differences in supervision type (segment vs. document labels) and training size (1K-2K segment labels vs. \u223c250K document labels).", "labels": [], "entities": []}, {"text": "However, the comparison is indicative of the utility of fine-grained sentiment predictors that do not rely on expensive segment-level annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Document-level sentiment classification  datasets used to train our models.", "labels": [], "entities": [{"text": "Document-level sentiment classification", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.7230215469996134}]}, {"text": " Table 2: SPOT dataset: numbers of documents and  segments with polarity annotations.", "labels": [], "entities": [{"text": "SPOT dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.7543688714504242}]}, {"text": " Table 3: Segment classification results (in macro- averaged F1).  \u2020 indicates that the system in question  is significantly different from MILNET gt (approxi- mate randomization test", "labels": [], "entities": [{"text": "Segment classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9665510058403015}, {"text": "macro- averaged F1", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.5626679286360741}]}, {"text": " Table 4: F1 scores  for neutral segments  (Yelp'13).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999554455280304}, {"text": "Yelp'13", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8914807438850403}]}, {"text": " Table 5: Accuracy scores  on the sentence classi- fication datasets intro- duced in Kotzias et al.  (2015).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976625442504883}]}, {"text": " Table 6: Human evaluation results (in percentages).   \u2020 indicates that the system in question is signifi- cantly different from MILNET (sign-test, p < 0.01).", "labels": [], "entities": [{"text": "MILNET", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.5440728068351746}]}]}