{"title": [{"text": "Leveraging Orthographic Similarity for Multilingual Neural Transliteration", "labels": [], "entities": [{"text": "Multilingual Neural Transliteration", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.6258561511834463}]}], "abstractContent": [{"text": "We address the task of joint training of translit-eration models for multiple language pairs (multilingual transliteration).", "labels": [], "entities": []}, {"text": "This is an instance of multitask learning, where individual tasks (language pairs) benefit from sharing knowledge with related tasks.", "labels": [], "entities": []}, {"text": "We focus on transliteration involving related tasks i.e., languages sharing writing systems and phonetic properties (orthographically similar languages).", "labels": [], "entities": []}, {"text": "We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity.", "labels": [], "entities": []}, {"text": "We show that multilingual transliteration significantly outperforms bilingual translitera-tion in different scenarios (average increase of 58% across a variety of languages we experimented with).", "labels": [], "entities": []}, {"text": "We also show that multilingual transliteration models can generalize well to languages/language pairs not encountered during training and hence perform well on the ze-roshot transliteration task.", "labels": [], "entities": []}, {"text": "We show that further improvements can be achieved by using phonetic feature input.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration is a key building block for multilingual and cross-lingual NLP since it is essential for (i) handling of names in applications like machine translation (MT) and cross-lingual information retrieval (CLIR), and (ii) user-friendly input methods.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.8367633044719696}, {"text": "cross-lingual information retrieval (CLIR)", "start_pos": 177, "end_pos": 219, "type": "TASK", "confidence": 0.7683903028567632}]}, {"text": "The transliteration problem has been extensively studied in literature fora variety of language pairs).", "labels": [], "entities": []}, {"text": "Previous work has looked at the most natural setup -training on a single language pair.", "labels": [], "entities": []}, {"text": "However, no prior work exists on jointly training multiple language pairs (referred to as multilingual transliteration henceforth).", "labels": [], "entities": []}, {"text": "Multilingual transliteration can be seen as an instance of multi-task learning, where training each language pair constitutes a task.", "labels": [], "entities": []}, {"text": "Multi-task learning works best when the tasks are related to each other, so sharing of knowledge across tasks is beneficial.", "labels": [], "entities": []}, {"text": "Thus, multilingual transliteration can be beneficial, if the languages involved are related.", "labels": [], "entities": []}, {"text": "We identify such a natural and practically useful scenario: multilingual transliteration involving languages that are related on account of sharing writing systems and phonetic properties.", "labels": [], "entities": []}, {"text": "We refer to such languages as orthographically similar languages.", "labels": [], "entities": []}, {"text": "We say that two languages are orthographically similar if they have: (i) highly overlapping phoneme sets, (ii) mutually compatible orthographic systems, and (iii) similar grapheme to phoneme mappings.", "labels": [], "entities": []}, {"text": "For instance, Indo-Aryan languages largely share the same set of phonemes.", "labels": [], "entities": []}, {"text": "They use different Indic scripts, but correspondences can be established between equivalent characters across scripts.", "labels": [], "entities": []}, {"text": "For example, the Hindi (Devanagari script) character \u0915 (ka) maps to the Bengali \u0995 (ka) which stands for the consonant sound (IPA: k).", "labels": [], "entities": []}, {"text": "The grapheme to phoneme mapping is also consistent for equivalent characters.", "labels": [], "entities": []}, {"text": "We can identify two major sources of orthographic similarity: (a) genetic relationship between languages (groups like Romance, Slavic, Indo-Aryan and Turkic languages) (b) prolonged contact between languages over along period of time, e.g. convergence in phonological properties of the Indo-Aryan and Dravidian languages in the Indian subcontinent, most strikingly retroflex consonants.", "labels": [], "entities": []}, {"text": "Dravidian and Indo-Aryan languages use compatible Indic scripts.", "labels": [], "entities": []}, {"text": "Another example is the Nigerian linguistic area comprising Niger-Congo languages like Yoruba, Fula, Igbo and Afro-Asiatic languages like Hausa (the most widely spoken language in Nigeria).", "labels": [], "entities": []}, {"text": "Most languages use the Latin script (some use Ajani, a modified Arabic script).", "labels": [], "entities": []}, {"text": "In this work, we explore multilingual transliteration involving orthographically similar languages.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, ours is the first work to address the task of multilingual transliteration.", "labels": [], "entities": []}, {"text": "We propose that transliteration involving orthographically similar languages is a scenario where multilingual training can be very beneficial.", "labels": [], "entities": []}, {"text": "Since these languages share phonological properties, the transliteration tasks are clearly related.", "labels": [], "entities": []}, {"text": "We can utilize this relatedness by sharing the vocabulary across all related languages.", "labels": [], "entities": []}, {"text": "The grapheme-to-grapheme correspondences enable vocabulary sharing.", "labels": [], "entities": [{"text": "vocabulary sharing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7303090393543243}]}, {"text": "It helps transfer knowledge across languages while training.", "labels": [], "entities": []}, {"text": "For instance, if the network learns that the English character l maps to the Hindi character \u0932 (la), it would also learn that l maps to the corresponding Kannada character \u0cb2 (la).", "labels": [], "entities": []}, {"text": "Data from both Kannada and Hindi datasets will reinforce the evidence for this mapping.", "labels": [], "entities": []}, {"text": "A similar argument can be made when both the source and target languages are related.", "labels": [], "entities": []}, {"text": "The graphemegrapheme correspondences arise from the underlying phoneme-phoneme correspondences.", "labels": [], "entities": []}, {"text": "The consistent grapheme-phoneme mappings help establish the grapheme-grapheme correspondences.", "labels": [], "entities": []}, {"text": "Due to the utilization of language relatedness, the benefits that are typically ascribed to multi-task learning) may also apply to multilingual transliteration.", "labels": [], "entities": []}, {"text": "Since related languages share characters, it is possible to share representations across languages.", "labels": [], "entities": []}, {"text": "This may help to generalize transliteration models since joint training provides an inductive bias which prefers models that are better at transliterating multiple language pairs.", "labels": [], "entities": []}, {"text": "The training may also benefit from implicit data augmentation since training data from multiple language pairs is available.", "labels": [], "entities": []}, {"text": "From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data.", "labels": [], "entities": []}, {"text": "This is particularly beneficial in low-resource scenarios.", "labels": [], "entities": []}, {"text": "Our work adds to the increasing body of work investigating multilingual training for various NLP tasks like POS tagging (), NER ( and machine translation () with a view to learn models that generalize across languages and make effective use of scarce training data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.81547611951828}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7986412346363068}]}, {"text": "The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters.", "labels": [], "entities": []}, {"text": "This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity.", "labels": [], "entities": []}, {"text": "We empirically show that models with maximal parameter sharing are beneficial, without increasing the model size.", "labels": [], "entities": []}, {"text": "(2) We show that multilingual transliteration exhibits significant improvement in transliteration accuracy over bilingual transliteration in different scenarios (average improvement of 58%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9897951483726501}]}, {"text": "Our results are backed by extensive experiments on 8 languages across 2 orthographically similar language groups.", "labels": [], "entities": []}, {"text": "(3) We perform an error analysis which suggests that representations learnt by the encoder in multilingual transliteration can reduce transliteration ambiguities.", "labels": [], "entities": []}, {"text": "Multilingual transliteration also seems better at learning canonical transliterations instead of alternative, phonetically equivalent transliterations.", "labels": [], "entities": []}, {"text": "These could explain the improved performance of multilingual transliteration.", "labels": [], "entities": []}, {"text": "(4) We explore the zeroshot transliteration task (i.e., transliteration between languages/language pairs not seen during training) and show that our multilingual model can generalize well to unseen languages/language pairs.", "labels": [], "entities": []}, {"text": "Notably, the zeroshot transliteration results mostly outperform the direct bilingual transliteration model.", "labels": [], "entities": []}, {"text": "(5) We have richer phonetic information at our disposal for some related languages.", "labels": [], "entities": []}, {"text": "We propose a novel method to incorporate phonetic input in the model and show that it provides modest gains for multilingual transliteration.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 formalizes the multilingual transliteration task and describes our proposed solution.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 discuss the experimental setup, results and analysis.", "labels": [], "entities": []}, {"text": "Section 6 dis-cusses various zeroshot transliteration scenarios, our solutions and the results of experiments.", "labels": [], "entities": []}, {"text": "Section 7 discusses incorporation of phonetic information for multilingual transliteration.", "labels": [], "entities": []}, {"text": "Section 8 concludes the work and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe our experimental setup.", "labels": [], "entities": []}, {"text": "Network Details: The CNN encoder has 4 filters (widths 1 to 4) of 128 hidden units each in the convolutional layer (encoder output size=512).", "labels": [], "entities": []}, {"text": "We use astride size of 1 and the SAME padding for the convolutional and max-pooling layers.", "labels": [], "entities": [{"text": "SAME", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7737607359886169}]}, {"text": "The decoder is a single layer of 512 LSTM units.", "labels": [], "entities": []}, {"text": "We used the same configuration for both bilingual and multilingual experiments across all datasets for convenience after exploration on some language pairs.", "labels": [], "entities": []}, {"text": "We apply dropout () (probability=0.5) at the output of the encoder and decoder, and SGD with the ADAM optimizer () (learning rate=0.001).", "labels": [], "entities": []}, {"text": "We trained our models fora maximum of 40 epochs (which we found sufficient for our models to converge) and a batch size of 32.", "labels": [], "entities": []}, {"text": "In each training epoch, we cycle through the parallel corpora of each language pair.", "labels": [], "entities": []}, {"text": "The parallel corpora are roughly of the same size.", "labels": [], "entities": []}, {"text": "Better training schedules could be explored in future.", "labels": [], "entities": []}, {"text": "Languages: We experimented with two sets of orthographically similar languages: Indian languages: (i) Hindi (hi), Bengali (bn) from the Indo-Aryan branch of Indo-European family (ii) Kannada (kn), Tamil (ta) from the Dravidian family.", "labels": [], "entities": []}, {"text": "We studied Indic-Indic transliteration and transliteration involving a non-Indian language (English\u2194Indic).", "labels": [], "entities": []}, {"text": "We mapped equivalent characters in different Indic scripts in order to build a common vocabulary based on the common offsets of the Unicode codepoints (.", "labels": [], "entities": []}, {"text": "Slavic languages: Czech (cs), Polish (pl), Slovenian (sl) and Slovak (sk).", "labels": [], "entities": []}, {"text": "We studied Arabic\u2194Slavic transliteration.", "labels": [], "entities": []}, {"text": "Arabic is a non-Slavic language (Semitic branch of Afro-Asiatic) and uses an abjad script in which vowel diacritics are omitted in general usage.", "labels": [], "entities": []}, {"text": "The languages chosen are representative of languages spoken by some major groups of peoples en-Indic Indic-en Indic-Indic ar-Slavic en-hi 12K hi-en 18K bn kn ta ar-cs 15K en-bn 13K bn-en 12K hi 3620 5085 5290 ar-pl 15K en-kn 10K kn-en 15K bn 2720 2901 ar-sl 10K en-ta 10K ta-en 15K kn 4216 ar-sk 10K: Examples of transliteration pairs from our datasets which exhibit orthographic similarity: Indic, Romance, Germanic, Slavic, etc.", "labels": [], "entities": []}, {"text": "These languages are spoken by around 2 billion people.", "labels": [], "entities": []}, {"text": "So our approach addresses a major chunk of the world's people.", "labels": [], "entities": []}, {"text": "Datasets: (See for statistics of datasets).", "labels": [], "entities": []}, {"text": "We used the official NEWS 2015 shared task dataset () for English to Indic transliteration.", "labels": [], "entities": [{"text": "NEWS 2015 shared task dataset", "start_pos": 21, "end_pos": 50, "type": "DATASET", "confidence": 0.8844006538391114}]}, {"text": "This dataset has been used for many editions of the NEWS shared tasks.", "labels": [], "entities": [{"text": "NEWS shared tasks", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.773903489112854}]}, {"text": "We split the NEWS 2015 training dataset as the train and validation data for Indic-English transliteration.", "labels": [], "entities": [{"text": "NEWS 2015 training dataset", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.9519105553627014}]}, {"text": "For testing, we used the NEWS 2015 dev-test set.", "labels": [], "entities": [{"text": "NEWS 2015 dev-test set", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9689907133579254}]}, {"text": "We created the Indian-Indian parallel transliteration corpora from the English to Indian language training corpora of the NEWS 2015 dataset by mining name pairs which have English names in common.", "labels": [], "entities": [{"text": "NEWS 2015 dataset", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.9111378987630209}]}, {"text": "We mined the Arabic-Slavic dataset from Wikidata), a structured knowledge base containing items (roughly entities of interest).", "labels": [], "entities": []}, {"text": "Each item has a label (title of item page) which is available in multiple languages.", "labels": [], "entities": []}, {"text": "We extracted labels from selected items referring to named entities (persons, organizations and locations) to ensure that we extract parallel transliterations (as opposed to translations).", "labels": [], "entities": []}, {"text": "Evaluation: We use top-1 exact match accuracy as the evaluation metric ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.5191671252250671}]}, {"text": "This is one of the metrics in the NEWS shared tasks on transliteration.", "labels": [], "entities": [{"text": "NEWS shared tasks", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.7265625596046448}]}], "tableCaptions": [{"text": " Table 5: Results of experiments under balanced data  conditions", "labels": [], "entities": []}, {"text": " Table 6: Comparison of multilingual architectures", "labels": [], "entities": []}, {"text": " Table 8: Onehot (O) vs. phonetic (P h ) input", "labels": [], "entities": []}]}