{"title": [{"text": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7706345319747925}]}, {"text": "Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses.", "labels": [], "entities": []}, {"text": "The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016).", "labels": [], "entities": []}, {"text": "A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 \"discourse atoms\" that gives a succinct description of which other words co-occur with that word sense.", "labels": [], "entities": []}, {"text": "Discourse atoms can be of independent interest, and make the method potentially more useful.", "labels": [], "entities": []}, {"text": "Empirical tests are used to verify and support the theory.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are constructed using Firth's hypothesis that a word's sense is captured by the distribution of other words around it.", "labels": [], "entities": []}, {"text": "Classical vector space models (see the survey by) use simple linear algebra on the matrix of word-word co-occurrence counts, whereas recent neural network and energy-based models such as word2vec use an objective that involves a nonconvex (thus, also nonlinear) function of the word co-occurrences ().", "labels": [], "entities": []}, {"text": "This nonlinearity makes it hard to discern how these modern embeddings capture the different senses of a polysemous word.", "labels": [], "entities": []}, {"text": "The monolithic view of embeddings, with the internal information extracted only via inner product, is felt to fail in capturing word senses (.", "labels": [], "entities": []}, {"text": "Researchers have instead sought to capture polysemy using more complicated representations, e.g., by inducing separate embeddings for each sense).", "labels": [], "entities": []}, {"text": "These embeddingper-sense representations grow naturally out of classic Word Sense Induction or WSI techniques that perform clustering on neighboring words.", "labels": [], "entities": [{"text": "Word Sense Induction or WSI", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6103652238845825}]}, {"text": "The current paper goes beyond this monolithic view, by describing how multiple senses of a word actually reside in linear superposition within the standard word embeddings (e.g.,) and GloVe).", "labels": [], "entities": []}, {"text": "By this we mean the following: consider a polysemous word, say tie, which can refer to an article of clothing, or a drawn match, or a physical act.", "labels": [], "entities": []}, {"text": "Let's take the usual viewpoint that tie is a single token that represents monosemous words tie1, tie2, ....", "labels": [], "entities": []}, {"text": "The theory and experiments in this paper strongly suggest that word embeddings computed using modern techniques such as GloVe and word2vec satisfy: where coefficients \u03b1 i 's are nonnegative and v tie1 , v tie2 , etc., are the hypothetical embeddings of the different senses-those that would have been induced in the thought experiment where all occurrences of the different senses were hand-labeled in the corpus.", "labels": [], "entities": []}, {"text": "This Linearity Assertion, whereby linear structure appears out of a highly nonlinear embedding technique, is explained theoretically in Section 2, and then empirically tested in a couple of ways in Section 4.", "labels": [], "entities": [{"text": "Linearity Assertion", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.8068989515304565}]}, {"text": "Section 3 uses the linearity assertion to show how to do WSI via sparse coding, which can be seen as a linear algebraic analog of the classic clusteringbased approaches, albeit with overlapping clusters.", "labels": [], "entities": [{"text": "WSI", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9282137155532837}]}, {"text": "On standard testbeds it is competitive with earlier embedding-for-each-sense approaches (Section 6).", "labels": [], "entities": []}, {"text": "A novelty of our WSI method is that it automatically links different senses of different words via our atoms of discourse (Section 3).", "labels": [], "entities": []}, {"text": "This can be seen as an answer to the suggestion in to enhance one-embedding-persense methods so that they can automatically link together senses for different words, e.g., recognize that the \"article of clothing\" sense of tie is connected to shoe, jacket, etc.", "labels": [], "entities": []}, {"text": "This paper is inspired by the solution of word analogies via linear algebraic methods (, and use of sparse coding on word embeddings to get useful representations for many NLP tasks.", "labels": [], "entities": []}, {"text": "Our theory builds conceptually upon the random walk on discourses model of, although we make a small but important change to explain empirical findings regarding polysemy.", "labels": [], "entities": []}, {"text": "Our WSI procedure applies (with minor variation in performance) to canonical embeddings such as word2vec and GloVe as well as the older vector space methods such as PMI).", "labels": [], "entities": []}, {"text": "This is not surprising since these embeddings are known to be interrelated ().", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments use 300-dimensional embeddings created using the SN objective in ( and a Wikipedia corpus of 3 billion tokens (Wikimedia, 2012), and the sparse coding is solved by standard k-SVD algorithm (.", "labels": [], "entities": []}, {"text": "Experimentation showed that the best sparsity parameter k (i.e., the maximum number of allowed senses per word) is 5, and the number of atoms m is about 2000.", "labels": [], "entities": []}, {"text": "For the number of senses k, we tried plausible alternatives (based upon suggestions of many colleagues) that allow k to vary for different words, for example to let k be correlated with the word frequency.", "labels": [], "entities": []}, {"text": "But a fixed choice of k = 5 seems to produce just as good results.", "labels": [], "entities": []}, {"text": "To understand why, realize that this method retains no information about the corpus except for the low dimensional word embeddings.", "labels": [], "entities": []}, {"text": "Since the sparse coding tends to express a word using fairly different atoms, examining shows that \u2211 j \u03b1 2 w,j is bounded by approximately \u2225v w \u2225 2 2 . So if too many \u03b1 w,j 's are allowed to be nonzero, then some must necessarily have small coefficients, which makes the corresponding components indistinguishable from noise.", "labels": [], "entities": []}, {"text": "In other words, raising k often picks not only atoms corresponding to additional senses, but also many that don't.", "labels": [], "entities": []}, {"text": "The best number of atoms m was found to be around 2000.", "labels": [], "entities": []}, {"text": "This was estimated by re-running the sparse coding algorithm multiple times with different random initializations, whereupon substantial overlap was found between the two bases: a large fraction of vectors in one basis were found to have a very close vector in the other.", "labels": [], "entities": []}, {"text": "Thus combining the bases while merging duplicates yielded a basis of about the same size.", "labels": [], "entities": []}, {"text": "Around 100 atoms are used by a large number of words or have no close-by words.", "labels": [], "entities": []}, {"text": "They appear semantically meaningless and are excluded by checking for this condition.", "labels": [], "entities": []}, {"text": "The content of each atom can be discerned by looking at the nearby words in cosine similarity.", "labels": [], "entities": []}, {"text": "Some examples are shown in.", "labels": [], "entities": []}, {"text": "Each word is represented using at most five atoms, which usually capture distinct senses (with some noise/mistakes).", "labels": [], "entities": []}, {"text": "The senses recovered for tie and spring are shown in.", "labels": [], "entities": [{"text": "tie", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9790514707565308}]}, {"text": "Similar results can be obtained by using other word embeddings like word2vec and GloVe.", "labels": [], "entities": []}, {"text": "We also observe sparse coding procedures assign nonnegative values to most coefficients \u03b1 w,j 's even if they are left unrestricted.", "labels": [], "entities": []}, {"text": "Probably this is because the appearances of a word are best explained by what discourse is being used to generate it, rather than what discourses are not being used.: Five discourse atoms linked to the words tie and spring.", "labels": [], "entities": []}, {"text": "Each atom is represented by its nearest 6 words.", "labels": [], "entities": []}, {"text": "The algorithm often makes a mistake in the last atom (or two), as happened here.", "labels": [], "entities": []}, {"text": "Atoms of discourse maybe reminiscent of results from other automated methods for obtaining a thematic understanding of text, such as topic modeling, described in the survey by.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7512065470218658}]}, {"text": "This is not surprising since the model (2) used to compute the embeddings is related to a log-linear topic model by.", "labels": [], "entities": []}, {"text": "However, the discourses here are computed via sparse coding on word embeddings, which can be seen as a linear algebraic alternative, resulting in fairly fine-grained topics.", "labels": [], "entities": []}, {"text": "Atoms are also reminiscent of coherent \"word clusters\" detected in the past using Brown clustering, or even sparse coding (Murphy et al., 2012).", "labels": [], "entities": []}, {"text": "The novelty in this paper is a clear interpretation of the sparse coding results as atoms of discourse, as well as its use to capture different word senses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The average relative errors and cosine sim- ilarities between the vectors of pseudowords and  those predicted by Theorem 2. m pairs of words are  randomly selected and for each pair, all occurrences  of the two words in the corpus is replaced by a pseu- doword. Then train the vectors for the pseudowords  on the new corpus.", "labels": [], "entities": []}, {"text": " Table 5: Some discourse atoms and their nearest 9 words. By Equation (2), words most likely to appear in  a discourse are those nearest to it.", "labels": [], "entities": [{"text": "Equation", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9304251670837402}]}, {"text": " Table 7: Performance of different vectors in the WSI  task of SemEval 2010. The parameter k is the num- ber of clusters used in the methods. Rows are di- vided into two blocks, the first of which shows the  results of the competitors, and the second shows  those of our algorithm. Best results in each block  are in boldface.", "labels": [], "entities": [{"text": "WSI  task of SemEval 2010", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.629987233877182}]}, {"text": " Table 8: The results for different methods in the task  of word similarity in context. The best result is in  boldface. Our result is close to the best.", "labels": [], "entities": [{"text": "word similarity in context", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.8108391836285591}]}]}