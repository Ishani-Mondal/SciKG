{"title": [{"text": "Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement", "labels": [], "entities": [{"text": "Predicting Machine Translation", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8746235569318136}]}], "abstractContent": [{"text": "Poorly translated text is often disfluent and difficult to read.", "labels": [], "entities": []}, {"text": "In contrast, well-formed translations require less time to process.", "labels": [], "entities": []}, {"text": "In this paper, we model the differences in reading patterns of Machine Translation (MT) evalua-tors using novel features extracted from their gaze data, and we learn to predict the quality scores given by those evaluators.", "labels": [], "entities": [{"text": "Machine Translation (MT) evalua-tors", "start_pos": 63, "end_pos": 99, "type": "TASK", "confidence": 0.8316931128501892}]}, {"text": "We test our predictions in a pairwise ranking scenario, measuring Kendall's tau correlation with the judgments.", "labels": [], "entities": []}, {"text": "We show that our features provide information beyond fluency, and can be combined with BLEU for better predictions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9984577894210815}]}, {"text": "Furthermore , our results show that reading patterns can be used to build semi-automatic met-rics that anticipate the scores given by the evaluators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human evaluation has been the preferred method for tracking the progress of MT systems.", "labels": [], "entities": [{"text": "Human evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6655665338039398}, {"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.995510458946228}]}, {"text": "In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale.", "labels": [], "entities": []}, {"text": "However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments.", "labels": [], "entities": []}, {"text": "As a result, evaluations suffered from low inter-and intra-annotator agreements ().", "labels": [], "entities": []}, {"text": "This caused a shift towards a ranking-based approach.", "labels": [], "entities": []}, {"text": "Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment.", "labels": [], "entities": []}, {"text": "The eye-mind hypothesis states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).", "labels": [], "entities": []}, {"text": "Based on this assumption, it has been possible to study reading behavior and patterns.", "labels": [], "entities": []}, {"text": "The overall difficulty of a sentence and its syntactic complexity affects reading behavior (.", "labels": [], "entities": []}, {"text": "Ill-formed sentences take longer to process, and may cause the reader to jump back while reading.", "labels": [], "entities": []}, {"text": "Hence, by looking into how evaluators read the translations and their accompanying references, we can learn about: (i) the complexity of a reference sentence, and (ii) the quality of a translation sentence.", "labels": [], "entities": []}, {"text": "Using reading patterns from evaluators could be a useful tool for MT evaluation: (i) to shed light into the evaluation process: e.g. the general reading behavior that evaluators follow to complete their task; (ii) to understand which parts of a translation are more difficult for the annotator; and (iii) to develop semi-automatic evaluation systems that use reading patterns to predict translation quality.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9715181589126587}]}, {"text": "In this paper, we make a first step towards (iii): using reading patterns as a method for distinguishing between good and bad translations.", "labels": [], "entities": []}, {"text": "Our hypothesis is that bad translations are difficult to read, which maybe reflected by the reading patterns of the evaluators.", "labels": [], "entities": []}, {"text": "Motivated by the notion of reading difficulty, we extracted novel features from the evaluator's gaze data, and used them to model and predict the quality of translations as perceived by evaluators.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a subset of the Spanish-English portion of the WMT'12 Evaluation task.", "labels": [], "entities": [{"text": "WMT'12 Evaluation task", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.7316602865854899}]}, {"text": "We selected 60 medium-length sentences which have been evaluated previously by at least 2 different annotators.", "labels": [], "entities": []}, {"text": "For each sentence we selected the best and worst translations according to a human evaluation score based on the expected wins.", "labels": [], "entities": []}, {"text": "As a result, we had 60 references with two corresponding translations each, adding up to a total of 120 evaluation tasks.", "labels": [], "entities": []}, {"text": "Each evaluation task was performed by 6 different evaluators, resulting in 720 evaluations.", "labels": [], "entities": []}, {"text": "The annotators were presented with a translationreference pair at a time.", "labels": [], "entities": []}, {"text": "The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between.", "labels": [], "entities": []}, {"text": "This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence.", "labels": [], "entities": [{"text": "remembering the content of a first translation", "start_pos": 75, "end_pos": 121, "type": "TASK", "confidence": 0.7484910658427647}]}, {"text": "During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0-100 ().", "labels": [], "entities": []}, {"text": "The observed inter-annotator agreement (Cohen's kappa) among our annotators was 0.321.", "labels": [], "entities": [{"text": "inter-annotator agreement (Cohen's kappa)", "start_pos": 13, "end_pos": 54, "type": "METRIC", "confidence": 0.6166487506457737}]}, {"text": "This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT'12 for the Spanish-English.", "labels": [], "entities": [{"text": "inter-annotator agreement of 0.284", "start_pos": 41, "end_pos": 75, "type": "METRIC", "confidence": 0.8212767094373703}, {"text": "WMT'12", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.9484056234359741}]}, {"text": "For reading patterns we use the EyeTribe eye-tracker at For a rough comparison only.", "labels": [], "entities": [{"text": "EyeTribe", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8233306407928467}]}, {"text": "Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data.", "labels": [], "entities": []}, {"text": "Still, there is a fair agreement between the our evaluators and the expected wins from WMT'12 (avg.", "labels": [], "entities": [{"text": "WMT'12", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9568549990653992}]}, {"text": "pairwise kappa of 0.381) a sampling frequency of 30Hz.", "labels": [], "entities": []}, {"text": "Please refer to for our Eye-Tracking setup and to know about iAppraise, an evaluation environment that supports eye-tracking.", "labels": [], "entities": []}, {"text": "In our evaluation, we used eye-tracking features to predict the quality of a translation in a pairwise scenario in a protocol similar to the one from WMT'12.", "labels": [], "entities": [{"text": "WMT'12", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.9373617768287659}]}, {"text": "Otherwise, we considered it a disagreement.", "labels": [], "entities": []}, {"text": "Finally, we computed Kendall's tau correlation coefficient as follows: \u03c4 = agg\u2212dis agg+dis . We evaluated the performance using a 10-fold crossvalidation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8117252588272095}]}, {"text": "While the folds were selected randomly, we ensured that all translations corresponding to the same sentence were included in the same fold, to prevent any overlap between train and test.", "labels": [], "entities": []}, {"text": "So far, we've shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7938413619995117}]}, {"text": "One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically.", "labels": [], "entities": []}, {"text": "That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation.", "labels": [], "entities": []}, {"text": "Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (), which uses lexical information and is designed to measure not only fluency but also adequacy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9984404444694519}]}, {"text": "In, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model.", "labels": [], "entities": []}, {"text": "In (II) we re-introduce the results of lexicalized jumps feature.", "labels": [], "entities": []}, {"text": "In (III) we present results of BLEU and the combination of eye-tracking features with it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9980760812759399}]}, {"text": "Finally in (IV) we present the humanto-human agreement measured in average Kendall's tau and in max human-to-human Kendall's tau.", "labels": [], "entities": []}, {"text": "Combinations of translation jumps In section I we present several combinations of features.", "labels": [], "entities": []}, {"text": "All of them include the backward jumps feature.", "labels": [], "entities": []}, {"text": "This feature provides predictive power (\u03c4 = 0.22), which is orthogonal to other features.", "labels": [], "entities": []}, {"text": "This is inline with our initial hypothesis that fora bad translation, an evaluator needs to go back and forth several times to understand it.", "labels": [], "entities": []}, {"text": "Combining the backward jumps with the total number of jumps (CTJ 1 ) slightly increases the correlation to \u03c4 = 0.25.", "labels": [], "entities": [{"text": "correlation", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.9959238767623901}]}, {"text": "Adding the jump distance (CTJ 2 ) also increases its \u03c4 to 0.27.", "labels": [], "entities": [{"text": "jump distance", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9521365463733673}]}, {"text": "While this correlation is lower than BLEU (\u03c4 = 0.34), it does showcase the predictive power of the reading patterns.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9995802044868469}]}, {"text": "Combinations with BLEU When we combined BLEU with the translation jumps, we observed an increment in the \u03c4 to 0.37.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9987466335296631}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9968793392181396}]}, {"text": "Combining BLEU with the lexicalized jumps, yields the best combination (\u03c4 = 0.42).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986264705657959}]}, {"text": "Although moderate, these increments suggest that the reading patterns could be capturing additional phenomenon besides adequacy and fluency, such as structural complexity.", "labels": [], "entities": []}, {"text": "These phenomena remain to be explored in future work.", "labels": [], "entities": []}, {"text": "Human performance On average, evaluators agreements with each other are fair (\u03c4 = 0.33) and below the best combination (CB 3 ), while the maximum agreement of any two evaluators is relatively higher (\u03c4 = 0.53).", "labels": [], "entities": [{"text": "CB 3 )", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.854473332564036}]}, {"text": "This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human.", "labels": [], "entities": []}, {"text": "However, there is still room for improvement with respect to the mostagreeing pair of evaluators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Result of combining several jump and lex- icalized features with BLEU. The column Feature  Sets shows the name of the systems whose features  are combined for that particular run. We also in- cluded the average and maximum observed tau be- tween any two evaluators, as a reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.997529923915863}]}]}