{"title": [{"text": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information.", "labels": [], "entities": []}, {"text": "Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense fora given word, and a decoder which predicts context words based on the chosen sense.", "labels": [], "entities": []}, {"text": "The two components are estimated jointly.", "labels": [], "entities": []}, {"text": "We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available attest time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words.", "labels": [], "entities": []}, {"text": "They have been evaluated intrinsically () and have also been used in concrete NLP applications to deal with word sparsity and improve generalization).", "labels": [], "entities": []}, {"text": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors.", "labels": [], "entities": []}, {"text": "In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind.", "labels": [], "entities": []}, {"text": "The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation ().", "labels": [], "entities": []}, {"text": "Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings ().", "labels": [], "entities": []}, {"text": "Our paper falls in the latter, much less explored category.", "labels": [], "entities": []}, {"text": "We adhere to the view of multilingual learning as a means of language grounding.", "labels": [], "entities": []}, {"text": "Intuitively, polysemy in one language can beat least partially resolved by looking at the translation of the word and its context in another language.", "labels": [], "entities": []}, {"text": "Better sense assignment can then lead to better sense-specific word embeddings.", "labels": [], "entities": []}, {"text": "We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language.", "labels": [], "entities": []}, {"text": "This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays.", "labels": [], "entities": []}, {"text": "Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the predicted senses.", "labels": [], "entities": []}, {"text": "We strive to remain flexible as to the form of parallel data used in training and support both the use of word-and sentence-level alignments.", "labels": [], "entities": [{"text": "word-and sentence-level alignments", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.5675494074821472}]}, {"text": "Our findings are: \u2022 The second-language signal effectively improves the quality of multi-sense embeddings as seen on a variety of intrinsic tasks for English, with the results superior to that of the baseline Skip-Gram model, even though the crosslingual information is not available attest time.", "labels": [], "entities": []}, {"text": "\u2022 This finding is robust across several settings, such as varying dimensionality, vocabulary size and amount of data.", "labels": [], "entities": []}, {"text": "\u2022 In the extrinsic POS-tagging task, the secondlanguage signal also offers improvements over monolingually-trained multi-sense embeddings, however, the standard Skip-Gram embeddings turnout to be the most robust in this task.", "labels": [], "entities": []}, {"text": "We make the implementation of all the models as well as the evaluation scripts available at http:// github.com/rug-compling/bimu.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the quality of our word representations on a number of tasks, both intrinsic and extrinsic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results, per-row best in bold. SG and MU are trained", "labels": [], "entities": [{"text": "MU", "start_pos": 48, "end_pos": 50, "type": "DATASET", "confidence": 0.5830285549163818}]}, {"text": " Table 4: Comparison of SCWS correlation scores of BIMU", "labels": [], "entities": [{"text": "SCWS correlation scores", "start_pos": 24, "end_pos": 47, "type": "METRIC", "confidence": 0.7216732899347941}, {"text": "BIMU", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7293351292610168}]}]}