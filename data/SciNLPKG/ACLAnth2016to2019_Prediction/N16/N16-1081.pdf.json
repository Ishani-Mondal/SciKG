{"title": [{"text": "Syntactic Parsing of Web Queries with Question Intent", "labels": [], "entities": [{"text": "Syntactic Parsing of Web Queries", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8730801224708558}]}], "abstractContent": [{"text": "Accurate automatic processing of Web queries is important for high-quality information retrieval from the Web.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7563276886940002}]}, {"text": "While the syntactic structure of a large portion of these queries is trivial, the structure of queries with question intent is much richer.", "labels": [], "entities": []}, {"text": "In this paper we therefore address the task of statistical syntactic parsing of such queries.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.6467366814613342}]}, {"text": "We first show that the standard dependency grammar does not account for the full range of syntactic structures manifested by queries with question intent.", "labels": [], "entities": []}, {"text": "To alleviate this issue we extend the dependency grammar to account for segments-independent syntactic units within a potentially larger syntactic structure.", "labels": [], "entities": []}, {"text": "We then propose two distant supervision approaches for the task.", "labels": [], "entities": []}, {"text": "Both algorithms do not require manually parsed queries for training.", "labels": [], "entities": []}, {"text": "Instead, they are trained on millions of (query, page title) pairs from the Community Question Answering (CQA) domain, where the CQA page was clicked by the user who initiated the query in a search engine.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.7699397703011831}]}, {"text": "Experiments on anew treebank 1 consisting of 5,000 Web queries from the CQA domain, manually parsed using the proposed grammar, show that our algorithms outperform alternative approaches trained on various sources: tens of thousands of manually parsed OntoNotes sentences, millions of unlabeled CQA queries and thousands of manually segmented CQA queries.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the World Wide Web grows in volume, it encompasses ever-increasing amounts of text.", "labels": [], "entities": []}, {"text": "A major gateway to this invaluable resource is Web queries which users compose to guide a search engine in retrieving the information they desire to inspect.", "labels": [], "entities": []}, {"text": "Automatic processing of Web queries is therefore of utmost importance.", "labels": [], "entities": [{"text": "Automatic processing of Web queries", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5929932653903961}]}, {"text": "Previous research suggested that many Web queries are trivial in structure, usually embodying entity lookup, e.g. \"frozen\" or \"condos in NY\".", "labels": [], "entities": []}, {"text": "However, with the increasing popularity of Community Question Answering (CQA) sites, such as Yahoo Answers, StackOverflow and social QA forums, more Web queries encompass information needs related to questions that these sites can answer.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.8004924356937408}]}, {"text": "We found that this subcategory of queries, which we call CQA queries (following), exhibits a wide range of structures.", "labels": [], "entities": []}, {"text": "This suggests that the processing of such queries, which constitute \u223c10% of all queries issued to search engines (, may benefit from syntactic analysis ().", "labels": [], "entities": []}, {"text": "Recent progress in statistical parsing ( has resulted in models that are both fast, parsing several hundred sentences per second, and accurate.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.6973943412303925}]}, {"text": "These parsers, however, still suffer from the problem of domain adaptation, excelling mostly when their training and test domains are similar.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7434053719043732}]}, {"text": "This problem is of particular importance in the heterogeneous Web  and is expected to worsen when addressing queries, due to their non-standard grammatical conventions.", "labels": [], "entities": []}, {"text": "Some recent work addresses the syntactic analysis of User Generated Content (UGC)).", "labels": [], "entities": [{"text": "syntactic analysis of User Generated Content (UGC))", "start_pos": 31, "end_pos": 82, "type": "TASK", "confidence": 0.818056563536326}]}, {"text": "Yet, these efforts generally focus on UGC aspects related to grammatical mistakes made by users ( and to the unique writing conventions of specific Web platforms, such as Twitter (.", "labels": [], "entities": []}, {"text": "Our analysis of thousands of CQA queries, however, reveals that regardless of such issues, CQA queries are generated by a well-defined grammar that sometimes deviates from the one used to generate the standard written language of edited resources such as newspapers.", "labels": [], "entities": []}, {"text": "Consequently, this work has two main contributions.", "labels": [], "entities": []}, {"text": "First, we extend the standard dependency grammar to de- scribe the syntax of queries with question intent.", "labels": [], "entities": []}, {"text": "The extended grammar is driven by the concept of a syntactic segment: an independent syntactic unit within a potentially larger syntactic structure.", "labels": [], "entities": []}, {"text": "A query may include several segments, potentially related to each other semantically but lacking an explicit syntactic connection.", "labels": [], "entities": []}, {"text": "Hence, query analysis consists of the query's segments and their internal dependency structure, and maybe complemented by the inter-segment semantic relationships.", "labels": [], "entities": [{"text": "query analysis", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.8507728576660156}]}, {"text": "Therefore, we constructed anew query treebank consisting of 5,000 CQA queries, manually annotated according to our extended grammar.", "labels": [], "entities": []}, {"text": "A comparison of direct application of an off-the-shelf parser) trained on edited text (OntoNotes 5 () to a raw query with the application of the same parser to the gold-standard segments of that query is given in.", "labels": [], "entities": []}, {"text": "Second, we develop two CQA query parsing algorithms that can adapt any given off-the-shelf dependency parser trained on standard edited text to produce syntactic structures that conform to the extended grammar.", "labels": [], "entities": [{"text": "CQA query parsing", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6592039962609609}]}, {"text": "Both our algorithms employ distant supervision in the form of a training set consisting of millions of (query, title) pairs.", "labels": [], "entities": []}, {"text": "The title is the title of the Yahoo Answers question page that was clicked by the user who initiated the query.", "labels": [], "entities": [{"text": "Yahoo Answers question page", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.8091267645359039}]}, {"text": "The alignment between the query and the title provides a valuable training signal for query segmentation and parsing, since the title is usually a grammatical question.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7693402171134949}]}, {"text": "Both algorithms employ an off-the-shelf parser, but differ on whether segmentation and parsing are performed in a pipeline or jointly.", "labels": [], "entities": []}, {"text": "We compared our algorithms to several alternatives: (a) Direct application of an off-the-shelf parser to queries; (b) A supervised variant of our pipeline algorithm where thousands of manually segmented queries replace the distant supervision source; and (c) A pipeline algorithm similar to ours where segmentation is based on the predictions of a query language model.", "labels": [], "entities": []}, {"text": "We report results on two query treebank tasks: (a) Dependency parsing, reporting Unlabeled Attachment Score (UAS); and (b) Query segmentation, which reflects the core aspect of the extended grammar compared to the standard one.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.727949470281601}, {"text": "reporting Unlabeled Attachment Score (UAS)", "start_pos": 71, "end_pos": 113, "type": "METRIC", "confidence": 0.8540475368499756}, {"text": "Query segmentation", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8125664591789246}]}, {"text": "In experiments on our new treebank, our joint model outperformed the alternatives on UAS for the full test set and for the subset of single-segment queries.", "labels": [], "entities": [{"text": "UAS", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.8052312135696411}]}, {"text": "Our pipeline model excelled both on UAS and on segmentation F1 for two large subsets that are automatically identifiable attest time: (a) Queries that consist mostly of content words (42.4% of the test set); and (b) Queries for which the confidence score of the off-the-shelf parser is at most 0.8 (30% of the test set).", "labels": [], "entities": [{"text": "segmentation F1", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7344949841499329}]}, {"text": "It also beat all other models on the subset of multi-segment queries.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider two evaluation tasks: (a) Dependency parsing, reporting Unlabeled Attachment Score (UAS); and (b) Query Segmentation, reporting the F1 score, where each segment is represented by its boundaries: in order for an observed segment to be considered correct, both of its ends must match those of a gold segment.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.793435662984848}, {"text": "reporting Unlabeled Attachment Score (UAS)", "start_pos": 58, "end_pos": 100, "type": "METRIC", "confidence": 0.7863638315882001}, {"text": "Query Segmentation", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8059519231319427}, {"text": "F1 score", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9871096014976501}]}, {"text": "All tested algorithms, except for our joint model, directly segment and parse queries and hence require these queries to be POS-tagged.", "labels": [], "entities": []}, {"text": "Thus, we POS-tagged the testset queries with the OpenNLP 10 POS tagger which was adapted to queries using the self-training algorithm of.", "labels": [], "entities": [{"text": "OpenNLP 10 POS tagger", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8935127854347229}]}, {"text": "Our self-training set consisted of 14M (query,title) pairs from the Yahoo Answers log.", "labels": [], "entities": [{"text": "Yahoo Answers log", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.8910169005393982}]}, {"text": "This tagger reached 88.2% accuracy on our query treebank, compared to 81.3% of the off-the-shelf tagger . Analyzing our development set we noticed that a very strong indicator that a query is a grammatical, and thus consists of a single segment, is when it starts with a WHword or an auxiliary verb.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993355870246887}]}, {"text": "Hence, in all pipeline models, except Gold, we do not segment such queries but rather directly apply the Clear parser to them.", "labels": [], "entities": []}, {"text": "In postmortem analysis of the test set we found that this indicator was correct for 93.2% of the 1600 detected queries with 40% recall with respect to the single-segment queries subset.", "labels": [], "entities": [{"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9989019632339478}]}, {"text": "present the results of our experiments.", "labels": [], "entities": []}, {"text": "The All Queries column in reports the performance of the tested models on the full test set.", "labels": [], "entities": []}, {"text": "Overall, methods based on distant and manual supervision are superior to the baseline methods in both measures.", "labels": [], "entities": []}, {"text": "Interestingly, our Q2QProj model performs best in terms of UAS (77.1%) although its segmentation F1 is mediocre (63.5).", "labels": [], "entities": [{"text": "UAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9986023306846619}]}, {"text": "In terms of UAS, the distant-supervised and supervised models approach the performance of the upper bound gold standard segmentation.", "labels": [], "entities": [{"text": "UAS", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.5349611639976501}]}, {"text": "For example, Q2QProj is outperformed by Gold by only 3.6%.", "labels": [], "entities": [{"text": "Gold", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.7314823269844055}]}, {"text": "The Clear parser baseline, which is not trained to identify multiple segments, lags 5.7-5.8 F1 points behind the models that employ CRF segmentation.", "labels": [], "entities": [{"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9980771541595459}]}, {"text": "This is translated to a difference in UAS of up to 1.7%.", "labels": [], "entities": [{"text": "difference", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9642819762229919}, {"text": "UAS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9616197347640991}]}, {"text": "The Lm baseline, on the other hand, scores substantially lower than the other models in both measures.", "labels": [], "entities": [{"text": "Lm baseline", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9362196028232574}]}, {"text": "This maybe an indication of the syntactic, rather than lexical, nature of our task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Syntactic properties of four types of Web domains. The four leftmost properties are proxies of syntactic complexity. The fifth property  (parser confidence) is a proxy of parsing difficulty. The \"root \u2192 N N  *  edges\" and \"root \u2192 V B *  edges\" columns present the fraction of edges  from the root of the parse tree that go to words POS-tagged as nouns and verbs respectively.", "labels": [], "entities": []}, {"text": " Table 2: UAS of out-of-the-box parsers trained on OntoNotes 5. RBG is reported on its best-performing setting, basic.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9077240824699402}, {"text": "OntoNotes 5", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9318014979362488}, {"text": "RBG", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.4142550826072693}]}, {"text": " Table 5: Performance of the various models on various automatically-extractable subsets of the data. All Queries is our test set, the others are its  subsets with: \u2264 1 ncw -at most one non-content word according to POS tags; LowConf -a Clear parser confidence score of at most 0.8. 'F1'", "labels": [], "entities": [{"text": "Clear parser confidence score", "start_pos": 237, "end_pos": 266, "type": "METRIC", "confidence": 0.7319527715444565}, {"text": "F1", "start_pos": 284, "end_pos": 286, "type": "METRIC", "confidence": 0.9945500493049622}]}, {"text": " Table 6: Performance on queries which have a single segment or mul-", "labels": [], "entities": []}]}