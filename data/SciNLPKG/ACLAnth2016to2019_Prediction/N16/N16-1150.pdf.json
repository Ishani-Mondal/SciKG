{"title": [{"text": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks", "labels": [], "entities": [{"text": "Capturing Semantic Similarity", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8647526303927103}, {"text": "Entity Linking", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.752828061580658}]}], "abstractContent": [{"text": "A key challenge in entity linking is making effective use of contextual information to dis-ambiguate mentions that might refer to different entities in different contexts.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7613261640071869}]}, {"text": "We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity.", "labels": [], "entities": []}, {"text": "These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich pa-rameterization gives them the capacity to learn which n-grams characterize different topics.", "labels": [], "entities": []}, {"text": "We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, out-performing the prior systems of Durrett and Klein (2014) and Nguyen et al.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the major challenges of entity linking is resolving contextually polysemous mentions.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7062833905220032}]}, {"text": "For example, Germany may refer to a nation, to that nation's government, or even to a soccer team.", "labels": [], "entities": []}, {"text": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved.", "labels": [], "entities": []}, {"text": "But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context.", "labels": [], "entities": []}, {"text": "In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods.", "labels": [], "entities": []}, {"text": "In this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks (CNNs).", "labels": [], "entities": []}, {"text": "CNNs have been shown to be effective for sentence classification tasks and for capturing similarity in models for entity linking) and other related tasks (), so we expect them to be effective at isolating the relevant topic semantics for entity linking.", "labels": [], "entities": [{"text": "sentence classification tasks", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.8263857960700989}, {"text": "entity linking", "start_pos": 238, "end_pos": 252, "type": "TASK", "confidence": 0.720832884311676}]}, {"text": "We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context.", "labels": [], "entities": []}, {"text": "Finally, we show how to integrate these networks with a preexisting entity linking system.", "labels": [], "entities": []}, {"text": "Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on 4 different entity linking datasets.", "labels": [], "entities": []}, {"text": "\u2022 ACE: This corpus was used in and.", "labels": [], "entities": [{"text": "ACE", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.8262187242507935}]}, {"text": "\u2022 CoNLL-YAGO (Hoffart et al., 2011): This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities.", "labels": [], "entities": [{"text": "CoNLL-YAGO", "start_pos": 2, "end_pos": 12, "type": "DATASET", "confidence": 0.9088516235351562}, {"text": "CoNLL 2003 dataset", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9262161453564962}]}, {"text": "\u2022 WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia.", "labels": [], "entities": [{"text": "WP", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.723850667476654}]}, {"text": "\u2022: Performance of the system in this work (Full) compared to two baselines from prior work and two ablations.", "labels": [], "entities": []}, {"text": "Our results outperform those of and.", "labels": [], "entities": []}, {"text": "In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit.", "labels": [], "entities": []}, {"text": "We use standard train-test splits for all datasets except for WP, where no standard split is available.", "labels": [], "entities": []}, {"text": "In this case, we randomly sample a test set.", "labels": [], "entities": []}, {"text": "For all experiments, we use word vectors computed by running word2vec) on all Wikipedia, as described in Section 3.2.", "labels": [], "entities": []}, {"text": "shows results for two baselines and three variants of our system.", "labels": [], "entities": []}, {"text": "Our main contribution is the combination of indicator features and CNN features (Full).", "labels": [], "entities": []}, {"text": "We see that this system outperforms the results of and the AIDA-LIGHT system of.", "labels": [], "entities": [{"text": "AIDA-LIGHT", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.6619053483009338}]}, {"text": "We can also compare to two ablations: using just the sparse features (a system which is a direct extension of) or using just the CNNderived features.", "labels": [], "entities": []}, {"text": "Our CNN features generally outperform the sparse features and improve even further when stacked with them.", "labels": [], "entities": []}, {"text": "This reflects that they capture orthogonal sources of information: for example, the sparse features can capture how frequently the target document was linked to, whereas the CNNs can capture document context in a more nuanced way.", "labels": [], "entities": []}, {"text": "These CNN features also clearly supersede the sparse features based on tf-idf (taken from (Ratinov et al., 2011)), showing that indeed that CNNs are better at learning semantic topic similarity than heuristics like tf-idf.", "labels": [], "entities": []}, {"text": "In the sparse feature system, the highest weighted  Comparison of using only topic information derived from the document and target article, only information derived from the mention itself and the target entity title, and the full set of information (six features, as shown in).", "labels": [], "entities": []}, {"text": "Neither the finest nor coarsest convolutional context can give the performance of the complete set.", "labels": [], "entities": []}, {"text": "Numbers are reported on a development set.", "labels": [], "entities": []}, {"text": "features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q.", "labels": [], "entities": []}, {"text": "This suggests that the system of Durrett and has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct.", "labels": [], "entities": [{"text": "Durrett", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.544093668460846}]}, {"text": "By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the system in this work (Full) com- pared to two baselines from prior work and two ablations.", "labels": [], "entities": []}, {"text": " Table 4: Results of the full model (sparse and convolutional  features) comparing word vectors derived from Google News  vs. Wikipedia on development sets for each corpus.", "labels": [], "entities": []}]}