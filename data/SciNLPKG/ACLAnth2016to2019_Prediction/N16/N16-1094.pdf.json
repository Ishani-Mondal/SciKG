{"title": [{"text": "Opinion Holder and Target Extraction on Opinion Compounds -A Linguistic Approach", "labels": [], "entities": [{"text": "Opinion Holder", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7260343134403229}]}], "abstractContent": [{"text": "We present an approach to the new task of opinion holder and target extraction on opinion compounds.", "labels": [], "entities": [{"text": "opinion holder", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.714925155043602}, {"text": "target extraction", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8039436638355255}]}, {"text": "Opinion compounds (e.g. user rating or victim support) are noun compounds whose head is an opinion noun.", "labels": [], "entities": []}, {"text": "We do not only examine features known to be effective for noun compound analysis, such as paraphrases and semantic classes of heads and modifiers, but also propose novel features tailored to this new task.", "labels": [], "entities": [{"text": "noun compound analysis", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8368140459060669}]}, {"text": "Among them, we examine paraphrases that jointly consider holders and targets, a verb detour in which noun heads are replaced by related verbs, a global head constraint allowing inferencing between different compounds, and the categorization of the sentiment view that the head conveys.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the key subtasks in sentiment analysis is opinion role extraction.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9742569029331207}, {"text": "opinion role extraction", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7347046136856079}]}, {"text": "It can be divided into the extraction of opinion holders (OH), i.e. entities expressing an opinion, and the extraction of opinion targets (OT), i.e. entities or propositions at which sentiment is directed.", "labels": [], "entities": [{"text": "extraction of opinion holders (OH)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6643779277801514}]}, {"text": "This task is vital for various applications involving sentiment analysis, e.g. opinion summarization or opinion question answering.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9633961319923401}, {"text": "opinion summarization", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8199017643928528}, {"text": "opinion question answering", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6965257525444031}]}, {"text": "Opinion role extraction is commonly regarded as a task in lexical semantics.", "labels": [], "entities": [{"text": "Opinion role extraction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8231629729270935}]}, {"text": "An opinion is evoked by some opinion word, e.g. criticized in (1), skeptical in (2) or intentions in (3), and its opinion roles are usually realized as syntactic dependents.", "labels": [], "entities": []}, {"text": "Opinion words come in many shapes, the most frequent types being opinion verbs (1), opinion adjectives (2) and opinion nouns.", "labels": [], "entities": []}, {"text": "These types of opinion words have extensively been studied in various sentimentrelated corpora, such as MPQA ( ).", "labels": [], "entities": [{"text": "MPQA", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.911159873008728}]}, {"text": "In this work, we examine opinion roles that are realized in opinion compounds.", "labels": [], "entities": []}, {"text": "We define an opinion compound as a noun compound, i.e. a sequence of two nouns, where the second noun, i.e. the head, is an opinion expression.", "labels": [], "entities": []}, {"text": "The first noun, i.e. the modifier, can represent an opinion holder (4)-(5), an opinion target (6)-(7) or neither (8)-(9).", "labels": [], "entities": []}, {"text": "Our aim is to automatically classify the modifier into these categories.", "labels": [], "entities": []}, {"text": "This task is challenging as, unlike with opinion roles expressed in the syntax (1)-(3), the immediate context of compounds does not contain explicit cues as to the relation between head and modifier.", "labels": [], "entities": []}, {"text": "Moreover, due to the high productivity of compounding, this task cannot be solved by compiling a (finite) compound lexicon that encodes for each compound the category of its modifier.", "labels": [], "entities": [{"text": "compounding", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9699522852897644}]}, {"text": "Notice that we focus exclusively on opinion role extraction.", "labels": [], "entities": [{"text": "opinion role extraction", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.77196071545283}]}, {"text": "We do not try to detect the polarity associated with the compound.", "labels": [], "entities": []}, {"text": "Neither do we consider implicature-related information about effects), but only inherent sentiment.", "labels": [], "entities": []}, {"text": "We study opinion role extraction on opinion compounds in German.", "labels": [], "entities": [{"text": "opinion role extraction", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.6612062454223633}]}, {"text": "German is known for its frequent compounds user rating; victim support; spring upswing immediate constituents user; victim; spring rating; support; upswing grammatical function modifier head: Internal structure of opinion compounds.", "labels": [], "entities": []}, {"text": "In the STEPS-corpus, the benchmark dataset for German opinion role extraction (), almost every other sentence contains an opinion compound.", "labels": [], "entities": [{"text": "STEPS-corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8117379546165466}, {"text": "German opinion role extraction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6523044258356094}]}, {"text": "Compounds can also be commonly found in other key languages, such as English.", "labels": [], "entities": []}, {"text": "Since the methods we apply to this task and the issues that they address are not language specific, our approach can be replicated on other languages.", "labels": [], "entities": []}, {"text": "Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds.", "labels": [], "entities": [{"text": "noun compound analysis", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.791906495889028}]}, {"text": "We also created anew gold standard for this task (see also \u00a73).", "labels": [], "entities": []}, {"text": "The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds.", "labels": [], "entities": [{"text": "STEPS-corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7273582816123962}]}, {"text": "We considered this amount insufficient for producing a gold standard.", "labels": [], "entities": []}, {"text": "Also, none of the existing datasets on noun compounds) contain any information regarding opinion roles.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset II 91.36: Role-purity of compounds with the same head.", "labels": [], "entities": [{"text": "Dataset II 91.36", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7798126339912415}]}, {"text": "MLNs area set of pairs (F i , w i ) where F i is a first-order logic formula and w i an associated realvalued weight.", "labels": [], "entities": []}, {"text": "They build a template for constructing a Markov network given a set of constants C.", "labels": [], "entities": []}, {"text": "The probability distribution that is estimated is a loglinear model where n i (x) is the number of groundings in F i in x and Z is some normalization constant.", "labels": [], "entities": []}, {"text": "We consider one binary MLN classifier for each of our three tasks ( \u00a74).", "labels": [], "entities": []}, {"text": "Most of our features are frequently occurring features (e.g. paraphrases ( \u00a75.1), subjectivity feature ( \u00a75.6), sentiment views ( \u00a75.7)).", "labels": [], "entities": []}, {"text": "Supervised classifiers only require few training data in order to assign appropriate weights to such features.", "labels": [], "entities": []}, {"text": "Therefore, we sample 20% of the instances for each task of the respective dataset as training data.", "labels": [], "entities": []}, {"text": "We test on the remaining 80% of the dataset.", "labels": [], "entities": []}, {"text": "This procedure is repeated 5 times.", "labels": [], "entities": []}, {"text": "The 5 training samples within each task are disjoint.", "labels": [], "entities": []}, {"text": "We report macroaverage F-score averaged over the 5 test samples.", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9055969715118408}]}, {"text": "We will first evaluate global features and then proceed to the local features.", "labels": [], "entities": []}, {"text": "A division of our feature set into these groups was presented in. compares the features that can be applied on all three tasks.", "labels": [], "entities": []}, {"text": "On average, PARA ( \u00a75.1- \u00a75.3) is slightly better than SEM ( \u00a75.4).", "labels": [], "entities": [{"text": "PARA", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9510180950164795}]}, {"text": "Since their combination always results in a significant improvement, we conclude that these features contain complementary information.", "labels": [], "entities": []}, {"text": "In the majority of cases, HEAD ( \u00a75.5) also yields significant improvement.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.5545056462287903}]}, {"text": "compares the different subtypes of paraphrases ( \u00a75.1- \u00a75.3).", "labels": [], "entities": []}, {"text": "For all tasks, notable improvements are obtained by adding the other types of paraphrases to the plain paraphrases.", "labels": [], "entities": []}, {"text": "While the joint paraphrases improve the plain paraphrases on all tasks, for the verb detour, improvements can be observed only for the extraction of holders and targets.", "labels": [], "entities": []}, {"text": "However, this improvement is significantly better than that of the joint paraphrases.", "labels": [], "entities": []}, {"text": "In summary, in order to obtain best possible results on all three types of classifications, we need all types of paraphrases.", "labels": [], "entities": []}, {"text": "examines the impact of the subjectivity feature ( \u00a75.6).", "labels": [], "entities": []}, {"text": "We closely compare this feature with the head constraint since we found both features only working in combination with other features.", "labels": [], "entities": []}, {"text": "In terms of statistical significance, the head constraint is more effective than the subjectivity feature.", "labels": [], "entities": []}, {"text": "shows that this feature has a notable impact on both PARA plain (i.e. the simplest feature set) and SEM+PARA+HEAD (i.e. the most complex feature set).", "labels": [], "entities": [{"text": "PARA", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9212540984153748}, {"text": "PARA+HEAD", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.676408052444458}]}, {"text": "This underlines that sentiment views are an important aspect for opinion role extraction.", "labels": [], "entities": [{"text": "opinion role extraction", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7765381137530009}]}, {"text": "Note that unlike we manually removed incorrect seeds from the set of automatically generated seeds (this affects less than 9% of the seeds).", "labels": [], "entities": []}, {"text": "all words in the sentences (bag of words) brown clusters of all words in the sentences (bag of clusters) part-of-speech sequences between head and modifier mentions part-of-speech tags before/after modifier mentions part-of-speech tags before/after head mentions dependency paths between head and modifier mentions proportion of opinion words in the sentences each training/test instance represents the set of all sentences in which head and modifier of a specific compound co-occur: Features for distant supervision (baseline) classifier.", "labels": [], "entities": []}, {"text": "compares the best result from our previous experiments against 3 baselines.", "labels": [], "entities": []}, {"text": "The first is a majority classifier predicting the majority class.", "labels": [], "entities": []}, {"text": "The second baseline is a classifier inspired by distant supervision.", "labels": [], "entities": []}, {"text": "As in our paraphrase features, this classifier considers the context in which modifier and head of a compound occur as separate constituents.", "labels": [], "entities": []}, {"text": "The difference is, however, that we consider every such co-occurrence (within the same sentence) as a context that conveys the same relation as the one that is (implicitly) conveyed by the compound.", "labels": [], "entities": []}, {"text": "Even though such an assumption is naive, it has been shown to produce quite reasonable performance in relation extraction (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.9133045673370361}]}, {"text": "The advantage of such an approach is that a generic relation extraction/opinion role extraction classifier can be trained on the resulting data.", "labels": [], "entities": [{"text": "relation extraction/opinion role extraction classifier", "start_pos": 52, "end_pos": 106, "type": "TASK", "confidence": 0.6963086468832833}]}, {"text": "Unlike our proposed method, it does not require features tailored to the specific task (e.g. manually written paraphrases).", "labels": [], "entities": []}, {"text": "Since the result-  ing feature set (see also).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The two different datasets.", "labels": [], "entities": []}, {"text": " Table 8: F-scores of features applicable to all tasks.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980641007423401}]}, {"text": " Table 9: F-scores of paraphrase features.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9933834671974182}]}, {"text": " Table 10: Comparison of SUBJ and HEAD evaluated on task", "labels": [], "entities": [{"text": "HEAD", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9475146532058716}]}, {"text": " Table 11: F-scores of sentiment view features.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9830219745635986}]}, {"text": " Table 13: Comparison of our approach against baselines; eval-", "labels": [], "entities": []}]}