{"title": [{"text": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents", "labels": [], "entities": [{"text": "Modeling Sentences and Documents", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.7331202030181885}]}], "abstractContent": [{"text": "The goal of sentence and document model-ing is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a general-purpose classification system for both sentences and documents.", "labels": [], "entities": []}, {"text": "DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators.", "labels": [], "entities": []}, {"text": "Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling , and thus is not restricted to sentence-level tasks.", "labels": [], "entities": [{"text": "phrase labeling", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7200512588024139}]}, {"text": "Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document.", "labels": [], "entities": []}, {"text": "Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification , and subjectivity classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9698661863803864}, {"text": "question type classification", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.7971690694491068}, {"text": "subjectivity classification", "start_pos": 174, "end_pos": 201, "type": "TASK", "confidence": 0.7133338749408722}]}], "introductionContent": [{"text": "Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6884305477142334}]}, {"text": "The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents).", "labels": [], "entities": []}, {"text": "As a traditional method, the bag-of-words model treats sentences and documents as unordered collections of words.", "labels": [], "entities": []}, {"text": "In this way, however, the bag-of-words model fails to encode word orders and syntactic structures.", "labels": [], "entities": []}, {"text": "By contrast, order-sensitive models based on neural networks are becoming increasingly popular thanks to their ability to capture word order information.", "labels": [], "entities": []}, {"text": "Many prevalent order-sensitive neural models can be categorized into two classes: Recursive models and Convolutional Neural Networks (CNN) models.", "labels": [], "entities": []}, {"text": "Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures.", "labels": [], "entities": []}, {"text": "For example,) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree.", "labels": [], "entities": []}, {"text": "In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached.", "labels": [], "entities": []}, {"text": "CNN based models, as the second category, utilize convolutional filters to extract local features () over embedding matrices consisting of pretrained word vectors.", "labels": [], "entities": []}, {"text": "Therefore, the model actually splits the sentence locally into n-grams by sliding windows.", "labels": [], "entities": []}, {"text": "However, despite their ability to account for word orders, order-sensitive models based on neural networks still suffer from several disadvantages.", "labels": [], "entities": []}, {"text": "First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains.", "labels": [], "entities": []}, {"text": "Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem), recursive models require heavy label-ing on phrases to add supervisions on internal nodes.", "labels": [], "entities": []}, {"text": "Furthermore, parsing is restricted to sentences and it is unclear how to model paragraphs and documents using recursive neural networks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9643329381942749}]}, {"text": "In CNN models, convolutional operators process word vectors sequentially using small windows.", "labels": [], "entities": []}, {"text": "Thus sentences are essentially treated as a bag of n-grams, and the long dependency information spanning sliding windows is lost.", "labels": [], "entities": []}, {"text": "These observations motivate us to construct a textual modeling architecture that captures long-term dependencies without relying on parsing for both sentence and document inputs.", "labels": [], "entities": []}, {"text": "Specifically, we propose Dependency Sensitive Convolutional Neural Networks (DSCNN), an end-to-end classification system that hierarchically builds textual representations with only root-level labels.", "labels": [], "entities": []}, {"text": "DSCNN consists of a convolutional layer built on top of Long Short-Term Memory (LSTM) networks.", "labels": [], "entities": [{"text": "DSCNN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8413757681846619}]}, {"text": "DSCNN takes slightly different forms depending on its input.", "labels": [], "entities": [{"text": "DSCNN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9223477244377136}]}, {"text": "For a single sentence, the LSTM network processes the sequence of word embeddings to capture long-distance dependencies within the sentence.", "labels": [], "entities": []}, {"text": "The hidden states of the LSTM are extracted to form the low-level representation, and a convolutional layer with variable-size filters and max-pooling operators follows to extract task-specific features for classification purposes.", "labels": [], "entities": []}, {"text": "As for document modeling), DSCNN first applies independent LSTM networks to each subsentence.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.6989371031522751}]}, {"text": "Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences.", "labels": [], "entities": []}, {"text": "We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.9652910232543945}, {"text": "question type classification", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.7465485135714213}, {"text": "subjectivity classification", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.7226355969905853}]}, {"text": "Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art.", "labels": [], "entities": []}, {"text": "In particular, our method achieves highest accuracies on MR sentiment analysis), TREC question classification (), and subjectivity classification task SUBJ () compared with several competitive baselines.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9605562686920166}, {"text": "MR sentiment analysis", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7653239170710245}, {"text": "TREC question classification", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.6920253237088522}, {"text": "subjectivity classification task SUBJ", "start_pos": 118, "end_pos": 155, "type": "TASK", "confidence": 0.5881722420454025}]}, {"text": "The remaining part of this paper is the following.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the background including LSTM networks and convolution operators.", "labels": [], "entities": []}, {"text": "We then describe our architectures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7941860854625702}, {"text": "document modeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7035659104585648}]}], "datasetContent": [{"text": "Movie Review Data (MR) proposed by) is a dataset for sentiment analysis of movie reviews.", "labels": [], "entities": [{"text": "Movie Review Data (MR)", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8839837710062662}, {"text": "sentiment analysis of movie reviews", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.9258356094360352}]}, {"text": "The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence.", "labels": [], "entities": []}, {"text": "We follow the practice of using 10-fold cross validation to report results.", "labels": [], "entities": []}, {"text": "The sentences are labeled in a fine-grained way (SST-5): {very negative, negative, neutral, positive, very positive}.", "labels": [], "entities": []}, {"text": "The dataset has been split into 8,544 training, 1,101 validation, and 2,210 testing sentences.", "labels": [], "entities": []}, {"text": "Without neutral sentences, SST can also be used in binary mode, where the split is 6,920 training, 872 validation, and 1,821 testing.", "labels": [], "entities": [{"text": "SST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9869418740272522}]}, {"text": "Furthermore, we apply DSCNN on question type classification task on TREC dataset (), where sentences are questions in the following 6 classes: {abbreviation, entity, description, location, numeric}.", "labels": [], "entities": [{"text": "question type classification", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7425729831059774}, {"text": "TREC dataset", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.951178789138794}]}, {"text": "The entire dataset consists of 5,452 training examples and 500 testing examples.", "labels": [], "entities": []}, {"text": "We also benchmark our system on the subjectivity classification dataset (SUBJ) released by ().", "labels": [], "entities": [{"text": "subjectivity classification dataset (SUBJ)", "start_pos": 36, "end_pos": 78, "type": "DATASET", "confidence": 0.743103876709938}]}, {"text": "The dataset contains 5,000 subjective sentences and 5,000 objective sentences.", "labels": [], "entities": []}, {"text": "We report 10-fold cross validation results as the baseline does.", "labels": [], "entities": []}, {"text": "For document-level dataset, we use Large Movie Review (IMDB) created by).", "labels": [], "entities": []}, {"text": "There are 25,000 training and 25,000 testing examples with binary sentiment polarity labels, and 50,000 unlabeled examples.", "labels": [], "entities": []}, {"text": "Different from Stanford Sentiment Treebank and Movie Review dataset, every example in this dataset has several sentences.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 15, "end_pos": 42, "type": "DATASET", "confidence": 0.864874005317688}, {"text": "Movie Review dataset", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.9251530766487122}]}], "tableCaptions": []}