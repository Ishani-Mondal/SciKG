{"title": [{"text": "Knowledge-Guided Linguistic Rewrites for Inference Rule Verification", "labels": [], "entities": [{"text": "Inference Rule Verification", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7516316970189413}]}], "abstractContent": [{"text": "A corpus of inference rules between a pair of relation phrases is typically generated using the statistical overlap of argument-pairs associated with the relations (e.g., PATTY, CLEAN).", "labels": [], "entities": [{"text": "PATTY", "start_pos": 171, "end_pos": 176, "type": "METRIC", "confidence": 0.792395830154419}]}, {"text": "We investigate knowledge-guided linguistic rewrites as a secondary source of evidence and find that they can vastly improve the quality of inference rule corpora, obtaining 27 to 33 point precision improvement while retaining substantial recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9772552847862244}, {"text": "recall", "start_pos": 238, "end_pos": 244, "type": "METRIC", "confidence": 0.9962735176086426}]}, {"text": "The facts inferred using cleaned inference rules are 29-32 points more accurate.", "labels": [], "entities": []}], "introductionContent": [{"text": "The visions of machine reading and deep language understanding) emphasize the ability to draw inferences from text to discover implicit information that may not be explicitly stated.", "labels": [], "entities": [{"text": "deep language understanding", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6313720146814982}]}, {"text": "This has natural applications to textual entailment (, KB completion (, and effective querying over Knowledge Bases (KBs).", "labels": [], "entities": [{"text": "KB completion", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7704668641090393}]}, {"text": "One popular approach for fact inference is to use a set of inference rules along with probabilistic models such as Markov Logic Networks ( or Bayesian Logic Programs () to produce humaninterpretable proof chains.", "labels": [], "entities": [{"text": "fact inference", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7698679864406586}]}, {"text": "While scalable (), this is bound by the coverage and quality of the background knowledge -the set of inference rules that enable the inference ().", "labels": [], "entities": [{"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9547206163406372}]}, {"text": "The paper focuses on generating a high precision subset of inference rules over Open Information Extraction (OpenIE)) relation phrases (see.", "labels": [], "entities": [{"text": "Open Information Extraction (OpenIE)) relation phrases", "start_pos": 80, "end_pos": 134, "type": "TASK", "confidence": 0.5976442620158195}]}, {"text": "OpenIE systems generate a schema-free KB where entities and relations are represented via normalized but not disambiguated textual strings.", "labels": [], "entities": []}, {"text": "Such OpenIE KBs scale to the Web.", "labels": [], "entities": []}, {"text": "Most existing large-scale corpora of inference rules are generated using distributional similarity, like argument-pair overlap (), but often eschew any linguistic or compositional insights.", "labels": [], "entities": []}, {"text": "Our early analysis revealed that such inference rules have very low precision, not enough to be useful for many real tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9983959794044495}]}, {"text": "For human-facing applications (such as IE-based demos), high precision is critical.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9986411929130554}]}, {"text": "Inference rules have a multiplicative impact, since one poor rule could potentially generate many bad KB facts.", "labels": [], "entities": []}, {"text": "Contributions: We investigate the hypothesis that \"knowledge-guided linguistic rewrites can provide independent verification for statistically-generated Open IE inference rules.\"", "labels": [], "entities": [{"text": "Open IE inference rules", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.6410225480794907}]}, {"text": "Our system KGLR's rewrites exploit the compositional structure of Open IE relation phrases alongside knowledge in resources like Wordnet and thesaurus.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9490392208099365}]}, {"text": "KGLR independently verifies rules from existing inference rule corpora and can be seen as additional annotation on existing inference rules.", "labels": [], "entities": [{"text": "KGLR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7795264720916748}]}, {"text": "The verified rules are 27 to 33 points more accurate than the original corpora and still retain a substantial recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9965517520904541}]}, {"text": "The precision of inferred knowledge also has a precision boost of over 29 points.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991575479507446}, {"text": "precision boost", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9828618466854095}]}, {"text": "We release our KGLR implementation, its annotations on two popular rule corpora along with gold set used for evaluation and the annotation guidelines for further use (available at https://github.com/dair-iitd/kglr.git).", "labels": [], "entities": []}], "datasetContent": [{"text": "KGLR verifies a subset of rules from CLEAN and PPDB e to produce, VCLEAN and VPPDB e . Our experiments answer these research questions: (1) What is the precision and size of the verified subsets compared to original corpora?, (2) How does additional knowledge generated after performing inference using these rules compare with each other? and (3) Which rewrites are critical to KGLR performance?", "labels": [], "entities": [{"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9985640645027161}]}, {"text": "Comparison of CLEAN and VCLEAN: The original CLEAN corpus has about 102K rules.", "labels": [], "entities": [{"text": "CLEAN corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8228603005409241}]}, {"text": "KGLR verifies about 36K rules and filter 66K rules out.", "labels": [], "entities": [{"text": "KGLR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8789812922477722}, {"text": "verifies", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.7928369045257568}]}, {"text": "To estimate the precisions of CLEAN and VCLEAN we independently sampled a random subset of 200 inference rules from each and asked two annotators (graduate level NLP students) to label the rules as corrector incorrect.", "labels": [], "entities": [{"text": "precisions", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9982519745826721}, {"text": "VCLEAN", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.46316882967948914}]}, {"text": "Rules were mixed together and the annotators were blind to the system that generated a rule.", "labels": [], "entities": []}, {"text": "Our initial annotation guideline was similar to that of textual entailment -label a rule as correct if the consequent can usually be inferred given the antecedent, for most naturally occurring argument-pairs for the antecedent.", "labels": [], "entities": []}, {"text": "Our annotators faced one issue with the guideline -some inference rules were valid if (X,Y) were bound to specific types, but not for others.", "labels": [], "entities": []}, {"text": "For example, (X, be born in, Y) \u2192 (Y, be birthplace of, X) is valid if Y is a location, not if Y is a year.", "labels": [], "entities": []}, {"text": "Even seemingly correct inference rules, e.g., (X, is the father of, Y) \u2192 (Y, is the child of, X), can make unusual incorrect inferences: (Gandhi, is the father of, India) does not imply (India, is the child of, Gandhi).", "labels": [], "entities": []}, {"text": "Unfortunately, these corpora don't associate argumenttype information with their inference rules.", "labels": [], "entities": []}, {"text": "To mitigate this we refined the annotation guidelines to accept inference rules as correct as long as they are valid for some type-pair.", "labels": [], "entities": []}, {"text": "The interannotator agreement with this modification was 94% (\u03ba = 0.88).", "labels": [], "entities": []}, {"text": "On the subset of the tags where the two annotators agreed we find the precision of CLEAN to be 48.9%, whereas VCLEAN was evaluated to be 82.5% precise -much more useful for real-world applications.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9996532201766968}, {"text": "VCLEAN", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.7605514526367188}, {"text": "precise", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.988645076751709}]}, {"text": "Multiplying the precision with their sizes, we find the effective yield 2 of CLEAN to be 50K compared to 30K for VCLEAN.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9989244341850281}, {"text": "effective yield 2", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.7997315923372904}, {"text": "VCLEAN", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9008136987686157}]}, {"text": "Overall, we find that VCLEAN obtains a 33 point precision improvement with an effective yield of about 60%.", "labels": [], "entities": [{"text": "VCLEAN", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.7521584033966064}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.99310702085495}]}, {"text": "Error Analysis: Most of VCLEAN errors are due to erroneous (or unusual) thesaurus synonyms.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9595822095870972}, {"text": "VCLEAN errors", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.6258809566497803}]}, {"text": "For missed recall, we analyzed CLEAN's sample missed by VCLEAN.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9751299619674683}, {"text": "CLEAN", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.965592622756958}, {"text": "VCLEAN", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.6377338171005249}]}, {"text": "We find that only about 13% of those are world knowledge rules (e.g., rule #6 in).", "labels": [], "entities": []}, {"text": "Other missed recall is because of some missing rewrites, missing thesaurus synonyms, spelling mistakes.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9965975880622864}]}, {"text": "These can potentially be captured by using other resources and adding rewrite rules.", "labels": [], "entities": []}, {"text": "Comparison of PPDB e and VPPDB e : Unlike CLEAN, PPDB2.0 associates a confidence value for each rule, which can be varied to obtain different levels of precision and yield.", "labels": [], "entities": [{"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9991775155067444}]}, {"text": "We control for yield so that we can compare precisions directly.", "labels": [], "entities": [{"text": "yield", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9759846925735474}, {"text": "precisions", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9917365908622742}]}, {"text": "We operate on PPDB e subset that has an Open IE-2 Yield is proportional to recall like relation phrase on both sides; this was identified by matching to ReVerb syntactic patterns).", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9943140745162964}]}, {"text": "This subset is of size 402K.", "labels": [], "entities": []}, {"text": "KGLR on this produces 85K verified rules (VPPDB e ).", "labels": [], "entities": [{"text": "KGLR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9391700625419617}, {"text": "85K verified rules (VPPDB e )", "start_pos": 22, "end_pos": 51, "type": "METRIC", "confidence": 0.7592294301305499}]}, {"text": "We find the threshold for confidence values in PPDB e that achieves the same yield (confidence > 0.342).", "labels": [], "entities": []}, {"text": "We perform annotation on PPDB e (0.342) and VPPDB e using same annotation guidelines as before.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement was 91% (\u03ba = 0.82).", "labels": [], "entities": []}, {"text": "On the subset of the tags where the two annotators agreed we find the precision of PPDB e to below -44.2%, whereas VPPDB e was evaluated to be 71.4% precise.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9996311664581299}, {"text": "precise", "start_pos": 149, "end_pos": 156, "type": "METRIC", "confidence": 0.980776309967041}]}, {"text": "We notice that about 4 in 5 PPDB relation phrases are of length 1 or 2 (whereas 50% of CLEAN relation phrases are of length \u2265 3).", "labels": [], "entities": []}, {"text": "This contributes to a slightly lower precision of VPPDB e , as most rules are proved by thesaurus synonymy and the power of KGLR to handle compositionality of longer relation phrases does not get exploited.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9981783628463745}]}, {"text": "Comparison of Inferred Facts: A typical use case of inference rules is in generating new facts by applying inference rules to a KB.", "labels": [], "entities": []}, {"text": "We independently apply VCLEAN's, CLEAN's, PPDB e 's and VPPDB e 's inference rules on a public corpus of 4.2 million ReVerb triples.", "labels": [], "entities": []}, {"text": "Since ReVerb itself has significant extraction errors (our estimate is 20% errors) and our goal is to evaluate the quality of inference, we restrict this evaluation to only the subset of accurate ReVerb extractions.", "labels": [], "entities": [{"text": "ReVerb extractions", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.6536119282245636}]}, {"text": "VCLEAN and CLEAN facts: We sampled about 200 facts inferred by VCLEAN rules and CLEAN rules each (applied over accurate ReVerb extractions) and gave the original sentence as well as inferred facts to two annotators.", "labels": [], "entities": []}, {"text": "We obtained a high inter-annotator agreement of 96.3%(\u03ba = 0.92) and we discarded disagreements from final analysis.", "labels": [], "entities": []}, {"text": "Overall, facts inferred by CLEAN achieved a precision of about 49.1% and those inferred by VCLEAN obtained a 81.6% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9993546605110168}, {"text": "VCLEAN", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.7329703569412231}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.989240825176239}]}, {"text": "The estimated yields of fact corpora (precision\u00d7size) are 7 and 4.5 million for CLEAN and VCLEAN respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.998307466506958}, {"text": "VCLEAN", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.5606415867805481}]}, {"text": "This yield estimate does not include the initial 4.2 million facts.", "labels": [], "entities": []}, {"text": "PPDB e and VPPDB e facts: As done previously, we sampled 200 facts inferred by PPDB e and VPPDB e rules, which were annotated by two annotators.", "labels": [], "entities": []}, {"text": "We obtained a good inter annotator agree-).", "labels": [], "entities": []}, {"text": "We ran KGLR by turning off one rewrite on a sample of 600 CLEAN rules (our development set) and calculating its precision and recall.", "labels": [], "entities": [{"text": "KGLR", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.6796772480010986}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9996165037155151}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9985968470573425}]}, {"text": "The ablation study highlights that most rewrites add some value to the performance of KGLR, however Antonyms and Dropping modifiers are particularly important for precision and Active-Passive and Redundant Preposition add substantial recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9990749359130859}, {"text": "recall", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.9975211024284363}]}], "tableCaptions": []}