{"title": [{"text": "Dynamic Feature Induction: The Last Gist to the State-of-the-Art", "labels": [], "entities": [{"text": "Dynamic Feature Induction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6493071615695953}]}], "abstractContent": [{"text": "We introduce a novel technique called dynamic feature induction that keeps inducing high dimensional features automatically until the feature space becomes 'more' linearly separable.", "labels": [], "entities": [{"text": "dynamic feature induction", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6917620102564493}]}, {"text": "Dynamic feature induction searches for the feature combinations that give strong clues for distinguishing certain label pairs, and generates joint features from these combinations.", "labels": [], "entities": []}, {"text": "These induced features are trained along with the primitive low dimensional features.", "labels": [], "entities": []}, {"text": "Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition , and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7745289504528046}, {"text": "named entity recognition", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.5997116466363271}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9995414018630981}, {"text": "F1-score", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9995243549346924}]}], "introductionContent": [{"text": "Feature engineering typically involves two processes: the process of discovering novel features with domain knowledge, and the process of optimizing combinations between existing features.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8593932688236237}]}, {"text": "Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do.", "labels": [], "entities": []}, {"text": "Optimizing feature combinations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features.", "labels": [], "entities": []}, {"text": "It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learning with less intensive feature engineering (.", "labels": [], "entities": []}, {"text": "Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks).", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7976129353046417}]}, {"text": "Coupled with embedding approaches (), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks).", "labels": [], "entities": []}, {"text": "However, neural networks are not as good at optimizing combinations between sparse features, which are still the most dominating factors in natural language processing.", "labels": [], "entities": []}, {"text": "This paper introduces anew technique called dynamic feature induction that automates the optimization of feature combinations (Section 3), and can be easily adapted to any NLP task using sparse features.", "labels": [], "entities": [{"text": "dynamic feature induction", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6515579720338186}]}, {"text": "Dynamic feature induction allows humans to focus on the first part of feature engineering, the discovery of novel features, while machines handle the second part.", "labels": [], "entities": [{"text": "feature induction", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7449069321155548}, {"text": "feature engineering", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.6913667470216751}]}, {"text": "Our approach was experimented with two core NLP tasks, part-of-speech tagging (Section 4) and named entity recognition (Section 5) and showed the state-of-the-art results for both tasks.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7809049189090729}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6006662845611572}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distributions of the Wall Street Journal corpus. TRN:", "labels": [], "entities": [{"text": "Wall Street Journal corpus. TRN", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.8664758801460266}]}, {"text": " Table 3: Part-of-speech tagging accuracies on the development", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8449098169803619}]}, {"text": " Table 4: Part-of-speech tagging accuracies on the evaluation set.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.751594215631485}]}, {"text": " Table 5: Distributions of the English corpus from the CoNLL'03", "labels": [], "entities": [{"text": "English corpus from the", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.8420402854681015}, {"text": "CoNLL'03", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.831525981426239}]}, {"text": " Table 7: Precision and recall on the development set for named", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981210827827454}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9994869232177734}]}, {"text": " Table 8: F1-scores on the development and the evaluation sets", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989480376243591}]}]}