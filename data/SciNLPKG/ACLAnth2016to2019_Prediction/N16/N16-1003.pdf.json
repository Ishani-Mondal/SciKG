{"title": [{"text": "Selecting Syntactic, Non-redundant Segments in Active Learning for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.721189334988594}]}], "abstractContent": [{"text": "Active learning is a framework that makes it possible to efficiently train statistical models by selecting informative examples from a pool of unlabeled data.", "labels": [], "entities": []}, {"text": "Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8516268193721771}]}, {"text": "However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content.", "labels": [], "entities": [{"text": "phrase-based active learning", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7060140570004781}, {"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9072170853614807}]}, {"text": "In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9928830862045288}]}, {"text": "Experiments using both simulation and extensive manual translation by professional translators find the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more confident in their translations 1 .", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 162, "end_pos": 172, "type": "METRIC", "confidence": 0.984478086233139}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), large quantities of high-quality bilingual data are essential to achieve high translation accuracy.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8149374077717463}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.8127181529998779}]}, {"text": "While in many cases large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 anyone of the preceding claims\ud97b\udf59, in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing).", "labels": [], "entities": []}, {"text": "In these cases, active learning ( \u00a72), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.9966327548027039}]}, {"text": "Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9934424161911011}]}, {"text": "These methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9785848259925842}]}, {"text": "For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in bilingual data), have low confidence according to the MT system ( , or are predicted to be poor translations by an MT quality estimation system.", "labels": [], "entities": []}, {"text": "However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate.", "labels": [], "entities": []}, {"text": "To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks).", "labels": [], "entities": []}, {"text": "For MT in particular, have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words ( \u00a73).", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9800384640693665}]}, {"text": "Nevertheless, our experiments identified two major practical problems with this method.", "labels": [], "entities": []}, {"text": "First, as shown in (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency.", "labels": [], "entities": [{"text": "translation of redundant phrases", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.8480778783559799}]}, {"text": "Second, it is common to see fragments of complex phrases such as \"one of the preceding,\" which maybe difficult for workers to translate into a contiguous phrase in the target language.", "labels": [], "entities": []}, {"text": "In this work, we propose two methods that aim to solve these two problems and improve the efficiency and reliability of segment-based active learning for SMT ( \u00a74).", "labels": [], "entities": [{"text": "SMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9932418465614319}]}, {"text": "For the problem of overlapping phrases, we note that by merging overlapping phrases, as shown in (b), we can reduce the number of redundant words annotated and improve training efficiency.", "labels": [], "entities": []}, {"text": "We adopt the idea of maximal substrings) which both encode this idea of redundancy, and can be calculated to arbitrary length in linear time using enhanced suffix arrays.", "labels": [], "entities": []}, {"text": "For the problem of phrase structure fragmentation, we propose a simple heuristic to count only well-formed syntactic constituents in a parse tree, as shown in.", "labels": [], "entities": [{"text": "phrase structure fragmentation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7442771991093954}]}, {"text": "To investigate the effect of our proposed methods on learning efficiency, we perform experiments on English-French and English-Japanese translation tasks in which we incrementally add new parallel data, update models and evaluate translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 242, "end_pos": 250, "type": "METRIC", "confidence": 0.8834468722343445}]}, {"text": "Results from both simulation experiments ( \u00a75) and 120 hours of work by professional translators ( \u00a76) demonstrate improved efficiency with respect to the number of words annotated.", "labels": [], "entities": []}, {"text": "We also found that human translators took more time, but were more confident in their results on segments selected by the proposed method.", "labels": [], "entities": []}], "datasetContent": [{"text": "To confirm that the results from the simulation in the previous section carryover to actual translators, we further performed experiments in which professional translators translated the selected segments.", "labels": [], "entities": []}, {"text": "This also allowed us to examine the actual amount of time required to perform translation, and how confident the translators were in their translations.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9803362488746643}]}, {"text": "We designed a web user interface as shown in, and outsourced to an external organization    The morphologies using scanning electron microscopy ( SEM ) were studied .\ud97b\udf59 in showing a sentence including the selected phrase, 7 highlighting the phrase, and requesting that the translator translate the highlighted part.", "labels": [], "entities": []}, {"text": "We also requested that every worker select from 3 levels indicating how confident they were of their translation.", "labels": [], "entities": []}, {"text": "In the background, the time required to complete the translation is measured from when the new phrase is shown until when the translation is submitted.", "labels": [], "entities": []}, {"text": "The methods selected for comparative evaluation are sentence selection based on 4-gram frequency (sent-by-4gram-freq) and phrase selection based on 4-gram frequency (4gram-freq) as baseline methods, and the phrase selection based on both parse trees and semi-maximality (reduced-struct-freq) as BLEU Score vs. Cumulative Duration (En-Ja) sent-by-4gram-freq 4gram-freq reduced-struct-freq the proposed method.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7372454404830933}, {"text": "BLEU Score", "start_pos": 295, "end_pos": 305, "type": "METRIC", "confidence": 0.9764473736286163}]}, {"text": "For each method we collected translations of 10k source words, alternating between segments selected by each method to prevent bias.", "labels": [], "entities": []}, {"text": "We used the same dataset as the English-Japanese translation task and the same tools in the simulation experiment (Section 5).", "labels": [], "entities": [{"text": "English-Japanese translation task", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7407109240690867}]}, {"text": "However, for training target language models, we interpolated one trained with the base data and a second trained with collected data by using SRILM) because the hand-made data set was too small to train a full language model using only this data.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.568789005279541}]}, {"text": "We tuned the interpolation coefficient such that it maximizes the perplexity for the tuning dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of phrases and average words/phrase in each method", "labels": [], "entities": []}, {"text": " Table 4: Total working time and average confidence level", "labels": [], "entities": [{"text": "confidence", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.5279515385627747}]}, {"text": " Table 5: Average working time of manual translation corre-", "labels": [], "entities": [{"text": "manual translation corre", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7528682351112366}]}, {"text": " Table 6: Average confidence level of manual translation corre-", "labels": [], "entities": [{"text": "manual translation corre", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7547479172547659}]}, {"text": " Table 7: BLEU score when training on phrases with a certain", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9595968127250671}]}]}