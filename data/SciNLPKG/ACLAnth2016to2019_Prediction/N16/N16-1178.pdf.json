{"title": [{"text": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "labels": [], "entities": [{"text": "Sentence Classification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8607282936573029}]}], "abstractContent": [{"text": "We introduce a novel, simple convolution neu-ral network (CNN) architecture-multi-group norm constraint CNN (MGNC-CNN)-that capitalizes on multiple sets of word embed-dings for sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.719435840845108}]}, {"text": "MGNC-CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector.", "labels": [], "entities": []}, {"text": "We then adopt a group regularization strategy that differentially penalizes weights associated with the subcom-ponents generated from the respective embedding sets.", "labels": [], "entities": []}, {"text": "This model is much simpler than comparable alternative architectures and requires substantially less training time.", "labels": [], "entities": []}, {"text": "Furthermore , it is flexible in that it does not require input word embeddings to be of the same dimensionality.", "labels": [], "entities": []}, {"text": "We show that MGNC-CNN consistently outperforms baseline models.", "labels": [], "entities": [{"text": "MGNC-CNN", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8091805577278137}]}], "introductionContent": [{"text": "Neural models have recently gained popularity for Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "For sentence classification, in particular, Convolution Neural Networks (CNN) have realized impressive performance).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7902421057224274}]}, {"text": "These models operate over word embeddings, i.e., dense, low dimensional vector representations of words that aim to capture salient semantic and syntactic properties).", "labels": [], "entities": []}, {"text": "An important consideration for such models is the specification of the word embeddings.", "labels": [], "entities": []}, {"text": "For example, initialize word vectors to random low-dimensional vectors to befit during training, while use fixed, one-hot encodings for each word.", "labels": [], "entities": []}, {"text": "By contrast, initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News (; these are then updated during training.", "labels": [], "entities": []}, {"text": "Initializing embeddings to pre-trained word vectors is intuitively appealing because it allows transfer of learned distributional semantics.", "labels": [], "entities": [{"text": "transfer of learned distributional semantics", "start_pos": 95, "end_pos": 139, "type": "TASK", "confidence": 0.7028234958648681}]}, {"text": "This has allowed a relatively simple CNN architecture to achieve remarkably strong results.", "labels": [], "entities": []}, {"text": "Many pre-trained word embeddings are now readily available on the web, induced using different models, corpora, and processing steps.", "labels": [], "entities": []}, {"text": "Different embeddings may encode different aspects of language: those based on bag-ofwords (BoW) statistics tend to capture associations (doctor and hospital), while embeddings based on dependency-parses encode similarity in terms of use (doctor and surgeon).", "labels": [], "entities": []}, {"text": "It is natural to consider how these embeddings might be combined to improve NLP models in general and CNNs in particular.", "labels": [], "entities": []}, {"text": "We propose MGNC-CNN, a novel, simple, scalable CNN architecture that can accommodate multiple off-the-shelf embeddings of variable sizes.", "labels": [], "entities": [{"text": "MGNC-CNN", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.8629518151283264}]}, {"text": "Our model treats different word embeddings as distinct groups, and applies CNNs independently to each, thus generating corresponding feature vectors (one per embedding) which are then concatenated at the classification layer.", "labels": [], "entities": []}, {"text": "Inspired by prior work exploiting regularization to encode structure for NLP tasks, we impose different regularization penalties on weights for features generated from the respective word embedding sets.", "labels": [], "entities": []}, {"text": "Our approach enjoys the following advantages compared to the only existing comparable model", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that", "labels": [], "entities": []}]}