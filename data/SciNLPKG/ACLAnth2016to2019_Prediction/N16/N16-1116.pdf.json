{"title": [{"text": "Unsupervised Ranking Model for Entity Coreference Resolution", "labels": [], "entities": [{"text": "Entity Coreference Resolution", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.7090675036112467}]}], "abstractContent": [{"text": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9338811039924622}, {"text": "deep language understanding", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6240238845348358}]}, {"text": "In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.7470227082570394}]}, {"text": "Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deter-ministic system (Lee et al., 2013) by 3.01%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.984809398651123}, {"text": "English data from the CoNLL-2012 shared task", "start_pos": 76, "end_pos": 120, "type": "DATASET", "confidence": 0.8165544399193355}]}], "introductionContent": [{"text": "Entity coreference resolution has become a critical component for many Natural Language Processing (NLP) tasks.", "labels": [], "entities": [{"text": "Entity coreference resolution", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8685439229011536}]}, {"text": "Systems requiring deep language understanding, such as information extraction), semantic event learning, and named entity linking) all benefit from entity coreference information.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7699612081050873}, {"text": "semantic event learning", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.6308602492014567}]}, {"text": "Entity coreference resolution is the task of identifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities.", "labels": [], "entities": [{"text": "Entity coreference resolution is the task of identifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities", "start_pos": 0, "end_pos": 151, "type": "Description", "confidence": 0.6823963820934296}]}, {"text": "In recent years, several supervised entity coreference resolution systems have been proposed, which, according to, can be categorized into three classes -mention-pair models, entity-mention models) and ranking models () -among which ranking models recently obtained state-of-the-art performance.", "labels": [], "entities": [{"text": "supervised entity coreference resolution", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.6853569895029068}]}, {"text": "However, the manually annotated corpora that these systems rely on are highly expensive to create, in particular when we want to build data for resource-poor languages.", "labels": [], "entities": []}, {"text": "That makes unsupervised approaches, which only require unannotated text for training, a desirable solution to this problem.", "labels": [], "entities": []}, {"text": "Several unsupervised learning algorithms have been applied to coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9745764136314392}]}, {"text": "presented a mention-pair nonparametric fully-generative Bayesian model for unsupervised coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8933182060718536}]}, {"text": "Based on this model, probabilistically induced coreference partitions via EM clustering.", "labels": [], "entities": [{"text": "EM clustering", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.7317447066307068}]}, {"text": "proposed an entity-mention model that is able to perform joint inference across mentions by using Markov Logic.", "labels": [], "entities": []}, {"text": "Unfortunately, these unsupervised systems' performance on accuracy significantly falls behind those of supervised systems, and are even worse than the deterministic rule-based systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.998496413230896}]}, {"text": "Furthermore, there is no previous work exploring the possibility of developing an unsupervised ranking model which achieved state-of-theart performance under supervised settings for entity coreference resolution.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 182, "end_pos": 211, "type": "TASK", "confidence": 0.7225471337636312}]}, {"text": "In this paper, we propose an unsupervised generative ranking model for entity coreference resolution.", "labels": [], "entities": [{"text": "generative ranking", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9194314181804657}, {"text": "entity coreference resolution", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.8073540727297465}]}, {"text": "Our experimental results on the English data from the CoNLL-2012 shared task ( show that our unsupervised system outperforms the Stanford deterministic system () by 3.01% absolute on the CoNLL official metric.", "labels": [], "entities": [{"text": "English data from the CoNLL-2012 shared task", "start_pos": 32, "end_pos": 76, "type": "DATASET", "confidence": 0.7901004510266441}, {"text": "CoNLL official metric", "start_pos": 187, "end_pos": 208, "type": "DATASET", "confidence": 0.951440433661143}]}, {"text": "The contributions of this work are (i) proposing the first unsupervised ranking model for entity coreference resolution.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.8146715760231018}]}, {"text": "(ii) giving empirical evaluations of this model on benchmark data sets.", "labels": [], "entities": []}, {"text": "(iii) considerably narrowing the gap to supervised coreference resolution accuracy.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.8399076163768768}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8911802172660828}]}], "datasetContent": [{"text": "Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010)) to train the model.", "labels": [], "entities": [{"text": "APW", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9256844520568848}, {"text": "NYT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7047343254089355}, {"text": "Gigaword Corpus", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8916658461093903}]}, {"text": "Following previous work, we remove duplicated documents and the documents which include fewer than 3 sentences.", "labels": [], "entities": []}, {"text": "The development and test data are the English data from the CoNLL-2012 shared task (, which is derived from the OntoNotes corpus ().", "labels": [], "entities": [{"text": "CoNLL-2012 shared task", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8090643684069315}, {"text": "OntoNotes corpus", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.9228844046592712}]}, {"text": "The corpora statistics are shown in.", "labels": [], "entities": []}, {"text": "Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees).", "labels": [], "entities": []}, {"text": "We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) ().", "labels": [], "entities": [{"text": "MUC", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.5520841479301453}, {"text": "B 3", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9611223638057709}]}, {"text": "In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAF m ) and BLANC (Recasens and Hovy, 2011).", "labels": [], "entities": [{"text": "Mention-based CEAF (CEAF m )", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.6574566066265106}, {"text": "BLANC", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9943807721138}]}, {"text": "All the results are given by the latest version of CoNLL-2012 scorer illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system ( and Multigraph: the unsupervised multigraph system, and one unsupervised system, namely MIR: the unsupervised system using most informative relations (.", "labels": [], "entities": [{"text": "CoNLL-2012 scorer", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9358337819576263}]}, {"text": "Our model outperforms the three baseline systems on all the evaluation metrics.", "labels": [], "entities": []}, {"text": "Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.6326020359992981}, {"text": "F1 score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9057754576206207}, {"text": "CoNLL 2011 shared task", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7527465522289276}, {"text": "CoNLL 2012 development and test sets", "start_pos": 158, "end_pos": 194, "type": "DATASET", "confidence": 0.8749336302280426}]}, {"text": "The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.5498220920562744}, {"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8781711161136627}]}, {"text": "Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score.", "labels": [], "entities": [{"text": "MIR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7043418884277344}, {"text": "CoNLL F1 score", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.7047463854153951}]}], "tableCaptions": [{"text": " Table 1: Feature set for representing a mention under different resolution modes. The Distance feature is for parameter q, while all", "labels": [], "entities": [{"text": "Distance", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9798723459243774}]}, {"text": " Table 2: Corpora statistics. \"ON-Dev\" and \"ON-Test\" are the", "labels": [], "entities": [{"text": "ON-Dev", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.736515462398529}]}, {"text": " Table 3: F1 scores of different evaluation metrics for our model, together with two deterministic systems and one unsupervised", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991853833198547}]}]}