{"title": [{"text": "A Joint Model of Orthography and Morphological Segmentation", "labels": [], "entities": [{"text": "Morphological Segmentation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6862741261720657}]}], "abstractContent": [{"text": "We present a model of morphological seg-mentation that jointly learns to segment and restore orthographic changes, e.g., funniest \u2192 fun-y-est.", "labels": [], "entities": []}, {"text": "We term this form of analysis canon-ical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest \u2192 funn-i-est.", "labels": [], "entities": []}, {"text": "We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7435159683227539}]}], "introductionContent": [{"text": "Morphological segmentation is useful for NLP applications, such as, automatic speech recognition (), keyword spotting (), machine translation ( and parsing.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8779441118240356}, {"text": "automatic speech recognition", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6019545396169027}, {"text": "keyword spotting", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7785345911979675}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.8448452055454254}]}, {"text": "Prior work cast the problem as surface segmentation: a word form w is segmented into a sequence of substrings whose concatenation is w.", "labels": [], "entities": [{"text": "surface segmentation", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7818326652050018}]}, {"text": "In this paper, we introduce the problem of canonical segmentation: w is analyzed as a sequence of canonical morphemes, based on a set of word forms that have been \"canonically\" annotated for supervised learning.", "labels": [], "entities": []}, {"text": "Each canonical morpheme c corresponds to a surface morphs, defined as its orthographic manifestation, i.e., as the substring of w that is generated by applying editing operations like insertion and deletion.", "labels": [], "entities": []}, {"text": "Consider the following example: funniest has a canonical segmentation fun-y-est with three morphs funn-i-est.", "labels": [], "entities": []}, {"text": "Arriving at the canonical analysis requires two edit operations: delete n in funn and replace i with yin i. gives examples of orthography (i.e., the concatentation of surface morphs), underlying form (i.e., the concatentation of canonical morphemes) and canonical segmentation in three languages.", "labels": [], "entities": []}, {"text": "Canonical segmentation is motivated in the following three ways: (i) Computational morphology is the study of how words and their meanings are composed from smaller units.", "labels": [], "entities": [{"text": "Canonical segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8942636251449585}, {"text": "Computational morphology", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.7975904643535614}]}, {"text": "This goal is better supported by canonical morphemes than by surface morphemes because the smaller units are more accurately modeled.", "labels": [], "entities": []}, {"text": "For funniest, composition can reason with canonical morphemes fun and y, whereas surface segmentation must work with funn and i.", "labels": [], "entities": []}, {"text": "(ii) Morphological analysis is typically done with attribute-value pairs (AVP), e.g.,.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.9851607382297516}]}, {"text": "While AVP is a good representation for inflectional morphology, it is not powerful enough for derivational morphology.", "labels": [], "entities": []}, {"text": "If we represent the derivation of funnier as [ lemma=FUN, deriv-suffix=-Y, degree=SUPER ], then it is no longer clear in this fixed representation whether degree = SUPER applies to fun or fun+y.", "labels": [], "entities": [{"text": "FUN", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9792501330375671}]}, {"text": "1 Canonical segmentation is more flexible-allowing us to express derivational relations without committing to a fixed attribute-value structure, which are used to study inflection.", "labels": [], "entities": [{"text": "Canonical segmentation", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.8298560082912445}]}, {"text": "This point is important due to the fundamental distinction between the creation of words through inflection vs. through derivation.", "labels": [], "entities": []}, {"text": "Inflection alters words to express syntactic relations (e.g., tense) with no major change in meaning nor POS.", "labels": [], "entities": [{"text": "POS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9627591371536255}]}, {"text": "For example, perturbed and perturbs are inflections of the verb perturb.", "labels": [], "entities": []}, {"text": "On the other hand, derivation modifies words more drastically-often changing the meaning or POS.", "labels": [], "entities": [{"text": "POS", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9216107726097107}]}, {"text": "For example, the noun perturbation derives from the verb stem perturb and the suffixation (.", "labels": [], "entities": []}, {"text": "(iii) Most NLP systems take word forms as atomic building blocks.", "labels": [], "entities": []}, {"text": "We propose canonical morphemes, an alternative representation that models the structure of a language's lexicon and supports applications that benefit from access to the internal structure of words.", "labels": [], "entities": []}, {"text": "This includes access to internal morphological structure, e.g., canonical morphemes like -y and -ly are recognized (independent of their orthographic manifestation) as derivational suffixes that cause predictable modifications; as well as access to internal semantic structure, e.g., the canonical segmentations of fun and funny share the canonical morpheme fun).", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "We present the challenging new task of canonical segmentation.", "labels": [], "entities": [{"text": "canonical segmentation", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6610277444124222}]}, {"text": "We develop a feature-rich structured joint model for canonical segmentation, which accounts for orthographic variation and segmentlevel structure.", "labels": [], "entities": [{"text": "canonical segmentation", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7019263505935669}]}, {"text": "We derive an efficient importance sampling algorithm for approximate inference.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8441613614559174}]}, {"text": "We present experiments on three languages: English, German and Indonesian.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide canonical segmentation experiments in three languages: English, German and Indonesian.", "labels": [], "entities": []}, {"text": "Evaluating morphological segmentation is tricky.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6622088104486465}]}, {"text": "The standard measure for the supervised task is border F 1 , which measures how often the segmentation boundaries posited by the model are correct.", "labels": [], "entities": [{"text": "border F 1", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9787406524022421}]}, {"text": "However, this measure assumes that the concatenation of the segments is identical to the input string (i.e., surface segmentation) and is thus not applicable to canonical segmentation.", "labels": [], "entities": []}, {"text": "On the other hand, the Morpho Challenge competition () uses a measure that samples a large number of word pairs from a linguistic gold standard.", "labels": [], "entities": []}, {"text": "A form is considered correct if the gold standard contains at least one overlapping morph and the model posits at least one overlapping morph-this is problematic because for languages with multi-morphemic words (e.g., German), one should consider all morphs.", "labels": [], "entities": []}, {"text": "Moreover, we can actually recover the linguistically annotated gold standard in contrast to unsupervised methods.", "labels": [], "entities": []}, {"text": "Instead, we report results under three measures: error rate, edit distance and morpheme F 1 . Error rate is the proportion of analyses that are completely correct.", "labels": [], "entities": [{"text": "error rate", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9775720238685608}, {"text": "edit distance", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.8993628621101379}, {"text": "morpheme F 1", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.8857752084732056}, {"text": "Error rate", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9914439022541046}]}, {"text": "Since error rate gives no partial credit, we also report edit distance between the predicted analysis and the gold standard, where both are encoded as strings using a distinguished boundary character at segment boundaries.", "labels": [], "entities": [{"text": "error rate", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9503395855426788}, {"text": "edit distance", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9634734392166138}]}, {"text": "Finally, morpheme F 1 (van den) considers overlap between the set of morphemes in the model's analysis and the set of morphemes in the gold standard.", "labels": [], "entities": [{"text": "morpheme F 1", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.613991916179657}]}, {"text": "In this case, precision asks how often did the predicted segmentation contain morphemes in the gold standard and recall asks how often were the gold standard morphemes in the predicted segmentation.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996386766433716}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9986304044723511}]}, {"text": "gives results for the three measures.", "labels": [], "entities": []}, {"text": "Under error rate and morpheme F 1 our joint model performs the best on all three languages, followed by our pipeline model and then the two baselines.", "labels": [], "entities": [{"text": "error rate", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9473505914211273}]}, {"text": "In fact, we observe that error rate and F 1 are quite correlated in general.", "labels": [], "entities": [{"text": "error rate", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9856031537055969}, {"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9935887157917023}]}, {"text": "Under edit distance, the joint model is the best model on German and Indonesian, but the pipeline model is superior on English.", "labels": [], "entities": []}, {"text": "Error analysis indicates that the lower performance is due to spurious insertions.", "labels": [], "entities": []}, {"text": "For example, our model incorrectly analyzes ruby (stone) as ruble-y, mistaking the ruby as an adjectival form of ruble (the Russian currency); the correct analysis is ruby \u2192 ruby.", "labels": [], "entities": []}, {"text": "We believe that a richer transduction component may fix some of these problems.", "labels": [], "entities": []}, {"text": "Overall, our joint model performs well; it is on average within one edit operation of the gold segmentation on three languages.", "labels": [], "entities": []}], "tableCaptions": []}