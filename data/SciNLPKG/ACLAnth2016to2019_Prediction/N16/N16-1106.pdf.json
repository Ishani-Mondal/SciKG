{"title": [{"text": "DAG-Structured Long Short-Term Memory for Semantic Compositionality", "labels": [], "entities": [{"text": "Semantic Compositionality", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.6931623816490173}]}], "abstractContent": [{"text": "Recurrent neural networks, particularly long short-term memory (LSTM), have recently shown to be very effective in a wide range of sequence modeling problems, core to which is effective learning of distributed representation for subsequences as well as the sequences they form.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7169536203145981}]}, {"text": "An assumption in almost all the previous models, however, posits that the learned representation (e.g., a distributed representation fora sentence), is fully compositional from the atomic components (e.g., representations for words), while non-compositionality is a basic phenomenon inhuman languages.", "labels": [], "entities": []}, {"text": "In this paper, we relieve the assumption by extending the chain-structured LSTM to directed acyclic graphs (DAGs), with the aim to endow linear-chain LSTMs with the capability of considering compositionality together with non-compositionality in the same semantic composition framework.", "labels": [], "entities": []}, {"text": "From a more general viewpoint, the proposed models incorporate additional prior knowledge into recurrent neural networks, which is interesting to us, considering most NLP tasks have relatively small training data and appropriate prior knowledge could be beneficial to help cover missing semantics.", "labels": [], "entities": []}, {"text": "Our experiments on sentiment composition demonstrate that the proposed models achieve the state-of-the-art performance , outperforming models that lack this ability.", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.9429571330547333}]}], "introductionContent": [{"text": "Recurrent neural networks, particularly long shortterm memory (LSTM), have recently shown to be very effective in a wide range of sequence modeling problems, including speech recognition (, automatic machine translation, and image-to-text conversion ( ), among many others.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7051219344139099}, {"text": "speech recognition", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7851723432540894}, {"text": "automatic machine translation", "start_pos": 190, "end_pos": 219, "type": "TASK", "confidence": 0.6294863323370615}, {"text": "image-to-text conversion", "start_pos": 225, "end_pos": 249, "type": "TASK", "confidence": 0.71531742811203}]}, {"text": "The specific memory copying and gating configurations in LSTM's memory blocks render an effective mechanism in capturing both short and distant interplays in an input sequence.", "labels": [], "entities": []}, {"text": "In modeling sequences, core to many problems is to learn effective distributed representations for subsequences and the sequences they form.", "labels": [], "entities": []}, {"text": "A strong assumption inmost previous models, however, posits that the learned representation (e.g., a distributed representation fora sentence) is fully compositional from the atomic components (e.g., representations for words), while non-compositionality is a basic phenomenon inhuman languages and other modalities, which does not only include rather rigid cases such as idiomatic expressions (e.g., kick the bucket) but also soft cases that are harder to make a binary judgment.", "labels": [], "entities": []}, {"text": "A framework with the capability to consider both compositionality and non-compositionality in semantic composition are of theoretic interest.", "labels": [], "entities": []}, {"text": "From a more pragmatical viewpoint, if one is able to holistically obtain the representations fora sequence (e.g., for the bigram must try in a customer-review corpus for sentiment analysis), it would be desirable that a composition model has the ability to choose the sources of knowledge it can trust more: the composition of subsequences of this sequence, the holistic representation, or a soft combination of them, in the process of semantic composition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8746920228004456}]}, {"text": "In such situations, whether this sequence (must try) is indeed compositional or non-compositional may often be blurry or may not bean explicit concern of applications.", "labels": [], "entities": []}, {"text": "In this paper, we extend the popular chainstructured LSTM to directed acyclic graph (DAG) structures, with the aim to endow conventional LSTM with the capability of considering compositionality and non-compositionality together.", "labels": [], "entities": []}, {"text": "From a more general viewpoint, the proposed models are along the line of incorporating external knowledge into recurrent neural models, which is interesting to us, considering that most NLP tasks have relatively limited amount of training data, and external prior knowledge could be beneficial to help cover missing semantics.", "labels": [], "entities": []}, {"text": "The proposed models unify the compositional power of recurrent neural networks (RNN) and additional prior knowledge.", "labels": [], "entities": []}, {"text": "In general, neural nets are powerful approaches for composition, which can fit very complicated compositional functions underlying the annotated data.", "labels": [], "entities": []}, {"text": "Over that, externally obtained semantics could help cope with missing information in limited training data.", "labels": [], "entities": []}, {"text": "We demonstrated the models' effectiveness in sentiment composition, a popular semantic composition problem that optimizes a sentiment objective.", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.9021396338939667}]}, {"text": "We show that the proposed models achieve the stateof-the-art performance on two benchmark datasets, without any feature engineering, by unifying the compositional strength of LSTM with external semantic knowledge.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we study the proposed models on a semantic composition task that determine the sentiment of apiece of text.", "labels": [], "entities": [{"text": "semantic composition task", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7982431451479594}]}, {"text": "We use social-media messages from the official SemEval Sentiment Analysis in Twitter competition.", "labels": [], "entities": [{"text": "SemEval Sentiment Analysis", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8127814928690592}]}, {"text": "Analyzing social-media text has attracted extensive attention () and have many applications.", "labels": [], "entities": [{"text": "Analyzing social-media text", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8626107772191366}]}, {"text": "Sentimental analysis of such data presents a unique set of challenges as well; for example, the tweet posts are often short, use informal languages, and are often not linguistically well-formed.", "labels": [], "entities": []}, {"text": "Syntactic analysis such as parsing is much less reliable in such data than in news articles, and sequential models without depending on deep linguistic analysis (e.g., parsing) are adopted by most previous work.", "labels": [], "entities": [{"text": "Syntactic analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7872306406497955}, {"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9414398074150085}]}, {"text": "In obtaining the sentiment of a text span, e.g., a sentence, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-of-phrases models (.", "labels": [], "entities": []}, {"text": "More recent work has started to model composition process, more closely.", "labels": [], "entities": []}, {"text": "In general, the composition process is critical in the formation of the sentiment of a text span, which has not been well modeled yet and more work would be desirable.", "labels": [], "entities": []}, {"text": "In our experiments, we use the official data from the SemEval-2013 ( and) Sentiment Analysis in Twitter challenges.", "labels": [], "entities": []}, {"text": "The task attempts to determine the sentiment category of a tweet; that is, detecting whether an entire tweet message conveys a positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "To give a rough idea about the data, the SemEval-2013 tweets were collected through the public streaming Twitter API during a period of one year: between January 2012 and January 2013.", "labels": [], "entities": []}, {"text": "The dataset is comprised of 5,192 positive and 2,150 negative and 6,383 neutral tweets split into the training (8,258 tweets), development (1,654 tweets), and test (3,813 tweets) sets.", "labels": [], "entities": []}, {"text": "For more details, please refer to ().", "labels": [], "entities": []}, {"text": "In our experiments, we report our results on the official in-domain (tweets) test data but not out-ofdomain (e.g., SMS) test data to better observe the supervised performances of our models but not the domain adaptation performance.", "labels": [], "entities": []}, {"text": "Following the official specification, we use macro-averaged F-score to evaluate the performances.", "labels": [], "entities": [{"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9698932766914368}]}], "tableCaptions": [{"text": " Table 1: Performances of different models in official evaluation", "labels": [], "entities": []}, {"text": " Table 2: Ablation performances (macro-averaged F-scores) of", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993484616279602}, {"text": "F-scores", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9050713777542114}]}]}