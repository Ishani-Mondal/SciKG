{"title": [{"text": "Joint Event Extraction via Recurrent Neural Networks", "labels": [], "entities": [{"text": "Joint Event Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6667091548442841}, {"text": "Recurrent Neural Networks", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.5184108515580496}]}], "abstractContent": [{"text": "Event extraction is a particularly challenging problem in information extraction.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7767452895641327}, {"text": "information extraction", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9162675738334656}]}, {"text": "The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013).", "labels": [], "entities": []}, {"text": "The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words.", "labels": [], "entities": []}, {"text": "The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.717939168214798}]}, {"text": "In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby benefiting from the advantages of the two models as well as addressing issues inherent in the existing approaches.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7919646799564362}]}, {"text": "We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 165, "end_pos": 181, "type": "DATASET", "confidence": 0.9826861619949341}]}], "introductionContent": [{"text": "We address the problem of event extraction (EE): identifying event triggers of specified types and their arguments in text.", "labels": [], "entities": [{"text": "event extraction (EE)", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.890729570388794}]}, {"text": "Triggers are often single verbs or normalizations that evoke some events of interest while arguments are the entities participating into such events.", "labels": [], "entities": [{"text": "Triggers", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9417823553085327}]}, {"text": "This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might be present in various expressions, and an expression might expresses different events in different contexts.", "labels": [], "entities": [{"text": "information extraction in natural language processing (NLP)", "start_pos": 45, "end_pos": 104, "type": "TASK", "confidence": 0.7549303703837924}]}, {"text": "There are two main approaches to EE: (i) the joint approach that predicts event triggers and arguments for sentences simultaneously as a structured prediction problem, and (ii) the pipelined approach that first performs trigger prediction and then identifies arguments in separate stages.", "labels": [], "entities": [{"text": "EE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9549590945243835}]}, {"text": "The most successful joint system for EE () is based on the structured perceptron algorithm with a large set of local and global features . These features are designed to capture the discrete structures that are intuitively helpful for EE using the NLP toolkits (e.g., part of speech tags, dependency and constituent tags).", "labels": [], "entities": []}, {"text": "The advantages of such a joint system are twofold: (i) mitigating the error propagation from the upstream component (trigger identification) to the downstream classifier (argument identification), and (ii) benefiting from the the inter-dependencies among event triggers and argument roles via global features.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence (taken from) in the ACE 2005 dataset: In Baghdad, a cameraman died when an American tank fired on the Palestine hotel.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9779268304506937}]}, {"text": "In this sentence, died and fired are the event triggers for the events of types Die and Attack, respectively.", "labels": [], "entities": []}, {"text": "In the pipelined approach, it is often simple for the argument classifiers to realize that camera-man is the Target argument of the Die event due to the proximity between cameraman and died in the sentence.", "labels": [], "entities": []}, {"text": "However, as cameraman is faraway from fired, the argument classifiers in the pipelined approach might fail to recognize cameraman as the Target argument for the event Attack with their local features.", "labels": [], "entities": []}, {"text": "The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence.", "labels": [], "entities": []}, {"text": "Despite the advantages presented above, the joint system by suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set).", "labels": [], "entities": []}, {"text": "The most successful pipelined system for EE to date) addresses these drawbacks of the joint system by via dynamic multi-pooling convolutional neural networks (DMCNN).", "labels": [], "entities": []}, {"text": "In this system, words are represented by the continuous representations () and features are automatically learnt from data by the DM-CNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset.", "labels": [], "entities": [{"text": "DM-CNN", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.9201831817626953}]}, {"text": "However, as the system by is pipelined, it still suffers from the inherent limitations of error propagation and failure to exploit the inter-dependencies between event triggers and argument roles ().", "labels": [], "entities": []}, {"text": "Finally, we notice that the discrete features, shown to be helpful in the previous studies for EE (, are not considered in.", "labels": [], "entities": [{"text": "EE", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.420621395111084}]}, {"text": "Guided by these characteristics of the EE systems by and, in this work, we propose to solve the EE problem with the joint approach via recurrent neural networks (RNNs)) augmented with the discrete features, thus inheriting all the benefits from both systems as well as overcoming their inherent issues.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to employ neural networks to do joint EE.", "labels": [], "entities": [{"text": "EE", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.6241512894630432}]}, {"text": "Our model involves two RNNs that run over the sentences in both forward and reverse directions to learn a richer representation for the sentences.", "labels": [], "entities": []}, {"text": "This representation is then utilized to predict event triggers and argument roles jointly.", "labels": [], "entities": []}, {"text": "In order to capture the inter-dependencies between triggers and argument roles, we introduce memory vectors/matrices to store the prediction information during the course of labeling the sentences.", "labels": [], "entities": []}, {"text": "We systematically explore various memory vector/matrices as well as different methods to learn word representations for the joint model.", "labels": [], "entities": []}, {"text": "The experimental results show that our system achieves the state-of-the-art performance on the widely used ACE 2005 dataset.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.9756076335906982}]}], "datasetContent": [{"text": "For all the experiments below, in the encoding phase, we use 50 dimensions for the entity type embeddings, 300 dimensions for the word embeddings and 300 units in the hidden layers for the RNNs.", "labels": [], "entities": []}, {"text": "Regarding the prediction phase, we employ the context window of 2 for the local features, and the feed-forward neural networks with one hidden layer for F trg , F arg and F binary (the size of the hidden layers are 600, 600 and 300 respectively).", "labels": [], "entities": []}, {"text": "Finally, for training, we use the mini-batch size = 50 and the parameter for the Frobenius norms = 3.", "labels": [], "entities": []}, {"text": "These parameter values are either inherited from the prior research or selected according to the validation data.", "labels": [], "entities": []}, {"text": "We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit 4 (modified to add the C-CBOW model).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.6919569770495096}]}, {"text": "Following, we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples.", "labels": [], "entities": []}, {"text": "We evaluate the model with the ACE 2005 corpus.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.985361913839976}]}, {"text": "For the purpose of comparison, we use the same data split as the previous work.", "labels": [], "entities": []}, {"text": "This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set.", "labels": [], "entities": []}, {"text": "Also, we follow the criteria of the previous work to judge the correctness of the predicted event mentions.", "labels": [], "entities": []}, {"text": "We investigate different techniques to obtain the pretrained word embeddings for initialization in the joint model of EE.", "labels": [], "entities": []}, {"text": "presents the performance (for both triggers and argument roles) on the development set when the CBOW, SKIP-GRAM and C-CBOW techniques are utilized to obtain word embeddings from the same corpus.", "labels": [], "entities": []}, {"text": "We also report the performance of the joint model when it is initialized with the Word2Vec word embeddings from (trained with the Skip-gram model on Google News) (WORD2VEC).", "labels": [], "entities": [{"text": "Word2Vec word embeddings", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.8803598086039225}]}, {"text": "Finally, for comparison, the performance of the random word embeddings (RANDOM) is also included.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9846696853637695}]}, {"text": "All of these word embeddings are updated during the training of the model.", "labels": [], "entities": []}, {"text": "The first observation from the table is that RAN-DOM is not good enough to initialize the word embeddings for joint EE and we need to borrow some pre-trained word embeddings for this purpose.", "labels": [], "entities": [{"text": "RAN-DOM", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.7953361868858337}]}, {"text": "Second, SKIP-GRAM, WORD2VEC and CBOW have comparable performance on trigger labeling while the argument labeling performance of SKIP-GRAM and WORD2VEC is much better than that of CBOW for the joint EE model.", "labels": [], "entities": []}, {"text": "Third and most importantly, among the compared word embeddings, it is clear that C-CBOW significantly outperforms all the others.", "labels": [], "entities": []}, {"text": "We believe that the better performance of C-CBOW stems from its concatenation of the multiple context word vectors, thus providing more information to learn better word embeddings than SKIP-GRAM and WORD2VEC.", "labels": [], "entities": []}, {"text": "In addition, the concate-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the Memory Vector/Matrices", "labels": [], "entities": []}, {"text": " Table 2: Performance of the Word Embedding Tech-", "labels": [], "entities": []}, {"text": " Table 3: Overall Performance on the Blind Test Data. \" \u2020\" designates the systems that employ the evidences beyond", "labels": [], "entities": [{"text": "Blind Test Data", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.705228348573049}]}, {"text": " Table 4: System Performance on Single Event Sentences", "labels": [], "entities": [{"text": "Single Event Sentences", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.6200611193974813}]}]}