{"title": [{"text": "Frustratingly Easy Cross-Lingual Transfer for Transition-Based Dependency Parsing", "labels": [], "entities": [{"text": "Cross-Lingual Transfer", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7311307489871979}, {"text": "Parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.5506676435470581}]}], "abstractContent": [{"text": "In this paper, we present a straightforward strategy for transferring dependency parsers across languages.", "labels": [], "entities": [{"text": "transferring dependency parsers across languages", "start_pos": 57, "end_pos": 105, "type": "TASK", "confidence": 0.743514460325241}]}, {"text": "The proposed method learns a parser from partially annotated data obtained through the projection of annotations across unambiguous word alignments.", "labels": [], "entities": []}, {"text": "It does not rely on any modeling of the reliability of dependency and/or alignment links and is therefore easy to implement and parameter free.", "labels": [], "entities": []}, {"text": "Experiments on six languages show that our method is at par with recent algorithmi-cally demanding methods, at a much cheaper computational cost.", "labels": [], "entities": []}, {"text": "It can thus serve as a fair baseline for transferring dependencies across languages with the use of parallel corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual learning techniques enable to transfer useful supervision information from well-resourced to under-resourced languages, helping the development of NLP tools fora large number of languages.", "labels": [], "entities": []}, {"text": "In this work, we present a simple method for transferring dependency parsers between languages.", "labels": [], "entities": [{"text": "transferring dependency parsers between languages", "start_pos": 45, "end_pos": 94, "type": "TASK", "confidence": 0.7706751763820648}]}, {"text": "Two main strategies have been considered to transfer syntactic annotations: (a) direct model transfer and (b) annotation transfer.", "labels": [], "entities": [{"text": "direct model transfer", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.6619716584682465}, {"text": "annotation transfer", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7125290185213089}]}, {"text": "The first approach assumes a common representation between the source and target languages (e.g. at the level of PoS tags), which enables to train a model on source data and to use it to parse target sentences.", "labels": [], "entities": [{"text": "parse target sentences", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.8808843294779459}]}, {"text": "The performance of 'pure' delexicalized dependency transfer can be significantly improved using additional techniques such as self-training, smart data selection, relexicalization and/or multi-source model transfer).", "labels": [], "entities": [{"text": "dependency transfer", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.714356005191803}, {"text": "multi-source model transfer", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.6835030317306519}]}, {"text": "The second approach (transfer of annotations) requires parallel sentences, in which word alignments are used to infer target syntactic structures from source dependencies.", "labels": [], "entities": [{"text": "transfer of annotations", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8844550053278605}]}, {"text": "The main difficulty here is to cope with cases of non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments.", "labels": [], "entities": []}, {"text": "Turning source trees into target trees indeed may require to filter poor alignments and to apply various heuristic transformation rules, such as the ones introduced in, later improved in.", "labels": [], "entities": []}, {"text": "In this study, we consider a simple, yet effective approach to transfer annotations, which entirely dispenses from the transfer rules of, the sharp filtering of partially annotated trees, the inclusion of fake root dependencies for unattached words, or the multi-step process of.", "labels": [], "entities": []}, {"text": "Our proposal is, in fact, quite as straightforward (apart from the use of parallel texts) as the delexicalized transfer method of  while achieving performances that surpass this state-of-the-art method by a wide margin, and competing with recent algorithmically costly methods: it globally outperforms the scores of) and even achieves the same performance as (2015) for 1 language out of 5.", "labels": [], "entities": []}, {"text": "It can thus be used as a fair and simple baseline when evaluating new transfer methodologies.", "labels": [], "entities": []}, {"text": "Our method relies on the observation (Section 2) that transition-based dependency parsers using the dynamic oracle strategy can be trained from partially annotated trees (in which some words may not have a governor) using exactly the same algorithm that is used to train from fully annotated tree.", "labels": [], "entities": []}, {"text": "As explained in Section 3, this observation allows us to design a simple transfer strategy that, first, (partially) projects syntactic annotations from a source language onto a target language via unambiguous word alignments and, second, learns a dependency parser from these partially annotated target data.", "labels": [], "entities": []}, {"text": "We then apply this strategy for six language pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first carryout a control experiment on datasets in which dependencies have been artificially removed to show that learning from partially annotated data is possible.", "labels": [], "entities": []}, {"text": "We compare the performance achieved by a parser trained on n% of the sentences of the train set with the performance of a parser trained on the whole train set, but in which only n% of the dependencies of each sentence are known.", "labels": [], "entities": []}, {"text": "In both conditions, the total number of dependencies considered during training is roughly the same.", "labels": [], "entities": []}, {"text": "plots the parsing performance for German, evaluated by the UAS, with respect to the percentage of dependencies that were kept.", "labels": [], "entities": [{"text": "UAS", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8419030904769897}]}, {"text": "To avoid any bias, the reported scores have been averaged over 10 runs.", "labels": [], "entities": []}, {"text": "Similar results are observed for 5 other languages of the Universal Dependency Treebank 2 (UDT).", "labels": [], "entities": [{"text": "Universal Dependency Treebank 2 (UDT)", "start_pos": 58, "end_pos": 95, "type": "DATASET", "confidence": 0.8129126259258815}]}, {"text": "Overall, these results show that learning a parser from partially annotated data is possible.", "labels": [], "entities": []}, {"text": "Two other conclusions can also be drawn.", "labels": [], "entities": []}, {"text": "First, it appears that the number of training examples can be reduced without significantly hurting the performance: removing half the training sentences only reduces the UAS by 1.2 absolute.", "labels": [], "entities": [{"text": "UAS", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.8995879292488098}]}, {"text": "Second, fora similar number of annotations (i.e. number of dependencies known), better results are achieved when more sentences are annotated, even if this annotation is only partial: in, the UAS of a parser trained on partially annotated sentences is higher than the UAS of a parser trained from a subset of the training set.", "labels": [], "entities": [{"text": "UAS", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.9873700141906738}, {"text": "UAS", "start_pos": 268, "end_pos": 271, "type": "METRIC", "confidence": 0.9916183352470398}]}, {"text": "Indeed, in a partial structure, information on unknown dependencies can be inferred from neighbouring dependencies because of the projectivity constraints.", "labels": [], "entities": []}, {"text": "Therefore, the set of gold actions is sometimes smaller than the set of possible actions and an update can happen even if the dependency is unknown.", "labels": [], "entities": []}, {"text": "For instance, when training a German dependency parser, 35,382 updates are performed when only 60% of the dependencies are known, to be compared with the 31,339 updates that take place when training on 60% of the fully annotated sentences.", "labels": [], "entities": []}, {"text": "All our experiments are carried out on six languages 5 of the Universal Dependency Treebank Project: German, English, Spanish, French, Italian and Swedish.", "labels": [], "entities": [{"text": "Universal Dependency Treebank Project", "start_pos": 62, "end_pos": 99, "type": "DATASET", "confidence": 0.7449168786406517}]}, {"text": "We considered as parallel corpora a subset of the Europarl corpus () that have exactly the same English sentences, collecting 1, 231, 216 parallel sentences for the 6 language pairs.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9937874972820282}]}, {"text": "For training the target partial data, we used our own implementation of the arc-eager dependency parser with a dynamic oracle, using the features described in (, with abeam size of 8.", "labels": [], "entities": []}, {"text": "The beam-search strategy is used for training (20 iterations) and decoding.", "labels": [], "entities": []}, {"text": "For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012), trained on the UDT corpus with abeam size of 40.", "labels": [], "entities": [{"text": "UDT corpus", "start_pos": 173, "end_pos": 183, "type": "DATASET", "confidence": 0.9738785028457642}]}, {"text": "Dependencies are then (partially) projected onto the target side of the corpus and filtered using the method described above.", "labels": [], "entities": []}, {"text": "As reported in, after filtering, the number of sentences in the train set varies between 15, 191 for German and 52, 554 for Swedish and the percentage of tokens receiving a dependency varies from 88.15% for French to 90.84% for German.", "labels": [], "entities": []}, {"text": "Our parser is then trained on the resulting partially annotated dataset and its performance evaluated on To account for the root dependency, we consider that both the source and target sentences contain an additional ROOT token that is always aligned.", "labels": [], "entities": []}, {"text": "5 These are the languages that are both in Europarl and UDT.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9908866882324219}, {"text": "UDT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8510074019432068}]}, {"text": "Here are the supervised scores obtained with the MateParser (predicted PoS-tags) on the source languages: 92.4 (en), 80.4 (de), 83.1 (es), 83.8 (fr), 84.2 (it) and 85.7 (sv).", "labels": [], "entities": []}, {"text": "the target UDT test set by the Unlabeled Attachment Score, UAS (excluding punctuation).", "labels": [], "entities": [{"text": "UDT", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.5695193409919739}, {"text": "UAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.7295646071434021}]}, {"text": "Gold PoS were used for evaluating in order to make results of our method comparable with state-of-art methods.", "labels": [], "entities": []}, {"text": "The proposed method is compared to three transfer method baselines: the relexicalisation procedure of, the method of Ma and Xia (2014) for transferring cross-lingual knowledge using entropy regularization, and the recent densitydriven approach of exploiting partially annotated data.", "labels": [], "entities": []}, {"text": "The results are first compared for cross-lingual transfer from English and second, applying a voting method 7 for transferring from multiple sources.", "labels": [], "entities": [{"text": "cross-lingual transfer from English", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.7551786601543427}]}, {"text": "Note, however, that a direct comparison with these results is not completely fair as systems were not trained with the same exact conditions (less features, lower beam size, etc).", "labels": [], "entities": []}, {"text": "As a baseline for comparing parsers, we also report the scores achieved by and by our method on fully projected sentences ('en-100%')..", "labels": [], "entities": []}, {"text": "It therefore appears that, while being much simpler, the proposed approach achieves results very competitive with state-of-the-art methods at a much cheaper computational cost: our results have been obtained by training a single parser with abeam size of 8, while use a parser with exact inference, the training and inference complexity of which is O(n 4 ) and the method of requires the costly training of 4 different parsers each using abeam size of 64.", "labels": [], "entities": [{"text": "O", "start_pos": 349, "end_pos": 350, "type": "METRIC", "confidence": 0.958697497844696}]}], "tableCaptions": [{"text": " Table 2: Number of sentences in projected and filtered target", "labels": [], "entities": []}, {"text": " Table 1: Parsing quality (evaluated in UAS) of our method and previous works: M11 stands for McDonald et al. (2011), MX14 for", "labels": [], "entities": [{"text": "UAS", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.6966570019721985}, {"text": "M11", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9309993982315063}, {"text": "MX14", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.5522516965866089}]}]}