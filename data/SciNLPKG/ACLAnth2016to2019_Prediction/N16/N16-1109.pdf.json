{"title": [{"text": "An Attentional Model for Speech Translation Without Transcription", "labels": [], "entities": [{"text": "Speech Translation Without Transcription", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.8471467792987823}]}], "abstractContent": [{"text": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than transcriptions.", "labels": [], "entities": []}, {"text": "This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages.", "labels": [], "entities": [{"text": "spoken document retrieval", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6370343863964081}]}, {"text": "We experiment with the neural, atten-tional model applied to this data.", "labels": [], "entities": []}, {"text": "On phone-to-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines.", "labels": [], "entities": [{"text": "phone-to-word alignment", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7644234895706177}, {"text": "translation reranking tasks", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.9539383252461752}]}, {"text": "On the more challenging speech-to-word alignment task, our model nearly matches GIZA++'s performance on gold transcriptions, but without recourse to transcriptions or to a lexicon.", "labels": [], "entities": [{"text": "speech-to-word alignment", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7077096253633499}]}], "introductionContent": [{"text": "For many low-resource languages, spoken language resources are more likely to come with translations than with transcriptions.", "labels": [], "entities": []}, {"text": "Most of the world's languages are not written, so there is no orthography for transcription.", "labels": [], "entities": []}, {"text": "Phonetic transcription is possible but too costly to produce at scale.", "labels": [], "entities": [{"text": "Phonetic transcription", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8499668538570404}]}, {"text": "Even when a minority language has an official orthography, people are often only literate in the language of formal education, such as the national language.", "labels": [], "entities": []}, {"text": "Nevertheless, it is relatively easy to provide written or spoken translations for audio sources.", "labels": [], "entities": []}, {"text": "Subtitled or dubbed movies area widespread example.", "labels": [], "entities": []}, {"text": "One application of models of bilingual speech data is documentation of endangered languages.", "labels": [], "entities": []}, {"text": "Since most speakers are bilingual in a higherresource language, they can listen to a source language recording sentence by sentence and provide a spoken translation).", "labels": [], "entities": []}, {"text": "By aligning this data at the word level, we hope to automatically identify regions of data where further evidence is needed, leading to a substantial, interpretable record of the language that can be studied even if the language falls out of use (.", "labels": [], "entities": []}, {"text": "We experiment with extensions of the neural, attentional model of , working at the phone level or directly on the speech signal.", "labels": [], "entities": []}, {"text": "We assume that the target language is a highresource language such as English that can be automatically transcribed; therefore, in our experiments, the target side is text rather than the output of an automatic speech recognition (ASR) system.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 201, "end_pos": 235, "type": "TASK", "confidence": 0.7910635570685068}]}, {"text": "In the first set of experiments, as a steppingstone to direct modeling of speech, we represent the source as a sequence of phones.", "labels": [], "entities": []}, {"text": "For phone-toword alignment, we obtain improvements of 9-24% absolute F1 over several baselines.", "labels": [], "entities": [{"text": "phone-toword alignment", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6301560997962952}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.948091447353363}]}, {"text": "For phone-to-word translation, we use our model to rerank n-best lists from Moses ( and observe improvements in BLEU of 0.9-1.7.", "labels": [], "entities": [{"text": "phone-to-word translation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7238734066486359}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9993963241577148}]}, {"text": "In the second set of experiments, we operate directly on the speech signal, represented as a sequence of Perceptual Linear Prediction (PLP) vectors.", "labels": [], "entities": []}, {"text": "Without using transcriptions or a lexicon, the model is able to align the source-language speech to its English translations nearly as well as GIZA++ using gold transcriptions.", "labels": [], "entities": []}, {"text": "Our main contributions are: (i) proposing anew task, alignment of speech with text translations, including a dataset extending the Spanish Fisher and CALLHOME datasets; (ii) extending the neural, attentional model to outperform existing models at both alignment and translation reranking when working on source-language phones; and (iii) demonstrating the feasibility of alignment directly on source-language speech.", "labels": [], "entities": [{"text": "Spanish Fisher and CALLHOME datasets", "start_pos": 131, "end_pos": 167, "type": "DATASET", "confidence": 0.8254819512367249}, {"text": "translation reranking", "start_pos": 266, "end_pos": 287, "type": "TASK", "confidence": 0.8761622309684753}]}], "datasetContent": [{"text": "We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversations between Spanish native speakers based in the US and their relatives abroad.", "labels": [], "entities": [{"text": "Spanish CALLHOME Corpus (LDC96S35)", "start_pos": 15, "end_pos": 49, "type": "DATASET", "confidence": 0.8107106039921442}]}, {"text": "While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the construction of the \"silver\" standard for evaluation, described below).", "labels": [], "entities": []}, {"text": "We also use the English translations produced by.", "labels": [], "entities": []}, {"text": "We treat the Spanish speech as a sequence of 39-dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal.", "labels": [], "entities": []}, {"text": "We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced \"silver\" standard alignments.", "labels": [], "entities": []}, {"text": "We used a forced aligner) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic) to align the Spanish transcription to the English translation.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9684289693832397}]}, {"text": "We then combined the two alignments to produce \"silver\" standard alignments between the Spanish speech and the English words.", "labels": [], "entities": []}, {"text": "Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing.", "labels": [], "entities": []}, {"text": "For each utterance we have the corresponding English translation, and for each word in the translation we have the corresponding span of Spanish speech.", "labels": [], "entities": []}, {"text": "The forced aligner produces the phonetic sequences that correspond to each utterance, which we use later in our first set of experiments as an intermediate representation for the Spanish speech.", "labels": [], "entities": []}, {"text": "In order to evaluate an automatic alignment between the Spanish speech and English translation against the \"silver\" standard alignment, we compute alignment precision, recall, and F1-score as usual, but on links between Spanish PLP vectors and English words.", "labels": [], "entities": [{"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.7794903516769409}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9992471933364868}, {"text": "F1-score", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9993708729743958}]}, {"text": "In our first set of experiments, we represent the source Spanish speech as a sequence of phones.", "labels": [], "entities": []}, {"text": "This sets an upper bound for our later experiments working directly on speech.", "labels": [], "entities": []}, {"text": "In this section, we represent the source Spanish speech as a sequence of 39 dimensional PLP vectors.", "labels": [], "entities": []}, {"text": "The frame length is 25ms, and overlapping frames are computed every 10ms.", "labels": [], "entities": []}, {"text": "As mentioned in Section 4.1, we used a pyramidal RNN to reduce the speech representation size.", "labels": [], "entities": []}, {"text": "Other than that, the model used here is identical to the first set of experiments.", "labels": [], "entities": []}, {"text": "Using this model directly for translation from speech does not yield useful output, as is to be expected from the small training data, noisy speech data, and an out-of-domain language model.", "labels": [], "entities": []}, {"text": "However, we are able to produce useful results for the ASR and alignment tasks, as presented below.", "labels": [], "entities": [{"text": "ASR", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.994261622428894}, {"text": "alignment", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.9337562918663025}]}, {"text": "17.7: Phone-error-rate (PER) for various models evaluated on TIMIT  To illustrate the utility of our approach to modelling speech input, first, we evaluate on the more common ASR task of phone recognition.", "labels": [], "entities": [{"text": "Phone-error-rate (PER)", "start_pos": 6, "end_pos": 28, "type": "METRIC", "confidence": 0.7929428070783615}, {"text": "TIMIT", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.6683199405670166}, {"text": "ASR task", "start_pos": 175, "end_pos": 183, "type": "TASK", "confidence": 0.9051192402839661}, {"text": "phone recognition", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.7005511522293091}]}, {"text": "This can be considered as a sub-problem of translation, and moreover, this allows us to benchmark our approach against the state-of-the-art in phone recognition.", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9874669909477234}, {"text": "phone recognition", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7668126225471497}]}, {"text": "We experimented on the TIMIT dataset.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.87335404753685}]}, {"text": "Following convention, we removed all the SA sentences, evaluated on the 24 speaker core test set and used the 50 auxiliary speaker development set for early stopping.", "labels": [], "entities": []}, {"text": "The model was trained to recognize 48 phonemes and was mapped to 39 phonemes for testing.", "labels": [], "entities": []}, {"text": "We extracted 39 dimensional PLP features from the TIMIT dataset and trained the same model without any modification.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9381249845027924}]}, {"text": "shows the performance of our model.", "labels": [], "entities": []}, {"text": "It performs reasonably well compared with the state-of-the-art (, considering that we didn't tune any hyper-parameters or feature representations for the task.", "labels": [], "entities": []}, {"text": "Moreover, our model is not designed for the monotonic constraints inherent to the ASR problem, which process the input without reordering.", "labels": [], "entities": [{"text": "ASR problem", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.9333071410655975}]}, {"text": "By simply adding a masking function (equation 2 from Chorowski et al.) to encourage the monotonic constraint in the alignment function, we observe a 2% PER improvement.", "labels": [], "entities": [{"text": "PER", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9986103773117065}]}, {"text": "This is close to the performance reported by), despite the fact that they employed user-adapted speech features.", "labels": [], "entities": []}, {"text": "We use alignment as a second evaluation, training and testing on parallel data comprising paired Spanish speech input with its English translations (as described in \u00a75), and using the speech-based modelling techniques (see \u00a74.)", "labels": [], "entities": []}, {"text": "We compare to a naive baseline where we assume that each English letter (not including spaces) corresponds to an equal number of Spanish frames.", "labels": [], "entities": []}, {"text": "The results of our attentional model and the baseline are summarized in Table 6.", "labels": [], "entities": []}, {"text": "The attentional model is substantially lower than the scores in, because the PLP vector representation is much less informative than the gold phonetic transcription.", "labels": [], "entities": []}, {"text": "Here, we have to identify phones and their boundaries in addition to phoneword alignment.", "labels": [], "entities": [{"text": "phoneword alignment", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7785901427268982}]}, {"text": "However, the naive baseline does surprisingly well, presumably because our (unrealistic) choice of Spanish-English does not have very much reordering.", "labels": [], "entities": []}, {"text": "presents some examples of Spanish speech and English text, showing a heat map of the alignment matrix \u03b1 (before smoothing).", "labels": [], "entities": []}, {"text": "Due to the pyramidal structure of the encoder, each column roughly corresponds to 80ms.", "labels": [], "entities": []}, {"text": "In the example on the left, the model is confident at aligning a little with columns 1-5, which corresponds roughly to their correct Spanish translation algo.", "labels": [], "entities": []}, {"text": "We misalign the word of with columns 8-10, when the correct alignment should be columns 5-6, corresponding Phones sil em ed i ho k e t e i Ba a ya ma r spa ye r o an t ea ye r sp sil Transcription eh , me dijo que te iba a llamar , ayer , o anteayer AM eh , he told me that she was going to call , yesterday before yesterday Giza oh , he told me that you called yesterday or before yesterday . Mod.", "labels": [], "entities": []}, {"text": "Pialign eh , she told me that I was going to call yesterday or before yesterday . Mod.", "labels": [], "entities": []}, {"text": "Pialign + AM eh , he told me that I was going to call , yesterday or before yesterday . Reference eh , he told me that he was going to call you , yesterday , or the day before yesterday . Phones silt en go k ea s er l e e la s e o a s i k om o a u n ha r Di n in fa n ti l sp sil Transcription tengo que hacerle el aseo as\u00ed como a un jard\u00edn infantil -AM I have to have to him like to like that to unkA GIZA I have to do the , the how a vegetable information in the . Mod.", "labels": [], "entities": [{"text": "AM", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9268546104431152}, {"text": "GIZA", "start_pos": 402, "end_pos": 406, "type": "METRIC", "confidence": 0.8327438235282898}, {"text": ". Mod.", "start_pos": 465, "end_pos": 471, "type": "DATASET", "confidence": 0.9457480112711588}]}, {"text": "pialign I have to do the that like to a and it was , didn't you don't have the . Mod.", "labels": [], "entities": []}, {"text": "pialign + AM I have to make the or like to a and it was , didn't you don't have theReference I have to clean it like a kindergarten with modified Pialign aligner (Mod.", "labels": [], "entities": [{"text": "AM", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9592534303665161}]}, {"text": "pialign) and using the attentional model as reranker on top of pialign.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU score on the translation task. Using the", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9697757363319397}, {"text": "translation task", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9193427264690399}]}, {"text": " Table 4: Examples of cross-lingual keyword spotting using the attentional model. The bolded terms in the retrieved text are based", "labels": [], "entities": [{"text": "cross-lingual keyword spotting", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6575850248336792}]}]}