{"title": [{"text": "Fast and Easy Short Answer Grading with High Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9884997010231018}]}], "abstractContent": [{"text": "We present a fast, simple, and high-accuracy short answer grading system.", "labels": [], "entities": []}, {"text": "Given a short-answer question and its correct answer, key measures of the correctness of a student response can be derived from its semantic similarity with the correct answer.", "labels": [], "entities": []}, {"text": "Our supervised model (1) utilizes recent advances in the identification of short-text similarity, and (2) augments text similarity features with key grading-specific constructs.", "labels": [], "entities": [{"text": "identification of short-text similarity", "start_pos": 57, "end_pos": 96, "type": "TASK", "confidence": 0.7854034006595612}]}, {"text": "We present experimental results where our model demonstrates top performance on multiple benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Short-answer questions area useful device for eliciting student understanding of specific concepts in a subject domain.", "labels": [], "entities": []}, {"text": "Numerous automated graders have been proposed for short answers based on their semantic similarity with one or more expert-provided correct answers.", "labels": [], "entities": []}, {"text": "From an application perspective, these systems vary considerably along a set of key dimensions: amount of human effort involved, accuracy, speed, and ease of implementation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9992973804473877}]}, {"text": "We explore a design that seeks to optimize performance along all these dimensions.", "labels": [], "entities": []}, {"text": "Systems developed for the more general task of short-text semantic similarity provide a good starting point for such a design.", "labels": [], "entities": [{"text": "short-text semantic similarity", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6465567350387573}]}, {"text": "Major progress has been made in this task in recent years, due primarily to the SemEval Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity (STS) task", "start_pos": 80, "end_pos": 126, "type": "TASK", "confidence": 0.8731856644153595}]}, {"text": "However, the utility of top STS systems has remained largely unexplored in the context of short answer grading.", "labels": [], "entities": []}, {"text": "We seek to bridge this gap by adopting the feature set of the best performing STS system at.", "labels": [], "entities": []}, {"text": "Besides high accuracy, this system also has a simple design and fast runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9988774657249451}]}, {"text": "Textual similarity alone, however, is inadequate as a measure of answer correctness.", "labels": [], "entities": [{"text": "answer correctness", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8248081803321838}]}, {"text": "For example, while the system makes the general assumption that all content words 1 contribute equally to the meaning of a sentence, domain keywords (e.g., \"mutation\" for biological evolution) are clearly more significant than arbitrary content words (e.g., \"consideration\") for academic text.", "labels": [], "entities": []}, {"text": "As another example, question demoting proposes discarding words that are present in the question text as a preprocessing step for grading.", "labels": [], "entities": [{"text": "question demoting", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.850693941116333}]}, {"text": "We augment our generic text similarity features with such grading-specific measures.", "labels": [], "entities": []}, {"text": "We train supervised models with our final feature set; in two different grading tasks, these models demonstrate significant performance improvement over the state of the art.", "labels": [], "entities": []}, {"text": "In summary, our contribution is a fast, simple, and high-performance short answer grading system which we also release as open-source software at: https://github.", "labels": [], "entities": []}, {"text": "com/ma-sultan/short-answer-grader.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our features on two grading tasks.", "labels": [], "entities": []}, {"text": "The first task, proposed by, asks to compute a real-valued score fora student response on a scale of 0 to 5.", "labels": [], "entities": []}, {"text": "The second task, proposed at SemEval-2013 (, asks to assign a label (e.g., corrector irrelevant) to a student response that shows how appropriate it is as an answer to the question.", "labels": [], "entities": []}, {"text": "Thus from a machine learning perspective, the first is a regression task and the second is a classification task.", "labels": [], "entities": []}, {"text": "We use the NLTK stopwords corpus) to identify function words.", "labels": [], "entities": [{"text": "NLTK stopwords corpus", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.8805532455444336}]}], "tableCaptions": [{"text": " Table 1: Performance on the Mohler et al. (2011) dataset with", "labels": [], "entities": [{"text": "Mohler et al. (2011) dataset", "start_pos": 29, "end_pos": 57, "type": "DATASET", "confidence": 0.5458456538617611}]}, {"text": " Table 2: Performance on the Mohler et al. (2011) dataset with", "labels": [], "entities": [{"text": "Mohler et al. (2011) dataset", "start_pos": 29, "end_pos": 57, "type": "DATASET", "confidence": 0.5475345589220524}]}, {"text": " Table 3: F1 scores on the SemEval-2013 datasets.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993367791175842}, {"text": "SemEval-2013 datasets", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.9313494265079498}]}, {"text": " Table 4: Ablation results on the Mohler et al. (2011) dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9932250380516052}, {"text": "Mohler et al. (2011) dataset", "start_pos": 34, "end_pos": 62, "type": "DATASET", "confidence": 0.661423671990633}]}]}