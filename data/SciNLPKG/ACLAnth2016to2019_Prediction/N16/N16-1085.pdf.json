{"title": [{"text": "Weak Semi-Markov CRFs for NP Chunking in Informal Text", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6097811162471771}]}], "abstractContent": [{"text": "This paper introduces anew annotated corpus based on an existing informal text corpus: the NUS SMS Corpus (Chen and Kan, 2013).", "labels": [], "entities": [{"text": "NUS SMS Corpus (Chen and Kan, 2013)", "start_pos": 91, "end_pos": 126, "type": "DATASET", "confidence": 0.9145604252815247}]}, {"text": "The new corpus includes 76,490 noun phrases from 26,500 SMS messages, annotated by university students.", "labels": [], "entities": []}, {"text": "We then explored several graphical models, including a novel variant of the semi-Markov conditional random fields (semi-CRF) for the task of noun phrase chunk-ing.", "labels": [], "entities": [{"text": "noun phrase chunk-ing", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7089325189590454}]}, {"text": "We demonstrated through empirical evaluations on the new dataset that the new variant yielded similar accuracy but ran in significantly lower running time compared to the conventional semi-CRF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9992731213569641}]}], "introductionContent": [{"text": "Processing user-generated text data is getting more popular recently as away to gather information, such as collecting facts about certain events, gathering and identifying user profiles (, or extracting information in open domain (.", "labels": [], "entities": []}, {"text": "Most recent work focus on the texts generated through Twitter, which, due to the design of Twitter, contain a lot of announcement-like messages mostly intended for general public.", "labels": [], "entities": []}, {"text": "In contrast, SMS was designed as away to communicate short personal messages to a known person, and hence SMS messages tend to be more conversational and more informal compared to tweets.", "labels": [], "entities": []}, {"text": "As conversational texts, SMS data often contains references to named entities such as people and locations relevant to certain events.", "labels": [], "entities": []}, {"text": "Recognizing those Hmm Dr teh says the research presentation should still prepare, butshe's not to sure whether they'd time to present references will be useful for further NLP tasks.", "labels": [], "entities": []}, {"text": "One way to recognize those named entities is to first create a list of candidates, which can be further filtered to get the desired named entities.", "labels": [], "entities": []}, {"text": "Nadeau ( lists several methods that work upon candidates for NER.", "labels": [], "entities": [{"text": "Nadeau", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9259690046310425}, {"text": "NER", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8473861813545227}]}, {"text": "As all named entities are nouns, recognizing noun phrases (NP) is therefore a task that can be potentially useful for further steps in the NLP pipeline to build upon.", "labels": [], "entities": [{"text": "recognizing noun phrases (NP)", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7686531643072764}]}, {"text": "shows an example SMS message within which noun phrases are highlighted.", "labels": [], "entities": []}, {"text": "As can be seen from this example, recognizing the NP information on such a dataset presents some additional challenges over conventional NP recognition tasks.", "labels": [], "entities": [{"text": "NP recognition tasks", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8834474484125773}]}, {"text": "Specifically, the texts are highly informal and noisy, with misspelling errors and without grammatical structures.", "labels": [], "entities": []}, {"text": "The correct casing and punctuation information is often missing.", "labels": [], "entities": []}, {"text": "The lack of spaces between adjacent words makes the detection of NP boundaries more challenging.", "labels": [], "entities": []}, {"text": "Furthermore, the lack of available annotated data for such informal datasets prevents researchers from understanding what effective models can be used to resolve the above issues.", "labels": [], "entities": []}, {"text": "In this work, we focus on tackling these issues while making the following two main contributions: \u2022 We build anew corpus of SMS data that is fully annotated with noun phrase information.", "labels": [], "entities": []}, {"text": "\u2022 We propose and build anew variant of semiMarkov CRF () for the task of NP chunking on our corpus, which is faster and yields a performance similar to the conventional semi-Markov CRF models.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.8444135189056396}]}], "datasetContent": [{"text": "All models were built by us using Java, and were optimized with L-BFGS.", "labels": [], "entities": []}, {"text": "Models are all tuned in the development set for optimal \u03bb.", "labels": [], "entities": []}, {"text": "The optimal \u03bb values are noted in.", "labels": [], "entities": []}, {"text": "Since the models that we consider are all wordbased , we tokenize the corpus using a regex-based tokenizer similar to the wordpunct_tokenize function in Python NLTK package.", "labels": [], "entities": []}, {"text": "We also included some rules to consider special anonymization tokens in the SMS dataset.", "labels": [], "entities": [{"text": "SMS dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8570632934570312}]}, {"text": "The gold character spans are converted into word labels in BIO format, reducing or extending the character spans as necessary to the closest word boundaries.", "labels": [], "entities": []}, {"text": "The converted annotations are regarded as gold word spans.", "labels": [], "entities": []}, {"text": "Note that this conversion is lossy due to the presence of improper NPs, which makes it impossible for the converted format to represent the original gold standard.", "labels": [], "entities": []}, {"text": "We evaluated the models in the original characterlevel spans and also in the converted word-level spans, to seethe impact of the lossy conversion on the scores.", "labels": [], "entities": []}, {"text": "In character-level evaluation, the system output is converted back into character boundaries and compared with the original gold standard, while in the word-level evaluation, the system output is compared directly with the gold word spans.", "labels": [], "entities": []}, {"text": "For this reason, we anticipate that the scores in word-level evaluation will be higher than in the character-level evaluation.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The scores for \"Gold\" in the character-level evaluation mark the upperbound of word-based models due to the presence of improper NPs.", "labels": [], "entities": [{"text": "Gold", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.978611409664154}]}, {"text": "The average time per training iteration on the base models is 1.311s, 2.072s, and 1.811s respectively for Linear CRF, Semi-CRF, and Weak Semi-CRF.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of messages, NPs, number of improper NPs", "labels": [], "entities": []}, {"text": " Table 2: Tuned regularization parameter \u03bb from the set {0.125,", "labels": [], "entities": []}, {"text": " Table 3: Scores on test set (both character-level and word-level  evaluation) using optimal \u03bb. +a, +b, and +s refer to the affix,  Brown cluster, and word shape features respectively. Best F1  scores are underlined, and values which are not significantly  different in 95% confidence interval are in bold", "labels": [], "entities": [{"text": "F1", "start_pos": 190, "end_pos": 192, "type": "METRIC", "confidence": 0.9959670305252075}]}]}