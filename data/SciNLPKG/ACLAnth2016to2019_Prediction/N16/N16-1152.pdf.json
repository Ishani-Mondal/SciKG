{"title": [{"text": "Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking", "labels": [], "entities": [{"text": "Answer Sentence Reranking", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.822359025478363}]}], "abstractContent": [{"text": "In this paper, we study, compare and combine two state-of-the-art approaches to automatic feature engineering: Convolution Tree Kernels (CTKs) and Convolutional Neural Networks (CNNs) for learning to rank answer sentences in a Question Answering (QA) setting.", "labels": [], "entities": [{"text": "automatic feature engineering", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6423257688681284}, {"text": "learning to rank answer sentences in a Question Answering (QA) setting", "start_pos": 188, "end_pos": 258, "type": "TASK", "confidence": 0.6994805221374218}]}, {"text": "When dealing with QA, the key aspect is to encode relational information between the constituents of question and answer in learning algorithms.", "labels": [], "entities": []}, {"text": "For this purpose, we propose novel CNNs using relational information and combined them with relational CTKs.", "labels": [], "entities": []}, {"text": "The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8270303606987}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9966542720794678}]}], "introductionContent": [{"text": "The increasing use of machine learning for the design of NLP applications pushes for fast methods for feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8420170843601227}]}, {"text": "In contrast, the latter typically requires considerable effort especially when dealing with highly semantic tasks such as QA.", "labels": [], "entities": []}, {"text": "For example, for an effective design of automated QA systems, the question text needs to be put in relation with the text passages retrieved from a document collection to enable an accurate extraction of the correct answers from passages.", "labels": [], "entities": [{"text": "QA", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9154329299926758}]}, {"text": "From a machine learning perspective, encoding the information above consists in manually defining expressive rules and features based on syntactic and semantic patterns.", "labels": [], "entities": []}, {"text": "Therefore, methods for automatizing feature engineering are remarkably important also in the light of fast prototyping of commercial applications.", "labels": [], "entities": [{"text": "automatizing feature engineering", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6539254387219747}]}, {"text": "To the best of our knowledge, two of the most effective methods for engineering features are: (i) kernel methods, which naturally map feature vectors or directly objects in richer feature spaces; and more recently (ii) approaches based on deep learning, which have been shown to be very effective.", "labels": [], "entities": []}, {"text": "Regarding the former, in, we firstly used CTKs in Support Vector Machines (SVMs) to generate features from a question (Q) and their candidate answer passages (AP).", "labels": [], "entities": [{"text": "candidate answer passages (AP)", "start_pos": 132, "end_pos": 162, "type": "METRIC", "confidence": 0.581082284450531}]}, {"text": "CTKs enable SVMs to learn in the space of convolutional subtrees of syntactic and semantic trees used for representing Q and AP.", "labels": [], "entities": []}, {"text": "This automatically engineers syntactic/semantic features.", "labels": [], "entities": []}, {"text": "One important characteristic we added in) is the use of relational links between Q and AP, which basically merged the two syntactic trees in a relational graph (containing relational features).", "labels": [], "entities": []}, {"text": "Although based on different principles, also CNNs can generate powerful features, e.g., see.", "labels": [], "entities": []}, {"text": "CNNs can effectively capture the compositional process of mapping the meaning of individual words in a sentence to a continuous representation of the sentence.", "labels": [], "entities": []}, {"text": "This way CNNs can efficiently learn to embed input sentences into low-dimensional vector space, preserving important syntactic and semantic aspects of the input sentence.", "labels": [], "entities": []}, {"text": "However, engineering features spanning two pieces of text such as in QA is a more complex task than classifying single sentences.", "labels": [], "entities": []}, {"text": "Indeed, only very recently, CNNs were proposed for QA by.", "labels": [], "entities": []}, {"text": "Although, such network achieved high accuracy, its design is still not enough to model relational features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9976413249969482}]}, {"text": "In this paper, we aim at comparing the ability of CTKs and CNNs of generating features for QA.", "labels": [], "entities": []}, {"text": "For this purpose, we first explore CTKs applied to shallow linguistic structures for automatically learning classification and ranking functions with SVMs.", "labels": [], "entities": []}, {"text": "At the same time, we assess a novel deep learning architecture for effectively modeling Q and AP pairs generating relational features we initially modeled in.", "labels": [], "entities": []}, {"text": "The main building blocks of our approach are two sentence models based on CNNs.", "labels": [], "entities": []}, {"text": "These work in parallel, mapping questions and answer sentences to fixed size vectors, which are then used to learn the semantic similarity between them.", "labels": [], "entities": []}, {"text": "To compute question-answer similarity score we adopt the approach used by.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.9261946082115173}]}, {"text": "Our main novelty is the way we model relational information: we inject overlapping words directly into the word embeddings as additional dimensions.", "labels": [], "entities": []}, {"text": "The augmented word representation is then passed through the layers of the convolutional feature extractors, which encode the relatedness between Q and AP pairs in a more structured manner.", "labels": [], "entities": []}, {"text": "Moreover, the embedding dimensions encoding overlapping words are parameters of the network and are tuned during training.", "labels": [], "entities": []}, {"text": "We experiment with two different QA benchmarks for sentence reranking TREC13 () and WikiQA (.", "labels": [], "entities": [{"text": "sentence reranking TREC13", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6963253815968832}, {"text": "WikiQA", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.9212764501571655}]}, {"text": "We compare CTKs and CNNs and then we also combine them.", "labels": [], "entities": [{"text": "CTKs", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8792615532875061}]}, {"text": "For this purpose, we design anew kernel that sum together CTKs and different embeddings extracted from different CNN layers.", "labels": [], "entities": []}, {"text": "Our CTK-based models achieve the state of the art on TREC 13, obtaining an MRR of 85.53 and an MAP of 75.18 largely outperforming all the previous best results.", "labels": [], "entities": [{"text": "TREC 13", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8257162272930145}, {"text": "MRR", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9990487694740295}, {"text": "MAP", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9994039535522461}]}, {"text": "On WikiQA, our CNNs perform almost on par with tree kernels, i.e., an MRR of 71.07 vs. 72.51 of CTK, which again is the current state of the art on such data.", "labels": [], "entities": [{"text": "MRR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9971330165863037}]}, {"text": "The combination between CTK and CNNs produces a further boost, achieving an MRR of 75.52 and an MAP of 73.99, confirming that the research line of combining these two interesting machine learning methods is very promising.", "labels": [], "entities": [{"text": "MRR", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9990054965019226}, {"text": "MAP", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9993886947631836}]}], "datasetContent": [{"text": "In these experiments, we compare the impact inaccuracy of two main methods for automatic feature engineering, i.e., CTKs and CNNs, for relational learning, using two different answer sentence selection datasets, WikiQA and TREC13.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 212, "end_pos": 218, "type": "DATASET", "confidence": 0.944247841835022}, {"text": "TREC13", "start_pos": 223, "end_pos": 229, "type": "DATASET", "confidence": 0.8116153478622437}]}, {"text": "We propose several strategies to combine CNNs with CTKs and we show that the two approaches are complementary as their joint use significantly boosts both models.", "labels": [], "entities": []}, {"text": "We utilized two datasets for testing our models: TREC13.", "labels": [], "entities": [{"text": "TREC13", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.6243100166320801}]}, {"text": "This is the factoid open-domain TREC QA corpus prepared by ().", "labels": [], "entities": [{"text": "TREC QA corpus", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.6064813733100891}]}, {"text": "The training data was assembled from the 1,229 TREC8-12 questions.", "labels": [], "entities": [{"text": "TREC8-12 questions", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.6590051501989365}]}, {"text": "The answers for the training questions were automatically marked in sentences by applying regular expressions, therefore the dataset can be noisy.", "labels": [], "entities": []}, {"text": "The test data contains 68 questions, whose answers were manually annotated.", "labels": [], "entities": []}, {"text": "We used 10 answer passages for each question for training our classifiers and all the answer passages available for each question for testing.", "labels": [], "entities": []}, {"text": "TREC13 is a small dataset with an even smaller test set, which makes the system evaluation rather unstable, i.e., a small difference in parameters and models can produce very different results.", "labels": [], "entities": [{"text": "TREC13", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5432190299034119}]}, {"text": "Moreover, as pointed by), it has significant lexical overlap between questions and answer candidates, therefore simple lexical match models may likely outperform more elaborate methods if trained and tested on it.", "labels": [], "entities": []}, {"text": "WikiQA dataset () is a larger dataset, created for open domain QA, which overcomes these problems.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9501716196537018}]}, {"text": "Its questions were sampled from the Bing query logs and candidate answers were extracted from the summary paragraphs of the associated Wikipedia pages.", "labels": [], "entities": [{"text": "Bing query logs", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.889504055182139}]}, {"text": "The train, test, and development sets contain 2,118, 633 and 296 questions, respectively.", "labels": [], "entities": []}, {"text": "There is no correct answer sentence for 1,245 training, 170 development and 390 test questions.", "labels": [], "entities": []}, {"text": "Consistently with (, we remove the questions without answers for our evaluations.", "labels": [], "entities": []}, {"text": "We used the Illinois chunker, question class and focus classifiers trained as in  and the Stanford CoreNLP () toolkit for the needed preprocessing.", "labels": [], "entities": [{"text": "Illinois chunker", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8465119004249573}, {"text": "Stanford CoreNLP () toolkit", "start_pos": 90, "end_pos": 117, "type": "DATASET", "confidence": 0.9122539162635803}]}, {"text": "We used SVM-light-TK 2 to train our models.", "labels": [], "entities": [{"text": "SVM-light-TK", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8918620347976685}]}, {"text": "The toolkit enables the use of structural kernels) in SVM-light.", "labels": [], "entities": []}, {"text": "We applied (i) the partial tree kernel (PTK) with its default parameters to all our structures and (ii) the polynomial kernel of degree 3 on all feature vectors we generate.", "labels": [], "entities": []}, {"text": "We used the scikit 3 logistic regression classifier implementation to train the metaclassifier on the outputs of CTKs and CNNs.", "labels": [], "entities": []}, {"text": "We pre-initialize the word embeddings by running the word2vec tool ( on the English Wikipedia dump and the jacana corpus as in ( words with frequency less than 5.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.7838154435157776}, {"text": "jacana corpus", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.8908790946006775}]}, {"text": "The dimensionality of the embeddings is set to 50.", "labels": [], "entities": []}, {"text": "The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings.", "labels": [], "entities": []}, {"text": "We use a single non-linear hidden layer (with hyperbolic tangent activation, Tanh), whose size is equal to the size of the previous layer.", "labels": [], "entities": []}, {"text": "The network is trained using SGD with shuffled mini-batches using the Adam update rule).", "labels": [], "entities": []}, {"text": "The batch size is set to 100 examples.", "labels": [], "entities": []}, {"text": "The network is trained fora fixed number of epochs (i.e., 3) for all the experiments.", "labels": [], "entities": []}, {"text": "We decided to avoid using early stopping, in order to do not overfit the development set and have a fair comparison with the CTKs models.", "labels": [], "entities": []}, {"text": "We used common QA metrics: Precision at rank 1 (P@1), i.e., the percentage of questions with a correct answer ranked at the first position, the Mean Reciprocal Rank (MRR) and the Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9603977799415588}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 144, "end_pos": 170, "type": "METRIC", "confidence": 0.9525809089342753}, {"text": "Mean Average Precision (MAP)", "start_pos": 179, "end_pos": 207, "type": "METRIC", "confidence": 0.9717592298984528}]}, {"text": "reports the results obtained on the WikiQA test set by state-of-the-art systems (lines 1-4) and our models, when removing the questions with no correct answers (this to be aligned with previous work).", "labels": [], "entities": [{"text": "WikiQA test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9501072565714518}]}, {"text": "More in detail: CNN c is the Convolutional Neural Network with word count, n/a n/a n/a CH, VJE 73.95 72.15 62.14 n/a n/a n/a CH+VJE, CNNR 73.43 71.58 60.49 n/a n/a n/a: Performance on the WikiQA on the development set ABCNN is the Attention-Based CNN, LSTM a,c is the long short-term memory network with attention and word count, and NASM c is the neural answer selection model with word count.", "labels": [], "entities": [{"text": "VJE 73.95 72.15", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8649332324663798}, {"text": "CNNR 73.43 71.58", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.9325593312581381}]}, {"text": "CNN R is the relational CNN described in Section 4.", "labels": [], "entities": [{"text": "CNN R", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.644639641046524}]}, {"text": "CH 4 is a tree kernel-based SVM reranker trained on the shallow pos-chunk tree representations of question and answer sentences (Sec.", "labels": [], "entities": []}, {"text": "3.1), where the subscript coarse refers to the model with the coarsegrained question classes as in.", "labels": [], "entities": []}, {"text": "V is a polynomial SVM reranker, where the subscripts AE, QE, JE indicate the use of the answer, question or joint embeddings (see Sec.", "labels": [], "entities": [{"text": "AE", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9588872194290161}, {"text": "JE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.6784813404083252}]}, {"text": "4.1) as the feature vector of SVM and + means that two embeddings were concatenated into a single vector.", "labels": [], "entities": []}, {"text": "The results show that our CNN R model performs comparably to, which is the most recent and accurate NN model and to CH coarse . The performance drops when the embeddings AE, QE and JE are used in a polynomial SVM reranker.", "labels": [], "entities": [{"text": "AE", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.9736499786376953}, {"text": "QE", "start_pos": 174, "end_pos": 176, "type": "METRIC", "confidence": 0.7793288826942444}, {"text": "JE", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.7058184742927551}]}, {"text": "In contrast, CH (using our tree structure enriched with fine-grained categories) outperforms all the models, showing the importance of syntactic relational information for the answer sentence selection task.", "labels": [], "entities": [{"text": "answer sentence selection task", "start_pos": 176, "end_pos": 206, "type": "TASK", "confidence": 0.7706142365932465}]}, {"text": "TREC13 corpus has been used for evaluation in a number of works starting from 2007.", "labels": [], "entities": [{"text": "TREC13 corpus", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7283301502466202}]}, {"text": "reports our as well as some state-of-the-art system results on TREC13.", "labels": [], "entities": [{"text": "TREC13", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.6543872356414795}]}, {"text": "It should be noted that, to be consistent with the previous work, we evaluated our models in the same setting as (), i.e., we (i) remove the questions having only corrector only incorrect answer sentence candidates and (ii) used the same evaluation script and the gold judgment file as they used.", "labels": [], "entities": []}, {"text": "As pointed out by Footnote 7 in (), the evaluation script always considers 4 questions to be answered incorrectly thus penalizing the overall system score.", "labels": [], "entities": [{"text": "Footnote", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9286730289459229}]}, {"text": "We note that our models, i.e., CNN R , V JE ,", "labels": [], "entities": [{"text": "CNN R", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.7110458016395569}, {"text": "JE", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.47649869322776794}]}], "tableCaptions": [{"text": " Table 1: Performance on the WikiQA dataset", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9444514214992523}]}, {"text": " Table 2: Performance on the WikiQA using the development set or half of the training set for training", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9316108822822571}]}, {"text": " Table 3: Performance on the WikiQA on the development set", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.8726403713226318}]}, {"text": " Table 4: Results on the TREC13, answer selection task.", "labels": [], "entities": [{"text": "TREC13", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.3575001060962677}, {"text": "answer selection task", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.913336714108785}]}]}