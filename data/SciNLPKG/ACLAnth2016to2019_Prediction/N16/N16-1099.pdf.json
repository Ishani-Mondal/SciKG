{"title": [{"text": "Dynamic Entity Representation with Max-pooling Improves Machine Reading", "labels": [], "entities": [{"text": "Dynamic Entity Representation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.773832897345225}, {"text": "Machine Reading", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7734778821468353}]}], "abstractContent": [{"text": "We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7108447253704071}]}, {"text": "Evaluated on a recent large scale dataset (Her-mann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for model-ing the accumulation of information on entities.", "labels": [], "entities": []}, {"text": "Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions.", "labels": [], "entities": []}, {"text": "Our code for the model is available at https://github.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine reading systems ( can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries.", "labels": [], "entities": []}, {"text": "Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (, by replacing named entities in the summaries with placeholders to form Cloze) style questions ().", "labels": [], "entities": []}, {"text": "These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers (.", "labels": [], "entities": []}, {"text": "( @entity1 ) @entity0 maybe @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a \u2026!", "labels": [], "entities": []}, {"text": "\" [X] \" star @entity0 presents a young child with a bionic arm!: A document-query-answer triple constructed from a news article and its bullet point summary.", "labels": [], "entities": []}, {"text": "An entity in the summary (Robert Downey Jr.) is replaced by the placeholder [X] to form a query.", "labels": [], "entities": []}, {"text": "All entities are anonymized to exclude world knowledge and focus on reading comprehension.", "labels": [], "entities": []}, {"text": "In this paper, we hypothesize that a reader without world knowledge can only understand a named entity by dynamically constructing its meaning from the contexts.", "labels": [], "entities": []}, {"text": "For example, in, a reader reading the sentence \"Robert Downey Jr. maybe Iron Man . .", "labels": [], "entities": []}, {"text": "\" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori.", "labels": [], "entities": []}, {"text": "Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . .", "labels": [], "entities": []}, {"text": "\". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences.", "labels": [], "entities": []}, {"text": "This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities ().", "labels": [], "entities": []}, {"text": "We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2).", "labels": [], "entities": []}, {"text": "Evaluation of our model, DER Network, exhibits better results than previous research (Section 3).", "labels": [], "entities": [{"text": "DER Network", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.6635790467262268}]}, {"text": "In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance.", "labels": [], "entities": []}, {"text": "Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the CNN-QA dataset () for evaluating our model's ability to answer questions about named entities.", "labels": [], "entities": [{"text": "CNN-QA dataset", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.9660234153270721}]}, {"text": "The dataset consists of (D, q, e)-triples, where the document Dis taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document.", "labels": [], "entities": []}, {"text": "The training set has 90k articles and 380k queries, and both validation and test sets have 1k articles and 3k queries.", "labels": [], "entities": []}, {"text": "An average article has about 25 entities and 700 word tokens.", "labels": [], "entities": []}, {"text": "One trains a machine reading system on the data by maximizing likelihood of correct answers.", "labels": [], "entities": []}, {"text": "We use Chainer 5 () to implement our model 6 . Experimental Settings Named entities in CNN-QA are already recognized.", "labels": [], "entities": [{"text": "CNN-QA", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9232419729232788}]}, {"text": "For preprocessing, we segment sentences at punctuation marks \".\", \"!\", and \"?\".", "labels": [], "entities": []}, {"text": "We train our model 8 with hyper-parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model., Max-pooling described in Section 2.2 drastically improves performance, showing the effect of accumulating information on entities.", "labels": [], "entities": []}, {"text": "Another technique, called \"Byway\", is based on the observation that the attention mechanism (5) must always promote some entity occurrences (since all weights sum to 1), which could be difficult if the entity does not answer the query.", "labels": [], "entities": []}, {"text": "To counter this, we make an artificial occurrence for each entity with no contexts, which serves as a byway to attend when no other occurrences can be reasonably related to the query.", "labels": [], "entities": []}, {"text": "This simple trick shows http://chainer.org/ 6 The implementation is available at https://github.", "labels": [], "entities": []}, {"text": "com/soskek/der-network.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on CNN-QA dataset. Results  marked by  *  are cited from Hermann et al. (2015)  and  *  *  from Hill et al. (2015).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967842102050781}, {"text": "CNN-QA dataset", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.9748145639896393}]}]}