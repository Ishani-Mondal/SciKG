{"title": [{"text": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs).", "labels": [], "entities": [{"text": "noise-contrastive estimation (NCE)", "start_pos": 72, "end_pos": 106, "type": "METRIC", "confidence": 0.7804521918296814}]}, {"text": "Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al.", "labels": [], "entities": [{"text": "One Billion Word Benchmark language modeling challenge", "start_pos": 78, "end_pos": 132, "type": "TASK", "confidence": 0.5579693530287061}]}, {"text": "When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality.", "labels": [], "entities": [{"text": "Arabic-English machine translation", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.6739710966746012}]}, {"text": "We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, paralleliz-ing computation across multiple GPUs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models are used to compute probabilities of sequences of words.", "labels": [], "entities": []}, {"text": "They are crucial for good performance in tasks like machine translation, speech recognition, and spelling correction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8186345100402832}, {"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7961203157901764}, {"text": "spelling correction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9538366198539734}]}, {"text": "They can be classified into two categories: count-based and continuous-space language models.", "labels": [], "entities": []}, {"text": "The language modeling literature abounds with successful approaches for learning-count based language models: modified Kneser-Ney smoothing, JelinekMercer smoothing, etc.", "labels": [], "entities": []}, {"text": "In recent years, continuousspace language models such as feed-forward neural probabilistic language models (NPLMs) and recurrent neural network language models (RNNs) 1 * Equal contribution.", "labels": [], "entities": []}, {"text": "1 Henceforth we will use terms like \"RNN\" and \"LSTM\" with the understanding that we are referring to language models that use these formalisms have outperformed their count-based counterparts (.", "labels": [], "entities": []}, {"text": "RNNs are more powerful than n-gram language models, as they can exploit longer word contexts to predict words.", "labels": [], "entities": []}, {"text": "Long short-term memory language models (LSTMs) area class of RNNs that have been designed to model long histories and are easier to train than standard RNNs.", "labels": [], "entities": []}, {"text": "LSTMs are currently the best performing language models on the Penn Treebank (PTB) dataset ().", "labels": [], "entities": [{"text": "Penn Treebank (PTB) dataset", "start_pos": 63, "end_pos": 90, "type": "DATASET", "confidence": 0.9761430621147156}]}, {"text": "The most common method for training LSTMs, maximum likelihood estimation (MLE), is prohibitively expensive for large vocabularies, as it involves time-intensive matrix-matrix multiplications.", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE)", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.654289777080218}]}, {"text": "Noise-contrastive estimation (NCE) has been a successful alternative to train continuous space language models with large vocabularies.", "labels": [], "entities": [{"text": "Noise-contrastive estimation (NCE)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7507306575775147}]}, {"text": "However, NCE in its standard form is not suitable for GPUs, as the computations are not amenable to dense matrix operations.", "labels": [], "entities": []}, {"text": "In this paper, we present a natural modification to the NCE objective function for language modeling that allows a very efficient GPU implementation.", "labels": [], "entities": [{"text": "NCE objective function", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8573315938313802}, {"text": "language modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7197780311107635}]}, {"text": "Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark (, with its full 780k word vocabulary.", "labels": [], "entities": []}, {"text": "We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model ().", "labels": [], "entities": []}, {"text": "We release our toolkit 2 to allow researchers to train large-scale, large-vocabulary LSTMs with NCE.", "labels": [], "entities": [{"text": "NCE", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9578432440757751}]}, {"text": "The contributions in this paper are the following: \u2022 A fast and simple approach for handling large vocabularies effectively on the GPU.", "labels": [], "entities": [{"text": "GPU", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.9454615712165833}]}, {"text": "\u2022 Significantly improved perplexities (43.2) on the One Billion Word benchmark over \u2022 Extrinsic machine translation improvement over a strong baseline.", "labels": [], "entities": [{"text": "One Billion Word benchmark", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.6133898198604584}, {"text": "Extrinsic machine translation", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.6303462485472361}]}, {"text": "\u2022 Fast decoding times because in practice there is no need to normalize.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two series of experiments to validate the efficiency of our approach and the quality of the models we learned using it: An intrinsic study of language model perplexity using the standard One Billion Word benchmark ( and an extrinsic end-to-end statistical machine translation task that uses an LSTM as one of several feature functions in re-ranking.", "labels": [], "entities": [{"text": "statistical machine translation task", "start_pos": 257, "end_pos": 293, "type": "TASK", "confidence": 0.714952290058136}]}, {"text": "Both experiments achieve excellent results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our NCE-based language model successfully re-ranks", "labels": [], "entities": []}]}