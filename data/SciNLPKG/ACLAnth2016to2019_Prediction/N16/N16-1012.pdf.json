{"title": [{"text": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks", "labels": [], "entities": [{"text": "Abstractive Sentence Summarization", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6708919405937195}]}], "abstractContent": [{"text": "ive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning.", "labels": [], "entities": [{"text": "Sentence Summarization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9156702160835266}]}, {"text": "We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence.", "labels": [], "entities": []}, {"text": "The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation.", "labels": [], "entities": []}, {"text": "Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets.", "labels": [], "entities": []}, {"text": "Our experiments show that the model significantly outperforms the recently proposed state-of-the-art method on the Giga-word corpus while performing competitively on the DUC-2004 shared task.", "labels": [], "entities": [{"text": "Giga-word corpus", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.8145611584186554}, {"text": "DUC-2004 shared task", "start_pos": 170, "end_pos": 190, "type": "DATASET", "confidence": 0.7931146423021952}]}], "introductionContent": [{"text": "Generating a condensed version of a passage while preserving its meaning is known as text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.6444849967956543}]}, {"text": "Tackling this task is an important step towards natural language understanding.", "labels": [], "entities": [{"text": "Tackling", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9843680262565613}, {"text": "natural language understanding", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6812052627404531}]}, {"text": "Summarization systems can be broadly classified into two categories.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9757933020591736}]}, {"text": "Extractive models generate summaries by cropping important segments from the original text and putting them together to form a coherent summary.", "labels": [], "entities": []}, {"text": "Abstractive models generate summaries from scratch without being constrained to reuse phrases from the original text.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel recurrent neural network for the problem of abstractive sentence summarization.", "labels": [], "entities": [{"text": "abstractive sentence summarization", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.6595351298650106}]}, {"text": "Inspired by the recently proposed architectures for machine translation ( ), our model consists of a conditional recurrent neural network, which acts as a decoder to generate the summary of an input sentence, much like a standard recurrent language model.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7878397107124329}]}, {"text": "In addition, at every time step the decoder also takes a conditioning input which is the output of an encoder module.", "labels": [], "entities": []}, {"text": "Depending on the current state of the RNN, the encoder computes scores over the words in the input sentence.", "labels": [], "entities": []}, {"text": "These scores can be interpreted as a soft alignment over the input text, informing the decoder which part of the input sentence it should focus onto generate the next word.", "labels": [], "entities": []}, {"text": "Both the decoder and encoder are jointly trained on a data set consisting of sentence-summary pairs.", "labels": [], "entities": []}, {"text": "Our model can be seen as an extension of the recently proposed model for the same problem by.", "labels": [], "entities": []}, {"text": "While they use a feed-forward neural language model for generation, we use a recurrent neural network.", "labels": [], "entities": []}, {"text": "Furthermore, our encoder is more sophisticated, in that it explicitly encodes the position information of the input words.", "labels": [], "entities": []}, {"text": "Lastly, our encoder uses a convolutional network to encode input words.", "labels": [], "entities": []}, {"text": "These extensions result in improved performance.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is a novel convolutional attention-based conditional recurrent neural network model for the problem of abstractive sentence summarization.", "labels": [], "entities": [{"text": "abstractive sentence summarization", "start_pos": 139, "end_pos": 173, "type": "TASK", "confidence": 0.6536215841770172}]}, {"text": "Empirically we show that our model beats the state-of-the-art systems of on multiple data sets.", "labels": [], "entities": []}, {"text": "Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task.", "labels": [], "entities": [{"text": "Gigaword data set", "start_pos": 197, "end_pos": 214, "type": "DATASET", "confidence": 0.9670131405194601}, {"text": "DUC-2004 task", "start_pos": 240, "end_pos": 253, "type": "DATASET", "confidence": 0.8700548410415649}]}], "datasetContent": [{"text": "Our models are trained on the annotated version of the Gigaword corpus () and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9500831663608551}, {"text": "tokenization", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.9634336233139038}, {"text": "sentence separation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.6966629475355148}]}, {"text": "We pair the first sentence of each article with its headline to form sentence-summary pairs.", "labels": [], "entities": []}, {"text": "The data is pre-processed in the same way as and we use the same splits for training, validation, and testing.", "labels": [], "entities": [{"text": "validation", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.9593824744224548}]}, {"text": "For Gigaword we report results on the same randomly held-out test set of 2000 sentence-summary pairs as (.", "labels": [], "entities": []}, {"text": "We also evaluate our models on the DUC-2004 evaluation data set comprising 500 pairs.", "labels": [], "entities": [{"text": "DUC-2004 evaluation data set", "start_pos": 35, "end_pos": 63, "type": "DATASET", "confidence": 0.9583516865968704}]}, {"text": "Our evaluation is based on three variants of ROUGE), namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.842812180519104}, {"text": "ROUGE-1", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9512077569961548}, {"text": "ROUGE-2", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9076562523841858}, {"text": "ROUGE-L", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9518297910690308}]}], "tableCaptions": [{"text": " Table 1: Perplexity on the Gigaword validation set. Bag-of-", "labels": [], "entities": [{"text": "Gigaword validation set", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.795699675877889}]}, {"text": " Table 2: F1 ROUGE scores on the Gigaword test set. ABS and", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9958643913269043}, {"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7254495620727539}, {"text": "Gigaword test set", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9627645611763}, {"text": "ABS", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9017965793609619}]}, {"text": " Table 3: ROUGE results (recall-only) on the DUC-2004 test", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9972332119941711}, {"text": "recall-only", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9964380264282227}, {"text": "DUC-2004 test", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9515986740589142}]}]}