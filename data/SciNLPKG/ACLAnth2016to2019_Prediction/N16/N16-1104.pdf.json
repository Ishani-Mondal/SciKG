{"title": [{"text": "Effective Crowd Annotation for Relation Extraction", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9744820594787598}]}], "abstractContent": [{"text": "Can crowdsourced annotation of training data boost performance for relation extraction over methods based solely on distant supervision?", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.891602486371994}]}, {"text": "While crowdsourcing has been shown effective for many NLP tasks, previous researchers found only minimal improvement when applying the method to relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8955208659172058}]}, {"text": "This paper demonstrates that a much larger boost is possible, e.g., raising F1 from 0.40 to 0.60.", "labels": [], "entities": [{"text": "F1", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9996540546417236}]}, {"text": "Furthermore, the gains are due to a simple , generalizable technique, Gated Instruction , which combines an interactive tutorial, feedback to correct errors during training, and improved screening.", "labels": [], "entities": [{"text": "Gated Instruction", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7183641791343689}, {"text": "screening", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9455559253692627}]}], "introductionContent": [{"text": "Relation extraction (RE) is the task of identifying instances of relations, such as nationality (person, country) or place of birth (person, location), in passages of natural text.", "labels": [], "entities": [{"text": "Relation extraction (RE) is the task of identifying instances of relations, such as nationality (person, country) or place of birth (person, location), in passages of natural text", "start_pos": 0, "end_pos": 179, "type": "Description", "confidence": 0.8162873604813138}]}, {"text": "Since RE enables abroad range of applications -including question answering and knowledge base population -it has attracted attention from many researchers.", "labels": [], "entities": [{"text": "RE", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9101003408432007}, {"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9132196009159088}]}, {"text": "Many approaches to RE use supervised machine learning, e.g.,), but these methods require a large, humanannotated training corpus that maybe unavailable.", "labels": [], "entities": [{"text": "RE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9953120946884155}]}, {"text": "In response, researchers developed methods for distant supervision (DS) in which a knowledge base such as Wikipedia or Freebase is used to automatically tag training examples from a text corpus ().", "labels": [], "entities": [{"text": "distant supervision (DS)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7029716074466705}]}, {"text": "Indeed, virtually all entries to recent TAC KBP relation extraction competitions use distant supervision).", "labels": [], "entities": [{"text": "TAC KBP relation extraction", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6586690247058868}]}, {"text": "However, distant supervision provides noisy training data with many false positives, and this limits the precision of the resulting extractors (see Section 2).", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9990863800048828}]}, {"text": "A natural assumption is that humanannotated training data, either alone or in conjunction with distant supervision, would give better precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9952455163002014}]}, {"text": "In particular, showed that, for many NLP tasks, crowdsourced data is as good as or better than that annotated by experts.", "labels": [], "entities": []}, {"text": "It is quite surprising, therefore, that researchers who have applied crowdsourced annotation to relation extraction argue the opposite, that crowdsourcing provides only minor improvement: \u2022 conclude that \"Human feedback has relatively small impact on precision and recall.\"", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.9081977605819702}, {"text": "precision", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.9985384941101074}, {"text": "recall", "start_pos": 265, "end_pos": 271, "type": "METRIC", "confidence": 0.9933841824531555}]}, {"text": "Instead, they advise applying distant supervision to vastly more data.", "labels": [], "entities": []}, {"text": "\u2022 assert \"Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data.\"", "labels": [], "entities": []}, {"text": "Instead, they introduce a complex feature-creation approach which improves the F1-score of MIML-RE, a state-of-the-art extractor (, just 4%, from 0.28 to 0.32 on a set of 41 TAC KBP relations.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9991768002510071}, {"text": "MIML-RE", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.8726570010185242}]}, {"text": "\u2022 explored a novel active learning method to control crowdsourcing, but found no improvement from adding the crowdsourced training to distant supervision using the default settings of MIML-RE, and only a 0.04 improvement in F1 when they initialized MIML-RE using the crowdsourced training.", "labels": [], "entities": [{"text": "MIML-RE", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.7971894145011902}, {"text": "F1", "start_pos": 224, "end_pos": 226, "type": "METRIC", "confidence": 0.9993798732757568}]}, {"text": "This paper reports quite a different result, showing up to a 0.20 boost to F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9955403208732605}]}, {"text": "By carefully designing a quality-controlled crowdsourcing workflow that uses Gated Instruction (GI), we are able to create much more accurate annotations than those produced by previous crowdsourcing methods.", "labels": [], "entities": []}, {"text": "GI (summarized in) includes an interactive tutorial to train workers, providing immediate feedback to correct mistakes during training.", "labels": [], "entities": [{"text": "GI", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.634602963924408}]}, {"text": "Workers are then screened by their accuracy on gold-standard questions while doing the annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992197751998901}]}, {"text": "We show that GI generates much better training data than crowdsourcing used by other researchers, and that this leads to dramatically improved extractors.", "labels": [], "entities": []}, {"text": "Adding GI-crowdsourced annotations of the example sentences selected by Angeli et al.'s active learning method provides a much larger boost to the performance of the learned extractors than when their traditional crowdsourcing methods are used.", "labels": [], "entities": []}, {"text": "In fact, the improvement due to our crowdsourcing method substantially outweighs the benefits of Angeli et al.'s active learning strategy as well.", "labels": [], "entities": []}, {"text": "In total, this paper makes the following contributions: \u2022 We present the design of the Gated Instruction crowdsourcing workflow with worker training and screening that ensures high-precision annotations for relation extraction training data in the presence of unreliable workers.", "labels": [], "entities": [{"text": "Gated Instruction crowdsourcing workflow", "start_pos": 87, "end_pos": 127, "type": "TASK", "confidence": 0.8142177015542984}, {"text": "relation extraction training", "start_pos": 207, "end_pos": 235, "type": "TASK", "confidence": 0.8665358622868856}]}, {"text": "\u2022 We demonstrate that Gated Instruction increases the annotation quality of crowdsourced training data, raising precision from 0.50 to 0.77 and recall from 0.70 to 0.78, compared to Angeli et al.'s crowdsourced tagging of the same sentences.", "labels": [], "entities": [{"text": "Gated Instruction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8567858040332794}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9991914629936218}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9993330836296082}]}, {"text": "We make the data available for future research (Section 4.1).", "labels": [], "entities": []}, {"text": "\u2022 show that increasing amounts of crowdsourced data can dramatically improve extractor performance.", "labels": [], "entities": []}, {"text": "When we augmented distant supervision with 20K instances using Gated Instruction, we show that F1 is raised from 0.40 to 0.60.", "labels": [], "entities": [{"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9995906949043274}]}, {"text": "\u2022 Gated Instruction may also reduce the cost of crowdsourcing.", "labels": [], "entities": [{"text": "Gated Instruction", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.9117822051048279}]}, {"text": "We show that with the high quality Gated Instruction annotations, a single annotation is more effective than majority vote over multiple annotators.", "labels": [], "entities": []}, {"text": "Our results provide a clear lesson for future researchers hoping to use crowdsourced data for NLP tasks.", "labels": [], "entities": []}, {"text": "Extreme care must be exercised in the details of the workflow design to ensure quality data and useful results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we address the following questions: \u2022 Does Gated Instruction produce training data with higher precision and recall than other research in crowdsourcing for relation extraction?", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9962708950042725}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9963456988334656}, {"text": "relation extraction", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.8765578269958496}]}, {"text": "\u2022 Does higher quality crowdsourced training data result in higher extractor performance when adding crowdsourcing to distant supervision?", "labels": [], "entities": []}, {"text": "\u2022 How does the boost in extractor performance on random training instances labeled with Gated Instruction compare to that with instances labeled using traditional crowdsourcing techniques selected with active learning?", "labels": [], "entities": []}, {"text": "\u2022 How does extractor performance increase with larger amounts of Gated Instruction training data?", "labels": [], "entities": []}, {"text": "\u2022 What's the most cost-effective way to aggregate worker votes?", "labels": [], "entities": []}, {"text": "Are multiple annotations needed, given high quality crowdsourcing?", "labels": [], "entities": []}], "tableCaptions": []}