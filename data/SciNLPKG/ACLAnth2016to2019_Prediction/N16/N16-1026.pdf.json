{"title": [], "abstractContent": [{"text": "We demonstrate that a state-of-the-art parser can be built using only a lexical tagging model and a deterministic grammar, with no explicit model of bi-lexical dependencies.", "labels": [], "entities": []}, {"text": "Instead , all dependencies are implicitly encoded in an LSTM supertagger that assigns CCG lexical categories.", "labels": [], "entities": []}, {"text": "The parser significantly out-performs all previously published CCG results , supports efficient and optimal A * decoding , and benefits substantially from semi-supervised tri-training.", "labels": [], "entities": []}, {"text": "We give a detailed analysis, demonstrating that the parser can recover long-range dependencies with high accuracy and that the semi-supervised learning enables significant accuracy gains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9965665340423584}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9961368441581726}]}, {"text": "By running the LSTM on a GPU, we are able to parse over 2600 sentences per second while improving state-of-the-art accuracy by 1.1 F1 in domain and up to 4.5 F1 out of domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9909852743148804}, {"text": "F1", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9881316423416138}, {"text": "F1", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9847295880317688}]}], "introductionContent": [{"text": "Combinatory Categorial Grammar (CCG) is a strongly lexicalized formalism-the vast majority of attachment decisions during parsing are specified by the selection of lexical entries for words (see 1 for examples).", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7828378081321716}]}, {"text": "State-of-the-art parsers typically include a supertagging model, to select possible lexical categories, and a bi-lexical dependency model, to resolve the remaining parse attachment ambiguities.", "labels": [], "entities": []}, {"text": "In this paper, we introduce along shortterm memory (LSTM) CCG parsing model that has no explicit model of bi-lexical dependencies, but instead relies on a bi-directional recurrent neural network (RNN) supertagger to capture all long distance dependencies.", "labels": [], "entities": [{"text": "along shortterm memory (LSTM) CCG parsing", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.6492773778736591}]}, {"text": "This approach has a number of advantages: it is conceptually simple, allows for the reuse of existing optimal and efficient parsing algorithms, benefits significantly from semi-supervised learning, and is highly accurate both in and out of domain.", "labels": [], "entities": []}, {"text": "The parser is publicly released.", "labels": [], "entities": []}, {"text": "Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output.", "labels": [], "entities": [{"text": "parsing", "start_pos": 159, "end_pos": 166, "type": "TASK", "confidence": 0.9608837962150574}]}, {"text": "Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing, constituency parse re-ranking (, and stringto-string transduction (, or (2) avoid recursive computations entirely.", "labels": [], "entities": [{"text": "constituency parse re-ranking", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.7781827449798584}]}, {"text": "Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs, and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse.", "labels": [], "entities": []}, {"text": "Our LSTM supertagger is bi-directional and includes a softmax potential over tags for each word in the sentence.", "labels": [], "entities": []}, {"text": "During training, we jointly optimize all LSTM parameters, including the word embeddings, to maximize the conditional likelihood of supertag sequences.", "labels": [], "entities": []}, {"text": "For inference, we use a recently introduced A* CCG parsing algorithm (, which efficiently searches for the: Four examples of prepositional phrase attachment in CCG.", "labels": [], "entities": [{"text": "A* CCG parsing", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.604235015809536}, {"text": "prepositional phrase attachment", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.7074631651242574}]}, {"text": "In the upper two parses, the attachment decision is determined by the choice of supertags.", "labels": [], "entities": []}, {"text": "In the lower parses, the attachment is ambiguous given the supertags.", "labels": [], "entities": [{"text": "attachment", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9553842544555664}]}, {"text": "In such cases, our parser deterministically attaches low (i.e. preferring the lower-right parse).", "labels": [], "entities": []}, {"text": "highest probability sequence of tags that combine to produce a complete parse tree.", "labels": [], "entities": []}, {"text": "Whenever there is parsing ambiguity not specified by the supertags, the model attaches low (see).", "labels": [], "entities": []}, {"text": "This approach is not only conceptually simple but also highly effective, as we demonstrate with extensive experiments.", "labels": [], "entities": []}, {"text": "Because the A* algorithm is extremely efficient and the LSTMs can be run in parallel on GPUs, the end-to-end parser can process over 2600 sentences per second.", "labels": [], "entities": []}, {"text": "This is more than three times the speed of any publicly available parser for any formalism.", "labels": [], "entities": []}, {"text": "Apart from, we are not aware of efficient algorithms for running other state-of-art-parsers on GPUs.", "labels": [], "entities": []}, {"text": "The LSTM parameters also benefit from semi-supervised training, which we demonstrate by employing a recently introduced tri-training scheme.", "labels": [], "entities": []}, {"text": "Finally, the recurrent nature of the LSTM allows for effective modelling of long distance dependencies, as we show empirically.", "labels": [], "entities": []}, {"text": "Our approach significantly advances the state-of-the-art on benchmark datasets-improving accuracy by 1.1 F1 in domain and up to 4.5 F1 out of domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.996191143989563}, {"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9925221800804138}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9880361557006836}]}], "datasetContent": [{"text": "We trained our parser on Sections 02-21 of CCGbank, using Section 00 for development, and Section 23 for test.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8275381922721863}]}, {"text": "Our experiments use a supertagger beam of 10 \u22124 -which does not affect the final scores, but reduces overheads such as building the initial agenda.", "labels": [], "entities": []}, {"text": "We also evaluate on two out-of-domain datasets used by, but did no development on this data.", "labels": [], "entities": []}, {"text": "In both cases, we use Rimell and Clark's scripts for converting CCG parses to the target dependency representations.", "labels": [], "entities": []}, {"text": "The datasets are: QUESTIONS 500 questions from TREC (.", "labels": [], "entities": [{"text": "QUESTIONS 500 questions from TREC", "start_pos": 18, "end_pos": 51, "type": "DATASET", "confidence": 0.6980143308639526}]}, {"text": "Questions frequently contain very long range dependencies, providing an interesting test of the LSTM supertagger's ability to capture unbounded dependencies.", "labels": [], "entities": []}, {"text": "We follow Rimell and Clark by re-training the supertagger on the concatenation of the CCGbank training data and 10 copies of the QUESTIONS training data.", "labels": [], "entities": [{"text": "CCGbank training data", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.948780874411265}, {"text": "QUESTIONS training data", "start_pos": 129, "end_pos": 152, "type": "DATASET", "confidence": 0.80831378698349}]}, {"text": "BIOINFER 500 sentences from biomedical abstracts.", "labels": [], "entities": [{"text": "BIOINFER", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8444383144378662}]}, {"text": "This dataset tests the parser's robustness to a large amount of unseen vocabulary.", "labels": [], "entities": []}, {"text": "Our LSTM parser outperforms existing work on question parsing, showing that it can successfully model the longrange dependencies found in questions.", "labels": [], "entities": [{"text": "LSTM parser", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7290156185626984}, {"text": "question parsing", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7137220054864883}]}, {"text": "Adding dependency features yields only a small improvement.", "labels": [], "entities": []}, {"text": "On the BIOINFER corpus, our tri-trained LSTM parser is 4.5 F1 better than the previous state-ofthe-art.", "labels": [], "entities": [{"text": "BIOINFER corpus", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.8331557810306549}, {"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9892402291297913}]}, {"text": "Dependency features appear to be much (2011b)'s joint parsing and supertagging model, due to differences in the experimental setup.", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.5853035897016525}]}, {"text": "These models are 0.3 and 1.5 F1 more accurate than the C&C baseline respectively, which is well within the margin of improvement obtained by our model.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9986463189125061}, {"text": "C&C baseline", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.96729876101017}]}, {"text": "In contrast to standard parsing algorithms, the efficiency of our model depends directly on the accuracy of the supertagger in guiding the search.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9987190961837769}]}, {"text": "We therefore measure the efficiency empirically.", "labels": [], "entities": []}, {"text": "Our parser runs more slowly than EASYCCG on CPU, due to the more complex tagging model (but is 4.8 F1 more accurate).", "labels": [], "entities": [{"text": "EASYCCG", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.8918430805206299}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.961818277835846}]}, {"text": "Adding dependencies substantially reduces efficiency, due to calculating sparse features.", "labels": [], "entities": []}, {"text": "Without dependencies, the run time is dominated by the LSTM supertagger.", "labels": [], "entities": []}, {"text": "Running the supertagger on a GPU reduces parsing times dramaticallyoutperforming SpaCy, the fastest publicly available parser (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9658019542694092}]}, {"text": "Roughly half the parsing time is spent on GPU supertagging, and half on CPU parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.979685366153717}, {"text": "CPU parsing", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.6662553548812866}]}, {"text": "To better exploit batching in the GPU, our implementation dynamically buckets sentences by length (bins of width 10), and tags batches when the bucket size reaches 3072 (the number of threads on our GPU).", "labels": [], "entities": []}, {"text": "We are not aware of any GPU implementations of shift-reduce parsers or lexicalized chart parsers, so it is unclear if most other state-ofthe-art parsers can be adapted to exploit GPUs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Supertagging accuracy on CCGbank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9879280924797058}, {"text": "CCGbank", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.93841153383255}]}, {"text": " Table 2: Labelled F1 for CCGbank dependencies  on the CCGbank test set (Section 23).", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8897131681442261}, {"text": "CCGbank test set", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9751716454823812}]}, {"text": " Table 4: Sentences parsed per second on our hard- ware. Parsers marked * use non-CCG formalisms  but are the fastest available CPU and GPU parsers.", "labels": [], "entities": []}, {"text": " Table 5: Development supertagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9915424585342407}]}, {"text": " Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9919450879096985}, {"text": "Long range", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9356725513935089}]}, {"text": " Table 7: Effect of simulating weaker grammars, by  allowing the specified atomic categories to unify. *  allows all atomic categories to unify, except con- junctions and punctuation. Results are on develop- ment sentences of length \u226440.", "labels": [], "entities": []}, {"text": " Table 8. Any improvements from the de- pendency model are small-it is difficult to improve", "labels": [], "entities": []}, {"text": " Table 8: Per-relation accuracy for several dependen- cies whose attachments are often ambiguous given  the supertags. Results are only on sentences where  the parsers assign the correct supertags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9911070466041565}]}]}