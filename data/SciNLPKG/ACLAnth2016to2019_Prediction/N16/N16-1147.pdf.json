{"title": [], "abstractContent": [{"text": "We introduce the first dataset for sequential vision-to-language, and explore how this data maybe used for the task of visual storytelling.", "labels": [], "entities": [{"text": "visual storytelling", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7901210188865662}]}, {"text": "The first release of this dataset, SIND 1 v.1, includes 81,743 unique photos in 20,211 sequences , aligned to both descriptive (caption) and story language.", "labels": [], "entities": []}, {"text": "We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress.", "labels": [], "entities": []}, {"text": "Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "labels": [], "entities": []}], "introductionContent": [{"text": "Beyond understanding simple objects and concrete scenes lies interpreting causal structure; making sense of visual input to tie disparate moments together as they give rise to a cohesive narrative of events through time.", "labels": [], "entities": [{"text": "interpreting causal structure", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.8739911715189616}]}, {"text": "This requires moving from reasoning about single images -static moments, devoid of context -to sequences of images that depict events as they occur and change.", "labels": [], "entities": []}, {"text": "On the vision side, progressing from single images to images in context allows us to begin to create an artificial intelligence (AI) that can reason about a visual moment given * T.H. and F.F. contributed equally to this work.", "labels": [], "entities": []}, {"text": "1 Sequential Images Narrative Dataset.", "labels": [], "entities": [{"text": "Sequential Images Narrative Dataset", "start_pos": 2, "end_pos": 37, "type": "DATASET", "confidence": 0.6160016059875488}]}, {"text": "The original release was made available through Microsoft.", "labels": [], "entities": []}, {"text": "Related future releases for Visual Storytelling (VIST) are on visionandlanguage.net/VIST.", "labels": [], "entities": [{"text": "Visual Storytelling (VIST)", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7718593120574951}]}, {"text": "A group of people that are sitting next to each other.", "labels": [], "entities": []}, {"text": "Adult male wearing sunglasses lying down on black pavement.", "labels": [], "entities": []}, {"text": "The sun is setting over the ocean and mountains.", "labels": [], "entities": []}, {"text": "Having a good time bonding and talking.", "labels": [], "entities": []}, {"text": "[M] got exhausted by the heat.", "labels": [], "entities": []}, {"text": "Sky illuminated with a brilliance of gold and orange hues.", "labels": [], "entities": []}, {"text": "what it has already seen.", "labels": [], "entities": []}, {"text": "On the language side, progressing from literal description to narrative helps to learn more evaluative, conversational, and abstract language.", "labels": [], "entities": []}, {"text": "This is the difference between, for example, \"sitting next to each other\" versus \"having a good time\", or \"sun is setting\" versus \"sky illuminated with a brilliance...\"", "labels": [], "entities": []}, {"text": "The first descriptions capture image content that is literal and concrete; the second requires further inference about what a good time may look like, or what is special and worth sharing about a particular sunset.", "labels": [], "entities": []}, {"text": "We introduce the first dataset of sequential images with corresponding descriptions, which captures some of these subtle but important differences, and advance the task of visual storytelling.", "labels": [], "entities": []}, {"text": "We release the data in three tiers of language for the same images: (1) Descriptions of imagesin-isolation (DII); (2) Descriptions of images-insequence (DIS); and (3) Stories for images-insequence (SIS).", "labels": [], "entities": []}, {"text": "This tiered approach reveals the effect of temporal context and the effect of narrative language.", "labels": [], "entities": []}, {"text": "As all the tiers are aligned to the same images, the dataset facilitates directly modeling the relationship between literal and more abstract visual concepts, including the relationship between visual imagery and typical event patterns.", "labels": [], "entities": []}, {"text": "We additionally propose an automatic evaluation metric which is best correlated with human judgments, and establish several strong baselines for the visual storytelling task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Extracting Photos We begin by generating a list of \"storyable\" event types.", "labels": [], "entities": []}, {"text": "We leverage the idea that \"storyable\" events tend to involve some form of posbeach breaking up easter (259) amusement park carnival church (243) building a house visit graduation ceremony (236) party market office birthday outdoor activity (267) father's day (221): The number of albums in our tiered dataset for the 15 most frequent kinds of stories.", "labels": [], "entities": []}, {"text": "Given the nature of the complex storytelling task, the best and most reliable evaluation for assessing the quality of generated stories is human judgment.", "labels": [], "entities": []}, {"text": "However, automatic evaluation metrics are useful to quickly benchmark progress.", "labels": [], "entities": []}, {"text": "To better understand which metric could serve as a proxy for human evaluation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set.", "labels": [], "entities": [{"text": "SIS training set", "start_pos": 204, "end_pos": 220, "type": "DATASET", "confidence": 0.7718672355016073}]}, {"text": "For the human judgements, we again use crowdsourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement \"If these were my photos, I would like using a story like this to share my experience with my friends\".", "labels": [], "entities": [{"text": "MTurk", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9646120667457581}]}, {"text": "We take the average of the five judgments as the final score for the story.", "labels": [], "entities": []}, {"text": "For the automatic metrics, we use METEOR, 8 smoothed-BLEU (, and Skip-Thoughts ( ) to compute similarity between each story fora given sequence.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9975433945655823}]}, {"text": "Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels.", "labels": [], "entities": []}, {"text": "As shows, METEOR correlates best with human judgment according to all the correlation coefficients.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.962822437286377}]}, {"text": "This signals that a metric such as ME-TEOR which incorporates paraphrasing correlates best with human judgement on this task.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.962722659111023}]}, {"text": "A more METEOR BLEU Skip-Thoughts r 0.22 (2.8e-28) 0.08 (1.0e-06) 0.18 (5.0e-27) \u03c1 0.20 (3.0e-31) 0.08 (8.9e-06) 0.16 (6.4e-22) \u03c4 0.14 (1.0e-33) 0.06 (8.7e-08) 0.11 (7.7e-24)  detailed study of automatic evaluation of stories is an area of interest fora future work.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9937762022018433}, {"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8828812837600708}]}, {"text": "We report baseline experiments on the storytelling task in, training on the SIS tier and testing on half the SIS validation set (valtest).", "labels": [], "entities": [{"text": "SIS validation set", "start_pos": 109, "end_pos": 127, "type": "DATASET", "confidence": 0.6581796209017435}]}, {"text": "Example output from each system is presented in.", "labels": [], "entities": []}, {"text": "To highlight some differences between story and caption generation, we also train on the DII tier in isolation, and produce captions per-image, rather than in sequence.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9491170048713684}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of  and to multiple images.", "labels": [], "entities": [{"text": "story generation", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7369699776172638}]}, {"text": "Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order.", "labels": [], "entities": []}, {"text": "This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary.", "labels": [], "entities": []}, {"text": "We use Gated Recurrent Units (GRUs) () for both the image encoder and story decoder.", "labels": [], "entities": []}, {"text": "In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously ( . However, for story generation, the results of this model subjectively appear to be very poor -the system produces generic, repetitive, highlevel descriptions (e.g., \"This is a picture of a dog\").", "labels": [], "entities": [{"text": "image captioning", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7242220044136047}, {"text": "story generation", "start_pos": 158, "end_pos": 174, "type": "TASK", "confidence": 0.7202894687652588}]}, {"text": "Beam=10 Greedy -Dups +Grounded 23. 27.76 30.11 31.42 +Viterbi This is a picture of a family.", "labels": [], "entities": [{"text": "Beam", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9928493499755859}, {"text": "Greedy -Dups", "start_pos": 8, "end_pos": 20, "type": "METRIC", "confidence": 0.9361105561256409}]}, {"text": "This is a picture of a cake.", "labels": [], "entities": []}, {"text": "This is a picture of a dog.", "labels": [], "entities": []}, {"text": "This is a picture of a beach.", "labels": [], "entities": []}, {"text": "This is a picture of a beach.", "labels": [], "entities": []}, {"text": "+Greedy The family gathered together fora meal.", "labels": [], "entities": []}, {"text": "The dog was excited to be there.", "labels": [], "entities": []}, {"text": "The dog was enjoying the water.", "labels": [], "entities": []}, {"text": "The dog was happy to be in the water.", "labels": [], "entities": []}, {"text": "-Dups The family gathered together fora meal.", "labels": [], "entities": []}, {"text": "The dog was excited to be there.", "labels": [], "entities": []}, {"text": "The kids were playing in the water.", "labels": [], "entities": []}, {"text": "The boat was a little too much to drink.", "labels": [], "entities": []}, {"text": "+Grounded The family got together fora cookout.", "labels": [], "entities": []}, {"text": "They had a lot of delicious food.", "labels": [], "entities": []}, {"text": "The dog was happy to be there.", "labels": [], "entities": []}, {"text": "They had a great time on the beach.", "labels": [], "entities": []}, {"text": "They even had a swim in the water.", "labels": [], "entities": []}, {"text": "This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly ( ).", "labels": [], "entities": []}, {"text": "To establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story.", "labels": [], "entities": []}, {"text": "The first heuristic is to lower the decoder beam size substantially.", "labels": [], "entities": []}, {"text": "We find that using abeam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9696549475193024}]}, {"text": "However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9876794219017029}]}, {"text": "This highlights a key difference in generating stories versus generating captions.", "labels": [], "entities": []}, {"text": "Although the stories produced using a greedy search result in significant gains, they include many repeated words and phrases, e.g., \"The kids had a great time.", "labels": [], "entities": []}, {"text": "And the kids had a great time.\"", "labels": [], "entities": []}, {"text": "We introduce a very simple heuristic to avoid this, where the same content word cannot be produced more than once within a given story.", "labels": [], "entities": []}, {"text": "This improves METEOR by another 2.3 points.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.990836501121521}]}, {"text": "An advantage of comparing captioning to storytelling side-by-side is that the captioning output maybe used to help inform the storytelling output.", "labels": [], "entities": []}, {"text": "To this end, we include an additional baseline where \"visually grounded\" words may only be produced if they are licensed by the caption model.", "labels": [], "entities": []}, {"text": "We define the set of visually grounded words to be those which occurred at higher frequency in the caption training than the story training: We train a separate model using the caption annotations, and produce an n-best list of captions for each image in the valtest set.", "labels": [], "entities": []}, {"text": "Words seen in at least 10 sentences in the 100-best list are marked as 'licensed' by the caption model.", "labels": [], "entities": []}, {"text": "Greedy decoding without duplication proceeds with the additional constraint that if a word is visually grounded, it can only be generated by the story model if it is licensed by the caption model for the same photo set.", "labels": [], "entities": []}, {"text": "This results in a further 1.3 METEOR improvement.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9965555667877197}]}, {"text": "It is interesting to note what a strong effect relatively simple heuristics have on the generated stories.", "labels": [], "entities": []}, {"text": "We do not intend to suggest that these heuristics are the right way to approach story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7132697999477386}]}, {"text": "Instead, the main purpose is to provide clear baselines that demonstrate that story generation has fundamentally different challenges from caption generation; and the space is wide open to explore for training and decoding methods to generate fluent stories.", "labels": [], "entities": [{"text": "story generation", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7451568841934204}, {"text": "caption generation", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9280283749103546}]}], "tableCaptions": [{"text": " Table 2: A summary of our dataset, following the pro- posed analyses of Ferraro et al. (2015), including the Fra- zier and Yngve measures of syntactic complexity. The  balanced Brown corpus (Marcus et al., 1999), provided  for comparison, contains only text. Perplexity (Ppl) is  calculated against a 5-gram language model learned on a  generic 30B English words dataset scraped from the web.", "labels": [], "entities": [{"text": "Perplexity (Ppl)", "start_pos": 260, "end_pos": 276, "type": "METRIC", "confidence": 0.8331561237573624}]}, {"text": " Table 4: Correlations of automatic scores against human  judgements, with p-values in parentheses.", "labels": [], "entities": []}, {"text": " Table 6: Captions generated per-image with METEOR  scores.", "labels": [], "entities": [{"text": "Captions", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9550800919532776}, {"text": "METEOR", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.98732590675354}]}]}