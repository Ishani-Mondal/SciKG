{"title": [{"text": "Learning a POS tagger for AAVE-like language *", "labels": [], "entities": [{"text": "AAVE-like language", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.5581306517124176}]}], "abstractContent": [{"text": "Part-of-speech (POS) taggers trained on newswire perform much worse on domains such as subtitles, lyrics, or tweets.", "labels": [], "entities": [{"text": "Part-of-speech (POS) taggers", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5932217717170716}]}, {"text": "In addition, these domains are also heterogeneous, e.g., with respect to registers and dialects.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of learning a POS tagger for subtitles, lyrics, and tweets associated with African-American Vernacular English (AAVE).", "labels": [], "entities": []}, {"text": "We learn from a mixture of randomly sampled and manually annotated Twitter data and unlabeled data, which we automatically and partially label using mined tag dictionaries.", "labels": [], "entities": []}, {"text": "Our POS tagger obtains a tagging accuracy of 89% on subtitles, 85% on lyrics, and 83% on tweets, with up to 55% error reductions over a state-of-the-art newswire POS tagger, and 15-25% error reductions over a state-of-the-art Twitter POS tagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9901986122131348}, {"text": "error reductions", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.9625647068023682}]}], "introductionContent": [{"text": "Modern part-of-speech (POS) taggers perform well on what some consider canonical language, as found in domains such as newswire, for which sufficient manually-annotated data is available.", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.6291270554065704}]}, {"text": "For many domains, such as subtitles, lyrics, and tweets, however, labeled data is scarce, if existing, and the performance of off-the-shelf POS taggers is prohibitive of downstream applications.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 140, "end_pos": 151, "type": "TASK", "confidence": 0.7135679721832275}]}, {"text": "Furthermore, subtitles, lyrics and tweets are very heterogeneous.", "labels": [], "entities": []}, {"text": "Subtitles span from Shakespeare to The Wire, and the lyrics of Elvis Costello are very different from those of Tupac Shakur.", "labels": [], "entities": []}, {"text": "Twitter can * This work was supported by be anything from teenagers discussing whereto go tonight, to researchers discussed the implications of new findings.", "labels": [], "entities": []}, {"text": "All three sources of data exhibit a very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors.", "labels": [], "entities": []}, {"text": "In this paper, we use a corpus of POS-annotated tweets recently released by CMU, 1 consisting of semi-randomly sampled US tweets.", "labels": [], "entities": [{"text": "CMU, 1", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.9012327194213867}]}, {"text": "We want to use this corpus to learn a POS tagger for subtitles, lyrics, and tweets, which are typically associated with African-American Vernacular English (AAVE).", "labels": [], "entities": []}, {"text": "We believe our POS tagger can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (, which relies on the accuracy of these NLP tools.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9967504739761353}]}, {"text": "We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7329550385475159}]}, {"text": "Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9988232254981995}]}, {"text": "Since similar words receive similar labels, this can give the model support for words not in the training data.", "labels": [], "entities": []}, {"text": "In this paper, we use word clusters and word embeddings in both our baseline and system models.", "labels": [], "entities": []}, {"text": "Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised learning, as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (.", "labels": [], "entities": [{"text": "importance sampling", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7673488259315491}, {"text": "domain adaptation", "start_pos": 258, "end_pos": 275, "type": "TASK", "confidence": 0.8274249732494354}]}, {"text": "In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (, and for imposing type constraints attest time in a semisupervised setting (.", "labels": [], "entities": []}, {"text": "Our best models are obtained using partially labeled training data created using tag dictionaries.", "labels": [], "entities": []}, {"text": "Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.6625317633152008}]}, {"text": "Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) anew domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soegaard/aave-pos16.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.8488649129867554}, {"text": "POS tagger", "start_pos": 271, "end_pos": 281, "type": "TASK", "confidence": 0.726653516292572}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracies on unseen words", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9969698786735535}]}]}