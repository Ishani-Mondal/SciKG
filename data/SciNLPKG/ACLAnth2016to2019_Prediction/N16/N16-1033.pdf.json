{"title": [{"text": "Joint Extraction of Events and Entities within a Document Context", "labels": [], "entities": [{"text": "Extraction of Events and Entities within a Document Context", "start_pos": 6, "end_pos": 65, "type": "TASK", "confidence": 0.8356509076224433}]}], "abstractContent": [{"text": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon.", "labels": [], "entities": []}, {"text": "The interpretation of events and entities is highly contextually dependent.", "labels": [], "entities": [{"text": "interpretation of events and entities", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.826235318183899}]}, {"text": "Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7368246167898178}]}, {"text": "In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document.", "labels": [], "entities": []}, {"text": "The goal is to enable access to document-level contextual information and facilitate context-aware predictions.", "labels": [], "entities": []}, {"text": "We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7644787132740021}, {"text": "entity extraction", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.773307204246521}]}], "introductionContent": [{"text": "Events are things that happen or occur; they involve entities (people, objects, etc.) who perform or are affected by the events and spatio-temporal aspects of the world.", "labels": [], "entities": [{"text": "Events are things that happen or occur; they involve entities (people, objects, etc.) who perform or are affected by the events and spatio-temporal aspects of the world", "start_pos": 0, "end_pos": 168, "type": "Description", "confidence": 0.8146055787801743}]}, {"text": "Understanding events and their descriptions in text is necessary for any generallyapplicable machine reading systems.", "labels": [], "entities": [{"text": "Understanding events and their descriptions in text", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8075500556400844}]}, {"text": "It is also essential in facilitating practical applications such as news summarization, information retrieval, and knowledge base construction.", "labels": [], "entities": [{"text": "news summarization", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7492835521697998}, {"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8300168812274933}, {"text": "knowledge base construction", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6914225220680237}]}, {"text": "The interpretation of event descriptions is highly contextually dependent.", "labels": [], "entities": [{"text": "interpretation of event descriptions", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7896976917982101}]}, {"text": "To make correct predictions, a model needs to account for mentions of events and entities together with the discourse context.", "labels": [], "entities": []}, {"text": "Consider, for example, the following excerpt from a news report: \"On Thursday, there was a massive U.S. aerial bombardment in which more than 300 Tomahawk cruise missiles rained down on Baghdad.", "labels": [], "entities": []}, {"text": "Earlier Saturday, Baghdad was again targeted.", "labels": [], "entities": [{"text": "Baghdad", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9219947457313538}]}, {"text": "The excerpt describes two U.S. attacks on Baghdad.", "labels": [], "entities": []}, {"text": "The two event anchors (triggers) are boldfaced and the mentions of entities and spatio-temporal information are italicized.", "labels": [], "entities": []}, {"text": "The first event anchor \"aerial bombardment\" along with its surrounding entity mentions -\"U.S.\", \"Tomahawk cruise missiles\", and \"Baghdad\", describe an attack from the U.S. on Baghdad with Tomahawk cruise missiles being the weapon.", "labels": [], "entities": [{"text": "aerial bombardment", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.733750730752945}]}, {"text": "The second sentence on its own contains little event-related information, but together with the context of the previous sentence, it indicates another U.S. attack on Baghdad.", "labels": [], "entities": []}, {"text": "State-of-the-art event extraction systems have difficulties inferring such information due to two main reasons.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7418539226055145}]}, {"text": "First, they extract events and entities in separate stages: entities such as people, organization, and locations are first extracted by a named entity tagger, and then these extracted entities are used as inputs for extracting events and their arguments (.", "labels": [], "entities": []}, {"text": "This often causes errors to propagate.", "labels": [], "entities": []}, {"text": "In the above example, if the entity tagger mistakenly identifies \"Baghdad\" as a person, then the event extractor will fail to extract \"Baghdad\" as the place where the attack happened.", "labels": [], "entities": []}, {"text": "In fact, previous work ( observes that using previously extracted entities in event extraction results in a substantial decrease in performance compared to using gold-standard entity information.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7226770520210266}]}, {"text": "Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (.", "labels": [], "entities": []}, {"text": "Very few attempts have been made to incorporate document context for event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7461932003498077}]}, {"text": "model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers' outputs to satisfy document-wide (or document-cluster-wide) consistency.", "labels": [], "entities": []}, {"text": "further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information.", "labels": [], "entities": []}, {"text": "Both approaches only propagate the highly confident predictions from the first stage to the second stage.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no unified model that jointly extracts events from sentences across the whole document.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context.", "labels": [], "entities": []}, {"text": "We first decompose the learning problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the cooccurrence relations between events across the document, and (3) learning for entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 260, "end_pos": 277, "type": "TASK", "confidence": 0.7590163946151733}]}, {"text": "Then we combine the learned models for these subproblems into a joint optimization framework that simultaneously extracts events, semantic roles, and entities in a document.", "labels": [], "entities": []}, {"text": "In summary, our main contributions are: 1.", "labels": [], "entities": []}, {"text": "We propose a structured model for learning within-event structures that can effectively capture the dependencies between an event and its arguments, and between the semantic roles and entity types for the arguments.", "labels": [], "entities": []}, {"text": "2. We introduce a joint inference framework that combines probabilistic models of within-event structures, event-event relations, and entity ex- https://github.com/bishanyang/ EventEntityExtractor.", "labels": [], "entities": []}, {"text": "traction for joint extraction of the set of entities and events over the whole document.", "labels": [], "entities": []}, {"text": "3. We conduct extensive experiments on the Automatic Content Extraction (ACE) corpus, and show that our approach significantly outperforms the state-of-the-art methods for event extraction and a strong baseline for entity extraction.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.7167910734812418}, {"text": "event extraction", "start_pos": 172, "end_pos": 188, "type": "TASK", "confidence": 0.7531799674034119}, {"text": "entity extraction", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.769224613904953}]}], "datasetContent": [{"text": "We conduct experiments on the ACE2005 corpus.", "labels": [], "entities": [{"text": "ACE2005 corpus", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9881725609302521}]}, {"text": "It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums.", "labels": [], "entities": []}, {"text": "We use the same data split as in. shows the data statistics.", "labels": [], "entities": []}, {"text": "We adopt the evaluation metrics for events as defined in.", "labels": [], "entities": []}, {"text": "An event trigger is correctly identified if its offsets match those of a goldstandard trigger; and it is correctly classified if its event subtype (33 in total) also match the subtype of the gold-standard trigger.", "labels": [], "entities": []}, {"text": "An event argument is correctly identified if its offsets and event subtype match those of any of the reference argument mentions in the document; and it is correctly classified if its semantic role (28 in total) is also correct.", "labels": [], "entities": []}, {"text": "For entities, a predicted mention is correctly extracted if its head offsets and entity type (9 in total) match those of the reference entity mention.", "labels": [], "entities": []}, {"text": "Note that our approach requires entity mention candidates and event trigger candidates as input.", "labels": [], "entities": []}, {"text": "Instead of enumerating all possible text spans, we generate high-quality entity mentions from the kbest predictions of our CRF entity extractor (in Section 3.3).", "labels": [], "entities": []}, {"text": "Similarly, we train a CRF for event trigger extraction using the same features except for the gazetteers, and generate trigger candidates based on the k-best predictions.", "labels": [], "entities": [{"text": "event trigger extraction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.7661914030710856}]}, {"text": "We set k = 50 for entities and k = 10 for event triggers based on performance on the development set.", "labels": [], "entities": []}, {"text": "They cover 92.3% of the gold-standard entity mentions and 96.3% of the gold-standard event triggers in the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Features for event triggers, event arguments, and entity mentions.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the ACE2005 dataset.", "labels": [], "entities": [{"text": "ACE2005 dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9864881038665771}]}, {"text": " Table 3: Event extraction results on the ACE2005 test set.  *  indicates that the difference in F1 compared to the second best model", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.678482711315155}, {"text": "ACE2005 test set", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9834086100260416}, {"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9994577765464783}]}, {"text": " Table 4: Comparison of the results (F1 score) of JOINTEVEN-", "labels": [], "entities": [{"text": "F1 score)", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9826149940490723}, {"text": "JOINTEVEN", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.605859637260437}]}, {"text": " Table 5: Entity extraction results on the ACE2005 test set.  *", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7303087562322617}, {"text": "ACE2005 test set", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9844974478085836}]}, {"text": " Table 6: Entity extraction results (F1 score) per entity type.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6981863379478455}, {"text": "F1 score)", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9795133074124655}]}, {"text": " Table 7: Classification of errors made by JOINTEVENTEN-", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9679383635520935}, {"text": "JOINTEVENTEN", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.40771543979644775}]}]}