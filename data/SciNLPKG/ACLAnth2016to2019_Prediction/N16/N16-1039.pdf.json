{"title": [{"text": "Distinguishing Literal and Non-Literal Usage of German Particle Verbs", "labels": [], "entities": [{"text": "Distinguishing Literal and Non-Literal Usage of German Particle Verbs", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.8198933998743693}]}], "abstractContent": [{"text": "This paper provides a binary, token-based classification of German particle verbs (PVs) into literal vs. non-literal usage.", "labels": [], "entities": []}, {"text": "A random forest improving standard features (e.g., bag-of-words; affective ratings) with PV-specific information and abstraction over common nouns significantly outperforms the majority baseline.", "labels": [], "entities": []}, {"text": "In addition, PV-specific classification experiments demonstrate the role of shared particle semantics and semantically related base verbs in PV meaning shifts.", "labels": [], "entities": [{"text": "PV-specific classification", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8423028886318207}, {"text": "PV meaning shifts", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.8171725471814474}]}], "introductionContent": [{"text": "Automatic detection of non-literal expressions (including metaphors and idioms) is critical for many natural language processing (NLP) tasks such as information extraction, machine translation, and sentiment analysis.", "labels": [], "entities": [{"text": "Automatic detection of non-literal expressions (including metaphors and idioms", "start_pos": 0, "end_pos": 78, "type": "TASK", "confidence": 0.8038186371326447}, {"text": "information extraction", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.8017094135284424}, {"text": "machine translation", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.8052665293216705}, {"text": "sentiment analysis", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.9556856155395508}]}, {"text": "For this reason, the last decade has seen an increase in research on identifying literal vs. non-literal meaning, as well as the establishment of workshops on metaphorical language in NLP.", "labels": [], "entities": [{"text": "identifying literal vs. non-literal meaning", "start_pos": 69, "end_pos": 112, "type": "TASK", "confidence": 0.7455568909645081}]}, {"text": "In this paper, we explore the prediction of literal vs. non-literal language usage of a computationally challenging class of multiword expressions: German particle verbs (PVs) such as anlachen (laugh at) are compositions of abase verb 1 sites.google.com/site/metaphorinnlp2016/ (BV) such as lachen (smile/laugh) and a verb particle such as an.", "labels": [], "entities": []}, {"text": "German PVs are highly productive (, and the particles are notoriously ambiguous (.", "labels": [], "entities": [{"text": "German PVs", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8600852787494659}]}, {"text": "Furthermore, the particles often trigger (regular) meaning shifts when they combine with base verbs, so the resulting PVs represent frequent cases of non-literal meaning.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We present a random forest classifier that correctly identifies 86.8% of literal vs. non-literal language usage within a novel dataset of 6 436 annotated sentences, in comparison to a majority baseline of 64.9%.", "labels": [], "entities": []}, {"text": "2. We successfully incorporate salient PVspecific features and noun clusters in addition to standard bag-of-words features and affective ratings.", "labels": [], "entities": []}, {"text": "3. We demonstrate that PVs with semantically similar particles and semantically similar base verbs can predict each others' literal vs. non-literal language usage.", "labels": [], "entities": []}, {"text": "4. We illustrate the potential and the limits of the most salient classification features in predicting PV non-literal language usage.", "labels": [], "entities": [{"text": "predicting PV non-literal language usage", "start_pos": 93, "end_pos": 133, "type": "TASK", "confidence": 0.7017333447933197}]}, {"text": "In the remainder of this paper we describe previous work on non-literal language identification and computational models of German particle verbs (Section 2), before we introduce our dataset on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, results and analyses (Section 5).", "labels": [], "entities": [{"text": "non-literal language identification", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.6668758193651835}]}], "datasetContent": [{"text": "We selected 165 particle verbs across 10 particles, based on previous experiments and datasets that incorporated German particle verbs with regular meaning shifts, various degrees of ambiguity, and across frequency ranges.", "labels": [], "entities": []}, {"text": "For the 165 PVs, we randomly extracted 50 sentences from DECOW14AX, a German web corpus containing 12 billion tokens (.", "labels": [], "entities": [{"text": "DECOW14AX, a German web corpus", "start_pos": 57, "end_pos": 87, "type": "DATASET", "confidence": 0.8452051182587942}]}, {"text": "The sentences were morphologically annotated and parsed using SMOR, MarMoT () and the MATE dependency parser.", "labels": [], "entities": [{"text": "SMOR", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.49647557735443115}]}, {"text": "Combining partof-speech and dependency information, we were able to reliably sample both separated and nonseparated PV occurrences (\"Der Ast bricht ab\" vs. \"Der Ast ist abgebrochen\").", "labels": [], "entities": []}, {"text": "Three German native speakers with a linguistic background annotated each of the 8 128 sen-tences 2 on a 6-point scale, ranging from clearly literal (0) to clearly non-literal (5) usage.", "labels": [], "entities": []}, {"text": "The total agreement of the annotators on all six categories was 43%, Fleiss' \u03ba = 0.35.", "labels": [], "entities": [{"text": "agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9915340542793274}, {"text": "Fleiss' \u03ba", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9378583133220673}]}, {"text": "Dividing the scale into two disjunctive ranges with three categories each ( and), the total agreement of the annotators on the two categories was 79%, Fleiss' \u03ba = 0.70.", "labels": [], "entities": [{"text": "agreement", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9953072667121887}, {"text": "Fleiss' \u03ba", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9151837527751923}]}, {"text": "In the experiments we used the binary-class distinction, and disregarded all cases of disagreement.", "labels": [], "entities": []}, {"text": "This final dataset comprises 6 436 sentences: 4 174 literal and 2 262 nonliteral uses across 159 particle verbs and 10 particles.", "labels": [], "entities": []}, {"text": "3 shows the distribution of literal and non-literal sentences across the particles.", "labels": [], "entities": []}, {"text": "Literal Non-Literal: Lit/Non-lit distribution across particles.", "labels": [], "entities": []}, {"text": "In this section, we present a series of binary classification experiments to distinguish literal and non-literal PV usage.", "labels": [], "entities": []}, {"text": "Section 5.1 presents the main experiments comparing our features in a global classification setup, and Section 5.2 presents PVspecific additional experiments that zoom into the role of particle types and into the role of semantically related PVs and BVs.", "labels": [], "entities": []}, {"text": "Section 5.3 provides a qualitative analysis of the features.", "labels": [], "entities": []}, {"text": "We used a random forest with multiple (in our case 100) random decision trees, 4 with each tree voting for an overall classification result.", "labels": [], "entities": []}, {"text": "The unigram information was represented by stacking the output of a multinomial naive bayes text classifier as a single feature into the random forest.", "labels": [], "entities": []}, {"text": "For all machine learning algorithms we relied on the WEKA toolkit.", "labels": [], "entities": [{"text": "WEKA toolkit", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9170449674129486}]}, {"text": "The experiments were performed in two modes, (a) without knowledge of the particle (i.e., the individual particle was not provided as a feature), and (b) with explicit knowledge of the particle.", "labels": [], "entities": []}, {"text": "In this way, we could identify the contribution of the particle.", "labels": [], "entities": []}, {"text": "The classification results are shown in.", "labels": [], "entities": []}, {"text": "We report on the feature type, and on the size 5 of the feature set f . We further present literal and non-literal f-scores F 1 , and accuracy with and without particle knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.99949049949646}]}, {"text": "We compare against the majority baseline (literal).", "labels": [], "entities": []}, {"text": "The right-most columns indicate whether the differences in performance are statistically significant, using the \u03c7 2 Experiments with other classification methods showed similar but inferior performance.", "labels": [], "entities": []}, {"text": "Simple Logistic Regression performed 2nd best.", "labels": [], "entities": []}, {"text": "Remember from Section 4.1 that the unigram information is based on all tokens Statistical significance of differences Acc/Acc + P . The results demonstrate that the classification results across all feature types are significantly better than the majority baseline.", "labels": [], "entities": [{"text": "Statistical significance of differences Acc/Acc + P", "start_pos": 78, "end_pos": 129, "type": "METRIC", "confidence": 0.7640501260757446}]}, {"text": "The single best performing feature type (cf. lines 1-6) is the unigram information; in combination with the particle information (+P ), the distributional PV/BVcontext fit is best.", "labels": [], "entities": [{"text": "BVcontext fit", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.8783234059810638}]}, {"text": "Combining the best feature types (2+4+6) once more improves the results, and ditto when adding noun cluster information.", "labels": [], "entities": []}, {"text": "We can also see that abstractness (AC) ratings outperform imageability (IMG) ratings.", "labels": [], "entities": []}, {"text": "So overall, the best performing feature set successfully combines unigrams that incorporate clusters for noun generalization; abstractness ratings; and PV-specific information regarding the distributional PV/BV-context fit and knowledge about the particle.", "labels": [], "entities": [{"text": "noun generalization", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7623748183250427}]}, {"text": "This setup correctly classifies literal sentences with an f-score of 88.8 and nonliteral sentences with an f-score of 77.3; overall accuracy is 86.8 over a baseline of 64.9.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9993616938591003}]}, {"text": "It is difficult to compare our results against previous approaches on different datasets and in different languages.", "labels": [], "entities": []}, {"text": "Regarding the closest approaches to our work, report an accuracy score of 82.0 using 10-fold cross-validation on a training dataset with a majority baseline of 59.2, combining multiple lexical semantic features on a dataset of 1 609 English subject-verb-object triples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994650483131409}]}, {"text": "Birke and Sarkar (2007) trained a single classifier for each of twentyfive verbs in the English TROFI verb dataset and reported only an average f-score: 64.9 against a The best cluster analysis in our experiments contained 750 noun clusters.", "labels": [], "entities": [{"text": "TROFI verb dataset", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.7015613416830698}]}, {"text": "(2011) obtained an average f-score of 63.9 and additionally report an accuracy score of 73.4 on the same dataset, using abstractness ratings.", "labels": [], "entities": [{"text": "f-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9911409616470337}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9995715022087097}]}, {"text": "In contrast to our work, the two approaches by and treated each group of sentences fora given target verb as a separate learning problem, while we learn one classifier across different verbs.", "labels": [], "entities": []}, {"text": "Our method 4 (AC Ratings) can be considered a German re-implementation of the approach by.", "labels": [], "entities": [{"text": "AC Ratings)", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.7791914741198221}]}, {"text": "In comparison to the results of previous work, our approach can safely be considered state-of-the-art.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main classification results.", "labels": [], "entities": [{"text": "Main classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.860940009355545}]}]}