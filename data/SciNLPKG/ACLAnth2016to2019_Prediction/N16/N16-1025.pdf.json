{"title": [{"text": "Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.8237335085868835}, {"text": "Shift-Reduce Parsing", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9336244463920593}]}], "abstractContent": [{"text": "We present expected F-measure training for shift-reduce parsing with RNNs, which enables the learning of a global parsing model optimized for sentence-level F1.", "labels": [], "entities": []}, {"text": "We apply the model to CCG parsing, where it improves over a strong greedy RNN baseline, by 1.47% F1, yielding state-of-the-art results for shift-reduce CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8448717892169952}, {"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9989964365959167}, {"text": "CCG parsing", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.5244302898645401}]}], "introductionContent": [{"text": "Shift-reduce parsing is a popular parsing paradigm, one reason being the potential for fast parsers based on the linear number of parsing actions needed to analyze a sentence (.", "labels": [], "entities": [{"text": "Shift-reduce parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8123022019863129}]}, {"text": "Recent work has shown that by combining distributed representations and neural network models), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models.", "labels": [], "entities": []}, {"text": "In practice, the most common objective for optimizing neural network shift-reduce parsing models is maximum likelihood.", "labels": [], "entities": [{"text": "neural network shift-reduce parsing", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.6232332736253738}]}, {"text": "In the greedy search setting, the log-likelihood of each target action is maximized during training, and the most likely action is committed to at each step of the parsing process during inference).", "labels": [], "entities": [{"text": "parsing process", "start_pos": 164, "end_pos": 179, "type": "TASK", "confidence": 0.8946042954921722}]}, {"text": "In the beam search setting, show that sentence-level likelihood, together with contrastive learning, can be used to derive a global model which incorporates beam search at both training and inference time, giving significant accuracy gains over a fully greedy model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9974808096885681}]}, {"text": "However, despite the effectiveness of optimizing likelihood, it is often desirable to directly optimize for task-specific metrics, which often leads to higher accuracies fora variety of models and applications.", "labels": [], "entities": []}, {"text": "In this paper, we present a global neural network parsing model, optimized fora task-specific loss based on expected F-measure.", "labels": [], "entities": [{"text": "global neural network parsing", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6064581274986267}, {"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9888389110565186}]}, {"text": "The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 167, "end_pos": 174, "type": "METRIC", "confidence": 0.9951022863388062}]}, {"text": "In contrast to, who optimize a CCG parser for F-measure via softmaxmargin (, we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores.", "labels": [], "entities": []}, {"text": "More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (, combining the benefits of global learning and taskspecific optimization.", "labels": [], "entities": [{"text": "neural beam search parsing", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6840933114290237}]}, {"text": "We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based.", "labels": [], "entities": []}, {"text": "Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions performed on the stack and queue in shift-reduce parsers, following and.", "labels": [], "entities": []}, {"text": "In comparison, our model is a natural extension of the feed-forward architecture in using Elman RNNs.", "labels": [], "entities": []}, {"text": "We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data).", "labels": [], "entities": [{"text": "CCG", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9206569194793701}, {"text": "CCGBank data", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9438191056251526}]}, {"text": "More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed ( \u00a74) -building on the supertagger of, we obtain accuracies higher than the shift-reduce CCG parsers of and.", "labels": [], "entities": [{"text": "RNN parsing", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.632746547460556}]}, {"text": "Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were performed on CCGBank () with the standard split.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9543929100036621}]}, {"text": "We used the C&C supertagger and the RNN supertagger model of as two supertagger baselines.", "labels": [], "entities": [{"text": "C&C supertagger", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9567749500274658}, {"text": "RNN", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9154170155525208}]}, {"text": "For the parsing experiments, the baselines were the shift-reduce CCG parsers of and and the C&C parser of.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9843802452087402}]}, {"text": "To train the RNN parser, we used 10-fold cross validation for both POS tagging and supertagging.", "labels": [], "entities": [{"text": "RNN parser", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.5984997153282166}, {"text": "POS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7187439799308777}]}, {"text": "For both development and test parsing experiments, we used the C&C POS tagger and automatically assigned POS tags.", "labels": [], "entities": []}, {"text": "The BRNN supertagging model was used as the supertagger by all RNN parsing models for both training and testing.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.83577561378479}, {"text": "RNN parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.68834288418293}]}, {"text": "F-score over directed, labeled CCG predicate-argument dependencies was used as the parser evaluation metric, obtained using the script from C&C.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9664377570152283}]}, {"text": "For the BRNN supertagging model, we used identical hyperparameter settings as in.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8323978185653687}]}, {"text": "For all RNN parsing models, the weights were uniformly initialized using the interval [\u22122.0, 2.0], and scaled by their fanin; the hidden layer size was 220, and 50-dimensional embeddings were used for all feature types and scaled Turian embeddings were used () for word embeddings.", "labels": [], "entities": [{"text": "RNN parsing", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.812773734331131}]}, {"text": "We also pretrained CCG lexcial category and POS embeddings by using the GENSIM word2vec implementation.", "labels": [], "entities": [{"text": "GENSIM word2vec implementation", "start_pos": 72, "end_pos": 102, "type": "DATASET", "confidence": 0.8918761809666952}]}, {"text": "The data used for this was obtained by parsing a Wikipedia dump using the   To train all the models, we used a fixed learning rate of 0.0025 and did not truncate the gradients for BPTT, except for training the greedy RNN parsing model where we used a BPTT step size of 9.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.6405776143074036}, {"text": "RNN parsing", "start_pos": 217, "end_pos": 228, "type": "TASK", "confidence": 0.6359482854604721}, {"text": "BPTT step size", "start_pos": 251, "end_pos": 265, "type": "METRIC", "confidence": 0.7528741359710693}]}, {"text": "We applied dropout at the input layer (, with a dropout rate of 0.25 for the supertagger and 0.30 for the parser.", "labels": [], "entities": []}, {"text": "The MaxEnt C&C supertagger uses POS tag features and a tag dictionary, neither of which are used by the RNN supertaggers.", "labels": [], "entities": [{"text": "MaxEnt C&C supertagger", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8486080884933471}]}, {"text": "For all supertaggers, the same set of 425 lexical categories is used).", "labels": [], "entities": []}, {"text": "On the test set, our BRNN supertagger achieves a 1-best accuracy of 93.52%, an absolute improvement of 0.52% over the RNN model, demonstrating the usefulness of contextual information from both input directions.", "labels": [], "entities": [{"text": "1-best", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9263322949409485}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.7722785472869873}]}, {"text": "shows multi-tagging accuracy comparison for the three supertaggers by varying the variablewidth beam probability cut-off value \u03b2 for each supertagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9932558536529541}, {"text": "variablewidth beam probability cut-off value \u03b2", "start_pos": 82, "end_pos": 128, "type": "METRIC", "confidence": 0.8245352705319723}]}, {"text": "The \u03b2 value determines the average number of supertags (ambiguity) assigned to each word by pruning supertags whose probabilities are not within \u03b2 times the probability of the 1-best supertag; for this experiment we used \u03b2 values ranging from 0.09 to 2 \u00d7 10 \u22124 and it can be seen that the BRNN supertagger consistently achieves better accuracies at similar ambiguity levels.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 289, "end_pos": 293, "type": "DATASET", "confidence": 0.9146292209625244}]}], "tableCaptions": [{"text": " Table 2: 1-best supertagging accuracy comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9942969679832458}]}, {"text": " Table 3: The effect on dev F1 by varying the beam size and  supertagger \u03b2 value for the greedy RNN model.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.938612163066864}, {"text": "supertagger \u03b2 value", "start_pos": 61, "end_pos": 80, "type": "METRIC", "confidence": 0.9161012967427572}]}, {"text": " Table 4: Final parsing results on Section 00 and Section 23 (100% coverage). Zhang and Clark (2011)* is a reimplementation  of the original. All speed results (sents/sec) are obtained using Section 23 and precomputation is used for all RNN parsers. LP  (labeled precision); LR (labeled recall); LF (labeled F-score over CCG dependencies); CAT (lexical category assignment accuracy).  All experiments using auto POS.", "labels": [], "entities": [{"text": "coverage", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9479530453681946}, {"text": "precision", "start_pos": 263, "end_pos": 272, "type": "METRIC", "confidence": 0.8105145692825317}, {"text": "recall", "start_pos": 287, "end_pos": 293, "type": "METRIC", "confidence": 0.9478545784950256}, {"text": "CAT", "start_pos": 340, "end_pos": 343, "type": "METRIC", "confidence": 0.8898690938949585}, {"text": "accuracy", "start_pos": 373, "end_pos": 381, "type": "METRIC", "confidence": 0.8174237608909607}]}]}