{"title": [{"text": "Question Answering over Knowledge Base using Factual Memory Networks", "labels": [], "entities": [{"text": "Question Answering over Knowledge Base", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7960287809371949}]}], "abstractContent": [{"text": "In the task of question answering, Memory Networks have recently shown to be quite effective towards complex reasoning as well as scalability, in spite of limited range of topics covered in training data.", "labels": [], "entities": [{"text": "question answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.876019686460495}]}, {"text": "In this paper, we introduce Factual Memory Network, which learns to answer questions by extracting and reasoning over relevant facts from a Knowledge Base.", "labels": [], "entities": [{"text": "Factual Memory Network", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7249963283538818}]}, {"text": "Our system generate distributed representation of questions and KB in same word vector space, extract a subset of initial candidate facts, then try to find a path to answer entity using multi-hop reasoning and refinement.", "labels": [], "entities": []}, {"text": "Additionally, we also improve the run-time efficiency of our model using various computational heuristics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-domain question answering (Open QA) is a longstanding problem that has been studied for decades.", "labels": [], "entities": [{"text": "Open-domain question answering (Open QA)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7933940887451172}]}, {"text": "Early systems took an information retrieval approach, where question answering is reduced to returning passages of text containing an answer as a substring.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7629879713058472}, {"text": "question answering", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7503754496574402}]}, {"text": "Recent advances in constructing largescale knowledge bases (KBs) have enabled new systems that return an exact answer from a KB.", "labels": [], "entities": []}, {"text": "A key challenge in Open QA is to be robust to the high variability found in natural language and the many ways of expressing knowledge in largescale KBs.", "labels": [], "entities": []}, {"text": "Another challenge is to link the natural language of questions with structured semantics of KBs.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel architecture based on memory networks () that can be trained end-to-end using pairs as training set, instead of strong supervision in the form of (question, associated facts in KB) pairs.", "labels": [], "entities": []}, {"text": "The major contributions of this paper are twofold: first, we introduce factual memory networks, which are used to answer questions in natural language (e.g \"Where was Bill Gates born?\") using facts stored in the form of (subject, predicate, object) triplets in a KB (e.g ('Bill Gates', 'place of birth', 'Seattle')).", "labels": [], "entities": []}, {"text": "We evaluate our system against current baselines on various benchmark datasets.", "labels": [], "entities": []}, {"text": "Since KBs can be extremely large, making it computationally inefficient to search overall entities and paths, our second goal of this paper is to increase the efficiency of our model in terms of various performance measures and provide better coverage of relevant facts, by intelligently selecting which nodes to expand.", "labels": [], "entities": []}], "datasetContent": [{"text": "The current dump of Freebase data was downloaded 4 and processed as described before.", "labels": [], "entities": [{"text": "Freebase data", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9783811569213867}]}, {"text": "Our data contained 1.9B triplets.", "labels": [], "entities": []}, {"text": "We used following splits of each evaluation dataset for training, validation and testing, same as (.", "labels": [], "entities": []}, {"text": "WebQuestions (WQ) : SimpleQuestions (SQ) : We also train on automatic questions generated from the KB, which are essential to learn embeddings for the entities not appearing in either WebQuestions or SimpleQuestions.", "labels": [], "entities": []}, {"text": "We generated one training question per fact following the same process as that used in.", "labels": [], "entities": []}, {"text": "The embedding dimension d was chosen 64 and max-norm cutoff was chosen as 4 using validation dataset.", "labels": [], "entities": []}, {"text": "We pre-train our word vectors using method described by () to initialize our embeddings.", "labels": [], "entities": []}, {"text": "We experimented with variations of our model on both test sets.", "labels": [], "entities": []}, {"text": "Specifically, we analyze the effect of question encoding (PE vs BOW), number of Hops and inclusion of pruning/fact-additions (P/FA) in our model.", "labels": [], "entities": [{"text": "BOW", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.8275015354156494}, {"text": "inclusion of pruning/fact-additions (P/FA)", "start_pos": 89, "end_pos": 131, "type": "METRIC", "confidence": 0.6830200582742691}]}, {"text": "In subsequent section, the word 'significant' implies that the results were statistically significant (p < 0.05) with paired T-test", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Evaluation datasets. Acc = Accuracy", "labels": [], "entities": [{"text": "Evaluation datasets", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.6744758933782578}, {"text": "Acc", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9996315240859985}, {"text": "Accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.98971027135849}]}]}