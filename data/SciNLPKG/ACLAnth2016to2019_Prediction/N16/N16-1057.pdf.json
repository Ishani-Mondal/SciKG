{"title": [{"text": "The Sensitivity of Topic Coherence Evaluation to Topic Cardinality", "labels": [], "entities": [{"text": "Topic Cardinality", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7209878712892532}]}], "abstractContent": [{"text": "When evaluating the quality of topics generated by a topic model, the convention is to score topic coherence-either manually or automatically-using the top-N topic words.", "labels": [], "entities": []}, {"text": "This hyper-parameter N , or the cardinality of the topic, is often overlooked and selected arbitrarily.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the impact of this cardinality hyper-parameter on topic coherence evaluation.", "labels": [], "entities": [{"text": "topic coherence evaluation", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.7123367587725321}]}, {"text": "For two automatic topic coherence methodologies, we observe that the correlation with human ratings decreases systematically as the cardinality increases.", "labels": [], "entities": []}, {"text": "More interestingly, we find that performance can be improved if the system scores and human ratings are aggregated over several topic cardinalities before computing the correlation.", "labels": [], "entities": []}, {"text": "In contrast to the standard practice of using a fixed value of N (e.g. N = 5 or N = 10), our results suggest that calculating topic coherence over several different cardi-nalities and averaging results in a substantially more stable and robust evaluation.", "labels": [], "entities": []}, {"text": "We release the code and the datasets used in this research, for reproducibility.", "labels": [], "entities": []}], "introductionContent": [{"text": "Latent Dirichlet Allocation (\"LDA\":) is an approach to document clustering, in which \"topics\" (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (\"LDA\":)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6672761787970861}, {"text": "document clustering", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6978669762611389}]}, {"text": "When the topic model output is to be presented to humans, optimisation of the number of topics is a non-trivial problem.", "labels": [], "entities": []}, {"text": "In the seminal paper of, e.g., the authors showed thatcontrary to expectations -extrinsically measured topic coherence correlates negatively with model perplexity.", "labels": [], "entities": []}, {"text": "They introduced the word intrusion task, whereby a randomly selected \"intruder\" word is injected into the top-N words of a given topic and users are asked to identify the intruder word.", "labels": [], "entities": [{"text": "word intrusion task", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.731962670882543}]}, {"text": "Low reliability in identifying the intruder word indicates low coherence, based on the intuition that the more coherent the topic, the more clearly the intruder word should bean outlier.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9861714243888855}]}, {"text": "Since then, several methodologies have been introduced to automate the evaluation of topic coherence.", "labels": [], "entities": []}, {"text": "found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings.", "labels": [], "entities": [{"text": "PMI", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7979335188865662}]}, {"text": "proposed replacing PMI with conditional probability based on co-document frequency.", "labels": [], "entities": []}, {"text": "showed that coherence can be measured by a classical distributional similarity approach.", "labels": [], "entities": []}, {"text": "More recently, proposed a methodology to automate the word intrusion task directly.", "labels": [], "entities": [{"text": "word intrusion task", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.74691441655159}]}, {"text": "Their results also reveal the differences between these methodologies in their assessment of topic coherence.", "labels": [], "entities": []}, {"text": "A hyper-parameter in all these methodologies is the number of topic words, or its cardinality.", "labels": [], "entities": []}, {"text": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for, N = 5, whereas for, and, N = 10.", "labels": [], "entities": []}, {"text": "The germ of this paper came when using the automatic word intrusion methodology (, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9984001517295837}, {"text": "intruder word prediction", "start_pos": 185, "end_pos": 209, "type": "TASK", "confidence": 0.6394153932730356}]}, {"text": "This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.", "labels": [], "entities": []}, {"text": "To investigate this, we develop anew dataset with human-annotated coherence judgements fora range of cardinality settings (N = {5, 10, 15, 20}).", "labels": [], "entities": []}, {"text": "We experiment with the automatic word intrusion () and discover that correlation with human ratings decreases systematically as cardinality increases.", "labels": [], "entities": []}, {"text": "We also test the PMI methodology (Newman et al., 2010) and make the same observation.", "labels": [], "entities": [{"text": "PMI", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.7754442095756531}]}, {"text": "To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.", "labels": [], "entities": []}, {"text": "This has broad implications for topic model evaluation.", "labels": [], "entities": [{"text": "topic model evaluation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8198115229606628}]}], "datasetContent": [{"text": "To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics fora range of cardinality settings.", "labels": [], "entities": []}, {"text": "Although there are existing datasets with human-annotated coherence scores, these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).", "labels": [], "entities": []}, {"text": "We thus develop anew dataset for this experiment.", "labels": [], "entities": []}, {"text": "Following, we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).", "labels": [], "entities": [{"text": "WIKI", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8613504767417908}, {"text": "NEWS", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.860715389251709}, {"text": "English Gigaword)", "start_pos": 213, "end_pos": 230, "type": "DATASET", "confidence": 0.8596269687016805}]}, {"text": "We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9141692519187927}, {"text": "NEWS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7112769484519958}]}, {"text": "We then generate 300 LDA topics for each of the sub-sampled collection.", "labels": [], "entities": []}, {"text": "There are two primary approaches to assessing topic coherence: injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.", "labels": [], "entities": []}, {"text": "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.", "labels": [], "entities": []}, {"text": "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.", "labels": [], "entities": []}, {"text": "To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent ().", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.963237981001536}]}, {"text": "For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}.", "labels": [], "entities": []}, {"text": "For example, for N = 5, we display the top-5 topic words for coherence judgement.", "labels": [], "entities": [{"text": "coherence judgement", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.6911371350288391}]}, {"text": "For annotation quality control, we embed a bad topic generated using random words into each HIT.", "labels": [], "entities": [{"text": "annotation quality control", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.813608725865682}]}, {"text": "Workers who fail to consistently rate these bad topics low are filtered out.", "labels": [], "entities": []}, {"text": "On average, we collected To understand the impact of cardinality (N ) on topic coherence, we analyse: (a) the mean topic rating for each N, and (b) the pairwise Pearson correlation coefficient between the same topics for different values of N (.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 161, "end_pos": 192, "type": "METRIC", "confidence": 0.918713370958964}]}, {"text": "Coherence decreases slightly but systematically as N increases, suggesting that users find topics less coherent (but marginally more consistently interpretable, as indicated by the slight drop in standard deviation) when more words are presented in a topic.", "labels": [], "entities": []}, {"text": "The strong pairwise correlations, however, indicate that the ratings are relatively stable across different cardinality settings.", "labels": [], "entities": []}, {"text": "To better understand the data, in we present scatter plots of the ratings for all pairwise cardinality settings (where a point represents a topic).", "labels": [], "entities": []}, {"text": "Note the vertical lines for x = 3.0 (cf. the weaker effect of horizontal lines for y = 3.0), in particular for the top 3 plots where we are comparing N = 5 against higher cardinality settings.", "labels": [], "entities": []}, {"text": "This implies that topics that are rated as perfectly coherent (3.0) for N = 5 exhibit some variance in coherence ratings when N increases.", "labels": [], "entities": []}, {"text": "Intuitively, it means that a number of perfectly coherent 5-word topics become less coherent as more words are presented.", "labels": [], "entities": []}, {"text": "proposed an automated approach to the word intrusion task.", "labels": [], "entities": [{"text": "word intrusion task", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7820630967617035}]}, {"text": "The methodology computes pairwise word association features for the top-N words, and trains a support vector regression model to rank the words.", "labels": [], "entities": []}, {"text": "The top-ranked word is then selected as the predicted intruder word.", "labels": [], "entities": []}, {"text": "Note that even though it is supervised, no manual annotation is required as the identity of the true intruder word is known.", "labels": [], "entities": []}, {"text": "Following the original paper, we use as features normalised PMI (NPMI) and two conditional probabilities (CP1 and CP2), computed over the full collection of WIKI (3.3 million articles) and NEWS (1.2 million articles), respectively.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 157, "end_pos": 161, "type": "DATASET", "confidence": 0.9670912027359009}, {"text": "NEWS", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.9519215822219849}]}, {"text": "We use 10-fold cross validation to predict the intruder words for all topics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean rating across different N (numbers in  parentheses denote standard deviations)", "labels": [], "entities": [{"text": "Mean rating", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9834474921226501}]}, {"text": " Table 3: Pearson correlation between system model  precision and human ratings across different values  of N for word intrusion. '  *  ' denotes statistical sig- nificance compared to aggregate correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9278568625450134}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9429171681404114}]}]}