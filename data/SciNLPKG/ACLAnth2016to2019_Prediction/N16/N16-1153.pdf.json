{"title": [{"text": "Semi-supervised Question Retrieval with Gated Convolutions", "labels": [], "entities": [{"text": "Question Retrieval", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7253759503364563}]}], "abstractContent": [{"text": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions.", "labels": [], "entities": [{"text": "Question answering forums", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.876236359278361}]}, {"text": "In this paper, we develop a methodology for finding semantically related questions.", "labels": [], "entities": []}, {"text": "The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented.", "labels": [], "entities": []}, {"text": "We design a recurrent and convo-lutional model (gated convolution) to effectively map questions to their semantic representations.", "labels": [], "entities": []}, {"text": "The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) forums such as Stack Exchange 2 are rapidly expanding and already contain millions of questions.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8957268357276916}]}, {"text": "The expanding scope and coverage of these forums often leads to many duplicate and interrelated questions, resulting in the same questions being answered multiple times.", "labels": [], "entities": []}, {"text": "By identifying similar questions, we can potentially reuse Title: How can I boot Ubuntu from a USB?", "labels": [], "entities": [{"text": "Title", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8345248699188232}]}, {"text": "Body: I bought a Compaq pc with Windows 8 a few months ago and now I want to install Ubuntu but still keep Windows 8.", "labels": [], "entities": []}, {"text": "I tried Webi but when my pc restarts it read ERROR 0x000007b.", "labels": [], "entities": [{"text": "Webi", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9592630863189697}, {"text": "ERROR", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9038023352622986}]}, {"text": "I know that Windows 8 has a thing about not letting you have Ubuntu but I still want to have both OS without actually losing all my data ...", "labels": [], "entities": []}, {"text": "Title: When I want to install Ubuntu on my laptop I'll have to erase all my data.", "labels": [], "entities": []}, {"text": "\"Alonge side windows\" doesnt appear Body: I want to install Ubuntu from a Usb drive.", "labels": [], "entities": []}, {"text": "It says I have to erase all my data but I want to install it alongside Windows 8.", "labels": [], "entities": []}, {"text": "The \"Install alongside windows\" option doesn't appear.", "labels": [], "entities": []}, {"text": "existing answers, reducing response times and unnecessary repeated work.", "labels": [], "entities": []}, {"text": "Unfortunately inmost forums, the process of identifying and referring to existing similar questions is done manually by forum participants with limited, scattered success.", "labels": [], "entities": [{"text": "identifying and referring to existing similar questions", "start_pos": 44, "end_pos": 99, "type": "TASK", "confidence": 0.6841836529118674}]}, {"text": "The task of automatically retrieving similar questions to a given user's question has recently attracted significant attention and has become a testbed for various representation learning approaches ().", "labels": [], "entities": []}, {"text": "However, the task has proven to be quite challenging -for instance, dos report a 22.3% classification accuracy, yielding a 4 percent gain over a simple word matching baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9205428957939148}]}, {"text": "Several factors make the problem difficult.", "labels": [], "entities": []}, {"text": "First, submitted questions are often long and contain extraneous information irrelevant to the main question being asked.", "labels": [], "entities": []}, {"text": "For instance, the first question in Figure 1 pertains to booting Ubuntu using a USB stick.", "labels": [], "entities": []}, {"text": "A large portion of the body contains tangential de-tails that are idiosyncratic to this user, such as references to Compaq pc, Webi and the error message.", "labels": [], "entities": []}, {"text": "Not surprisingly, these features are not repeated in the second question in about a closely related topic.", "labels": [], "entities": []}, {"text": "The extraneous detail can easily confuse simple word-matching algorithms.", "labels": [], "entities": []}, {"text": "Indeed, for this reason, some existing methods for question retrieval restrict attention to the question title only.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8214865922927856}]}, {"text": "While titles (when available) can succinctly summarize the intent, they also sometimes lack crucial detail available in the question body.", "labels": [], "entities": []}, {"text": "For example, the title of the second question does not refer to installation from a USB drive.", "labels": [], "entities": []}, {"text": "The second challenge arises from the noisy annotations.", "labels": [], "entities": []}, {"text": "Indeed, the pairs of questions marked as similar by forum participants are largely incomplete.", "labels": [], "entities": []}, {"text": "Our manual inspection of a sample set of questions from AskUbuntu 3 shows that only 5% of similar pairs have been annotated by the users, with a precision of around 79%.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9977598190307617}]}, {"text": "In this paper, we design a neural network model and an associated training paradigm to address these challenges.", "labels": [], "entities": []}, {"text": "On a high level, our model is used as an encoder to map the title, body, or the combination to a vector representation.", "labels": [], "entities": []}, {"text": "The resulting \"question vector\" representation is then compared to other questions via cosine similarity.", "labels": [], "entities": []}, {"text": "We introduce several departures from typical architectures on a finer level.", "labels": [], "entities": []}, {"text": "In particular, we incorporate adaptive gating in non-consecutive CNNs ( ) in order to focus temporal averaging in these models on key pieces of the questions.", "labels": [], "entities": []}, {"text": "Gating plays a similar role in LSTMs, though LSTMs do not reach the same level of performance in our setting.", "labels": [], "entities": []}, {"text": "Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus.", "labels": [], "entities": []}, {"text": "The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body.", "labels": [], "entities": []}, {"text": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7549484968185425}, {"text": "document summarization", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.6238142848014832}]}, {"text": "The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost.", "labels": [], "entities": []}, {"text": "3 http://askubuntu.com/ We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos.", "labels": [], "entities": [{"text": "AskUbuntu corpus from Stack Exchange", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.9047093868255616}]}, {"text": "During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions.", "labels": [], "entities": []}, {"text": "This clean data is used in two splits, one for development and hyper parameter tuning and another for testing.", "labels": [], "entities": [{"text": "hyper parameter tuning", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.5839863916238149}]}, {"text": "We evaluate our model and the baselines using standard information retrieval (IR) measures such as Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at n (P@n).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 99, "end_pos": 127, "type": "METRIC", "confidence": 0.9739958047866821}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 129, "end_pos": 155, "type": "METRIC", "confidence": 0.9526645938555399}, {"text": "Precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9332290887832642}]}, {"text": "Our full model achieves a MRR of 75.6% and P@1 of 62.0%, yielding 8% absolute improvement over a standard IR baseline, and 4% over standard neural network architectures (including CNNs, LSTMs and GRUs).", "labels": [], "entities": [{"text": "MRR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9808932542800903}, {"text": "P@1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.985476553440094}]}], "datasetContent": [{"text": "Dataset We use the Stack Exchange AskUbuntu dataset used in prior work (dos.", "labels": [], "entities": [{"text": "Stack Exchange AskUbuntu dataset", "start_pos": 19, "end_pos": 51, "type": "DATASET", "confidence": 0.6184367090463638}]}, {"text": "This dataset contains 167,765 unique questions, each consisting of a title and a body 5 , and a set of user-marked similar question pairs.", "labels": [], "entities": []}, {"text": "We provide various statistics from this dataset in.", "labels": [], "entities": []}, {"text": "Gold Standard for Evaluation User-marked similar question pairs on QA sites are often known to be incomplete.", "labels": [], "entities": []}, {"text": "In order to evaluate this in our dataset, we took a sample set of questions paired with 20 candidate questions retrieved by a search engine trained on the AskUbuntu data.", "labels": [], "entities": [{"text": "AskUbuntu data", "start_pos": 155, "end_pos": 169, "type": "DATASET", "confidence": 0.9708963632583618}]}, {"text": "The search engine used is the well-known BM25 model (Robert- We truncate the body section at a maximum of 100 words. son and Zaragoza, 2009).", "labels": [], "entities": [{"text": "BM25", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9542869329452515}]}, {"text": "Our manual evaluation of the candidates showed that only 5% of the similar questions were marked by users, with a precision of 79%.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9994018077850342}]}, {"text": "Clearly, this low recall would not lead to a realistic evaluation if we used user marks as our gold standard.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9994794726371765}]}, {"text": "Instead, we make use of expert annotations carried out on a subset of questions.", "labels": [], "entities": []}, {"text": "Training Set We use user-marked similar pairs as positive pairs in training since the annotations have high precision and do not require additional manual annotations.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9965016841888428}]}, {"text": "This allows us to use a much larger training set.", "labels": [], "entities": []}, {"text": "We use random questions from the corpus paired with each query question pi as negative pairs in training.", "labels": [], "entities": []}, {"text": "We randomly sample 20 questions as negative examples for each pi during each epoch.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Various statistics from our Training, Dev, and Test", "labels": [], "entities": [{"text": "Dev", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7259059548377991}]}, {"text": " Table 2: Comparative results of all methods on the question similarity task. Higher numbers are better. For neural network models,", "labels": [], "entities": []}, {"text": " Table 3: Configuration of neural models. d is the hidden dimen-", "labels": [], "entities": []}, {"text": " Table 3. We  used MRR to identify the best training epoch and  the model configuration. For the same model con- figuration, we report average performance across 5", "labels": [], "entities": [{"text": "MRR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7208561897277832}]}, {"text": " Table 4: Choice of pooling strategies.", "labels": [], "entities": []}, {"text": " Table 5: Comparision between model variants on the test set", "labels": [], "entities": []}]}