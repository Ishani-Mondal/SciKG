{"title": [{"text": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings", "labels": [], "entities": [{"text": "Unsupervised Visual Sense Disambiguation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5638373866677284}]}], "abstractContent": [{"text": "We introduce anew task, visual sense disam-biguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image.", "labels": [], "entities": []}, {"text": "Just as textual word sense disambigua-tion is useful fora wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration.", "labels": [], "entities": [{"text": "textual word sense disambigua-tion", "start_pos": 8, "end_pos": 42, "type": "TASK", "confidence": 0.5748582258820534}, {"text": "visual sense disambiguation", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6840478777885437}, {"text": "image retrieval", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.7431743443012238}, {"text": "image description", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7777983844280243}, {"text": "text illustration", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.8090705573558807}]}, {"text": "We introduce VerSe, anew dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels.", "labels": [], "entities": []}, {"text": "We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual , or multimodal embeddings.", "labels": [], "entities": [{"text": "visual sense disambiguation", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6845420201619467}]}, {"text": "We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while mul-timodal embeddings perform well on unanno-tated images.", "labels": [], "entities": []}, {"text": "We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambigua-tion task.", "labels": [], "entities": []}, {"text": "VerSe is made publicly available and can be downloaded at: https://github.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9296289086341858}]}, {"text": "com/spandanagella/verse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is a widely studied task in natural language processing: given a word and its context, assign the correct sense of the word based on a pre-defined sense inventory.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8124516755342484}]}, {"text": "WSD is useful fora range of NLP tasks, including information retrieval, information extraction, machine translation, content analysis, and lexicography (see for an overview).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8022378385066986}, {"text": "information extraction", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7977034449577332}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8271979689598083}, {"text": "content analysis", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7163250297307968}]}, {"text": "Standard WSD disambiguates words based on their textual context; however, in a multimodal setting (e.g., newspaper articles with photographs), visual context is also available and can be used for disambiguation.", "labels": [], "entities": [{"text": "WSD disambiguates words", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.916726271311442}]}, {"text": "Based on this observation, we introduce anew task, visual sense disambiguation (VSD) for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one depicted in the image.", "labels": [], "entities": [{"text": "visual sense disambiguation (VSD)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.778543214003245}]}, {"text": "While VSD approaches for nouns exist, VSD for verbs is a novel, more challenging task, and related in interesting ways to action recognition in computer vision.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7309103161096573}]}, {"text": "As an example consider the verb play, which can have the senses participate in sport, play on an instrument, and be engaged in playful activity, depending on its visual context, see.", "labels": [], "entities": []}, {"text": "We expect visual sense disambiguation to be useful for multimodal tasks such as image retrieval.", "labels": [], "entities": [{"text": "visual sense disambiguation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6148222188154856}, {"text": "image retrieval", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7469679117202759}]}, {"text": "As an example consider the output of Google Image Search for the query sit: it recognizes that the verb has multiple senses and tries to cluster relevant images.", "labels": [], "entities": []}, {"text": "However, the result does not capture the polysemy of the verb well, and would clearly benefit from VSD (see).", "labels": [], "entities": [{"text": "VSD", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.41671818494796753}]}, {"text": "Visual sense disambiguation has previously been attempted for nouns (e.g., apple can mean fruit or computer), which is a substantially easier task that can be solved with the help of an object detector (.", "labels": [], "entities": [{"text": "Visual sense disambiguation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6549077828725179}]}, {"text": "VSD for nouns is helped by resources such as ImageNet (, a large image database containing 1.4 million images for 21,841 noun synsets and organized according to the WordNet hierarchy.", "labels": [], "entities": []}, {"text": "However, we are not aware of any previous work on VSD for verbs, and no ImageNet for verbs exists.", "labels": [], "entities": []}, {"text": "Not only image retrieval would benefit from VSD for verbs, but also other multimodal tasks that have recently received a lot of interest, such as automatic image description and visual question answering (.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7801075875759125}, {"text": "automatic image description", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.6338597337404887}, {"text": "question answering", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.7163453251123428}]}, {"text": "In this work, we explore the new task of visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image.", "labels": [], "entities": [{"text": "visual sense disambiguation", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7791186769803365}]}, {"text": "We present VerSe, anew dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels.", "labels": [], "entities": []}, {"text": "VerSe contains 3518 images, each annotated with one of 90 verbs, and the OntoNotes sense realized in the image.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9254640340805054}]}, {"text": "We propose an algorithm based on the Lesk WSD algorithm in order to perform unsupervised visual sense disambiguation on our dataset.", "labels": [], "entities": [{"text": "visual sense disambiguation", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.7044147253036499}]}, {"text": "We focus in particular on how to best represent word senses for visual disambiguation, and explore the use of textual, visual, and multimodal embeddings.", "labels": [], "entities": []}, {"text": "Textual embeddings fora given image can be constructed over object labels or image descriptions, which are available as gold-standard in the COCO and TUHOI datasets, or can be computed automatically using object detectors and image description models.", "labels": [], "entities": [{"text": "COCO and TUHOI datasets", "start_pos": 141, "end_pos": 164, "type": "DATASET", "confidence": 0.7405759915709496}]}, {"text": "Our results show that textual embeddings perform best when gold-standard textual annotations are available, while multimodal embeddings perform best when automatically generated object labels are used.", "labels": [], "entities": []}, {"text": "Interestingly, we find that automatically generated image descriptions result in inferior performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Verbs Acts Images Sen Des PPMI 2 24 4800 N N Stanford 40 Actions ( 33 40 9532 N N PASCAL 2012 9 11 4588 N N 89 Actions ( 36 89 2038 N N TUHOI ( -2974 10805 N N COCO-a ( 140 162 10000 NY HICO (  Most of the datasets relevant for verb sense disambiguation were created by the computer vision community for the task of human action recognition (see for an overview).", "labels": [], "entities": [{"text": "PASCAL 2012 9", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9116170803705851}, {"text": "verb sense disambiguation", "start_pos": 228, "end_pos": 253, "type": "TASK", "confidence": 0.7763892014821371}, {"text": "human action recognition", "start_pos": 316, "end_pos": 340, "type": "TASK", "confidence": 0.6762141287326813}]}, {"text": "These datasets are annotated with a limited number of actions, where an action is conceptualized as verb-object pair: ride horse, ride bicycle, play tennis, play guitar, etc.", "labels": [], "entities": []}, {"text": "Verb sense ambiguity is ignored in almost all action recognition datasets, which misses important generalizations: for instance, the actions ride horse and ride bicycle represent the same sense of ride and thus share visual, textual, and conceptual features, while this is not the case for play tennis and play guitar.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7207423746585846}]}, {"text": "This is the issue we address by creating a dataset with explicit sense labels.", "labels": [], "entities": []}, {"text": "VerSe is built on top of two existing datasets, TUHOI and COCO.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9136168360710144}, {"text": "TUHOI", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.6237224340438843}, {"text": "COCO", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8422789573669434}]}, {"text": "The Trento Universal HumanObject Interaction (TUHOI) dataset contains 10,805 images covering 2974 actions.", "labels": [], "entities": [{"text": "Trento Universal HumanObject Interaction (TUHOI) dataset", "start_pos": 4, "end_pos": 60, "type": "DATASET", "confidence": 0.7224360480904579}]}, {"text": "Action (humanobject interaction) categories were annotated using crowdsourcing: each image was labeled by multiple annotators with a description in the form of a verb or a verb-object pair.", "labels": [], "entities": []}, {"text": "The main drawback of TUHOI is that 1576 out of 2974 action categories occur only once, limiting its usefulness for VSD.", "labels": [], "entities": [{"text": "TUHOI", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8253570795059204}, {"text": "VSD", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9643369317054749}]}, {"text": "The Microsoft Common Objects in Context (COCO) dataset is very popular in the language/vision community, as it consists of over 120k images with extensive annotation, including labels for 91 object categories and five descriptions per image.", "labels": [], "entities": [{"text": "Microsoft Common Objects in Context (COCO) dataset", "start_pos": 4, "end_pos": 54, "type": "DATASET", "confidence": 0.5617320040861765}]}, {"text": "COCO contains no explicit action annotation, but verbs and verb phrases can be extracted from the descriptions.", "labels": [], "entities": [{"text": "COCO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8309265375137329}]}, {"text": "(But note that not all the COCO images depict actions.)", "labels": [], "entities": []}, {"text": "The recently created Humans Interacting with Common Objects (HICO) dataset is conceptually similar to VerSe.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9071925282478333}]}, {"text": "It consists of 47774 images annotated with 111 verbs and 600 human-object interaction categories.", "labels": [], "entities": []}, {"text": "Unlike other existing datasets, HICO uses sense-based distinctions: actions are denoted by sense-object pairs, rather than by verb-object pairs.", "labels": [], "entities": []}, {"text": "HICO doesn't aim for complete coverage, but restricts itself to the top three WordNet senses of a verb.", "labels": [], "entities": [{"text": "HICO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8454779982566833}]}, {"text": "The dataset would be suitable for performing visual sense disambiguation, but has so far not been used in this way.", "labels": [], "entities": [{"text": "visual sense disambiguation", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6725354790687561}]}, {"text": "We want to build an unsupervised visual sense disambiguation system, i.e., a system that takes an image and a verb and returns the correct sense of the verb.", "labels": [], "entities": []}, {"text": "As discussed in Section 2.1, most exist- ing datasets are not suitable for this task, as they do not include word sense annotation.", "labels": [], "entities": []}, {"text": "We therefore develop our own dataset with gold-standard sense annotation.", "labels": [], "entities": []}, {"text": "The Verb Sense (VerSe) dataset is based on COCO and TUHOI and covers 90 verbs and around 3500 images.", "labels": [], "entities": [{"text": "Verb Sense (VerSe) dataset", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.45883672932783764}, {"text": "TUHOI", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.6961255669593811}]}, {"text": "VerSe serves two main purposes: (1) to show the feasibility of annotating images with verb senses (rather than verbs or actions); (2) to function as test bed for evaluating automatic visual sense disambiguation methods.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7610734105110168}, {"text": "automatic visual sense disambiguation", "start_pos": 173, "end_pos": 210, "type": "TASK", "confidence": 0.7029226869344711}]}, {"text": "Verb Selection Action recognition datasets often use a limited number of verbs (see).", "labels": [], "entities": [{"text": "Verb Selection Action recognition", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8322481065988541}]}, {"text": "We addressed this issue by using images that come with descriptions, which in the case of action images typically contain verbs.", "labels": [], "entities": []}, {"text": "The COCO dataset includes images in the form of sentences, the TUHOI dataset is annotated with verbs or prepositional verb phrases fora given object (e.g., sit on chair), which we use in lieu of descriptions.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9344499409198761}, {"text": "TUHOI dataset", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.8782734870910645}]}, {"text": "We extracted all verbs from all the descriptions in the two datasets and then selected those verbs that have more than one sense in the OntoNotes dictionary, which resulted in 148 verbs in total (94 from COCO and 133 from TUHOI).", "labels": [], "entities": [{"text": "OntoNotes dictionary", "start_pos": 136, "end_pos": 156, "type": "DATASET", "confidence": 0.9228654503822327}]}, {"text": "Depictability Annotation A verb can have multiple senses, but not all of them maybe depictable, e.g., senses describing cognitive and perception processes.", "labels": [], "entities": []}, {"text": "Consider two senses of touch: make physical contact is depictable, whereas affect emotionally describes a cognitive process and is not depictable.", "labels": [], "entities": []}, {"text": "We therefore need to annotate the synsets of a verb as depictable or non-depictable.", "labels": [], "entities": []}, {"text": "Amazon Mechanical Turk (AMT) workers were presented with the definitions of all the synsets of a verb, along with ex-  amples, as given by OntoNotes.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8158199389775594}, {"text": "OntoNotes", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.9394579529762268}]}, {"text": "An example for this annotation is shown in.", "labels": [], "entities": []}, {"text": "We used OntoNotes instead of WordNet, as WordNet senses are very fine-grained and potentially make depictability and sense annotation (see below) harder.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.956562340259552}]}, {"text": "Granularity issues with WordNet for text-based WSD are well documented.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7991147637367249}]}, {"text": "OntoNotes lists a total of 921 senses for our 148 target verbs.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9073315858840942}]}, {"text": "For each synset, three AMT workers selected all depictable senses.", "labels": [], "entities": []}, {"text": "The majority label was used as the gold standard for subsequent experiments.", "labels": [], "entities": []}, {"text": "This resulted in a 504 depictable senses.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement (ITA) as measured by Fleiss' Kappa was 0.645.", "labels": [], "entities": [{"text": "Inter-annotator agreement (ITA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8962460875511169}, {"text": "Fleiss' Kappa", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7118683159351349}]}, {"text": "Sense Annotation We then annotated a subset of the images in COCO and TUHOI with verb senses.", "labels": [], "entities": [{"text": "COCO", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8677356839179993}, {"text": "TUHOI", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8142778277397156}]}, {"text": "For every image we assigned the verb that occurs most frequently in the descriptions for that image (for TUHOI, the descriptions are verb-object pairs, see above).", "labels": [], "entities": []}, {"text": "However, many verbs are represented by only a few images, while a few verbs are represented by a large number of images.", "labels": [], "entities": []}, {"text": "The datasets therefore show a Zipfian distribution of linguistic units, which is expected and has been observed previously for COCO (.", "labels": [], "entities": [{"text": "COCO", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.8047937154769897}]}, {"text": "For sense annotation, we selected only verbs for which either COCO or TUHOI contained five or more images, resulting in a set of 90 verbs (out of the total 148).", "labels": [], "entities": [{"text": "sense annotation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8314798176288605}, {"text": "TUHOI", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8489600419998169}]}, {"text": "All images for these verbs were included, giving us a dataset of 3518 images: 2340 images for 82 verbs from COCO and 1188 images for 61 verbs from TUHOI (some verbs occur in both datasets).", "labels": [], "entities": []}, {"text": "These image-verb pairs formed the basis for sense annotation.", "labels": [], "entities": [{"text": "sense annotation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7659591734409332}]}, {"text": "AMT workers were presented with the image and all the depictable OntoNotes senses of the associated verb.", "labels": [], "entities": [{"text": "AMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6852150559425354}]}, {"text": "The workers had to chose the sense of the verb that was instantiated in the image (or \"none of the above\", in the case of irrelevant images).", "labels": [], "entities": []}, {"text": "Annotators were given sense definitions and examples, as for the depictability annotation (see).", "labels": [], "entities": []}, {"text": "For every image-verb pair, five annotators performed the sense annotation task.", "labels": [], "entities": []}, {"text": "A total of 157 annotators participated, reaching an inter-annotator agreement of 0.659 (Fleiss' Kappa).", "labels": [], "entities": [{"text": "Fleiss' Kappa)", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.7128811875979105}]}, {"text": "Out of 3528 images, we discarded 18 images annotated with \"none of the above\", resulting in a set of 3510 images covering 90 verbs and 163 senses.", "labels": [], "entities": []}, {"text": "We present statistics of our dataset in; we group the verbs into motion verbs and non-motion verb using Levin (1993) classes.", "labels": [], "entities": []}, {"text": "Along with the unsupervised experiments we investigated the performance of textual and visual representations of images in a simplest supervised setting.", "labels": [], "entities": []}, {"text": "We trained logistic regression classifiers for sense prediction by dividing the images in VerSe dataset into train and test splits.", "labels": [], "entities": [{"text": "sense prediction", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.8985981941223145}, {"text": "VerSe dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9351293444633484}]}, {"text": "To train the classifiers we selected all the verbs which has atleast 20 images annotated and has at least two senses in VerSe.", "labels": [], "entities": [{"text": "VerSe", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.9035285115242004}]}, {"text": "This resulted in 19 motion verbs and 19 non-motion verbs.", "labels": [], "entities": []}, {"text": "Similar to our unsupervised experiments we explore multimodal features by using both textual and visual features for classification (similar to concatenation in unsupervised experiments).", "labels": [], "entities": []}, {"text": "In we report accuracy scores for 19 motion verbs using a supervised logistic regression classifier and for comparison we also report the scores of our proposed unsupervised algorithm for both GOLD and PRED setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9989321827888489}, {"text": "GOLD", "start_pos": 192, "end_pos": 196, "type": "TASK", "confidence": 0.4756506383419037}]}, {"text": "Similarly in we report the accuracy scores for 19 non-motion verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996387958526611}]}, {"text": "We observe that all supervised classifiers for both motion and non-motion verbs performing better than first sense baseline.", "labels": [], "entities": []}, {"text": "Similar to our findings using an unsupervised approach we find that inmost cases multimodal features obtained using concatenating textual and visual features has outperformed textual or visual features alone especially in the PRED setting which is arguably the more realistic scenario.", "labels": [], "entities": []}, {"text": "We observe that the features from PRED image descriptions showed better results for nonmotion verbs for both supervised and unsupervised approaches whereas PRED object features showed better results for motion verbs.", "labels": [], "entities": []}, {"text": "We also observe that supervised classifiers outperform most frequent sense for motion verbs and for non-motion verbs our scores match with most frequent sense heuristic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of VerSe with existing action  recognition datasets. Acts (actions) are verb-object  pairs; Sen indicates whether sense ambiguity is ex- plicitly handled; Des indicates whether image de- scriptions are included.", "labels": [], "entities": []}, {"text": " Table 2: Overview of VerSe dataset divided into  motion and non-motion verbs; Depct: depictable  senses; ITA: inter-annotator agreement.", "labels": [], "entities": [{"text": "VerSe dataset", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8340145647525787}, {"text": "Depct", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9200729727745056}, {"text": "ITA", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9254285097122192}]}, {"text": " Table 3: Accuracy scores for motion and non-motion verbs using for different types of sense and image  representations (O: object labels, C: image descriptions, CNN: image features, FS: first sense heuristic,  MFS: most frequent sense heuristic). Configurations that performed better than FS in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976581335067749}, {"text": "FS", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.9522233605384827}, {"text": "FS", "start_pos": 290, "end_pos": 292, "type": "METRIC", "confidence": 0.9194662570953369}]}, {"text": " Table 4: Accuracy scores for motion verbs for both  supervised and unsupervised approaches using dif- ferent types of sense and image representation fea- tures.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980100989341736}]}]}