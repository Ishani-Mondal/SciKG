{"title": [{"text": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "labels": [], "entities": [{"text": "Morphological Inflection Generation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7993491888046265}]}], "abstractContent": [{"text": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation.", "labels": [], "entities": [{"text": "Morphological inflection generation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8909579714139303}]}, {"text": "We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7821862399578094}]}, {"text": "Our model is language independent and can be trained in both supervised and semi-supervised settings.", "labels": [], "entities": []}, {"text": "We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.7825257480144501}]}], "introductionContent": [{"text": "Inflection is the word-formation mechanism to express different grammatical categories such as tense, mood, voice, aspect, person, gender, number and case.", "labels": [], "entities": []}, {"text": "Inflectional morphology is often realized by the concatenation of bound morphemes (prefixes and suffixes) to a root form or stem, but nonconcatenative processes such as ablaut and infixation are found in many languages as well.", "labels": [], "entities": []}, {"text": "shows the possible inflected forms of the German stem Kalb (calf) when it is used in different cases and numbers.", "labels": [], "entities": []}, {"text": "The inflected forms are the result of both ablaut (e.g., a\u2192\u00e4) and suffixation (e.g., +ern).", "labels": [], "entities": []}, {"text": "Inflection generation is useful for reducing data sparsity in morphologically complex languages.", "labels": [], "entities": [{"text": "Inflection generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.756815105676651}]}, {"text": "For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6912454863389333}]}, {"text": "Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem ().", "labels": [], "entities": []}, {"text": "Modeling inflection generation has also been used to improve language modeling (), identification of multi-word expressions), among other applications.", "labels": [], "entities": [{"text": "Modeling inflection generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8808606863021851}, {"text": "language modeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7567074596881866}, {"text": "identification of multi-word expressions", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.8158495724201202}]}, {"text": "The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology.", "labels": [], "entities": []}, {"text": "Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile, and the composed transducers can be impractically large.", "labels": [], "entities": []}, {"text": "As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction.", "labels": [], "entities": []}, {"text": "However, these impose either assumptions about the set of possible morphological processes inflection generation kalb k\u00e4lber case=nominative number=plural (e.g. affixation) or require careful feature engineering.", "labels": [], "entities": []}, {"text": "In this paper, we present a model of inflection generation based on a neural network sequence to sequence transducer.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7459745407104492}]}, {"text": "The root form is represented as sequence of characters, and this is the input to an encoder-decoder architecture ().", "labels": [], "entities": []}, {"text": "The model transforms its input to a sequence of output characters representing the inflected form ( \u00a74).", "labels": [], "entities": []}, {"text": "Our model makes no assumptions about morphological processes, and our features are simply the individual characters.", "labels": [], "entities": []}, {"text": "The model is trained on pairs of root form and inflected forms obtained from inflection tables extracted from Wiktionary.", "labels": [], "entities": []}, {"text": "We improve the supervised model with unlabeled data, by integrating a character language model trained on the vocabulary of the language.", "labels": [], "entities": []}, {"text": "Our experiments show that the model achieves better or comparable results to state-of-the-art methods on the benchmark inflection generation tasks ( \u00a75).", "labels": [], "entities": [{"text": "benchmark inflection generation tasks", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.7128417044878006}]}, {"text": "For example, our model is able to learn longrange relations between character sequences in the string aiding the inflection generation process required by Finnish vowel harmony ( \u00a76), which helps it obtain the current best results in that language.", "labels": [], "entities": [{"text": "Finnish vowel harmony", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.5524334112803141}]}], "datasetContent": [{"text": "We now conduct experiments using the described models.", "labels": [], "entities": []}, {"text": "Note that not all previously published models present results on all settings, and thus we compare our results to them wherever appropriate.", "labels": [], "entities": []}, {"text": "Across all models described in this paper, we use the following hyperparameters.", "labels": [], "entities": []}, {"text": "In both the encoder and decoder models we use single layer LSTMs with the hidden vector of length 100.", "labels": [], "entities": []}, {"text": "The length of character vectors is the size of character vocabulary according to each dataset.", "labels": [], "entities": []}, {"text": "The parameters are regularized with 2 , with the regularization constant 10 \u22125 . The number of models for ensembling are k = 5.", "labels": [], "entities": []}, {"text": "Models are trained for at most 30 epochs and the model with best result on development set is selected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Features used to rerank the inflected outputs. x, y", "labels": [], "entities": []}, {"text": " Table 2. We use pairwise ranking opti- mization (PRO) to learn the reranking model", "labels": [], "entities": []}, {"text": " Table 4: Individual form prediction accuracy for factored su-", "labels": [], "entities": [{"text": "Individual form prediction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7395883599917094}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9692312479019165}]}, {"text": " Table 5: Individual form prediction accuracy for joint super-", "labels": [], "entities": [{"text": "Individual form prediction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7472593784332275}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9320470690727234}]}, {"text": " Table 6: Individual form prediction accuracy for factored", "labels": [], "entities": [{"text": "Individual form prediction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7578758001327515}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.957580029964447}, {"text": "factored", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.5418152213096619}]}, {"text": " Table 7: Avg. accuracy across datasets of the encoder-decoder,", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9984159469604492}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9572975635528564}]}]}