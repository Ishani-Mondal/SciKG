{"title": [{"text": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Recently there has been a lot of interest in learning common representations for multiple views of data.", "labels": [], "entities": []}, {"text": "Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions).", "labels": [], "entities": []}, {"text": "In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V 1 and V 2) but parallel data is available between each of these views and a pivot view (V 3).", "labels": [], "entities": []}, {"text": "We propose a model for learning a common representation for V 1 , V 2 and V 3 using only the parallel data available between V 1 V 3 and V 2 V 3.", "labels": [], "entities": []}, {"text": "The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as abridge between them.", "labels": [], "entities": []}, {"text": "There are two specific downstream applications that we focus on (i) transfer learning between languages L 1 ,L 2 ,...,L n using a pivot language Land (ii) cross modal access between images and a language L 1 using a pivot language L 2.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art performance in multilingual document classification on the publicly available multilingual TED corpus and promising results in multilingual multimodal retrieval on anew dataset created and released as apart of this work.", "labels": [], "entities": [{"text": "multilingual document classification", "start_pos": 51, "end_pos": 87, "type": "TASK", "confidence": 0.6336427728335062}, {"text": "TED corpus", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.7544896006584167}, {"text": "multilingual multimodal retrieval", "start_pos": 163, "end_pos": 196, "type": "TASK", "confidence": 0.5934057335058848}]}], "introductionContent": [{"text": "The proliferation of multilingual and multimodal content online has ensured that multiple views of the same data exist.", "labels": [], "entities": []}, {"text": "For example, it is common to find the same article published in multiple languages online in multilingual news articles, multilingual wikipedia articles, etc.", "labels": [], "entities": []}, {"text": "Such multiple views can even belong to different modalities.", "labels": [], "entities": []}, {"text": "For example, images and their textual descriptions are two views of the same entity.", "labels": [], "entities": []}, {"text": "Similarly, audio, video and subtitles of a movie are multiple views of the same entity.", "labels": [], "entities": []}, {"text": "Learning common representations for such multiple views of data will help in several downstream applications.", "labels": [], "entities": []}, {"text": "For example, learning a common representation for images and their textual descriptions could help in finding images which match a given textual description.", "labels": [], "entities": []}, {"text": "Further, such common representations can also facilitate transfer learning between views.", "labels": [], "entities": []}, {"text": "For example, a document classifier trained on one language (view) can be used to classify documents in another language by representing documents of both languages in a common subspace.", "labels": [], "entities": []}, {"text": "Existing approaches to common representation learning except) typically require parallel data between all views.", "labels": [], "entities": [{"text": "common representation learning", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.7314276297887167}]}, {"text": "However, in many realworld scenarios such parallel data may not be available.", "labels": [], "entities": []}, {"text": "For example, while there are many publicly available datasets containing images and their corresponding English captions, it is very hard to find datasets containing images and their corresponding captions in Russian, Dutch, Hindi, Urdu, etc.", "labels": [], "entities": []}, {"text": "In this work, we are interested in addressing such scenarios.", "labels": [], "entities": []}, {"text": "More specifically, we consider scenarios where we haven different views but parallel data is only available between each of these views, and a pivot view.", "labels": [], "entities": []}, {"text": "In particular, there is no parallel data available between the non-pivot views.", "labels": [], "entities": []}, {"text": "To this end, we propose Bridge Correlational Neural Networks (Bridge CorrNets) which learn aligned representations across multiple views using a pivot view.", "labels": [], "entities": []}, {"text": "We build on the work of () but unlike their model, which only addresses scenarios where direct parallel data is available between two views, our model can work for n(\u22652) views even when no parallel data is available between all of them.", "labels": [], "entities": []}, {"text": "Our model only requires parallel data between each of these n views and a pivot view.", "labels": [], "entities": []}, {"text": "During training, our model maximizes the correlation between the representations of the pivot view and each of then views.", "labels": [], "entities": []}, {"text": "Intuitively, the pivot view ensures that similar entities across different views get mapped close to each other since the model would learn to map each of them close to the corresponding entity in the pivot view.", "labels": [], "entities": []}, {"text": "We evaluate our approach using two downstream applications.", "labels": [], "entities": []}, {"text": "First, we employ our model to facilitate transfer learning between multiple languages using English as the pivot language.", "labels": [], "entities": []}, {"text": "For this, we do an extensive evaluation using 110 sourcetarget language pairs and clearly show that we outperform the current state-of-the art approach).", "labels": [], "entities": []}, {"text": "Second, we employ our model to enable cross modal access between images and French/German captions using English as the pivot view.", "labels": [], "entities": [{"text": "cross modal access between images and French/German captions", "start_pos": 38, "end_pos": 98, "type": "TASK", "confidence": 0.5654061168432236}]}, {"text": "For this, we created a test dataset consisting of images and their captions in French and German in addition to the English captions which were publicly available.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this task of retrieving images given French/German captions (and vice versa) without direct parallel training data between them has not been addressed in the past.", "labels": [], "entities": [{"text": "retrieving images given French/German captions", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.6328725389071873}]}, {"text": "Even on this task we report promising results.", "labels": [], "entities": []}, {"text": "Code and data used in this paper can be downloaded from http: //sarathchandar.in/bridge-corrnet.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the two datasets that we used for our experiments.", "labels": [], "entities": []}, {"text": "The MSCOCO dataset 2 contains images and their English captions.", "labels": [], "entities": [{"text": "MSCOCO dataset 2", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9668813149134318}]}, {"text": "On an average there are 5 captions per image.", "labels": [], "entities": []}, {"text": "The standard train/valid/test splits for this dataset are also available online.", "labels": [], "entities": []}, {"text": "However, the reference captions for the images in the test split are not provided.", "labels": [], "entities": []}, {"text": "Since we need such reference captions for evaluations, we create anew train/valid/test of this dataset.", "labels": [], "entities": []}, {"text": "Specifically, we take 80K images from the standard train split and 40K images from the standard valid split.", "labels": [], "entities": []}, {"text": "We then randomly split the merged 120K images into train(118K), validation (1K) and test set (1K).", "labels": [], "entities": []}, {"text": "We then create a multilingual version of the test data by collecting French and German translations for all the 5 captions for each image in the test set.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to do this.", "labels": [], "entities": []}, {"text": "We used the CrowdFlower platform and solicited one French and one German translation for each of the 5000 captions using native speakers.", "labels": [], "entities": []}, {"text": "We got each translation verified by 3 annotators.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.963013231754303}]}, {"text": "We restricted the geographical location of annotators based on the target language.", "labels": [], "entities": []}, {"text": "We found that roughly 70% of the French translations and 60% of the German translations were marked as correct by a majority of the verifiers.", "labels": [], "entities": []}, {"text": "On further inspection with the help of in-house annotators, we found that the errors were mainly syntactic and the content words are translated correctly inmost of the cases.", "labels": [], "entities": []}, {"text": "Since none of the approaches described in this work rely on syntax, we decided to use all the 5000 translations as test data.", "labels": [], "entities": []}, {"text": "This multilingual image caption test data (MIC test data) will be made publicly available 3 and will hopefully assist further research in this area.", "labels": [], "entities": [{"text": "multilingual image caption test data (MIC test data)", "start_pos": 5, "end_pos": 57, "type": "DATASET", "confidence": 0.7358847886323929}]}, {"text": "From the TED corpus described earlier, we consider English transcriptions and their translations in 11 languages, viz., Arabic, German, Spanish, French, Italian, Dutch, Polish, Portuguese (Brazilian), Roman, Russian and Turkish.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8632709980010986}]}, {"text": "Following the setup of Hermann and Blunsom (2014b), we consider the task of cross language learning between each of the 11 C 2 non-English language pairs.", "labels": [], "entities": [{"text": "cross language learning", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7164301673571268}]}, {"text": "The task is to classify documents in a language when no labeled training data is available in this language but training data is available in another language.", "labels": [], "entities": []}, {"text": "This involves the following steps: 1.", "labels": [], "entities": []}, {"text": "Train classifier: Consider one language as the source language and the remaining 10 languages as target languages.", "labels": [], "entities": []}, {"text": "Train a document classifier using the labeled data of the source language, where each training document is represented using the hidden representation computed using a trained Bridge Corrnet model.", "labels": [], "entities": []}, {"text": "As in ( we used an averaged perceptron trained for 10 epochs as the classifier for all our experiments.", "labels": [], "entities": []}, {"text": "The train split provided by http://sarathchandar.in/bridge-corrnet 2014b) is used for training.", "labels": [], "entities": [{"text": "bridge-corrnet 2014b", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.8424731492996216}]}, {"text": "2. Cross language classification: For every target language, compute a hidden representation for every document in its test set using Bridge CorrNet.", "labels": [], "entities": [{"text": "Cross language classification", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.7499019900957743}]}, {"text": "Now use the classifier trained in the previous step to classify this document.", "labels": [], "entities": []}, {"text": "The test split provided by) is used for testing.", "labels": [], "entities": []}, {"text": "In this experiment, we are interested in retrieving images given their captions in French (or German) and vice versa.", "labels": [], "entities": []}, {"text": "However, for training we do not have any parallel data containing images and their French (or German) captions.", "labels": [], "entities": []}, {"text": "Instead, we have the following datasets: (i) a dataset Z 1 containing images and their English captions and (ii) a dataset Z 2 containing English and their parallel French (or German) documents.", "labels": [], "entities": []}, {"text": "For Z 1 , we use the training split of MSCOCO dataset which contains 118K images and their English captions (see Section 4.2).", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9485243856906891}]}, {"text": "For Z 2 , we use the English-French (or German) parallel documents from the train split of the TED corpus (see Section 4.1).", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8901257514953613}]}, {"text": "We use English as the pivot language and train Bridge Corrnet using Z = {Z 1 , Z 2 } to learn common representations for images, English text and French (or German) text.", "labels": [], "entities": []}, {"text": "For text, we use bag-of-words representation and for image, we use the 4096 (fc6) representation got from a pretrained ConvNet (BVLC Reference CaffeNet ().", "labels": [], "entities": [{"text": "BVLC Reference CaffeNet", "start_pos": 128, "end_pos": 151, "type": "DATASET", "confidence": 0.9054456551869711}]}, {"text": "We learn hidden representations of size D = 200 by training Bridge Corrnet for 20 epochs using stochastic gradient descent with mini-batches of size 20.", "labels": [], "entities": []}, {"text": "Each mini-batch contains data from only one of the Z i s.", "labels": [], "entities": []}, {"text": "For the task of retrieving captions given an image, we consider the 1000 images in our test set (see section 4.2) as queries.", "labels": [], "entities": []}, {"text": "The 5000 French (or German) captions corresponding to these images (5 per image) are considered as documents.", "labels": [], "entities": []}, {"text": "The task is then to retrieve the relevant captions for each image.", "labels": [], "entities": []}, {"text": "We represent all the captions and images in the common space as computed using Bridge Corrnet.", "labels": [], "entities": []}, {"text": "For a given query, we rank all the captions based on the Euclidean distance between the representation of the image and the caption.", "labels": [], "entities": []}, {"text": "For the task of retrieving images given a caption, we simply reverse the role of the captions and images.", "labels": [], "entities": []}, {"text": "In other words, each of the 5000 captions is treated as a query and the 1000 images are treated as documents.", "labels": [], "entities": []}, {"text": "\u03bb was tuned to each task using a training/validation split.", "labels": [], "entities": []}, {"text": "For the task of retrieving French/German captions given an image, \u03bb was tuned using the performance on the validation set for retrieving French (or German) sentences fora given English sentence.", "labels": [], "entities": [{"text": "retrieving French/German captions given an image", "start_pos": 16, "end_pos": 64, "type": "TASK", "confidence": 0.8339806422591209}]}, {"text": "For the other task, \u03bb was tuned using the performance on the validation set for retrieving images, given English captions.", "labels": [], "entities": []}, {"text": "We do not use any image-French/German parallel data for tuning the hyperparameters.", "labels": [], "entities": []}, {"text": "We use recall@k as the performance metric and compare the following methods in 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.996333122253418}]}, {"text": "En-Image CorrNet: This is the CorrNet model trained using only Z 1 as defined earlier in this section.", "labels": [], "entities": []}, {"text": "The task is to retrieve English captions fora given image (or vice versa).", "labels": [], "entities": [{"text": "retrieve English captions", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.625381757815679}]}, {"text": "This gives us an idea about the performance we could expect if direct parallel data is available between images and their captions in some language.", "labels": [], "entities": []}, {"text": "We used the publicly available implementation of CorrNet provided by).", "labels": [], "entities": [{"text": "CorrNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8956397771835327}]}, {"text": "2. Bridge CorrNet: This is the Bridge CorrNet model trained using Z 1 and Z 2 as defined earlier in this section.", "labels": [], "entities": []}, {"text": "The task is to retrieve French (or German) captions fora given image (or vice versa).", "labels": [], "entities": [{"text": "retrieve French (or German) captions", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.5774761821542468}]}, {"text": "3. Bridge MAE: The Multimodal Autoencoder (MAE) proposed by) was the only competing model which was easily extendable to the bridge case.", "labels": [], "entities": [{"text": "Bridge MAE", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.6245284229516983}]}, {"text": "We train their model using Z 1 and Z 2 to minimize a suitably modified objective function.", "labels": [], "entities": []}, {"text": "We then use the representations learned to retrieve French (or German) captions fora given image (or vice versa).", "labels": [], "entities": []}, {"text": "4. 2-CorrNet: Here, we train two individual CorrNets using Z 1 and Z 2 respectively.", "labels": [], "entities": []}, {"text": "For the task of retrieving images given a French (or German) caption we first find its nearest English caption using the Fr-En (or De-En) CorrNet.", "labels": [], "entities": [{"text": "retrieving images given a French (or German) caption", "start_pos": 16, "end_pos": 68, "type": "TASK", "confidence": 0.572129687666893}, {"text": "Fr-En (or De-En) CorrNet", "start_pos": 121, "end_pos": 145, "type": "DATASET", "confidence": 0.7623893519242605}]}, {"text": "We then use this English caption to retrieve images using the En-Image CorrNet.", "labels": [], "entities": []}, {"text": "Similarly, for retrieving captions given an image we use the En-Image CorrNet followed by the En-Fr (or En-De) CorrNet.", "labels": [], "entities": []}, {"text": "5. CorrNet + MT: Here, we train an En-Image CorrNet using Z 1 and an Fr/De-En MT system 4 using Z 2 . For the task of retrieving images given a French (or German) caption we translate the caption to English using the MT system.", "labels": [], "entities": []}, {"text": "We then use this English caption to retrieve images using the En-Image CorrNet.", "labels": [], "entities": []}, {"text": "For retrieving captions given images, we first translate all the 5000 French (or Germam) captions to English.", "labels": [], "entities": []}, {"text": "We then embed these English translations (documents) and images (queries) in the com-: Performance of different models for image to caption (I to C) and caption to image (C to I) retrieval mon space computed using Image-En CorrNet and do a retrieval as explained earlier.", "labels": [], "entities": []}, {"text": "6. Random: A random image is returned for the given caption (and vice versa).", "labels": [], "entities": [{"text": "Random", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9479783177375793}]}, {"text": "From, we observe that CorrNet + MT is a very strong competitor and gives the best results.", "labels": [], "entities": [{"text": "CorrNet + MT", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.7567103505134583}]}, {"text": "The main reason for this is that over the years MT has matured enough for language pairs such as Fr-En and De-En and it can generate almost perfect translations for short sentences (such as captions).", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9394376277923584}, {"text": "Fr-En", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9095597863197327}]}, {"text": "In fact, the results for this method are almost comparable to what we could have hoped for if we had direct parallel data between Fr-Images and DeImages (as approximated by the first row in the table which reports cross-modal retrieval results between En-Images using direct parallel data between them for training).", "labels": [], "entities": []}, {"text": "However, we would like to argue that learning a joint embedding for multiple views instead of having multiple pairwise systems is a more elegant solution and definitely merits further attention.", "labels": [], "entities": []}, {"text": "Further, a \"translation system\" may not be available when we are dealing with modalities other than text (for example, there are no audio-to-video translation systems).", "labels": [], "entities": []}, {"text": "In such cases, BridgeCorrNet could still be employed.", "labels": [], "entities": [{"text": "BridgeCorrNet", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.9549353122711182}]}, {"text": "In this context, the performance of BridgeCorrNet is definitely promising and shows that a model which jointly learns representations for multiple views can perform better than methods which learn pair-wise common representations (2-CorrNet).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1-scores for TED corpus document classification results when training and testing on two languages that do not share", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984025359153748}, {"text": "TED corpus document classification", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8941857516765594}]}, {"text": " Table 2: F1-scores for TED corpus document classification results when training and testing on two languages that do not share", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9982091188430786}, {"text": "TED corpus document classification", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8932790458202362}]}, {"text": " Table 3: : F1-scores on the TED corpus document classification task when training and evaluating on the same language. Results", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9988021850585938}, {"text": "TED corpus document classification task", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.7722482562065125}]}, {"text": " Table 4: Performance of different models for image to caption (I to C) and caption to image (C to I) retrieval", "labels": [], "entities": [{"text": "caption to image (C to I) retrieval", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.5770933628082275}]}]}