{"title": [], "abstractContent": [{"text": "Semantic Role Labeling (SRL) captures semantic roles (or participants) such as agent, patient, and theme associated with verbs from the text.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8187003036340078}]}, {"text": "While it provides important intermediate semantic representations for many traditional NLP tasks (such as information extraction and question answering), it does not capture grounded semantics so that an artificial agent can reason, learn, and perform the actions with respect to the physical environment.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7219169735908508}, {"text": "question answering", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7299647927284241}]}, {"text": "To address this problem, this paper extends traditional SRL to grounded SRL where arguments of verbs are grounded to participants of actions in the physical world.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9836656451225281}]}, {"text": "By integrating language and vision processing through joint inference, our approach not only grounds explicit roles, but also grounds implicit roles that are not explicitly mentioned in language descriptions.", "labels": [], "entities": []}, {"text": "This paper describes our empirical results and discusses challenges and future directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments).", "labels": [], "entities": []}, {"text": "For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc.", "labels": [], "entities": []}, {"text": "Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles; Gildea and Jurafsky,.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8126841684182485}]}, {"text": "For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source.", "labels": [], "entities": []}, {"text": "SRL captures important semantic representations for actions associated with verbs, which have shown beneficial fora variety of applications such as information extraction) and question answering.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9086803197860718}, {"text": "information extraction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.8010386526584625}, {"text": "question answering", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.9122745096683502}]}, {"text": "However, the traditional SRL is not targeted to represent verb semantics that are grounded to the physical world so that artificial agents can truly understand the ongoing activities and (learn to) perform the specified actions.", "labels": [], "entities": [{"text": "SRL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9585309028625488}]}, {"text": "To address this issue, we propose anew task on grounded semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.6123668352762858}]}, {"text": "shows an example of grounded SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7623319625854492}]}, {"text": "The sentence the woman takes out a cucumber from the refrigerator describes an activity in a visual scene.", "labels": [], "entities": []}, {"text": "The semantic role representation from linguistic processing (including implicit roles such as destination) is first extracted and then grounded to tracks of visual entities as shown in the video.", "labels": [], "entities": []}, {"text": "For example, the verb phrase takeout is grounded to a trajectory of the right hand.", "labels": [], "entities": []}, {"text": "The role agent is grounded to the person who actually does the take-out action in the visual scene (track 1) ; the patient is grounded to the cucumber taken out (track 3); and the source is grounded to the refrigerator (track 4).", "labels": [], "entities": []}, {"text": "The implicit role of destination (which is not explicitly mentioned in the language description) is grounded to the cutting board (track 5).", "labels": [], "entities": []}, {"text": "To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information.", "labels": [], "entities": []}, {"text": "In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain () in our investigation.", "labels": [], "entities": []}, {"text": "We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.6698002417882284}]}, {"text": "Compared to previous works on language grounding, our work presents several contributions.", "labels": [], "entities": [{"text": "language grounding", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7713494002819061}]}, {"text": "First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world.", "labels": [], "entities": []}, {"text": "By incorporating semantic role information, our approach has led to better grounding performance.", "labels": [], "entities": []}, {"text": "Second, most previous works only focused on a small number of verbs with limited activities.", "labels": [], "entities": []}, {"text": "We base our investigation on a wider range of verbs and in a much more complex domain where object recognition and tracking are notably more difficult.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8091443479061127}]}, {"text": "Third, our work results in additional layers of annotation to part of the TACoS dataset.", "labels": [], "entities": [{"text": "TACoS dataset", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9217209815979004}]}, {"text": "This annotation captures the structure of actions informed by semantic roles from the video.", "labels": [], "entities": []}, {"text": "The annotated data is available for download . It will provide a benchmark for future work on grounded SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.6668965220451355}]}], "datasetContent": [{"text": "We conducted our investigation based on a subset of the TACoS corpus (.", "labels": [], "entities": [{"text": "TACoS corpus", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9057115912437439}]}, {"text": "This dataset contains a set of video clips paired with natural language descriptions related to several cooking tasks.", "labels": [], "entities": []}, {"text": "The natural language descriptions were collected through crowd-sourcing on top of the \"MPII Cooking Composite Activities\" video corpus (.", "labels": [], "entities": [{"text": "MPII Cooking Composite Activities\" video corpus", "start_pos": 87, "end_pos": 134, "type": "DATASET", "confidence": 0.8645865065710885}]}, {"text": "In this paper, we selected two tasks \"cutting cucumber\" and \"cutting bread\" as our experimental data.", "labels": [], "entities": []}, {"text": "Each task has 5 videos showing how different people perform the same task.", "labels": [], "entities": []}, {"text": "Each video is segmented to a sequence of video clips where each video clip comes with one or more language descriptions.", "labels": [], "entities": []}, {"text": "The original TACoS dataset does not contain annotation for grounded semantic roles.", "labels": [], "entities": [{"text": "TACoS dataset", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8840798735618591}]}, {"text": "To support our investigation and evaluation, we had made a significant effort adding the following annotations.", "labels": [], "entities": []}, {"text": "For each video clip, we annotated the objects' bounding boxes, their tracks, and their labels (cucumber, cutting board, etc.) using VATIC ().", "labels": [], "entities": []}, {"text": "On average, each video clip is annotated with 15 tracks of objects.", "labels": [], "entities": []}, {"text": "For each sentence, we annotated the ground truth parsing structure and the semantic frame for each verb.", "labels": [], "entities": [{"text": "ground truth parsing", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7406114935874939}]}, {"text": "The ground truth parsing structure is the representation of dependency parsing results.", "labels": [], "entities": [{"text": "ground truth parsing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7046090960502625}, {"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7206722795963287}]}, {"text": "The semantic frame of a verb includes slots, fillers, and their groundings.", "labels": [], "entities": []}, {"text": "For each semantic role (including both explicit roles and implicit roles) of a given verb, we also annotated the ground truth grounding in terms of the object tracks and locations.", "labels": [], "entities": []}, {"text": "In total, our annotated dataset includes 976 pairs of video clips and corresponding sentences, 1094 verbs occurrences, and 3593 groundings of semantic roles.", "labels": [], "entities": []}, {"text": "To check annotation agreement, 10% of the data was annotated by two annotators.", "labels": [], "entities": []}, {"text": "The kappa statistics is 0.83.", "labels": [], "entities": [{"text": "kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9521594047546387}]}, {"text": "From this dataset, we selected 11 most frequent verbs (i.e., get, take, wash, cut, rinse, slice, place, peel, put, remove, open) in our current investigation for the following reasons.", "labels": [], "entities": []}, {"text": "First, they are used more frequently so that we can have sufficient samples of each verb to learn the model.", "labels": [], "entities": []}, {"text": "Second, they cover different types of actions: some are more related to the change of the state such as take, and some are more related to the process such as wash.", "labels": [], "entities": []}, {"text": "As it turns out, these verbs also have different semantic role patterns as shown in.", "labels": [], "entities": []}, {"text": "The patient roles of all these verbs are explicitly specified.", "labels": [], "entities": []}, {"text": "This is not surprising as all these verbs are transitive verbs.", "labels": [], "entities": []}, {"text": "There is a large variation for other roles.", "labels": [], "entities": []}, {"text": "For example, for the verb take, the destination is rarely specified by lingories.", "labels": [], "entities": []}, {"text": "This is partly due to the fact that some verb occurrences take more than one objects as grounding to a role.", "labels": [], "entities": []}, {"text": "It is also possibly due to missed/duplicated annotation for some categories.", "labels": [], "entities": []}, {"text": "guistic expressions (i.e., only 2 instances), however it can be inferred from the video.", "labels": [], "entities": []}, {"text": "For the verb cut, the location and the tool are also rarely specified by linguistic expressions.", "labels": [], "entities": []}, {"text": "Nevertheless, these implicit roles contribute to the overall understanding of actions and should also be grounded too.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our approach, we compare it with two approaches.", "labels": [], "entities": []}, {"text": "\u2022 Baseline: To identify the grounding for each semantic role, the first baseline chooses the most possible track based on the object type conditional distribution given the verb and semantic role.", "labels": [], "entities": []}, {"text": "If an object type corresponds to multiple tracks in the video, e.g., multiple drawers or knives, we then randomly select one of the tracks as grounding.", "labels": [], "entities": []}, {"text": "We ran this baseline method five times and reported the average performance.", "labels": [], "entities": []}, {"text": "\u2022 Tellex (2011): The second approach we compared with is based on an implementation).", "labels": [], "entities": [{"text": "Tellex (2011)", "start_pos": 2, "end_pos": 15, "type": "DATASET", "confidence": 0.837745726108551}]}, {"text": "The difference is that they don't explicitly model fine-grained semantic role information.", "labels": [], "entities": []}, {"text": "For a better comparison, we map the grounding results from this approach to different explicit semantic roles according to the SRL annotation of the sentence.", "labels": [], "entities": []}, {"text": "Note that this approach is notable to ground implicit roles.", "labels": [], "entities": []}, {"text": "More specifically, we compare these two approaches with two variations of our system: \u2022 GSRL wo V : The CRF model using linguistic features and track label features (described in Section 4.2).", "labels": [], "entities": []}, {"text": "\u2022 GSRL: The full CRF model using linguistic features, track label features, and visual features(described in Section 4.2).", "labels": [], "entities": [{"text": "GSRL", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.8381439447402954}]}, {"text": "Both automated language processing and vision processing are error-prone.", "labels": [], "entities": [{"text": "vision processing", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7666337192058563}]}, {"text": "To further understand the limitations of grounded SRL, we compare performance under different configurations along the two dimensions: (1) the CRF structure is built upon annotated ground-truth language parsing versus automated language parsing; (2) object tracking and labeling is based on annotation versus automated processing.", "labels": [], "entities": [{"text": "SRL", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8944568634033203}, {"text": "ground-truth language parsing", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.7298567295074463}, {"text": "object tracking and labeling", "start_pos": 250, "end_pos": 278, "type": "TASK", "confidence": 0.7178389877080917}]}, {"text": "These lead to four different experimental configurations.", "labels": [], "entities": []}, {"text": "For experiments that are based on annotated object tracks, we can simply use the traditional accuracy that directly measures the percentage of grounded tracks that are correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9966433048248291}]}, {"text": "However, for experiments using automated tracking, evaluation can be difficult as tracking itself poses significant challenges.", "labels": [], "entities": []}, {"text": "The grounding results (to tracks) cannot be directly compared with the annotated ground-truth tracks.", "labels": [], "entities": []}, {"text": "To address this problem, we have defined anew metric called approximate accuracy.", "labels": [], "entities": [{"text": "approximate", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.880283772945404}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7795939445495605}]}, {"text": "This metric is motivated by previous computer vision work that evaluates tracking performance ().", "labels": [], "entities": []}, {"text": "Suppose the ground truth grounding fora role is track gt and the predicted grounding is track pt.", "labels": [], "entities": []}, {"text": "The two tracks gt and pt are often not the same (although may have some overlaps).", "labels": [], "entities": []}, {"text": "Suppose the number of frames in the video clip is k.", "labels": [], "entities": []}, {"text": "For each frame, we calculate the distance between the centroids of these two tracks.", "labels": [], "entities": []}, {"text": "If their distance is below a predefined threshold, we consider the two tracks overlap in this frame.", "labels": [], "entities": []}, {"text": "We consider the grounding is correct if the ratio of the overlapping frames between gt and pt exceeds 50%.", "labels": [], "entities": []}, {"text": "As can be seen, this is a lenient and an approximate measure of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9986724853515625}]}], "tableCaptions": [{"text": " Table 1: Statistics for a set of verbs and their semantic roles", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results based on annotated language parsing.", "labels": [], "entities": [{"text": "language parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7069550901651382}]}, {"text": " Table 3: Evaluation results based on automated language parsing.", "labels": [], "entities": [{"text": "language parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.6980827152729034}]}]}