{"title": [{"text": "Non-decreasing Sub-modular Function for Comprehensible Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.5683844089508057}]}], "abstractContent": [{"text": "Extractive summarization techniques typically aim to maximize the information coverage of the summary with respect to the original corpus and report accuracies in ROUGE scores.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8923025131225586}, {"text": "ROUGE", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.9603093862533569}]}, {"text": "Automated text summarization techniques should consider the dimensions of comprehensibility, coherence and readability.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6846936494112015}]}, {"text": "In the current work, we identify the discourse structure which provides the context for the creation of a sentence.", "labels": [], "entities": []}, {"text": "We leverage the information from the structure to frame a monotone (non-decreasing) sub-modular scoring function for generating comprehensible summaries.", "labels": [], "entities": []}, {"text": "Our approach improves the overall quality of comprehensibility of the summary in terms of human evaluation and gives sufficient content coverage with comparable ROUGE score.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 161, "end_pos": 172, "type": "METRIC", "confidence": 0.9735862910747528}]}, {"text": "We also formulate a metric to measure summary comprehensibility in terms of Contextual Independence of a sentence.", "labels": [], "entities": [{"text": "Contextual Independence", "start_pos": 76, "end_pos": 99, "type": "METRIC", "confidence": 0.9433045089244843}]}, {"text": "The metric is shown to be representative of human judgement of text comprehensi-bility.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extractive summarization techniques aim at selecting a subset of sentences from a corpus which can be a representative of the original corpus in the target summary space.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8954745233058929}]}, {"text": "Extensive work has been done on extractive summarization aimed at maximizing the information coverage of the summary with respect to the original corpus and accuracies have been reported in terms of ROUGE score.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.7136871516704559}, {"text": "accuracies", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.9902030229568481}, {"text": "ROUGE score", "start_pos": 199, "end_pos": 210, "type": "METRIC", "confidence": 0.9786765873432159}]}, {"text": "But, if a sentence is heavily dependent on its previous context in the original corpus, placing it in the summary in a different context can render a wrong inference to the reader of the summary.", "labels": [], "entities": []}, {"text": "The main intuition behind our approach begins with a crucial question about the linguistic nature of a text.", "labels": [], "entities": []}, {"text": "Is text a bag of words every time?", "labels": [], "entities": []}, {"text": "Psycholinguistic studies suggest that local coherence plays a vital role in inference formation while reading a text.", "labels": [], "entities": [{"text": "inference formation while reading a text", "start_pos": 76, "end_pos": 116, "type": "TASK", "confidence": 0.7825325826803843}]}, {"text": "Local coherence is undoubtedly necessary for global coherence and has received considerable attention in Computational Linguistics.", "labels": [], "entities": []}, {"text": "Linguistically, every sentence is uttered not in isolation but within a context in a given discourse.", "labels": [], "entities": []}, {"text": "To make a coherent reading, sentences use various discourse connectives that bind one sentence with another.", "labels": [], "entities": []}, {"text": "A set of such structurally related sentences forms a Locally Coherent Discourse Unit (hereafter referred to as LDU).", "labels": [], "entities": []}, {"text": "In the current work, we suggest that it is important to leverage this structural coherence to improve the comprehensibility of the generated summary.", "labels": [], "entities": []}, {"text": "It should be noted that the concept of LDU is different from the elementary discourse units (EDUs) as discussed in Rhetorical Structure Theory(.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.7048562169075012}]}, {"text": "RST is interested in describing the structure of a text in terms of relations that hold between parts of text.", "labels": [], "entities": [{"text": "RST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9267677664756775}]}, {"text": "Any part of the text such as nuclear discourse clauses, satellite discourse clauses can be treated as elementary discourse units.", "labels": [], "entities": []}, {"text": "In contrast, with LDUs, we are interested in identifying which sequence of sentences makeup one extractable unit that has to betaken together for an extractive summarization task.", "labels": [], "entities": [{"text": "extractive summarization task", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.6322682996590933}]}, {"text": "The most recent works on extractive summarization can be generalized into three steps given below:- 3.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8310734033584595}]}, {"text": "Selecting a set of sentences which maximizes the total score as the summary for target text During this process, a sentence is severed from its original context in the corpus and is eventually placed in a different context.", "labels": [], "entities": []}, {"text": "If the level of dependence of the sentence on its context is high, then it has a higher chance to deliver an erroneous reading, when placed out of context.", "labels": [], "entities": []}, {"text": "To understand the issue, look at the below sentences in a summary.", "labels": [], "entities": []}, {"text": "The baroque was a style of art that existed from the late 1500s to the middle of the 18th century.", "labels": [], "entities": []}, {"text": "In 16th century, their ruler laced the old Gothic art with a newer baroque style.", "labels": [], "entities": []}, {"text": "A resultant summary which contains the above two sentences one after another, can be a topically relevant summary.", "labels": [], "entities": []}, {"text": "Both talk about 'Baroque style', 'art','century' etc and could possibly be optimal candidates for the target summary.", "labels": [], "entities": []}, {"text": "Nevertheless, it invokes an incomprehensible reading fora human reader because the subject of the second sentence is 'their ruler' whose anaphora is not resolved in the context.", "labels": [], "entities": []}, {"text": "Hence it is important that we do not consider a document as mere sequence of sentences or bag of words but rather as a series of LDUs.", "labels": [], "entities": []}, {"text": "In spite of all attempts for developing abstractive summarization techniques to mimic the human way of summarizing a text, extractive techniques still standout as more reliable for practical purposes.", "labels": [], "entities": [{"text": "summarizing a text", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8802710771560669}]}, {"text": "So it is inevitable to enhance the extractive summarization techniques along the dimensions of readability, coherence and comprehensibility.", "labels": [], "entities": []}, {"text": "The problem of extractive summarization can be formulated as a function maximization problem in the space of all candidate summaries as follows.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6123942136764526}]}, {"text": "(1) where F is an objective function, S * is the summary which maximizes F with an adopted optimization method, S is a candidate summary, c i is the cost of selecting a sentence i into summary, b is the upper bound on the total cost and V is the set of total number of sentences in the corpus.", "labels": [], "entities": []}, {"text": "The current work is inspired by two of the previous works namely (Lin and Bilmes, 2011) and GFlow (.", "labels": [], "entities": [{"text": "GFlow", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.8450489044189453}]}, {"text": "Lin & Bilmes observed that if the objective function to score candidate summaries is monotone sub-modular, a greedy approach can ensure the approximation of the summary at the global maximum by a factor of 0.632 as follows.", "labels": [], "entities": []}, {"text": "wher\u00ea S is the summary obtained from monotone sub-modular function F and S opt is the summary at the global maximum of F.", "labels": [], "entities": []}, {"text": "G-Flow aimed at generating coherent summaries by constructing a sentence-level discourse graph for the entire corpus and the information from the graph is utilized to quantify the coherence of candidate summaries.", "labels": [], "entities": []}, {"text": "Ina short summary space, the sentences which structurally depend on other sentences are not encouraged.", "labels": [], "entities": []}, {"text": "So the summaries are more comprehensible than those produced by the systems which blindly aim at achieving maximum content coverage.", "labels": [], "entities": []}, {"text": "The need to create a discourse graph can be a big hurdle to scale the summarizer to large data sets.", "labels": [], "entities": []}, {"text": "Also the scoring function of G-Flow is not monotone sub-modular and cannot guarantee the approximation of optimum summary as per the relation 2.", "labels": [], "entities": [{"text": "summary", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.7699328064918518}]}, {"text": "The space of current work is to establish a scheme for comprehensible summarization with a monotone sub-modular objective function.", "labels": [], "entities": []}, {"text": "Within the scope of this paper, when we say comprehensibility we mean how much relevant structural context does each sentence have for better conveyability of the discourse intended by the summary.", "labels": [], "entities": []}, {"text": "In the current work, we try to assign a score for each sentence based on its level of contextual independence (discussed in subsequent sections).", "labels": [], "entities": []}, {"text": "The particular score is combined as a linear component in the candidate summary scoring function of Lin and Bilmes () to score sentences.", "labels": [], "entities": []}, {"text": "While adding the third component, monotone sub-modularity of the scoring function is not disturbed since the contextual independence of individual sentences is constant with respect to a given corpus.", "labels": [], "entities": []}, {"text": "We observed an improvement in systemgenerated summary in terms of human evaluation for comprehensibility while maintaining a reasonable level of content coverage in terms of ROUGE score.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 174, "end_pos": 185, "type": "METRIC", "confidence": 0.9792469143867493}]}, {"text": "We framed a comprehensibility index to represent the level of comprehensibility of a system generated summary using contextual independence score of individual sentences.", "labels": [], "entities": []}, {"text": "Comprehensibility index for the generated summary is the average contextual independence score of a sentence in the summary.", "labels": [], "entities": [{"text": "Comprehensibility index", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.9360799193382263}, {"text": "contextual independence score", "start_pos": 65, "end_pos": 94, "type": "METRIC", "confidence": 0.6973994473616282}]}, {"text": "We verified, through human evaluators, whether the comprehensibility index is actually representative of the human comprehensibility.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have to separately evaluate the accuracy of LDU identification, improvement of comprehensibility of system-generated summary when Contextual Independence is used as a bias term in summarization process and how much reliable the comprehensibility index is, as a metric to estimate the comprehensibility of the summary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9995360374450684}, {"text": "LDU identification", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7524113655090332}, {"text": "summarization process", "start_pos": 183, "end_pos": 204, "type": "TASK", "confidence": 0.8793675303459167}]}, {"text": "To evaluate the comprehensibility index, we have taken into consideration, the summaries generated by Lin& Bilmes () and G-Flow systems() for each of the corpus in DUC-2004 dataset.", "labels": [], "entities": [{"text": "DUC-2004 dataset", "start_pos": 164, "end_pos": 180, "type": "DATASET", "confidence": 0.9864232540130615}]}, {"text": "The four linguists participated in another evaluation experiment where we conveyed them about comprehensibility judgement like in previous experiment.", "labels": [], "entities": []}, {"text": "For each corpus in the dataset, they were made to choose the more comprehensible of the two summaries generated by GFlow and Lin & Bilmes provided in a random order.", "labels": [], "entities": []}, {"text": "For the evaluation of the comprehensibility index given by equation 8, we define the accuracy of Comprehensibility Index as the percentage of times the Compreh(S) value was greater for summaries which are chosen by humans unambiguously.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9996408224105835}, {"text": "Comprehensibility Index", "start_pos": 97, "end_pos": 120, "type": "METRIC", "confidence": 0.7812493443489075}, {"text": "Compreh(S) value", "start_pos": 152, "end_pos": 168, "type": "METRIC", "confidence": 0.9454271912574768}]}, {"text": "The details are provided in table 5.", "labels": [], "entities": []}, {"text": "While considering both the experiments involving human evaluators, the agreement between the evaluators was 0.79 in terms of Cohen's kappa measure().", "labels": [], "entities": []}, {"text": "Considering the subjective nature of annotation, we believe % of times G-Flow was chosen 67% % of times Lin & Bilmes was chosen 13% Ambiguous 20% Accuracy of Compreh(S) 79% Average Compreh(S) for G-Flow 0.73 Average Compreh(S) for Lin& Bilmes 0.54: Comprehensibility Index Evaluation Details this is a reasonably good measure of how informative the human judgements were.", "labels": [], "entities": [{"text": "Ambiguous", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9956686496734619}, {"text": "Accuracy of Compreh(S)", "start_pos": 146, "end_pos": 168, "type": "METRIC", "confidence": 0.8911557793617249}, {"text": "Average Compreh(S)", "start_pos": 173, "end_pos": 191, "type": "METRIC", "confidence": 0.9018990755081177}, {"text": "Comprehensibility Index Evaluation", "start_pos": 249, "end_pos": 283, "type": "METRIC", "confidence": 0.8562828898429871}]}], "tableCaptions": [{"text": " Table 2. The positive classification represents the  contextual independence of a sentence.", "labels": [], "entities": []}, {"text": " Table 4: Preference of summary based on compre- hensibility", "labels": [], "entities": [{"text": "Preference", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9890788197517395}, {"text": "compre- hensibility", "start_pos": 41, "end_pos": 60, "type": "METRIC", "confidence": 0.92094486951828}]}]}