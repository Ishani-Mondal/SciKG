{"title": [], "abstractContent": [{"text": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.", "labels": [], "entities": [{"text": "multi-source machine translation", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.6530374884605408}]}, {"text": "Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9861410856246948}, {"text": "attention-based neural translation", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.648572713136673}]}], "introductionContent": [{"text": "Kay (2000) points out that if a document is translated once, it is likely to be translated again and again into other languages.", "labels": [], "entities": []}, {"text": "This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.7811359524726867}]}, {"text": "The translation system now has two strings as input, which can reduce ambiguity via \"triangulation\" (Kay's term).", "labels": [], "entities": []}, {"text": "For example, the normally ambiguous English word \"bank\" maybe more easily translated into French in the presence of a second, German input string containing the word \"Flussufer\" (river bank).", "labels": [], "entities": []}, {"text": "Och and Ney (2001) describe such a multi-source MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9506884813308716}]}, {"text": "They first train separate bilingual MT systems F \u2192E, G\u2192E, etc.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9652097225189209}]}, {"text": "At runtime, they separately translate input strings f and g into candidate target strings e 1 and e 2 , then select the best one of the two.", "labels": [], "entities": []}, {"text": "A typical selection factor is the product of the system scores.", "labels": [], "entities": []}, {"text": "revisits such factors in the context of log-linear models and Bleu score, while re-rank F \u2192E n-best lists using n-gram precision with respect to G\u2192E translations.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9332442879676819}]}, {"text": "exploits hypothesis selection in multi-source MT to expand available corpora, via co-training.", "labels": [], "entities": [{"text": "hypothesis selection", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8035449385643005}, {"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.8268358707427979}]}, {"text": "Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators.", "labels": [], "entities": []}, {"text": "These methods include confusion networks, source-side string combination (, and median strings.", "labels": [], "entities": []}, {"text": "The above work all relies on base MT systems trained on bilingual data, using traditional methods.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9816874861717224}]}, {"text": "This follows early work in sentence alignment ( and word alignment, which exploited trilingual text, but did not build trilingual models.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7806522846221924}, {"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7674688696861267}]}, {"text": "Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive.", "labels": [], "entities": []}, {"text": "In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously.", "labels": [], "entities": []}, {"text": "We view this as a kind of multi-tape transduction) with two input tapes and one output tape.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We train a P(e|f, g) model directly on trilingual data, and we use it to decode anew source string pair (f, g) into target string e.", "labels": [], "entities": []}, {"text": "\u2022 We show positive Bleu improvements over strong single-source baselines.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9327012896537781}]}, {"text": "\u2022 We show that improvements are best when the two source languages are more distant from each other.", "labels": [], "entities": []}, {"text": "We are able to achieve these results using", "labels": [], "entities": []}], "datasetContent": [{"text": "We use English, French, and German data from a subset of the WMT 2014 dataset ().", "labels": [], "entities": [{"text": "WMT 2014 dataset", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9627877076466879}]}, {"text": "shows statistics for our training set.", "labels": [], "entities": []}, {"text": "For development, we use the 3000 sentences supplied by WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9751396775245667}]}, {"text": "For testing, we use a 1503-line trilingual subset of the WMT test set.", "labels": [], "entities": [{"text": "WMT test set", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9130652149518331}]}, {"text": "For the single-source models, we follow the training procedure used in, but with 15 epochs and halving the learning rate every full epoch after the 10th epoch.", "labels": [], "entities": []}, {"text": "We also re-scale the normalized gradient when norm > 5.", "labels": [], "entities": []}, {"text": "For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in.", "labels": [], "entities": []}, {"text": "The dropout rate is 0.2, the initial parameter range is [-0.1, +0.1], and the learning rate is 1.0.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.946510910987854}, {"text": "learning rate", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9669860899448395}]}, {"text": "For the normal and multi-source attention models, we adjust these parameters to 0.3, [-0.08, +0.08], and 0.7, respectively, to adjust for overfitting.", "labels": [], "entities": []}, {"text": "shows our results for target English, with source languages French and German.", "labels": [], "entities": []}, {"text": "We see that the Basic combination method yields a +4.8 Bleu improvement over the strongest single-source, attention-based system.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9895210266113281}]}, {"text": "It also improves Bleu by +2.2 over the non-attention baseline.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9992109537124634}]}, {"text": "The ChildSum method gives improvements of +4.4 and +1.4.", "labels": [], "entities": []}, {"text": "We confirm that two copies of the same French input yields no BLEU improvement.", "labels": [], "entities": [{"text": "French input", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9075405597686768}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9994751811027527}]}, {"text": "shows the action of the multi-attention model during decoding.", "labels": [], "entities": []}, {"text": "When our source languages are English and French), we observe smaller BLEU gains (up to +1.1).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9995476603507996}]}, {"text": "This is evidence that the more distinct the source languages, the better they disambiguate each other.", "labels": [], "entities": []}, {"text": "Source 1: UNK Aspekte sind ebenfalls wichtig .", "labels": [], "entities": [{"text": "UNK Aspekte sind ebenfalls wichtig", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.9339988350868225}]}], "tableCaptions": []}