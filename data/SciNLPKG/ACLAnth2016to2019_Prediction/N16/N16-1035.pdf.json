{"title": [{"text": "Top-down Tree Long Short-Term Memory Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.6858956217765808}]}, {"text": "In this paper we develop Tree Long Short-Term Memory (TREELSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence.", "labels": [], "entities": []}, {"text": "TREELSTM defines the probability of a sentence by estimating the generation probability of its dependency tree.", "labels": [], "entities": [{"text": "TREELSTM", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7727710008621216}]}, {"text": "At each time step, anode is generated based on the representation of the generated sub-tree.", "labels": [], "entities": []}, {"text": "We further enhance the modeling power of TREELSTM by explicitly representing the correlations between left and right dependents.", "labels": [], "entities": [{"text": "TREELSTM", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8595259785652161}]}, {"text": "Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art.", "labels": [], "entities": [{"text": "MSR sentence completion challenge", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.9175878614187241}]}, {"text": "We also report results on dependency parsing reranking achieving competitive performance.", "labels": [], "entities": [{"text": "dependency parsing reranking", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8323353926340739}]}], "introductionContent": [{"text": "Neural language models have been gaining increasing attention as a competitive alternative to n-grams.", "labels": [], "entities": []}, {"text": "The main idea is to represent each word using a real-valued feature vector capturing the contexts in which it occurs.", "labels": [], "entities": []}, {"text": "The conditional probability of the next word is then modeled as a smooth function of the feature vectors of the preceding words and the next word.", "labels": [], "entities": []}, {"text": "In essence, similar representations are learned for words found in similar contexts resulting in similar predictions for the next word.", "labels": [], "entities": []}, {"text": "Previous approaches have mainly employed feed-forward ( and recurrent neural networks () in order to map the feature vectors of the context words to the distribution for the next word.", "labels": [], "entities": []}, {"text": "Recently, RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and) have emerged as a popular architecture due to their strong ability to capture long-term dependencies.", "labels": [], "entities": []}, {"text": "LSTMs have been successfully applied to a variety of tasks ranging from machine translation ), to speech recognition (, and image description generation ( . Despite superior performance in many applications, neural language models essentially predict sequences of words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7551357746124268}, {"text": "speech recognition", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8254828155040741}, {"text": "image description generation", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.8254414002100626}]}, {"text": "Many NLP tasks, however, exploit syntactic information operating over tree structures (e.g., dependency or constituent trees).", "labels": [], "entities": []}, {"text": "In this paper we develop a novel neural network model which combines the advantages of the LSTM architecture and syntactic structure.", "labels": [], "entities": []}, {"text": "Our model estimates the probability of a sentence by estimating the generation probability of its dependency tree.", "labels": [], "entities": []}, {"text": "Instead of explicitly encoding tree structure as a set of features, we use four LSTM networks to model four types of dependency edges which altogether specify how the tree is built.", "labels": [], "entities": []}, {"text": "At each time step, one LSTM is activated which predicts the next word conditioned on the sub-tree generated so far.", "labels": [], "entities": []}, {"text": "To learn the representations of the conditioned sub-tree, we force the four LSTMs to share their hidden layers.", "labels": [], "entities": []}, {"text": "Our model is also capable of generating trees just by sampling from a trained model and can be seamlessly integrated with text generation applications.", "labels": [], "entities": []}, {"text": "Our approach is related to but ultimately different from recursive neural networks a class of models which operate on structured inputs.", "labels": [], "entities": []}, {"text": "Given a (binary) parse tree, they recursively generate parent representations in a bottom-up fashion, by combining tokens to produce representations for phrases, and eventually the whole sentence.", "labels": [], "entities": []}, {"text": "The learned representations can be then used in classification tasks such as sentiment analysis) and paraphrase detection).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9615525603294373}, {"text": "paraphrase detection", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.9395365715026855}]}, {"text": "learn distributed representations over syntactic trees by generalizing the LSTM architecture to tree-structured network topologies.", "labels": [], "entities": []}, {"text": "The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability.", "labels": [], "entities": []}, {"text": "Syntactic language models have along history in NLP dating back to (see also and).", "labels": [], "entities": []}, {"text": "These models differ in how grammar structures in a parsing tree are used when predicting the next word.", "labels": [], "entities": [{"text": "predicting the next word", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8254275172948837}]}, {"text": "Other work develops dependency-based language models for specific applications such as machine translation, speech recognition () or sentence completion.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.81470987200737}, {"text": "speech recognition", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.743159607052803}, {"text": "sentence completion", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.778713047504425}]}, {"text": "All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. and estimate the parameters of a structured language model using feed-forward neural networks (.", "labels": [], "entities": []}, {"text": "re-implement the model of with RNNs.", "labels": [], "entities": []}, {"text": "They view sentences as sequences of words over a tree.", "labels": [], "entities": []}, {"text": "While they ignore the tree structures themselves, we model them explicitly.", "labels": [], "entities": []}, {"text": "Our model shares with other structured-based language models the ability to take dependency information into account.", "labels": [], "entities": []}, {"text": "It differs in the following respects: (a) it does not artificially restrict the depth of the dependencies it considers and can thus be viewed as an infinite order dependency language model; (b) it not only estimates the probability of a string but is also capable of generating dependency trees; (c) finally, contrary to previous dependencybased language models which encode syntactic information as features, our model takes tree structure into account more directly via representing different types of dependency edges explicitly using LSTMs.", "labels": [], "entities": []}, {"text": "Therefore, there is no need to manually determine which dependency tree features should be used or how large the feature embeddings should be.", "labels": [], "entities": []}, {"text": "We evaluate our model on the MSR sentence completion challenge, a benchmark language modeling dataset.", "labels": [], "entities": [{"text": "MSR sentence completion challenge", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8835482746362686}]}, {"text": "Our results outperform the best published results on this dataset.", "labels": [], "entities": []}, {"text": "Since our model is a general tree estimator, we also use it to rerank the top K dependency trees from the (second order) MSTPasrser and obtain performance on par with recently proposed dependency parsers.", "labels": [], "entities": [{"text": "MSTPasrser", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.9601421356201172}]}], "datasetContent": [{"text": "We assess the performance of our model on two tasks: the Microsoft Research (MSR) sentence completion challenge , and dependency parsing reranking.", "labels": [], "entities": [{"text": "Microsoft Research (MSR) sentence completion challenge", "start_pos": 57, "end_pos": 111, "type": "TASK", "confidence": 0.7831390388309956}, {"text": "dependency parsing reranking", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.9068432648976644}]}, {"text": "We also demonstrate the tree generation capability of our models.", "labels": [], "entities": [{"text": "tree generation", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7574573159217834}]}, {"text": "In the following, we first present details on model training and then present our results.", "labels": [], "entities": []}, {"text": "We implemented our models using the Torch library) and our code is available at https:// github.com/XingxingZhang/td-treelstm.", "labels": [], "entities": [{"text": "Torch library", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9618930220603943}]}], "tableCaptions": [{"text": " Table 1: Model accuracy on the MSR sentence completion task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9808392524719238}, {"text": "MSR sentence completion task", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.9104907512664795}]}]}