{"title": [{"text": "A Low-Rank Approximation Approach to Learning Joint Embeddings of News Stories and Images for Timeline Summarization", "labels": [], "entities": [{"text": "Approximation Approach", "start_pos": 11, "end_pos": 33, "type": "METRIC", "confidence": 0.833292156457901}, {"text": "Timeline Summarization", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.5997313410043716}]}], "abstractContent": [{"text": "A key challenge for timeline summarization is to generate a concise, yet complete storyline from large collections of news stories.", "labels": [], "entities": [{"text": "timeline summarization", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7294352948665619}]}, {"text": "Previous studies in extractive timeline generation are limited in two ways: first, most prior work focuses on fully-observable ranking models or clustering models with hand-designed features that may not generalize well.", "labels": [], "entities": [{"text": "extractive timeline generation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.8115267554918925}]}, {"text": "Second, most summarization corpora are text-only, which means that text is the sole source of information considered in timeline summarization, and thus, the rich visual content from news images is ignored.", "labels": [], "entities": [{"text": "summarization corpora", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.9048844277858734}, {"text": "timeline summarization", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.5662829875946045}]}, {"text": "To solve these issues, we leverage the success of matrix factorization techniques from recommender systems, and cast the problem as a sentence recommendation task, using a representation learning approach.", "labels": [], "entities": []}, {"text": "To augment text-only corpora, for each candidate sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture.", "labels": [], "entities": []}, {"text": "Finally , we propose a scalable low-rank approximation approach for learning joint embed-dings of news stories and images.", "labels": [], "entities": []}, {"text": "In experiments , we compare our model to various competitive baselines, and demonstrate the state-of-the-art performance of the proposed text-based and multimodal approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Timeline summarization is the task of organizing crucial milestones of a news story in a temporal order, e.g. (. A * This work was performed when William Wang and Dragomir Radev were visiting timeline example for the 2010 British Oil spill generated by our system is shown in.", "labels": [], "entities": [{"text": "Timeline summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5002685785293579}, {"text": "British Oil spill generated", "start_pos": 222, "end_pos": 249, "type": "DATASET", "confidence": 0.9499358385801315}]}, {"text": "The task is challenging, because the input often includes a large number of news articles as the story is developing each day, but only a small portion of the key information is needed for timeline generation.", "labels": [], "entities": [{"text": "timeline generation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.7671774625778198}]}, {"text": "In addition to the conciseness requirement, timeline summarization also has to be complete-all key information, in whatever form, must be presented in the final summary.", "labels": [], "entities": [{"text": "timeline summarization", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.5033857971429825}]}, {"text": "To distill key insights from news reports, prior work in summarization often relies on feature engineering, and uses clustering techniques ( to select important events to be included in the final summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9854707717895508}]}, {"text": "While this approach is unsupervised, the process of feature engineering is always expensive, and the number of clusters is not easy to estimate.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7979843616485596}]}, {"text": "To present a complete summary, researchers from the natural language processing (NLP) community often solely rely on the textual information, while studies in the computer vision (CV) community rely solely on the image and video information.", "labels": [], "entities": []}, {"text": "However, even though news images are abundantly available together with news stories, approaches that jointly learn textual and visual representations for summarization are not common.", "labels": [], "entities": [{"text": "summarization", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9891063570976257}]}, {"text": "In this paper, we take a more radical approach to timeline summarization.", "labels": [], "entities": [{"text": "timeline summarization", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7770916223526001}]}, {"text": "We formulate the problem as a sentence recommendation task-instead of recommending items to users as in a recommender system, we recommend important sentences to a timeline.", "labels": [], "entities": [{"text": "sentence recommendation task-instead", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7467415034770966}]}, {"text": "Our approach does not require feature engineering: by using a matrix factorization framework, we are essentially performing representation learn- ing to model the continuous representation of sentences and words.", "labels": [], "entities": []}, {"text": "Since most previous timeline summarization work (and therefore, corpora) only focuses on textual information, we also provide a novel web-based approach for harvesting news images: we query Yahoo!", "labels": [], "entities": [{"text": "timeline summarization", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6785341203212738}]}, {"text": "image search with sentences from news articles, and extract visual cues using a 15-layer convolutional neural network architecture.", "labels": [], "entities": [{"text": "image search", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7513906359672546}]}, {"text": "By unifying text and images in the lowrank approximation framework, our approach learns a joint embedding of news story texts and images in a principled manner.", "labels": [], "entities": []}, {"text": "In empirical evaluations, we conduct experiments on two publicly available datasets, and demonstrate the efficiency and effectiveness of our approach.", "labels": [], "entities": []}, {"text": "By comparing to various baselines, we show that our approach is highly scalable and achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Our main contributions are three-fold: \u2022 We propose a novel matrix factorization approach for extractive summarization, leveraging the success of collaborative filtering; \u2022 We are among the first to consider representation learning of a joint embedding for text and images in timeline summarization; \u2022 Our model significantly outperforms various competitive baselines on two publicly available datasets.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.5444105565547943}, {"text": "timeline summarization", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.6679594963788986}]}], "datasetContent": [{"text": "In this section, we investigate the empirical performance of the proposed method, comparing to various baselines.", "labels": [], "entities": []}, {"text": "We first discuss our experimental settings, including our primary dataset and baselines.", "labels": [], "entities": []}, {"text": "Then, we discuss our evaluation results.", "labels": [], "entities": []}, {"text": "We demonstrate the robustness of our approach by varying the latent dimensions of the low-rank matrices.", "labels": [], "entities": []}, {"text": "Next, we show additional experiments on a headline-based timeline summarization dataset.", "labels": [], "entities": [{"text": "headline-based timeline summarization dataset", "start_pos": 42, "end_pos": 87, "type": "DATASET", "confidence": 0.6174895092844963}]}, {"text": "Finally, we provide a qualitative analysis of the output of our system.", "labels": [], "entities": []}, {"text": "We use the 17 timelines dataset which has been used in several prior studies).", "labels": [], "entities": []}, {"text": "It includes 17 timelines from 9 topics 1 from major news agencies such as CNN, BBC, and NBC News.", "labels": [], "entities": []}, {"text": "Only English documents are included.", "labels": [], "entities": []}, {"text": "The dataset contains 4,650 news documents.", "labels": [], "entities": []}, {"text": "Image Search to retrieve the top-ranked image for each sentence.", "labels": [], "entities": [{"text": "Image Search", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6573742628097534}]}, {"text": "We follow exactly the same topic-based cross-validation setup that was used in prior work (Tran et al., 2013b): we train on eight topics, test on the remaining topic, and repeat the process eight times.", "labels": [], "entities": []}, {"text": "The number of training iterations was set to 20; the k was set to 200 for the text only model, and 300 for the joint text/image model; and the vocabulary is 10K words for all systems.", "labels": [], "entities": []}, {"text": "The common summarization metrics ROUGE-1, ROUGE-2, and ROUGE-S are used to evaluate the quality of the machine-generated timelines.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9450552463531494}, {"text": "ROUGE-2", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9573282599449158}, {"text": "ROUGE-S", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9854276776313782}]}, {"text": "We consider the following baselines: The nine topics are the BP oil spill, Egyptian protests, Financial crisis, H1N1, Haiti earthquake, Iraq War, Libya War, Michael Jackson death, and Syrian crisis.", "labels": [], "entities": []}, {"text": "We are not aware of any publicly available dataset for timeline summarization that includes both text and images.", "labels": [], "entities": [{"text": "timeline summarization", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6164592504501343}]}, {"text": "Most of these datasets are text-only, not including the original article file or links to accompanying images.", "labels": [], "entities": []}, {"text": "We adopted this Web-based corpus enhancement technique as a proximity for news images.", "labels": [], "entities": []}, {"text": "Our low-rank approximation technique can be applied to the original news images in the same way.", "labels": [], "entities": []}, {"text": "\u2022 Random: summary sentences are randomly selected from the corpus.", "labels": [], "entities": []}, {"text": "\u2022 MEAD: a feature-rich, classic multi-document summarization system () that uses centroid-based summarization techniques.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.6636210680007935}]}, {"text": "(): a multidocument summarization system that uses TFIDF scores to indicate the \"popularity\" of a sentence compared to other sentences.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.905874490737915}]}, {"text": "\u2022 ETS (Yan et al., 2011b): a state-of-the-art unsupervised timeline summarization system.", "labels": [], "entities": [{"text": "ETS (Yan et al., 2011b)", "start_pos": 2, "end_pos": 25, "type": "DATASET", "confidence": 0.7870078980922699}]}, {"text": "\u2022): another state-of-the-art timeline summarization system based on learning to rank techniques, and for which results on the 17 Timelines dataset have been previously reported.", "labels": [], "entities": [{"text": "17 Timelines dataset", "start_pos": 126, "end_pos": 146, "type": "DATASET", "confidence": 0.8619811534881592}]}, {"text": "\u2022 Regression: apart of a state-of-the-art extractive summarization method () that formulates the sentence extraction task as a supervised regression problem.", "labels": [], "entities": [{"text": "Regression", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9133319854736328}, {"text": "sentence extraction task", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.7660166621208191}]}, {"text": "We use a stateof-the-art regression implementation in Vowpal Wabbit 3 . We report results for our system and the baselines on the 17 timelines dataset in.", "labels": [], "entities": [{"text": "Vowpal Wabbit 3", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9015537103017172}]}, {"text": "We see that the random baseline clearly performs worse than the other methods.", "labels": [], "entities": []}, {"text": "Even though shows an example of the retrieved images we used.", "labels": [], "entities": []}, {"text": "In general, images retrieved by using more important sentences (measured by ROUGE) include objects, as well a more vivid and detailed scene.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9972769618034363}]}, {"text": "To evaluate the robustness of our approach, we show the performance of our method on the recently released crisis dataset (  this dataset: we train on three topics, and test on the other.", "labels": [], "entities": []}, {"text": "Here k is set to 300, and the vocabulary is 10K words for all systems.", "labels": [], "entities": []}, {"text": "shows the performance of our system.", "labels": [], "entities": []}, {"text": "Our system is significantly better than the strong supervised regression baseline.", "labels": [], "entities": []}, {"text": "When considering joint learning of text and vision, we see that there is a further improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparing the timeline summarization per- formance to various baselines on the 17 Timelines  dataset. The best-performing results are highlighted  in bold.", "labels": [], "entities": [{"text": "17 Timelines  dataset", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.6862074931462606}]}, {"text": " Table 3: Comparing the timeline summarization per- formance to the state-of-the-art supervised sentence  regression approach on the crisis dataset. The best- performing results are highlighted in bold.", "labels": [], "entities": []}]}