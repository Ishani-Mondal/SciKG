{"title": [{"text": "Transition-Based Syntactic Linearization with Lookahead Features", "labels": [], "entities": []}], "abstractContent": [{"text": "It has been shown that transition-based methods can be used for syntactic word ordering and tree linearization, achieving significantly faster speed compared with traditional best-first methods.", "labels": [], "entities": [{"text": "syntactic word ordering", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.6271918217341105}]}, {"text": "State-of-the-art transition-based models give competitive results on abstract word ordering and unlabeled tree lin-earization, but significantly worse results on labeled tree linearization.", "labels": [], "entities": []}, {"text": "We demonstrate that the main cause for the performance bottleneck is the sparsity of SHIFT transition actions rather than heavy pruning.", "labels": [], "entities": [{"text": "SHIFT transition", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.641684502363205}]}, {"text": "To address this issue , we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency.", "labels": [], "entities": []}, {"text": "Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9654346108436584}]}], "introductionContent": [{"text": "Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words, which is practically relevant to text-to-text applications such as summarization () and machine translation (.", "labels": [], "entities": [{"text": "Word ordering", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7396158277988434}, {"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.9900583624839783}, {"text": "machine translation", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.7919865548610687}]}, {"text": "built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and * Part of the work was done when the author was a visiting student at Singapore University of yields a sentence together with its dependency parse tree that conforms to input syntactic constraints.", "labels": [], "entities": [{"text": "POS", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9888262748718262}]}, {"text": "The system is flexible with respect to input constraints, performing abstract word ordering when no constraints are given, but gives increasingly confined outputs when more POS and dependency relations are specified.", "labels": [], "entities": []}, {"text": "It has been applied to syntactic linearization ( ) and machine translation ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8427893817424774}]}, {"text": "One limitation of is relatively low time efficiency, due to the use of time-constrained best-first-search) for decoding.", "labels": [], "entities": []}, {"text": "In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality.", "labels": [], "entities": []}, {"text": "Recently,  proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the output.", "labels": [], "entities": []}, {"text": "The system of  achieves significant speed improvements without sacrificing accuracies when working with unlabeled dependency trees.", "labels": [], "entities": []}, {"text": "With labeled dependency trees as input constraints, however, the system of  gives much lower accuracies compared with.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9805092811584473}]}, {"text": "While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scores the output transition sequence by summing the scores of each transition action.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9990178346633911}]}, {"text": "Transition actions are treated as anatomic output component in each feature instance.", "labels": [], "entities": []}, {"text": "This works effectively for most structured prediction tasks, including parsing ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9768125414848328}]}, {"text": "For word ordering, however, transition actions are significantly more complex and sparse compared with parsing, which limits the power of the traditional feature model.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.786344975233078}]}, {"text": "We instead breakdown complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead features to be defined according to the next action to be applied.", "labels": [], "entities": []}, {"text": "On the other hand, this change in the feature structure prevents legitimate actions to be scored simultaneously for each configuration state, thereby reducing decoding efficiency.", "labels": [], "entities": []}, {"text": "Experiments show that our method is slightly slower compared with , but achieves significantly better accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9811753630638123}]}, {"text": "It gives the best results for all standard benchmarks, being over thirty times faster than.", "labels": [], "entities": []}, {"text": "The new feature structures can be applied to other transition-based systems also.", "labels": [], "entities": []}, {"text": "2 Transition-based linearization  uses a transition-based model for word ordering, building output sentences using a sequence of state transitions.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.7390101850032806}]}, {"text": "Instead of scoring output syntax trees directly, it scores the transition action sequence for structural disambiguation.", "labels": [], "entities": []}, {"text": "Liu et al.'s transition system extends from transition-based parsers (, where a state consists of a stack to hold partially built outputs.", "labels": [], "entities": []}, {"text": "Transition-based parsers use a queue to maintain input word sequences.", "labels": [], "entities": []}, {"text": "However, for word ordering, the input is a set without order.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7860904037952423}]}, {"text": "Accordingly, Liu et al. uses a set to maintain the input.", "labels": [], "entities": []}, {"text": "The transition actions are: \u2022 SHIFT-Word-POS, which removes Word from the set, assigns POS to it and pushes it onto the stack as the top word S 0 ; \u2022 LEFTARC-LABEL, which removes the second top of stack S 1 and builds a dependency arc \u2022 RIGHTARC-LABEL, which removes the top of stack S 0 and builds a dependency arc Using the state transition system, the bag of words {John, loves, Mary} can be ordered by (SHIFT-John-NNP, SHIFT-loves-VBZ, LEFTARC-SBJ, SHIFT-Mary-NNP, RIGHTARC-OBJ).", "labels": [], "entities": [{"text": "LEFTARC-LABEL", "start_pos": 150, "end_pos": 163, "type": "METRIC", "confidence": 0.9586375951766968}]}, {"text": "The features include word(w), pos(p) and dependency label(l) information of words on the stack (S 0 , S 1 , ... from the top).", "labels": [], "entities": [{"text": "dependency label(l) information", "start_pos": 41, "end_pos": 72, "type": "METRIC", "confidence": 0.7358408321936926}]}, {"text": "For example, the word on top of stack is S 0 wand the POS of the stack top is S 0 p.", "labels": [], "entities": [{"text": "POS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9890251755714417}]}, {"text": "The full set of feature templates can be found in of , reproduced herein.", "labels": [], "entities": []}, {"text": "These templates are called configuration features.", "labels": [], "entities": []}, {"text": "When instantiated, they are combined with each legal output action to score the action.", "labels": [], "entities": []}, {"text": "Therefore, actions are atomic in feature instances.", "labels": [], "entities": []}, {"text": "Formally, given a configuration C, the score of a possible action a is calculated as: where \u03b8 is the model parameter vector of the model and \u03a6(C, a) denotes a sparse feature vector that consists of features with configuration and action components i.e \u03a6(C, a) is sparse.", "labels": [], "entities": []}, {"text": "\u03b8 has to be loaded for each a.", "labels": [], "entities": []}, {"text": "For efficiency considerations and following transition-based models,  scores all possible actions given a configuration simultaneously.", "labels": [], "entities": []}, {"text": "This is effectively the same as formulating the score into Here A is the full set of actions and \u03a6(C) is fixed, and \u03b8 a for all a can be loaded simultaneously.", "labels": [], "entities": []}, {"text": "Ina hash-based parameter model, it significantly improves the time efficiency.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work we conduct experiments on the Penn TreeBank (PTB), using Wall Street Journal sections 2-21 for training, 22 for development and 23 for testing.", "labels": [], "entities": [{"text": "Penn TreeBank (PTB)", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9763723373413086}, {"text": "Wall Street Journal sections 2-21", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.9613331913948059}]}, {"text": "Gold-standard dependency trees are derived from bracketed sentences using Penn2Malt, and base noun phrases are treated as a single word.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9794923067092896}]}, {"text": "The BLEU score is used to evaluate the performance of linearization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9988815188407898}]}, {"text": "shows a difference in scores between transition-based linearization system of and best-first system of Zhang (2013) (Z13).", "labels": [], "entities": []}, {"text": "L15 performs better for word ordering with unlabeled dependency arcs, but poorly for the task of labeled syntactic linearization.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7666817903518677}]}, {"text": "shows a series of development experiments comparing our system with Z13 and L15.", "labels": [], "entities": []}, {"text": "We vary the amount of input syntactic constraints by randomly sampling from POS and dependency labels of the development set.", "labels": [], "entities": []}, {"text": "Our system gives consistently higher accuracies when compared with both Z13 and L15.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9979875087738037}]}, {"text": "Compared to L15, the increase in scores for unconstrained word ordering is due to the introduction of reduced feature sparsity.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.7270227521657944}]}, {"text": "The improvements on tree linearization tasks involving partial to full dependency constraints are also due to lookahead features that leverage tree information to reduce ambiguity early.", "labels": [], "entities": []}, {"text": "Though slower than L15, our system is over 30 times faster compared to Z13.", "labels": [], "entities": []}, {"text": "We compare final test scores with previous methods in the literature in.", "labels": [], "entities": []}, {"text": "Our system improves upon the previous best scores by 8.7 BLEU points for the task of unlabeled syntactic linearization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9989079236984253}]}, {"text": "For the task of labeled syntactic linearization, we achieve the score of 91.8 BLEU points, the highest results reported so far.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9856681823730469}]}, {"text": "output . In the first example 'will' is the ROOT node with two child nodes 'also' and 'compete'.", "labels": [], "entities": []}, {"text": "Lookahead feature for child dependency labels L cls , L clns on the node 'will' can help order the segment 'also will compete' correctly in our system.", "labels": [], "entities": []}, {"text": "Without such features, the system of L15 yields an output that starts with 'The spinoff with Fujitsu' which is locally fluent, but leaving the words 'also' and 'will' difficult to handle.", "labels": [], "entities": [{"text": "Fujitsu", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9348970651626587}]}, {"text": "In the second example, 'Dr. Talcott' is OOV.", "labels": [], "entities": [{"text": "Dr. Talcott'", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8402760823567709}, {"text": "OOV", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9942401647567749}]}, {"text": "Hence system of L15 is notable to score it and thus order it correctly.", "labels": [], "entities": []}, {"text": "Our system makes use of both POS and dependency label of 'Dr. Talcott' to order it correctly.", "labels": [], "entities": [{"text": "POS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9973018169403076}]}], "tableCaptions": [{"text": " Table 3: Development partial-tree linearization results. BL -BLEU score; SP -number of milliseconds per  sentence. Z13 -best-first system of Zhang", "labels": [], "entities": [{"text": "BL -BLEU score", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.895889401435852}, {"text": "SP -number of milliseconds", "start_pos": 74, "end_pos": 100, "type": "METRIC", "confidence": 0.9317026495933532}]}, {"text": " Table 4: Final results. W09 -Wann et al. (2009),  Z11 -Zhang and Clark (2011b)", "labels": [], "entities": [{"text": "W09 -Wann et al. (2009)", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8380489150683085}]}]}