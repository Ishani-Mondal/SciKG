{"title": [{"text": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", "labels": [], "entities": [{"text": "Cross-Language Text Classification", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.6959081689516703}]}], "abstractContent": [{"text": "In many languages, sparse availability of resources causes numerous challenges for tex-tual analysis tasks.", "labels": [], "entities": [{"text": "tex-tual analysis tasks", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8629103302955627}]}, {"text": "Text classification is one of such standard tasks that is hindered due to limited availability of label information in low-resource languages.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8286722302436829}]}, {"text": "Transferring knowledge (i.e. label information) from high-resource to low-resource languages might improve text classification as compared to the other approaches like machine translation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8501145541667938}, {"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7774040699005127}]}, {"text": "We introduce BRAVE (Bilingual paRAgraph VEctors), a model to learn bilingual distributed representations (i.e. embeddings) of words without word alignments either from sentence-aligned parallel or label-aligned non-parallel document corpora to support cross-language text classification.", "labels": [], "entities": [{"text": "BRAVE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9945328235626221}, {"text": "cross-language text classification", "start_pos": 252, "end_pos": 286, "type": "TASK", "confidence": 0.735124409198761}]}, {"text": "Empirical analysis shows that classification models trained with our bilingual embeddings outperforms other state-of-the-art systems on three different cross-language text classification tasks.", "labels": [], "entities": [{"text": "cross-language text classification", "start_pos": 152, "end_pos": 186, "type": "TASK", "confidence": 0.6190546154975891}]}], "introductionContent": [{"text": "The availability of language-specific annotated resources is crucial for the efficiency of natural language processing tasks.", "labels": [], "entities": []}, {"text": "Still, many languages lack rich annotated resources that support various tasks such as part-of-speech tagging, dependency parsing and text classification.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6923259943723679}, {"text": "dependency parsing", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8615691661834717}, {"text": "text classification", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.775894820690155}]}, {"text": "While the growth of multilingual information on the web has provided an opportunity to build these missing annotated resources, but still lots of manual effort is required to achieve high quality resources for every language separately.", "labels": [], "entities": []}, {"text": "Another possibility is to utilize the unlabeled data present in those languages or transfer knowledge from annotation-rich languages.", "labels": [], "entities": []}, {"text": "For the first alternative, recent advancements made in learning monolingual distributed representations of words () (i.e. monolingual word embeddings) capturing syntactic and semantic information in an unsupervised manner was useful in numerous NLP tasks).", "labels": [], "entities": []}, {"text": "However, this may not be sufficient for several other tasks such as cross-language information retrieval, cross-language word semantic similarity), cross-language text classification (CLTC, henceforth) ( and machine translation () due to irregularities across languages.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.7275065382321676}, {"text": "cross-language word semantic similarity", "start_pos": 106, "end_pos": 145, "type": "TASK", "confidence": 0.7449495121836662}, {"text": "cross-language text classification", "start_pos": 148, "end_pos": 182, "type": "TASK", "confidence": 0.7067505121231079}, {"text": "machine translation", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.8070334494113922}]}, {"text": "In these kind of scenarios, transfer of knowledge can be useful.", "labels": [], "entities": []}, {"text": "Several approaches () tried to induce monolingual distributed representations into a language independent space (i.e. bilingual or multilingual word embeddings) by jointly training on pair of languages.", "labels": [], "entities": []}, {"text": "Although the overall goal of these approaches is to capture linguistic regularities in words that share same semantic and syntactic space across languages, they differ in their implementation.", "labels": [], "entities": []}, {"text": "One set of methods either perform offline alignment of trained monolingual embeddings or jointly-train both monolingual and cross-lingual objectives, while the other set uses only cross-lingual objective.", "labels": [], "entities": []}, {"text": "Jointly-trained or offline alignment methods can be further divided based on the type of par-Cross-Language Setups Objective Method Tasks Parallel Corpus () CLDC Word-Aligned ( MT,NER Word-Aligned Monolingual+ ( MT Word-Aligned Cross-lingual) Word Similarity Word-Aligned ( Word Similarity Word-Aligned ( POS,SuS Word-Aligned (  CLDC,MT Sentence-Aligned ( CLDC,MT Sentence-Aligned Cross-lingual ( ) CLDC Sentence-Aligned) CLDC Sentence-Aligned (  Word Similarity, CLDC Sentence-Aligned (  CLDC Sentence-Aligned allel corpus (e.g. word-aligned, sentence-aligned) they use for learning the cross-lingual objective.", "labels": [], "entities": []}, {"text": "Table 1 summarizes different setups to learn bilingual or multilingual embeddings for the various tasks.", "labels": [], "entities": []}, {"text": "Methods in the that use word-aligned parallel corpus as offline alignment) assume single correspondence between the words across languages and ignore polysemy.", "labels": [], "entities": []}, {"text": "While, the jointlytrain methods () that use word-alignment parallel corpus and consider polysemy perform computationally expensive operation of considering all possible interactions between the pairs of words in vocabulary of two different languages.", "labels": [], "entities": []}, {"text": "Methods () that overcame the complexity issues of word-aligned models by using sentence-aligned parallel corpora limits themselves to only cross-lingual objective, thus making these approaches unable to explore monolingual corpora.", "labels": [], "entities": []}, {"text": "Jointly-trained models ( overcame the issues of both word-aligned and purely cross-lingual objective models by using monolingual and sentencealigned parallel corpora.", "labels": [], "entities": []}, {"text": "Nonetheless, these approaches still have certain drawbacks such as usage of only bag-of-words from the parallel sentences ignoring order of words.", "labels": [], "entities": []}, {"text": "Thus, they are missing to capture the non-compositional meaning of entire sentence.", "labels": [], "entities": []}, {"text": "Also, learned bilingual embeddings were heavily biased towards the sampled sentence-aligned parallel corpora.", "labels": [], "entities": []}, {"text": "It is also sometimes hard to acquire sentence-level parallel corpora for every language pair.", "labels": [], "entities": []}, {"text": "To subdue this concern, few approaches () used pivot languages like English or comparable documentaligned corpora to learn bilingual embeddings specific to only one task.", "labels": [], "entities": []}, {"text": "This major downside can be observed in other aforementioned methods also, which are inflexible to handle different types of parallel corpora and have a tight-binding between cross-lingual objectives and the parallel corpora.", "labels": [], "entities": []}, {"text": "For example, a method using sentence-level parallel corpora cannot be altered to leverage document-level parallel corpora (if available) that might have better performance for some tasks.", "labels": [], "entities": []}, {"text": "Also, none of the approaches do leverage widely available label/classaligned non-parallel documents (e.g. sentiment labels, multi-class datasets) across languages which share special semantics such as sentiment or correlation between concepts as opposed to parallel texts.", "labels": [], "entities": []}, {"text": "In this paper, we introduce BRAVE a jointlytrained flexible model that learns bilingual embeddings based on the availability of the type of corpora (e.g. sentence-aligned parallel or label/classaligned non-parallel document) by just altering the cross-lingual objective.", "labels": [], "entities": [{"text": "BRAVE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9418538212776184}]}, {"text": "BRAVE leverages paragraph vector embeddings ( of the monolingual corpora to effectively conceal semantics of the text sequences across languages and build a cross-lingual objective.", "labels": [], "entities": [{"text": "BRAVE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.4505617320537567}]}, {"text": "Method closely related to our approach is by  who uses shared context sentence vector across lan-guages to learn multilingual text sequences.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 We jointly train monolingual part of parallel corpora with the improved cross-lingual alignment function that extends beyond bag-of-word models.", "labels": [], "entities": []}, {"text": "\u2022 Introduced a novel approach to leverage nonparallel data sets such as label or class aligned documents in different languages for learning bilingual cues.", "labels": [], "entities": []}, {"text": "\u2022 Experimental evaluation on three different CLTC tasks, namely cross-language document classification, multi-label classification and cross-language sentiment classification using learned bilingual word embeddings.", "labels": [], "entities": [{"text": "cross-language document classification", "start_pos": 64, "end_pos": 102, "type": "TASK", "confidence": 0.6907310485839844}, {"text": "multi-label classification", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7888275384902954}, {"text": "cross-language sentiment classification", "start_pos": 135, "end_pos": 174, "type": "TASK", "confidence": 0.8541717131932577}]}], "datasetContent": [{"text": "In this section, we report results on three different CLTC tasks to comprehend whether our learned bilingual embeddings are semantically useful across languages.", "labels": [], "entities": []}, {"text": "First, cross-language document classification (CLDC) task proposed by using the subset of Reuters RCV1/RCV2 corpora ().", "labels": [], "entities": [{"text": "cross-language document classification (CLDC)", "start_pos": 7, "end_pos": 52, "type": "TASK", "confidence": 0.7513003001610438}, {"text": "Reuters RCV1/RCV2 corpora", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.9429375410079956}]}, {"text": "Second, a multi-label CLDC task with more languages using TED corpus 1 of  . Subsequently, a crosslanguage sentiment classification (CLSC) proposed by Prettenhofer et al., (2010) on a multi-domain sentiment dataset.", "labels": [], "entities": [{"text": "crosslanguage sentiment classification", "start_pos": 93, "end_pos": 131, "type": "TASK", "confidence": 0.6932104428609213}]}], "tableCaptions": [{"text": " Table 2: CLDC Accuracy with 1000 labeled examples on RCV1/RCV2 Corpus. en/de, en/fr and en/es results of Majority class, MT, I-Matrix and  BAE-cr are adopted from Sarath Chandar et al.,", "labels": [], "entities": [{"text": "CLDC", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.5042552351951599}, {"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.855715811252594}, {"text": "RCV1/RCV2 Corpus", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.8970464020967484}, {"text": "BAE-cr", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9962944388389587}]}, {"text": " Table 3: Cumulative F1-scores on TED Corpus using training data in English language and evaluation on other languages (i.e. German (de),  Spanish (es), French (fr), Italian (it), Dutch (nl), Portugese (pt), Polish (po), Romanian (ro), Russian (ru) and Turkish (tr)) and vice versa. MT- Baseline, DOC/ADD, DOC/BI represents single language pair of Hermann et al., (2014) as document features. Underline shows the best results  amongst embedding models.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8331474661827087}, {"text": "TED Corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9018752574920654}, {"text": "MT- Baseline", "start_pos": 283, "end_pos": 295, "type": "DATASET", "confidence": 0.7379537224769592}, {"text": "BI", "start_pos": 310, "end_pos": 312, "type": "METRIC", "confidence": 0.8641818761825562}]}, {"text": " Table 4: Average classification accuracies and standard deviations for 12 CLSC tasks. Results of other baselines are adopted from CL-SCL", "labels": [], "entities": [{"text": "accuracies", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.819257915019989}, {"text": "CL-SCL", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.8563262820243835}]}]}