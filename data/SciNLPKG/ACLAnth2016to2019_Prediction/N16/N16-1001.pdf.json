{"title": [{"text": "Achieving Accurate Conclusions in Evaluation of Automatic Machine Translation Metrics", "labels": [], "entities": [{"text": "Achieving Accurate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7412419021129608}, {"text": "Evaluation of Automatic Machine Translation Metrics", "start_pos": 34, "end_pos": 85, "type": "TASK", "confidence": 0.6051170378923416}]}], "abstractContent": [{"text": "Automatic Machine Translation metrics, such as BLEU, are widely used in empirical evaluation as a substitute for human assessment.", "labels": [], "entities": [{"text": "Automatic Machine Translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5999725461006165}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9960198998451233}]}, {"text": "Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment.", "labels": [], "entities": []}, {"text": "When a newly proposed metric achieves a stronger correlation over that of a baseline, it is important to take into account the uncertainty inherent in correlation point estimates prior to concluding improvements in metric performance.", "labels": [], "entities": []}, {"text": "Confidence intervals for correlations with human judgment are rarely reported in metric evaluations , however, and when they have been reported, the most suitable methods have unfortunately not been applied.", "labels": [], "entities": [{"text": "Confidence intervals", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9790851473808289}]}, {"text": "For example, incorrect assumptions about correlation sampling distributions made in past evaluations risk over-estimation of significant differences in metric performance.", "labels": [], "entities": []}, {"text": "In this paper, we provide analysis of each of the issues that may lead to inaccuracies before providing detail of a method that overcomes previous challenges.", "labels": [], "entities": []}, {"text": "Additionally, we propose anew method of translation sampling that in contrast achieves genuine high conclusivity in evaluation of the relative performance of metrics.", "labels": [], "entities": [{"text": "translation sampling", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9766218364238739}]}], "introductionContent": [{"text": "In empirical evaluation of Machine Translation (MT), automatic metrics are widely used as a substitute for human assessment for the purpose of measuring differences in MT system performance.", "labels": [], "entities": [{"text": "empirical evaluation of Machine Translation (MT)", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.7349285185337067}, {"text": "MT system", "start_pos": 168, "end_pos": 177, "type": "TASK", "confidence": 0.8985720872879028}]}, {"text": "The performance of a newly proposed metric is itself measured by the degree to which its automatic scores fora sample of MT systems correlate with human assessment of that same set of systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9635652303695679}]}, {"text": "A main venue for evaluation of MT metrics is the annual Workshop for Statistical Machine Translation (WMT) ( ) where large-scale human evaluation takes place, primarily for the purpose of ranking systems competing in the translation shared task, but additionally to use the resulting system rankings for evaluation of automatic metrics.", "labels": [], "entities": [{"text": "MT metrics", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.874606728553772}, {"text": "Statistical Machine Translation (WMT)", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.7720267673333486}]}, {"text": "Since 2014, WMT has used the Pearson correlation as the official measure for evaluation of metrics.", "labels": [], "entities": [{"text": "WMT", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7001632452011108}, {"text": "Pearson correlation", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.9058635234832764}]}, {"text": "Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore.", "labels": [], "entities": [{"text": "Pearson correlation point", "start_pos": 80, "end_pos": 105, "type": "METRIC", "confidence": 0.8319388429323832}, {"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9033951759338379}]}, {"text": "shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT-14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, REDSYSSENT (, with the popular metric BLEU (), this involves comparison of the correlation point estimate of REDSYSSENT, r = 0.993, with the weaker correlation point estimate of BLEU, r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems.", "labels": [], "entities": [{"text": "WMT-14 metrics shared task", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.5373807102441788}, {"text": "REDSYSSENT", "start_pos": 229, "end_pos": 239, "type": "METRIC", "confidence": 0.9898438453674316}, {"text": "BLEU", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9929301142692566}, {"text": "BLEU", "start_pos": 407, "end_pos": 411, "type": "METRIC", "confidence": 0.9971652626991272}]}, {"text": "When anew metric achieves a stronger correlation with human assessment over a baseline metric, such as the increase achieved by REDSYSSENT over BLEU, it is important to consider the uncertainty surrounding the difference in correlation.", "labels": [], "entities": [{"text": "REDSYSSENT", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.8958867788314819}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9949420094490051}]}, {"text": "Confidence intervals are very rarely reported in metric evaluations, however, and when attempts have been made,  the most appropriate method has unfortunately not been applied.", "labels": [], "entities": [{"text": "Confidence intervals", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9754755795001984}]}, {"text": "For example, although WMT constitutes a main authority on MT evaluation, and have made the best attempt to provide confidence intervals for metric correlations we could find, when we closely examine results of WMT-14 Czech-toEnglish metrics shared task, reproduced herein, a discrepancy can be identified.", "labels": [], "entities": [{"text": "WMT", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.9137479066848755}, {"text": "MT evaluation", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9837175011634827}, {"text": "WMT-14 Czech-toEnglish metrics shared task", "start_pos": 210, "end_pos": 252, "type": "DATASET", "confidence": 0.813780677318573}]}, {"text": "For the nine top-performing metrics participating in the shared task, upper confidence interval limits are reported to exceed 1.0.", "labels": [], "entities": []}, {"text": "Confidence intervals reported in the metrics shared task unfortunately also risk inaccurate conclusions about the relative performance of metrics for other less obvious reasons and risk conclusions that over-estimate the presence of significant differences.", "labels": [], "entities": []}, {"text": "False-positives are problematic in metric evaluations because, if a given metric is mistakenly concluded to significantly outperform a competing metric, it is possible that had a larger sample of MT systems been employed in the evaluation, that the reverse conclusion should in fact be made.", "labels": [], "entities": []}, {"text": "We demonstrate how this can occur for metrics, showing that in reality in current metric evaluation settings, it is only possible to identify a very small number of signifi- cant differences in performance.", "labels": [], "entities": []}, {"text": "A main cause is the small number of MT systems employed in evaluations, and we propose anew sampling technique, hybrid super-sampling, that overcomes previous challenges and facilities the evaluation of metrics with reference to a practically unlimited number of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9682278633117676}]}], "datasetContent": [{"text": "Alongside the correlation sample point estimates achieved by metrics, WMT reports confidence intervals for correlations that unfortunately risk overestimation of significant differences in metric performance, reasons for which we outline below).", "labels": [], "entities": [{"text": "WMT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.5658460259437561}]}, {"text": "Results of past metric evaluations have been highly inconclusive with relatively few significant differences in performance possible to identify for metrics.", "labels": [], "entities": []}, {"text": "The lack of conclusivity in metric evaluations is mainly caused by the small number of systems used to evaluate metrics.", "labels": [], "entities": [{"text": "metric evaluations", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8896885812282562}]}, {"text": "For example, in the original experiments used to justify the use of automatic metric BLEU, reported correlations with human as- Due to space limitations, it was only possible to include confidence intervals for differences in correlation fora subset of German-to-English WMT-15 metrics).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9001455903053284}]}, {"text": "Confidence intervals for the the remaining metrics and language pairs are available at https://github.com/ygraham/ MT-metric-confidence-intervals for which very few significant differences in performance are identified.", "labels": [], "entities": []}, {"text": "sessment were fora sample size of as small as 5, comprising three automatic systems and two human translators ().", "labels": [], "entities": []}, {"text": "WMT have improved on this for some language pairs at least, as in the past four evaluations sample sizes have ranged from 5 (Czech-to-English WMT-14) to 22 systems (German-to-English WMT-12/WMT-13).", "labels": [], "entities": [{"text": "WMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9089657664299011}, {"text": "WMT-13", "start_pos": 190, "end_pos": 196, "type": "DATASET", "confidence": 0.8368895649909973}]}, {"text": "Even at the maximum sample size of 22 systems, however, correlation point estimates are computed with a high degree of uncertainty.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WMT-14 Czech-to-English metrics shared task Pear-", "labels": [], "entities": [{"text": "WMT-14 Czech-to-English metrics shared task Pear", "start_pos": 10, "end_pos": 58, "type": "TASK", "confidence": 0.46828727920850116}]}, {"text": " Table 2: WMT and Fisher r-to-z (Fisher) confidence intervals", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.460951030254364}, {"text": "Fisher r-to-z (Fisher) confidence intervals", "start_pos": 18, "end_pos": 61, "type": "METRIC", "confidence": 0.6901652472359794}]}]}