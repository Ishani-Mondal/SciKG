{"title": [], "abstractContent": [{"text": "We introduce recurrent neural network grammars , probabilistic models of sentences with explicit phrase structure.", "labels": [], "entities": []}, {"text": "We explain efficient inference procedures that allow application to both parsing and language modeling.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9657868146896362}, {"text": "language modeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6496889591217041}]}, {"text": "Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequential recurrent neural networks (RNNs) are remarkably effective models of natural language.", "labels": [], "entities": []}, {"text": "In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs () as well as in various conditional language modeling tasks such as machine translation (, image caption generation (, and dialogue generation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.8480202257633209}, {"text": "image caption generation", "start_pos": 243, "end_pos": 267, "type": "TASK", "confidence": 0.8313777446746826}, {"text": "dialogue generation", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.832544356584549}]}, {"text": "Despite these impressive results, sequential models area priori inappropriate models of natural language, since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order.", "labels": [], "entities": []}, {"text": "In this paper, we introduce recurrent neural network grammars (RNNGs; \u00a72), anew generative probabilistic model of sentences that explicitly models nested, hierarchical relationships among words and phrases.", "labels": [], "entities": []}, {"text": "RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions.", "labels": [], "entities": [{"text": "context-free grammar generation", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.7278293569882711}]}, {"text": "The foundation of this work is a top-down variant of transition-based parsing ( \u00a73).", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.5640957057476044}]}, {"text": "We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation.", "labels": [], "entities": []}, {"text": "While several transition-based neural models of syntactic generation exist), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion.", "labels": [], "entities": [{"text": "syntactic generation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7416156828403473}]}, {"text": "While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation.", "labels": [], "entities": []}, {"text": "RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information ( \u00a74).", "labels": [], "entities": []}, {"text": "The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures.", "labels": [], "entities": []}, {"text": "Similar to previously published discriminative bottomup transition-based parsers, greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers.", "labels": [], "entities": []}, {"text": "The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve a second practical challenge with RNNGs: approximating the marginal likelihood and MAP tree of a sentence under the generative model.", "labels": [], "entities": [{"text": "marginal likelihood and MAP tree", "start_pos": 193, "end_pos": 225, "type": "METRIC", "confidence": 0.6777926445007324}]}, {"text": "We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( \u00a75).", "labels": [], "entities": []}, {"text": "Experiments show that RNNGs are effective for both language modeling and parsing ( \u00a76).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7899174094200134}]}, {"text": "Our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models.", "labels": [], "entities": []}, {"text": "Surprisingly-although inline with previous parsing results showing the effectiveness of generative models)-parsing with the generative model obtains significantly better results than parsing with the discriminative model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present results of our two models both on parsing (discriminative and generative) and as a language model (generative only) in English and Chinese.", "labels": [], "entities": []}, {"text": "For English, \u00a72-21 of the Penn Treebank are used as training corpus for both, with \u00a724 held out as validation, and \u00a723 used for evaluation.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.941836804151535}]}, {"text": "Singleton words in the training corpus with unknown word classes using the the Berkeley parser's mapping rules.", "labels": [], "entities": []}, {"text": "Orthographic case distinctions are preserved, and numbers (beyond singletons) are not normalized.", "labels": [], "entities": [{"text": "Orthographic case distinctions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6248396535714468}]}, {"text": "For Chinese, we use the Penn Chinese Treebank Version 5.1 (CTB) ( Model and training parameters.", "labels": [], "entities": [{"text": "Penn Chinese Treebank Version 5.1 (CTB)", "start_pos": 24, "end_pos": 63, "type": "DATASET", "confidence": 0.957746297121048}]}, {"text": "For the discriminative model, we used hidden dimensions of 128 and 2-layer LSTMs (larger numbers of dimensions reduced validation set performance).", "labels": [], "entities": []}, {"text": "For the generative model, we used 256 dimensions and 2-layer LSTMs.", "labels": [], "entities": [{"text": "generative", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9753495454788208}]}, {"text": "For both models, we tuned the dropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 (discriminative) and 0.3 (generative).", "labels": [], "entities": []}, {"text": "For the sequential LSTM baseline for the language model, we also found an optimal dropout rate of 0.3.", "labels": [], "entities": []}, {"text": "For training we used stochastic gradient descent with a learning rate of 0.1.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.9684105217456818}]}, {"text": "All parameters were initialized according to recommendations given by.", "labels": [], "entities": []}, {"text": "gives the performance of our parser on Section 23, as well as the performance of several representative models.", "labels": [], "entities": []}, {"text": "For the discriminative model, we used a greedy decoding rule as opposed to beam search in some shift-reduce baselines.", "labels": [], "entities": []}, {"text": "For the generative model, we obtained 100 independent samples from a flattened distribution of the discriminative parser (by exponentiating each probability by \u03b1 = 0.8 and renormalizing) and reranked them according to the generative model.", "labels": [], "entities": [{"text": "generative", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9677547812461853}]}, {"text": "Chinese parsing results were obtained with the same methodology as in English and show the same pattern).", "labels": [], "entities": [{"text": "Chinese parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.4177216738462448}]}, {"text": "We report held-out perword perplexities of three language models, both sequential and syntactic.", "labels": [], "entities": []}, {"text": "Log probabilities are normalized by the number of words (excluding the stop", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Language model perplexity results.", "labels": [], "entities": []}]}