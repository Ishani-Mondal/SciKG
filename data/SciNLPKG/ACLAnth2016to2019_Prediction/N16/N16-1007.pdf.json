{"title": [{"text": "Neural Network-Based Abstract Generation for Opinions and Arguments", "labels": [], "entities": [{"text": "Neural Network-Based Abstract Generation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6504889577627182}]}], "abstractContent": [{"text": "We study the problem of generating abstrac-tive summaries for opinionated text.", "labels": [], "entities": []}, {"text": "We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries.", "labels": [], "entities": []}, {"text": "An importance-based sampling method is designed to allow the en-coder to integrate information from an important subset of input.", "labels": [], "entities": []}, {"text": "Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summariza-tion systems on two newly collected datasets of movie reviews and arguments.", "labels": [], "entities": []}, {"text": "Our system summaries are also rated as more informative and grammatical inhuman evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Collecting opinions from others is an integral part of our daily activities.", "labels": [], "entities": [{"text": "Collecting opinions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8969541192054749}]}, {"text": "Discovering what other people think can help us navigate through different aspects of life, ranging from making decisions on regular tasks to judging fundamental societal issues and forming personal ideology.", "labels": [], "entities": []}, {"text": "To efficiently absorb the massive amount of opinionated information, there is a pressing need for automated systems that can generate concise and fluent opinion summary about an entity or a topic.", "labels": [], "entities": []}, {"text": "In spite of substantial researches in opinion summarization, the most prominent approaches mainly rely on extractive summarization methods, where phrases or sentences from the original documents are selected for inclusion in the summary ().", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.686238557100296}, {"text": "extractive summarization", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.5452833771705627}]}, {"text": "One of the problems that extractive methods suffer from Movie: The Martian Reviews: -One the smartest, sweetest, and most satisfyingly suspenseful sci-fi films in years.", "labels": [], "entities": [{"text": "Movie: The Martian Reviews", "start_pos": 56, "end_pos": 82, "type": "DATASET", "confidence": 0.7116790175437927}]}, {"text": "-...an intimate sci-fi epic that is smart, spectacular and stirring.", "labels": [], "entities": []}, {"text": "-The Martian is a thrilling, human and moving sci-fi picture that is easily the most emotionally engaging film Ridley Scott has made...", "labels": [], "entities": []}, {"text": "-It's pretty sunny and often funny, a space oddity fora director not known for pictures with a sense of humor.", "labels": [], "entities": []}, {"text": "-The Martian highlights the book's best qualities, tones down its worst, and adds its own style...", "labels": [], "entities": [{"text": "style", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9683414101600647}]}, {"text": "Opinion Consensus (Summary): Smart, thrilling, and surprisingly funny, The Martian offers a faithful adaptation of the bestselling book that brings out the best in leading man Matt Damon and director Ridley Scott.", "labels": [], "entities": []}, {"text": "Topic: This House supports the death penalty.", "labels": [], "entities": []}, {"text": "Arguments: -The state has a responsibility to protect the lives of innocent citizens, and enacting the death penalty may save lives by reducing the rate of violent crime.", "labels": [], "entities": []}, {"text": "-While the prospect of life in prison maybe frightening, surely death is a more daunting prospect.", "labels": [], "entities": []}, {"text": "-A 1985 study by Stephen K. Layson at the University of North Carolina showed that a single execution deters 18 murders.", "labels": [], "entities": []}, {"text": "-Reducing the wait time on death row prior to execution can dramatically increase its deterrent effect in the United States.", "labels": [], "entities": []}, {"text": "Claim (Summary): The death penalty deters crime.", "labels": [], "entities": [{"text": "Claim (Summary)", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.8042375147342682}]}, {"text": "Figure 1: Examples for an opinion consensus of professional reviews (critics) about movie \"The Martian\" from www.rottentomatoes.com, and a claim about \"death penalty\" supported by arguments from idebate.org.", "labels": [], "entities": []}, {"text": "Content with similar meaning is highlighted in the same color. is that they unavoidably include secondary or redundant information.", "labels": [], "entities": []}, {"text": "On the contrary, abstractive summarization methods, which are able to generate text beyond the original input, can produce more coherent and concise summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.6960969567298889}]}, {"text": "In this paper, we present an attention-based neural network model for generating abstractive summaries of opinionated text.", "labels": [], "entities": []}, {"text": "Our system takes as input a set of text units containing opinions about the same topic (e.g. reviews fora movie, or arguments fora controversial social issue), and then outputs a one-sentence abstractive summary that describes the opinion consensus of the input.", "labels": [], "entities": []}, {"text": "Specifically, we investigate our abstract generation model on two types of opinionated text: movie reviews and arguments on controversial topics.", "labels": [], "entities": []}, {"text": "The first example contains a set of professional reviews (or critics) about movie \"The Martian\" and an opinion consensus written by an editor.", "labels": [], "entities": []}, {"text": "It would be more useful to automatically generate fluent opinion consensus rather than simply extracting features (e.g. plot, music, etc) and opinion phrases as done in previous summarization work ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.9733973741531372}]}, {"text": "The second example lists a set of arguments on \"death penalty\", where each argument supports the central claim \"death penalty deters crime\".", "labels": [], "entities": []}, {"text": "Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues.", "labels": [], "entities": []}, {"text": "Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation.", "labels": [], "entities": [{"text": "claim generation", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.7334414720535278}]}, {"text": "Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (.", "labels": [], "entities": []}, {"text": "Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure.", "labels": [], "entities": []}, {"text": "Another line of work requires a large amount of human input to enforce summary quality.", "labels": [], "entities": []}, {"text": "For example, utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions.", "labels": [], "entities": []}, {"text": "To address the challenges above, we propose to use an attention-based abstract generation modela data-driven approach trained to generate informative, concise, and fluent opinion summaries.", "labels": [], "entities": []}, {"text": "Our method is based on the recently proposed framework of neural encoder-decoder models), which translates a sentence in a source language into a target language.", "labels": [], "entities": []}, {"text": "Different from previous work, our summarization system is designed to support multiple input text units.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9776817560195923}]}, {"text": "An attention-based model () is deployed to allow the encoder to automatically search for salient information within context.", "labels": [], "entities": []}, {"text": "Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text.", "labels": [], "entities": []}, {"text": "The importance score of a text unit is estimated from a novel regression model with pairwise preference-based regularizer.", "labels": [], "entities": [{"text": "importance score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9372644126415253}]}, {"text": "With importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments.", "labels": [], "entities": []}, {"text": "Automatic evaluation by BLEU () indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9978347420692444}]}, {"text": "For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9837760329246521}, {"text": "Rotten Tomatoes movie reviews", "start_pos": 50, "end_pos": 79, "type": "DATASET", "confidence": 0.8921796530485153}]}, {"text": "ROUGE evaluation ( also indicates that our system summaries have reasonable information coverage.", "labels": [], "entities": [{"text": "ROUGE evaluation", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9173829257488251}]}, {"text": "Human judges further rated our summaries to be more informative and grammatical than compared systems.", "labels": [], "entities": [{"text": "summaries", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.9606098532676697}]}], "datasetContent": [{"text": "We pre-process the datasets with Stanford CoreNLP () for tokenization and extracting POS tags and dependency relations.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9194985628128052}, {"text": "tokenization", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9626691937446594}]}, {"text": "For RottenTomatoes dataset, we replace movie titles with a generic label in training, and substitute it with the movie name if there is any generic label generated in testing.", "labels": [], "entities": [{"text": "RottenTomatoes dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9601234197616577}]}, {"text": "The size of word representation is set to 300, both for input and output words.", "labels": [], "entities": []}, {"text": "These can be initialized randomly or using pre-trained embeddings learned from Google news ().", "labels": [], "entities": []}, {"text": "We also extend our model with additional features described in.", "labels": [], "entities": []}, {"text": "Discrete features, such as POS tags, are mapped into word representation via lookup tables.", "labels": [], "entities": []}, {"text": "For continuous features (e.g TF-IDF scores), they are attached to word vectors as additional values.", "labels": [], "entities": []}, {"text": "-part of a named entity?", "labels": [], "entities": []}, {"text": "-category in General Inquirer -capitalized?", "labels": [], "entities": []}, {"text": "-sentiment polarity -POS tag (General Inquirer, MPQA) -dependency relation -TF-IDF score) as evaluation metric, which computes the precision of n-grams in generated summaries with gold-standard abstracts as the reference.", "labels": [], "entities": [{"text": "TF-IDF score", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.8808905184268951}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9857056140899658}]}, {"text": "Finally, the importance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 and 5.3. Decoding is performed by beam search with abeam size of 20, i.e. we keep 20 most probable output sequences in stack at each step.", "labels": [], "entities": [{"text": "importance-based sampling rate (K)", "start_pos": 13, "end_pos": 47, "type": "METRIC", "confidence": 0.8991666833559672}]}, {"text": "Outputs with end of sentence token are also considered for re-ranking.", "labels": [], "entities": []}, {"text": "Decoding stops when every beam in stack generates the end of sentence token.", "labels": [], "entities": []}, {"text": "We first evaluate the importance estimation component described in Section 3.5.", "labels": [], "entities": [{"text": "importance estimation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6603983938694}]}, {"text": "We compare with Support Vector Regression (SVR) (Smola and Vapnik, 1997) and two baselines: (1) a length baseline that ranks text units based on their length, and (2) a centroid baseline that ranks text units according to their centroidness, which is computed as the cosine similarity between a text unit and centroid of the cluster to be summarized ().", "labels": [], "entities": []}, {"text": "We evaluate using mean reciprocal rank (MRR), and normalized discounted cumulative gain at top 3 and 5 returned results (NDCG@3).", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 18, "end_pos": 44, "type": "METRIC", "confidence": 0.9096834560235342}, {"text": "normalized discounted cumulative gain", "start_pos": 50, "end_pos": 87, "type": "METRIC", "confidence": 0.7406023666262627}, {"text": "NDCG", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.6710622906684875}]}, {"text": "Text units are considered relevant if they have at least one overlapping content word with the gold-standard summary.", "labels": [], "entities": []}, {"text": "From, we can see that our importance estimation model produces uniformly better ranking performance on both datasets.", "labels": [], "entities": []}, {"text": "For automatic summary evaluation, we consider three popular metrics.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8555252552032471}]}, {"text": "ROUGE () is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9812914133071899}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9901026487350464}]}, {"text": "ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9699867367744446}]}, {"text": "We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9986321330070496}, {"text": "precision-based", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9926304817199707}]}, {"text": "As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 5, "end_pos": 20, "type": "METRIC", "confidence": 0.9094660878181458}]}, {"text": "For comparisons, we first compare with an abstractive summarization method presented in on the RottenTomatoes dataset.", "labels": [], "entities": [{"text": "RottenTomatoes dataset", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.9960657954216003}]}, {"text": "utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic structures of product reviews.", "labels": [], "entities": []}, {"text": "For both datasets, we consider two extractive summarization approaches: (1) LEXRANK () is an unsupervised method that computes text centrality based on PageRank algorithm; (2) propose a supervised SUBMODULAR summarization model which is trained with Support Vector Machines.", "labels": [], "entities": [{"text": "LEXRANK", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9905856251716614}, {"text": "text centrality", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.6825283616781235}, {"text": "SUBMODULAR summarization", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.7705115675926208}]}, {"text": "In addition, LONGEST sentence is picked up as a baseline.", "labels": [], "entities": [{"text": "LONGEST", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9842016100883484}]}, {"text": "Four variations of our system are tested.", "labels": [], "entities": []}, {"text": "One uses randomly initialized word embeddings.", "labels": [], "entities": []}, {"text": "The rest of them use pre-trained word embeddings, additional features in, and their combination.", "labels": [], "entities": []}, {"text": "For all systems, we generate a one-sentence summary.", "labels": [], "entities": []}, {"text": "Our system with pre-trained word embeddings and additional features achieves the best BLEU scores on both datasets (in boldface) with statistical significance (two-tailed Wilcoxon signed rank test, p < 0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9986942410469055}]}, {"text": "Notice that our system summaries are conciser (i.e. shorter on average), which lead to higher scores on precision based-metrics, e.g. BLEU, and lower scores on recall-based metrics, e.g. METEOR and ROUGE.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9891063570976257}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9963576197624207}, {"text": "recall-based", "start_pos": 160, "end_pos": 172, "type": "METRIC", "confidence": 0.9963298439979553}, {"text": "METEOR", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.8712921142578125}, {"text": "ROUGE", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.9644981622695923}]}, {"text": "On RottenTomatoes dataset, where summaries generated by different systems are similar in length, our system still outperforms other methods in METEOR and ROUGE in addition to their significantly better BLEU scores.", "labels": [], "entities": [{"text": "RottenTomatoes dataset", "start_pos": 3, "end_pos": 25, "type": "DATASET", "confidence": 0.9873612225055695}, {"text": "METEOR", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.8878440260887146}, {"text": "ROUGE", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.9815917015075684}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9994885921478271}]}, {"text": "This is not true on Idebate, since the length of summaries by extract-based systems is significantly longer.", "labels": [], "entities": []}, {"text": "But the BLEU scores of our system are considerably higher.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 8, "end_pos": 19, "type": "METRIC", "confidence": 0.9625560939311981}]}, {"text": "Among our four systems, models with pre-trained word embeddings in general achieve better scores.", "labels": [], "entities": []}, {"text": "Though additional features do not always improve the performance, we find that they help our systems converge faster.", "labels": [], "entities": []}, {"text": "For human evaluation, we consider three aspects: informativeness that indicates how much salient information is contained in the summary, grammaticality that measures whether a summary is grammatical, and compactness that denotes whether a summary contains unnecessary information.", "labels": [], "entities": []}, {"text": "Each aspect is rated on a 1 to 5 scale (5 is the best).", "labels": [], "entities": []}, {"text": "The judges are: Automatic evaluation results by BLEU, METEOR, and ROUGE SU-4 scores (multiplied by 100) for abstract generation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9988489151000977}, {"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9474154710769653}, {"text": "ROUGE SU-4 scores", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.8216695189476013}]}, {"text": "The average lengths for human written summaries are 11.5 and 24.6 for RottenTomatoes and Idebate.", "labels": [], "entities": [{"text": "RottenTomatoes", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.97492516040802}]}, {"text": "The best performing system for each column is highlighted in boldface, where our system with pre-trained word embeddings and additional features achieves the best BLEU scores on both datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9986349940299988}]}, {"text": "Our systems that are statistically significantly better than the comparisons are highlighted with * (two-tailed Wilcoxon signed rank test, p < 0.05).", "labels": [], "entities": [{"text": "two-tailed Wilcoxon signed rank test", "start_pos": 101, "end_pos": 137, "type": "METRIC", "confidence": 0.6493800997734069}]}, {"text": "Our system also has the best METEOR and ROUGE scores (in italics) on RottenTomatoes dataset among learning-based systems.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9897088408470154}, {"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.987532913684845}, {"text": "RottenTomatoes dataset", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.9938731789588928}]}, {"text": "Our system achieves the best informativeness and grammaticality scores among the three learning-based systems.", "labels": [], "entities": []}, {"text": "Our summaries are ranked as the best in 18% of the evaluations, and are also ranked higher than compared systems on average.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Automatic evaluation results by BLEU, METEOR, and ROUGE SU-4 scores (multiplied by 100) for abstract generation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9989585876464844}, {"text": "METEOR", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9560064673423767}, {"text": "ROUGE SU-4 scores", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.8479201396306356}]}, {"text": " Table 4: Human evaluation results for abstract generation sys-", "labels": [], "entities": []}, {"text": " Table 4. As it can be  seen, our system outperforms the abstract-based sys- tem OPINOSIS in all aspects, and also achieves bet- ter informativeness and grammaticality scores than  LEXRANK, which extracts sentences in their origi- nal form. Our system summaries are ranked as the  best in 18% of the evaluations, and has an average  ranking of 2.3, which is higher than both OPINOSIS  and LEXRANK on average. An inter-rater agree- ment of Krippendorff's \u03b1 of 0.71 is achieved for", "labels": [], "entities": [{"text": "inter-rater agree- ment of Krippendorff's \u03b1", "start_pos": 412, "end_pos": 455, "type": "METRIC", "confidence": 0.7334487214684486}]}]}