{"title": [{"text": "Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships", "labels": [], "entities": [{"text": "Feuding Families and Former Friends", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8733224987983703}, {"text": "Dynamic Fictional Relationships", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6097277700901031}]}], "abstractContent": [{"text": "Understanding how a fictional relationship between two characters changes overtime (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship.", "labels": [], "entities": [{"text": "digital humanities scholarship", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.7254024744033813}]}, {"text": "We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories.", "labels": [], "entities": []}, {"text": "While previous work on characterizing literary relationships relies on plot summaries annotated with predefined labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels.", "labels": [], "entities": [{"text": "characterizing literary relationships", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.863947848478953}]}, {"text": "We find that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness).", "labels": [], "entities": []}, {"text": "Our model outperforms topic model baselines on two crowdsourced tasks, and we also find interesting correlations to annotations in an existing dataset.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our dataset consists of 1,383 fictional works pulled from Project Gutenberg and other Internet sources.", "labels": [], "entities": []}, {"text": "Project Gutenberg has a limited selection (outside of science fiction) of mostly classic literature, so we add more contemporary novels from various genres such as mystery, romance, and fantasy to our dataset.", "labels": [], "entities": []}, {"text": "To identify character mentions, we run the Book-NLP pipeline of, which includes character name clustering, quoted speaker identification, and coreference resolution.", "labels": [], "entities": [{"text": "character name clustering", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.7059916456540426}, {"text": "quoted speaker identification", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.6131832301616669}, {"text": "coreference resolution", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.9501786530017853}]}, {"text": "1 For ev-ery detected character mention, we define a span as beginning 100 tokens before the mention and ending 100 tokens after the mention.", "labels": [], "entities": []}, {"text": "We do not use sentence or paragraph boundaries because they vary considerably depending on the author (e.g., William Faulkner routinely wrote single sentences longer than many of Hemingway's paragraphs).", "labels": [], "entities": []}, {"text": "All spans in our dataset contain mentions to exactly two characters.", "labels": [], "entities": []}, {"text": "This is a rather strict requirement that forces a reduction in data size, but spans in which more than two characters are mentioned are generally noisier.", "labels": [], "entities": []}, {"text": "Once we have identified usable spans in the dataset, we apply a second filtering step that removes relationships containing fewer than five spans.", "labels": [], "entities": []}, {"text": "Without this filter, our dataset is dominated by fleeting interactions between minor characters; this is undesirable since our focus is on longer, mutable relationships.", "labels": [], "entities": []}, {"text": "Finally, we filter our vocabulary by removing the 500 most frequently occurring words, as well as all words that occur in fewer than 100 books.", "labels": [], "entities": []}, {"text": "The latter step helps correct for variation in time period and genre (e.g., \"thou\" and \"thy\" found in older works like the Canterbury Tales).", "labels": [], "entities": [{"text": "Canterbury Tales", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8938119113445282}]}, {"text": "Our final dataset contains 20,013 relationships and 380,408 spans, while our vocabulary contains 16,223 words.", "labels": [], "entities": []}, {"text": "In our descriptor interpretability experiments, we vary the number of descriptors (topics) for all models (K = 10, 30, 50).", "labels": [], "entities": []}, {"text": "We train LDA and NUBBI for 100 iterations with a collapsed Gibbs sampler, and the HTMM uses the default setting of 100 EM iterations.", "labels": [], "entities": []}, {"text": "For the RMN, we initialize the word embedding matrix L with 300-dimensional GloVe embeddings trained on the Common Crawl (Pennington et al., NUBBI requires additional spans that mention only a single character to differentiate character topics from relationship topics.", "labels": [], "entities": [{"text": "RMN", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8402925133705139}, {"text": "NUBBI", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.8841051459312439}]}, {"text": "None of the other models receives these extra data.", "labels": [], "entities": []}, {"text": "The RMN learns more interpretable descriptors than three topic model baselines.", "labels": [], "entities": []}], "tableCaptions": []}