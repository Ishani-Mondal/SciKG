{"title": [{"text": "Message from the General Chair Message from the Program Co-Chairs Patterns of Wisdom: Discourse-Level Style in Multi-Sentence Quotations A Joint Model of Orthography and Morphological Segmentation Best Long Papers Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships Learning to Compose Neural Networks for Question Answering Long Paper, Runners Up Multi-way, Multilingual Neural Machine Translation with a Shared Attention Mechanism Black Holes and White Rabbits: Metaphor Identification with Visual Features Invited Talk: How can NLP help cure cancer? Invited Talk: Evaluating Natural Language Generation Systems", "labels": [], "entities": [{"text": "Question Answering Long Paper", "start_pos": 349, "end_pos": 378, "type": "TASK", "confidence": 0.827388197183609}, {"text": "Multilingual Neural Machine Translation", "start_pos": 402, "end_pos": 441, "type": "TASK", "confidence": 0.6705410927534103}, {"text": "Evaluating Natural Language Generation", "start_pos": 610, "end_pos": 648, "type": "TASK", "confidence": 0.6858221814036369}]}], "abstractContent": [{"text": "Natural Language Generation (NLG) systems have different characteristics than other NLP systems, which effects how they are evaluated.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.754324346780777}]}, {"text": "In particular, it can be difficult to meaningfully evaluate NLG texts by comparing them against gold-standard reference texts, because (A) there are usually many possible texts which are acceptable to users and (B) some NLG systems produce texts which are better (as judged by human users) than human-written corpus texts.", "labels": [], "entities": []}, {"text": "Partially because of these reasons, the NLG community places much more emphasis on human-based evaluations than most areas of NLP.", "labels": [], "entities": []}, {"text": "I will discuss the various ways in which NLG systems are evaluated, focusing on human-based evaluations.", "labels": [], "entities": []}, {"text": "These typically either measure the success of generated texts at achieving a goal (eg, measuring how many people change their behaviour after reading behaviour-change texts produced by an NLG system); or ask human subjects to rate various aspects of generated texts (such as readability, accuracy, and appropriateness), often on Likert scales.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9985174536705017}]}, {"text": "I will use examples from evaluations I have carried out, and highlight some of the lessons I have learnt, including the importance of reporting negative results, the difference between laboratory and real-world evaluations, and the need to look at worse-case as well as average-case performance.", "labels": [], "entities": []}, {"text": "I hope my talk will be interesting and relevant to anyone who is interested in the evaluation of NLP systems.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}