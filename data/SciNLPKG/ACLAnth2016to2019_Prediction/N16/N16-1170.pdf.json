{"title": [{"text": "Learning Natural Language Inference with LSTM", "labels": [], "entities": [{"text": "Learning Natural Language Inference", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5825997740030289}, {"text": "LSTM", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.4872837960720062}]}], "abstractContent": [{"text": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7877914855877558}]}, {"text": "The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 22, "end_pos": 71, "type": "DATASET", "confidence": 0.6017767041921616}, {"text": "natural language inference (NLI)", "start_pos": 176, "end_pos": 208, "type": "TASK", "confidence": 0.7721515496571859}]}, {"text": "In this paper, we propose a special long short-term memory (LSTM) architecture for NLI.", "labels": [], "entities": []}, {"text": "Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea.", "labels": [], "entities": []}, {"text": "Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-byword matching of the hypothesis with the premise.", "labels": [], "entities": []}, {"text": "This LSTM is able to place more emphasis on important word-level matching results.", "labels": [], "entities": [{"text": "word-level matching", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.680995523929596}]}, {"text": "In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label.", "labels": [], "entities": []}, {"text": "On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.8485205173492432}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9996028542518616}]}], "introductionContent": [{"text": "Natural language inference (NLI) is the problem of determining whether from a premise sentence P one can infer another hypothesis sentence H).", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7882261325915655}]}, {"text": "NLI is a fundamentally important problem that has applications in many tasks including question answering, semantic search and automatic text summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9080907106399536}, {"text": "semantic search", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.8107895851135254}, {"text": "automatic text summarization", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.5929542084534963}]}, {"text": "There has been much interest in NLI in the past decade, especially surrounding the PASCAL Recognizing Textual Entailment (RTE) Challenge ( ).", "labels": [], "entities": [{"text": "PASCAL Recognizing Textual Entailment (RTE) Challenge", "start_pos": 83, "end_pos": 136, "type": "TASK", "confidence": 0.7378273941576481}]}, {"text": "Existing solutions to NLI range from shallow approaches based on lexical similarities ( ) to advanced methods that consider syntax (, perform explicit sentence alignment or use formal logic).", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.7685153484344482}]}, {"text": "Recently, released the Stanford Natural Language Inference (SNLI) corpus for the purpose of encouraging more learning-centered approaches to NLI.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 23, "end_pos": 72, "type": "DATASET", "confidence": 0.6245080940425396}]}, {"text": "This corpus contains around 570K sentence pairs with three labels: entailment, contradiction and neutral.", "labels": [], "entities": []}, {"text": "The size of the corpus makes it now feasible to train deep neural network models, which typically require a large amount of training data.", "labels": [], "entities": []}, {"text": "tested a straightforward architecture of deep neural networks for NLI.", "labels": [], "entities": []}, {"text": "In their architecture, the premise and the hypothesis are each represented by a sentence embedding vector.", "labels": [], "entities": []}, {"text": "The two vectors are then fed into a multi-layer neural network to train a classifier.", "labels": [], "entities": []}, {"text": "achieved an accuracy of 77.6% when long short-term memory (LSTM) networks were used to obtain the sentence embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999489426612854}]}, {"text": "A more recent work by improved the performance by applying a neural attention model.", "labels": [], "entities": []}, {"text": "While their basic architecture is still based on sentence embeddings for the premise and the hypothesis, a key difference is that the embedding of the premise takes into consideration the alignment between the premise and the hypothesis.", "labels": [], "entities": []}, {"text": "This so-called attention-weighted representation of the premise was shown to help push the accuracy to 83.5% on the SNLI corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9996440410614014}, {"text": "SNLI corpus", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.7536672651767731}]}, {"text": "A limitation of the aforementioned two models is that they reduce both the premise and the hypothesis to a single embedding vector before matching them; i.e., in the end, they use two embedding vectors to perform sentence-level matching.", "labels": [], "entities": [{"text": "sentence-level matching", "start_pos": 213, "end_pos": 236, "type": "TASK", "confidence": 0.6831717938184738}]}, {"text": "However, not all word or phrase-level matching results are equally important.", "labels": [], "entities": [{"text": "word or phrase-level matching", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.5732192844152451}]}, {"text": "For example, the matching between stop words in the two sentences is not likely to contribute much to the final prediction.", "labels": [], "entities": []}, {"text": "Also, fora hypothesis to contradict a premise, a single word or phrase-level mismatch (e.g., a mismatch of the subjects of the two sentences) maybe sufficient and other matching results are less important, but this intuition is hard to be captured if we directly match two sentence embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew LSTM-based architecture for learning natural language inference.", "labels": [], "entities": [{"text": "learning natural language inference", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.6737343668937683}]}, {"text": "Different from previous models, our prediction is not based on whole sentence embeddings of the premise and the hypothesis.", "labels": [], "entities": []}, {"text": "Instead, we use an LSTM to perform word-by-word matching of the hypothesis with the premise.", "labels": [], "entities": []}, {"text": "Our LSTM sequentially processes the hypothesis, and at each position, it tries to match the current word in the hypothesis with an attention-weighted representation of the premise.", "labels": [], "entities": []}, {"text": "Matching results that are critical for the final prediction will be \"remembered\" by the LSTM while less important matching results will be \"forgotten.\"", "labels": [], "entities": [{"text": "LSTM", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.7148311734199524}]}, {"text": "We refer to this architecture a match-LSTM, or mLSTM for short.", "labels": [], "entities": []}, {"text": "Experiments show that our mLSTM model achieves an accuracy of 86.1% on the SNLI corpus, outperforming the state of the art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994407296180725}, {"text": "SNLI corpus", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9289705157279968}]}, {"text": "Furthermore, through further analyses of the learned parameters, we show that the mLSTM architecture can indeed pickup the more important word-level matching results that need to be remembered for the final prediction.", "labels": [], "entities": []}, {"text": "In particular, we observe that good wordlevel matching results are generally \"forgotten\" but important mismatches, which often indicate a contradiction or a neutral relationship, tend to be \"remembered.\"", "labels": [], "entities": [{"text": "wordlevel matching", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6564178466796875}]}], "datasetContent": [{"text": "Data: We use the SNLI corpus to test the effectiveness of our model.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8254787921905518}]}, {"text": "The original data set contains 570,152 sentence pairs, each labeled with one of the following relationships: entailment, contradiction, neutral and -, where -indicates alack of consensus from the human annotators.", "labels": [], "entities": []}, {"text": "We discard the sentence pairs labeled with -and keep the remaining ones for our experiments.", "labels": [], "entities": []}, {"text": "In the end, we have 549,367 pairs for training, 9,842 pairs for development and 9,824 pairs for testing.", "labels": [], "entities": []}, {"text": "This follows the same data partition used by in their experiments.", "labels": [], "entities": []}, {"text": "We perform three-class classification and use accuracy as our evaluation metric.", "labels": [], "entities": [{"text": "three-class classification", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6720772683620453}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9992637038230896}]}, {"text": "Parameters: We use the Adam method () with hyperparameters \u03b2 1 set to 0.9 and \u03b2 2 set to 0.999 for optimization.", "labels": [], "entities": []}, {"text": "The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration.", "labels": [], "entities": []}, {"text": "The batch size is set to be 30.", "labels": [], "entities": []}, {"text": "We experiment with d = 150 and d = 300 where dis the dimension of all the hidden states.", "labels": [], "entities": []}, {"text": "Methods for comparison: We mainly want to compare our model with the word-by-word attention model by because this model achieved the state-of-the-art performance on the SNLI corpus.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 169, "end_pos": 180, "type": "DATASET", "confidence": 0.8887568712234497}]}, {"text": "To ensure fair comparison, besides comparing with the accuracy reported by, we also re-implemented their model and report the performance of our implementation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9972314238548279}]}, {"text": "We also consider a few variations of our model.", "labels": [], "entities": []}, {"text": "Specifically, the following models are implemented and tested in our experiments: \u2022 Word-by-word attention (d = 150): This is our implementation of the word-by-word attention model by, where we set the dimension of the hidden states to 150.", "labels": [], "entities": []}, {"text": "The differences between our implementation and the original implementation by are the following: (1) We also add a NULL token to the premise for matching.", "labels": [], "entities": []}, {"text": "(2) We do not feed the last cell state of the LSTM for the premise to the LSTM for the hypothesis, to keep it consistent with the implementation of our model.", "labels": [], "entities": []}, {"text": "(3) For word representation, we also use the GloVe word embeddings and we do not update the word embeddings.", "labels": [], "entities": [{"text": "word representation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8057054579257965}]}, {"text": "For unseen words, we adopt the same strategy as described in Section 2.3.", "labels": [], "entities": []}, {"text": "\u2022 mLSTM (d = 150): This is our mLSTM model with d set to 150.", "labels": [], "entities": []}, {"text": "\u2022 mLSTM with bi-LSTM sentence modeling (d = 150): This is the same as the model above except that when we derive the hidden states h s j and ht k of the two sentences, we use bi-LSTMs (Graves, 2012) instead of LSTMs.", "labels": [], "entities": []}, {"text": "We implement this model to see whether biLSTMs allow us to better align the sentences.", "labels": [], "entities": []}, {"text": "\u2022 mLSTM (d = 300): This is our mLSTM model with d set to 300.", "labels": [], "entities": []}, {"text": "\u2022 mLSTM with word embedding (d = 300): This is the same as the model above except that we directly use the word embedding vectors x s j and x t k instead of the hidden states h s j and ht kin our model.", "labels": [], "entities": []}, {"text": "In this case, each attention vector a k is a weighted sum of {x s j } M j=1 . We experiment with this setting because we hypothesize that the effectiveness of our model is largely related to the mLSTM architecture rather than the use of LSTMs to process the original sentences.", "labels": [], "entities": []}, {"text": "compares the performance of the various models we tested together with some previously reported results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment results in terms of accuracy. d is the dimension of the hidden states. |\u03b8|W+M is the total number of parameters", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992664456367493}]}, {"text": " Table 2: The confusion matrix of the results by mLSTM with", "labels": [], "entities": [{"text": "mLSTM", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.856009304523468}]}, {"text": " Table 2. We  can see that there is more confusion between neu- tral and entailment and between neutral and contra-", "labels": [], "entities": []}]}