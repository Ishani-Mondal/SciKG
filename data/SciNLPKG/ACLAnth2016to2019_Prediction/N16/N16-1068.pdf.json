{"title": [{"text": "Structured Prediction with Output Embeddings for Semantic Image Annotation", "labels": [], "entities": [{"text": "Structured Prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7863803207874298}, {"text": "Semantic Image Annotation", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6200363139311472}]}], "abstractContent": [{"text": "We address the task of annotating images with semantic tuples.", "labels": [], "entities": []}, {"text": "Solving this problem requires an algorithm able to deal with hundreds of classes for each argument of the tuple.", "labels": [], "entities": []}, {"text": "In such contexts, data sparsity becomes a key challenge.", "labels": [], "entities": []}, {"text": "We propose handling this spar-sity by incorporating feature representations of both the inputs (images) and outputs (ar-gument classes) into a factorized log-linear model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many important problems in machine learning can be framed as structured prediction tasks where the goal is to learn functions that map inputs to structured outputs such as sequences, trees or general graphs.", "labels": [], "entities": []}, {"text": "A wide range of applications involve learning overlarge state spaces, e.g., if the output is a labeled graph, each node of the graph may take values over a potentially large set of labels.", "labels": [], "entities": []}, {"text": "Data sparsity then becomes a challenge, as there will be many classes with very few training examples.", "labels": [], "entities": []}, {"text": "Within this context, we are interested in the task of predicting semantic tuples for images.", "labels": [], "entities": [{"text": "predicting semantic tuples", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8501484195391337}]}, {"text": "That is, given an input image we seek to predict what are the events or actions (referred here as predicates), who and what are the participants (referred here as actors) of the actions and where is the action taking place (referred here as locatives).", "labels": [], "entities": []}, {"text": "For example, an image might be annotated with the semantic tuples: run, dog, park and play, dog, grass.", "labels": [], "entities": []}, {"text": "We call each field of a tuple an argument.", "labels": [], "entities": []}, {"text": "To handle the data sparsity challenge imposed by the large state space, we will leverage an approach that has proven to be useful in multiclass and multilabel prediction tasks ().", "labels": [], "entities": [{"text": "multilabel prediction tasks", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.7618004480997721}]}, {"text": "The idea is to represent a value for an argument a using a feature vector representation \u03c6 \u2208 IR n . We will integrate this argument representation into the structured prediction model.", "labels": [], "entities": []}, {"text": "In summary, our main contribution is to propose an approach that incorporates feature representations of the outputs into a structured prediction model, and apply it to the problem of annotating images with semantic tuples.", "labels": [], "entities": []}, {"text": "We present an experimental study using different output feature representations and analyze how they affect performance for different argument types.", "labels": [], "entities": []}], "datasetContent": [{"text": "For image features we use the 4,096-dimensional second to last layer of BVLC implementation of 'AlexNet' ImageNet model, a Convolutional Neural Network (CNN) as described in.", "labels": [], "entities": []}, {"text": "To test our method we used the 100 test images that were annotated with ground-truth semantic tuples.", "labels": [], "entities": []}, {"text": "To measure performance we first predict the top tuple for each image and then measure accuracy for each argument type (i.e. the number of correct predictions among the top 1 triplets).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995052814483643}]}, {"text": "The regularization parameters of each model were set using the validation set.", "labels": [], "entities": []}, {"text": "We compare the performance of the following models: 1) Baseline Separate Predictors (SPred): We consider a baseline made of independent predictors for each argument type.", "labels": [], "entities": [{"text": "Baseline Separate Predictors (SPred)", "start_pos": 55, "end_pos": 91, "type": "METRIC", "confidence": 0.7304577628771464}]}, {"text": "More specifically we train one-vs-all SVMs (we also tried multi-class SVMs but they did not improve performance) to independently predict locatives, predicates and actors.", "labels": [], "entities": []}, {"text": "For each argument type and candidate label we have a score computed by the corresponding SVM.", "labels": [], "entities": []}, {"text": "Given an image we generate the top tuples that maximize the sum of scores for each argument type; 2) Baseline KCCA: This model implements the Kernel Canonical Correlation Anal- <act=people,pre=gather,loc=air>3 ysis approach of.", "labels": [], "entities": []}, {"text": "We first note that this approach is able to rank a list of candidate captions but cannot directly generate tuples.", "labels": [], "entities": []}, {"text": "To generate tuples for test images, we first find the caption in the training set that has the highest ranking score for that image and then extract the corresponding semantic tuples from that caption; 3) Indicator Features (IND), this is a standard factorized log-linear model that does not use any feature representation for the outputs; 4) A model that uses the skip-gram continuous word representation of outputs (SCWR); 5) A model that uses that semantic equivalence representation of outputs (SER); 6) A combined model that makes predictions using the best feature representation for each argument type (COMBO).", "labels": [], "entities": [{"text": "Indicator Features (IND)", "start_pos": 205, "end_pos": 229, "type": "METRIC", "confidence": 0.691516387462616}, {"text": "SER", "start_pos": 499, "end_pos": 502, "type": "METRIC", "confidence": 0.9179269075393677}, {"text": "COMBO", "start_pos": 610, "end_pos": 615, "type": "METRIC", "confidence": 0.7880755662918091}]}, {"text": "We observe that our proposed method performs significantly better than the baselines.", "labels": [], "entities": []}, {"text": "The second observation is that the best performing output feature representation is different for different argument types, for the locatives the best representation is SER, for the predicates is the SCWR and for the actors using an output feature representation actually hurts performance.", "labels": [], "entities": [{"text": "SER", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9977378845214844}]}, {"text": "The biggest improvement we get is on the predicate arguments, where we improve almost by 10% in average precision over the baseline using the skip-gram word representation.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9874287247657776}]}, {"text": "Overall, the model that uses the best representation performs better than the indicator baseline.", "labels": [], "entities": []}, {"text": "Regarding the rank of the parameter matrices, we observed that the learned models can work well even if we drop the rank to 10% of its maximum rank.", "labels": [], "entities": []}, {"text": "This shows that the learned models are efficient in the sense that they can work well with lowdimensional projections of the features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Output Feature Representation.", "labels": [], "entities": [{"text": "Output Feature Representation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.5840205550193787}]}]}