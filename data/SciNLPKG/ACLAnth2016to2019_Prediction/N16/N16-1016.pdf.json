{"title": [{"text": "A Long Short-Term Memory Framework for Predicting Humor in Dialogues", "labels": [], "entities": [{"text": "Predicting Humor in Dialogues", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.9468766450881958}]}], "abstractContent": [{"text": "We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues.", "labels": [], "entities": [{"text": "predict humor in dialogues", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.8159277439117432}]}, {"text": "We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react.", "labels": [], "entities": []}, {"text": "We model the setup-punchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network.", "labels": [], "entities": []}, {"text": "Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9992305040359497}]}, {"text": "We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9992372989654541}]}, {"text": "We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.", "labels": [], "entities": [{"text": "humor prediction", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7690757215023041}]}], "introductionContent": [{"text": "There has been many recent attempts to detect and understand humor, irony and sarcasm from sentences, usually taken from Twitter (, customer reviews (Reyes and Rosso, 2012) or generic canned jokes ( . and included the surrounding context.", "labels": [], "entities": [{"text": "understand humor, irony and sarcasm from sentences", "start_pos": 50, "end_pos": 100, "type": "TASK", "confidence": 0.6864337399601936}]}, {"text": "Our work has a different focus from the above.", "labels": [], "entities": []}, {"text": "We analyze transcripts of funny dialogues, a genre somehow neglected but important for human-robot interaction.", "labels": [], "entities": []}, {"text": "Laughter is the natural reaction of people to a verbal or textual humorous stimulus.", "labels": [], "entities": [{"text": "Laughter is the natural reaction of people to a verbal or textual humorous stimulus", "start_pos": 0, "end_pos": 83, "type": "TASK", "confidence": 0.6741294754402978}]}, {"text": "We want to predict when the audience would laugh.", "labels": [], "entities": []}, {"text": "Compared to atypical canned joke or a sarcastic Tweet, a dialog utterance is perceived as funny only in relation to the dialog context and the past history.", "labels": [], "entities": []}, {"text": "Ina spontaneous setting a funny dialog is usually built through a setup which prepares the audience to receive the humorous discourse stimuli, followed by a punchline which releases the tension and triggers the laughter reaction).", "labels": [], "entities": []}, {"text": "Automatic understanding of a humorous dialog is a first step to build an effective empathetic machine fully able to react to the user's humor and to other discourse stimuli.", "labels": [], "entities": []}, {"text": "We are ultimately interested in developing robots that can bond with humans better.", "labels": [], "entities": []}, {"text": "As a source of situational humor we study a popular TV sitcom: \"The Big Bang Theory\".", "labels": [], "entities": []}, {"text": "The domain of sitcoms is of interest as it provides a full dialog setting, together with an indication of when the audience is expected to laugh, given by the background canned laughters.", "labels": [], "entities": []}, {"text": "An example of dialog from this sitcom, as well as of the setup-punchline schema, is shown below (punchlines in bold): PENNY: Okay, Sheldon, what can I get you?", "labels": [], "entities": [{"text": "PENNY", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9807196259498596}]}, {"text": "PENNY: Could you be a little more specific?", "labels": [], "entities": [{"text": "PENNY", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7912696599960327}]}, {"text": "LAUGH PENNY: I'm sorry, honey, I don't know milliliters.", "labels": [], "entities": [{"text": "LAUGH", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7744423151016235}, {"text": "PENNY", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.8505637049674988}]}, {"text": "Blame President James Jimmy Carter.", "labels": [], "entities": [{"text": "President James Jimmy Carter", "start_pos": 6, "end_pos": 34, "type": "DATASET", "confidence": 0.7294641137123108}]}, {"text": "LAUGH He started America on a path to the metric system but then just gave up.", "labels": [], "entities": [{"text": "LAUGH", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6199634671211243}, {"text": "America", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9003873467445374}, {"text": "metric", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9764904975891113}]}, {"text": "LAUGH The utterances before the punchline are the setup.", "labels": [], "entities": [{"text": "LAUGH", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.908018946647644}]}, {"text": "Without them, the punchlines may not be perceived as humorous (the last utterance, out of context, maybe apolitical complaint), only with proper setup a laughter would be triggered.", "labels": [], "entities": []}, {"text": "The humorous intent is also strengthen by the fact the dialog takes place in a bar (evident from the previous and following utterances), where a request of 40 ml of \"Ethyl Alcohol\" is unusual and weird.", "labels": [], "entities": []}, {"text": "Our previous attempts on the same corpus showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal.", "labels": [], "entities": []}, {"text": "For this reason we propose a method based on a Long Short-Term Memory network), where we encode each sentence through a Convolutional Neural Network).", "labels": [], "entities": []}, {"text": "LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (), dependency parsing ) and conversation modelling.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7718082070350647}, {"text": "dependency parsing", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7629479765892029}, {"text": "conversation modelling", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.8728479743003845}]}, {"text": "This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks.", "labels": [], "entities": [{"text": "humor response prediction", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.8132254282633463}, {"text": "general humor detection tasks", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.6916727274656296}]}], "datasetContent": [{"text": "In the neural network we set the size to 100 for all the hidden layers of the CNN and the LSTM, and 5 to the convolutional window.", "labels": [], "entities": []}, {"text": "We applied a dropout regularization layer () after the output of the LSTM, and L2 regularization on the softmax output layer.", "labels": [], "entities": []}, {"text": "The network was trained with standard backpropagation, using each scene as a training unit.", "labels": [], "entities": []}, {"text": "The development set was used to tune the hyperparameters, and to determine the early stopping condition.", "labels": [], "entities": []}, {"text": "When the error on the development set began to increase for the first time we kept training only the final softmax layer, this improved the overall results.", "labels": [], "entities": []}, {"text": "The neural network was implemented with THEANO toolkit (.", "labels": [], "entities": [{"text": "THEANO", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8286410570144653}]}, {"text": "We ran experiments with and without the extra high-level feature vector.", "labels": [], "entities": []}, {"text": "As a baseline for comparison we used an implementation of the Conditional Random Field) from CRFSuite, with L2 regularization.", "labels": [], "entities": [{"text": "CRFSuite", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9030442237854004}]}, {"text": "We ran experiments using the same high level feature vector added at the end of the neural network, 1-2-3gram features of a window made by the utterance and the four previous, and the two feature sets combined.", "labels": [], "entities": []}, {"text": "We also compared the overall system where we replace the CNN with an LSTM sentence encoder (, where we kept the same input features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison between a CNN and a LSTM sentence", "labels": [], "entities": []}]}