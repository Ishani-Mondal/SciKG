{"title": [{"text": "Learning Distributed Word Representations For Bidirectional LSTM Recurrent Neural Network *", "labels": [], "entities": [{"text": "Learning Distributed Word Representations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5429050996899605}]}], "abstractContent": [{"text": "Bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) has been successfully applied in many tagging tasks.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.926476240158081}]}, {"text": "BLSTM-RNN relies on the distributed representation of words, which implies that the former can be futhermore improved through learning the latter better.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8262243866920471}]}, {"text": "In this work, we propose a novel approach to learn distributed word representations by training BLSTM-RNN on a specially designed task which only relies on unlabeled data.", "labels": [], "entities": [{"text": "learn distributed word representations", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.5753916352987289}]}, {"text": "Our experimental results show that the proposed approach learns useful distributed word representations, as the trained representations significantly elevate the performance of BLSTM-RNN on three tagging tasks: part-of-speech tagging, chunking and named entity recognition, surpassing word representations trained by other published methods.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 211, "end_pos": 233, "type": "TASK", "confidence": 0.7206101417541504}, {"text": "named entity recognition", "start_pos": 248, "end_pos": 272, "type": "TASK", "confidence": 0.6122488081455231}]}], "introductionContent": [{"text": "Distributed word representations represent word with areal valued vector, which is also referred to * The work was partially supported by the National Natural Science Foundation of China as word embedding.", "labels": [], "entities": []}, {"text": "Well learned distributed word representations have been shown capable of capturing semantic and syntactic regularities) and enhancing neural network model by being used as features.", "labels": [], "entities": []}, {"text": "Sequence tagging is a basic structure learning task for natural language processing.", "labels": [], "entities": [{"text": "Sequence tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9165765643119812}, {"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6369792719682058}]}, {"text": "Many primary processing tasks over sentence such as word segmentation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7453519999980927}, {"text": "named entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6175116300582886}, {"text": "part-of-speech tagging", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7114275097846985}]}, {"text": "Recently, many state-of-the-art systems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) recurrent neural network (RNN), for example, slot filling (, part-of-speech tagging (, and dependency parsing etc.", "labels": [], "entities": [{"text": "tagging related tasks", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.867261528968811}, {"text": "slot filling", "start_pos": 176, "end_pos": 188, "type": "TASK", "confidence": 0.8404674828052521}, {"text": "part-of-speech tagging", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.7292750775814056}, {"text": "dependency parsing", "start_pos": 222, "end_pos": 240, "type": "TASK", "confidence": 0.7746372520923615}]}, {"text": "All of these systems use distributed representation of words to involve word level information.", "labels": [], "entities": []}, {"text": "Better trained word representations would further improve the state-of-the-art performance of these tasks which makes it worthy to research the training methods of word representations.", "labels": [], "entities": []}, {"text": "The existing training methods of word representation can generally be divided into two classes: 1) Matrix factorization methods.", "labels": [], "entities": [{"text": "word representation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7733826041221619}]}, {"text": "These methods utilize low-rank approximation to decompose a large matrix that contains corpus statistics.", "labels": [], "entities": []}, {"text": "One typical work is the latent semantic analysis (LSA)) in which the matrix records \"term-document\" information, i.e., the rows cor-respond to words, and the columns correspond to different documents in the corpus.", "labels": [], "entities": [{"text": "latent semantic analysis (LSA))", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7862066576878229}]}, {"text": "Another work is hyperspace analogue to language (HAL)) which decomposes the matrix recording \"term-term\" information, i.e., the rows correspond to words and columns correspond to the number of times that a word occurs in the given context.", "labels": [], "entities": [{"text": "hyperspace analogue to language (HAL))", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.650189220905304}]}, {"text": "This type of methods learn representations by training a neural network model to make prediction within local context windows.", "labels": [], "entities": []}, {"text": "For example, ( learns word representation through a feedforward neural network language model which predicts a word given its previous several words.) trains a neural network to judge the validity of a given context.", "labels": [], "entities": [{"text": "word representation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7066693603992462}]}, {"text": "( proposes skip-gram and continuous bag-of-words (CBOW) models based on a single-layer network architecture.", "labels": [], "entities": []}, {"text": "The objective of skip-gram model is to predict the context given the word itself, while the objective of CBOW is to predict a word given its context.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.6803086400032043}]}, {"text": "Aside from these two sets of methods, distributed representation can also be obtained by training recurrent neural network (RNN) language model proposed by) or GloVe model proposed by) which trains a log-bilinear model on word-word co-occurrence counts.", "labels": [], "entities": []}, {"text": "All of these methods suffer from shortcomings that might limit the quality of trained word distributions.", "labels": [], "entities": []}, {"text": "The matrix factorization family only uses the statistics of co-occurrence counts, disregarding of the position of word in sentence and word order.", "labels": [], "entities": []}, {"text": "The window-based methods only consider local context, which is incapable of involving information outside the context window.", "labels": [], "entities": []}, {"text": "While RNN language model theoretically considers all information of the previous sequence, but fails to involve the information of the posterior sequence.", "labels": [], "entities": []}, {"text": "The word-word cooccurence counts that GloVe model relies on also only include information within a limited sized context window.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method to obtain word representation by training BLSTM-RNN model on a specifically designed tagging task.", "labels": [], "entities": [{"text": "word representation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7484990060329437}]}, {"text": "Since BLSTM-RNN theoretically involves all information of input sentence, our approach avoids those shortages suffered by most current methods.", "labels": [], "entities": []}, {"text": "We firstly introduce the structure of BLSTM-RNN used to learn word representations in section 2.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.5498668551445007}]}, {"text": "Then the tagging task for training BLSTM-RNN is described in section 3.", "labels": [], "entities": [{"text": "tagging", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9647805690765381}, {"text": "BLSTM-RNN", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.458453506231308}]}, {"text": "Experiments are presented in section 4, followed by conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "To construct corpus for training word representations, we use North American news which contains about 536 million words as unlabeled data.", "labels": [], "entities": [{"text": "training word representations", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6435817778110504}]}, {"text": "The North American news data is first tokenized with the Penn Treebank tokenizer script . Consecutive digits occurring within a word are replaced with the symbol \"#\" . For example, both words \"tel92\" and \"tel6\" are converted into \"tel#\".", "labels": [], "entities": [{"text": "Penn Treebank tokenizer script", "start_pos": 57, "end_pos": 87, "type": "DATASET", "confidence": 0.9651299864053726}]}, {"text": "The vocabulary is limited to the most frequent 100,000 words in North American news corpus, plus one single \"UNK\" symbol for replacing all out of vocabulary words.", "labels": [], "entities": []}, {"text": "The threshold to determine whether a word is replaced is 0.2, which means about 20% tokens in corpus are replaced with tokens randomly selected from vocabulary.", "labels": [], "entities": []}, {"text": "BLSTM-RNN is implemented based on CUR-RENNT (), an open source GPU-based toolkit of BLSTM-RNN.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9230613708496094}]}, {"text": "The dimension of word representation as well as input layer size of BLSTM-RNN is 100 and hidden layer size is 128.", "labels": [], "entities": [{"text": "word representation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7065243870019913}, {"text": "BLSTM-RNN", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.7910112738609314}]}, {"text": "Three published methods for training word representations are compared: Skip-gram (), CBOW (Mikolov et al., 2013a) and GloVe ().", "labels": [], "entities": [{"text": "training word representations", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6264383892218272}, {"text": "GloVe", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9239662289619446}]}, {"text": "They are reported superior in capturing meaningful latent structures than other previous works in (), thus are regarded as the stateof-the-art approach of training word representations.", "labels": [], "entities": [{"text": "training word representations", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.5954335629940033}]}, {"text": "We train the Skip-gram and CBOW model using the word2vec tool () with a context window size of 10 and 10 negative samples.", "labels": [], "entities": []}, {"text": "For training GloVe, we use the GloVe tool () with a context window size 10.", "labels": [], "entities": []}, {"text": "These configurations are set by following).", "labels": [], "entities": []}, {"text": "Training corpus, vocabulary and dimension of word representations are set the same as that in experiment for training word representations with BLSTM-RNN 2 . Original Sentence: They seem to be prepared to make . .", "labels": [], "entities": []}, {"text": "The quality of trained distributed representation is evaluated by the performance of BLSTM-RNN which uses the trained representations on practical tasks.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.7534359097480774}]}, {"text": "The representations which lead to better performance are considered containing more useful latent information and are judged better.", "labels": [], "entities": []}, {"text": "The structure of BLSTM-RNN to test word representations is the same as that in Figure1.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.6160092353820801}]}, {"text": "To use trained representation, we initialize the weight matrix W 1 with these external representations.", "labels": [], "entities": []}, {"text": "For words without corresponding external representations, their representations are initialized with uniformly distributed random values, ranging from -0.1 to 0.1.", "labels": [], "entities": []}, {"text": "Three typical tagging tasks are used for the evaluation: partof-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER).", "labels": [], "entities": [{"text": "partof-speech tagging (POS)", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8114675045013428}, {"text": "named entity recognition (NER)", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7649457901716232}]}, {"text": "\u2022 The POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.812717080116272}, {"text": "Wall Street Journal data from Penn Treebank III", "start_pos": 49, "end_pos": 96, "type": "DATASET", "confidence": 0.9518495500087738}]}, {"text": "Training, development and test sets are split according to in).", "labels": [], "entities": []}, {"text": "Performance is evaluated by the accuracy of predicted tags on test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9988693594932556}]}, {"text": "\u2022 CHUNK experiment is conducted on the data of CoNLL-2000 shared task).", "labels": [], "entities": [{"text": "CHUNK", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.6823016405105591}, {"text": "CoNLL-2000 shared task", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.8386494517326355}]}, {"text": "Performance is assessed by the F1 score computed by the evaluation script released by the CoNLL-2000 shared task 3 . \u2022 NER experiment is conducted on the CoNLL-2003 shared task).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9719828367233276}, {"text": "CoNLL-2000 shared task 3", "start_pos": 90, "end_pos": 114, "type": "DATASET", "confidence": 0.9024316817522049}, {"text": "CoNLL-2003 shared task", "start_pos": 154, "end_pos": 176, "type": "DATASET", "confidence": 0.9076681534449259}]}, {"text": "Performance is measured by the F1 score calculated by the evaluation script of the CoNLL-2003 shared task 4 . To focus on the effect of word representation, for all tasks, we use the network with the same hidden structure and input features.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9769721329212189}, {"text": "CoNLL-2003 shared task 4", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.874276652932167}, {"text": "word representation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.6929781585931778}]}, {"text": "The size of input layer is 100, size of BLSTM hidden layer is 128 and output layer size is set as the number of tag types according to the specific tagging task.", "labels": [], "entities": [{"text": "BLSTM hidden layer", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.6561072667439779}]}, {"text": "Input features are composed of word identity and threedimensional binary vector to indicate if the word is full lowercase, full uppercase or leading with a capital letter.", "labels": [], "entities": []}, {"text": "presents the performance of BLSTM-RNN with different distributed representations on these three tasks.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.6682443618774414}]}, {"text": "BLSTM is the baseline system that does not involve external word representations.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6500080227851868}]}, {"text": "Among all representations, BLSTMWE which is trained by our approach gets the best performance on all three tasks.", "labels": [], "entities": [{"text": "BLSTMWE", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9647864699363708}]}, {"text": "It shows our approach is more helpful for BLSTM-RNN.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.6733596324920654}]}, {"text": "Besides, all of the three published word representations also significantly en- shows the performance of word representations trained on corpora with different size.", "labels": [], "entities": []}, {"text": "BLSTMWE (10M), BLSTMWE (100M) and BLSTMWE (536M) are word representations respectively trained by BLSTM-RNN on the first 10 million words, first 100 million words and all 536 million words of the North American news corpus.", "labels": [], "entities": [{"text": "BLSTMWE", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6063849925994873}, {"text": "BLSTMWE", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9156520366668701}, {"text": "BLSTMWE", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9875425100326538}, {"text": "BLSTM-RNN", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.8079236149787903}]}, {"text": "As expected, there is a monotonic increase in performance as the corpus size increases.", "labels": [], "entities": []}, {"text": "This observation suggests that the result might be further improved by using even bigger unlabeled data.", "labels": [], "entities": []}, {"text": "presents running time with different methods to train word representations on 536 million words corpus.", "labels": [], "entities": []}, {"text": "BLSTM-RNN is trained on one NVIDIA Tesla M2090 GPU.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.6965373754501343}, {"text": "NVIDIA Tesla M2090 GPU", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.6545697152614594}]}, {"text": "The other three methods are trained on a 12 core, 2.53GHz Intel Xeon E5649 machine, using 12 threads.", "labels": [], "entities": []}, {"text": "Though with the help of GPU, BLSTM-RNN is still slower than the other methods.", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.8914943337440491}]}, {"text": "However, it should be noted that the speed of our approach is acceptable compared with previous neural network language model based methods, including (), as our model uses a much simpler output layer which only has two nodes, avoiding the time consuming computation of the big softmax output layer in language model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of BLSTM-RNN with different representations on three tagging tasks", "labels": [], "entities": [{"text": "BLSTM-RNN", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.5706462860107422}]}, {"text": " Table 2: Performance of BLSTMWE trained on corpora with different size", "labels": [], "entities": [{"text": "BLSTMWE", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8742808699607849}]}, {"text": " Table 3: Running time of different training methods", "labels": [], "entities": [{"text": "Running time", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9371773302555084}]}]}