{"title": [{"text": "Recent Progress in Deep Learning for NLP", "labels": [], "entities": [{"text": "NLP", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.6213794350624084}]}], "abstractContent": [{"text": "Neural network\u00adbased methods have been viewed as one of the major driving force in the recent development of natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.7876790463924408}]}, {"text": "We all have witnessed with great excitement how this subfield advances: new ideas emerge at an unprecedented speed and old ideas resurge in unexpected ways.", "labels": [], "entities": []}, {"text": "Ina nutshell, there are two major trends: \u2022 Ideas and techniques from other fields of machine learning and artificial intelligence (A.I.) have increasing impact on neural network\u00adbased NLP methods.", "labels": [], "entities": []}, {"text": "\u2022 With end\u00adto\u00adend models taking on more complex tasks, the design of architecture and mechanisms often needs more domain knowledge from linguists and other domain experts.", "labels": [], "entities": []}, {"text": "Both trends are important to researchers in the computational linguistics community.", "labels": [], "entities": []}, {"text": "Fundamental ideas like external memory or reinforcement learning, although introduced to NLP only recently, have quickly lead to significant improvement on tasks like natural language generation and question answering.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.6734074155489603}, {"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.8681675791740417}]}, {"text": "On the other hand, with complicated neural systems with many cooperating components, it calls for linguistic knowledge in designing the right mechanism, architecture, and sometimes training setting.", "labels": [], "entities": []}, {"text": "As a simple example, the introducing of automatic alignment in neural machine translation, has quickly led to the state\u00adof\u00adthe\u00adart performance in machine translation and triggered a large body of sequence\u00adto\u00adsequence models.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.766766091187795}, {"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.756422609090805}]}, {"text": "It is therefore important to get the researchers in computational linguistics community acquainted with the recent progress in deep learning for NLP.", "labels": [], "entities": []}, {"text": "We will focus on the work and ideas strongly related to the core of natural language and yet not so familiar to the majority of the community, which can be roughly categorized into: 1) the differentiable data\u00adstructures, and 2) the learning paradigms for NLP.", "labels": [], "entities": []}, {"text": "Differentiable data\u00adstructures, starting with the memory equipped with continuous operations in Neural Turing Machine, have been the foundation of deep models with sophisticated operations.", "labels": [], "entities": []}, {"text": "Some members of it, such as Memory Network, have become famous on tasks like question answering and machine translation, while other development in this direction, including those with clear and important application in NLP, are relatively new to this community.", "labels": [], "entities": [{"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8935279250144958}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.818406492471695}]}, {"text": "Deep learning, with its promise on end\u00adto\u00adend learning, not only enables the training of complex NLP models from scratch, but also extends the training setting to include remote and indirect supervision.", "labels": [], "entities": []}, {"text": "We will introduce not only the end\u00adto\u00adend learning in its general notion, but also newly emerged 11", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}