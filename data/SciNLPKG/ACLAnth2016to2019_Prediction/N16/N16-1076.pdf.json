{"title": [{"text": "Weighting Finite-State Transductions With Neural Context", "labels": [], "entities": [{"text": "Weighting Finite-State Transductions", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8338704705238342}]}], "abstractContent": [{"text": "How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another?", "labels": [], "entities": []}, {"text": "A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks.", "labels": [], "entities": []}, {"text": "In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks.", "labels": [], "entities": []}, {"text": "A stack of bidirec-tional LSTMs reads the input string from left-to-right and right-to-left, in order to summarize the input context in which a transducer arc is applied.", "labels": [], "entities": []}, {"text": "We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton.", "labels": [], "entities": []}, {"text": "This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming.", "labels": [], "entities": []}, {"text": "We illustrate our method on the tasks of morphological reinflection and lemmatization.", "labels": [], "entities": [{"text": "morphological reinflection", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7989965975284576}]}], "introductionContent": [{"text": "Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics.", "labels": [], "entities": [{"text": "Mapping one character sequence", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8300842046737671}, {"text": "structured prediction", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7274322509765625}]}, {"text": "Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, morphology, and phonology.", "labels": [], "entities": []}, {"text": "The two sequences may have different lengths.", "labels": [], "entities": []}, {"text": "Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights.", "labels": [], "entities": []}, {"text": "This requires manual design of the transducer states and the features extracted from those states.", "labels": [], "entities": []}, {"text": "Alternatively, deep learning has recently been tried for sequence-to-sequence transduction).", "labels": [], "entities": []}, {"text": "While training these systems could discover contextual features that a hand-crafted parametric WFST might miss, they dispense with important structure in the problem, namely the monotonic input-output alignment.", "labels": [], "entities": []}, {"text": "This paper describes a natural hybrid approach that marries simple FSTs with features extracted by recurrent neural networks.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 67, "end_pos": 71, "type": "TASK", "confidence": 0.9591968655586243}]}, {"text": "Our novel architecture allows efficient modeling of globally normalized probability distributions over string-valued output spaces, simultaneously with automatic feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7527601420879364}]}, {"text": "We evaluate on morphological reinflection and lemmatization tasks, showing that our approach strongly outperforms a standard WFST baseline as well as neural sequence-tosequence models with attention.", "labels": [], "entities": [{"text": "WFST baseline", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.898216187953949}]}, {"text": "Our approach also compares reasonably with a state-of-the-art WFST approach that uses task-specific latent variables.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach on two morphological generation tasks of reinflection (section 10.1) and lemmatization (section 10.2).", "labels": [], "entities": []}, {"text": "In the reinflection task, the goal is to transduce verbs from one inflected form into another, whereas the lemmatization task requires the model to reduce an inflected verb to its root form.", "labels": [], "entities": []}, {"text": "We compare our WFST-LSTM against two standard baselines, a WFST with hand-engineered features and the Moses phrase-based MT system (, as well as the more complex latent-variable model of.", "labels": [], "entities": [{"text": "WFST-LSTM", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.8710662722587585}, {"text": "WFST", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8981070518493652}]}, {"text": "The comparison with is of noted interest since their latent variables are structured particularly for morphological transduction tasks-we are directly testing the ability of the LSTM to structure its hidden layer as effectively as linguistically motivated latent-variables.", "labels": [], "entities": []}, {"text": "Additionally, we provide detailed ablation studies and learning curves which show that our neural-WFSA hybrid model can generalize even with very low amounts of training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Exact match accuracy on the morphological re- inflection task. All the results in the first half of the table  are taken from Dreyer (2011), whose experimental setup  we copied exactly. The Moses15 result is obtained by ap- plying the SMT toolkit Moses (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.958198070526123}, {"text": "SMT toolkit Moses", "start_pos": 245, "end_pos": 262, "type": "TASK", "confidence": 0.7196632623672485}]}, {"text": " Table 2: Lemmatization results on Basque, English, Irish  and Tagalog. Comparison systems marked with (W) are  taken from Wicentowski (2002) and systems marked with  a (D) are taken from Dreyer (2011). We outperform base- lines on all languages and are competitive with the latent- variable approach (ngrams + x + l), beating it in two  cases: Irish and Tagalog.", "labels": [], "entities": []}, {"text": " Table 3: Ablation experiments: Exact-match accuracy of  the different systems averaged on 5 folds of validation  portions of the morphological induction dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9793047308921814}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9631839394569397}]}]}