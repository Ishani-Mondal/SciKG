{"title": [{"text": "\u200b Scalable Statistical Relational Learning for NLP", "labels": [], "entities": [{"text": "Scalable Statistical Relational Learning", "start_pos": 2, "end_pos": 42, "type": "TASK", "confidence": 0.6825341284275055}, {"text": "NLP", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.6690926551818848}]}], "abstractContent": [{"text": "Prerequisites:\u200b No prior knowledge of statistical relational learning is required.", "labels": [], "entities": []}, {"text": "Abstract: Statistical Relational Learning (SRL) is an interdisciplinary research area that combines first\u00adorder logic and machine learning methods for probabilistic inference.", "labels": [], "entities": [{"text": "Abstract: Statistical Relational Learning (SRL)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.754924152046442}]}, {"text": "Although many Natural Language Processing (NLP) tasks (including text classification, semantic parsing, information extraction, coreference resolution, and sentiment analysis) can be formulated as inference in a first\u00adorder logic, most probabilistic first\u00adorder logics are not efficient enough to be used for large\u00adscale versions of these tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7493197619915009}, {"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7225655317306519}, {"text": "information extraction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8038271069526672}, {"text": "coreference resolution", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.9112691283226013}, {"text": "sentiment analysis", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.9046836793422699}]}, {"text": "In this tutorial, we provide a gentle introduction to the theoretical foundation of probabilistic logics, as well as their applications in NLP.", "labels": [], "entities": []}, {"text": "We describe recent advances in designing scalable probabilistic logics, with a special focus on ProPPR.", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9170189499855042}]}, {"text": "Finally, we provide a hands\u00adon demo about scalable probabilistic logic programming for solving practical NLP problems.", "labels": [], "entities": []}, {"text": "Outline: \u2022 Part 1: Foundations and Applications of Probabilistic First\u00adOrder Logic \u2022 We will provide a brief review of some first\u00adorder learning systems that have been developed in the past: Markov Logic Networks (Richardson and Domingos, 2006), Stochastic Logic Programs (Muggleton, 1996).", "labels": [], "entities": [{"text": "Stochastic Logic Programs (Muggleton, 1996)", "start_pos": 246, "end_pos": 289, "type": "TASK", "confidence": 0.6038136035203934}]}, {"text": "In this part, we introduce the semantics of the above languages with their inference (and learning) approaches.", "labels": [], "entities": []}, {"text": "We analyze and discuss the core ideas behind of such language.", "labels": [], "entities": []}, {"text": "We show various applications of probabilistic logics in NLP.", "labels": [], "entities": []}, {"text": "\u2022 Part 2: Scalable Probabilistic Logics: A Case Study of ProPPR.", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8585750460624695}]}, {"text": "\u2022 We will focus on the efficiency issue, and introduce recent advances of scalable probabilistic logics, including lifted inference techniques (Van den Broeck and Suciu, 2014) and probabilistic soft logic (Bach et al., 2015).", "labels": [], "entities": []}, {"text": "In particular, we will take CMU's ProPPR (Wang et al., 2013) as a case study.", "labels": [], "entities": [{"text": "CMU's ProPPR (Wang et al., 2013)", "start_pos": 28, "end_pos": 60, "type": "DATASET", "confidence": 0.9136546790599823}]}, {"text": "We describe the main contributions of ProPPR: including its approximate personalized PageRank inference scheme, parallel stochastic gradient descent learning method, and its flexibility in theory engineering.", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.84034264087677}]}, {"text": "We then introduce the structure learning methods in ProPPR (Wang et al., CIKM 2014), including a structured regularization method as an alternative to predicate invention (Wang et al., IJCAI 2015).", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.8889522552490234}, {"text": "IJCAI", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.8814025521278381}]}, {"text": "We will also cover our latest attempt of learning first\u00adorder logic formula embeddings, and discuss its relationship to (and possible connections between) even newer approaches to modeling knowledge bases, relationships, and inference using deep learning methods.", "labels": [], "entities": []}, {"text": "To conclude this part, we show an interesting application of ProPPR (Wang et al., ACL\u00adIJCNLP 2015): a joint information extraction and knowledge reasoning engine.", "labels": [], "entities": [{"text": "ProPPR", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8754115104675293}, {"text": "information extraction and knowledge reasoning", "start_pos": 108, "end_pos": 154, "type": "TASK", "confidence": 0.6862086117267608}]}, {"text": "\u2022 Part 3: Demos and Practical Applications.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}