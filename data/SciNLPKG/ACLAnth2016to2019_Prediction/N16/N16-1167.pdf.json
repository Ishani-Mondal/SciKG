{"title": [{"text": "Lexical Coherence Graph Modeling Using Word Embeddings", "labels": [], "entities": [{"text": "Lexical Coherence Graph Modeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8038249760866165}]}], "abstractContent": [{"text": "Coherence is established by semantic connections between sentences of a text which can be modeled by lexical relations.", "labels": [], "entities": []}, {"text": "In this paper , we introduce the lexical coherence graph (LCG), anew graph-based model to represent lexical relations among sentences.", "labels": [], "entities": []}, {"text": "The frequency of subgraphs (coherence patterns) of this graph captures the connectivity style of sentence nodes in this graph.", "labels": [], "entities": []}, {"text": "The coherence of a text is encoded by a vector of these frequencies.", "labels": [], "entities": []}, {"text": "We evaluate the LCG model on the readability ranking task.", "labels": [], "entities": []}, {"text": "The results of the experiments show that the LCG model obtains higher accuracy than state-of-the-art coherence models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.99903404712677}]}, {"text": "Using larger subgraphs yields higher accuracy, because they capture more structural information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991976618766785}]}, {"text": "However, larger sub-graphs can be sparse.", "labels": [], "entities": []}, {"text": "We adapt Kneser-Ney smoothing to smooth subgraphs' frequencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The concept of coherence is based on cohesive semantic relations connecting elements of a text.", "labels": [], "entities": []}, {"text": "Cohesive relations are expressed through grammar and the vocabulary of a language.", "labels": [], "entities": []}, {"text": "The former is referred to as grammatical coherence, the latter as lexical coherence.", "labels": [], "entities": []}, {"text": "Grammatical coherence encompasses coreference, substitution, ellipsis, etc.", "labels": [], "entities": []}, {"text": "Lexical coherence comprises semantic connections among words of a text.", "labels": [], "entities": []}, {"text": "In this paper we measure text coherence by modeling lexical coherence.", "labels": [], "entities": []}, {"text": "Lexical relations specify cohesive relations over the sentences of a text.", "labels": [], "entities": []}, {"text": "These lexical relations can be any kind of semantic relation: repetition, synonymy, hyperonymy, meronymy, etc.", "labels": [], "entities": []}, {"text": "These lexical items mayor may not have the same reference (.", "labels": [], "entities": []}, {"text": "Why does the little boy wriggle all the time?", "labels": [], "entities": []}, {"text": "In this example the lexical items boy and girls are semantically related.", "labels": [], "entities": []}, {"text": "Although they do not refer to the same entity, they still connect these two sentences.", "labels": [], "entities": []}, {"text": "There is coherence between any pair of lexical items that stand to each other in some lexicosemantic relation.", "labels": [], "entities": []}, {"text": "For textual purposes it is not required to determine the type of the relation.", "labels": [], "entities": []}, {"text": "It is only necessary to recognize semantically related lexical items, and these relations can be learned by cooccurring lexical items.", "labels": [], "entities": []}, {"text": "One can use world knowledge resources to determine semantic relations.", "labels": [], "entities": []}, {"text": "This way is expensive in terms of determining the best resource, e.g. WordNet vs. Freebase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9539494514465332}]}, {"text": "WordNet lacks broad coverage in particular with proper names, Freebase is restricted to nominal concepts and entities.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9679241180419922}]}, {"text": "Recent improvements in embedding representations of words let us efficiently compute semantic relations among lexical items in the vector space.", "labels": [], "entities": []}, {"text": "These models use a vector of numbers to encode the meaning of words.", "labels": [], "entities": []}, {"text": "We use these vectors to check the existence of any kind of semantic relations between two words.", "labels": [], "entities": []}, {"text": "In the following example the sentences are connected because of the semantic relation between king and queen which can be induced byword em-bedding models).", "labels": [], "entities": []}, {"text": "The king was in his counting-house, counting out his money, The queen was in the parlour, eating bread and honey.", "labels": [], "entities": []}, {"text": "We model lexical coherence between sentences by a lexical coherence graph (LCG).", "labels": [], "entities": []}, {"text": "We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text.", "labels": [], "entities": []}, {"text": "An important task for evaluating a coherence model is readability assessment.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7707775235176086}]}, {"text": "The goal of this task is to rate texts based on their readability.", "labels": [], "entities": []}, {"text": "The more coherent a text, the faster to read and easier to understand it is.", "labels": [], "entities": []}, {"text": "Other coherence models) are also evaluated on this task.", "labels": [], "entities": []}, {"text": "use the entity grid ( to capture the coherence of a text for readability assessment.", "labels": [], "entities": []}, {"text": "extend the entity graph ( as coherence model to measure the readability of texts.", "labels": [], "entities": []}, {"text": "They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text.", "labels": [], "entities": []}, {"text": "We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs.", "labels": [], "entities": []}, {"text": "Although using the frequency of subgraphs of the lexical coherence graph encodes coherence features well, the subgraph frequency method, in general, is suffering from a sparsity problem when the subgraphs get larger.", "labels": [], "entities": []}, {"text": "Large subgraphs capture more structural information, but they occur only rarely.", "labels": [], "entities": []}, {"text": "We resolve this sparsity issue by adapting KneserNey smoothing) to smooth subgraph counts (Section 3).", "labels": [], "entities": []}, {"text": "We estimate the probability of unseen subgraphs, i.e. coherence patterns.", "labels": [], "entities": []}, {"text": "This prediction lets us measure the coherence of a text even when its corresponding graph representation contains a subgraph which does not occur in the training data.", "labels": [], "entities": []}, {"text": "If the unseen coherence pattern is similar to seen ones, smoothing gives it closer probability to seen coherence patterns in comparison to dissimilar unseen ones.", "labels": [], "entities": []}, {"text": "This is due to the base probability factor in Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We evaluate our LCG model on the two readability datasets provided by and De, respectively (Section 4).", "labels": [], "entities": []}, {"text": "The results indicate that the LCG model outperforms state-of-the-art systems.", "labels": [], "entities": []}, {"text": "By applying Kneser-Ney smoothing we solve the sparsity problem.", "labels": [], "entities": []}, {"text": "Smoothing allows us to exploit the high informativity of large subgraphs which leads to new state-of-the-art results in readability assessment.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.6883372664451599}]}], "datasetContent": [{"text": "We evaluate our coherence model on the task of ranking texts by their readability.", "labels": [], "entities": []}, {"text": "The intuition is that more coherent texts are easier to read.", "labels": [], "entities": []}, {"text": "We assume this is wsj-0382 which does not exist in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9967572093009949}]}, {"text": "We furthermore remove wsj--2090 which does not exist in the final release of the Penn Discourse Treebank.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.9935522874196371}]}, {"text": "We also remove wsj-1398 which is a poem and, hence, not very informative for readability assessment.", "labels": [], "entities": []}, {"text": "LME: left text is much easier, LSE: left text is somewhat easier, ED: both texts are equally difficult, RSE: right text is somewhat easier, RME: right text is much easier.", "labels": [], "entities": [{"text": "LSE", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9696752429008484}, {"text": "ED", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9933854937553406}, {"text": "RSE", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9489645957946777}]}, {"text": "We map these labels to three class labels: +1: for text pairs where the left text is easier to read (LME or LSE), 0: for text pairs where both texts are equally difficult to read (ED),  Word Embeddings and Classification.", "labels": [], "entities": []}, {"text": "In order to reduce the effect of very frequent words, stop words are filtered by using the SMART English stop word list.", "labels": [], "entities": [{"text": "SMART English stop word list", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.8074558854103089}]}, {"text": "We use a pretrained model of GloVe for word embeddings.", "labels": [], "entities": []}, {"text": "This model is trained on Common Crawl with 840B tokens, 2.2M vocabulary.", "labels": [], "entities": []}, {"text": "We represent each word by a vector with length 300 ().", "labels": [], "entities": []}, {"text": "For handling out-of-vocabulary words, we assign a random vector to each word and memorize it for its next occurrence (.", "labels": [], "entities": []}, {"text": "The classification task is done by the SVM implementation in WEKA (SMO) with the linear kernel function.", "labels": [], "entities": [{"text": "WEKA (SMO)", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.8474058508872986}]}, {"text": "All settings are set to the default values.", "labels": [], "entities": []}, {"text": "The evaluation is computed by 10-fold cross validation.", "labels": [], "entities": []}, {"text": "In order to compare the performance of LCG with the entity graph model, we follow and use the gSpan method () to compute all common subgraphs on each dataset and their frequencies.", "labels": [], "entities": []}, {"text": "Note that gSpan does not count all possible k-node subgraphs, whereas for applying Kneser-Ney smoothing it is necessary to count all possible k-node subgraphs, because the probability should be distributed among all possible subgraphs.", "labels": [], "entities": []}, {"text": "This also helps to estimate the probability of unseen patterns.", "labels": [], "entities": []}, {"text": "We use a random sampling method) to obtain the frequency of subgraphs in a sentence graph.", "labels": [], "entities": []}, {"text": "In this regard, we take 10, 000 samples of the given sentence graph by randomly selecting k nodes of the graph to count the occurrence of k-node subgraphs in this graph.", "labels": [], "entities": []}, {"text": "We compute the base probability for at most k = 6.", "labels": [], "entities": []}, {"text": "We find the best value ford in a greedy manner.", "labels": [], "entities": []}, {"text": "First, we initialized with 0.001.", "labels": [], "entities": []}, {"text": "In each iteration we compute the performance.", "labels": [], "entities": []}, {"text": "Then we multiply the discount factor by 10.", "labels": [], "entities": [{"text": "discount", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9580811858177185}]}, {"text": "We iterate as long as the discount factor is less than 1000.", "labels": [], "entities": []}, {"text": "We report the best performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Properties of the different genres in the De Clercq", "labels": [], "entities": []}, {"text": " Table 3: De Clercq dataset.", "labels": [], "entities": [{"text": "De Clercq dataset", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8335911432902018}]}, {"text": " Table 4: Accuracy of EGraph+PRN and LCG on different gen-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979812502861023}, {"text": "LCG", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8642494082450867}]}, {"text": " Table 5: Applying smoothing method yields to higher accuracy", "labels": [], "entities": [{"text": "Applying", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9961567521095276}]}]}