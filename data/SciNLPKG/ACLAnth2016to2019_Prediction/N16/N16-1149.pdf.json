{"title": [{"text": "Incorporating Side Information into Recurrent Neural Network Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent neural network language models (RNNLM) have recently demonstrated vast potential in modelling long-term dependencies for NLP problems, ranging from speech recognition to machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7317338138818741}, {"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.785985141992569}]}, {"text": "In this work, we propose methods for conditioning RNNLMs on external side information, e.g., metadata such as keywords, description, document title or topic headline.", "labels": [], "entities": []}, {"text": "Our experiments show consistent improvements of RNNLMs using side information over the baselines for two different datasets and genres in two languages.", "labels": [], "entities": []}, {"text": "Interestingly, we found that side information in a foreign language can be highly beneficial in modelling texts in another language , serving as a form of cross-lingual language modelling.", "labels": [], "entities": [{"text": "cross-lingual language modelling", "start_pos": 155, "end_pos": 187, "type": "TASK", "confidence": 0.7354169885317484}]}], "introductionContent": [{"text": "Neural network approaches to language modelling (LM) have made remarkable performance gains over traditional count-based ngram LMs ().", "labels": [], "entities": [{"text": "language modelling (LM)", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8579798221588135}]}, {"text": "They offer several desirable characteristics, including the capacity to generalise overlarge vocabularies through the use of vector space representation, and -for recurrent models -the ability to encode long distance dependencies that are impossible to include with a limited context windows used in conventional ngram LMs.", "labels": [], "entities": []}, {"text": "These early papers have spawned a cottage industry in neural LM based applications, where text generation is a key component, including conditional language models for image captioning) and neural machine translation).", "labels": [], "entities": [{"text": "text generation", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.7262668162584305}, {"text": "image captioning", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.7271709740161896}, {"text": "neural machine translation", "start_pos": 190, "end_pos": 216, "type": "TASK", "confidence": 0.6955234209696451}]}, {"text": "Inspired by these works for conditioning LMs on complex side information, such as images and foreign text, in this paper we investigate the possibility of improving LMs in a more traditional setting, that is when applied directly to text documents.", "labels": [], "entities": []}, {"text": "Typically corpora include rich side information, such as document titles, authorship, timestamp, keywords and soon, although this information is usually discarded when applying statistical models.", "labels": [], "entities": [{"text": "soon", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9560707807540894}]}, {"text": "However, this information can be highly informative, for instance, keywords, titles or descriptions, often include central topics which will be helpful in modelling or understanding the document text.", "labels": [], "entities": []}, {"text": "We propose mechanisms for encoding this side information into a vector space representation, and means of incorporating it into the generating process in a RNNLM framework.", "labels": [], "entities": []}, {"text": "Evaluating on two corpora and two different languages, we show consistently significant perplexity reductions over the state-of-theart RNNLM models.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose a framework for encoding structured and unstructured side information, and its incorporation into a RNNLM.", "labels": [], "entities": [{"text": "encoding structured and unstructured side information", "start_pos": 27, "end_pos": 80, "type": "TASK", "confidence": 0.7639393210411072}]}, {"text": "2. We introduce anew corpus, the RIE corpus, based on the Europarl web archive, with rich annotations of several types of meta-data.", "labels": [], "entities": [{"text": "RIE corpus", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9080708622932434}, {"text": "Europarl web archive", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9928282697995504}]}, {"text": "3. We provide empirical analysis showing consistent improvements from using side information across two datasets in two languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments on two datasets with different genres in two languages.", "labels": [], "entities": []}, {"text": "As the first dataset, we use the IWSLT2014 MT track on TED Talks 1 due to its self-contained rich auxiliary information, including: title, description, keywords, and author related information.", "labels": [], "entities": [{"text": "IWSLT2014 MT track", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8521140615145365}, {"text": "TED Talks 1", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.7109120686848959}]}, {"text": "We chose the English-French pair for our experiments 2 . The statistics of the training set is shown in used dev2010 (7 talks/817 sentences) for early stopping of training neural network models.", "labels": [], "entities": []}, {"text": "For evaluation, we used different testing sets over years, including tst2010 (10/1587), tst2011 (7/768), tst2012 (10/1083).", "labels": [], "entities": []}, {"text": "As the second dataset, we crawled the entire European Parliament 3 website, focusing on plenary sessions.", "labels": [], "entities": [{"text": "European Parliament 3 website", "start_pos": 45, "end_pos": 74, "type": "DATASET", "confidence": 0.934828981757164}]}, {"text": "Such sessions contain useful structural information, namely multilingual texts divided into speaker sessions and topics.", "labels": [], "entities": []}, {"text": "We believe that those texts are interesting and challenging for language modelling tasks.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8175264000892639}]}, {"text": "Our dataset contains 724 plenary sessions over 12.5 years until June 2011 with multilingual texts in 22 languages . We refer to this dataset by RIE 5 (Rich Information Europarl).", "labels": [], "entities": [{"text": "RIE 5 (Rich Information Europarl)", "start_pos": 144, "end_pos": 177, "type": "DATASET", "confidence": 0.8825411966868809}]}, {"text": "We randomly select 200/5/30 plenary sessions as the training/development/test sets, respectively.", "labels": [], "entities": []}, {"text": "We believe that the new data including side information pose another challenge for language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8568206429481506}]}, {"text": "Furthermore, the sizes of our working datasets are an order of magnitude larger than the standard Penn Treebank set which is often used for evaluating neural language models.", "labels": [], "entities": [{"text": "Penn Treebank set", "start_pos": 98, "end_pos": 115, "type": "DATASET", "confidence": 0.9865312178929647}]}, {"text": "We have used cnn 6 to implement our models.", "labels": [], "entities": []}, {"text": "We use the same configurations for all neural models: 512 input embedding and hidden layer dimensions, 2 hidden layers, and vocabulary sizes as given in.", "labels": [], "entities": []}, {"text": "We used the same vocabulary for the auxiliary and modelled text.", "labels": [], "entities": []}, {"text": "We trained a conventional 5\u2212gram language model using modified Kneser-Ney smoothing, with the KenLM toolkit  and description as auxiliary side information respectively.", "labels": [], "entities": []}, {"text": "bold: Statistically significant better than the best baseline.", "labels": [], "entities": []}, {"text": "Wilcoxon signed-rank test to measure the statistical significance (p < 0.05) on differences between sentence-level perplexity scores of improved models compared to the best baseline.", "labels": [], "entities": []}, {"text": "Throughout our experiments, punctuation, stop words and sentence markers (\u2329s\u232a, \u2329/s\u232a, \u2329unk\u232a) are filtered out in all auxiliary inputs.", "labels": [], "entities": []}, {"text": "We observed that this filtering was required for BOW to work reasonably well.", "labels": [], "entities": [{"text": "BOW", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.44349515438079834}]}, {"text": "For each model, the best perplexity score on development set is used for early stopping of training models, which was obtained after 2-5 and 2-3 epochs on TED Talks and RIE datasets, respectively.", "labels": [], "entities": [{"text": "RIE datasets", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.8852076828479767}]}, {"text": "The perplexity results on TED Talks dataset are presented in   kinds of side information, including keywords, title, description.", "labels": [], "entities": [{"text": "TED Talks dataset", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.8089491526285807}]}, {"text": "We attempted to inject those into different RNNLM layers, resulting in model variants as shown in.", "labels": [], "entities": []}, {"text": "First, we chose \"keywords\" (+k) information as an anchor to figure out which incorporation method works well.", "labels": [], "entities": []}, {"text": "Comparing input+add+k, input+mlp+k and input+stack+k, the largest decrease is obtained by output+mlp+k consistently across all test sets (and development sets, not shown here).", "labels": [], "entities": []}, {"text": "We further evaluated the addition of other side information (e.g., \"description\" (+d), \"title\" (+t)), finding that +d has similar effect as +k whereas +t has a mixed effect, being detrimental for one test set (test2011).", "labels": [], "entities": []}, {"text": "We suspect that it is due to often-times short sentences of titles in that test, after our filtering step, leading to a shortage of useful information fed into neural network learning.", "labels": [], "entities": []}, {"text": "Interestingly, the best performance is obtained when incorporating both +k and +d, showing that there is complementary information in the two auxiliary inputs.", "labels": [], "entities": []}, {"text": "Further, we also achieved the similar results in the counterpart of English part (in French) using output+mlp with both +t and +d as shown in Table 3.", "labels": [], "entities": []}, {"text": "In French data, no \"keywords\" information is available.", "labels": [], "entities": [{"text": "French data", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.86097651720047}]}, {"text": "For this reason, we run additional experiments by injecting English keywords as side information into neural models of French.", "labels": [], "entities": []}, {"text": "Interestingly, we found that \"keywords\" side information in English effectively improves the modelling of French texts as shown in, serving as anew form of cross-lingual language modelling.", "labels": [], "entities": [{"text": "cross-lingual language modelling", "start_pos": 156, "end_pos": 188, "type": "TASK", "confidence": 0.6925257841746012}]}, {"text": "We further achieved similar results by incorporating the topic headline in the RIE dataset.", "labels": [], "entities": [{"text": "RIE dataset", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.9660604596138}]}, {"text": "The consistently-improved results (in) demonstrate the robustness of the output+mlp approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the training sets, showing in each cell the", "labels": [], "entities": []}, {"text": " Table 2: Perplexity scores based on the English part of TED", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9519447088241577}, {"text": "English part of", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.7290818492571512}, {"text": "TED", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.7918029427528381}]}, {"text": " Table 4: Perplexity scores based on the sampled RIE dataset.", "labels": [], "entities": [{"text": "RIE dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.8763578534126282}]}]}