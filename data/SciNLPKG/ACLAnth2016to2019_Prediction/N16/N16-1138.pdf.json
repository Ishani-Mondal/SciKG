{"title": [{"text": "Emergent: a novel data-set for stance classification", "labels": [], "entities": [{"text": "stance classification", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.9696735441684723}]}], "abstractContent": [{"text": "We present Emergent, a novel data-set derived from a digital journalism project for rumour debunking.", "labels": [], "entities": [{"text": "rumour debunking", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7595658302307129}]}, {"text": "The data-set contains 300 rumoured claims and 2,595 associated news articles, collected and labelled by journalists with an estimation of their veracity (true, false or unverified).", "labels": [], "entities": []}, {"text": "Each associated article is summarized into a headline and labelled to indicate whether its stance is for, against, or observing the claim, where observing indicates that the article merely repeats the claim.", "labels": [], "entities": []}, {"text": "Thus, Emergent provides a real-world data source fora variety of natural language processing tasks in the context of fact-checking.", "labels": [], "entities": []}, {"text": "Further to presenting the dataset, we address the task of determining the article headline stance with respect to the claim.", "labels": [], "entities": []}, {"text": "For this purpose we use a logistic regression classifier and develop features that examine the headline and its agreement with the claim.", "labels": [], "entities": []}, {"text": "The accuracy achieved was 73% which is 26% higher than the one achieved by the Excitement Open Platform (Magnini et al., 2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997286200523376}]}], "introductionContent": [{"text": "The advent of New Media, such as Twitter, Facebook, etc., enables news stories and rumours to be published in real-time to a global audience, bypassing the usual verification procedures used by more traditional Old Media news outlets.", "labels": [], "entities": []}, {"text": "However, the line between Old and New Media is becoming blurred as news aggregators lift stories from social media and re-publish them without fact-checking.", "labels": [], "entities": []}, {"text": "This issue could be helped by developing methods for automated fact-checking of news stories, part of the reporter's black box envisioned in and one of the main objectives in computational journalism.", "labels": [], "entities": []}, {"text": "While this task is related to a variety of natural language processing tasks such as textual entailment and machine comprehension, it poses additional challenges due to its opendomain, real-world nature.", "labels": [], "entities": []}, {"text": "Previous work by Vlachos and proposed using data from fact-checking websites such as Politifact 1 , but the labelling provided by the journalists is only the degree of truthfulness of the claims, without any machinereadable verdicts to supervise the various steps in deciding it.", "labels": [], "entities": []}, {"text": "Thus, the task defined by the dataset proposed remains too challenging for the NLP methods currently available.", "labels": [], "entities": []}, {"text": "In this paper we propose to use data from the Emergent Project), a rumour debunking project carried out in collaboration with the Tow Center for Digital Journalism at Columbia Journalism School 2 . Consisting of 300 claims and 2,595 associated news articles, the Emergent project contains a rich source of labelled data that can be used in a variety of NLP tasks, created by journalists as part of their normal workflow, thus real-world and at no annotation cost.", "labels": [], "entities": []}, {"text": "We leverage the Emergent dataset to investigate the task of classifying the stance of a news article headline with respect to its associated claim, i.e. for each article headline we assign a stance label which is one of for, against, or observing, indicating whether the article is supporting, refuting, or just reporting the claim, respectively.", "labels": [], "entities": [{"text": "Emergent dataset", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.9483596682548523}]}, {"text": "The large number of claims in the dataset allows us to assess the generalization of the method evaluation to new claims more reliably than in previous work that either used a small number of claims (e. g. seven in or did not separate training claims from testing claims).", "labels": [], "entities": []}, {"text": "We develop a stance classification approach based on multiclass logistic regression, using features extracted from the article headline and the claim, achieving an accuracy of 73% on our test data-set, also demonstrating that features relying on syntax, word alignment and paraphrasing contribute to the performance.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8798045516014099}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9992830157279968}, {"text": "word alignment", "start_pos": 254, "end_pos": 268, "type": "TASK", "confidence": 0.7306706011295319}]}, {"text": "Since the task bears similarities with textual entailment, we compare it against the Excitement Open Platform () which achieved a substantially lower accuracy of 47%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9990069270133972}]}], "datasetContent": [], "tableCaptions": []}