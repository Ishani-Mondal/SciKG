{"title": [], "abstractContent": [{"text": "This paper proposes an incremental learning strategy for neural word embedding methods, such as SkipGrams and Global Vectors.", "labels": [], "entities": []}, {"text": "Since our method iteratively generates embedding vectors one dimension at a time, obtained vectors equip a unique property.", "labels": [], "entities": []}, {"text": "Namely, any right-truncated vector matches the solution of the corresponding lower-dimensional embedding.", "labels": [], "entities": []}, {"text": "Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding vectors obtained from 'neural word embedding methods', such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "These NLP tasks include part-of-speech tagging, dependency parsing (, semantic role labeling (, machine translation), sentiment analysis (, and question answering (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6893404871225357}, {"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8565541803836823}, {"text": "semantic role labeling", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6563759942849478}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7275027632713318}, {"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.9592681229114532}, {"text": "question answering", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8860307335853577}]}, {"text": "The main purpose of this paper is to further enhance the 'usability' of obtained embedding vectors in actual use.", "labels": [], "entities": []}, {"text": "To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off.", "labels": [], "entities": []}, {"text": "Indeed, the actual dimensions of the previous studies listed above are diverse; often around 50, and at most 1000.", "labels": [], "entities": []}, {"text": "It is worth noting here that each dimension of embedding vectors obtained by conventional methods has no interpretable meaning.", "labels": [], "entities": []}, {"text": "Thus, we basically need to retrain D -dimensional embedding vectors even if we already have a well-trained D-dimensional vector.", "labels": [], "entities": []}, {"text": "In addition, we cannot take full advantage of freely available high-quality pre-trained embedding vectors 2 since their dimensions are already given and fixed, i.e., D = 300.", "labels": [], "entities": [{"text": "D", "start_pos": 166, "end_pos": 167, "type": "METRIC", "confidence": 0.9789960384368896}]}, {"text": "To reduce the additional computational cost of the retraining, and to improve the 'usability' of embedding vectors, we propose a framework for incrementally determining embeddings one dimension at a time from 1 to D.", "labels": [], "entities": []}, {"text": "As a result, our method always offers the relation that 'any D -right-truncated em-bedding vector is the solution for D -dimensional embeddings of our method'.", "labels": [], "entities": []}, {"text": "Therefore, in actual use, we only need to construct a relatively higherdimensional embedding vector 'just once', i.e., D = 1000, and then truncate it to an appropriate dimension for the application.", "labels": [], "entities": []}], "datasetContent": [{"text": "As in previously reported neural word embedding papers, our training data was taken from a Wikipedia dump.", "labels": [], "entities": [{"text": "Wikipedia dump", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8785725831985474}]}, {"text": "We used hyperwords tool 5 for our data preparation (.", "labels": [], "entities": []}, {"text": "We compared our method, ITACO, with the widely used conventional methods, SGNS and GloVe.", "labels": [], "entities": []}, {"text": "We used the word2vec implementation 6 to obtain word embeddings of SGNS, and glove implementation 7 for GloVe.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.9398769736289978}]}, {"text": "Many tunable hyper-parameters were selected based on the recommended default values of each implementation, or suggestion explained in ().", "labels": [], "entities": []}, {"text": "For ITACO, we selected the Glove objective to solve Eqs.", "labels": [], "entities": [{"text": "ITACO", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7598279714584351}]}, {"text": "6 and 7 since it requires a lower calculation cost than the SGNS objective.", "labels": [], "entities": []}, {"text": "We prepared three types of linguistic benchmark tasks, namely word similarity estimation (Similarity), word analogy estimation (Analogy), and sentence completion (SentComp) tasks.", "labels": [], "entities": [{"text": "word similarity estimation", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.5899104177951813}, {"text": "word analogy estimation", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.7763490478197733}, {"text": "sentence completion", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7174121290445328}]}, {"text": "We gathered  nine datasets for Similarity), three for Analogy ( , and one for SentComp (.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.958286464214325}]}, {"text": "shows all the results of our experiments 8 . The rows labeled '(trunc)' show the performance of D-right-truncated embedding vectors, whose original vector of dimension is D = 1000.", "labels": [], "entities": []}, {"text": "Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method.", "labels": [], "entities": [{"text": "D", "start_pos": 69, "end_pos": 70, "type": "METRIC", "confidence": 0.9483100771903992}]}, {"text": "Next, the rows labeled '(retrain)' show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting and corresponding D.", "labels": [], "entities": []}, {"text": "Note that the results of 'ITACO (retrain)' are identical to those of 'ITACO (trunc)'.", "labels": [], "entities": []}, {"text": "Moreover, 'GloVe (trunc)' and 'GloVe (retrain)' in D = 1000 are equivalent, as are 'SGNS (trunc)' and 'SGNS (retrain)'.", "labels": [], "entities": []}, {"text": "Thus, these results Results for SGNS and GloVe are the average performance often runs as suggested in were omitted from the table.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.6497904062271118}, {"text": "GloVe", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.7285819053649902}]}, {"text": "First, comparing '(retrain)' and '(trunc)' in SGNS and GloVe, our experimental results first explicitly revealed that SGNS and GloVe with the simple truncation approach '(trunc)' cannot provide effective lower-dimensional embedding vectors.", "labels": [], "entities": []}, {"text": "This observation strongly supports the significance of existence of our proposed method, ITACO.", "labels": [], "entities": [{"text": "ITACO", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.6828214526176453}]}, {"text": "Second, inmost cases ITACO successfully provided almost the same performance level as the best SGNS and GloVe (retrain) results.", "labels": [], "entities": [{"text": "ITACO", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.5514600276947021}, {"text": "GloVe", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9091128706932068}]}, {"text": "We emphasize that ITACO constructed embedding vectors 'just once', while SGNS and GloVe required us to retrain embedding vectors in the corresponding times.", "labels": [], "entities": []}, {"text": "In addition, single run of ITACO for D = 1000 took approximately 12,000 seconds in our machine environment, which was almost equivalent to run 4 iterations of SGNS and 8 iterations of GloVe.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9363031983375549}]}, {"text": "The results of SGNS and GloVe in were obtained by 10 iterations and 20 iterations, respectively, which are one of the standard settings to run SGNS and GloVe . This fact verified that ITACO can run efficiently as in the same level as SGNS and GloVe.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of right-truncated embedding vectors (trunc),", "labels": [], "entities": []}]}