{"title": [{"text": "Agreement on Target-bidirectional Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7350027660528818}]}], "abstractContent": [{"text": "Neural machine translation (NMT) with recurrent neural networks, has proven to bean effective technique for end-to-end machine translation.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.787835935751597}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7490080296993256}]}, {"text": "However, in spite of its promising advances over traditional translation methods , it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 245, "end_pos": 264, "type": "TASK", "confidence": 0.7165303528308868}]}, {"text": "To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japanese-to-English and Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6670311093330383}, {"text": "Chinese-to-English translation tasks", "start_pos": 151, "end_pos": 187, "type": "TASK", "confidence": 0.781921366850535}]}, {"text": "Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9991263747215271}]}, {"text": "With the help of an ensemble technique, this new end-to-end NMT approach finally outperformed phrase-based and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9988918900489807}]}], "introductionContent": [{"text": "Recurrent neural network (RNN) has achieved great successes on several structured prediction tasks, in which RNNs are required to make a sequence of dependent predictions.", "labels": [], "entities": [{"text": "Recurrent neural network (RNN)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7447079420089722}]}, {"text": "One of its advantages is that an unbounded history is available to enrich the context for the prediction at the current time-step.", "labels": [], "entities": []}, {"text": "Despite its successes, recently, ( ) pointed out that the RNN suffers from a fundamental issue of generating unbalanced outputs: that is to say the suffixes of its outputs are typically worse than the prefixes.", "labels": [], "entities": []}, {"text": "This is due to the fact that later predictions directly depend on the accuracy of previous predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.998222291469574}]}, {"text": "They empirically demonstrated this issue on two simple sequence-to-sequence learning tasks: machine transliteration and grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.720785066485405}]}, {"text": "On the more general sequence-to-sequence learning task of machine translation (MT), neural machine translation (NMT) based on RNNs has recently become an active research topic).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8357492327690125}, {"text": "neural machine translation (NMT)", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.7796557744344076}]}, {"text": "Compared to those two simple tasks, MT involves in much larger vocabulary and frequent reordering between input and output sequences.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9924198985099792}]}, {"text": "This makes the prediction at each time-step far more challenging.", "labels": [], "entities": []}, {"text": "In addition, sequences in MT are much longer, with averaged length of 36.7 being about 5 times longer than that in grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9818817973136902}]}, {"text": "Therefore, we believe that the history is more likely to contain incorrect predictions and the issue of unbalanced outputs maybe more serious.", "labels": [], "entities": []}, {"text": "This hypothesis is supported later (see in \u00a74.1), by an analysis that shows the quality of the prefixes of translation hypotheses is much higher than that of the suffixes.", "labels": [], "entities": []}, {"text": "To address this issue for NMT, in this paper we extend the agreement model proposed in (  to the task of machine translation.", "labels": [], "entities": [{"text": "NMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9018360376358032}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7729655802249908}]}, {"text": "Its key idea is to encourage the agreement between a pair of target-directional (left-to-right and right-to-left) NMT models in order to produce more balanced translations and thus improve the overall translation quality.", "labels": [], "entities": []}, {"text": "Our contribution is two-fold: \u2022 We introduce a simple and general method to address the issue of unbalanced outputs for NMT ( \u00a73).", "labels": [], "entities": []}, {"text": "This method is robust without any extra hyperparameters to tune and is easy to implement.", "labels": [], "entities": []}, {"text": "In addition, it is general enough to be applied on top of any of the existing RNN translation models, although it was implemented on top of the model in () in this paper.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7911892235279083}]}, {"text": "\u2022 We provide an empirical evaluation of the technique on large scale Japanese-to-English and Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.7544763684272766}]}, {"text": "The results show our model can generate more balanced translation results, and achieves substantial improvements (of up to 1.4 BLEU points) over the strongest NMT baseline ( \u00a74).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9987801909446716}]}, {"text": "With the help of an ensemble technique, our new end-to-end NMT gains up to 5.6 BLEU points over phrase-based and hierarchical phrasebased Moses () systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9992591738700867}]}], "datasetContent": [{"text": "We conducted experiments on two challenging translation tasks: Japanese-to-English (JP-EN) and Chinese-to-English (CH-EN), using case-insensitive BLEU for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9836406111717224}]}, {"text": "For the JP-EN task, we use the data from NTCIR-9 (Goto et al., 2011): the training data consisted of 2.0M sentence pairs, The development and test sets contained 2K sentences with a single referece, respectively.", "labels": [], "entities": [{"text": "NTCIR-9", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9697098731994629}]}, {"text": "For the CH-EN task, we used the data from the NIST2008 Open Machine Translation Campaign: the training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences).", "labels": [], "entities": [{"text": "NIST2008 Open Machine Translation", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.8211180567741394}]}, {"text": "The first two were the conventional state-of-the-art translation systems, phrase-based and hierarchical phrase-based systems, which are from the latest version of well-known Moses (   (NMT-J) was also implemented using NMT (Bahdanau et al., 2014).", "labels": [], "entities": []}, {"text": "We followed the standard pipeline to train and run Moses.", "labels": [], "entities": []}, {"text": "GIZA++) with grow-diag-final-and was used to build the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9724709391593933}]}, {"text": "We trained 5-gram target language models using the training set for JP-EN and the Gigaword corpus for CH-EN, and used a lexicalized distortion model.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8706141412258148}]}, {"text": "All experiments were run with the default settings except fora distortion-limit of 12 in the JP-EN experiment, as suggested by.", "labels": [], "entities": [{"text": "JP-EN experiment", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9004445672035217}]}, {"text": "To alleviate the negative effects of randomness, the final reported results are averaged over five runs of MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.6033322811126709}]}, {"text": "To ensure a fair comparison, we employed the same settings for all NMT systems.", "labels": [], "entities": []}, {"text": "Specifically, except for the maximum sequence length (seqlen, which was to 80), and the stopping iteration which was selected using development data, we used the default settings set out in () for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, and the beam size for decoding was 12.", "labels": [], "entities": []}, {"text": "Training was conducted on a single Tesla K80 GPU, and it took about 6 days to train a single NMT system on our large-scale data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU comparison of the proposed model NMT-Joint", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998144268989563}, {"text": "NMT-Joint", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.678755521774292}]}, {"text": " Table 3: BLEU comparison of the proposed model NMT-Joint", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981316924095154}, {"text": "NMT-Joint", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.6775813698768616}]}]}