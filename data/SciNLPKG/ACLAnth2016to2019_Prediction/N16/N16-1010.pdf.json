{"title": [{"text": "Automatic Summarization of Student Course Feedback", "labels": [], "entities": [{"text": "Summarization of Student Course Feedback", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.7861801505088806}]}], "abstractContent": [{"text": "Student course feedback is generated daily in both classrooms and online course discussion forums.", "labels": [], "entities": []}, {"text": "Traditionally, instructors manually analyze these responses in a costly manner.", "labels": [], "entities": []}, {"text": "In this work, we propose anew approach to summarizing student course feedback based on the integer linear programming (ILP) framework.", "labels": [], "entities": [{"text": "summarizing student course feedback", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.8768535703420639}]}, {"text": "Our approach allows different student responses to share co-occurrence statistics and alleviates sparsity issues.", "labels": [], "entities": []}, {"text": "Experimental results on a student feedback corpus show that our approach outperforms a range of baselines in terms of both ROUGE scores and human evaluation .", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.9756999611854553}]}], "introductionContent": [{"text": "Instructors love to solicit feedback from students.", "labels": [], "entities": []}, {"text": "Rich information from student responses can reveal complex teaching problems, help teachers adjust their teaching strategies, and create more effective teaching and learning experiences.", "labels": [], "entities": []}, {"text": "Text-based student feedback is often manually analyzed by teaching evaluation centers in a costly manner.", "labels": [], "entities": []}, {"text": "Albeit useful, the approach does not scale well.", "labels": [], "entities": []}, {"text": "It is therefore desirable to automatically summarize the student feedback produced in online and offline environments.", "labels": [], "entities": []}, {"text": "In this work, student responses are collected from an introductory materials science and engineering course, taught in a classroom setting.", "labels": [], "entities": []}, {"text": "Students are presented with prompts after each lecture and asked to provide feedback.", "labels": [], "entities": []}, {"text": "These prompts solicit \"reflective feedback\") from the students.", "labels": [], "entities": []}, {"text": "An example is presented in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our dataset is collected from an introductory materials science and engineering class taught in a major U.S. university.", "labels": [], "entities": []}, {"text": "The class has 25 lectures and enrolled 53 undergrad students.", "labels": [], "entities": []}, {"text": "The students are asked to provide feedback after each lecture based on three prompts: 1) \"describe what you found most interesting in today's class,\" 2) \"describe what was confusing or needed more detail,\" and 3) \"describe what you learned about how you learn.\"", "labels": [], "entities": []}, {"text": "These openended prompts are carefully designed to encourage students to self-reflect, allowing them to \"recapture experience, think about it and evaluate it\" ().", "labels": [], "entities": []}, {"text": "The average response length is 10\u00b18.3 words.", "labels": [], "entities": []}, {"text": "If we concatenate all the responses to each lecture and prompt into a \"pseudo-document\", the document contains 378 words on average.", "labels": [], "entities": []}, {"text": "The reference summaries are created by a teaching assistant.", "labels": [], "entities": []}, {"text": "She is allowed to create abstract summaries using her own words in addition to selecting phrases directly from the responses.", "labels": [], "entities": []}, {"text": "Because summary annotation is costly and recruiting annotators with proper background is nontrivial, 12 out of the 25 lectures are annotated with reference summaries.", "labels": [], "entities": [{"text": "summary annotation", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8234377801418304}]}, {"text": "There is one gold-standard summary per lecture and question prompt, yielding 36 documentsummary pairs . On average, a reference summary contains 30 words, corresponding to 7.9% of the total words in student responses.", "labels": [], "entities": []}, {"text": "43.5% of the bigrams inhuman summaries appear in the responses.", "labels": [], "entities": []}, {"text": "Our proposed approach is compared against a range of baselines.", "labels": [], "entities": []}, {"text": "They are 1) MEAD ( ), a centroid-based summarization system that scores sentences based on length, centroid, and position; 2) LEXRANK (), a graph-based summarization approach based on eigenvector centrality; 3) SUMBASIC (, an approach that assumes words occurring frequently in a document cluster have a higher chance of being included in the summary; 4) BASELINE-ILP (), a baseline ILP framework without data imputation.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9386540055274963}, {"text": "LEXRANK", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9725603461265564}, {"text": "BASELINE-ILP", "start_pos": 355, "end_pos": 367, "type": "METRIC", "confidence": 0.805302619934082}]}, {"text": "For the ILP based approaches, we use bigrams as concepts (bigrams consisting of only stopwords are removed 2 ) and sentence frequency as concept weights.", "labels": [], "entities": []}, {"text": "We use all the sentences in 25 lectures to construct the concept-sentence co-occurrence matrix and perform data imputation.", "labels": [], "entities": []}, {"text": "It allows us to leverage the co-occurrence statistics both within and across lectures.", "labels": [], "entities": []}, {"text": "For the soft-impute algorithm, we perform grid search (on a scale of with stepsize 0.5) to tune the hyper-parameter \u03bb.", "labels": [], "entities": [{"text": "grid search", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.7938885390758514}]}, {"text": "To make the most use of annotated lectures, we split them into three folds.", "labels": [], "entities": []}, {"text": "In each one, we tune \u03bb on two folds and test it on the other fold.", "labels": [], "entities": []}, {"text": "Finally, we report the averaged results.", "labels": [], "entities": []}, {"text": "In all experiments, summary length is set to be 30 words or less, corresponding to the  average number of words inhuman summaries.", "labels": [], "entities": [{"text": "summary length", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.7049037516117096}]}, {"text": "In, we present summarization results evaluated by ROUGE) and human judges.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9768730401992798}]}, {"text": "ROUGE is a standard evaluation metric that compares system and reference summaries based on ngram overlaps.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.917779266834259}]}, {"text": "Our proposed approach outperforms all the baselines based on three standard ROUGE metrics.", "labels": [], "entities": []}, {"text": "When examining the imputed sentenceconcept co-occurrence matrix, we notice some interesting examples that indicate the effectiveness of the proposed approach, shown in.", "labels": [], "entities": []}, {"text": "Because ROUGE cannot thoroughly capture the semantic similarity between system and reference summaries, we further perform human evaluation.", "labels": [], "entities": []}, {"text": "For each lecture and prompt, we present the prompt, a pair of system outputs in a random order, and the human summary to five Amazon turkers.", "labels": [], "entities": []}, {"text": "The turkers are asked to indicate their preference for system A or B based on the semantic resemblance to the human summary on a 5-Likert scale ('Strongly preferred A', 'Slightly preferred A', 'No preference', 'Slightly preferred B', 'Strongly preferred B').", "labels": [], "entities": []}, {"text": "They are rewarded $0.08 per task.", "labels": [], "entities": []}, {"text": "We use two strategies to control the quality of the human evaluation.", "labels": [], "entities": []}, {"text": "First, we require the turkers to have a Human Intelligence Task (HIT) approval rate of 90% or above.", "labels": [], "entities": [{"text": "Human Intelligence Task (HIT) approval rate", "start_pos": 40, "end_pos": 83, "type": "METRIC", "confidence": 0.6151984333992004}]}, {"text": "Second, we insert some quality checkpoints by asking the turkers to compare two summaries of same text content but different sentence orders.", "labels": [], "entities": []}, {"text": "Turkers who did not pass these tests are filtered out.", "labels": [], "entities": []}, {"text": "Due to budget constraints, we conduct pairwise comparisons for three systems.", "labels": [], "entities": []}, {"text": "The total number of comparisons 3 F-scores are slightly lower than P/R because of the averaging effect and can be illustrated in one example.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9761894941329956}]}, {"text": "Suppose we have P1=0.1, R1=0.4, F1=0.16 and P2=0.4, R2=0.1, F2=0.16.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9978979825973511}]}, {"text": "Then the macro-averaged P/R/F-scores are: P=0.25, R=0.25, F=0.16.", "labels": [], "entities": [{"text": "R=0.25", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9457054932912191}, {"text": "F", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.9918171763420105}]}, {"text": "In this case, the F-score is lower than both P and R.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9994922876358032}]}], "tableCaptions": [{"text": " Table 2: Summarization results evaluated by ROUGE (%) and human judges. Shaded area indicates that the performance difference", "labels": [], "entities": [{"text": "Summarization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8232393264770508}, {"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9977489113807678}]}]}