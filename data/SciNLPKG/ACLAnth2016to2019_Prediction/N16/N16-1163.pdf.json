{"title": [{"text": "Retrofitting Sense-Specific Word Vectors Using Parallel Text", "labels": [], "entities": [{"text": "Retrofitting Sense-Specific Word Vectors", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8785697966814041}]}], "abstractContent": [{"text": "(2015) recently proposed to learn sense-specific word representations by \"retrofitting\" standard distributional word representations to an existing ontology.", "labels": [], "entities": []}, {"text": "We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them.", "labels": [], "entities": []}, {"text": "We create such a graph using translations learned from parallel corpora.", "labels": [], "entities": []}, {"text": "On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them.", "labels": [], "entities": []}, {"text": "While these models have demonstrated impressive success in capturing some aspects of word meaning), they generally fail to capture the fact that single word forms often have multiple meanings.", "labels": [], "entities": []}, {"text": "This can lead to counterintuitive results-for example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs.", "labels": [], "entities": []}, {"text": "Ina recent paper, Jauhar et al.", "labels": [], "entities": []}, {"text": "(2015) introduce a method for \"retrofitting\" generic word vectors to create sense-specific vectors using the WordNet semantic lexicon.", "labels": [], "entities": [{"text": "WordNet semantic lexicon", "start_pos": 109, "end_pos": 133, "type": "DATASET", "confidence": 0.9186541438102722}]}, {"text": "From WordNet, they create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.9359028935432434}]}, {"text": "This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form.", "labels": [], "entities": []}, {"text": "This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text, and the method does not require performing word sense disambiguation (.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 218, "end_pos": 243, "type": "TASK", "confidence": 0.6241985956827799}]}, {"text": "In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process-the word sense graph-can be inferred based on another widely available resource: bilingual parallel text.", "labels": [], "entities": []}, {"text": "This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (.", "labels": [], "entities": []}, {"text": "Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation.", "labels": [], "entities": []}, {"text": "We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec-tor space, using bilingual correspondences to define word senses.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7051699459552765}]}, {"text": "When evaluated on semantic judgment tasks, the vector spaces derived from this graph perform comparably to and sometimes better than the WordNet-based space of Jauhar et al., indicating that parallel text is a viable alternative to WordNet for defining graph structure.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 232, "end_pos": 239, "type": "DATASET", "confidence": 0.9460465312004089}]}, {"text": "Combining the output of parallel-data-based and WordNet-based retrofitted VSMs consistently improves performance, suggesting that the different sense graph methods make complementary contributions to this sense-specific retrofitting process.", "labels": [], "entities": [{"text": "WordNet-based retrofitted VSMs", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.8335371017456055}]}], "datasetContent": [{"text": "We evaluate on both the synonym selection and word similarity rating tasks used by Jauhar et al.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9414916038513184}]}, {"text": "Synonym selection nicely demonstrates the advantages afforded by sense partitioning: if we believe that spin means \"make up a story\", then we are not likely to perform well on a question in which the correct synonym is twirl.", "labels": [], "entities": [{"text": "Synonym selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8307085335254669}, {"text": "sense partitioning", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7353948652744293}]}, {"text": "Word similarity rating, on the other hand, is a classic test of the extent to which vector representations simulate human intuitions of word relations in general.", "labels": [], "entities": []}, {"text": "For synonym selection, we follow Jauhar et al. in testing with ESL-50), RD-300), and TOEFL-80 (Landauer and Dumais, 1997), using maxSim for multi-  Initial word representations.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9371019899845123}, {"text": "TOEFL-80", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7682536244392395}, {"text": "multi-  Initial word representations", "start_pos": 140, "end_pos": 176, "type": "TASK", "confidence": 0.6162255108356476}]}, {"text": "We use the word2vec () skip-gram architecture to train 80-dimensional word vectors (in keeping with, based on evidence that this model shows consistently strong performance on a wide array of tasks (. Training is on ukWaC), a diverse 2B-word web corpus.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 216, "end_pos": 221, "type": "DATASET", "confidence": 0.9803061485290527}]}, {"text": "Sense-graph construction from parallel text.", "labels": [], "entities": [{"text": "Sense-graph construction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6547351479530334}]}, {"text": "To construct the sense graph per Section 2, we use \u223c5.8M lines of segmented Chinese-English parallel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes corpus ().", "labels": [], "entities": [{"text": "DARPA BOLT project", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.7168670992056528}, {"text": "OntoNotes corpus", "start_pos": 226, "end_pos": 242, "type": "DATASET", "confidence": 0.9259544909000397}]}, {"text": "We perform word alignment with the Berkeley aligner ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7526380121707916}]}, {"text": "We filter out noisy alignments using the Gtest statistic, with a threshold selected during tuning on a development set.", "labels": [], "entities": [{"text": "Gtest", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9473516941070557}]}, {"text": "We set \u03b1 (see Equation 1) to 1.0.", "labels": [], "entities": []}, {"text": "Each sensesense edge e i (c j ), e i (c j ) has individual weight 0 < \u03b2 r \u2264 1, computed by obtaining the G-test statistic for the alignment of e i with c j and for the alignment of e i with c j , running these values through a logistic function, and averaging.", "labels": [], "entities": []}, {"text": "Parameters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the development set.", "labels": [], "entities": [{"text": "G-test statistic threshold", "start_pos": 50, "end_pos": 76, "type": "METRIC", "confidence": 0.931181530157725}]}, {"text": "Note that we have not currently incorporated special treatment for alignments of a single word to a multi-word phrase.", "labels": [], "entities": []}, {"text": "This does create the possibility of noisy or uninformative sense annotations (e.g., sense annotations corresponding to parts of aligned Chinese phrases) when such alignments are not filtered out by the G-test thresholding.", "labels": [], "entities": []}, {"text": "We evaluate the following experimental conditions: Skip-gram (SG) uses the un-retrofitted word2vec vectors, Word-Net (WN) retrofits using the WordNet-based sense graph, and Parallel Data (PD) retrofits using the sense graph built from parallel text.", "labels": [], "entities": [{"text": "WordNet-based sense graph", "start_pos": 142, "end_pos": 167, "type": "DATASET", "confidence": 0.8645280599594116}]}, {"text": "We also combine the two retrofitting approaches (PD-WN).", "labels": [], "entities": []}, {"text": "For synonym selection, we compute maxSim overall sense pairs for WN and PD separately, and select the sense pair with the overall maximum cosine similarity across the two.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.898339033126831}]}, {"text": "For similarity rating, we explore two PD-WN combination approaches: for each word pair, we take the avgSim from each separate model, and then we (a) take the average of the values given by the two models (avg), or (b) take the maximum value between the two models (max).", "labels": [], "entities": [{"text": "similarity rating", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8081143498420715}]}, {"text": "shows that combining our new method with Jauhar et al.'s WN retrofitting performs best on synonym selection across all datasets, and both retrofitted models consistently outperform the noretrofitting model (SG).", "labels": [], "entities": [{"text": "WN retrofitting", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.8315908312797546}, {"text": "synonym selection", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.9306670427322388}]}, {"text": "Error analysis on RD-87, the only set on which WN substantially outperforms PD, suggests that PD's errors are driven by the large number of lower frequency items that characterize this dataset.", "labels": [], "entities": [{"text": "RD-87", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.703826904296875}]}, {"text": "Given that WordNet is a hand-curated lexicon while the parallel data mirrors actual usage, it is not surprising that the latter suffers when it comes to low frequency items.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9298821091651917}]}], "tableCaptions": [{"text": " Table 2) are less clearly interpretable, top perfor- mance being divided between the PD model and the  combined models-with the exception of WS-353.  We note that WS-353 is a test set for which human", "labels": [], "entities": [{"text": "WS-353", "start_pos": 142, "end_pos": 148, "type": "DATASET", "confidence": 0.9727044701576233}, {"text": "WS-353", "start_pos": 164, "end_pos": 170, "type": "DATASET", "confidence": 0.904230535030365}]}, {"text": " Table 2: Similarity rating task results", "labels": [], "entities": [{"text": "Similarity rating task", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8690126538276672}]}]}