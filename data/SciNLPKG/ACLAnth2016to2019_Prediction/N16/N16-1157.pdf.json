{"title": [], "abstractContent": [{"text": "As more historical texts are digitized, there is interest in applying natural language processing tools to these archives.", "labels": [], "entities": []}, {"text": "However, the performance of these tools is often unsatisfactory , due to language change and genre differences.", "labels": [], "entities": []}, {"text": "Spelling normalization heuristics are the dominant solution for dealing with historical texts, but this approach fails to account for changes in usage and vocabulary.", "labels": [], "entities": [{"text": "Spelling normalization heuristics", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9426591197649637}]}, {"text": "In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7677953243255615}]}, {"text": "We evaluate several domain adaptation methods on the task of tagging Early Modern English and Modern British English texts in the Penn Corpora of Historical English.", "labels": [], "entities": [{"text": "tagging Early Modern English and Modern British English texts", "start_pos": 61, "end_pos": 122, "type": "TASK", "confidence": 0.7394846147961087}, {"text": "Penn Corpora of Historical English", "start_pos": 130, "end_pos": 164, "type": "DATASET", "confidence": 0.9708157181739807}]}, {"text": "We demonstrate that the Feature Embedding method for unsu-pervised domain adaptation outperforms word embeddings and Brown clusters, showing the importance of embedding the entire feature space, rather than just individual words.", "labels": [], "entities": [{"text": "unsu-pervised domain adaptation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7424087723096212}]}, {"text": "Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts.", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8841045796871185}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9074804782867432}, {"text": "Early Modern English texts", "start_pos": 186, "end_pos": 212, "type": "DATASET", "confidence": 0.6961927860975266}]}], "introductionContent": [{"text": "There is growing interest in applying natural language processing (NLP) techniques to historical texts, with applications in information retrieval), linguistics (, and the digital humanities Original: and drewe vnto hym all ryottours & wylde dysposed persones Normalization: and drew unto him all ryottours & wild disposed persons).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7009302377700806}]}, {"text": "However, these texts differ from contemporary training corpora in a number of linguistic respects, including the lexicon (, morphology, and syntax ().", "labels": [], "entities": []}, {"text": "This imposes significant challenges for modern NLP tools: for example, the accuracy of the CLAWS part-of-speech Tagger () drops from 97% on the British National Corpus to 82% on Early Modern English texts (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994350075721741}, {"text": "CLAWS part-of-speech Tagger", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6389434734980265}, {"text": "British National Corpus", "start_pos": 144, "end_pos": 167, "type": "DATASET", "confidence": 0.9378063678741455}]}, {"text": "There are two main approaches that could improve the accuracy of NLP systems on historical texts: normalization and domain adaptation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.997708797454834}, {"text": "normalization", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.9560779929161072}, {"text": "domain adaptation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7525018453598022}]}, {"text": "Normalization Spelling normalization (also called canonicalization) involves mapping historical spellings to their canonical forms in modern languages, thus bridging the gap between contemporary training corpora and target historical texts.", "labels": [], "entities": [{"text": "Normalization Spelling normalization", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.9164247512817383}]}, {"text": "shows one historical sentence and its normalization by VARD (.", "labels": [], "entities": [{"text": "VARD", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8493984341621399}]}, {"text": "report an increase of about 3% accuracy on adaptation of POS tagging from Modern English texts to Early Modern English texts if the target texts were automatically normalized by the VARD system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995726943016052}, {"text": "POS tagging from Modern English texts", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.689103384812673}, {"text": "VARD", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.5086632966995239}]}, {"text": "However, normalization is not always a well-defined problem, and it does not address the full range of linguistic changes overtime, such as unknown words, morphological differences, and changes in the meanings of words ().", "labels": [], "entities": []}, {"text": "In the example above, the word 'ryottours' is not successfully normalized to 'rioters'; the syntax is comprehensible to contemporary English speakers, but usages such as 'wild disposed' and 'drew unto' are sufficiently unusual as to pose problems for NLP systems trained on contemporary texts.", "labels": [], "entities": [{"text": "ryottours'", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9566519260406494}]}, {"text": "Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8084843456745148}]}, {"text": "It is natural to think of historical texts as a distinct domain from contemporary training corpora, and show that the accuracy of historical Portuguese POS tagging can be significantly improved by domain adaption.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9990002512931824}, {"text": "POS tagging", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.6910389065742493}]}, {"text": "However, we are unaware of prior work that empirically evaluates the efficacy of this approach on Early Modern English texts.", "labels": [], "entities": [{"text": "Early Modern English texts", "start_pos": 98, "end_pos": 124, "type": "DATASET", "confidence": 0.6503385603427887}]}, {"text": "Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may influence the text's linguistic properties.", "labels": [], "entities": []}, {"text": "Multi-domain adaptation) and multi-attribute domain adaptation can potentially exploit these metadata attributes to obtain further improvements.", "labels": [], "entities": [{"text": "Multi-domain adaptation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6988634020090103}, {"text": "multi-attribute domain adaptation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.625531534353892}]}, {"text": "This paper presents the first comprehensive empirical comparison of effectiveness of these approaches for part-of-speech tagging on historical texts.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7712019681930542}]}, {"text": "We focus on the two historical treebanks of the Penn Corpora of Historical English -the Penn Parsed Corpus of Modern British English ( and the Penn-Helsinki Parsed Corpus of Early Modern English (.", "labels": [], "entities": [{"text": "Penn Corpora of Historical English", "start_pos": 48, "end_pos": 82, "type": "DATASET", "confidence": 0.9711548447608948}, {"text": "Penn Parsed Corpus of Modern British English", "start_pos": 88, "end_pos": 132, "type": "DATASET", "confidence": 0.9304649318967547}, {"text": "Penn-Helsinki Parsed Corpus of Early Modern English", "start_pos": 143, "end_pos": 194, "type": "DATASET", "confidence": 0.9089304464203971}]}, {"text": "These datasets enable a range of analyses, which isolate the key issues in dealing with historical corpora: \u2022 In one set of analyses, we focus on the PPCMBE and the PPCEME corpora, training on more recent texts and testing on earlier texts.", "labels": [], "entities": [{"text": "PPCMBE", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.9358329176902771}, {"text": "PPCEME corpora", "start_pos": 165, "end_pos": 179, "type": "DATASET", "confidence": 0.9326574504375458}]}, {"text": "This isolates the impact of language change on tagging performance.", "labels": [], "entities": []}, {"text": "\u2022 In another set of analyses, we train on the Penn Treebank (, and test on the historical corpora, using the tag mappings from.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9959078431129456}]}, {"text": "We apply the well-known Stanford CoreNLP tagger to this task (), thus replicating the most typical situation for users of existing language technology.", "labels": [], "entities": []}, {"text": "\u2022 We show that FEMA, a domain adaptation algorithm that is specifically designed for sequence labeling problems (, achieves an increase of nearly 4% in tagging accuracy when adapting from the PTB to the PPCEME.", "labels": [], "entities": [{"text": "FEMA", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9783444404602051}, {"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7056879252195358}, {"text": "sequence labeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6024559140205383}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9590830206871033}, {"text": "PTB", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.9204739928245544}]}, {"text": "\u2022 We compare the impact of normalization with domain adaptation, and demonstrate that they are largely complementary.", "labels": [], "entities": []}, {"text": "\u2022 Error analysis shows that the improvements obtained by domain adaptation are largely due to better handling of out-of-vocabulary (OOV) tokens.", "labels": [], "entities": [{"text": "Error", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9537298083305359}, {"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.708918035030365}]}, {"text": "Many of the most frequent errors on in-vocabulary (IV) tokens are caused by mismatches in the tagsets or annotation guidelines, and maybe difficult to address without labeled data in the target domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate these unsupervised domain adaptation approaches on part-of-speech tagging for historical English (the PPCMBE and the PPCEME), in two settings: (1) temporal adaptation within each individual corpus, where we train POS taggers on the most modern data in the corpus and test on increasingly distant datasets; (2) adaptation of English POS tagging from modern news text to historical texts.", "labels": [], "entities": [{"text": "adaptation of English POS tagging from modern news text", "start_pos": 322, "end_pos": 377, "type": "TASK", "confidence": 0.6981644166840447}]}, {"text": "The first setting focuses on temporal differences, and eliminates other factors that may impair tagging performance, such as different annotation schemes and text genres.", "labels": [], "entities": []}, {"text": "The second setting is the standard and well-studied evaluation scenario for POS tagging, where we train on the Wall Street Journal (WSJ) text from the PTB and test on historical texts.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.959725022315979}, {"text": "Wall Street Journal (WSJ) text from the PTB", "start_pos": 111, "end_pos": 154, "type": "DATASET", "confidence": 0.9497200608253479}]}, {"text": "In addition, we evaluate the effectiveness of the VARD normalization tool () for improving POS tagging performance on the PPCEME corpus.", "labels": [], "entities": [{"text": "VARD normalization", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.855579286813736}, {"text": "POS tagging", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.802663117647171}, {"text": "PPCEME corpus", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.7632988393306732}]}, {"text": "The datasets used in the experiments are described in \u00a7 2.", "labels": [], "entities": []}, {"text": "All the hyperparameters are tuned on development data in the source domain.", "labels": [], "entities": []}, {"text": "In the case where there is no specific development dataset (adaptation within the historical corpora), we randomly sample 10% sentences from the training datasets for hyperparameter tuning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the Penn Parsed Corpus of Modern British", "labels": [], "entities": [{"text": "Penn Parsed Corpus of Modern British", "start_pos": 28, "end_pos": 64, "type": "DATASET", "confidence": 0.9450220763683319}]}, {"text": " Table 2: Statistics of the Penn Parsed Corpus of Early Modern", "labels": [], "entities": [{"text": "Penn Parsed Corpus of Early Modern", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.932764450709025}]}, {"text": " Table 3: Accuracy results for temporal adaptation in the PPCMBE and the PPCEME of historical English. Percentage error", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976480603218079}, {"text": "PPCMBE", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.9283350110054016}]}, {"text": " Table 4: Accuracy results for adapting from the PTB to the PPCMBE and the PPCEME of historical English.  *  Error reduction for", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981549382209778}, {"text": "PTB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9635446071624756}, {"text": "PPCMBE", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9668846130371094}, {"text": "PPCEME", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.8857992887496948}, {"text": "Error reduction", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.972461998462677}]}, {"text": " Table 5: Tagging accuracies of adaptation of our baseline SVM", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9702799916267395}, {"text": "SVM", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.6472656726837158}]}, {"text": " Table 6: Accuracy (recall) rates per tag with the SVM model,", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989203214645386}, {"text": "recall) rates", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.9573228557904562}]}, {"text": " Table 7: Tagging accuracies of domain adaptation models from", "labels": [], "entities": [{"text": "Tagging accuracies of domain adaptation", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.6733749330043792}]}]}