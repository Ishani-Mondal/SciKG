{"title": [{"text": "Learning Distributed Representations of Sentences from Unlabelled Data", "labels": [], "entities": [{"text": "Learning Distributed Representations of Sentences from Unlabelled Data", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.7838317528367043}]}], "abstractContent": [{"text": "Unsupervised methods for learning distributed representations of words are ubiquitous in to-day's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data.", "labels": [], "entities": []}, {"text": "This paper is a systematic comparison of models that learn such representations.", "labels": [], "entities": []}, {"text": "We find that the optimal approach depends critically on the intended application.", "labels": [], "entities": []}, {"text": "Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-bilinear models work best for building representation spaces that can be decoded with simple spatial distance met-rics.", "labels": [], "entities": []}, {"text": "We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations -dense real-valued vectors that encode the semantics of linguistic unitsare ubiquitous in today's NLP research.", "labels": [], "entities": []}, {"text": "For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words).", "labels": [], "entities": [{"text": "predicting adjacent words", "start_pos": 207, "end_pos": 232, "type": "TASK", "confidence": 0.8793954650561014}]}, {"text": "These methods are well understood empirically () and theoretically ().", "labels": [], "entities": []}, {"text": "The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (, and can be added as features to improve the performance of numerous language processing systems.", "labels": [], "entities": []}, {"text": "By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.", "labels": [], "entities": []}, {"text": "With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors.", "labels": [], "entities": []}, {"text": "Examples include machine translation), image captioning () and dialogue systems (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8015647232532501}, {"text": "image captioning", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7349767088890076}]}, {"text": "While it has been observed informally that the internal sentence representations of such models can reflect semantic intuitions (), it is not known which architectures or objectives yield the 'best' or most useful representations.", "labels": [], "entities": []}, {"text": "Resolving this question could ultimately have a significant impact on language processing systems.", "labels": [], "entities": []}, {"text": "Indeed, it is phrases and sentences, rather than individual words, that encode the human-like general world knowledge (or 'common sense') that is a critical missing part of most current language understanding systems.", "labels": [], "entities": []}, {"text": "We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences.", "labels": [], "entities": []}, {"text": "We focus on methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains.", "labels": [], "entities": []}, {"text": "We also propose two new phrase or sentence representation learning objectives -Sequential Denoising Autoencoders and FastSent, a sentence-level log-bilinear bag-ofwords model.", "labels": [], "entities": []}, {"text": "We compare all methods on two types of task -supervised and unsupervised evaluations -reflecting different ways in which representations are ultimately to be used.", "labels": [], "entities": []}, {"text": "In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.", "labels": [], "entities": []}, {"text": "We observe notable differences in approaches depending on the nature of the evaluation metric.", "labels": [], "entities": []}, {"text": "In particular, deeper or more complex models (which require greater time and resources to train) generally perform best in the supervised setting, whereas shallow log-bilinear models work best on unsupervised benchmarks.", "labels": [], "entities": []}, {"text": "Specifically, SkipThought Vectors () perform best on the majority of supervised evaluations, but SDAEs are the top performer on paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.9041931331157684}]}, {"text": "In contrast, on the (unsupervised) SICK sentence relatedness benchmark, FastSent, a simple, log-bilinear variant of the SkipThought objective, performs better than all other models.", "labels": [], "entities": [{"text": "SICK sentence relatedness", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7817562619845072}]}, {"text": "Interestingly, the method that exhibits strongest performance across both supervised and unsupervised benchmarks is a bag-ofwords model trained to compose word embeddings using dictionary definitions (.", "labels": [], "entities": []}, {"text": "Taken together, these findings constitute valuable guidelines for the application of phrasal or sentential representation-learning to language understanding systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (), movie review sentiment (MR) (Pang and), product reviews (CR) (), subjectivity classification (SUBJ) (), opinion polarity (MPQA) ( and question type classification (TREC)).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7592180073261261}, {"text": "paraphrase identification (MSRP)", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.8107811689376831}, {"text": "movie review sentiment (MR)", "start_pos": 101, "end_pos": 128, "type": "METRIC", "confidence": 0.6120528280735016}, {"text": "subjectivity classification (SUBJ)", "start_pos": 166, "end_pos": 200, "type": "TASK", "confidence": 0.7454737544059753}, {"text": "question type classification (TREC))", "start_pos": 235, "end_pos": 271, "type": "TASK", "confidence": 0.6457857340574265}]}, {"text": "We follow the procedure (and code) of: a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined.", "labels": [], "entities": []}, {"text": "We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements..", "labels": [], "entities": [{"text": "semantic sentence relatedness", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.6815714041392008}]}, {"text": "All available pairs are used for testing apart from the 500 SICK 'trial' pairs, which are held-out for tuning hyperparameters (representation size of log-bilinear models, and noise parameters in SDAE).", "labels": [], "entities": []}, {"text": "The optimal settings on this task are then applied to both supervised and unsupervised evaluations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Properties of models compared in this study  OS: requires training corpus of sentences in order. R: requires  structured resource for training. WO: encoder sensitive to word  order. SD: dimension of sentence representation. WD: dimen- sion of word representation. TR: approximate training time  (hours) on the dataset in this paper. * indicates trained on GPU.  TE: approximate time (s) taken to encode 0.5m sentences.", "labels": [], "entities": [{"text": "GPU", "start_pos": 366, "end_pos": 369, "type": "DATASET", "confidence": 0.9245595335960388}, {"text": "TE", "start_pos": 372, "end_pos": 374, "type": "METRIC", "confidence": 0.9870087504386902}]}, {"text": " Table 3: Performance of sentence representation models on supervised evaluations (Section 3.1). Bold numbers indicate best  performance in class. Underlined indicates best overall.", "labels": [], "entities": []}, {"text": " Table 4: Performance of sentence representation models (Spearman/Pearson correlations) on unsupervised (relatedness) evalua- tions (Section 3.2). Models are grouped according to training data as indicated in Table 3.", "labels": [], "entities": []}]}