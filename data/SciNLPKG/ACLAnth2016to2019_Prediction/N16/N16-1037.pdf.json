{"title": [{"text": "A Latent Variable Recurrent Neural Network for Discourse Relation Language Models", "labels": [], "entities": [{"text": "Discourse Relation Language", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.7887995640436808}]}], "abstractContent": [{"text": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences.", "labels": [], "entities": []}, {"text": "A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations.", "labels": [], "entities": []}, {"text": "The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task.", "labels": [], "entities": []}, {"text": "The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.6340458691120148}, {"text": "word prediction", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7915768325328827}]}, {"text": "As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.5735976174473763}, {"text": "Penn Discourse Treebank", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.9752986431121826}, {"text": "dialog act classification", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.6949464281400045}, {"text": "Switchboard corpus", "start_pos": 185, "end_pos": 203, "type": "DATASET", "confidence": 0.7757363617420197}]}, {"text": "Furthermore, by marginalizing over latent discourse relations attest time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) has recently experienced a neural network \"tsunami\".", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7796934147675832}]}, {"text": "A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words) to longer-range linguistic contexts at the sentence level) and beyond ().", "labels": [], "entities": []}, {"text": "Because they are discriminatively trained, these methods can learn representations that yield very accurate predictive models (e.g., ).", "labels": [], "entities": []}, {"text": "However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility.", "labels": [], "entities": []}, {"text": "By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable attest or training time, elegantly modeling multiple linguistic phenomena in a joint framework ().", "labels": [], "entities": []}, {"text": "But because these graphical models represent uncertainty for every element in the model, adding too many layers of latent variables makes them difficult to train.", "labels": [], "entities": []}, {"text": "In this paper, we present a hybrid architecture that combines a recurrent neural network language model with a latent variable model over shallow discourse structure.", "labels": [], "entities": []}, {"text": "In this way, the model learns a discriminatively-trained distributed representation of the local contextual features that drive word choice at the intra-sentence level, using techniques that are now state-of-the-art in language modeling (.", "labels": [], "entities": []}, {"text": "However, the model treats shallow discourse structure -specifically, the relationships between pairs of adjacent sentencesas a latent variable.", "labels": [], "entities": []}, {"text": "As a result, the model can act as both a discourse relation classifier and a language model.", "labels": [], "entities": []}, {"text": "Specifically: \u2022 If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classification in the Penn Discourse Treebank (  and dialog act classification in Switch-board).", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 143, "end_pos": 185, "type": "TASK", "confidence": 0.6397670209407806}, {"text": "Penn Discourse Treebank", "start_pos": 193, "end_pos": 216, "type": "DATASET", "confidence": 0.9742189844449362}, {"text": "dialog act classification", "start_pos": 224, "end_pos": 249, "type": "TASK", "confidence": 0.7060360113779703}]}, {"text": "The model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse relations ( . \u2022 If the model is trained to maximize the joint likelihood of the discourse relations and the text, it is possible to marginalize over discourse relations attest time, outperforming language models that do not account for discourse structure.", "labels": [], "entities": []}, {"text": "In contrast to recent work on continuous latent variables in recurrent neural networks, which require complex variational autoencoders to represent uncertainty over the latent variables, our model is simple to implement and train, requiring only minimal modifications to existing recurrent neural network architectures that are implemented in commonly-used toolkits such as Theano, Torch, and CNN.", "labels": [], "entities": [{"text": "Theano", "start_pos": 374, "end_pos": 380, "type": "DATASET", "confidence": 0.9694268107414246}]}, {"text": "We focus on a class of shallow discourse relations, which hold between pairs of adjacent sentences (or utterances).", "labels": [], "entities": []}, {"text": "These relations describe how the adjacent sentences are related: for example, they maybe in CONTRAST, or the latter sentence may offer an answer to a question posed by the previous sentence.", "labels": [], "entities": []}, {"text": "Shallow relations do not capture the full range of discourse phenomena), but they account for two well-known problems: implicit discourse relation classification in the Penn Discourse Treebank, which was the 2015 CoNLL shared task ( ; and dialog act classification, which characterizes the structure of interpersonal communication in the Switchboard corpus (), and is a key component of contemporary dialog systems.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 119, "end_pos": 161, "type": "TASK", "confidence": 0.5900446474552155}, {"text": "Penn Discourse Treebank", "start_pos": 169, "end_pos": 192, "type": "DATASET", "confidence": 0.9646658698717753}, {"text": "dialog act classification", "start_pos": 239, "end_pos": 264, "type": "TASK", "confidence": 0.6838136911392212}, {"text": "Switchboard corpus", "start_pos": 338, "end_pos": 356, "type": "DATASET", "confidence": 0.7354552745819092}]}, {"text": "Our model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6310422122478485}, {"text": "Penn Discourse Treebank", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.9757356643676758}, {"text": "dialog act classification", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6949862241744995}, {"text": "Switchboard corpus", "start_pos": 172, "end_pos": 190, "type": "DATASET", "confidence": 0.772502213716507}]}], "datasetContent": [{"text": "Our main evaluation is discourse relation prediction, using the PDTB and SWDA corpora.", "labels": [], "entities": [{"text": "discourse relation prediction", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.8161801298459371}, {"text": "PDTB", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9088117480278015}, {"text": "SWDA corpora", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8218257427215576}]}, {"text": "We also evaluate on language modeling, to determine whether incorporating discourse annotations at training time and then marginalizing them attest time can improve performance.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7350176572799683}]}], "tableCaptions": [{"text": " Table 1: Multiclass relation identification on the first-level", "labels": [], "entities": [{"text": "Multiclass relation identification", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.8226435383160909}]}, {"text": " Table 2: The results of dialogue act tagging.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7046621044476827}]}, {"text": " Table 3: Language model perplexities (PPLX), lower is better.", "labels": [], "entities": []}]}