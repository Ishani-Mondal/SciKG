{"title": [], "abstractContent": [{"text": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions , lengths, and labels are separate entries in our vocabulary.", "labels": [], "entities": []}, {"text": "Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model.", "labels": [], "entities": []}, {"text": "Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources).", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.7700623869895935}, {"text": "Named Entity Recognition", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.6735129753748575}]}, {"text": "Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The long-term trajectory of research in Natural Language Processing has seen the replacement of rules and specific linguistic knowledge with machine learned components.", "labels": [], "entities": []}, {"text": "Perhaps the most standardized way that knowledge is still injected into largely statistical systems is through the processing pipeline: Some set of basic language-specific tokens are identified in a first step.", "labels": [], "entities": []}, {"text": "Sequences of tokens are segmented into sentences in a second step.", "labels": [], "entities": []}, {"text": "The resulting sentences are fed one at a time for syntactic analysis: Part-of-Speech (POS) tagging and parsing.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7160129249095917}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6474335789680481}]}, {"text": "Next, the predicted syntactic structure is typically used as features in semantic analysis, Named Entity Recognition (NER), Semantic Role Labeling, etc.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.8360285758972168}, {"text": "Named Entity Recognition (NER)", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.7856041689713796}, {"text": "Semantic Role Labeling", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7347593307495117}]}, {"text": "While each step of the pipeline now relies more on data and models than on hand-curated rules, the pipeline structure itself encodes one particular understanding of how meaning attaches to raw strings.", "labels": [], "entities": []}, {"text": "One motivation for our work is to try removing this structural dependence.", "labels": [], "entities": []}, {"text": "Rather than rely on the intermediate representations invented for specific subtasks (for example, Penn Treebank tokenization), we are allowing the model to learn whatever internal structure is most conducive to producing the annotations of interest.", "labels": [], "entities": [{"text": "Penn Treebank tokenization", "start_pos": 98, "end_pos": 124, "type": "DATASET", "confidence": 0.8996923168500265}]}, {"text": "To this end, we describe a Recurrent Neural Network (RNN) model that reads raw input string segments, one byte at a time, and produces output span annotations corresponding to specific byte regions in the input . This is truly language annotation from scratch (see and).", "labels": [], "entities": []}, {"text": "Two key innovations facilitate this approach.", "labels": [], "entities": []}, {"text": "First, Long Short Term Memory (LSTM) models) allow us to replace the traditional independence assumptions in text processing with structural constraints on memory.", "labels": [], "entities": []}, {"text": "While we have long known that long-term dependencies are important in language, we had no mechanism other than conditional independence to keep sparsity in check.", "labels": [], "entities": []}, {"text": "The memory in an LSTM, however, is not constrained by any explicit assumptions of independence.", "labels": [], "entities": []}, {"text": "Rather, its ability to learn patterns is limited only by the structure of the network and the size of the memory (and of course the Generate Output (GO) symbol, then produces the argmax output of a softmax overall possible start positions, lengths, and labels (as well as STOP, signifying no additional outputs).", "labels": [], "entities": [{"text": "Generate Output (GO) symbol", "start_pos": 132, "end_pos": 159, "type": "METRIC", "confidence": 0.8146367967128754}, {"text": "STOP", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.8557001352310181}]}, {"text": "The prediction from the previous time step is fed as an input to the next time step.", "labels": [], "entities": []}, {"text": "amount of training data).", "labels": [], "entities": []}, {"text": "Second, sequence-to-sequence models), allow for flexible input/output dynamics.", "labels": [], "entities": []}, {"text": "Traditional models, including feedforward neural networks, read fixed-length inputs and generate fixed-length outputs by following a fixed set of computational steps.", "labels": [], "entities": []}, {"text": "Instead, we can now read an entire segment of text before producing an arbitrary number of outputs, allowing the model to learn a function best suited to the task.", "labels": [], "entities": []}, {"text": "We leverage these two ideas with a basic strategy: Decompose inputs and outputs into their component pieces, then read and predict them as sequences.", "labels": [], "entities": []}, {"text": "Rather than read words, we are reading a sequence of unicode bytes 2 ; rather than producing a label for each word, we are producing triples, that correspond to the spans of interest, as a sequence of three separate predictions (see.", "labels": [], "entities": []}, {"text": "This forces the model to learn how the components of words and labels interact so all the structure typically imposed by the NLP pipeline (as well as the rules of unicode) are left to the LSTM to model.", "labels": [], "entities": []}, {"text": "Decomposed inputs and outputs have a few important benefits.", "labels": [], "entities": []}, {"text": "First, they reduce the size of the vocabulary relative to word-level inputs, so the resulting models are extremely compact (on the order of a million parameters).", "labels": [], "entities": []}, {"text": "Second, because unicode is essentially a universal language, we can train models to analyze many languages at once.", "labels": [], "entities": []}, {"text": "In fact, by stacking LSTMs, we are able to learn representations that appear to generalize across languages, improving performance significantly (without using any additional parameters) over models trained on a single language.", "labels": [], "entities": []}, {"text": "This is the first account, to our knowledge, of a multilingual model that achieves good results across many languages, thus bypassing all the language-specific engineering usually required to build models in different languages . We describe results similar to or better than the stateof-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources).", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 299, "end_pos": 321, "type": "TASK", "confidence": 0.7779446244239807}, {"text": "Named Entity Recognition", "start_pos": 326, "end_pos": 350, "type": "TASK", "confidence": 0.6603560845057169}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work; Section 3 describes our model; Section 4 gives training details including anew variety of dropout (); Section 5 gives inference details; Section 6 presents results on POS tagging and NER across many languages; Finally, we summarize our contributions in section 7.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.811647891998291}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: A comparison of NER systems. The results are F1", "labels": [], "entities": [{"text": "NER", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9250277876853943}]}, {"text": " Table 3: BTS Part-of-speech tagging average accuracy across", "labels": [], "entities": [{"text": "BTS Part-of-speech tagging", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.68897412220637}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9449949860572815}]}, {"text": " Table 4: Macro-averaged (across 4 languages) F1 for the NER", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9979730248451233}, {"text": "NER", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.5597668886184692}]}]}