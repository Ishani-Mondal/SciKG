{"title": [{"text": "Comparing Convolutional Neural Networks to Traditional Models for Slot Filling", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.8735713958740234}]}], "abstractContent": [{"text": "We address relation classification in the context of slot filling, the task of finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X founded Apple\".", "labels": [], "entities": [{"text": "relation classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8400252163410187}, {"text": "slot filling", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.8478812575340271}]}, {"text": "We propose a convo-lutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-of-the-art and traditional approaches of relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.8173289895057678}]}, {"text": "Finally, we combine different methods and show that the combination is better than individual approaches.", "labels": [], "entities": []}, {"text": "We also analyze the effect of genre differences on performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Structured knowledge about the world is useful for many natural language processing (NLP) tasks, such as disambiguation, question answering or semantic search.", "labels": [], "entities": [{"text": "question answering", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.8517486751079559}, {"text": "semantic search", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.685900866985321}]}, {"text": "However, the extraction of structured information from natural language text is challenging because one relation can be expressed in many different ways.", "labels": [], "entities": [{"text": "extraction of structured information from natural language text", "start_pos": 13, "end_pos": 76, "type": "TASK", "confidence": 0.7432581186294556}]}, {"text": "The TAC Slot Filling (SF) Shared Task defines slot filling as extracting fillers fora set of predefined relations (\"slots\") from a large corpus of text data.", "labels": [], "entities": [{"text": "TAC Slot Filling (SF) Shared Task", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8523951023817062}, {"text": "slot filling", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.7841392457485199}]}, {"text": "Exemplary relations are the city of birth of a person or the employees or founders of a company.", "labels": [], "entities": []}, {"text": "Participants are provided with an evaluation corpus and a query file consisting of pairs of entities and slots.", "labels": [], "entities": []}, {"text": "For each entity slot pair (e.g. \"Apple\" and \"founded by\"), the systems have to return the second argument (\"filler\") of the relation (e.g. \"Steve Jobs\") as well as a supporting sentence from the evaluation corpus.", "labels": [], "entities": []}, {"text": "The key challenge in slot filling is relation classification: given a sentence s of the evaluation corpus containing the name of a queried entity (e.g., \"Apple\") and a filler candidate (e.g., \"Steve Jobs\"), we need to decide whether s expresses the relation (\"founded by\", in this case).", "labels": [], "entities": [{"text": "slot filling", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8115379810333252}, {"text": "relation classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8240233063697815}]}, {"text": "We will refer to the mentions of the two arguments of the relation as name and filler.", "labels": [], "entities": []}, {"text": "Performance on relation classification is crucial for slot filling since its effectiveness directly depends on it.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.9260522425174713}, {"text": "slot filling", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.9338799118995667}]}, {"text": "In this paper, we investigate three complementary approaches to relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.9162367284297943}]}, {"text": "The first approach is pattern matching, a leading approach in the TAC evaluations.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8780028820037842}, {"text": "TAC evaluations", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8449190557003021}]}, {"text": "Fillers are validated based on patterns.", "labels": [], "entities": []}, {"text": "In this work, we consider patterns learned with distant supervision and patterns extracted from Universal Schema relations.", "labels": [], "entities": []}, {"text": "The second approach is support vector machines.", "labels": [], "entities": []}, {"text": "We evaluate two different feature sets: a bag-ofword feature set (BOW) and more sophisticated skip n-gram features.", "labels": [], "entities": [{"text": "bag-ofword feature set (BOW)", "start_pos": 42, "end_pos": 70, "type": "METRIC", "confidence": 0.6160586525996526}]}, {"text": "Our third approach is a convolutional neural network (CNN).", "labels": [], "entities": []}, {"text": "CNNs have been applied to NLP tasks like sentiment analysis, part-of-speech tagging and semantic role labeling.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.959730863571167}, {"text": "part-of-speech tagging", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7337184250354767}, {"text": "semantic role labeling", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.664490540822347}]}, {"text": "They can recognize phrase patterns independent of their position in the sentence.", "labels": [], "entities": []}, {"text": "Furthermore, they make use of word embeddings that directly reflect word similarity ().", "labels": [], "entities": []}, {"text": "Hence, we expect them to be robust models for the task of classifying filler candidates and to generalize well to unseen test data.", "labels": [], "entities": []}, {"text": "In this work, we train different variants of CNNs: As a baseline, we reimplement the recently developed piecewise CNN (.", "labels": [], "entities": []}, {"text": "Then, we extend this model by splitting the contexts not only for pooling but also for convolution (contextwise CNN).", "labels": [], "entities": []}, {"text": "Currently, there is no benchmark for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9374129772186279}]}, {"text": "Therefore, it is not possible to directly compare results that were submitted to the Shared Task to new results.", "labels": [], "entities": []}, {"text": "Comparable manual annotations for new results, for instance, cannot be easily obtained.", "labels": [], "entities": []}, {"text": "There are also many different system components, such as document retrieval from the evaluation corpus and coreference resolution, that affect Shared Task performance and that are quite different in nature from relation classification.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.9542845189571381}, {"text": "relation classification", "start_pos": 211, "end_pos": 234, "type": "TASK", "confidence": 0.8525325059890747}]}, {"text": "Even in the subtask of relation classification, it is not possible to directly use existing relation classification benchmarks (e.g.,) since data and relations can be quite different.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8139298856258392}]}, {"text": "Many benchmark relations, for instance, correspond to Freebase relations but not all slots are modeled in Freebase and some slots even comprise more than one Freebase relation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9737460613250732}]}, {"text": "While most relation classification benchmarks either use newswire or web data, the SF task includes documents from both domains (and discussion fora).", "labels": [], "entities": [{"text": "relation classification benchmarks", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.9028518001238505}, {"text": "SF task", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.8133106827735901}]}, {"text": "Another difference to traditional relation classification benchmarks arises from the pipeline aspect of slot filling.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8183985948562622}, {"text": "slot filling", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.8858523964881897}]}, {"text": "Depending on the previous steps, the input for the relation classification models can be incomplete, noisy, include coreferent mentions, etc.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7452697157859802}]}, {"text": "The official SF Shared Task evaluations only assess whole systems (with potential subsequent faults in their pipelines ().", "labels": [], "entities": [{"text": "SF Shared Task evaluations", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.6723561137914658}]}, {"text": "Thus, we expect component wise comparisons to be a valuable addition to the Shared Task: With comparisons of single components, teams would be able to improve their modules more specifically.", "labels": [], "entities": []}, {"text": "To start with one of the most important components, we have created a benchmark for slot filling relation classification, based on 2012 -2014 TAC Shared Task data.", "labels": [], "entities": [{"text": "slot filling relation classification", "start_pos": 84, "end_pos": 120, "type": "TASK", "confidence": 0.9302189350128174}, {"text": "TAC Shared Task data", "start_pos": 142, "end_pos": 162, "type": "DATASET", "confidence": 0.6968839168548584}]}, {"text": "It will be described below and published along with this paper.", "labels": [], "entities": []}, {"text": "In addition to presenting model results on this benchmark dataset, we also show that these results correlate with end-to-end SF results.", "labels": [], "entities": []}, {"text": "Hence, optimizing a model on this dataset will also help improving results in the end-to-end setting.", "labels": [], "entities": []}, {"text": "In our experiments, we found that our models suffer from large genre differences in the TAC data.", "labels": [], "entities": [{"text": "TAC data", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.7320765554904938}]}, {"text": "Hence, the SF Shared Task is a task that conflates an 1 http://cistern.cis.lmu.de investigation of domain (or genre) adaptation with the one of slot filling.", "labels": [], "entities": [{"text": "domain (or genre) adaptation", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.7053141593933105}, {"text": "slot filling", "start_pos": 144, "end_pos": 156, "type": "TASK", "confidence": 0.7843968570232391}]}, {"text": "We argue that both problems are important NLP problems and provide datasets and results for both within and across genres.", "labels": [], "entities": []}, {"text": "We hope that this new resource will encourage others to test their models on our dataset and that this will help promote research on slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 133, "end_pos": 145, "type": "TASK", "confidence": 0.9427765011787415}]}, {"text": "In summary, our contributions are as follows.", "labels": [], "entities": []}, {"text": "(i) We investigate the complementary strengths and weaknesses of different approaches to relation classification and show that their combination can better deal with a diverse set of problems that slot filling poses than each of the approaches individually.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.9433965981006622}]}, {"text": "(ii) We propose to split the context at the relation arguments before passing it to the CNN in order to better deal with the special characteristics of a sentence in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 166, "end_pos": 189, "type": "TASK", "confidence": 0.8106363415718079}]}, {"text": "This outperforms the state-of-the-art piecewise CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9406368732452393}]}, {"text": "(iii) We analyze the effect of genre on slot filling and show that it is an important conflating variable that needs to be carefully examined in research on slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.91332146525383}, {"text": "slot filling", "start_pos": 157, "end_pos": 169, "type": "TASK", "confidence": 0.8934034705162048}]}, {"text": "(iv) We provide a benchmark for slot filling relation classification that will facilitate direct comparisons of models in the future and show that results on this dataset are correlated with end-to-end system results.", "labels": [], "entities": [{"text": "slot filling relation classification", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.929133340716362}]}, {"text": "Section 2 gives an overview of related work.", "labels": [], "entities": []}, {"text": "Section 3 discusses the challenges that slot filling systems face.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9177010953426361}]}, {"text": "In Section 4, we describe our slot filling models.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.8733130395412445}]}, {"text": "Section 5 presents experimental setup and results.", "labels": [], "entities": []}, {"text": "Section 6 analyzes the results.", "labels": [], "entities": []}, {"text": "We present our conclusions in Section 7 and describe the resources we publish in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the main challenges in building and evaluating relation classification models for SF is the shortage of training and evaluation data.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7835642099380493}, {"text": "SF", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9783629179000854}]}, {"text": "Each group has their own datasets and comparisons across groups are difficult.", "labels": [], "entities": []}, {"text": "Therefore, we have developed a script that creates a clean dataset based on manually annotated system outputs from previous Shared Task evaluations.", "labels": [], "entities": []}, {"text": "In the future, it can be used by all participants to evaluate components of their slot filling systems.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.8519142866134644}]}, {"text": "The script only extracts sentences that contain mentions of both name and filler.", "labels": [], "entities": []}, {"text": "It conducts a heuristic check based on NER tags to determine whether the name in the sentence is a valid mention of the query name or is referring to another entity.", "labels": [], "entities": []}, {"text": "In the latter case, the example is filtered out.", "labels": [], "entities": []}, {"text": "One difficulty is that many published offsets are incorrect.", "labels": [], "entities": []}, {"text": "We tried to match these using heuristics.", "labels": [], "entities": []}, {"text": "In general, we apply filters that ensure high quality of the resulting evaluation data even if that means that a considerable part of the TAC system output is discarded.", "labels": [], "entities": []}, {"text": "In total, we extracted 39,386 high-quality evaluation instances out of the 59,755 system output instances published by TAC and annotated as either completely corrector completely incorrect.", "labels": [], "entities": [{"text": "TAC", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.8971067070960999}]}, {"text": "A table in the supplementary material gives statistics: the number of positive and negative examples per slot and year (without duplicates).", "labels": [], "entities": []}, {"text": "For 2013, the most examples were extracted.", "labels": [], "entities": []}, {"text": "The lower number for 2014 is probably due to the newly introduced inference across documents.", "labels": [], "entities": []}, {"text": "This limits the number of sentences with mentions of both name and filler.", "labels": [], "entities": []}, {"text": "The average ratio of positive to negative examples is 1:4.", "labels": [], "entities": []}, {"text": "The number of positive examples per slot and year ranges from 0 (org:member of, 2014) to 581 (per:title, 2013), the number of negative examples from 5 (org:website, 2014) to).", "labels": [], "entities": []}, {"text": "In contrast to other relation classification benchmarks, this dataset is not based on a knowledge base (such as Freebase) and unrelated text (such as web documents) but directly on the SF assessments.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8615092933177948}]}, {"text": "Thus, it includes exactly the SF relations and addresses the challenges of the end-to-end task: noisy data, possibly incomplete extractions of sentences and data from different domains.", "labels": [], "entities": []}, {"text": "We use the data from 2012/2013 as development and the data from 2014 as evaluation set.", "labels": [], "entities": []}, {"text": "We evaluate the models described in Section 4, select the best models and combine them.", "labels": [], "entities": []}, {"text": "First, we compare the performance of PATdist and PATuschema on our dataset.", "labels": [], "entities": [{"text": "PATuschema", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.7982395887374878}]}, {"text": "We evaluate the pattern matchers on all slots presented in  Experiments with CNNs.", "labels": [], "entities": []}, {"text": "Finally, we compare the performance of CNNpiece, CNNpieceExt and CNNcontext.", "labels": [], "entities": [{"text": "CNNpiece", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9641701579093933}, {"text": "CNNpieceExt", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9476497769355774}, {"text": "CNNcontext", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.9648683667182922}]}, {"text": "While the baseline network CNNpiece ( achieves F 1 of .52 on dev, CNNpieceExt has an F 1 score of .55 and CNNcontext an F 1 of .60.", "labels": [], "entities": [{"text": "CNNpiece", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9475376009941101}, {"text": "F 1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9957259595394135}, {"text": "CNNpieceExt", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.9535506367683411}, {"text": "F 1 score", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9894684155782064}, {"text": "CNNcontext", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.8721888065338135}, {"text": "F 1", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9867537617683411}]}, {"text": "The difference of CNNpiece and CNNpieceExt is due to the additional hidden layer and k-max pooling.", "labels": [], "entities": [{"text": "CNNpiece", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9362282752990723}, {"text": "CNNpieceExt", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9396330714225769}]}, {"text": "The considerable difference in performance of CNNpieceExt and CNNcontext shows that splitting the context for convolution has a positive effect on the performance of the network.", "labels": [], "entities": [{"text": "CNNpieceExt", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9534959197044373}, {"text": "CNNcontext", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.9302992820739746}]}, {"text": "shows the slot wise results of the best patterns (PATdist), SVMs (SVMskip) and CNNs (CNNcontext).", "labels": [], "entities": []}, {"text": "Furthermore, it provides a comparison with two baseline models: Mintz++ and MIMLRE.", "labels": [], "entities": [{"text": "MIMLRE", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8064926266670227}]}, {"text": "SVM and CNN clearly outperform these baselines.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9343003034591675}, {"text": "CNN", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.943300724029541}]}, {"text": "They also outperform PAT for almost all slots.", "labels": [], "entities": [{"text": "PAT", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.48167693614959717}]}, {"text": "The difference between dev and eval results varies a lot among the slots.", "labels": [], "entities": []}, {"text": "We suspect that this is a result of genre differences in the data and analyze this in Section 6.4.", "labels": [], "entities": []}, {"text": "Slot wise results of the other models (PATuschema, SVMbow, CNNpiece, CNNpieceExt) can be found in the supplementary material.", "labels": [], "entities": [{"text": "CNNpiece", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9066749215126038}, {"text": "CNNpieceExt", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8550699949264526}]}, {"text": "Comparing PAT, SVM and CNN, 5 different patterns emerge for different slots.", "labels": [], "entities": [{"text": "PAT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7740663886070251}, {"text": "CNN", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8274810314178467}]}, {"text": "Each is best on a subset of the slots (see bold numbers).", "labels": [], "entities": []}, {"text": "This indicates that relation classification for slot filling is not a uniform problem: each slot has special properties and the three approaches are good at modeling a different subset of these properties.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8524714410305023}, {"text": "slot filling", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.8377944529056549}]}, {"text": "Given the big differences, we expect to gain performance by combining the three approaches.", "labels": [], "entities": []}, {"text": "Indeed, CMB (PATdist + SVMskip + CNNcontext), the combination of the three best performing models, obtains the best results in average (in bold).", "labels": [], "entities": []}, {"text": "Section 6.3 shows that the performance on our dataset is highly correlated with SF end-to-end per- In prior experiments, we also compared with recurrent neural networks.", "labels": [], "entities": [{"text": "SF", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9974472522735596}]}, {"text": "RNN performance was comparable to CNNs, but required much more training time and parameter tuning.", "labels": [], "entities": []}, {"text": "Therefore, we focus on CNNs in this paper.", "labels": [], "entities": []}, {"text": "Thus, our results indicate that a combination of different models ist the most promising approach to getting good performance on slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.9330676794052124}]}], "tableCaptions": [{"text": " Table 1: Performance on Slot Filling benchmark dataset (dev: data from 2012/2013, eval: from 2014). CMB denotes the combina-", "labels": [], "entities": [{"text": "Slot Filling benchmark dataset", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.7585550099611282}]}, {"text": " Table 2: Distribution of genres", "labels": [], "entities": []}, {"text": " Table 3: Genre specific F1 scores. Genre specific training data", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9739999771118164}]}]}