{"title": [{"text": "Clustering for Simultaneous Extraction of Aspects and Features from Reviews", "labels": [], "entities": [{"text": "Simultaneous Extraction of Aspects", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.9165937900543213}]}], "abstractContent": [{"text": "This paper presents a clustering approach that simultaneously identifies product features and groups them into aspect categories from on-line reviews.", "labels": [], "entities": []}, {"text": "Unlike prior approaches that first extract features and then group them into categories , the proposed approach combines feature and aspect discovery instead of chaining them.", "labels": [], "entities": [{"text": "aspect discovery", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7256430685520172}]}, {"text": "In addition, prior work on feature extraction tends to require seed terms and focus on identifying explicit features, while the proposed approach extracts both explicit and implicit features, and does not require seed terms.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7333443462848663}]}, {"text": "We evaluate this approach on reviews from three domains.", "labels": [], "entities": []}, {"text": "The results show that it outperforms several state-of-the-art methods on both tasks across all three domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "If you are thinking of buying a TV for watching football, you might go to websites such as Amazon to read customer reviews on TV products.", "labels": [], "entities": []}, {"text": "However, there are many products and each of them may have hundreds of reviews.", "labels": [], "entities": []}, {"text": "It would be helpful to have an aspect-based sentiment summarization for each product.", "labels": [], "entities": [{"text": "aspect-based sentiment summarization", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.6138281424840292}]}, {"text": "Based on other customers' opinions on multiple aspects such assize, picture quality, motion-smoothing, and sound quality, you might be able to make the decision without going through all the reviews.", "labels": [], "entities": [{"text": "assize", "start_pos": 60, "end_pos": 66, "type": "TASK", "confidence": 0.9210351705551147}]}, {"text": "To support such summarization, it is essential to have an algorithm that extracts product features and aspects from reviews.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9727482199668884}]}, {"text": "* This author's research was done during an internship with Samsung Research America.", "labels": [], "entities": [{"text": "Samsung Research America", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.8297926386197408}]}, {"text": "Features are components and attributes of a product.", "labels": [], "entities": []}, {"text": "A feature can be directly mentioned as an opinion target (i.e., explicit feature) or implied by opinion words (i.e., implicit feature).", "labels": [], "entities": []}, {"text": "Different feature expressions maybe used to describe the same aspect of a product.", "labels": [], "entities": []}, {"text": "Aspect can be represented as a group of features.", "labels": [], "entities": []}, {"text": "Consider the following review sentences, in which we denote explicit and implicit features in boldface and italics, respectively.", "labels": [], "entities": []}, {"text": "1. This phone has great display and perfect size.", "labels": [], "entities": [{"text": "display", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9728808999061584}, {"text": "size", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.966559112071991}]}, {"text": "It's very fast with all great features.", "labels": [], "entities": []}, {"text": "2. Good features for an inexpensive android.", "labels": [], "entities": []}, {"text": "The screen is big and vibrant.", "labels": [], "entities": []}, {"text": "Great speed makes smooth viewing of tv programs or sports.", "labels": [], "entities": []}, {"text": "3. The phone runs fast and smooth, and has great price.", "labels": [], "entities": []}, {"text": "In review 1, display is an explicit feature, and opinion word \"fast\" implies implicit feature speed.", "labels": [], "entities": [{"text": "display", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9068505764007568}]}, {"text": "The task is to identify both explicit and implicit features, and group them into aspects, e.g., {speed, fast, smooth}, {size, big}, {price, inexpensive}.", "labels": [], "entities": []}, {"text": "Many existing studies (;) have focused on extracting features without grouping them into aspects.", "labels": [], "entities": []}, {"text": "Some methods have been proposed to group features given that feature expressions have been identified beforehand (), or can be learned from semi-structured Pros and Cons reviews ().", "labels": [], "entities": []}, {"text": "In recent years, topic models have been widely studied for their use in aspect discovery with the advantage that they extract features and group them simultaneously.", "labels": [], "entities": [{"text": "aspect discovery", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.8690043389797211}]}, {"text": "However, researchers have found some limitations of such methods, e.g., the produced topics may not be coherent or directly interpretable as aspects), the extracted aspects are not fine-grained, and it is ineffective when dealing with aspect sparsity (.", "labels": [], "entities": []}, {"text": "In this paper, we present a clustering algorithm that extracts features and groups them into aspects from product reviews.", "labels": [], "entities": []}, {"text": "Our work differs from prior studies in three ways.", "labels": [], "entities": []}, {"text": "First, it identifies both features and aspects simultaneously.", "labels": [], "entities": []}, {"text": "Existing clusteringbased solutions () take a two-step approach that first identifies features and then employs standard clustering algorithms (e.g., k-means) to group features into aspects.", "labels": [], "entities": []}, {"text": "We propose that these two steps can be combined into a single clustering process, in which different words describing the same aspect can be automatically grouped into one cluster, and features and aspects can be identified at the same time.", "labels": [], "entities": []}, {"text": "Second, both explicit and implicit features are extracted and grouped into aspects.", "labels": [], "entities": []}, {"text": "While most existing methods deal with explicit features (e.g., \"speed\", \"size\"), much less effort has been made to identify implicit features implied by opinion words (e.g., \"fast\", \"big\"), which is challenging because many general opinion words such as \"good\" or \"great\" do not indicate product features, therefore they should not be identified as features or grouped into aspects.", "labels": [], "entities": []}, {"text": "Third, it is unsupervised and does not require seed terms, hand-crafted patterns, or any other labeling efforts.", "labels": [], "entities": []}, {"text": "Specifically, the algorithm takes a set of reviews on a product (e.g., TV, cell phone) as input and produces aspect clusters as output.", "labels": [], "entities": []}, {"text": "It first uses a part-ofspeech tagger to identify nouns/noun phrases, verbs and adjectives as candidates.", "labels": [], "entities": []}, {"text": "Instead of applying the clustering algorithm to all the candidates, only the frequent ones are clustered to generate seed clusters, and then the remaining candidates are placed into the closest seed clusters.", "labels": [], "entities": []}, {"text": "This does not only speedup the algorithm, but it also reduces the noise that might be introduced by infrequent terms in the clustering process.", "labels": [], "entities": []}, {"text": "We propose a novel domain-specific similarity measure incorporating both statistical association and semantic similarity between a pair of candidates, which recognizes features referring to the same aspects in a particular domain.", "labels": [], "entities": []}, {"text": "To further improve the quality of clusters, several problemspecific merging constraints are used to prevent the clusters referring to different aspects from being merged during the clustering process.", "labels": [], "entities": []}, {"text": "The algorithm stops when it cannot find another pair of clusters satisfying these constraints.", "labels": [], "entities": []}, {"text": "This algorithm is evaluated on reviews from three domains: cellphone, TV and GPS.", "labels": [], "entities": []}, {"text": "Its effectiveness is demonstrated through comparison with several state-of-the-art methods on both tasks of feature extraction and aspect discovery.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7832500338554382}, {"text": "aspect discovery", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.8640138804912567}]}, {"text": "Experimental results show that our method consistently yields better results than these existing methods on both tasks across all the domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the effectiveness of the proposed approach on feature extraction and aspect discovery.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8379024267196655}, {"text": "aspect discovery", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.9029777944087982}]}, {"text": "Table 1 describes the datasets from three different domains that were used in the experiments.", "labels": [], "entities": []}, {"text": "The cellphone reviews were collected from the online shop of a cellphone company, and the GPS and TV reviews were collected from Amazon.", "labels": [], "entities": []}, {"text": "Three human annotators manually annotate the datasets to create gold standards of features and aspects.", "labels": [], "entities": []}, {"text": "These annotators first identify feature expressions from reviews independently.", "labels": [], "entities": []}, {"text": "The expressions that were agreed by at least two annotators were selected as features.", "labels": [], "entities": []}, {"text": "Then the authors manually specified a set of aspects based on these features, and asked three annotators to label each feature with an aspect category.", "labels": [], "entities": []}, {"text": "The average inter-annotator agreement on aspect annotation was \u03ba = 0.687 (stddev = 0.154) according to Cohen's Kappa statistic.", "labels": [], "entities": []}, {"text": "To obtain the gold standard annotation of aspects, the annotators discussed to reach an agreement when there was a disagreement on the aspect category of a feature.", "labels": [], "entities": []}, {"text": "We are making the datasets and annotations publicly available . shows the number of reviews, aspects, unique explicit/implicit features manually identified by annotators, and candidates of explicit (i.e., noun and noun phrase) and implicit (i.e., adjective and verb) features extracted from the datasets in three domains.", "labels": [], "entities": []}, {"text": "We use \"CAFE\" (Clustering for Aspect and Feature Extraction) to denote the proposed method.", "labels": [], "entities": []}, {"text": "We assume the number of aspects k is specified by the users, and set k = 50 throughout all the experiments.", "labels": [], "entities": []}, {"text": "We use s = 500, \u03b4 = 0.8, w g = wt = 0.2, w gt = 0.6 as the default setting of CAFE, and study the effect of parameters in Section \"Influence of Parameters\".", "labels": [], "entities": []}, {"text": "In addition, we evaluate each individual similarity metric -\"CAFE-g\", \"CAFE-t\" and \"CAFE-gt\" denote the variations of \"CAFE\" that use sim g (x i , x j ), sim t (x i , x j ), and sim gt (x i , x j ) as the similarity measure, respectively.", "labels": [], "entities": []}, {"text": "We empirically set \u03b4 = 0.4 for \"CAFE-g\", \u03b4 = 0.84 for \"CAFE-t\" and \u03b4 = 0.88 for \"CAFE-gt\".", "labels": [], "entities": []}, {"text": "We compared CAFE against the following two stateof-the-art methods on feature extraction: \u2022 PROP: A double propagation approach () that extracts features using handcrafted rules based on dependency relations between features and opinion words.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7273741215467453}]}, {"text": "\u2022 LRTBOOT: A bootstrapping approach) that extracts features by mining pairwise feature-feature, feature-opinion, opinionopinion associations between terms in the corpus, where the association is measured by the likelihood ratio tests (LRT   quency and manually selected the top 10 genuine features as seeds for them.", "labels": [], "entities": []}, {"text": "According to the study (, the performance for LRTBOOT remained almost constant when increasing the seeds from 1 to 50.", "labels": [], "entities": [{"text": "LRTBOOT", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.5503930449485779}]}, {"text": "Three association thresholds need to be specified for LRTBOOT.", "labels": [], "entities": []}, {"text": "Following the original study in which the experiments were conducted on cell-phone reviews, we set ff th = 21.0, ooth = 12.0, and performed grid search for the value off oth.", "labels": [], "entities": []}, {"text": "The best results were achieved at f oth = 9.0 for cell-phone reviews, and at f oth = 12.0 for GPS and TV reviews.", "labels": [], "entities": [{"text": "f oth", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.6868928968906403}]}, {"text": "precision+recall , where N result and N gold are the number of features in the result and the gold standard, respectively, and N agree is the number of features that are agreed by both sides.", "labels": [], "entities": [{"text": "precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9952180981636047}, {"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9640552401542664}]}, {"text": "Because PROP and LRTBOOT extract only explicit features, the evaluation was conducted on the quality of explicit features.", "labels": [], "entities": []}, {"text": "The performance of identifying implicit features will be examined by evaluation on aspect discovery, because implicit features have to be merged into aspects to be detected.", "labels": [], "entities": [{"text": "aspect discovery", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.821457177400589}]}, {"text": "shows the best results (in terms of Fscore) of feature extraction by different methods.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9986528158187866}, {"text": "feature extraction", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7459406852722168}]}, {"text": "Both PROP and LRTBOOT obtain high recall and relatively low precision.", "labels": [], "entities": [{"text": "PROP", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.7425356507301331}, {"text": "LRTBOOT", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9694421291351318}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996898174285889}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9985326528549194}]}, {"text": "CAFE greatly improves precision, with a relatively small loss of recall, resulting in 21.68% and 9.36% improvement in macro-averaged F-score over PROP and LRTBOOT, respectively.", "labels": [], "entities": [{"text": "CAFE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4725968837738037}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996193647384644}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9991281628608704}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9299356341362}, {"text": "PROP", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.8344428539276123}, {"text": "LRTBOOT", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.93343585729599}]}, {"text": "We also plot precision-recall curves at various parameter settings for CAFE and LRT-BOOT in.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9973098039627075}, {"text": "CAFE", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.6380336284637451}, {"text": "LRT-BOOT", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8167924284934998}]}, {"text": "For CAFE, we kept s = 500, w g = wt = 0.2, w gt = 0.6, and increased \u03b4 from 0.64 to 0.96.", "labels": [], "entities": [{"text": "CAFE", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6361482739448547}, {"text": "\u03b4", "start_pos": 69, "end_pos": 70, "type": "METRIC", "confidence": 0.9801300764083862}]}, {"text": "For LRTBOOT, we kept ff th = 21.0, ooth = 12.0, and increased f oth from 6.0 to 30.0.", "labels": [], "entities": [{"text": "LRTBOOT", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.774419903755188}]}, {"text": "For PROP, only one precision-recall point was obtained.", "labels": [], "entities": [{"text": "PROP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7744578123092651}, {"text": "precision-recall", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9978704452514648}]}, {"text": "From, we see that the curve of CAFE lies well above those of LRTBOOT and PROP across three datasets.", "labels": [], "entities": [{"text": "CAFE", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7762951254844666}, {"text": "LRTBOOT", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9403461813926697}, {"text": "PROP", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.8117547035217285}]}, {"text": "Though LRTBOOT achieved similar precision as CAFE did at the recall rate of approximately 0.37 for GPS reviews and at the recall rate of approximately 0.49 for TV reviews, it performed worse than CAFE at increasing recall levels for both datasets.", "labels": [], "entities": [{"text": "LRTBOOT", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.8996868133544922}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994899034500122}, {"text": "recall rate", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9875257015228271}, {"text": "recall rate", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9861946702003479}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9988001585006714}]}, {"text": "The key difference between CAFE and the baselines is that CAFE groups terms into clusters and identifies the terms in the selected aspect clusters as features, while both baselines enlarge a feature seed set by mining syntactical or statistical associations between features and opinion words.", "labels": [], "entities": []}, {"text": "The results suggest that features can be more precisely identified via aspect clustering.", "labels": [], "entities": [{"text": "aspect clustering", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7201906591653824}]}, {"text": "Generally, CAFE is superior to its variations, and CAFE-g outperforms CAFE-gt and CAFE-t.", "labels": [], "entities": [{"text": "CAFE", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.6553859114646912}]}, {"text": "For comparison with CAFE on aspect discovery, we implemented the following three methods: \u2022 MuReinf: A clustering method () that utilizes the mutual reinforcement as-sociation between features and opinion words to iteratively group them into clusters.", "labels": [], "entities": [{"text": "aspect discovery", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.900490790605545}]}, {"text": "Similar to the proposed method, it is unsupervised, clustering-based, and handling implicit features.", "labels": [], "entities": []}, {"text": "\u2022 L-EM: A semi-supervised learning method) that adapts the Naive Bayesian-based EM algorithm to group synonym features into categories.", "labels": [], "entities": []}, {"text": "Because semisupervised learning needs some labeled examples, the proposed method first automatically generates some labeled examples (i.e., the groups of synonym feature expressions) based on features sharing common words and lexical similarity.", "labels": [], "entities": []}, {"text": "\u2022 L-LDA: A baseline method that is based on LDA.", "labels": [], "entities": []}, {"text": "The same labeled examples generated by L-EM are used as seeds for each topic in topic modeling.", "labels": [], "entities": []}, {"text": "These three methods require features to be extracted beforehand, and focus on grouping features into aspects.", "labels": [], "entities": []}, {"text": "Both LRTBOOT and CAFE are used to provide the input features to them.", "labels": [], "entities": [{"text": "LRTBOOT", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.7616697549819946}, {"text": "CAFE", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.806845486164093}]}, {"text": "We set \u03b1 = 0.6 for MuReinf, because their study ( showed that the method achieved best results at \u03b1 > 0.5.", "labels": [], "entities": []}, {"text": "All three methods utilize dictionary-based semantic similarity to some extent.", "labels": [], "entities": []}, {"text": "Since CAFE uses the UMBC Semantic Similarity Service, we use the same service to provide the semantic similarity for all the methods.", "labels": [], "entities": []}, {"text": "The results were evaluated using Rand Index, a standard measure of the similarity between the clustering results and a gold standard.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.8367584049701691}]}, {"text": "Given a set of n objects and two partitions of them, the Rand Index is defined as n\u00d7(n\u22121) . The idea is that the agreements/disagreements between two partitions are checked on n \u00d7 (n \u2212 1) pairs of objects.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.6251890063285828}]}, {"text": "Among all the pairs, there area pairs belonging to the same cluster in both partitions, and b pairs belonging to different clusters in both partitions.", "labels": [], "entities": []}, {"text": "In this study, the gold standard and the aspect clusters may not share the exact same set of features due to the noise in feature extraction, therefore we consider n the number of expressions in the union of two sets.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7111104130744934}]}, {"text": "shows the Rand Index achieved by different methods.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9479881525039673}]}, {"text": "Among the methods that generate partitions of the same features provided by CAFE, CAFE achieves the best macro-averaged Rand Index, followed by CAFE + MuReinf, CAFE + L-LDA, and CAFE + L-EM.", "labels": [], "entities": [{"text": "macro-averaged Rand Index", "start_pos": 105, "end_pos": 130, "type": "METRIC", "confidence": 0.7073947191238403}]}, {"text": "CAFE outperforms the variations using the single similarity metric, i.e., CAFE-g, CAFE-t and CAFE-gt.", "labels": [], "entities": []}, {"text": "The results imply the effectiveness of our domain-specific similarity measure in identifying synonym features in a particular domain.", "labels": [], "entities": []}, {"text": "Using the input features from LRT-BOOT, the performance of MuReinf, L-EM and L-LDA decrease on all three domains, compared with using the input features from CAFE.", "labels": [], "entities": []}, {"text": "The decrease is more significant for L-EM and L-LDA than for MuReinf, which suggest that the semi-supervised methods L-EM and L-LDA are more dependent on the quality of input features.", "labels": [], "entities": []}, {"text": "illustrates a sample of the discovered aspects and features by CAFE.", "labels": [], "entities": [{"text": "CAFE", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9217275381088257}]}, {"text": "The algorithm identifies the important aspects in general sense as well as the important aspects that are not so obvious thus could be easily missed by human judges, e.g., suction cup for GPS and glare for TV.", "labels": [], "entities": []}, {"text": "In addition, both explicit and implicit features are identified and grouped into the aspects, e.g., expensive and price, big and size, sensitive and signal, etc.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets and gold standards.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results of feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7564423978328705}]}, {"text": " Table 3: Rand Index of aspect identification.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8755921423435211}, {"text": "aspect identification", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.818678230047226}]}, {"text": " Table 4: Examples of discovered aspects and features by the proposed approach CAFE. Explicit and implicit features are denoted", "labels": [], "entities": []}]}