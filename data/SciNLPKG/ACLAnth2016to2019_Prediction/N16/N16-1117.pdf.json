{"title": [{"text": "Embedding Lexical Features via Low-Rank Tensors", "labels": [], "entities": [{"text": "Low-Rank Tensors", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7122044861316681}]}], "abstractContent": [{"text": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features.", "labels": [], "entities": []}, {"text": "Such combination results in large numbers of features, which can lead to over-fitting.", "labels": [], "entities": []}, {"text": "We present anew model that represents complex lexical features-comprised of parts for words, contextual information and labels-in a tensor that captures conjunction information among these parts.", "labels": [], "entities": []}, {"text": "We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed.", "labels": [], "entities": []}, {"text": "Furthermore , we investigate two methods for handling features that include n-grams of mixed lengths.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7916387319564819}, {"text": "preposition disambiguation", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6968951225280762}]}], "introductionContent": [{"text": "Statistical NLP models usually rely on handdesigned features, customized for each task.", "labels": [], "entities": []}, {"text": "These features typically combine lexical and contextual information with the label to be scored.", "labels": [], "entities": []}, {"text": "In relation extraction, for example, there is a parameter for the presence of a specific relation occurring with a feature conjoining a word type (lexical) with dependency path information (contextual).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8718739449977875}]}, {"text": "In measuring phrase semantic similarity, a word type is conjoined with its position in the phrase to signal its role.", "labels": [], "entities": [{"text": "measuring phrase semantic similarity", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.6488426625728607}]}, {"text": "shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.823332816362381}]}, {"text": "* Paper submitted during Mo Yu's PhD study at HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.7286256551742554}]}, {"text": "To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings () help improve NER, dependency parsing and semantic role labeling).", "labels": [], "entities": [{"text": "NER", "start_pos": 247, "end_pos": 250, "type": "TASK", "confidence": 0.9390122890472412}, {"text": "dependency parsing", "start_pos": 252, "end_pos": 270, "type": "TASK", "confidence": 0.7904011607170105}, {"text": "semantic role labeling", "start_pos": 275, "end_pos": 297, "type": "TASK", "confidence": 0.6272757252057394}]}, {"text": "However, using only word embeddings is not sufficient to represent complex lexical features (e.g. \u03c6 in).", "labels": [], "entities": []}, {"text": "In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration.", "labels": [], "entities": []}, {"text": "Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.8556981384754181}]}, {"text": "To address the above problems, we propose a general and unified approach to reduce the feature space by constructing low-dimensional feature representations, which provides anew way of combining word embeddings, traditional non-lexical properties, and label information.", "labels": [], "entities": []}, {"text": "Our model exploits the inner structure of features by breaking the feature into multiple parts: lexical, non-lexical and (optional) label.", "labels": [], "entities": []}, {"text": "We demonstrate that the full feature is an outer product among these parts.", "labels": [], "entities": []}, {"text": "Thus, a parameter tensor scores each feature to produce a prediction.", "labels": [], "entities": []}, {"text": "Our model then reduces the number of param- between \"see\" and \"with\" in (a), we may rely on lexical features in (b).", "labels": [], "entities": []}, {"text": "Here p, c, g are indices of the word \"with\", its child (\"telescope\") and a candidate head.", "labels": [], "entities": []}, {"text": "shows what the fifth feature (\u03c6) is like, when the candidate is \"see\".", "labels": [], "entities": []}, {"text": "As is common in multi-class classification tasks, each template generates a different feature for each label y.", "labels": [], "entities": [{"text": "multi-class classification tasks", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.7485068341096243}]}, {"text": "Thus a feature \u03c6 = w g \u2227 w c \u2227 u \u2227 y is the conjunction of the four parts. is the one-hot representation of \u03c6, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors.", "labels": [], "entities": []}, {"text": "v(x) = 1 means the vector v has a single non-zero element in the x position.", "labels": [], "entities": []}, {"text": "eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of  but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP).", "labels": [], "entities": []}, {"text": "Our models use fewer parameters than previous work that learns a separate representation for each feature.", "labels": [], "entities": []}, {"text": "CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both.", "labels": [], "entities": [{"text": "CP approximation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8443328142166138}, {"text": "prediction", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.9595084190368652}]}, {"text": "Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths.", "labels": [], "entities": []}, {"text": "Our model makes the following contributions when contrasted with prior work:  applied CP to combine different views of features.", "labels": [], "entities": []}, {"text": "Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified features, while  generates new features (usually different from ours) by combining some \"atom\" features.", "labels": [], "entities": []}, {"text": "Thus their work may ignore some useful features; it relies on binary features as supplementary but our model needs not.", "labels": [], "entities": []}, {"text": "(2) 's factorization relies on views with explicit meanings, e.g. head/modifier/arc in dependency parsing, making it less general.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7301898002624512}]}, {"text": "Therefore its applications to tasks like relation extraction are less obvious.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9207100570201874}]}, {"text": "Compared to our previous work , this work allows for higherorder interactions, mixed-length n-gram features, lower-rank representations.", "labels": [], "entities": []}, {"text": "We also demonstrate the strength of our new model via applications to new tasks.", "labels": [], "entities": []}, {"text": "The resulting method learns smoothed feature representations combining lexical, non-lexical and label information, achieving state-of-the-art performance on several tasks: relation extraction, preposition semantics and PP-attachment.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.8346534669399261}]}], "datasetContent": [{"text": "We evaluate LRFR on three tasks: relation extraction, PP attachment and preposition disambiguation (see fora task summary).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8492697477340698}, {"text": "PP attachment", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8424769341945648}, {"text": "preposition disambiguation", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6809948235750198}]}, {"text": "We include detailed feature templates in PP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8407354652881622}, {"text": "English data sets", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.8041683038075765}]}, {"text": "The preposition disambiguation task was designed for compositional semantics, which is an important application of deep learning and distributed representations.", "labels": [], "entities": [{"text": "preposition disambiguation task", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8344105283419291}]}, {"text": "On all these tasks, we compare to the state-of-the-art.", "labels": [], "entities": []}, {"text": "We use the same word embeddings in on PP-attachment fora fair comparison.", "labels": [], "entities": []}, {"text": "For the other experiments, we use the same 200-d word embeddings in .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of each task. PP-attachment and preposition disambiguation have both unigram and bigram fea-", "labels": [], "entities": []}, {"text": " Table 3: Results on test for relation extraction. Y(es)/N(o) indicates whether embeddings are updated during training.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.854670912027359}, {"text": "Y(es)/N(o)", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9286632314324379}]}, {"text": " Table 4: PP-attachment test accuracy. The baseline results are from Belinkov et al. (2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9945340156555176}]}, {"text": " Table 5: Accuracy for spatial classification of PPs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9946961402893066}, {"text": "spatial classification of PPs", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7580255717039108}]}]}