{"title": [{"text": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "labels": [], "entities": [{"text": "Improve Chinese Word Embeddings", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8424321264028549}]}], "abstractContent": [{"text": "Recently, researchers have demonstrated that both Chinese word and its component characters provide rich semantic information when learning Chinese word embeddings.", "labels": [], "entities": []}, {"text": "However , they ignored the semantic similarity across component characters in a word.", "labels": [], "entities": []}, {"text": "In this paper, we learn the semantic contribution of characters to a word by exploiting the similarity between a word and its component characters with the semantic knowledge obtained from other languages.", "labels": [], "entities": []}, {"text": "We propose a similarity-based method to learn Chinese word and character embeddings jointly.", "labels": [], "entities": []}, {"text": "This method is also capable of disambiguating Chinese characters and distinguishing non-compositional Chi-nese words.", "labels": [], "entities": []}, {"text": "Experiments on word similarity and text classification demonstrate the effectiveness of our method.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8026709854602814}, {"text": "text classification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.763465404510498}]}], "introductionContent": [{"text": "Distributed representations of knowledge has received wide attention in recent years.", "labels": [], "entities": [{"text": "Distributed representations of knowledge", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9083149880170822}]}, {"text": "Researchers have proposed various models to learn it at different granularity levels.", "labels": [], "entities": []}, {"text": "Distributed word representations, also known as word embeddings, were learned in ().", "labels": [], "entities": []}, {"text": "Larger granularity levels than words have also been investigated, including phrase level (, sentence level (, and * Corresponding author document level ().", "labels": [], "entities": []}, {"text": "For language like Chinese, some smaller units than word also provide rich semantic information.", "labels": [], "entities": []}, {"text": "For example, Chinese characters in word, Chinese radicals in character.", "labels": [], "entities": []}, {"text": "These internal structures have been proved to be useful for Chinese word and character embeddings.", "labels": [], "entities": []}, {"text": "took Chinese characters in a word into account when modeling the semantic meaning of the word.", "labels": [], "entities": []}, {"text": "They proposed a character-enhanced word embeddings model (CWE) by adding the embedding of component characters in a word with the same weight to the word embedding.", "labels": [], "entities": []}, {"text": "However, the internal characters in a Chinese word have different semantic contributions to its meaning.", "labels": [], "entities": []}, {"text": "Take Chinese word \"\" \" (frog) as an example.", "labels": [], "entities": []}, {"text": "The character \"\"\" (blue or green) is to decorate character \"\" (frog).", "labels": [], "entities": []}, {"text": "It is obvious that the latter character contributes more than the former one to the word meaning.", "labels": [], "entities": []}, {"text": "In, they proposed a component-enhanced Chinese character embeddings model based on the feature that most Chinese characters are phono-semantic compounds.", "labels": [], "entities": []}, {"text": "They considered characters and bi-characters as the basic embedding units.", "labels": [], "entities": []}, {"text": "However, some bi-characters are meaningless, and may not form a Chinese word.", "labels": [], "entities": []}, {"text": "These bi-characters may undermine embeddings of others.", "labels": [], "entities": []}, {"text": "This paper, motivated by, exploits the internal structures of Chinese word, namely the Chinese characters.", "labels": [], "entities": []}, {"text": "We propose a method to calculate the semantic contribution of characters to a word in a cross-lingual manner.", "labels": [], "entities": []}, {"text": "The basic idea is that the semantic contribution of Chinese characters inmost Chinese words can be learned from their translations in other languages.", "labels": [], "entities": []}, {"text": "Such as the word \"\" \" we mentioned above.", "labels": [], "entities": []}, {"text": "The word embeddings of other languages are used to calculate semantic contribution of characters to the word they compose.", "labels": [], "entities": []}, {"text": "Moreover, Chinese characters are more ambiguous than words.", "labels": [], "entities": []}, {"text": "To tackle this problem, multiple-prototype character embeddings is proposed.", "labels": [], "entities": []}, {"text": "Different meanings of characters will be represented by different embeddings.", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "We provide a method to calculate the semantic contribution of Chinese characters to the word they compose with English translation.", "labels": [], "entities": []}, {"text": "Compared with English, there are fewer human-made resources to supervise the learning process of Chinese word and character embeddings.", "labels": [], "entities": []}, {"text": "While translation resources are always easy to be accessed on the Internet.", "labels": [], "entities": []}, {"text": "2. We propose a novel way to disambiguate Chinese characters with translating resources.", "labels": [], "entities": []}, {"text": "There are some limitations in existing cluster-based algorithms (.", "labels": [], "entities": []}, {"text": "They either fixed the number of clusters or proposed a nonparametric way to learn it for each word.", "labels": [], "entities": []}, {"text": "However, the number of clusters for words varies a lot.", "labels": [], "entities": []}, {"text": "For nonparametric method, different hyperparameters have to be tune to control the number of clusters for different datasets.", "labels": [], "entities": []}, {"text": "3. We provide a method to distinguish whether a Chinese word is semantically compositional automatically.", "labels": [], "entities": []}, {"text": "Not all Chinese words exhibit semantic compositions from their component characters.", "labels": [], "entities": []}, {"text": "For example, entity names, transliterated words like \" \u00e2 u\" (sofa), single-morpheme multi-character words like \"\u00fe}\" (wander).", "labels": [], "entities": []}, {"text": "In, they performed part-of-speech tagging to identify entity names.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7228637337684631}]}, {"text": "The transliterated words are tagged manually, which requires human work and need to be updated when new words are created.", "labels": [], "entities": []}, {"text": "The evaluations on word similarity, text classification, Chinese characters disambiguation, and qualitative analysis of word embeddings demonstrate the effectiveness of our method.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7424390912055969}, {"text": "text classification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7723598778247833}, {"text": "Chinese characters disambiguation", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6450341840585073}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Evaluation on wordsim-240 and wordsim-296", "labels": [], "entities": []}, {"text": " Table 4: 2 groups datasets of text classification, the first col-", "labels": [], "entities": [{"text": "text classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7407387048006058}]}, {"text": " Table 5: Evaluation accuracies (%) on text classification.", "labels": [], "entities": [{"text": "Evaluation accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8108615875244141}, {"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7676889896392822}]}, {"text": " Table 9: Precision, recall, F-score of transliterated words when", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9905704259872437}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9976515173912048}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9965440630912781}]}, {"text": " Table 10: Evaluation accuracies (%) on ambiguous characters", "labels": [], "entities": [{"text": "Evaluation accuracies", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.7786898016929626}]}]}