{"title": [{"text": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "labels": [], "entities": [{"text": "Incorporating Structural Alignment Biases", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7729305103421211}, {"text": "Attentional Neural Translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7326154510180155}]}], "abstractContent": [{"text": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7842437326908112}]}, {"text": "However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models.", "labels": [], "entities": []}, {"text": "In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions.", "labels": [], "entities": [{"text": "attentional neural translation", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6234323084354401}]}, {"text": "We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7208625227212906}, {"text": "statistical machine translation", "start_pos": 191, "end_pos": 222, "type": "TASK", "confidence": 0.7379725972811381}]}, {"text": "This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem.", "labels": [], "entities": []}, {"text": "Broadly, neural approaches are based around the notion of an encoder-decoder (, in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation.", "labels": [], "entities": []}, {"text": "We focus on the attentional model of translation () which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence.", "labels": [], "entities": []}, {"text": "The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (.", "labels": [], "entities": [{"text": "word-based machine translation", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.6664779384930929}]}, {"text": "In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured.", "labels": [], "entities": [{"text": "word based translation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6416111687819163}]}, {"text": "We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics or joint modelling (;).", "labels": [], "entities": []}, {"text": "We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs.", "labels": [], "entities": [{"text": "low resource translation", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.7917581597963969}]}, {"text": "Our results demonstrate consistent improvements over vanilla encoder- decoder and attentional model in terms of the perplexity and BLEU score, e.g. up to 3.5 BLEU points when re-ranking the candidate translations generated by a state-of-the-art phrase based model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9877671897411346}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9985877275466919}]}], "datasetContent": [{"text": "We conducted our experiments with four language pairs, translating between English \u2194 Romanian, Estonian, Russian and Chinese.", "labels": [], "entities": []}, {"text": "These languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian).", "labels": [], "entities": []}, {"text": "We focus on a (simulated) low resource setting, where only a limited amount of training data is available.", "labels": [], "entities": []}, {"text": "This serves to demonstrate the robustness and generalisation of our model on sparse data -something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting.", "labels": [], "entities": []}, {"text": "shows the statistics of the training sets.", "labels": [], "entities": []}, {"text": "6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016.", "labels": [], "entities": [{"text": "BTEC corpus", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9873175024986267}]}, {"text": "We used 'devset1 2' and 'devset 3' as the development and test sets, respectively, and in both cases used only the first reference for evaluation.", "labels": [], "entities": []}, {"text": "For Romanian and Estonian, the data come from the Europarl corpus (, where we used 100K sentence pairs for training, and 3K for development and 2K for testing.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9931743144989014}]}, {"text": "The RussianEnglish data was taken from a web derived corpus ().", "labels": [], "entities": [{"text": "RussianEnglish data", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.976495087146759}]}, {"text": "The dataset is split into three parts using the same technique as for the Europarl sets.", "labels": [], "entities": [{"text": "Europarl sets", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9854936599731445}]}, {"text": "During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words.", "labels": [], "entities": []}, {"text": "For the Europarl data, we also removed sentences containing headings and other meeting formalities.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.994026780128479}]}, {"text": "We have implemented our neural translation model with linguistic features in C++ using the CNN library.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.741338461637497}, {"text": "CNN library", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9318240284919739}]}, {"text": "We compared our proposed model against our implementations of the attentional model ( and encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses () with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.6139924451708794}]}, {"text": "We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora.", "labels": [], "entities": [{"text": "KenLM (Heafield, 2011)", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.7424008945624033}]}, {"text": "Following previous work, we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU () measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9985355138778687}]}, {"text": "We applied bootstrap resampling) to measure statistical significance, p < 0.05, of our models compared to a baseline.", "labels": [], "entities": []}, {"text": "For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model.", "labels": [], "entities": []}, {"text": "We trained the re-ranking models using MERT on development sets with 100-best translations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.5925499200820923}]}, {"text": "The perplexity results of the neural models for the two translation directions across the four language pairs are presented in.", "labels": [], "entities": []}, {"text": "In all cases, our work achieves lower perplexities compared to the vanilla attentional model and the encoder-decoder architecture, owing to the linguistic constraints.", "labels": [], "entities": []}, {"text": "We also obtained similar patterns of improvements when decoding, using a greedy decoding strategy, as shown in.", "labels": [], "entities": []}, {"text": "The exception was for en\u2192ru, where the addition of the global fertility (in addition to the other aligment features) was detrimental, resulting in a decrease in BLEU score (5.94\u21925.26).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9840628504753113}]}, {"text": "This maybe due to highly noisy nature of the web text corpus of Russian-English language pair, compared to the much cleaner sources for the other language pairs.", "labels": [], "entities": []}, {"text": "Greedy decoding does not appear to be competitive for neural models trained on small parallel corpora, not reaching the level of a phrase-based baseline (see).", "labels": [], "entities": []}, {"text": "Despite this, however, these models still provide substantial gains when used for reranking (as shown in) for translating into English from the other four languages.", "labels": [], "entities": []}, {"text": "We compare reranking settings using the log probabilities produced by our model as additional features 10 vs. using log probabilities from the vanilla attentional model and the encoder-decoder.", "labels": [], "entities": []}, {"text": "The re-rankers based on our model are significantly better than the rest for Chinese and Estonian, and on par with the other for Russian and Romanian\u2192English.", "labels": [], "entities": []}, {"text": "In all cases our model has performance at least 1 BLEU point better than the baseline phrase-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9994528889656067}]}, {"text": "It is worth not- We include two features: the normalised log-probability of the translation, evaluated in both translation directions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the training sets, showing in each cell the", "labels": [], "entities": []}, {"text": " Table 2: Perplexity results for attentional model variants eval-", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9362859129905701}]}, {"text": " Table 3: Perplexity on the test sets for the two translation di-", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9646115303039551}]}, {"text": " Table 4: BLEU scores on the test sets for the two translation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995430707931519}]}, {"text": " Table 5: BLEU scores on the test sets for re-ranking. bold:", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985625147819519}]}]}