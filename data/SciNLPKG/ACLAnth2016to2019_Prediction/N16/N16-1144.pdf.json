{"title": [{"text": "Consensus Maximization Fusion of Probabilistic Information Extractors", "labels": [], "entities": [{"text": "Consensus Maximization Fusion of Probabilistic Information Extractors", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.7982391204152789}]}], "abstractContent": [{"text": "Current approaches to Information Extraction (IE) are capable of extracting large amounts of facts with associated probabilities.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8583074390888215}]}, {"text": "Because no current IE system is perfect, complementary and conflicting facts are obtained when different systems are run over the same data.", "labels": [], "entities": [{"text": "IE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9668043255805969}]}, {"text": "Knowledge Fusion (KF) is the problem of ag-gregating facts from different extractors.", "labels": [], "entities": [{"text": "Knowledge Fusion (KF)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8186587870121003}]}, {"text": "Existing methods approach KF using supervised learning or deep linguistic knowledge, which either lack sufficient data or are not robust enough.", "labels": [], "entities": [{"text": "KF", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9635536074638367}]}, {"text": "We propose a semi-supervised application of Consensus Maximization to the KF problem, using a combination of supervised and unsupervised models.", "labels": [], "entities": [{"text": "Consensus Maximization", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8421584963798523}]}, {"text": "Consensus Maxi-mization Fusion (CM Fusion) is able to promote high quality facts and eliminate incorrect ones.", "labels": [], "entities": [{"text": "Consensus Maxi-mization Fusion (CM Fusion", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7171523670355479}]}, {"text": "We demonstrate the effectiveness of our system on the NIST Slot Filler Validation contest, which seeks to evaluate and aggregate multiple independent information ex-tractors.", "labels": [], "entities": [{"text": "NIST Slot Filler Validation contest", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.7682804763317108}]}, {"text": "Our system achieved the highest F1 score relative to other system submissions.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9859161376953125}]}], "introductionContent": [{"text": "The abundance of unstructured text on the web such as news, discussion forums, wiki pages, etc. has increased the interest of the research community in the automatic extraction of information at scale.", "labels": [], "entities": [{"text": "automatic extraction of information", "start_pos": 156, "end_pos": 191, "type": "TASK", "confidence": 0.7695887759327888}]}, {"text": "Information extractors can be used to constructor expand Knowledge Bases (KBs) through a process known as Knowledge Base Population (KBP) or Construction.", "labels": [], "entities": [{"text": "Information extractors", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6903521418571472}]}, {"text": "Facts in a KB are typically modeled as (subject, relation, object) triples such ascity of headquarters, Menlo Park).", "labels": [], "entities": []}, {"text": "No current information extractor is perfectly accurate and different models exhibit different strengths and weaknesses.", "labels": [], "entities": [{"text": "information extractor", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.7066050469875336}]}, {"text": "As a result, many stateof-the-art KBs in academia and industry employ multiple complementary information extractors for KBP.", "labels": [], "entities": []}, {"text": "NELL) employs rules, statistically-learned pattern extractors, and context extractors among others.) uses heuristic extractors at text and ontological levels and Google has multiple extractors crawling text, tables, and HTML.", "labels": [], "entities": []}, {"text": "Different extraction systems may also agree or disagree on the information they extract.", "labels": [], "entities": []}, {"text": "Consider three systems that extract the facts (Facebook, org:city of headquarters, Menlo Park), (Facebook, org:city of headquarters, Palo Alto), and (Facebook, org:city of headquarters, Menlo Park) with probabilities 0.6, 0.3, and 0.5 respectively.", "labels": [], "entities": []}, {"text": "The Palo Alto extraction is erroneous and should be removed.", "labels": [], "entities": [{"text": "Palo Alto extraction", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8671923875808716}]}, {"text": "The two Menlo Park extractions should be promoted by agreement and have their confidences increased.", "labels": [], "entities": []}, {"text": "The aggregation of facts from multiple extractors into a single probabilistic triple is known as Knowledge Fusion (KF) () and can be modeled as an ensemble learning problem.", "labels": [], "entities": []}, {"text": "Previous ensemble approaches at the output layer divide into unsupervised and supervised methods (.", "labels": [], "entities": []}, {"text": "Unsupervised methods establish a consensus or majority vote among extractors without distinguishing the merit of each, but perform poorly if all the extractors are weak.", "labels": [], "entities": []}, {"text": "Supervised methods such as stacking achieve better performance by learning weights for each extractor and combining them as a weighted sum.", "labels": [], "entities": [{"text": "stacking", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.9820238947868347}]}, {"text": "The difficulty in obtaining training data usually results in high precision, but low recall among all facts.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9993454813957214}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9990004897117615}]}, {"text": "As a solution to the low recall problem we present a probabilistic ensemble fusion model based on Consensus Maximization (CM) (, which is a semi-supervised learning method able to combine the strengths of both supervised and unsupervised approaches.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9853120446205139}, {"text": "Consensus Maximization (CM)", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6958446979522706}]}, {"text": "In the knowledge fusion domain, where the number of unsupervised systems can be rather large compared to those with manually labeled data, Consensus Maximization Fusion is able to leverage both for improved performance.", "labels": [], "entities": [{"text": "knowledge fusion", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.6979343891143799}, {"text": "Consensus Maximization Fusion", "start_pos": 139, "end_pos": 168, "type": "TASK", "confidence": 0.8475432991981506}]}, {"text": "We apply our CM Fusion approach to the NIST Slot Filling Validation (SFV) task, an ensemble learning problem that aims to combine multiple information extractors participating in the NIST English Slot Filling (ESF) task.", "labels": [], "entities": [{"text": "CM Fusion", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.8152298033237457}, {"text": "NIST Slot Filling Validation (SFV) task", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.8026359602808952}, {"text": "NIST English Slot Filling (ESF) task", "start_pos": 183, "end_pos": 219, "type": "TASK", "confidence": 0.7694594711065292}]}, {"text": "Our experiments show an improved F1 score relative to the current state-ofthe-art SFV systems.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9822439253330231}]}, {"text": "We make the following overall contributions in this paper: \u2022 Present a novel probabilistic fusion system that incorporates Consensus Maximization to solve the Knowledge Fusion problem.", "labels": [], "entities": [{"text": "Consensus Maximization", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.6748179793357849}, {"text": "Knowledge Fusion problem", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.7690818806489309}]}, {"text": "\u2022 Develop an application of our system to the NIST Slot Filling Validation task.", "labels": [], "entities": [{"text": "NIST Slot Filling Validation task", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.787350732088089}]}, {"text": "\u2022 Outline an evaluation of our system that improves upon the previous state-of-the-art F1 score by 28%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9790311455726624}]}, {"text": "Though we choose to focus on the SFV task in this paper, there are clear applications beyond this and to the Knowledge Fusion problem in general.", "labels": [], "entities": [{"text": "SFV task", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.8432418704032898}, {"text": "Knowledge Fusion problem", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.8318007191022238}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss background material on the ESF and SFV tasks as well as the Consensus Maximization algorithm.", "labels": [], "entities": [{"text": "ESF", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7234370708465576}, {"text": "Consensus Maximization", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7747693359851837}]}, {"text": "Section 3 outlines our CM Fusion system and how it maps into the knowledge fusion problem.", "labels": [], "entities": [{"text": "CM Fusion", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.8339983224868774}, {"text": "knowledge fusion", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7652312219142914}]}, {"text": "Our experiments are detailed in Section 4 and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system using a collection of queries supplied by TAC-KBP SFV.", "labels": [], "entities": [{"text": "TAC-KBP SFV", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9411162734031677}]}, {"text": "In this section we more fully describe the dataset and our experiment methodology.", "labels": [], "entities": []}, {"text": "We compare to the current state-ofthe-art ESF and SFV systems and show an improvement in F1 score.", "labels": [], "entities": [{"text": "ESF", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9372127652168274}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.972452312707901}]}, {"text": "Finally, we provide an analysis of our results.", "labels": [], "entities": []}, {"text": "Three datasets were used for training and testing spread across the three years that the English Slot Filling task has been run.", "labels": [], "entities": [{"text": "English Slot Filling task", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7989842444658279}]}, {"text": "Each team in the competition submits multiple models vying for the highest score on unseen training data.", "labels": [], "entities": []}, {"text": "Each submission is viewed as a different system in our ensemble.", "labels": [], "entities": []}, {"text": "2014 contains 65 and 2015 contains 69.", "labels": [], "entities": []}, {"text": "In 2013 and 2014 we use only ESF data, but 2015 adds Cold Start Knowledge Base (CSKB) data.", "labels": [], "entities": [{"text": "ESF data", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.840190201997757}, {"text": "Cold Start Knowledge Base (CSKB) data", "start_pos": 53, "end_pos": 90, "type": "DATASET", "confidence": 0.9096795171499252}]}, {"text": "The total number of labeled queries in both 2013 and 2014 were 100 each.", "labels": [], "entities": []}, {"text": "2015 had 9340 unlabeled queries.", "labels": [], "entities": []}, {"text": "In addition, 2015 had 164 labeled queries supplied for initial system assessment that was incorporated into the training data for 2015 submissions.", "labels": [], "entities": []}, {"text": "We performed two distinct experiments and compared multiple baselines for each.", "labels": [], "entities": []}, {"text": "shows results where we trained on 2013 only and tested on 2014 data only.", "labels": [], "entities": []}, {"text": "results are for training on 2013 and 2014 data and testing on 2015 data.", "labels": [], "entities": []}, {"text": "All systems were evaluated using the standard metrics of precision, recall, and F1 which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999688982963562}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.998638927936554}, {"text": "F1", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.999840259552002}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9988287091255188}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9767580628395081}]}, {"text": "Precision is the amount of correctly extracted facts compared to the total facts extracted by the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9893962740898132}]}, {"text": "Recall is the amount of correct facts compared all facts in the ground truth.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9929097294807434}]}, {"text": "In review:  The 0-hop queries are scored trivially on the correctness of the slot filler.", "labels": [], "entities": []}, {"text": "The 1-hop queries require two verifications to undergo scoring.", "labels": [], "entities": []}, {"text": "The 0-hop query from which it was derived must both exist and be correct.", "labels": [], "entities": []}, {"text": "Any slot fillers that don't meet this criteria are omitted even if their 1-hop slot was ultimately correct.", "labels": [], "entities": []}, {"text": "For example, the 1-hop extraction (Palo Alto, gpe:headquarters in city, Hewlett-Packard) will be ignored even though it is correct because the 0-hop query (Facebook, gpe:city of headquarters, Palo Alto) that derives it is incorrect (Facebook is headquarted in Menlo Park).", "labels": [], "entities": []}, {"text": "shows results using 2013 ESF systems as training data for the meta-classifiers and 2014 ESF systems as testing data.", "labels": [], "entities": []}, {"text": "The first 6 rows area combination of classifiers and feature vectors using stacking as described in.", "labels": [], "entities": []}, {"text": "MV refers to the majority voting described ().", "labels": [], "entities": [{"text": "MV", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.939628005027771}]}, {"text": "Majority voting accepts a fact if a certain number of systems above some learned threshold have extracted it.", "labels": [], "entities": []}, {"text": "Both of these systems showcased results on the 2014 SFV task.", "labels": [], "entities": [{"text": "2014 SFV task", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7507686018943787}]}, {"text": "The Stanford system () was the best performing ESF system during the 2014 competition.", "labels": [], "entities": []}, {"text": "The final row is our CM Fusion algorithm.", "labels": [], "entities": [{"text": "CM Fusion", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.6611794084310532}]}, {"text": "Only 0-hop queries are used for these results.", "labels": [], "entities": []}, {"text": "For each query, the first two rows refer to our CM Fusion system with different training data and the final row the best performing 2015 ESF system for that particular query type.", "labels": [], "entities": [{"text": "CM Fusion", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.830632746219635}]}, {"text": "2015 results are not officially published yet for either ESF or SFV.", "labels": [], "entities": [{"text": "ESF", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8864748477935791}, {"text": "SFV", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6040606498718262}]}, {"text": "Nevertheless, at 2015 workshop announcing the results, CM Fusion was awarded as the top ranked SFV system by F1 score.", "labels": [], "entities": [{"text": "CM Fusion", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.7441166937351227}, {"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9678723216056824}]}], "tableCaptions": [{"text": " Table 1: Slot fillers extracted by multiple systems for the query", "labels": [], "entities": [{"text": "Slot fillers extracted", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9203813870747884}]}, {"text": " Table 3: Result comparisons for the testing on the 2014 SFV", "labels": [], "entities": [{"text": "2014", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.6160268783569336}, {"text": "SFV", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.811731219291687}]}]}