{"title": [{"text": "Improving event prediction by representing script participants", "labels": [], "entities": [{"text": "Improving event prediction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9225589831670126}]}], "abstractContent": [{"text": "Automatically learning script knowledge has proved difficult, with previous work not or just barely beating a most-frequent baseline.", "labels": [], "entities": []}, {"text": "Script knowledge is a type of world knowledge which can however be useful for various task in NLP and psycholinguistic modelling.", "labels": [], "entities": []}, {"text": "We here propose a model that includes participant information (i.e., knowledge about which participants are relevant fora script) and show, on the Dinners from Hell corpus as well as the InScript corpus, that this knowledge helps us to significantly improve prediction performance on the narrative cloze task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scripts represent knowledge about typical event sequences (, for example the sequence of events happening when eating at a restaurant.", "labels": [], "entities": []}, {"text": "Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest.", "labels": [], "entities": []}, {"text": "Script knowledge is a form of structured world knowledge that is useful in NLP applications for natural language understanding tasks (e.g., ambiguity resolution, as well as for psycholinguistic models of human language processing, which need to represent event knowledge to model human expectations () of upcoming referents and utterances.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.7310856282711029}, {"text": "ambiguity resolution", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7820769250392914}]}, {"text": "One recent line of research has tried to learn scripts in an unsupervised way from large text collections.", "labels": [], "entities": []}, {"text": "The core idea in Chambers and Jurafsky; is to use coreference chains to identify events involving the same entity, with the intuition that these events would, if observed in many texts, be likely to represent a prototypical event sequence.", "labels": [], "entities": []}, {"text": "show that this method is also applicable for learning specific targeted scripts from a domain-specific corpus, shown at the example of \"Dinners From Hell\" stories and the restaurant script.", "labels": [], "entities": []}, {"text": "(P&M) have demonstrated that using richer event representations containing multiple arguments improves prediciton accuracy on the narrative cloze task over the simpler models by.", "labels": [], "entities": [{"text": "P&M)", "start_pos": 1, "end_pos": 5, "type": "DATASET", "confidence": 0.805748701095581}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.921495258808136}]}, {"text": "While they represent a script event as a pair of a verb and a dependency (an example of an event chain would be <call,obj>; <bring,subj>; <take,subj>), which is problematic for weak verbs and verb ambiguity, P&M represent events using a multi-argument event representation, e.g., call(guest,waiter,*); bring(waiter,menu,*); take(waiter,order,*).", "labels": [], "entities": []}, {"text": "This richer event representation however still has some shortcomings.", "labels": [], "entities": []}, {"text": "As the representation is based on coreference chains, the model runs into difficulties for entities that are in a chain of length one.", "labels": [], "entities": []}, {"text": "Entities in a chain are internally mapped onto variables, but all single entities are mapped onto a common category Other.", "labels": [], "entities": []}, {"text": "This means that all information about such referents is lost, e.g. enjoy(customer, fish, * ) cannot be distinguished from enjoy(customer, silence, * ) when neither fish or silence have appeared before in the text.", "labels": [], "entities": []}, {"text": "The coreference chains provide a good approxi-mation for identifying events that involve the same participants.", "labels": [], "entities": []}, {"text": "But would performance improve substantially if we could represent event participants?", "labels": [], "entities": []}, {"text": "This specifically addresses the problem of unlinked coreference chains (e.g., \"food\", \"it\", \"steak\") not appearing in the same coreference chain even though they represent the same role within the script, and the problem of mapping referents which are not part of a chain onto a single \"other\" representation.", "labels": [], "entities": []}, {"text": "show that referring expressions in a script can be automatically categorized in terms of the role they play within the script by using coreference chains, as well as information from WordNet (telling us e.g., that a steak is a kind of food).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 183, "end_pos": 190, "type": "DATASET", "confidence": 0.9674052596092224}]}, {"text": "In this paper, we extend the existing approach by P&M and demonstrate that explicitly labelling participants (instead of using coreference chains) leads to improved event prediction performance.", "labels": [], "entities": [{"text": "event prediction", "start_pos": 165, "end_pos": 181, "type": "TASK", "confidence": 0.7330112457275391}]}, {"text": "We furthermore provide a systematic evaluation of the effect of automatically-annotated coreference chains vs. gold coreference chains, and automaticallyannotated script participants vs. gold participant annotation.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the Dinners from Hell corpus (, as well as the newly available InScript corpus (.", "labels": [], "entities": [{"text": "InScript corpus", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8023048341274261}]}, {"text": "Following earlier work, we evaluate the quality of script models using the so-called narrative cloze task, where the model has to predict a missing event given surrounding events in the text.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance for our model is reported with both au-", "labels": [], "entities": []}]}