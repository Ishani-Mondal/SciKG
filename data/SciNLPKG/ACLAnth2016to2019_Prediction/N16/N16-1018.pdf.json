{"title": [{"text": "Counter-fitting Word Vectors to Linguistic Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity.", "labels": [], "entities": []}, {"text": "Applying this method to publicly available pre-trained word vectors leads to anew state of the art performance on the SimLex-999 dataset.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.9411337673664093}]}, {"text": "We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.8158380587895712}]}], "introductionContent": [{"text": "Many popular methods that induce representations for words rely on the distributional hypothesis -the assumption that semantically similar or related words appear in similar contexts.", "labels": [], "entities": []}, {"text": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora).", "labels": [], "entities": []}, {"text": "Word vectors trained using these methods have proven useful for many downstream tasks including machine translation () and dependency parsing (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8426160216331482}, {"text": "dependency parsing", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.856263667345047}]}, {"text": "One drawback of learning word embeddings from co-occurrence information in corpora is that it tends to coalesce the notions of semantic similarity and conceptual association ().", "labels": [], "entities": []}, {"text": "Furthermore, even methods that can distinguish similarity from association (e.g., based on syntactic co-occurrences) will generally fail to tell synonyms from antonyms ( as east and west or expensive and inexpensive appear in near-identical contexts, which means that distributional models produce very similar word vectors for such words.", "labels": [], "entities": []}, {"text": "Examples of such anomalies in GloVe vectors can be seen in, where words such as cheaper and inexpensive are deemed similar to (their antonym) expensive.", "labels": [], "entities": []}, {"text": "A second drawback is that similarity and antonymy can be application-or domain-specific.", "labels": [], "entities": []}, {"text": "In our case, we are interested in exploiting distributional knowledge for the dialogue state tracking task (DST).", "labels": [], "entities": [{"text": "dialogue state tracking task (DST)", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7981558995587485}]}, {"text": "The DST component of a dialogue system is responsible for interpreting users' utterances and updating the system's belief state -a probability distribution overall possible states of the dialogue.", "labels": [], "entities": []}, {"text": "For example, a DST for the restaurant domain needs to detect whether the user wants a cheap or expensive restaurant.", "labels": [], "entities": [{"text": "DST", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9846183657646179}]}, {"text": "Being able to generalise using distributional information while still distinguishing between semantically different yet conceptually related words (e.g. cheaper and pricey) is critical for the performance of dialogue systems.", "labels": [], "entities": []}, {"text": "In particular, a dialogue system can be led seriously astray by false synonyms.", "labels": [], "entities": []}, {"text": "We propose a method that addresses these two drawbacks by using synonymy and antonymy relations drawn from either a general lexical resource or an application-specific ontology to fine-tune distributional word vectors.", "labels": [], "entities": []}, {"text": "Our method, which we term counter-fitting, is a lightweight post-processing procedure in the spirit of retrofitting ( . The second row of illustrates the results of counter-fitting: the nearest neighbours capture true similarity much more intuitively than the original GloVe vectors.", "labels": [], "entities": []}, {"text": "The procedure improves word vector quality regardless of the initial word vectors provided as input.", "labels": [], "entities": []}, {"text": "1 By applying counter-fitting to the Paragram-SL999 word vectors provided by, we achieve new state-of-the-art performance on SimLex-999, a dataset designed to measure how well different models judge semantic similarity between words ().", "labels": [], "entities": []}, {"text": "We also show that the counter-fitting method can inject knowledge of dialogue domain ontologies into word vector space representations to facilitate the construction of semantic dictionaries which improve DST performance across two different dialogue domains.", "labels": [], "entities": [{"text": "DST", "start_pos": 205, "end_pos": 208, "type": "TASK", "confidence": 0.9869829416275024}]}, {"text": "Our tool and word vectors are available at github.com/nmrksic/counter-fitting.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance on SimLex-999. Retrofitting uses  the code and (PPDB) data provided by the authors", "labels": [], "entities": [{"text": "Retrofitting", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9159922003746033}]}, {"text": " Table 3: SimLex-999 performance when different sets of  linguistic constraints are used for counter-fitting", "labels": [], "entities": []}, {"text": " Table 5: Number of dialogues in the dataset splits used  for the Dialogue State Tracking experiments", "labels": [], "entities": [{"text": "Dialogue State Tracking", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.712559183438619}]}, {"text": " Table 6: Performance of RNN belief trackers (ensembles  of four models) with different semantic dictionaries", "labels": [], "entities": [{"text": "RNN belief trackers", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6682083209355673}]}]}