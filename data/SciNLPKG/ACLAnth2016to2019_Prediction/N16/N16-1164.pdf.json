{"title": [{"text": "End-to-End Argumentation Mining in Student Essays", "labels": [], "entities": [{"text": "End-to-End Argumentation Mining", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6774496634801229}]}], "abstractContent": [{"text": "Understanding the argumentative structure of a persuasive essay involves addressing two challenging tasks: identifying the components of the essay's argument and identifying the relations that occur between them.", "labels": [], "entities": []}, {"text": "We examine the under-investigated task of end-to-end argument mining in persuasive student essays , where we (1) present the first results on end-to-end argument mining in student essays using a pipeline approach; (2) address error propagation inherent in the pipeline approach by performing joint inference over the outputs of the tasks in an Integer Linear Programming (ILP) framework; and (3) propose a novel objective function that enables F-score to be maximized directly by an ILP solver.", "labels": [], "entities": [{"text": "end-to-end argument mining", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6971714496612549}, {"text": "end-to-end argument mining", "start_pos": 142, "end_pos": 168, "type": "TASK", "confidence": 0.7611629366874695}, {"text": "F-score", "start_pos": 444, "end_pos": 451, "type": "METRIC", "confidence": 0.9232062101364136}]}, {"text": "We evaluate our joint-inference approach with our novel objective function on a publicly-available corpus of 90 essays, where it yields an 18.5% relative error reduction in F-score over the pipeline system.", "labels": [], "entities": [{"text": "F-score", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.9932734966278076}]}], "introductionContent": [{"text": "There has been a surge of interest in argumentation mining in recent years.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.9876041412353516}]}, {"text": "Argumentation mining typically involves addressing two subtasks: (1) argument component identification (ACI), which consists of identifying the locations and types of the components that makeup the arguments (i.e., Major Claims, Claims, and Premises), and (2) relation identification (RI), which involves identifying the type of relation that holds between two argument components (i.e., Support, Attack, None).", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8478697538375854}, {"text": "argument component identification (ACI)", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.7892439663410187}, {"text": "relation identification (RI)", "start_pos": 260, "end_pos": 288, "type": "TASK", "confidence": 0.8087165355682373}]}, {"text": "As a first step towards mining arguments in persuasive essays, Stab and Gurevych (S&G) annotated a corpus of 90 student essays with argument components and their relations.", "labels": [], "entities": []}, {"text": "To illustrate, consider the following excerpt from one essay: From this point of view, I firmly believe that (1) we should attach more importance to cooperation during primary education.", "labels": [], "entities": []}, {"text": "First of all, (2) through cooperation, children can learn about interpersonal skills which are significant in the future life of all students.", "labels": [], "entities": []}, {"text": "(3) What we acquired from teamwork is not only how to achieve the same goal with others but more importantly, how to get along with others.", "labels": [], "entities": []}, {"text": "In this example, premise (3) supports claim (2), which in turn supports major claim (1).", "labels": [], "entities": []}, {"text": "Using their annotated corpus, S&G presented initial results on simplified versions of the ACI and RI tasks).", "labels": [], "entities": []}, {"text": "Specifically, they applied their learned ACI classifier to classify only gold argument components (i.e., text spans corresponding to a Major Claim, Claim, or Premise in the gold standard) or sentences that contain no gold argument components (as non-argumentative).", "labels": [], "entities": []}, {"text": "Similarly, they applied their learned RI classifier to classify only the relation between two gold argument components.", "labels": [], "entities": [{"text": "RI classifier", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7684261202812195}]}, {"text": "In other words, they simplified both tasks by avoiding the challenging task of identifying the locations of argument components.", "labels": [], "entities": []}, {"text": "Consequently, their approach cannot be applied in a realistic setting where the input is an unannotated essay.", "labels": [], "entities": []}, {"text": "Motivated by this weakness, we examine in this paper argument mining in persuasive student essays in a considerably more challenging setting than that of S&G: the end-to-end setting.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7375987768173218}, {"text": "S&G", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.8503408829371134}]}, {"text": "In other words, we perform argument mining on raw, unannotated essays.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.8849367201328278}]}, {"text": "Our work makes three contributions.", "labels": [], "entities": []}, {"text": "First, we present the first results on end-to-end argument mining in student essays using a pipeline approach, where the ACI task is performed prior to the RI task.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7477118968963623}]}, {"text": "Second, to avoid the error propagation problem inherent in the pipeline approach, we perform joint inference over the outputs of the ACI and RI classifiers in an Integer Linear Programming (ILP) framework (), where we design constraints to enforce global consistency.", "labels": [], "entities": []}, {"text": "Finally, we argue that the typical objective function used extensively in ILP programs for NLP tasks is not ideal for tasks whose primary evaluation metric is F-score, and subsequently propose a novel objective function that enables F-score to be maximized directly in an ILP framework.", "labels": [], "entities": [{"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9557129740715027}]}, {"text": "We believe that the impact of our work goes beyond argument mining, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.846310019493103}, {"text": "F-score optimizing objective function", "start_pos": 75, "end_pos": 112, "type": "METRIC", "confidence": 0.895874559879303}]}], "datasetContent": [{"text": "As mentioned before, we use as our corpus the 90 essays annotated with argumentative discourse structures by S&G.", "labels": [], "entities": []}, {"text": "All of our experiments are conducted via five-fold cross-validation on this corpus.", "labels": [], "entities": []}, {"text": "In each fold experiment, we reserve 60% of the essays for training, 20% for development (selecting features and tuning \u03b1), and 20% for testing.", "labels": [], "entities": []}, {"text": "To calculate F-score on each task using Equation 16, we need to explain what constitutes a true positive, false positive, or false negative on each task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9980137348175049}]}, {"text": "Given that j is a true argument component and i is an ACC, the formulas for the ACI task are: where gl(j) is the gold standard label of j, pl(i) is the predicted label of i, n is the non-argumentative class, and i . = j means i is a match for j. i and j are considered an exact match if they have exactly: Five-fold cross-validation average percentages for argument component identification (ACI) and relation identification (RI) for OUR system and the pipeline-based BASEline system.", "labels": [], "entities": [{"text": "relation identification (RI", "start_pos": 401, "end_pos": 428, "type": "TASK", "confidence": 0.6913645789027214}, {"text": "OUR", "start_pos": 434, "end_pos": 437, "type": "DATASET", "confidence": 0.7731189131736755}]}, {"text": "Column abbreviations are Major Claim F-score (MC-F), Claim F-score (C-F), Premise F-score (P-F), Precision (P), Recall (R), F-score (F), Support F-score (S-F), and Attack F-score (A-F).", "labels": [], "entities": [{"text": "Major Claim F-score (MC-F)", "start_pos": 25, "end_pos": 51, "type": "METRIC", "confidence": 0.7710089882214864}, {"text": "Claim F-score (C-F)", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.8905536890029907}, {"text": "Premise F-score (P-F)", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.8952597856521607}, {"text": "Precision (P)", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9485858082771301}, {"text": "Recall (R)", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9454622566699982}, {"text": "F-score (F)", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.9203927218914032}, {"text": "Support F-score", "start_pos": 137, "end_pos": 152, "type": "METRIC", "confidence": 0.9087525606155396}, {"text": "Attack F-score (A-F)", "start_pos": 164, "end_pos": 184, "type": "METRIC", "confidence": 0.9306115508079529}]}, {"text": "the same boundaries, whereas they are considered an approximate match if they share over half their tokens.", "labels": [], "entities": []}, {"text": "We perform most of our analysis on approximate match results rather than exact match results as it can be difficult even for human annotators to identify exactly the same boundaries for an argument component.", "labels": [], "entities": []}, {"text": "We use the same formulas for calculating these numbers for the RI problem except that j and i represent a true relation and an RC respectively, two relations approximately (exactly) match if both their source and target ACCs approximately (exactly) match, and n is the no-relation class.", "labels": [], "entities": [{"text": "RI problem", "start_pos": 63, "end_pos": 73, "type": "TASK", "confidence": 0.9101078808307648}]}], "tableCaptions": [{"text": " Table 3: Five-fold cross-validation average percentages for argument component identification (ACI) and relation identification", "labels": [], "entities": [{"text": "argument component identification (ACI)", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.7805542349815369}, {"text": "relation identification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.8874819278717041}]}, {"text": " Table 4: Ablation results. How OUR system performs on  one development set as measured by percent Precision,  Recall, and F-score if each improvement or feature set is  removed.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980555772781372}, {"text": "Precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9947546720504761}, {"text": "Recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9671791791915894}, {"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9973821043968201}]}]}