{"title": [{"text": "Multimodal Semantic Learning from Child-Directed Input", "labels": [], "entities": []}], "abstractContent": [{"text": "Children learn the meaning of words by being exposed to perceptually rich situations (lin-guistic discourse, visual scenes, etc).", "labels": [], "entities": []}, {"text": "Current computational learning models typically simulate these rich situations through impoverished symbolic approximations.", "labels": [], "entities": []}, {"text": "In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes.", "labels": [], "entities": []}, {"text": "The model integrates linguistic and extra-linguistic information (visual and social cues), handles referen-tial uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational models of word learning typically approximate the perceptual context that learners are exposed to through artificial proxies, e.g., representing a visual scene via a collection of symbols such as cat and dog, signaling the presence of a cat, a dog, etc.", "labels": [], "entities": []}, {"text": "(, inter alia).", "labels": [], "entities": []}, {"text": "1 While large amounts of data can be generated in this way, they will not display the complexity and richness of the signal found in the natural environment a child is exposed to.", "labels": [], "entities": []}, {"text": "We take a step towards a more realistic setup by introducing a model that operates on naturalistic images of the objects present in a communicative episode.", "labels": [], "entities": []}, {"text": "Inspired by recent computational models of meaning (, that integrate distributed linguistic and visual information, we build upon the Multimodal Skip-Gram (MSG) model of. and enhance it to handle cross-referential uncertainty.", "labels": [], "entities": []}, {"text": "Moreover, we extend the cues commonly used in multimodal learning (e.g., objects in the environment) to include social cues (e.g., eyegaze, gestures, body posture, etc.) that reflect speakers' intentions and generally contribute to the unfolding of the communicative situation).", "labels": [], "entities": []}, {"text": "As a first step towards developing full-fleged learning systems that leverage all signals available within a communicative setup, in our extended model we incorporate information regarding the objects that caregivers are holding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow the evaluation protocol of and.", "labels": [], "entities": []}, {"text": "Given 37 test words and the corresponding 17 objects (see), all found in the corpus, we rank the objects with respect to each word.", "labels": [], "entities": []}, {"text": "A mean Best-F score is then derived by computing, for each word, the top F score across the precision-recall curve, and averaging it across the words.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 92, "end_pos": 108, "type": "METRIC", "confidence": 0.9937276840209961}]}, {"text": "MSG rankings are obtained by directly ordering the visual representations of the objects by cosine similarity to the MSG word vectors.", "labels": [], "entities": []}, {"text": "reports our results compared to those in earlier studies, all of which did not use actual visual representations of objects but rather arbitrary symbolic IDs.", "labels": [], "entities": []}, {"text": "Bayesian CSL is the original Bayesian cross-situational model of, also including social cues (not limited, like us, to mother's touch).", "labels": [], "entities": []}, {"text": "BEAGLE is the best semantic-space result across a range of distributional models and word-object matching methods from Kievit-.", "labels": [], "entities": [{"text": "BEAGLE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9847726821899414}, {"text": "word-object matching", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7066803872585297}]}, {"text": "Their distributional models were trained in a batch mode, and by treating object IDs as words so that standard word-vector-based similarity methods could be used to rank objects with respect to words.", "labels": [], "entities": []}, {"text": "Plain MSG is outperforming nearly all earlier approaches by a large margin.", "labels": [], "entities": []}, {"text": "The only method bettering it is the BEAGLE+PMI combination of Kievit-Kylar et al.", "labels": [], "entities": [{"text": "BEAGLE+PMI", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.7502261201540629}]}, {"text": "(PMI measures direct co-occurrence of test words and object IDs).", "labels": [], "entities": []}, {"text": "The latter was obtained through a grid search of all possible model combinations performed directly on the test set, and relied on a weight parameter optimized on the corpus by assuming access to gold annotation.", "labels": [], "entities": []}, {"text": "It is thus not comparable to the untuned MSG.", "labels": [], "entities": []}, {"text": "Plain MSG, then, performs remarkably well, even without any mechanism attempting to track wordobject matching across scenes.", "labels": [], "entities": [{"text": "wordobject matching across scenes", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.7970557734370232}]}, {"text": "Still, letting the model pay more attention to the objects currently most tightly associated to a word (AttentiveMSG) brings a large improvement over plain MSG, and a further improvement is brought about by giving more weight to objects touched by the mother (AttentiveSocialMSG).", "labels": [], "entities": []}, {"text": "As concrete examples, plain MSG associated the word cow with a pig, whereas AttentiveMSG correctly shifts attention to the cow.", "labels": [], "entities": []}, {"text": "In turn, AttentiveSocialMSG associates to the right object several words that AttentiveMSG wrongly pairs with the hand holding them, instead.", "labels": [], "entities": []}, {"text": "One might fear the better performance of our models might be due to the skip-gram method being superior to the older distributional semantic approaches tested by, independently of the extra visual information we exploit.", "labels": [], "entities": []}, {"text": "In other words, it could be that MSG has simply learned to treat, say, the lamb visual vector as an arbitrary signature, functioning as a semantically opaque ID for the relevant object, without exploiting the visual resemblance between lamb and sheep.", "labels": [], "entities": []}, {"text": "In this case, we should obtain similar performance when arbitrarily shuffling the visual vectors across object types (e.g., consistently replacing each occurrence of the lamb visual vector with, say, the hand visual vector).", "labels": [], "entities": []}, {"text": "The lower results obtained in this control condition (ASMSG+shuffled visual vector) confirm that our performance boost is largely due to exploitation of genuine visual information.", "labels": [], "entities": [{"text": "ASMSG+shuffled visual vector)", "start_pos": 54, "end_pos": 83, "type": "METRIC", "confidence": 0.8354280094305674}]}, {"text": "Since our approach is incremental (unlike the vast majority of traditional distributional models that operate on batch mode), it can in principle exploit the fact that the linguistic and visual flows in the corpus are meaningfully ordered (discourse and visual environment will evolve in a coherent manner: a hat appears on the scene, it's therefor awhile, in the meantime a few statements about hats are uttered, etc.).", "labels": [], "entities": []}, {"text": "The dramatic quality drop in the ASMSG+randomized sentences condition, where AttentiveSocialMSG was trained on IFC after randomizing sentence order, confirms the coherent situation flow is crucial to our good performance.", "labels": [], "entities": [{"text": "ASMSG+randomized sentences", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7436182945966721}]}, {"text": "Given the small size of the input corpus, good performance on the word-object association already counts as indirect evidence that MSG, like children, can learn from small amounts of data.", "labels": [], "entities": []}, {"text": "In we take a more specific look at this challenge by reporting AttentiveSocialMSG performance on the task of ranking object visual representations for test words that occurred only once in IFC, considering both the standard evaluation set and a much larger confusion set including visual vectors for 5.1K distinct objects (those of).", "labels": [], "entities": [{"text": "IFC", "start_pos": 189, "end_pos": 192, "type": "DATASET", "confidence": 0.8318967223167419}]}, {"text": "Remarkably, in all but one case, the model associates the test word to the right object from the small set, and to either the right objector another relevant visual concept (e.g., a ranch for moocows) when the extended set is considered.", "labels": [], "entities": []}, {"text": "The exception is kitty, and even for this word the model ranks the correct object as second in the smaller set, and well above chance for the larger one.", "labels": [], "entities": []}, {"text": "Our approach, just like humans (, can often get a word meaning right based on a single exposure to it.", "labels": [], "entities": []}, {"text": "Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations.", "labels": [], "entities": []}, {"text": "Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category.", "labels": [], "entities": []}, {"text": "We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by.", "labels": [], "entities": []}, {"text": "Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object (not used in IFC), of a highly visually similar object, of a mildly visually similar object and of a dissimilar one (for duck: duck, chicken, finch and garage, respectively).", "labels": [], "entities": []}, {"text": "The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation.", "labels": [], "entities": []}, {"text": "Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%).", "labels": [], "entities": [{"text": "Precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9992220401763916}, {"text": "chance", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9941644072532654}]}, {"text": "In the majority of cases the top-10 intruders are instances of the most visually related concepts (60% of intruders, vs. 33% expected by chance).", "labels": [], "entities": []}, {"text": "For example, the model retrieves pictures of sheep for the word lamb, or bulls for cow.", "labels": [], "entities": []}, {"text": "Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Best-F results for the MSG variations and alternative", "labels": [], "entities": []}, {"text": " Table 2: Test words occurring only once in IFC, together", "labels": [], "entities": [{"text": "IFC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.6527414321899414}]}]}