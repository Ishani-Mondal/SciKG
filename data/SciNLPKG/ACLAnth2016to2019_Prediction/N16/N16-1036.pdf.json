{"title": [{"text": "Recurrent Memory Networks for Language Modeling", "labels": [], "entities": [{"text": "Recurrent Memory Networks", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8720592061678568}, {"text": "Language Modeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7173551470041275}]}], "abstractContent": [{"text": "Recurrent Neural Networks (RNNs) have obtained excellent result in many natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "However, understanding and interpreting the source of this success remains a challenge.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.966783344745636}]}, {"text": "In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data.", "labels": [], "entities": [{"text": "Recurrent Memory Network (RMN)", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.5405716896057129}]}, {"text": "We demonstrate the power of RMN on language modeling and sentence completion tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7212603688240051}, {"text": "sentence completion tasks", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.8123185833295187}]}, {"text": "On language modeling, RMN out-performs Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.689608171582222}]}, {"text": "Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures.", "labels": [], "entities": []}, {"text": "On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state of the art by a large margin.", "labels": [], "entities": [{"text": "Sentence Completion Challenge", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.9312755862871805}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9992985725402832}]}], "introductionContent": [{"text": "Recurrent Neural Networks (RNNs)) are remarkably powerful models for sequential data.", "labels": [], "entities": [{"text": "Recurrent Neural Networks (RNNs))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5890730420748392}]}, {"text": "Long Short-Term Memory (LSTM)), a specific architecture of RNN, has a track record of success in many natural language processing tasks such as language modeling), dependency parsing, sentence com-Within the context of natural language processing, a common assumption is that LSTMs are able to capture certain linguistic phenomena.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.707896962761879}, {"text": "dependency parsing", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.8829044997692108}]}, {"text": "Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: carefully design two artificial datasets where sentences have explicit recursive structures.", "labels": [], "entities": []}, {"text": "They show empirically that while processing the input linearly, LSTMs can implicitly exploit recursive structures of languages.", "labels": [], "entities": []}, {"text": "find that using explicit syntactic features within LSTMs in their sentence compression model hurts the performance of overall system.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7220429629087448}]}, {"text": "They then hypothesize that a basic LSTM is powerful enough to capture syntactic aspects which are useful for compression.", "labels": [], "entities": []}, {"text": "To understand and explain which linguistic dimensions are captured by an LSTM is non-trivial.", "labels": [], "entities": []}, {"text": "This is due to the fact that the sequences of input histories are compressed into several dense vectors by the LSTM's components whose purposes with respect to representing linguistic information is not evident.", "labels": [], "entities": []}, {"text": "To our knowledge, the only attempt to better understand the reasons of an LSTM's performance and limitations is the work of by means of visualization experiments and cell activation statistics in the context of character-level language modeling.", "labels": [], "entities": [{"text": "character-level language modeling", "start_pos": 211, "end_pos": 244, "type": "TASK", "confidence": 0.6322187781333923}]}, {"text": "Our work is motivated by the difficulty in understanding and interpreting existing RNN architectures from a linguistic point of view.", "labels": [], "entities": []}, {"text": "We propose Recurrent Memory Network (RMN), a novel RNN architecture that combines the strengths of both LSTM and Memory Network (.", "labels": [], "entities": [{"text": "Recurrent Memory Network (RMN)", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.5903036991755167}]}, {"text": "In RMN, the Memory Block component-a variant of Memory Network-accesses the most recent input words and selectively attends to words that are relevant for predicting the next word given the current LSTM state.", "labels": [], "entities": []}, {"text": "By looking at the attention distribution over history words, our RMN allows us not only to interpret the results but also to discover underlying dependencies present in the data.", "labels": [], "entities": []}, {"text": "In this paper, we make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a novel RNN architecture that complements LSTM in language modeling.", "labels": [], "entities": []}, {"text": "We demonstrate that our RMN outperforms competitive LSTM baselines in terms of perplexity on three large German, Italian, and English datasets.", "labels": [], "entities": []}, {"text": "2. We perform an analysis along various linguistic dimensions that our model captures.", "labels": [], "entities": []}, {"text": "This is possible only because the Memory Block allows us to look into its internal states and its explicit use of additional inputs at each time step.", "labels": [], "entities": []}, {"text": "3. We show that, with a simple modification, our RMN can be successfully applied to NLP tasks other than language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.6587199568748474}]}, {"text": "On the Sentence Completion Challenge (, our model achieves an impressive 69.2% accuracy, surpassing the previous state of the art 58.9% by a large margin.", "labels": [], "entities": [{"text": "Sentence Completion Challenge", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.9390082359313965}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9990465044975281}]}], "datasetContent": [{"text": "Language models play a crucial role in many NLP applications such as machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8284063935279846}, {"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8045777380466461}]}, {"text": "Language modeling also serves as a standard test bed for newly proposed models (.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6771620213985443}]}, {"text": "We conjecture that, by explicitly accessing history words, RMNs will offer better predictive power than the existing recurrent architectures.", "labels": [], "entities": []}, {"text": "We therefore evaluate our RMN architectures against state-of-theart LSTMs in terms of perplexity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics. |s| denotes the average sen- tence length and |V | the vocabulary size.", "labels": [], "entities": []}, {"text": " Table 2: Perplexity comparison including RMN  variants with and without temporal matrix (tM) and  linear (l) versus gating (g) composition function.", "labels": [], "entities": []}, {"text": " Table 3: Mean frequency (\u00b5) of (most-attended- word, word-to-predict) pairs grouped by relative dis- tance (d).", "labels": [], "entities": [{"text": "Mean frequency (\u00b5)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9764671564102173}, {"text": "relative dis- tance (d)", "start_pos": 88, "end_pos": 111, "type": "METRIC", "confidence": 0.9026200601032802}]}, {"text": " Table 4: Accuracy on 1,040 test sentences. We use  perplexity to choose the best model. Dimension of  word embeddings, LSTM hidden states, and gate g  parameters are set to d.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9863148927688599}]}]}