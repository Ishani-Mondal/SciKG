{"title": [{"text": "Ultradense Word Embeddings by Orthogonal Transformation", "labels": [], "entities": [{"text": "Ultradense Word Embeddings", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7084729274113973}]}], "abstractContent": [{"text": "Embeddings are generic representations that are useful for many NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant fora task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space.", "labels": [], "entities": [{"text": "DENSIFIER", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9212671518325806}]}, {"text": "We show that ultradense embeddings generated by DENSI-FIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information-sentiment, con-creteness and frequency.", "labels": [], "entities": [{"text": "DENSI-FIER", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.7981125116348267}]}, {"text": "On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense sub-space is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "labels": [], "entities": [{"text": "SemEval2015 10B sentiment analysis task", "start_pos": 7, "end_pos": 46, "type": "TASK", "confidence": 0.7379355192184448}]}], "introductionContent": [{"text": "Embeddings are useful for many tasks, including word similarity (e.g.,), named entity recognition (NER) (e.g.,) and sentiment analysis (e.g.,,,).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.72829470038414}, {"text": "named entity recognition (NER)", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7279019306103388}, {"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9466453790664673}]}, {"text": "Embeddings are generic representations, containing different types of information about a word.", "labels": [], "entities": []}, {"text": "Statistical models can be trained to make best use of these generic representations fora specific application like NER or sentiment analysis.", "labels": [], "entities": [{"text": "NER", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9321109652519226}, {"text": "sentiment analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9072572588920593}]}, {"text": "Our hypothesis in this paper is that the information useful for any given task is contained in an ultradense subspace Eu . We propose the new method DENSIFIER to identify Eu . Given a set of word embeddings, DENSIFIER learns an orthogonal transformation of the original space E o on a task-specific training set.", "labels": [], "entities": []}, {"text": "The orthogonality of the transformation can be considered a hard regularizer.", "labels": [], "entities": []}, {"text": "The benefit of the proposed method is that embeddings are most useful if learned on unlabeled corpora and performance-enhanced on abroad array of tasks.", "labels": [], "entities": []}, {"text": "This means we should try to keep all information offered by them.", "labels": [], "entities": []}, {"text": "Orthogonal transformations \"reorder\" the space without adding or removing information and preserve the bilinear form, i.e., Euclidean distance and cosine.", "labels": [], "entities": []}, {"text": "The transformed embeddings concentrate all information relevant for the task in Eu . The benefits of Eu compared to E o are (i) highquality and (ii) efficient representations.", "labels": [], "entities": []}, {"text": "(i) DENSI-FIER moves non-task-related information outside of Eu , i.e., into the orthogonal complement of Eu . As a result, Eu provides higher-quality representations for the task than E o ; e.g., noise that could result in overfitting is reduced in Eu compared to E o . (ii) Eu has a dimensionality smaller by a factor of 100 in our experiments.", "labels": [], "entities": []}, {"text": "As a result, training statistical models on these embeddings is much faster.", "labels": [], "entities": []}, {"text": "These models also have many fewer parameters, thus again helping to prevent overfitting, especially for complex, deep neural networks.", "labels": [], "entities": []}, {"text": "We show the benefits of ultradense representations in two text polarity classification tasks (SemEval2015 Task 10B, Czech movie reviews).", "labels": [], "entities": [{"text": "text polarity classification", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7234643697738647}]}, {"text": "In the most extreme form, ultradense representations -i.e., Eu -have a single dimension.", "labels": [], "entities": []}, {"text": "We exploit this for creating lexicons in which words are annotated with lexical information, e.g., with sentiment.", "labels": [], "entities": []}, {"text": "Specifically, we create high-coverage lexicons with up to 3 million words (i) for three lexical properties: for sentiment, concreteness and frequency; (ii) for five languages: Czech, English, French, German and Spanish; (iii) for two domains, Twitter and News, in a domain adaptation setup.", "labels": [], "entities": []}, {"text": "The main advantages of this method of lexicon creation are: (i) We need a training lexicon of only a few hundred words, thus making our method effective for new domains and languages and requiring only a minimal manual annotation effort.", "labels": [], "entities": [{"text": "lexicon creation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8444046974182129}]}, {"text": "(ii) The method is applicable to any set of embeddings, including phrase and sentence embeddings.", "labels": [], "entities": []}, {"text": "Assuming the availability of a small hand-labeled lexicon, DENSIFIER automatically creates a domain dependent lexicon based on a set of embeddings learned on a large corpus of the domain.", "labels": [], "entities": []}, {"text": "(iii) While the input lexicon is discrete -e.g., positive (+1) and negative (-1) sentiment -the output lexicon is continuous and this more fine-grained assessment is potentially more informative than a simple binary distinction.", "labels": [], "entities": []}, {"text": "We show that lexicons created by DENSIFIER beat the state of the art on SemEval2015 Task 10E (determining association strength).", "labels": [], "entities": [{"text": "SemEval2015 Task 10E", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7186908920605978}]}, {"text": "One of our goals is to make embeddings more interpretable.", "labels": [], "entities": []}, {"text": "The work on sentiment, concreteness and frequency we present in this paper is a first step towards a general decomposition of embedding spaces into meaningful, dense subspaces.", "labels": [], "entities": []}, {"text": "This would lead to cleaner and more easily interpretable representations -as well as representations that are more effective and efficient.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for lexicon creation. #tokens: size of embedding training corpus (in billion). #types: size of output lexicon (in", "labels": [], "entities": []}, {"text": " Table 2: Top 10 words in the output lexicons for the domains Twitter and News (English) and Web (German).", "labels": [], "entities": []}, {"text": " Table 3: Results for Lexicon Creation. The first column gives", "labels": [], "entities": [{"text": "Lexicon Creation", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7881378531455994}]}, {"text": " Table 4: Performance on Text Polarity Classification", "labels": [], "entities": [{"text": "Text Polarity Classification", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.742190937201182}]}]}