{"title": [{"text": "Unsupervised Compound Splitting With Distributional Semantics Rivals Supervised Methods", "labels": [], "entities": [{"text": "Unsupervised Compound Splitting", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5676798125108083}]}], "abstractContent": [{"text": "In this paper we present a word decom-pounding method that is based on distribu-tional semantics.", "labels": [], "entities": []}, {"text": "Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus.", "labels": [], "entities": []}, {"text": "The core idea of our approach is that parts of compounds (like \"candle\" and \"stick\") are semantically similar to the entire compound, which helps to exclude spurious splits (like \"candles\" and \"tick\").", "labels": [], "entities": []}, {"text": "We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised base-lines.", "labels": [], "entities": []}, {"text": "For Dutch, our method performs only slightly below a rule-based optimized compound splitter.", "labels": [], "entities": [{"text": "compound splitter", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6512634754180908}]}], "introductionContent": [{"text": "Germanic and agglutinative languages (e.g. German, Swedish, Finnish, Korean) have a productive morphology that allows the formation of not spaceseparated compounds in a much larger extent than e.g. in English.", "labels": [], "entities": []}, {"text": "The task of separating such compounds into their corresponding single word (sub-) units is called compound splitting or decompounding.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7177298963069916}]}, {"text": "Decompounding showed impact in several NLP applications, e.g. ASR), MT ( or IR (Monz and de, and is generally perceived as a crucial component for the processing of respective languages.", "labels": [], "entities": []}, {"text": "However, most existing systems rely on dictionaries or are trained in a supervised fashion.", "labels": [], "entities": []}, {"text": "Both approaches require substantial manual work and do not adapt to vocabulary change.", "labels": [], "entities": []}, {"text": "In this paper we introduce an unsupervised method for decompounding that relies on distributional semantics.", "labels": [], "entities": []}, {"text": "For the computation of the semantic model we solely rely on a tokenized monolingual corpus and do not require any further linguistic processing.", "labels": [], "entities": []}, {"text": "Most previous research on compound splitting concentrates on the detection of lemmas that form the compound.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7766854465007782}]}, {"text": "Whereas this is important for several tasks, in this work we focus on the splitting of a compound into its word units without any base form reduction, arguing that lemmatization is either part of the task pipeline anyways (e.g. IR) or not required (e.g. for ASR).", "labels": [], "entities": [{"text": "ASR)", "start_pos": 258, "end_pos": 262, "type": "TASK", "confidence": 0.860083818435669}]}], "datasetContent": [{"text": "For testing the performance of our method, we chose four datasets.", "labels": [], "entities": []}, {"text": "The first dataset was manually labeled by  and consists of 700 German nouns from different frequency bands.", "labels": [], "entities": []}, {"text": "The second dataset consists of 158,653 nouns from the German newspaper magazine c't 6 and was created by.", "labels": [], "entities": [{"text": "German newspaper magazine c't 6", "start_pos": 54, "end_pos": 85, "type": "DATASET", "confidence": 0.9004882077376047}]}, {"text": "As third dataset we use a noun compound dataset of 54,571 nouns from GermaNet , which has been constructed by.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8866022229194641}]}, {"text": "8 While converting these datasets for the task of compound splitting, we do not separate words in the gold standard, which comprise of prepositions, e.g. the word Abgang (outflow) is not split into Ab-gang (off walk).", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7595611810684204}]}, {"text": "To show the language independency of our method, we apply it to a Whereas our method mostly does not assume language knowledge, we uppercase the first letter of each wi, when we apply our method on German texts.", "labels": [], "entities": []}, {"text": "6 http://heise.de/ct 7 available at: http://www.sfs.uni-tuebingen.", "labels": [], "entities": []}, {"text": "de/lsd/documents/compounds/split_ compounds_from_GermaNet10.0.txt We follow and remove all words including dashs.", "labels": [], "entities": []}, {"text": "This only affects the GermaNet dataset and reduces the effective test set to 53,118 nouns.", "labels": [], "entities": [{"text": "GermaNet dataset", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.896232008934021}]}, {"text": "Dutch compound dataset proposed by van.", "labels": [], "entities": [{"text": "Dutch compound dataset", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8285632133483887}]}, {"text": "This dataset comprises of 21,997 nouns.", "labels": [], "entities": []}, {"text": "The corpus-based DT is computed following the approach by.", "labels": [], "entities": []}, {"text": "For each word, we use the left and the right neighboring word as context representation to compute the DT.", "labels": [], "entities": []}, {"text": "For the generation of the split candidates we rely on the l = 200 most similar entries for each word.", "labels": [], "entities": []}, {"text": "The German DT is computed based on 70 million newspaper sentences, which are extracted from the Leipzig Corpora Collection (LCC) ().", "labels": [], "entities": [{"text": "German DT", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6869484484195709}, {"text": "Leipzig Corpora Collection (LCC)", "start_pos": 96, "end_pos": 128, "type": "DATASET", "confidence": 0.9497256974379221}]}, {"text": "For the generation of the Dutch DT, we use the Dutch web corpus, which is composed from 259 million sentences.", "labels": [], "entities": [{"text": "Dutch DT", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.8100621104240417}, {"text": "Dutch web corpus", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9728383223215739}]}, {"text": "We evaluate the performance of the algorithms using precision and recall as defined by.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9994183778762817}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9990037083625793}]}, {"text": "As unsupervised baselines we use the split ranking by, called KK, and the semantic analogy-based splitter (SAS) from.", "labels": [], "entities": []}, {"text": "As advanced systems we apply the lexicon-and rule-based JWordSplitter (JWS) and the supervised decompounding algorithm (ASV), introduced by . For all algorithms, we converted the splits to capture all characters in the words, reverting base forms to full forms.", "labels": [], "entities": []}, {"text": "For Dutch, we apply the KK baseline and the NL Splitter.", "labels": [], "entities": [{"text": "KK baseline", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.7281982600688934}, {"text": "NL Splitter", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9357166588306427}]}], "tableCaptions": [{"text": " Table 2: Precision (P), Recall (R) and F1-Measure (F1) for the", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9428458958864212}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.959991455078125}, {"text": "F1-Measure (F1)", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9261680394411087}]}, {"text": " Table 3: Results for three German datasets and for one Dutch", "labels": [], "entities": [{"text": "German datasets", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.7607273757457733}]}, {"text": " Table 4: Number of compounds that have been split incorrectly", "labels": [], "entities": []}]}