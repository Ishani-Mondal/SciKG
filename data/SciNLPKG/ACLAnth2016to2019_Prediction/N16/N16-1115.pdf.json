{"title": [{"text": "Search Space Pruning: A Simple Solution for Better Coreference Resolvers", "labels": [], "entities": [{"text": "Coreference Resolvers", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9592384099960327}]}], "abstractContent": [{"text": "There is a significant gap between the performance of a coreference resolution system on gold mentions and on system mentions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9164215922355652}]}, {"text": "This gap is due to the large and unbalanced search space in coreference resolution when using system mentions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9562043249607086}]}, {"text": "In this paper we show that search space pruning is a simple but efficient way of improving coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.9554957747459412}]}, {"text": "By incorporating our pruning method in one of the state-of-the-art coreference resolution systems, we achieve the best reported overall score on the CoNLL 2012 English test set.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.9190956056118011}, {"text": "CoNLL 2012 English test set", "start_pos": 149, "end_pos": 176, "type": "DATASET", "confidence": 0.956203556060791}]}, {"text": "A version of our pruning method is available with the Cort coreference resolution source code.", "labels": [], "entities": [{"text": "Cort coreference resolution source code", "start_pos": 54, "end_pos": 93, "type": "DATASET", "confidence": 0.8001113533973694}]}], "introductionContent": [{"text": "Coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9238431453704834}]}, {"text": "It is a very challenging task in natural language processing and it is still far from being solved, i.e. the best reported overall CoNLL score on the CoNLL 2012 English test set is 63.39.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6530676484107971}, {"text": "CoNLL 2012 English test set", "start_pos": 150, "end_pos": 177, "type": "DATASET", "confidence": 0.9453411340713501}]}, {"text": "Text spans referring to an entity are called mentions.", "labels": [], "entities": []}, {"text": "Mentions are the primary objects in a coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.923101544380188}]}, {"text": "As with most previous work on coreference resolution, we only consider mentions that are noun phrases.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9873709082603455}]}, {"text": "However, not all of the noun phrases are mentions.", "labels": [], "entities": []}, {"text": "A noun phrase may not refer to any entity at all.", "labels": [], "entities": []}, {"text": "The pronoun it in the sentence it is raining is an example of a non-referential noun phrase.", "labels": [], "entities": []}, {"text": "Noun phrases which do refer to an entity (mentions) can be further divided into two categories: mentions referring to entities which only appear once in the discourse (i.e. singletons), and mentions realizing entities that have been referred to more than once in the text (i.e. coreferent mentions).", "labels": [], "entities": []}, {"text": "Henceforth, we refer to both singletons and non-referential phrases as non-coreferent mentions.", "labels": [], "entities": []}, {"text": "A large number of mentions that appear in a text are non-coreferent.", "labels": [], "entities": []}, {"text": "For instance, more than 80% of mentions are singletons in the OntoNotes English development set (.", "labels": [], "entities": [{"text": "OntoNotes English development set", "start_pos": 62, "end_pos": 95, "type": "DATASET", "confidence": 0.8916508555412292}]}, {"text": "The latent ranking model is the best performing model for coreference resolution to date).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9734354913234711}]}, {"text": "If we use gold mentions, the latent ranking model of achieves an overall score of 80% on the CoNLL 2012 English test set.", "labels": [], "entities": [{"text": "CoNLL 2012 English test set", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.9600139379501342}]}, {"text": "This result shows that once we have the ideal pruned search space, the ranking model with the current set of features is reasonably capable of finding corresponding entities of mentions.", "labels": [], "entities": []}, {"text": "The substantial gap (17%) between the results of the gold mentions and system mentions implies that search space pruning is a promising direction for further improvements in coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.9705297648906708}]}, {"text": "examine different search space pruning methods that exist for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9687846302986145}]}, {"text": "Among those, anaphoricity detection is the most popular method (e.g.,,,,,,,, and), while singleton detection is a more recent method).", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7978890240192413}, {"text": "singleton detection", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7978632152080536}]}, {"text": "Anaphoricity detection examines whether a phrase is anaphoric.", "labels": [], "entities": [{"text": "Anaphoricity detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7970534265041351}]}, {"text": "Singleton detection examines whether a phrase belongs to a coreference chain regardless of being anaphor or antecedent.", "labels": [], "entities": [{"text": "Singleton detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9151544570922852}]}, {"text": "Therefore, anaphoricity detection only prunes the search space of anaphors while singleton detection prunes the search space of both anaphors and antecedents.", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8731451630592346}]}, {"text": "Except for, all of the state-of-the-art coreference resolvers explicitly model anaphoricity detection.", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7398751378059387}]}, {"text": "Therefore, modeling search space pruning as singleton detection can provide additional information for the state-of-the-art coreference resolution systems.", "labels": [], "entities": [{"text": "singleton detection", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8004842102527618}, {"text": "coreference resolution", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.9426047503948212}]}, {"text": "In this paper we propose a simple but efficient singleton detection model.", "labels": [], "entities": [{"text": "singleton detection", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7897762656211853}]}, {"text": "We first perform intrinsic evaluations and show that our simple model significantly improves the state-of-the-art results in singleton detection by a large margin.", "labels": [], "entities": [{"text": "singleton detection", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8304196298122406}]}, {"text": "We then evaluate our singleton model extrinsically on coreference resolution showing that search space pruning improves different coreference resolution models.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.9609823524951935}, {"text": "coreference resolution", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8499575257301331}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the the CoNLL 2012 English development set.", "labels": [], "entities": [{"text": "CoNLL 2012 English development set", "start_pos": 29, "end_pos": 63, "type": "DATASET", "confidence": 0.9620758771896363}]}, {"text": " Table 2. For each mention type,", "labels": [], "entities": []}, {"text": " Table 2: Precision error ratio.", "labels": [], "entities": [{"text": "Precision error ratio", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9464835921923319}]}, {"text": " Table 3: Recall error ratio.", "labels": [], "entities": [{"text": "Recall error ratio", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9449249108632406}]}, {"text": " Table 4: Results on the English test set. All the improvements made by our singleton detection models are statistically significant.", "labels": [], "entities": [{"text": "English test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9742449522018433}, {"text": "singleton detection", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7774401009082794}]}]}