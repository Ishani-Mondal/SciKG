{"title": [{"text": "Generation from Abstract Meaning Representation using Tree Transducers", "labels": [], "entities": [{"text": "Generation from Abstract Meaning Representation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6442143499851227}]}], "abstractContent": [{"text": "Language generation from purely semantic representations is a challenging task.", "labels": [], "entities": [{"text": "Language generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.720629557967186}]}, {"text": "This paper addresses generating English from the Abstract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7849577416976293}]}, {"text": "The new method is trained statistically from AMR-annotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-to-string transducers to generate English.", "labels": [], "entities": []}, {"text": "The method relies on discriminative learning and an argument realization model to overcome data sparsity.", "labels": [], "entities": [{"text": "argument realization", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7207646667957306}]}, {"text": "Initial tests on held-out data show good promise despite the complexity of the task.", "labels": [], "entities": []}, {"text": "The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr", "labels": [], "entities": [{"text": "JAMR", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8295572400093079}]}], "introductionContent": [{"text": "We consider natural language generation from the Abstract Meaning Representation (AMR;.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6978536446889242}, {"text": "Abstract Meaning Representation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7036880056063334}]}, {"text": "AMR encodes the meaning of a sentence as a rooted, directed, acyclic graph, where concepts are nodes, and edges are relationships among the concepts.", "labels": [], "entities": []}, {"text": "Because AMR models propositional meaning 1 while abstracting away from surface syntactic realizations, and is designed with human annotation in mind, it suggests a separation of (i) engineering the application-specific propositions that need to be communicated about from (ii) general-purpose realization details, modeled by a generator shareable across many applications.", "labels": [], "entities": [{"text": "AMR", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9398281574249268}]}, {"text": "The latter is our focus here.", "labels": [], "entities": []}, {"text": "Because any AMR graph has numerous valid realizations, and leaves underspecified many important details-including tense, number, definiteness, whether a concept should be referred to nominally or verbally, and more-transforming an AMR graph into an English sentence is a nontrivial problem.", "labels": [], "entities": []}, {"text": "To our knowledge, our system is the first for generating English from AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7093374729156494}]}, {"text": "The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (.", "labels": [], "entities": [{"text": "statistical natural language generation (NLG)", "start_pos": 18, "end_pos": 63, "type": "TASK", "confidence": 0.729339280298778}, {"text": "AMR bank", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8648035526275635}]}, {"text": "It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model ().", "labels": [], "entities": []}, {"text": "The decoder bears a strong similarity to state-of-the-art machine translation systems (, but with a rule extraction approach tailored to the NLG problem.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7553212344646454}, {"text": "rule extraction", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7599361538887024}]}], "datasetContent": [{"text": "We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset.", "labels": [], "entities": [{"text": "AMR Annotation Release version 1.0 (LDC2014T12) dataset", "start_pos": 19, "end_pos": 74, "type": "DATASET", "confidence": 0.8503360748291016}]}, {"text": "We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set.", "labels": [], "entities": [{"text": "MT09 data", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9144551753997803}]}, {"text": "Statistics for this dataset and splits are given in.", "labels": [], "entities": []}, {"text": "We use a 5-gram language model trained with) on Gigaword (LDC2011T07), and use 100-best synthetic rules.", "labels": [], "entities": [{"text": "Gigaword (LDC2011T07", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8307886918385824}]}, {"text": "We evaluate with the Bleu scoring metric) (  Removing the basic and abstract rules has little impact on the results.", "labels": [], "entities": [{"text": "Bleu scoring metric", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.9028964638710022}]}, {"text": "This maybe because the synthetic rule model already contains much of the information in the basic and abstract rules.", "labels": [], "entities": []}, {"text": "Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Uncased Bleu scores with various types of  rules removed from the full system.", "labels": [], "entities": []}]}