{"title": [], "abstractContent": [{"text": "In this paper, we examine the impact of employing contextual, structural information from a tree-structured document set to derive a language model.", "labels": [], "entities": []}, {"text": "Our results show that this information significantly improves the accuracy of the resultant model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994205236434937}]}], "introductionContent": [{"text": "Conventional Language Models (LMs) are based on n-grams, and thus rely upon a limited number of preceding words to assign a probability to the next word in a document.", "labels": [], "entities": []}, {"text": "Recently, proposed a Recurrent Neural Network (RNN) LM which uses a vector representation of all the preceding words in a sentence as the context for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7006935030221939}]}, {"text": "This model, which theoretically can utilize an infinite context window within a sentence, yields an LM with lower perplexity than that of n-gram-based LMs.", "labels": [], "entities": []}, {"text": "However, the model does not leverage the wider contextual information provided by words in other sentences in a document or in related documents.", "labels": [], "entities": []}, {"text": "Several researchers have explored extending the contextual information of an RNN-based LM.", "labels": [], "entities": []}, {"text": "proposed a contextdependent RNN LM that employs Latent Dirichlet Allocation for modeling along span of context.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation", "start_pos": 48, "end_pos": 75, "type": "METRIC", "confidence": 0.7484280665715536}]}, {"text": "offered a bag-of-words representation of preceding sentences as the context for the RNN LM.", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.8559379577636719}]}, {"text": "used a DocumentContext LM (DCLM) to leverage both intra-and inter-sentence context.", "labels": [], "entities": []}, {"text": "These works focused on contextual information at the document level for LM, but did not consider information at the inter-document level.", "labels": [], "entities": []}, {"text": "Many document sets on the Internet are structured, which means there are connections between different documents.", "labels": [], "entities": []}, {"text": "This phenomenon is prominent in social media, where all the posts are directly linked to several other posts.", "labels": [], "entities": []}, {"text": "We posit that these related documents could hold important information about a particular post, including the topic and language use, and propose an RNN-based LM architecture that utilizes both intra-and inter-document contextual information.", "labels": [], "entities": []}, {"text": "Our approach, which was tested on the social media dataset reddit, yielded promising results, which significantly improve on the state of the art.", "labels": [], "entities": [{"text": "social media dataset reddit", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.8619428724050522}]}], "datasetContent": [{"text": "We used pre-collected reddit data, 1 which as of December, 2015, consists of approximately 1.7 billion comments in JSON format.", "labels": [], "entities": []}, {"text": "A comment thread starts with a \"topic\", which might be a link or an image.", "labels": [], "entities": []}, {"text": "The users then begin to comment on the topic, or reply to previous comments.", "labels": [], "entities": []}, {"text": "Over time, this process creates a tree-structured document repository, where a level indicator is assigned to each comment, e.g., a response to the root topic is assigned level 1, and the reply to a level n comment is assigned level n + 1.", "labels": [], "entities": []}, {"text": "We parsed the raw data in JSON format into a tree structure, removing threads that have less than three comments, contain deleted comments, or do not have comments above   RNN-LSTM.", "labels": [], "entities": []}, {"text": "Given a sentence {x t } t\u2208 , where x t is the vector representation of the t-th word in the sentence, and N is the length of the sentence, Mikolov et al.'s (2010) RNN LM can be defined as:", "labels": [], "entities": []}], "tableCaptions": []}