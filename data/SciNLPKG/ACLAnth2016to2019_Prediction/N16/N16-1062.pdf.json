{"title": [{"text": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "labels": [], "entities": [{"text": "Sequential Short-Text Classification", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7689833243687948}]}], "abstractContent": [{"text": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.757615864276886}]}, {"text": "However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one.", "labels": [], "entities": []}, {"text": "In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art results on three different datasets for dialog act prediction .", "labels": [], "entities": [{"text": "dialog act prediction", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.8148923714955648}]}], "introductionContent": [{"text": "Short-text classification is an important task in many areas of natural language processing, including sentiment analysis, question answering, or dialog management.", "labels": [], "entities": [{"text": "Short-text classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7598998248577118}, {"text": "natural language processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.669156551361084}, {"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9692350625991821}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8936483860015869}, {"text": "dialog management", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.8391874134540558}]}, {"text": "Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features), combining SVMs with naive Bayes (, and building dependency trees with Conditional Random Fields (.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7022310197353363}]}, {"text": "Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs)) and recursive neural networks ().", "labels": [], "entities": []}, {"text": "Most ANN systems classify short texts in isolation, i.e., without considering preceding short * These authors contributed equally to this texts.", "labels": [], "entities": [{"text": "ANN", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9428295493125916}]}, {"text": "However, short texts usually appear in sequence (e.g., sentences in a document or utterances in a dialog), and therefore using information from preceding short texts may improve the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.950739324092865}]}, {"text": "Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs)), maximum entropy (), naive Bayes (, and conditional random fields (CRFs) (.", "labels": [], "entities": [{"text": "sequential short-text classification", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7761935591697693}]}, {"text": "Inspired by the performance of ANN-based systems for non-sequential short-text classification, we introduce a model based on recurrent neural networks (RNNs) and CNNs for sequential short-text classification, and evaluate it on the dialog act classification task.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7839022576808929}, {"text": "sequential short-text classification", "start_pos": 171, "end_pos": 207, "type": "TASK", "confidence": 0.627925435702006}, {"text": "dialog act classification task", "start_pos": 232, "end_pos": 262, "type": "TASK", "confidence": 0.7696072161197662}]}, {"text": "A dialog act characterizes an utterance in a dialog based on a combination of pragmatic, semantic, and syntactic criteria.", "labels": [], "entities": []}, {"text": "Its accurate detection is useful fora range of applications, from speech recognition to automatic summarization ().", "labels": [], "entities": [{"text": "accurate detection", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.7480032444000244}, {"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8824031352996826}, {"text": "summarization", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.7384231686592102}]}, {"text": "Our model achieves state-of-the-art results on three different datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on the dialog act classification task using the following datasets: \u2022 DSTC 4: Dialog State Tracking Challenge 4 ().", "labels": [], "entities": [{"text": "dialog act classification task", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8383767753839493}, {"text": "DSTC 4", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8760667741298676}, {"text": "Dialog State Tracking Challenge 4", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.797500491142273}]}, {"text": "\u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus ().", "labels": [], "entities": [{"text": "MRDA: ICSI Meeting Recorder Dialog Act Corpus", "start_pos": 2, "end_pos": 47, "type": "DATASET", "confidence": 0.8763572573661804}]}, {"text": "The 5 classes are introduced in ().", "labels": [], "entities": []}, {"text": "\u2022 SwDA: Switchboard Dialog Act Corpus (", "labels": [], "entities": [{"text": "Switchboard Dialog Act Corpus", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.6807155609130859}]}], "tableCaptions": [{"text": " Table 1: Dataset overview. |C| is the number of classes, |V |  the vocabulary size. For the train, validation and test sets, we  indicate the number of dialogs (i.e., sequences) followed by the  number of utterances (i.e., short texts) in parenthesis.", "labels": [], "entities": []}, {"text": " Table 2: Experiments ranges and choices of hyperparameters.  Unidir refers to the regular RNNs presented in Section 2.1.1,  and bidir refers to bidirectional RNNs introduced in (Schuster  and Paliwal, 1997).", "labels": [], "entities": []}, {"text": " Table 3: Accuracy (%) on different architectures and history sizes d1, d2. For each setting, we report average (minimum, maximum)  computed on 5 runs. Sequential classification (d1 + d2 > 0) outperforms non-sequential classification (d1 = d2 = 0). Overall,  the CNN model outperformed the LSTM model for all datasets, albeit by a small margin except for SwDA. We also tried gated  recurrent units (GRUs) (", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965575933456421}]}, {"text": " Table 4: Accuracy (%) of our models and other methods from  the literature. The majority class model predicts the most fre- quent class. SVM:", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981385469436646}]}]}