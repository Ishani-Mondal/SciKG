{"title": [{"text": "K-Embeddings: Learning Conceptual Embeddings for Words using Context", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a technique for adding contex-tual distinctions to word embeddings by extending the usual embedding process-into two phases.", "labels": [], "entities": []}, {"text": "The first phase resembles existing methods, but also constructs K classifications of concepts.", "labels": [], "entities": []}, {"text": "The second phase uses these classifications in developing refined K embed-dings for words, namely word K-embeddings.", "labels": [], "entities": []}, {"text": "The technique is iterative, scalable, and can be combined with other methods (including Word2Vec) in achieving still more expressive representations.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9264137744903564}]}, {"text": "Experimental results show consistently large performance gains on a Semantic-Syntactic Word Relationship test set for different K settings.", "labels": [], "entities": []}, {"text": "For example, an overall gain of 20% is recorded at K = 5.", "labels": [], "entities": [{"text": "K", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9765399694442749}]}, {"text": "In addition, we demonstrate that an iterative process can further tune the embeddings and gain an extra 1% (K = 10 in 3 iterations) on the same benchmark.", "labels": [], "entities": []}, {"text": "The examples also show that polysemous concepts are meaningfully embedded in our K different conceptual embeddings for words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural-based word embeddings are vectorial representations of words in high dimensional real valued space.", "labels": [], "entities": []}, {"text": "Success with these representations have resulted in their being considered for an increasing range of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Recent advances in word embeddings have shown great effects that are pushing forward state-of-the-art results in NLP (.", "labels": [], "entities": []}, {"text": "Embedding learning models for words are also being adapted for tasks in other research fields.", "labels": [], "entities": []}, {"text": "The Continuous bag of words (CBOW) and Skip-gram () are currently considered as state-of-theart in learning algorithms for word embeddings.", "labels": [], "entities": []}, {"text": "The ability of words to assume different roles (syntax) or meanings (semantics) presents a basic challenge to the notion of word embedding.", "labels": [], "entities": []}, {"text": "External resources and features are introduced to address this challenge.", "labels": [], "entities": []}, {"text": "In general, individuals with no linguistic background can generally resolve these differences without difficulty.", "labels": [], "entities": []}, {"text": "For example, they can distinguish \"bank\" as referring to a riverside or a financial establishment without semantic or syntactic analysis.", "labels": [], "entities": []}, {"text": "Distinctions of role and meaning often follow from context.", "labels": [], "entities": []}, {"text": "The idea of exploiting context in linguistics was introduced with a distributional hypothesis: \"linguistic items with similar distributions have similar meanings\".", "labels": [], "entities": []}, {"text": "Firth soon afterwards emphasized this in a famous quote: \"a word is characterized by the company it keeps\".", "labels": [], "entities": [{"text": "Firth", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8577651977539062}]}, {"text": "We propose to exploit only context information to distinguish different concepts behind words in this paper.", "labels": [], "entities": []}, {"text": "The contribution of this paper is to note that a two-phase word embedding training can be helpful in adding contextual information to existing embedding methods: \u2022 we use learned context embeddings to effi-ciently cluster word contexts into K classifications of concepts, independent of the word embeddings.", "labels": [], "entities": []}, {"text": "\u2022 this approach can complement existing sophisticated, linguistically-based features, and can be used with word embeddings to achieve gains in performance by considering contextual distinctions for words.", "labels": [], "entities": []}, {"text": "\u2022 two-phase word embedding may have other applications as well, conceivably permitting some 'non-linear' refinements of linear embeddings.", "labels": [], "entities": []}, {"text": "In the next section we present our learning strategy for word K-embeddings, outlining how the value of K affects its power in increasing syntactic and semantic distinctions.", "labels": [], "entities": []}, {"text": "Following this, a largescale experiment serves to validate the idea -from several different perspectives.", "labels": [], "entities": []}, {"text": "Finally, we offer conclusions about how adding contextual distinctions to word embeddings (with our second phase of embedding) can gain power in distinguishing among different aspects of words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Total embeddings and vocabulary size for different K", "labels": [], "entities": []}, {"text": " Table 3: Performance of K = 10 in five iterations", "labels": [], "entities": []}]}