{"title": [{"text": "An End-to-end Approach to Learning Semantic Frames with Feedforward Neural Network", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an end-to-end method for learning verb-specific semantic frames with feedfor-ward neural network (FNN).", "labels": [], "entities": []}, {"text": "Previous works in this area mainly adopt a multi-step procedure including part-of-speech tagging, dependency parsing and soon.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7433024048805237}, {"text": "dependency parsing", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8945770263671875}]}, {"text": "On the contrary, our method uses a FNN model that maps verb-specific sentences directly to semantic frames.", "labels": [], "entities": []}, {"text": "The simple model gets good results on annotated data and has a good generalization ability.", "labels": [], "entities": []}, {"text": "Finally we get 0.82 F-score on 63 verbs and 0.73 F-score on 407 verbs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9854564070701599}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9843820929527283}]}], "introductionContent": [{"text": "Lexical items usually have particular requirements for their semantic roles.", "labels": [], "entities": []}, {"text": "Semantic frames are the structures of the linked semantic roles near the lexical items.", "labels": [], "entities": []}, {"text": "A semantic frame specifies its characteristic interactions with things necessarily or typically associated with it).", "labels": [], "entities": []}, {"text": "It is valuable to build such resources.", "labels": [], "entities": []}, {"text": "These resources can be effectively used in many natural language processing (NLP) tasks, such as question answering () and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.9056843817234039}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8174364268779755}]}, {"text": "Current semantic frame resources, such as FrameNet (),) and VerbNet (, have been manually created.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8280363082885742}]}, {"text": "These resources have promising applications, but they are time-consuming and expensive.", "labels": [], "entities": []}, {"text": "El Maarouf and used a * The corresponding author.", "labels": [], "entities": []}, {"text": "bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English 1 (PDEV).", "labels": [], "entities": [{"text": "Pattern Dictionary of English 1 (PDEV)", "start_pos": 59, "end_pos": 97, "type": "DATASET", "confidence": 0.6712184995412827}]}, {"text": "El used a Support Vector Machine (SVM) model to classify the patterns of PDE-V . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment.", "labels": [], "entities": []}, {"text": "But the models above are tested only on 25 verbs and they are not end-to-end.", "labels": [], "entities": []}, {"text": "Popescu used Finite State Automata (FSA) to learn the pattern of semantic frames.", "labels": [], "entities": []}, {"text": "But the generalization ability of this rule-based method maybe weak.", "labels": [], "entities": []}, {"text": "Recently, some unsupervised studies have focused on acquiring semantic frames from raw corpora).", "labels": [], "entities": []}, {"text": "Materna used LDA-Frame for identifying semantic frames based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process.", "labels": [], "entities": []}, {"text": "Chinese Restaurant Process to induce semantic frames from a syntactically annotated corpus.", "labels": [], "entities": []}, {"text": "These unsupervised approaches have a different goal compared with supervised approaches.", "labels": [], "entities": []}, {"text": "They aim at identifying the semantic frames by clustering the parsed sentences but they do not learn from semantic frames that have been built.", "labels": [], "entities": []}, {"text": "These unsupervised approaches are also under a pipeline framework and not end-to-end.", "labels": [], "entities": []}, {"text": "One related resource to our work is Corpus Pattern Analysis (CPA) frames.", "labels": [], "entities": [{"text": "Corpus Pattern Analysis (CPA) frames", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.8251816545213971}]}, {"text": "CPA proposes a heuristic procedure to obtain semantic frames.", "labels": [], "entities": [{"text": "CPA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9176165461540222}]}, {"text": "Most current supervised and unsupervised approaches are under similar pipeline procedure.", "labels": [], "entities": []}, {"text": "The procedure can be summarized as follows with an example sentence \"The old music deeply moved 1 the old man\": step 1 Identify the arguments near \"moved\", which can be expressed as (subject:music, object:man) step 2 Attach meanings to above arguments, which can be expressed as (subject:Entity, object:Human) step 3 Clustering or classifying the arguments to get semantic frames.", "labels": [], "entities": []}, {"text": "However, step 1 and 2 are proved to be difficult in SemEval-2015 task 15.", "labels": [], "entities": [{"text": "SemEval-2015 task 15", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7926812966664633}]}, {"text": "This paper presents an end-to-end approach by directly learning semantic frames from verb-specific sentences.", "labels": [], "entities": []}, {"text": "One key component of our model is well pre-trained word vectors.", "labels": [], "entities": []}, {"text": "These vectors capture fine-grained semantic and syntactic regularities () and make our model have a good generalization ability.", "labels": [], "entities": []}, {"text": "Another key component is FNN model.", "labels": [], "entities": [{"text": "FNN", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.3858909010887146}]}, {"text": "A supervised signal allows FN-N to learn the semantic frames directly.", "labels": [], "entities": []}, {"text": "As a result, this simple model achieves good results.", "labels": [], "entities": []}, {"text": "On the instances resources of PDEV, we got 0.82 F-score on 63 verbs and 0.73 on 407 verbs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9963120818138123}]}, {"text": "The contributions of this paper are summarized as follows: \u2022 Semantic frames can be learned with neural network in an end-to-end map and we also analysed our method in detail.", "labels": [], "entities": []}, {"text": "\u2022 We showed the power of pre-trained vectors and simple neural network for the learning of semantic frames.", "labels": [], "entities": [{"text": "learning of semantic frames", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7847377210855484}]}, {"text": "It is helpful in developing a more powerful approach.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate the learned semantic frames on annotated data precisely and got good results with not much training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "SemEval-2015 Task 15 is a CPA (Hanks, 2012) dictionary entry building task.", "labels": [], "entities": [{"text": "CPA (Hanks, 2012) dictionary entry building task", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.7367722451686859}]}, {"text": "The task has three subtasks.", "labels": [], "entities": []}, {"text": "Two related subtasks are summarized as follows 3 : \u2022 CPA parsing.", "labels": [], "entities": [{"text": "CPA parsing", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7600790560245514}]}, {"text": "This task requires identifying syntactic arguments and their semantic type of the target verb.", "labels": [], "entities": []}, {"text": "The result of this task followed by our example sentence can be \"The old [subject/Entity music] deeply moved the old [object/Human man]\".", "labels": [], "entities": []}, {"text": "The syntactic arguments in the example are \"subject\" and \"object\" respectively labelled on the word \"music\" and \"man\".", "labels": [], "entities": []}, {"text": "Their semantic types are \"Entity\" and \"Human\".", "labels": [], "entities": []}, {"text": "Thus a pattern of the target verb \"move\" can be \"[subject/Entity] move [object/Human]\".", "labels": [], "entities": []}, {"text": "The result of the first task give the patterns of the sentences.", "labels": [], "entities": []}, {"text": "This task aims at clustering the most similar sentences according to the found patterns.", "labels": [], "entities": []}, {"text": "Two sentences which belong to the similar pattern are more likely in the same cluster.", "labels": [], "entities": []}, {"text": "SemEval-2015 Task 15 has two datasets which are called Microcheck dataset and Wingspread dataset.", "labels": [], "entities": [{"text": "SemEval-2015 Task 15", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.6872310439745585}, {"text": "Microcheck dataset", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9836580157279968}, {"text": "Wingspread dataset", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.9518642127513885}]}, {"text": "The dataset of SemEval-2015 Task 15 was derived from PDEV (.", "labels": [], "entities": [{"text": "SemEval-2015 Task 15", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7579991022745768}, {"text": "PDEV", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9392223358154297}]}, {"text": "That is to say, all the sentences in SemEval-2015 Task 15 are from PDE-V.", "labels": [], "entities": [{"text": "SemEval-2015 Task 15", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7531347274780273}, {"text": "PDE-V", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.906919002532959}]}, {"text": "These datasets have a lot of verbs and have many sentences for each verb.", "labels": [], "entities": []}, {"text": "Each sentence of each verb corresponds to one index of the semantic frames.", "labels": [], "entities": []}, {"text": "Note that the semantic frames are verb-specific and each verb has a close set of its own semantic frames.", "labels": [], "entities": []}, {"text": "Thus in our experiment we build one model for each verb.", "labels": [], "entities": []}, {"text": "Our task is to classify each sentence directly into one semantic frame which is different from C-PA clustering, but we will also test our model with clustering metric against other systems.", "labels": [], "entities": [{"text": "C-PA clustering", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.6257990747690201}]}, {"text": "We only remove punctuation for these datasets.", "labels": [], "entities": []}, {"text": "To test our model we split these datasets into training data and testing data.", "labels": [], "entities": []}, {"text": "Summary statistics of the these datasets are in.", "labels": [], "entities": []}, {"text": "In, and, Verb number is the number of verbs, Training data and Testing data represent the average number of sentences for each verb and Semantic frame number is the average number of semantic frames for each verb.", "labels": [], "entities": [{"text": "Verb number", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9522479474544525}]}, {"text": "Details of creating the datasets are as follows: \u2022 MTDSEM: Microcheck test dataset of SemEval-2015 Task 15.", "labels": [], "entities": [{"text": "MTDSEM", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.5834577679634094}, {"text": "Microcheck test dataset of SemEval-2015 Task 15", "start_pos": 59, "end_pos": 106, "type": "DATASET", "confidence": 0.7724077531269619}]}, {"text": "For each verb in MTDSEM we select training sentences from PDEV that doesn't appear in MTDSEM.", "labels": [], "entities": [{"text": "MTDSEM", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.6174845695495605}, {"text": "MTDSEM", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8169331550598145}]}, {"text": "\u2022 PDEV1: For each verb, we filter PDEV with the number of sentences not less than 100 and the number of semantic frames not less than 2.", "labels": [], "entities": []}, {"text": "Then we split the filtered data into training data and testing data, respectively accounted for 70% and 30% for each semantic frame of each verb.", "labels": [], "entities": []}, {"text": "\u2022 PDEV2: Same with PDEV1, but with the difference of threshold number of sentences set to 700.", "labels": [], "entities": []}, {"text": "PDEV2 ensures that the model has relatively enough training data.", "labels": [], "entities": [{"text": "PDEV2", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8964360356330872}]}, {"text": "\u2022 MTTSEM: Microcheck train dataset and test dataset of SemEval-2015 Task 15.", "labels": [], "entities": [{"text": "MTTSEM", "start_pos": 2, "end_pos": 8, "type": "DATASET", "confidence": 0.6315183639526367}, {"text": "Microcheck train dataset", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.7199413577715555}, {"text": "SemEval-2015 Task 15", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.6547653675079346}]}, {"text": "We split MTTSEM as above to get training data and testing data for each verb.", "labels": [], "entities": [{"text": "MTTSEM", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.5826672315597534}]}, {"text": "The summary statistic of this dataset is separately shown in.", "labels": [], "entities": []}, {"text": "We use the publicly available word2vec vectors that were trained through GloVe model () on Wikipedia and Gigaword.", "labels": [], "entities": []}, {"text": "The vectors have dimensionality of 300.", "labels": [], "entities": []}, {"text": "The word vectors not in pre-trained vectors are set to zero.", "labels": [], "entities": []}, {"text": "We build one model for each verb.", "labels": [], "entities": []}, {"text": "Training is done by stochastic gradient descent with shuffled minibatches and we keep the word vectors static only update other parameters.", "labels": [], "entities": []}, {"text": "In our experiments we keep all the same hyperparameters for each verb.", "labels": [], "entities": []}, {"text": "we set learning rate to 0.1, lw and rw to 5, minibatch size to 5, L2 regularization parameter \u03b2 to 0.0001, the number of hidden unit to 30 and \u03bb to 0.", "labels": [], "entities": [{"text": "minibatch size", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9619258046150208}]}, {"text": "Because of limited training data, we do not use early stopping.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7731322050094604}]}, {"text": "Training will stop when the zero-one loss is zero over training data for each verb.", "labels": [], "entities": []}, {"text": "The official evaluation method used B-cubed definition of Precision and Recall (Bagga and Baldwin, 1998) for CPA clustering.", "labels": [], "entities": [{"text": "B-cubed", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9545599818229675}, {"text": "Precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9910905361175537}, {"text": "Recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8092570304870605}, {"text": "CPA clustering", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7938563823699951}]}, {"text": "The final score is the average of B-cubed F-scores overall verbs.", "labels": [], "entities": [{"text": "B-cubed F-scores", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.818792998790741}]}, {"text": "Since our task can be regarded as a supervised classification, we also use the micro-average F-score to evaluate our results.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.8522163033485413}]}, {"text": "same cluster to all the sentences and is evaluated by B-cubed F-score for clustering.", "labels": [], "entities": [{"text": "B-cubed", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9842438697814941}, {"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.5165335536003113}]}, {"text": "So its score depends on the distribution of semantic frames.", "labels": [], "entities": []}, {"text": "The higher the score is, the more concentrated the distribution of semantic frames is.", "labels": [], "entities": []}, {"text": "SEB to get higher score usually indicates other methods are more likely to get high scores, so we use it as abase score.", "labels": [], "entities": [{"text": "SEB", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9763134121894836}]}, {"text": "DU-LUTH (Pedersen, 2015) treated this task as an unsupervised word sense discrimination or induction problem.", "labels": [], "entities": [{"text": "DU-LUTH", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8700597286224365}, {"text": "word sense discrimination", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6387965977191925}]}, {"text": "The number of semantic frames was predicted on the basis of the best value for the clustering criterion function.", "labels": [], "entities": []}, {"text": "BOB90 4 used a supervised approach to tackle the clustering problem ( and get the best score on MTDSEM.", "labels": [], "entities": [{"text": "BOB90", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9360053539276123}, {"text": "clustering", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9672644734382629}, {"text": "MTDSEM", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.7165838479995728}]}, {"text": "An example result of FNN model on PDEV is shown in 4 Discussions", "labels": [], "entities": [{"text": "PDEV", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9239891171455383}]}], "tableCaptions": [{"text": " Table 1: Summary statistics for the datasets (left) and results of our FNN model against other methods (right). On the right side,", "labels": [], "entities": []}, {"text": " Table 2: Example results of our FNN model mapping verb-specific sentences to semantic frames on PDEV.", "labels": [], "entities": []}, {"text": " Table 3: Results on MTTSEM with different preprocessing.", "labels": [], "entities": [{"text": "MTTSEM", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.5563474297523499}]}]}