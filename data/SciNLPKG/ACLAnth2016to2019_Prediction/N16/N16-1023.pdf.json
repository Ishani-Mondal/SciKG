{"title": [{"text": "Stating the Obvious: Extracting Visual Common Sense Knowledge", "labels": [], "entities": [{"text": "Stating the Obvious", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8761253952980042}, {"text": "Extracting Visual Common Sense Knowledge", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.8713309049606324}]}], "abstractContent": [{"text": "Obtaining commonsense knowledge using current information extraction techniques is extremely challenging.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7353056073188782}]}, {"text": "In this work, we instead propose to derive simple commonsense statements from fully annotated object detection corpora such as the Microsoft Common Objects in Context dataset.", "labels": [], "entities": [{"text": "Microsoft Common Objects in Context dataset", "start_pos": 131, "end_pos": 174, "type": "DATASET", "confidence": 0.5692285597324371}]}, {"text": "We show that many thousands of commonsense facts can be extracted from such corpora at high quality.", "labels": [], "entities": []}, {"text": "Furthermore, using WordNet and a novel sub-modular k-coverage formulation, we are able to generalize our initial set of commonsense assertions to unseen objects and uncover over 400k potentially useful facts.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.962696373462677}]}], "introductionContent": [{"text": "How can we discover that bowls can hold broccoli, that if a knife touches a cake then a person is probably cutting cake, or that cutlery can be on dining tables?", "labels": [], "entities": []}, {"text": "We propose to leverage the effort of computer vision researchers in creating large scale datasets for object detection and use these resources instead to extract symbolic representations of visual commonsense.", "labels": [], "entities": [{"text": "object detection", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7422084659337997}]}, {"text": "The knowledge we compile is physical, not commonly covered in text and more exhaustive than what people can usually produce.", "labels": [], "entities": []}, {"text": "Our focus is particularly on visual commonsense, defined as the information about spatial and functional properties of entities in the world.", "labels": [], "entities": []}, {"text": "We propose to extract three types of knowledge from the Microsoft Common Objects in Context dataset () (MS-COCO), consisting of 300,000 images, covering 80 objects, with object segments and natural language captions.", "labels": [], "entities": [{"text": "Microsoft Common Objects in Context dataset () (MS-COCO)", "start_pos": 56, "end_pos": 112, "type": "DATASET", "confidence": 0.6287204444408416}]}, {"text": "First, we find spatial relations, e.g. holds(bed, dog), from outlines of co-occurring objects.", "labels": [], "entities": []}, {"text": "Next, we construct entailment rules like holds(bed, dog) \u21d2 laying-on(dog, bed) by associating spatial relations with text in captions.", "labels": [], "entities": []}, {"text": "Finally, we uncover general facts such as holds(furniture, domestic animal), applicable to object types not present in MS-COCO by using WordNet and a novel submodular kcoverage formulation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9740675091743469}]}, {"text": "Evaluations using crowdsourcing show our methods can discover many thousands of high quality explicit statements of visual commonsense.", "labels": [], "entities": []}, {"text": "While some of this knowledge can be potentially extracted from text (, we found that from our top 100 extracted spatial relations, e.g. holds(bed, dog), only 4 are present in some form in the AtLocation relations in the popular ConceptNet knowledge base.", "labels": [], "entities": [{"text": "ConceptNet knowledge base", "start_pos": 228, "end_pos": 253, "type": "DATASET", "confidence": 0.7291139662265778}]}, {"text": "This shows that the knowledge we derive provides complimentary information for other more general knowledge bases.", "labels": [], "entities": []}, {"text": "Such commonsense facts have proved useful for query expansion) and could benefit entailment (, grounded entailment (, or visual recognition tasks ().", "labels": [], "entities": [{"text": "query expansion", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8167087137699127}]}], "datasetContent": [{"text": "Object-Object Relations: We filter out from the initial set of candidate relations the ones that occur less than 20 times.", "labels": [], "entities": []}, {"text": "We extract more than 3.1k unique statements (6k including symmetric spatial relations).", "labels": [], "entities": []}, {"text": "Entailment Relations: We use skipgrams of length 2-6 allowing at most 6 skips, filter candidates such that they occur at least 5 times, and return the top 10 most likely entailments per spatial relation.", "labels": [], "entities": []}, {"text": "Overall, 6.3k unique statements are extracted (10k including symmetric relations).", "labels": [], "entities": []}, {"text": "Generalized Relations: We optimize Equation 4 only for object-object relations because the closed world assumption makes counts for implications sparse.", "labels": [], "entities": []}, {"text": "The parameter \u00b5 is set to the average of the scores, \u03bb = 0.05 and k = 200.", "labels": [], "entities": []}, {"text": "We evaluated the quality of the commonsense we derive on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.9794436097145081}]}, {"text": "Annotators are presented with possible facts and asked to grade statements on a five point scale.", "labels": [], "entities": []}, {"text": "Each fact was evaluated by 10 workers and we normalize their average responses to a scale from 0 to 1.", "labels": [], "entities": []}, {"text": "shows plots of quality vs. coverage, where coverage means the top percent of relations sorted by our predicted quality scores.", "labels": [], "entities": []}, {"text": "Object-Object Relations As a baseline, 1000 randomly sampled relations have a quality of 0.225.", "labels": [], "entities": []}, {"text": "shows our PMI measure ranks many high quality facts at the top, with the top quintile of the ranking being rated above 0.63 in quality.", "labels": [], "entities": [{"text": "PMI measure", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7546958923339844}]}, {"text": "Facts about persons are higher quality, likely because this category is in over 50% of the images in MS-COCO.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9574640989303589}]}, {"text": "all person animal vehicle furniture random relations (a) Object-object relations all person animal vehicle furniture  Entailment Relations Turkers were instructed to assign the lowest score when they could not understand the consequent of the entailment relation.", "labels": [], "entities": []}, {"text": "As a baseline, 1000 randomly sampled implications that meet our patterns have a quality of 0.33.", "labels": [], "entities": []}, {"text": "shows that extracting high quality entailment is harder than object-object relations likely because supposition and consequent need to coordinate.", "labels": [], "entities": []}, {"text": "Relations involving furniture are rated higher and manual inspection revealed that many relations about furniture imply stative verbs or spatial terms.", "labels": [], "entities": []}, {"text": "Generalized Relations To evaluate generalizations,, we also present users with definitions . As a baseline, 200 randomly sampled generalizations from our 3k object-object relations have a quality of 0.53.", "labels": [], "entities": []}, {"text": "Generalizations we find are high quality and cover over 400k objects facts not present in MS-COCO.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9444722533226013}]}, {"text": "Examples from the 200 we derive include: holds(dining-table, cutlery), holds(bowl, edible fruit) or on(domestic animal, bed).", "labels": [], "entities": []}], "tableCaptions": []}