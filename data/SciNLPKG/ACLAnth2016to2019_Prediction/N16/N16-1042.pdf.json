{"title": [{"text": "Grammatical error correction using neural machine translation", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8598436514536539}, {"text": "neural machine translation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7677057186762491}]}], "abstractContent": [{"text": "This paper presents the first study using neu-ral machine translation (NMT) for grammatical error correction (GEC).", "labels": [], "entities": [{"text": "neu-ral machine translation (NMT)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.6806445767482122}, {"text": "grammatical error correction (GEC)", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.7292388379573822}]}, {"text": "We propose a two-step approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task.", "labels": [], "entities": [{"text": "GEC task", "start_pos": 128, "end_pos": 136, "type": "TASK", "confidence": 0.8654190599918365}]}, {"text": "Our best NMT-based system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set.", "labels": [], "entities": [{"text": "CLC", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9440506100654602}, {"text": "SMT-based", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.9696022868156433}, {"text": "FCE test set", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.9723546703656515}]}, {"text": "The same system achieves an F 0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperform-ing the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively .", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9900480111440023}, {"text": "CoNLL-2014 shared task test set", "start_pos": 57, "end_pos": 88, "type": "DATASET", "confidence": 0.879768681526184}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in text written by non-native English writers.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.835258940855662}, {"text": "detecting and correcting grammatical errors in text written by non-native English writers", "start_pos": 50, "end_pos": 139, "type": "TASK", "confidence": 0.8385147924224535}]}, {"text": "Unlike building machine learning classifiers for specific error types (e.g. determiner or preposition errors), the idea of 'translating' a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously).", "labels": [], "entities": []}, {"text": "Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task ( . Recently, several neural machine translation (NMT) models have been developed with promising results.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8195418814818064}, {"text": "GEC", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9499372243881226}, {"text": "neural machine translation (NMT)", "start_pos": 172, "end_pos": 204, "type": "TASK", "confidence": 0.8199603855609894}]}, {"text": "Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model), NMT learns a single large neural network which inputs a sentence and outputs a translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9865459203720093}]}, {"text": "NMT is appealing for GEC as it maybe possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9208309650421143}, {"text": "GEC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.509530782699585}]}, {"text": "NMTbased systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC.", "labels": [], "entities": []}, {"text": "However, NMT models typically limit vocabulary size on both source and target sides due to the complexity of training (.", "labels": [], "entities": []}, {"text": "Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are replaced with UNK symbol.", "labels": [], "entities": []}, {"text": "This problem is more serious for GEC as non-native text contains not only rare words (e.g. proper nouns), but also misspelled words (i.e. spelling errors).", "labels": [], "entities": [{"text": "GEC", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7433546781539917}]}, {"text": "By replacing all the OOV words with the same UNK symbol, useful information is discarded, resulting in systems that are notable to correct misspelled words or even keep some of the error-free original words, as in the following examples (OOV words are underlined): Original sentence ...", "labels": [], "entities": []}, {"text": "I am goign to make a plan ...", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the publicly available FCE dataset), which is apart of the Cambridge Learner Corpus (CLC) As we can see, the source side vocabulary size is much larger than that of the target side.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9509094655513763}, {"text": "Cambridge Learner Corpus (CLC", "start_pos": 66, "end_pos": 95, "type": "DATASET", "confidence": 0.9400185942649841}]}, {"text": "Training and test data is pre-processed using RASP ().", "labels": [], "entities": [{"text": "RASP", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.44259709119796753}]}, {"text": "System performance is evaluated using three automatic evaluation metrics: I-measure, M 2 Scorer (Dahlmeier and Ng, 2012) and GLEU ().", "labels": [], "entities": [{"text": "I-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.8922016024589539}, {"text": "M 2 Scorer", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9317662517229716}, {"text": "GLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.998422384262085}]}, {"text": "In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source).", "labels": [], "entities": [{"text": "I-measure", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.8852730989456177}, {"text": "Improvement (I) score", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.9755030512809754}]}, {"text": "The M 2 Scorer was the official scorer in the CoNLL shared tasks ( ), with F 0.5 being the reported metric in the 2014 edition.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8374281128247579}, {"text": "CoNLL shared tasks", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.6794770558675131}, {"text": "F 0.5", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9859554469585419}]}, {"text": "GLEU is a simple variant of BLEU (), which shows better correlation with human judgments on the CoNLL-2014 shared task test set.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9320297837257385}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9978783130645752}, {"text": "CoNLL-2014 shared task test set", "start_pos": 96, "end_pos": 127, "type": "DATASET", "confidence": 0.8997054100036621}]}], "tableCaptions": [{"text": " Table 1: System performance on the FCE test set (in percent-", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.96403968334198}]}, {"text": " Table 2: System performance on the CoNLL-2014 test set with-", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9834837317466736}]}]}