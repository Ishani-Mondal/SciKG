{"title": [{"text": "\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier", "labels": [], "entities": [{"text": "Explaining the Predictions", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.868095854918162}]}], "abstractContent": [{"text": "Despite widespread adoption in NLP, machine learning models remain mostly black boxes.", "labels": [], "entities": []}, {"text": "Understanding the reasons behind predictions is, however, quite important in assessing trust in a model.", "labels": [], "entities": []}, {"text": "Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy anew model.", "labels": [], "entities": []}, {"text": "In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner.", "labels": [], "entities": [{"text": "LIME", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9832707047462463}]}, {"text": "We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner.", "labels": [], "entities": []}, {"text": "We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection , and sentiment analysis, with classifiers like neural networks and SVMs.", "labels": [], "entities": [{"text": "document classification", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7893215119838715}, {"text": "politeness detection", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8796953558921814}, {"text": "sentiment analysis", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9590341746807098}]}, {"text": "The user interactions include explanations of free-form text, challenging users to identify the better clas-sifier from a pair, and perform basic feature engineering to improve the classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning is at the core of many recent advances in natural language processing.", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8052034676074982}, {"text": "natural language processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6487391591072083}]}, {"text": "Unfortunately, the important role of humans is an oft-overlooked aspect in the field.", "labels": [], "entities": []}, {"text": "Whether humans are directly using machine learning classifiers as tools, or are deploying models into products that need to be shipped, a vital concern remains: if the users do not trust a model or a prediction, they will not use it.", "labels": [], "entities": []}, {"text": "It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed \"in the wild\".", "labels": [], "entities": []}, {"text": "Both are directly impacted by how much the human understands a model's behavior, as opposed to seeing it as a black box.", "labels": [], "entities": []}, {"text": "Recent resurgence of neural networks has resulted in state-of-art models whose working is quite opaque to the user, exacerbating this problem.", "labels": [], "entities": []}, {"text": "A common surrogate for ascertaining trust in a model is to evaluate accuracy on held-out annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9988716244697571}]}, {"text": "However, there are several ways this evaluation can go wrong.", "labels": [], "entities": []}, {"text": "Data leakage, for example, defined as the unintentional leakage of signal into the training (and validation) data that would not occur in the wild), potentially increases accuracy.", "labels": [], "entities": [{"text": "Data leakage", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6180757731199265}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9972048401832581}]}, {"text": "Practitioners are also known to overestimate the accuracy of their models based on cross validation (, as real-world data is often significantly different.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999077320098877}]}, {"text": "Another particularly hard to detect problem is dataset shift), where training data is different than test data.", "labels": [], "entities": []}, {"text": "Further, there is frequently a mismatch between that metrics that we can compute and optimize (e.g. accuracy) and the actual metrics of interest such as user engagement and retention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9976096153259277}]}, {"text": "A practitioner may wish to choose a less accurate model for content recommendation that does not place high importance in features related to \"clickbait\" articles (which may hurt user retention), even if exploiting such features increases the accuracy of the model in cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9984813332557678}]}, {"text": "In this paper, we describe a system that explains why a classifier made a prediction by identifying useful portions of the input.", "labels": [], "entities": []}, {"text": "It has been observed that providing an explanation can increase the acceptance of computer-generated movie recommendations () and other automated systems (, and we explore their utility for NLP.", "labels": [], "entities": []}, {"text": "Specifically, we present: \u2022 LIME, an algorithm that can explain the predictions of any classifier, by approximating it locally with an interpretable model.", "labels": [], "entities": [{"text": "LIME", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9945136308670044}]}, {"text": "\u2022 SP-LIME, a method that selects a set of representative explanations to address the \"trusting the model\" problem, via submodular optimization.", "labels": [], "entities": []}, {"text": "\u2022 A demonstration designed to present the benefits of these explanation methods, on multiple NLP classification applications, classifier algorithms, and trust-related tasks.", "labels": [], "entities": [{"text": "NLP classification", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.798907607793808}]}], "datasetContent": [], "tableCaptions": []}