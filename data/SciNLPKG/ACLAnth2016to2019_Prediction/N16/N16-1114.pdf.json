{"title": [{"text": "Learning Global Features for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.958579957485199}]}], "abstractContent": [{"text": "There is compelling evidence that corefer-ence prediction would benefit from modeling global information about entity-clusters.", "labels": [], "entities": [{"text": "corefer-ence prediction", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.9064741432666779}]}, {"text": "Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features.", "labels": [], "entities": []}, {"text": "We instead propose to use recurrent neural networks (RNNs) to learn latent , global representations of entity clusters directly from their mentions.", "labels": [], "entities": []}, {"text": "We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coref-erence system that outperforms the state of the art without requiring any additional search.", "labels": [], "entities": [{"text": "prediction of pronominal mentions", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.8825656026601791}]}], "introductionContent": [{"text": "While structured, non-local coreference models would seem to hold promise for avoiding many common coreference errors (as discussed further in Section 3), the results of employing such models in practice are decidedly mixed, and state-of-the-art results can be obtained using a completely local, mention-ranking system.", "labels": [], "entities": []}, {"text": "In this work, we posit that global context is indeed necessary for further improvements in coreference resolution, but argue that informative cluster, rather than mention, level features are very difficult to devise, limiting their effectiveness.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.974961906671524}]}, {"text": "Accordingly, we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network (shown in Section 4).", "labels": [], "entities": []}, {"text": "Our model has no manually defined cluster features, but instead learns a global representation from the individual mentions present in each cluster.", "labels": [], "entities": []}, {"text": "We incorporate these representations into a mention-ranking style coreference system.", "labels": [], "entities": []}, {"text": "The entire model, including the recurrent neural network and the mention-ranking sub-system, is trained end-to-end on the coreference task.", "labels": [], "entities": []}, {"text": "We train the model as a local classifier with fixed context (that is, as a history-based model).", "labels": [], "entities": []}, {"text": "As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model.", "labels": [], "entities": []}, {"text": "Experiments compare the use of learned global features to several strong baseline systems for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.9655627906322479}]}, {"text": "We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems).", "labels": [], "entities": []}, {"text": "Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.6147250384092331}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in  order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and  Wiseman et al. (2015). F 1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared  with Wiseman et al. (2015) for all metrics.", "labels": [], "entities": [{"text": "CoNLL 2012 English test set", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.964124572277069}, {"text": "F 1 gains", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9620049993197123}]}, {"text": " Table 2: F 1 scores of models described in text on CoNLL  2012 development set. Rows in grey highlight models  using oracle history.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9666614333788554}, {"text": "CoNLL  2012 development set", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.97944575548172}]}]}