{"title": [{"text": "iAppraise: A Manual Machine Translation Evaluation Environment Supporting Eye-tracking", "labels": [], "entities": [{"text": "Manual Machine Translation Evaluation", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7165211066603661}]}], "abstractContent": [{"text": "We present iAppraise: an open-source framework that enables the use of eye-tracking for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.9843754470348358}]}, {"text": "It connects Appraise, an open-source toolkit for MT evaluation, to a low-cost eye-tracking device, to make its usage accessible to a broader audience.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.7751711010932922}, {"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9864507019519806}]}, {"text": "It also provides a set of tools for extracting and exploiting gaze data, which facilitate eye-tracking analysis.", "labels": [], "entities": [{"text": "eye-tracking analysis", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.8084500730037689}]}, {"text": "In this paper, we describe different modules of the framework, and explain how the tool can be used in a MT evaluation scenario.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9443577826023102}]}, {"text": "During the demonstration, the users will be able to perform an evaluation task, observe their own reading behavior during a replay of the session , and export and extract features from the data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is one of the difficult problems in Machine Translation (MT).", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9440252184867859}, {"text": "Machine Translation (MT)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8551538348197937}]}, {"text": "Despite its clear drawbacks, 1 human evaluation remains the most reliable method to evaluate MT systems and track the advances in Machine Translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9874634742736816}, {"text": "Machine Translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.859242856502533}]}, {"text": "Appraise is an opensource toolkit designed to facilitate the human evaluation of machine translation.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5604877471923828}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7412314414978027}]}, {"text": "It has been adopted as the preferred tool in the WMT evaluation campaigns (, and thus, it is currently used by dozens of researchers.", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.9125125408172607}]}, {"text": "According to the eye-mind hypothesis) people cognitively process objects that are in front of their eyes.", "labels": [], "entities": []}, {"text": "This has enabled researchers to analyze and understand how people perform certain tasks like reading It is subjective, expensive, time-consuming, boring, etc..", "labels": [], "entities": []}, {"text": "In recent times, eyetracking has also been used in Machine Translation to identify and classify translation errors (, to evaluate the usability of automatic translations, and to improve the consistency of the human evaluation process, etc.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8538742065429688}]}, {"text": "Furthermore, tracking how evaluators consume MT output, can help to reduce human evaluation subjectivity, as we could use evidence of what people do (i.e. unbiased reading patterns) and not only what they say they think (i.e. user-biased evaluation scores).", "labels": [], "entities": [{"text": "MT output", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.8693082630634308}]}, {"text": "However, the main limitation for the adoption of eye-tracking research has been the steep learning curve that is associated with eye-tracking analysis and the high-cost of eye-tracking devices.", "labels": [], "entities": [{"text": "eye-tracking analysis", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.7800473570823669}]}, {"text": "In this paper, we present iAppraise: an opensource framework that enables the use of eyetracking for MT evaluation, and facilitates the replication and dissemination of eye-tracking research in MT.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.978227287530899}, {"text": "MT", "start_pos": 194, "end_pos": 196, "type": "TASK", "confidence": 0.9521809220314026}]}, {"text": "First, it is designed to work with the increasingly popular, low-cost 2 eye-tracker eyeTribe.", "labels": [], "entities": [{"text": "eyeTribe", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.8761146664619446}]}, {"text": "Secondly, it provides a set of tools for extracting and exploiting gaze features, which facilitate eye-tracking analysis.", "labels": [], "entities": [{"text": "eye-tracking analysis", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7925468683242798}]}, {"text": "Lastly, it integrates fully with the Appraise toolkit, making it accessible to a larger audience.", "labels": [], "entities": [{"text": "Appraise toolkit", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.869037538766861}]}, {"text": "Our setup allows to track eye-movements during the MT evaluation process.", "labels": [], "entities": [{"text": "MT evaluation process", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9271891911824545}]}, {"text": "The data generated can be used to visualize a re-enactment of the evaluation session in real-time, thus providing useful qualitative insights on the evaluation; or to extract features for further quantitative analysis.", "labels": [], "entities": []}, {"text": "The applications for this toolkit are multiple.", "labels": [], "entities": []}, {"text": "Using reading patterns from evaluators could be a useful tool for MT evaluation: (i) to shed light into the evaluation process: e.g. the general reading behavior that evaluators follow to complete their task; (ii) to understand which parts of a translation are more difficult for the annotator; and (iii) to develop automatic evaluation systems that use reading patterns to predict translation quality.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9728242754936218}]}, {"text": "In an effort carried using this framework, we proposed a model to predict the quality of the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9849151372909546}]}, {"text": "Our results showed that reading patterns obtained from the eyemovements of the evaluators can help to anticipate the evaluation scores to be given by them.", "labels": [], "entities": []}, {"text": "We found that the features extracted from the eye-tracking data (discussed in Section 2.6) capture more than just the fluency of a translation.", "labels": [], "entities": []}, {"text": "Details of findings are reported in.", "labels": [], "entities": []}, {"text": "In this paper, we describe the overall architecture of iAppraise: the communication modules, the user interface, and the analysis package.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}