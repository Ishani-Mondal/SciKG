{"title": [{"text": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter", "labels": [], "entities": [{"text": "Predictive", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9175722599029541}, {"text": "Hate Speech Detection", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.6724383135636648}]}], "abstractContent": [{"text": "Hate speech in the form of racist and sex-ist remarks area common occurrence on social media.", "labels": [], "entities": []}, {"text": "For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lo-mas, 2015).", "labels": [], "entities": [{"text": "BBC, 2015; Lo-mas, 2015)", "start_pos": 174, "end_pos": 198, "type": "DATASET", "confidence": 0.9336208626627922}]}, {"text": "We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets.", "labels": [], "entities": []}, {"text": "We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hate-speech detection.", "labels": [], "entities": [{"text": "hate-speech detection", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8030801117420197}]}, {"text": "We also present a dictionary based the most indicative words in our data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hate speech is an unfortunately common occurrence on the Internet) and in some cases culminates in severe threats to individuals.", "labels": [], "entities": [{"text": "Hate speech", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8599010109901428}]}, {"text": "Social media sites therefore face the problem of identifying and censoring problematic posts while weighing the right to freedom of speech.", "labels": [], "entities": []}, {"text": "The importance of detecting and moderating hate speech is evident from the strong connection between hate speech and actual hate crimes.", "labels": [], "entities": [{"text": "detecting and moderating hate speech", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7155793070793152}]}, {"text": "Early identification of users promoting hate speech could enable outreach programs that attempt to prevent an escalation from speech to action.", "labels": [], "entities": []}, {"text": "Sites such as Twitter and Facebook have been seeking to actively combat hate speech.", "labels": [], "entities": []}, {"text": "Most recently, Facebook announced that they would seek to combat racism and xenophobia aimed at refugees.", "labels": [], "entities": []}, {"text": "Currently, much of this moderation requires manual review of questionable documents, which not only limits how much a human annotator can be reviewed, but also introduces subjective notions of what constitutes hate speech.", "labels": [], "entities": []}, {"text": "A reaction to the \"Black Lives Matter\" movement, a campaign to highlight the devaluation of lives of African-American citizens sparked by extrajudicial killings of black men and women, at the Facebook campus shows how individual biases manifest in evaluating hate speech.", "labels": [], "entities": []}, {"text": "In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.", "labels": [], "entities": []}, {"text": "While online hate speech is a growing phenomenon (), its distribution is not uniform across all demographics.", "labels": [], "entities": []}, {"text": "Neither is the awareness of what constitutes hate speech.", "labels": [], "entities": []}, {"text": "Considering that hate speech is not evenly distributed in the United States of America and perpetrators of hate speech should be a small minority from a limited demographic group.", "labels": [], "entities": []}, {"text": "Including available demographic information as features should thus help identification accuracy.", "labels": [], "entities": [{"text": "identification", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9720982313156128}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9418808221817017}]}, {"text": "Our contribution We provide a data set of 16k tweets annotated for hate speech.", "labels": [], "entities": []}, {"text": "We also investigate which of the features we use provide the best identification performance.", "labels": [], "entities": []}, {"text": "We analyze the features that improve detection of hate speech in our corpus, and find that despite presumed differences in the geographic and word-length distribution, they have little to no positive effect on performance, and rarely improve over character-level features.", "labels": [], "entities": []}, {"text": "The exception to this rule is gender.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the influence of different features on prediction in a classification task.", "labels": [], "entities": []}, {"text": "We use a logistic regression classifier and 10-fold cross validation to test the influence of various features on prediction performance, and to quantify their expressiveness.", "labels": [], "entities": []}, {"text": "Model Selection In order to pick the most suitable features, we perform a grid search overall possible feature set combinations, finding that using character n-grams outperforms using word ngrams by at least 5 F1-points (60.42 vs. 69.86) using similar features.", "labels": [], "entities": [{"text": "F1-points", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9918742775917053}]}, {"text": "For that reason, we do not consider word n-grams.", "labels": [], "entities": []}, {"text": "To determine whether a difference between two feature sets is statistically significant (at p < 0.05), we run a bootstrap sampling test on the predictions of the two systems.", "labels": [], "entities": []}, {"text": "The test takes 10,000 samples and compares whether the better system is the same as the better system on the entire data set.", "labels": [], "entities": []}, {"text": "The resulting (p-) value of the bootstrap testis thus the fraction of samples where the winner differs from the entire data set, giving the p-value a very intuitive interpretation.", "labels": [], "entities": []}, {"text": "Results We find that using character n-grams of lengths up to 4, along with gender as an additional feature provides the best results.", "labels": [], "entities": []}, {"text": "We further find that using location or length is detrimental to our scores.", "labels": [], "entities": [{"text": "length", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.974770188331604}]}, {"text": "By using our n-gram features we achieve the results shown in.", "labels": [], "entities": []}, {"text": "We find that across our features only adding gender information improves our F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9986236095428467}]}, {"text": "All other features and feature combinations are detrimental to the performance of the system.", "labels": [], "entities": []}, {"text": "We find that gender, the only additional feature that provides an improvement, is not statistically significant, whereas the addition of location as well as gender is significant, at p = 0.0355.", "labels": [], "entities": []}, {"text": "Features We collect unigrams, bigrams, trigrams, and fourgrams for each tweet and the user description.", "labels": [], "entities": []}, {"text": "To assess the informativeness of the features we sum the model coefficients for each feature over the 10 folds of cross validation.", "labels": [], "entities": []}, {"text": "This allows fora more robust estimate.", "labels": [], "entities": []}, {"text": "We find that the most influential features for the logistic regression (see) largely correspond with the most frequent terms in.", "labels": [], "entities": []}, {"text": "We see, for instance different n-gram lengths of the word \"Islam\" and \"sexist\".", "labels": [], "entities": []}, {"text": "Intuitively, it makes sense that not only will the most frequent terms be indicative, but also that character n-grams would outperform word ngrams, due to character n-gram matrices being far less sparse than the word n-gram matrices.", "labels": [], "entities": []}, {"text": "One of the notable differences between the ngrams for our two categories is the occurrence of a gender-based slur, and normal words pertaining to women.", "labels": [], "entities": []}, {"text": "On the other hand, all of the racist features are n-grams of normal terms, which are re-appropriated for building a negative discourse.", "labels": [], "entities": []}, {"text": "One such example is: \"@BYRONFBERRY Good.", "labels": [], "entities": [{"text": "BYRONFBERRY", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9119526147842407}]}, {"text": "Time to confront the cult of hatred and murder #Islam\".", "labels": [], "entities": []}, {"text": "char n-grams +gender +gender +loc word n-grams 'lim': Most indicative character n-gram features for hate-speech detection Gender (F1 73.89) We train our model on character bi-to fourgrams and the gender information for each use obtained as described in section 3.", "labels": [], "entities": [{"text": "hate-speech detection", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7358711659908295}, {"text": "F1", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.904735803604126}]}, {"text": "We find that this combination yields the highest score (see), though the score only increases slightly.", "labels": [], "entities": []}, {"text": "Length (F1 73.66) This feature set contains the total of each tweet and description and average lengths of the words occurring along with the ngrams of lengths 1 to 4.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9666604399681091}, {"text": "F1 73.66", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9052772223949432}]}, {"text": "Gender + location (F1 73.62) In this feature set contains the locations obtained in 5 along with our 1 to 4-grams, and the gender for each user.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9777517914772034}]}, {"text": "Adding locations occurs to be slightly detrimental to the performance of the classifier.", "labels": [], "entities": []}, {"text": "Gender + location + length (F1 73.47) For completeness we train on gender, geographic information, and length features along with 1 to 4-grams.", "labels": [], "entities": [{"text": "length", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8464978337287903}, {"text": "F1 73.47", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9363736212253571}]}, {"text": "Our score decreases by the use of all features, as we expected given the results of using location in combination with gender, and length.", "labels": [], "entities": [{"text": "length", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9802449345588684}]}], "tableCaptions": [{"text": " Table 1: Distribution of genders in hate-speech  documents.", "labels": [], "entities": [{"text": "Distribution of genders in hate-speech  documents", "start_pos": 10, "end_pos": 59, "type": "TASK", "confidence": 0.8338188330332438}]}, {"text": " Table 2: Distribution of ten most frequently occurring terms", "labels": [], "entities": []}, {"text": " Table 4: F1 achieved by using different features sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9976775050163269}]}]}