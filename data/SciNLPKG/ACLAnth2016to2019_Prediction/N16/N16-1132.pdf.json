{"title": [{"text": "Bootstrapping Translation Detection and Sentence Extraction from Comparable Corpora", "labels": [], "entities": [{"text": "Bootstrapping Translation Detection", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7321099837621053}, {"text": "Sentence Extraction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7531796097755432}]}], "abstractContent": [{"text": "Most work on extracting parallel text from comparable corpora depends on linguistic resources such as seed parallel documents or translation dictionaries.", "labels": [], "entities": [{"text": "extracting parallel text from comparable corpora", "start_pos": 13, "end_pos": 61, "type": "TASK", "confidence": 0.8235904574394226}]}, {"text": "This paper presents a simple baseline approach for bootstrapping a parallel collection.", "labels": [], "entities": []}, {"text": "It starts by observing documents published on similar dates and the co-occurrence of a small number of identical tokens across languages.", "labels": [], "entities": []}, {"text": "It then uses fast, on-line inference fora latent variable model to represent multilingual documents in a shared topic space where it can do efficient nearest-neighbor search.", "labels": [], "entities": []}, {"text": "Starting from the Giga-word collections in English and Spanish, we train a translation system that outperforms one trained on the WMT'11 parallel training set.", "labels": [], "entities": [{"text": "WMT'11 parallel training set", "start_pos": 130, "end_pos": 158, "type": "DATASET", "confidence": 0.867277979850769}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), the quality of the translation model is highly dependent on the amount of parallel data used to build it.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8129882564147314}]}, {"text": "Parallel data has usually been generated through the process of human translation, which imposes significant costs when building systems for new languages and domains.", "labels": [], "entities": [{"text": "human translation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7940613627433777}]}, {"text": "To alleviate this problem, researchers have considered comparable corpora-a collection of multilingual documents that are only topically aligned but not necessary translations of each other ().", "labels": [], "entities": []}, {"text": "While most previous approaches for mining comparable corpora heavily depend on initializing the learning process with some translation dictionaries or parallel text, we use multilingual topic models to detect document translation pairs and extract parallel sentences with only minimum cross-language prior knowledge: the publication dates of articles and the tendency of some vocabulary to overlap across languages.", "labels": [], "entities": []}, {"text": "Processing only four years of Gigaword news stories in English and Spanish, we are able to outperform the WMT'11 baseline system trained on parallel News Commentary corpus).", "labels": [], "entities": [{"text": "WMT'11 baseline", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.9138028919696808}]}], "datasetContent": [{"text": "We demonstrate the performance of the bootstrapping approach on the task of extracting parallel sentences to train a translation system.", "labels": [], "entities": []}, {"text": "We evaluate MT systems trained on extracted parallel sentences and compare their performance against MT systems created using clean parallel collections.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.975073516368866}]}, {"text": "MT systems were evaluated with the standard BLEU metric) on two official WMT test sets that cover different domains: News (WMT'11) and Europarl (WMT'08).", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9691600203514099}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9985947012901306}, {"text": "WMT test sets", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8640507260958353}, {"text": "News (WMT'11)", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.8261656761169434}, {"text": "Europarl (WMT'08)", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.836496040225029}]}, {"text": "We trained the Moses SMT system () following the WMT shared task guidelines for building a baseline system with one of two parallel training collections from WMT'11: English-Spanish News Commentary (v6) and Europarl (v6).", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6420595645904541}, {"text": "WMT", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8563795685768127}, {"text": "WMT'11", "start_pos": 158, "end_pos": 164, "type": "DATASET", "confidence": 0.9669790267944336}, {"text": "Europarl", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.921713650226593}]}, {"text": "MT systems were trained using test-domain specific language models (LM) -English News Commentary for News test and English Europarl for the Europarl test.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9673023819923401}, {"text": "Europarl test", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9435091614723206}]}, {"text": "Our comparable corpus consists of news stories from the English (LDC2011T07) and Spanish (LDC2011T12) Gigaword collections.", "labels": [], "entities": [{"text": "LDC2011T07) and Spanish (LDC2011T12) Gigaword collections", "start_pos": 65, "end_pos": 122, "type": "DATASET", "confidence": 0.6678369210826026}]}, {"text": "We perform the following processing in each step of the pipeline.", "labels": [], "entities": []}, {"text": "We run OCD on days of news originating from multiple news agencies or more specifically on news stories originating from the same day which we consider as the \"minimal supervision\" in initiating the bootstrapping process.", "labels": [], "entities": [{"text": "OCD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9464942216873169}]}, {"text": "Since the OCD approach generates a single list of ranked document translation pairs, for the second stage of our pipeline we consider the top n document translation pairs.", "labels": [], "entities": []}, {"text": "We define n to be all document translation pairs whose Cos similarity is between the range of the max (i.e. the top 1 scored document translation pair in the single ranked list) and max 2 . Unlike previous thresholding based on absolute values), this approach allows us to utilize threshold values that are automatically adjusted to the dynamic range of the Cos distance of a particular corpus.", "labels": [], "entities": [{"text": "Cos similarity", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.9411841034889221}]}, {"text": "Sentences from the top n news stories are extracted and are further aligned.", "labels": [], "entities": []}, {"text": "The output of the aligner is then used as a training set for the PLTM model.", "labels": [], "entities": []}, {"text": "We represent each of the news stories using the per story aligned sentences.", "labels": [], "entities": []}, {"text": "Once trained, we use the PLTM model to infer topics back onto the news stories.", "labels": [], "entities": []}, {"text": "We then again create a single ranked list of translation news story pairs by computing divergence based similarity using He distance ( \u00a73.2).", "labels": [], "entities": [{"text": "translation news story pairs", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8923321962356567}, {"text": "He distance", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.9572901725769043}]}, {"text": "Keeping the top n ranked news story pairs, we obtain a list of what we believe are parallel documents which we then use to extract sentence pairs.", "labels": [], "entities": []}, {"text": "Sentences are finally processed through an aligner and then used as the training corpus to our MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9794613122940063}]}, {"text": "The Gigaword collection contains news stories generated from various agencies in different languages.", "labels": [], "entities": [{"text": "Gigaword collection", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8958347141742706}]}, {"text": "On any given day, a news story in English mayor may not cover the same topic as one in a different language.", "labels": [], "entities": []}, {"text": "To perform a fair evaluation with the WMT'11 News test, we considered stories published in non-overlapping years.", "labels": [], "entities": [{"text": "WMT'11 News test", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9281755685806274}]}, {"text": "shows the performance comparison, on the News test set (WMT'11), of the MT system trained on extracted parallel sentences from four years of Gigaword data (GW) with a MT system trained on two WMT'11 baseline parallel collections: Europarl (EP) and News Commentary (NC).", "labels": [], "entities": [{"text": "News test set (WMT'11)", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.907072921593984}, {"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9738233685493469}, {"text": "WMT'11 baseline parallel collections", "start_pos": 192, "end_pos": 228, "type": "DATASET", "confidence": 0.8855830729007721}, {"text": "Europarl (EP)", "start_pos": 230, "end_pos": 243, "type": "DATASET", "confidence": 0.8421739488840103}, {"text": "News Commentary (NC)", "start_pos": 248, "end_pos": 268, "type": "DATASET", "confidence": 0.8229553461074829}]}, {"text": "While over 10 times bigger than NC, EP is out of domain and thus performs only slightly better.", "labels": [], "entities": [{"text": "NC", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.888350248336792}, {"text": "EP", "start_pos": 36, "end_pos": 38, "type": "DATASET", "confidence": 0.582297682762146}]}, {"text": "On the News test set, parallel sentences automatically extracted from only four years of Gigaword data outperform systems trained on clean NC or EP bitext.", "labels": [], "entities": [{"text": "News test set", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9798867106437683}, {"text": "Gigaword data", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.8499201834201813}, {"text": "EP bitext", "start_pos": 145, "end_pos": 154, "type": "DATASET", "confidence": 0.7338525354862213}]}, {"text": "In order to determine statistically significant differences between the results of different MT systems we ran the randomization test on the News test set with 10k iterations.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9778379797935486}, {"text": "News test set", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.9950730204582214}]}, {"text": "In each iteration we performed permutations across the translation sentences obtained from the two MT systems whose statistical difference in performance we evaluate.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.8975996971130371}]}, {"text": "shows the performance comparison on the Europarl test set (WMT'08) between the MT system trained on the extracted parallel sentences and the two MT baseline systems.", "labels": [], "entities": [{"text": "Europarl test set (WMT'08)", "start_pos": 40, "end_pos": 66, "type": "DATASET", "confidence": 0.9605120122432709}]}, {"text": "On this test set, unsurprisingly, EP training performed very well.", "labels": [], "entities": [{"text": "EP training", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9006684720516205}]}, {"text": "gives a summary of ablation experiments that we performed across the two stages of our bootstrapping approach.", "labels": [], "entities": []}, {"text": "More specifically, we ex-   plored using bitext extracted by OCD alone, without PLTM reestimation, to train a MT system.", "labels": [], "entities": [{"text": "PLTM reestimation", "start_pos": 80, "end_pos": 97, "type": "METRIC", "confidence": 0.932671070098877}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9834669232368469}]}, {"text": "Both extracted bitext sets also contained many duplicate sentence pairs.", "labels": [], "entities": []}, {"text": "In this set of experiments we also explored the effect of deduplicating them, i.e. going over the extracted set of English-Spanish sentence pairs and removing the duplicate ones.", "labels": [], "entities": []}, {"text": "Bitext extracted by OCD alone without PLTM reestimation performed only slightly worse on WMT'11.", "labels": [], "entities": [{"text": "PLTM reestimation", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.8855763077735901}, {"text": "WMT'11", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.9885938167572021}]}, {"text": "The OCD-only data, however, only showed 70% overlap with OCD+PLTM (GW).", "labels": [], "entities": [{"text": "OCD-only data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7485020756721497}, {"text": "overlap", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9853175282478333}, {"text": "OCD+PLTM (GW)", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.5791668544212977}]}, {"text": "Deduplicating the two bitexts (dedup.) hurts OCD somewhat more than OCD+PLTM.", "labels": [], "entities": [{"text": "OCD", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6449066400527954}, {"text": "PLTM", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.8628777265548706}]}, {"text": "On the Europarl test set, however, deduplicating OCD+PLTM bitext caused a significant boost from 23.88 to 24.67, while causing slight performance drop for OCD (cf. NC-trained 25.43).", "labels": [], "entities": [{"text": "Europarl test set", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9947457313537598}, {"text": "PLTM bitext", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.8411089479923248}, {"text": "OCD", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.5717244148254395}]}, {"text": "These interactions of test domain, redundancy, and model settings leave room for further studies of the performance of our bootstrapping approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score values computed over the WMT'11  News test set with MT systems developed using extracted  and parallel sources of training data. * denotes statistical  significance level (p-value\u22640.001) above NC.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9722094237804413}, {"text": "WMT'11  News test set", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9771980792284012}, {"text": "statistical  significance level", "start_pos": 160, "end_pos": 191, "type": "METRIC", "confidence": 0.8642030556996664}, {"text": "NC", "start_pos": 214, "end_pos": 216, "type": "METRIC", "confidence": 0.9714180827140808}]}, {"text": " Table 2: BLEU score values computed over the WMT'08  Europarl test set with MT systems developed using ex- tracted and parallel sources of training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9695375561714172}, {"text": "WMT'08  Europarl test set", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.9000902622938156}]}, {"text": " Table 3: Summary of ablation experiments: BLEU score  values of MT systems trained on extracted bitext by OCD  alone and with PLTM reestimation along with the dedu- plication (dedup.) effect. * denotes statistical significance  level (p-value\u22640.001) above NC.  \u2021 denotes statistical sig- nificance level (p-value\u22640.05) above NC.  \u2020 denotes sta- tistical significance level (p-value\u22640.001) above OCD.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.998213529586792}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9727481007575989}, {"text": "OCD", "start_pos": 396, "end_pos": 399, "type": "DATASET", "confidence": 0.9447539448738098}]}]}