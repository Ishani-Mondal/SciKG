{"title": [{"text": "Intra-Topic Variability Normalization based on Linear Projection for Topic Classification", "labels": [], "entities": [{"text": "Intra-Topic Variability Normalization", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6405240794022878}]}], "abstractContent": [{"text": "This paper proposes a variability normaliza-tion algorithm to reduce the variability between intra-topic documents for topic classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.8352127373218536}]}, {"text": "Firstly, an optimization problem is constructed based on linear variability removable assumption.", "labels": [], "entities": []}, {"text": "Secondly, anew feature space for document representation is found by solving the optimization problem with kernel principle component analysis (KPCA).", "labels": [], "entities": [{"text": "document representation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.739008367061615}]}, {"text": "Finally , effective feature transformation is taken through linear projection.", "labels": [], "entities": []}, {"text": "As for experiments, state-of-the-art SVM and KNN algorithm are adopted for topic classification respectively.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7327224910259247}]}, {"text": "Experimental results on a free-style conversational corpus show that the proposed variability normalization algorithm for topic classification achieves 3.8% absolute improvement for micro-F 1 measure.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.8114363849163055}]}], "introductionContent": [{"text": "Topic classification is now faced with the problem of enormous variability between documents due to the exponential growth of free-style unstructured texts in recent years.", "labels": [], "entities": [{"text": "Topic classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9043053984642029}]}, {"text": "This paper treats variability as differences between text documents and aims at reducing the intra-topic document variability for better topic classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7386473119258881}]}, {"text": "There are various factors to cause the intra-topic variability problem, such as the different language usages of different persons).", "labels": [], "entities": []}, {"text": "In freestyle conversations experimented in this paper, different people would use very different words to express their opinions.", "labels": [], "entities": []}, {"text": "Therefore, documents in a same topic could be quite different because of the intra-topic variability problem.", "labels": [], "entities": []}, {"text": "In this work, we are interested in finding a robust document representation strategy to address the intra-topic variability problem.", "labels": [], "entities": []}, {"text": "Traditional method represents document by a high-dimensional TF-IDF vector based on the bag-of-word approach.", "labels": [], "entities": []}, {"text": "However, the TF-IDF feature reveals little semantic similarity information between terms, which would increase the differences between intra-topic documents when different words are used.", "labels": [], "entities": []}, {"text": "Beyond the TF-IDF strategy, there are two class of techniques, i.e., unsupervised technique and supervised technique for document representations.", "labels": [], "entities": []}, {"text": "The unsupervised technique includes some latent semantic analysis methods.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6260896027088165}]}, {"text": "The typical method is Latent Semantic Indexing (LSI) while the features estimated by LSI are linear combinations of the original features.", "labels": [], "entities": [{"text": "Latent Semantic Indexing (LSI)", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6525425712267557}]}, {"text": "Meanwhile, the popular Latent Dirichlet Allocation () algorithm was proposed to represent document by a generative probabilistic model (.", "labels": [], "entities": []}, {"text": "Moreover, in recent years, many neural network based methods have been investigated for document representations ().", "labels": [], "entities": [{"text": "document representations", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.755431205034256}]}, {"text": "For example, in), a model called paragraph vector was designed to represent each document by a dense vector while the vector is trained by predicting all words in the corresponding document.", "labels": [], "entities": []}, {"text": "On the other hand, supervised technique for document representation includes some discrim-inative approaches, e.g., Linear Discriminant Analysis () and supervised latent semantic indexing (.", "labels": [], "entities": [{"text": "document representation", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7683890163898468}, {"text": "Linear Discriminant Analysis", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.6018168528874716}, {"text": "supervised latent semantic indexing", "start_pos": 152, "end_pos": 187, "type": "TASK", "confidence": 0.6028656214475632}]}, {"text": "Meanwhile, some improved linear analysis methods were proposed for encoding documents with a reliable similarity information.", "labels": [], "entities": [{"text": "linear analysis", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7692115306854248}]}, {"text": "However, all those works for document representation paid little attention to the variability of intra-topic documents.", "labels": [], "entities": [{"text": "document representation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7259361445903778}]}, {"text": "Therefore, they could hardly solve the intra-topic variability problem in a direct way.", "labels": [], "entities": []}, {"text": "This paper makes a preliminary investigation to deal with the intra-topic variability problem.", "labels": [], "entities": []}, {"text": "The main purpose of this work is to find anew feature space with minimized intra-topic variability.", "labels": [], "entities": []}, {"text": "An objective criterion is constructed for optimization.", "labels": [], "entities": []}, {"text": "Mathematically, we make use of the topic label information of the training set to create a weighting matrix, and then sum overall the differences of intra-topic documents.", "labels": [], "entities": []}, {"text": "Then a robust feature space with minimized intra-topic variability is generated by solving the optimization problem with effective KPCA based algorithm.", "labels": [], "entities": []}, {"text": "Finally, we accomplish the variability normalization operations for the baseline features.", "labels": [], "entities": [{"text": "variability normalization", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7435772716999054}]}, {"text": "We also employ the linear discriminant analysis as a supplementary algorithm.", "labels": [], "entities": []}, {"text": "As for experiments, state-of-the-art SVM and KNN algorithms are employed for topic classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7811789512634277}]}, {"text": "System performances are evaluated on a challenging free-style conversational database.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we introduce the proposed variability normalization algorithm for topic classification in detail.", "labels": [], "entities": [{"text": "variability normalization", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7702908217906952}, {"text": "topic classification", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.851593554019928}]}, {"text": "After it, section 3 presents experimental setup and results.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future work would be given in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the proposed variability normalization method in atypical topic classification problem.", "labels": [], "entities": [{"text": "variability normalization", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8014099895954132}, {"text": "topic classification", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.729320615530014}]}, {"text": "We will firstly introduce the experimental setup, including dataset, evaluation criteria and system description.", "labels": [], "entities": []}, {"text": "After it, all the experimental results would be reported in detail.", "labels": [], "entities": []}, {"text": "The data set used in this paper is the text transcripts of free-style conversational speech database, Fisher English corpus released by LDC, which contains 11699 recorded conversations ().", "labels": [], "entities": [{"text": "Fisher English corpus released by LDC", "start_pos": 102, "end_pos": 139, "type": "DATASET", "confidence": 0.9437414606412252}]}, {"text": "This corpus is collected from 40 different topics, and each document includes relatively a distinct topic (e.g. \"Comedy\", \"Smoking\", \"Terrorism\", etc.) as well as topics covering similar subject areas (e.g. \"Airport Security\", \"Bioterrorism\", \"Issues in the Middle East\").", "labels": [], "entities": [{"text": "Airport Security\"", "start_pos": 208, "end_pos": 225, "type": "TASK", "confidence": 0.7370879550774893}]}, {"text": "This paper randomly chooses 60 documents and 50 documents per topic for the training set and testing set respectively.", "labels": [], "entities": []}, {"text": "Another 50 documents for each topic are randomly selected to for the development set.", "labels": [], "entities": []}, {"text": "We use two types of criteria to make a comprehensive evaluations for this work.", "labels": [], "entities": []}, {"text": "The first evaluation creterion is F 1 measure corresponding to the recall and precision rates fora typical classification system.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9844979445139567}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9993698000907898}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9647965431213379}]}, {"text": "In detail, we would report micro-average F 1 and macro-average F 1 results.", "labels": [], "entities": [{"text": "micro-average F 1", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.8232061465581259}, {"text": "F 1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.8667936027050018}]}, {"text": "In consideration of topic classification is similar to topic verification, we choose equal error rate (EER) to be the second criterion, which is the equal value of miss probability and false probability.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8125889599323273}, {"text": "topic verification", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8171613216400146}, {"text": "equal error rate (EER)", "start_pos": 85, "end_pos": 107, "type": "METRIC", "confidence": 0.8852680027484894}]}], "tableCaptions": [{"text": " Table 2: Classification results using KNN algorithm", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.976468563079834}]}, {"text": " Table 3: Classification results using SVM algorithm", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.976136326789856}]}]}