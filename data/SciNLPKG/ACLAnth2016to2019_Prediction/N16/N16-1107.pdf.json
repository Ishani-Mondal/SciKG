{"title": [{"text": "Bayesian Supervised Domain Adaptation for Short Text Similarity", "labels": [], "entities": [{"text": "Bayesian Supervised Domain Adaptation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.608510248363018}, {"text": "Short Text Similarity", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.5860897699991862}]}], "abstractContent": [{"text": "Identification of short text similarity (STS) is a high-utility NLP task with applications in a variety of domains.", "labels": [], "entities": [{"text": "Identification of short text similarity (STS)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9109072536230087}]}, {"text": "We explore adaptation of STS algorithms to different target domains and applications.", "labels": [], "entities": []}, {"text": "A two-level hierarchical Bayesian model is employed for domain adaptation (DA) of a linear STS model to text from different sources (e.g., news, tweets).", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.8439319252967834}]}, {"text": "This model is then further extended for multitask learning (MTL) of three related tasks: STS, short answer scoring (SAS) and answer sentence ranking (ASR).", "labels": [], "entities": [{"text": "multitask learning (MTL)", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.648967158794403}, {"text": "STS", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.8044057488441467}, {"text": "short answer scoring (SAS)", "start_pos": 94, "end_pos": 120, "type": "METRIC", "confidence": 0.5294609218835831}, {"text": "answer sentence ranking (ASR)", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.6090806573629379}]}, {"text": "In our experiments, the adaptive model demonstrates better overall cross-domain and cross-task performance over two non-adaptive base-lines.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Short Text Similarity (STS) Given two short texts, STS provides a real-valued score that represents their degree of semantic similarity.", "labels": [], "entities": [{"text": "Short Text Similarity (STS", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6541356146335602}, {"text": "STS", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9003776907920837}]}, {"text": "Our STS datasets come from the SemEval 2012-2015 corpora, containing over 14,000 human-annotated sentence pairs (via Amazon Mechanical Turk) from domains like news, tweets, forum posts, and image descriptions.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.715426042675972}, {"text": "SemEval 2012-2015 corpora", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.6382494767506918}]}, {"text": "For our experiments, we select ten datasets from ten different domains, containing 6,450 sentence pairs.", "labels": [], "entities": []}, {"text": "1 This selection is intended to maximize (a) the number of domains, (b) domain uniqueness: of three different news headlines datasets, for example, we select the most recent (2015), discarding older ones, and (c) amount of per-domain data available: we exclude the FNWN (2013) dataset with 189 annotations, for example, because it limits per-domain training data in our experiments.", "labels": [], "entities": [{"text": "FNWN (2013) dataset", "start_pos": 265, "end_pos": 284, "type": "DATASET", "confidence": 0.9341900825500489}]}, {"text": "Sizes of the selected datasets range from 375 to 750 pairs.", "labels": [], "entities": []}, {"text": "Average correlation (Pearson's r) among annotators ranges from 58.6% to 88.8% on individual datasets (above 70% for most)).", "labels": [], "entities": [{"text": "correlation", "start_pos": 8, "end_pos": 19, "type": "METRIC", "confidence": 0.9754659533500671}, {"text": "Pearson's r)", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.9552015364170074}]}, {"text": "Short Answer Scoring (SAS) SAS comes in different forms; we explore a form where fora shortanswer question, a gold answer is provided, and the goal is to grade student answers based on how similar they are to the gold answer (.", "labels": [], "entities": [{"text": "Short Answer Scoring (SAS) SAS", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7785964225019727}]}, {"text": "We use a dataset of undergraduate data structures questions and student responses graded by two judges).", "labels": [], "entities": []}, {"text": "These questions are spread across ten different assignments and two examinations, each on a related set of topics (e.g., programming basics, sorting algorithms).", "labels": [], "entities": [{"text": "sorting algorithms)", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8784615596135458}]}, {"text": "Inter-annotator agreement is 58.6% (Pearson's \u03c1) and 0.659 (RMSE on a 5-point scale).", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.6905346512794495}, {"text": "Pearson's \u03c1)", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.8314138501882553}, {"text": "RMSE", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9928944706916809}]}, {"text": "We discard assignments with fewer than 200 pairs, retaining 1,182 student responses to forty questions spread across five assignments and tests.", "labels": [], "entities": []}, {"text": "Answer Sentence Ranking (ASR) Given a factoid question and a set of candidate answer sentences, ASR orders candidates so that sentences containing the answer are ranked higher.", "labels": [], "entities": [{"text": "Answer Sentence Ranking (ASR", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6438170969486237}]}, {"text": "Text similarity is the foundation of most prior work: a candidate sentence's relevance is based on its similarity with the question (.", "labels": [], "entities": [{"text": "Text similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6586726307868958}]}, {"text": "For our ASR experiments, we use factoid questions developed by from Text REtrieval Conferences (TREC) 8-13.", "labels": [], "entities": [{"text": "ASR", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9895311594009399}, {"text": "Text REtrieval Conferences (TREC) 8-13", "start_pos": 68, "end_pos": 106, "type": "TASK", "confidence": 0.5168159774371556}]}, {"text": "Candidate QA pairs of a question and a candidate were labeled with whether the candidate answers the question.", "labels": [], "entities": []}, {"text": "The questions are of different types (e.g., what, where); we retain 2,247 QA pairs under four question types, each with at least 200 answer candidates in the combined development and test sets.", "labels": [], "entities": []}, {"text": "Each question type represents a unique topical domain-who questions are about persons and how many questions are about quantities.", "labels": [], "entities": []}, {"text": "For each of the three tasks, we first assess the performance of our base model to (1) verify our samplingbased Bayesian implementations, and (2) compare to the state of the art.", "labels": [], "entities": []}, {"text": "We train each model with a Metropolis-within-Gibbs sampler with 50,000 samples using PyMC (, discarding the first half of the samples as burnin.", "labels": [], "entities": [{"text": "PyMC", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.8827574849128723}]}, {"text": "The variances m \u03c3w and m \u03c3 S are both set to 100.", "labels": [], "entities": []}, {"text": "Base models are evaluated on the entire test set for each task, and the same training examples as in the state-of-the-art systems are used.", "labels": [], "entities": []}, {"text": "Following SemEval, we report a weighted sum of correlations (Pearson's r) across all test sets for STS, where the weight of a test set is proportional to its number of pairs.", "labels": [], "entities": [{"text": "Pearson's r)", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.898756667971611}, {"text": "STS", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9438136219978333}]}, {"text": "Our model and are almost identical on all twenty test sets from SemEval 2012-2015, supporting the correctness of our Bayesian implementation.", "labels": [], "entities": []}, {"text": "Finally, for ASR, we adopt two metrics widely used in information retrieval: mean average precision (MAP) and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "ASR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9942569136619568}, {"text": "information retrieval", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7821124196052551}, {"text": "mean average precision (MAP)", "start_pos": 77, "end_pos": 105, "type": "METRIC", "confidence": 0.9523636202017466}, {"text": "mean reciprocal rank (MRR)", "start_pos": 110, "end_pos": 136, "type": "METRIC", "confidence": 0.8777946432431539}]}, {"text": "MAP assesses the quality of the ranking as a whole whereas MRR evaluates only the top-ranked answer sentence.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5579684972763062}, {"text": "MRR", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8911167979240417}]}, {"text": "report a convolutional neural network model of text similarity which shows top ASR results on the dataset.", "labels": [], "entities": [{"text": "ASR", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.6390834450721741}]}, {"text": "Our model outperforms this model on both metrics.", "labels": [], "entities": []}, {"text": "Answers-forums .9847 1 .9999 Answers-students (2015) .9850 1 .9983 1 .9915 .9970 .9971 . .9992 .9986 1 Deft-forum 1 .9775 .9943 .9946 .9990 1 Tweet-news .: Feature weights and correlations of different models in three extreme scenarios.", "labels": [], "entities": []}, {"text": "In each case, the adaptive model learns relative weights that are more similar to those in the best baseline model.", "labels": [], "entities": []}, {"text": "seventy-five training pairs per domain and how well each model does.", "labels": [], "entities": []}, {"text": "All three domains have very different outcomes for the baseline models.", "labels": [], "entities": []}, {"text": "We show weights for the alignment (w 1 ) and embedding features (w 2 ).", "labels": [], "entities": []}, {"text": "In each domain, (1) the relative weights learned by the two baseline models are very different, and (2) the adaptive model learns relative weights that are closer to those of the best model.", "labels": [], "entities": []}, {"text": "In SMT, for example, the predictor weights learned by the adaptive model have a ratio very similar to the global model's and does just as well.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9901372194290161}]}, {"text": "On Answers-students, however, it learns weights similar to those of the in-domain model, again approaching best results for the domain.", "labels": [], "entities": []}, {"text": "Now, the labor of cleaning up at the karaoke parlor is realized.", "labels": [], "entities": []}, {"text": "Gold= Marcel Desailly, the France captain and Chelsea defender, believes the latter is true.: Sentence pairs from SMT and MSRpar-test with gold similarity scores and model errors (Global, Individual and Adaptive).", "labels": [], "entities": []}, {"text": "The adaptive model error is very close to the best model error in each case.", "labels": [], "entities": []}, {"text": "shows the effect of this on two specific sentence pairs as examples.", "labels": [], "entities": []}, {"text": "The first pair is from SMT; the adaptive model has a much lower error than the individual model on this pair, as it learns a higher relative weight for the embedding feature in this domain via inductive transfer from out-of-domain annotations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9678843021392822}]}, {"text": "The second pair, from MSRpar-test, shows the opposite: in-domain annotations help the adaptive model fix the faulty output of the global model by upweighting the alignment feature and downweighting the embedding feature.", "labels": [], "entities": [{"text": "MSRpar-test", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.886914849281311}]}, {"text": "The adaptive model gains from the strengths of both in-domain (higher relevance) and out-of-domain (more training data) annotations, leading to good results even in extreme scenarios (e.g., in domains with unique parameter distributions or noisy annotations).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation ratios of the three models vs.  the best model across STS domains. Best scores are  boldfaced, worst scores are underlined. The adap- tive model has the best (1) overall score, and (2)  consistency across domains.", "labels": [], "entities": [{"text": "consistency", "start_pos": 208, "end_pos": 219, "type": "METRIC", "confidence": 0.9980669617652893}]}, {"text": " Table 3: Feature weights and correlations of different  models in three extreme scenarios. In each case, the  adaptive model learns relative weights that are more  similar to those in the best baseline model.", "labels": [], "entities": []}, {"text": " Table 4: Sentence pairs from SMT and MSRpar-test  with gold similarity scores and model errors (Global,  Individual and Adaptive). The adaptive model error  is very close to the best model error in each case.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9414302706718445}]}]}