{"title": [{"text": "Visualizing and Understanding Neural Models in NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret.", "labels": [], "entities": []}, {"text": "For example it's not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases.", "labels": [], "entities": []}, {"text": "In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision.", "labels": [], "entities": []}, {"text": "We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allowing us to see well-known markedness asymmetries in negation.", "labels": [], "entities": []}, {"text": "We then introduce methods for visualizing a unit's salience, the amount that it contributes to the final composed meaning from first-order derivatives.", "labels": [], "entities": []}, {"text": "Our general-purpose methods may have wide applications for understanding com-positionality and other semantic properties of deep networks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural models match or outperform the performance of other state-of-the-art systems on a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Yet unlike traditional feature-based classifiers that assign and optimize weights to varieties of human interpretable features (parts-of-speech, named entities, word shapes, syntactic parse features etc) the behavior of deep learning models is much less easily interpreted.", "labels": [], "entities": [{"text": "assign and optimize weights to varieties of human interpretable features (parts-of-speech, named entities, word shapes, syntactic parse features", "start_pos": 54, "end_pos": 198, "type": "TASK", "confidence": 0.7197549979795109}]}, {"text": "Deep learning models mainly operate on word embeddings (low-dimensional, continuous, real-valued vectors) through multi-layer neural architectures, each layer of which is characterized as an array of hidden neuron units.", "labels": [], "entities": []}, {"text": "It is unclear how deep learning models deal with composition, implementing functions like negation or intensification, or combining meaning from different parts of the sentence, filtering away the informational chaff from the wheat, to build sentence meaning.", "labels": [], "entities": []}, {"text": "In this paper, we explore multiple strategies to interpret meaning composition in neural models.", "labels": [], "entities": [{"text": "interpret meaning composition", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6783996025721232}]}, {"text": "We employ traditional methods like representation plotting, and introduce simple strategies for measuring how much a neural unit contributes to meaning composition, its 'salience' or importance using first derivatives.", "labels": [], "entities": [{"text": "representation plotting", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.9302899539470673}, {"text": "meaning composition", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7051127403974533}]}, {"text": "Visualization techniques/models represented in this work shed important light on how neural models work: For example, we illustrate that LSTM's success is due to its ability in maintaining a much sharper focus on the important key words than other models; Composition in multiple clauses works competitively, and that the models are able to capture negative asymmetry, an important property of semantic compositionally in natural language understanding; there is sharp dimensional locality, with certain dimensions marking negation and quantification in a manner that was surprisingly localist.", "labels": [], "entities": [{"text": "semantic compositionally in natural language understanding", "start_pos": 394, "end_pos": 452, "type": "TASK", "confidence": 0.7054290374120077}]}, {"text": "Though our attempts only touch superficial points in neural models, and each method has its pros and cons, together they may offer some insights into the behaviors of neural models in language based tasks, marking one initial step toward understanding how they achieve meaning composition in natural language processing.", "labels": [], "entities": [{"text": "meaning composition", "start_pos": 269, "end_pos": 288, "type": "TASK", "confidence": 0.7388669550418854}]}, {"text": "The next section describes some visualization models in vision and NLP that have inspired this work.", "labels": [], "entities": []}, {"text": "We describe datasets and the adopted neural models in Section 3.", "labels": [], "entities": []}, {"text": "Different visualization strategies and correspondent analytical results are presented separately in Section 4,5,6, followed by a brief conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explored two datasets on which neural models are trained, one of which is of relatively small scale and the other of large scale.", "labels": [], "entities": []}], "tableCaptions": []}