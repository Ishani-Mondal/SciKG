{"title": [{"text": "Automatic Generation and Scoring of Positive Interpretations from Negated Statements", "labels": [], "entities": [{"text": "Automatic Generation and Scoring of Positive Interpretations from Negated Statements", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.7696139365434647}]}], "abstractContent": [{"text": "This paper presents a methodology to extract positive interpretations from negated statements.", "labels": [], "entities": []}, {"text": "First, we automatically generate plausible interpretations using well-known grammar rules and manipulating semantic roles.", "labels": [], "entities": []}, {"text": "Second, we score plausible alternatives according to their likelihood.", "labels": [], "entities": []}, {"text": "Manual annotations show that the positive interpretations are intuitive to humans, and experimental results show that the scoring task can be automated.", "labels": [], "entities": []}], "introductionContent": [{"text": "Negation is an intricate phenomenon present in all human languages, and studied from a theoretical perspective since Aristotle.", "labels": [], "entities": [{"text": "Negation", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9363080859184265}]}, {"text": "Acquiring and understanding negation is more challenging than language in general: children acquire negation after learning to communicate, and adults take longer to process negative sentences than positive ones.", "labels": [], "entities": [{"text": "understanding negation", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6353764235973358}]}, {"text": "In any given language, humans communicate in positive terms most of the time, and use negation to express something unusual or an exception.", "labels": [], "entities": []}, {"text": "In classical logic, negation is a simple unary operator that reverses the truth value of a proposition.", "labels": [], "entities": []}, {"text": "In natural language, negation is always marked and it is used to reverse polarity, i.e., turning something affirmative into negative, or something negative into affirmative.", "labels": [], "entities": []}, {"text": "Albeit most sentences are affirmative, negation is rather ubiquitous: In scientific papers, 13.76% of statements contain a negation (; in product reviews, 19% (; and in a selection of Conan Doyle stories, 22.23%).", "labels": [], "entities": []}, {"text": "From a theoretical perspective, it is accepted that negation conveys positive meaning).", "labels": [], "entities": []}, {"text": "For example, when reading (1) John doesn't eat meat, humans intuitively understand that (1a) John eats something other than meat, and (1b) Some people eat meat, but not John.", "labels": [], "entities": []}, {"text": "Extracting positive interpretations from negated statements automatically is not straightforward: a negated statement may convey one or more positive interpretations, and not all positive interpretations are equally likely.", "labels": [], "entities": [{"text": "Extracting positive interpretations from negated statements", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8732168078422546}]}, {"text": "For example, from (2) They didn't order the right parts, it is very likely that (2a) They ordered the wrong parts, but (2b) Somebody ordered the right parts, but not they is unlikely.", "labels": [], "entities": []}, {"text": "This paper presents a methodology to automatically extract and score positive interpretations from negated statements, as intuitively done by humans when reading text.", "labels": [], "entities": []}, {"text": "A key feature of the work presented here is that it is not tied to any existing approach to extract meaning from text-we generate positive interpretations in plain text, and these positive interpretations can be semantically represented with any existing approach.", "labels": [], "entities": []}, {"text": "The main contributions are: (1) procedure to automatically generate plausible positive interpretations from negated statements, (2) annotations scoring plausible positive interpretations, 1 and (3) experiments detailing results with several combinations of features, as well as goldstandard and predicted linguistic information.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results obtained with several combinations of features in.", "labels": [], "entities": []}, {"text": "We detail results obtained with features extracted from gold-standard and predicted linguistic annotations (part-of-speech tags, parse trees, semantic roles, etc.) as annotated in the gold and auto files from the CoNLL-2011 Shared Task release of).", "labels": [], "entities": [{"text": "CoNLL-2011 Shared Task release", "start_pos": 213, "end_pos": 243, "type": "DATASET", "confidence": 0.8308757841587067}]}, {"text": "All models are trained with gold-standard linguistic annotations, and tested with either goldstandard or predicted linguistic annotations.", "labels": [], "entities": []}, {"text": "Testing with gold-standard linguistic annotations.", "labels": [], "entities": []}, {"text": "Using only the label of the semantic role from which the positive interpretation was generated (sem role label), yields a Pearson correlation of 0.603.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 122, "end_pos": 141, "type": "METRIC", "confidence": 0.969274491071701}]}, {"text": "Using Verb features is virtually useless (Pearson correlation: -0.025), this is due to the Source Feature description Verb Word form and part-of-speech tag of verb sem role Semantic role label of sem role, sem role label Number of tokens in sem role Word form and part-of-speech tag of head of sem role Syntactic node of sem role, its parent and left and right siblings in the parse tree verb-sem role Whether verb occurs before or after than sem role in the negated statement Syntactic node of lowest common ancestor of verb and sem role Syntactic paths from verb to sem role verbarg-struct Flags indicating whether verb has each possible semantic role Semantic role labels of the first and last roles of verb Syntactic nodes and heads of each semantic role attaching to verb  fact that our corpus includes at most 5 instances for each negated verb in OntoNotes (Section 4).", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 853, "end_pos": 862, "type": "DATASET", "confidence": 0.9033927917480469}]}, {"text": "Adding sem role features yields a correlation of 0.630, and incorporating verb-sem role features is useless (Pearson: 0.627).", "labels": [], "entities": [{"text": "Pearson", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.8915492296218872}]}, {"text": "Considering all features, however, yields the highest correlation, 0.642.", "labels": [], "entities": [{"text": "correlation", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9837678074836731}]}, {"text": "Testing with predicted linguistic annotations.", "labels": [], "entities": []}, {"text": "We assigned 20% of instances to the test split, totalling 378 instances (Section 6).", "labels": [], "entities": []}, {"text": "However, some of these instances cannot be obtained using predicted role labels: a missing or incorrect semantic role will unequivocally lead to positive interpretations that are not in our corpus and thus evaluation is not straightforward.", "labels": [], "entities": []}, {"text": "Results presented with predicted linguistic annotations are calculated using only the 268 (out of 378) positive interpretations that are generated from predicted semantic roles and are also generated (and thus annotated in our corpus) from gold-standard linguistic annotations.", "labels": [], "entities": []}, {"text": "Results using only sem role label feature (0.642) are better or very similar than using any combination of features (0.638-0.650) except Verb features alone, which perform poorly as explained earlier.", "labels": [], "entities": []}, {"text": "These results should betaken with a grain of salt: the number of test instances is much lower.", "labels": [], "entities": []}, {"text": "Additionally, these 268 test instances correspond to positive interpretations generated from semantic roles that were predicted correctly automatically.", "labels": [], "entities": []}, {"text": "Role labels are predicted better for shorter sentences without complicated syntactic structure; positive interpretations for this kind sentences are also easier to score, e.g., Statement 4 in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Basic corpus analysis. For each semantic role, we", "labels": [], "entities": [{"text": "corpus analysis", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7571285665035248}]}, {"text": " Table 3: Annotation examples. We show all positive interpretations automatically generated and their scores (out of 5).", "labels": [], "entities": []}, {"text": " Table 4: Features used to score positive interpretations from negated statements. Features are extracted from the negated verb", "labels": [], "entities": []}, {"text": " Table 5: Pearson correlations in the test set using gold-standard and predicted linguistic annotations (part-of-speech tags, parse", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9195273816585541}]}]}