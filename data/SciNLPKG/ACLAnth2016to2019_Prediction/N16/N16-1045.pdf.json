{"title": [{"text": "Expectation-Regulated Neural Model for Event Mention Extraction", "labels": [], "entities": [{"text": "Event Mention Extraction", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7861796418825785}]}], "abstractContent": [{"text": "We tackle the task of extracting tweets that mention a specific event from all tweets that contain relevant keywords, for which the main challenges include unbalanced positive and negative cases, and the unavailability of manually labeled training data.", "labels": [], "entities": []}, {"text": "Existing methods leverage a few manually given seed events and large unlabeled tweets to train a classi-fier, by using expectation regularization training with discrete ngram features.", "labels": [], "entities": []}, {"text": "We propose a LSTM-based neural model that learns tweet-level features automatically.", "labels": [], "entities": []}, {"text": "Compared with discrete ngram features, the neural model can potentially capture non-local dependencies and deep semantic information, which are more effective for disambiguating subtle semantic differences between true event mentions and false cases that use similar wording patterns.", "labels": [], "entities": []}, {"text": "Results on both tweets and forum posts show that our neural model is more effective compared with a state-of-the-art discrete baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "A Distributed Denial of Service (DDoS) attack employs multiple compromised systems to interrupt or suspend services of a host connected to the Internet.", "labels": [], "entities": [{"text": "Distributed Denial of Service (DDoS)", "start_pos": 2, "end_pos": 38, "type": "TASK", "confidence": 0.7288025617599487}]}, {"text": "Victims are often high-profile web servers such as banks or credit card payment gateways, and therefore a single attack may cause considerable loss.", "labels": [], "entities": []}, {"text": "The aim of this paper is to build an automatic system which can extract DDoS event mentions from social media, a timely information source for events taking place around the world, so that the mined emerging incidents can serve as early DDoS warnings or signs for Internet service providers.", "labels": [], "entities": []}, {"text": "proposed the first work to extract cybersecurity event mentions from raw Twitter stream.", "labels": [], "entities": []}, {"text": "They investigated three different event categories, namely DDoS attacks, data breaches and account hijacking, by tracking the keywords ddos, breach and hacked, respectively.", "labels": [], "entities": []}, {"text": "Not all tweets containing the keywords describe events.", "labels": [], "entities": []}, {"text": "For example, the tweet \"give me paypall or i will tell my mum and ddos u\" shows a metaphor rather than a DDoS event.", "labels": [], "entities": []}, {"text": "As a result, the event mention extraction task involves a classification task that filters out true events from all tweets that contain event keywords.", "labels": [], "entities": [{"text": "event mention extraction task", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7218536585569382}]}, {"text": "Two main challenges exist for this task.", "labels": [], "entities": []}, {"text": "First, the numbers of positive and negative examples are typically unbalanced.", "labels": [], "entities": []}, {"text": "In our datasets, only about 22% of the tweets that contain the term ddos are mentions to DDoS attack events.", "labels": [], "entities": []}, {"text": "Second, there is typically little manual annotation available.", "labels": [], "entities": []}, {"text": "tackled the challenges by weakly supervising a classification model with a small number of human-provided seed events.", "labels": [], "entities": []}, {"text": "In particular, Ritter et al. exploit expectation regularization (ER;) for semi-supervised learning from large amounts of raw tweets that contain the event keyword.", "labels": [], "entities": []}, {"text": "They show that the ER approach outperforms semisupervised expectation-maximization and one-class support vector machine on the task.", "labels": [], "entities": []}, {"text": "They build a logistic regression classifier, using few humanlabeled seed events and domain knowledge on the ratio between positive and negative examples for ER in training.", "labels": [], "entities": []}, {"text": "Results show that the regulariza-tion method was effective on classifying unbalanced datasets.", "labels": [], "entities": []}, {"text": "Ritter et al. use manually-defined discrete features.", "labels": [], "entities": []}, {"text": "However, the event mention extraction task is highly semantic-driven, and simple textual patterns may suffer limitations in representing subtle semantic differences between true event mentions and false cases with similar word patterns.", "labels": [], "entities": [{"text": "event mention extraction", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.626022736231486}]}, {"text": "Recently, deep learning received increasing research attention in the NLP community.", "labels": [], "entities": [{"text": "deep learning", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8518504202365875}]}, {"text": "One important advantage of deep learning is automatic representation learning, which can effectively encodes syntactic and information about words, phrases and sentences in low-dimensional dense vectors.", "labels": [], "entities": [{"text": "automatic representation learning", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.6524422466754913}]}, {"text": "In this paper we exploit a deep neural model for event mention extraction, using word embeddings and a novel LSTM-based neural network structure to automatically obtain features fora tweet.", "labels": [], "entities": [{"text": "event mention extraction", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.7272217869758606}]}, {"text": "Results on two human-annotated datasets show that the proposed LSTM-based representation yields significant improvements over.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow and evaluate the performance by the area under the precision-recall curve (AUC), where precision is the fraction of retrieved instances that are event mentions, and re-  call is the fraction of gold event mention instances that are retrieved.", "labels": [], "entities": [{"text": "precision-recall curve (AUC)", "start_pos": 61, "end_pos": 89, "type": "METRIC", "confidence": 0.9726867437362671}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9986380934715271}, {"text": "re-  call", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9541684985160828}]}, {"text": "Precision-recall (PR) curves offer informative pictures on the classification of unbalanced classes).", "labels": [], "entities": [{"text": "Precision-recall (PR)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.9268548786640167}]}, {"text": "For the proposed model, we empirically set the LSTM output vector ht , the NTN output V , and the size of the hidden layer to 32. 7 For the ER model, the human-provided label expectation prior\u02dcpprior\u02dc prior\u02dcp is set to 0.22 since the percentage of positives in the development set is 22%, and the parameter \u03bb U is set to one-tenth of the positive training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 5: AUCs of different model architectures.", "labels": [], "entities": []}, {"text": " Table 8: Top 5 and bottom 5 ranked dark web sentences as determined by the baseline and the proposed  LSTM-based model. Format: class label|baseline score|neural score.", "labels": [], "entities": []}]}