{"title": [{"text": "Association for Computational Linguistics Joint Learning with Global Inference for Comment Classification in Community Question Answering", "labels": [], "entities": [{"text": "Comment Classification in Community Question Answering", "start_pos": 83, "end_pos": 137, "type": "TASK", "confidence": 0.7329031626383463}]}], "abstractContent": [{"text": "This paper addresses the problem of comment classification in community Question Answering.", "labels": [], "entities": [{"text": "comment classification in community Question Answering", "start_pos": 36, "end_pos": 90, "type": "TASK", "confidence": 0.6213294565677643}]}, {"text": "Following the state of the art, we approach the task with a global inference process to exploit the information of all comments in the answer-thread in the form of a fully connected graph.", "labels": [], "entities": []}, {"text": "Our contribution comprises two novel joint learning models that are on-line and integrate inference within learning.", "labels": [], "entities": []}, {"text": "The first one jointly learns two node-and edge-level MaxEnt classifiers with stochastic gradient descent and integrates the inference step with loopy belief propagation.", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.6869152983029684}]}, {"text": "The second model is an instance of fully connected pairwise CRFs (FCCRF).", "labels": [], "entities": []}, {"text": "The FCCRF model significantly outperforms all other approaches and yields the best results on the task to date.", "labels": [], "entities": [{"text": "FCCRF", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.41440731287002563}]}, {"text": "Crucial elements for its success are the global normalization and an Ising-like edge potential.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7129689455032349}]}], "introductionContent": [{"text": "Online community fora have been gaining a lot of popularity in recent years.", "labels": [], "entities": []}, {"text": "Many of them, such as Stack Exchange , are quite open, allowing anybody to ask and anybody to answer a question, which makes them very valuable sources of information.", "labels": [], "entities": [{"text": "Stack Exchange", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8559917509555817}]}, {"text": "Yet, this same democratic nature resulted in some questions accumulating a large number of answers, many of which are of low quality.", "labels": [], "entities": []}, {"text": "While nowadays online fora are typically searched using standard search engines that index entire threads, this is not optimal, as it can be very time-consuming fora user to go through and make sense of along thread.", "labels": [], "entities": []}, {"text": "Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently;.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7621491750081381}]}, {"text": "There have been also related shared tasks at SemEval-2015 2 and SemEval-2016.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers.", "labels": [], "entities": [{"text": "classifying comments in the answer-thread of a given question as good or bad answers", "start_pos": 53, "end_pos": 137, "type": "TASK", "confidence": 0.704805565731866}]}, {"text": "presents an excerpt of areal example from the QatarLiving dataset from.", "labels": [], "entities": [{"text": "QatarLiving dataset", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9882121980190277}]}, {"text": "There is a question on top (Q) followed by eight comments (A 1 , A 2 , \u00b7 \u00b7 \u00b7 , A 8 ).", "labels": [], "entities": [{"text": "A 8 )", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9464896519978842}]}, {"text": "According to the human annotations ('Human'), all comments but 3 and 5 are good answers to Q.", "labels": [], "entities": []}, {"text": "The comments also contain the predictions of a good-vs-bad binary classifier trained with state-of-the-art features on this dataset (; its errors are highlighted in red.", "labels": [], "entities": []}, {"text": "Many comments are short, making it difficult for the classifier to make the right decisions, but some errors could be corrected using information in the other comments.", "labels": [], "entities": []}, {"text": "For instance, comments 4 and 7 are similar to each other, but also to comments 2 and 8 ('Hobby shop', 'City Center', etc.).", "labels": [], "entities": [{"text": "City Center'", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.8234518965085348}]}, {"text": "It seems reasonable to think that similar comments should have the same labels, so comments 2, 4, 7 and 8 should all be labeled consistently as good comments.", "labels": [], "entities": []}, {"text": "Indeed, recent work has shown the benefit of using varied thread-level information for answer classification, either by developing features modeling the thread structure and dialogue , or by applying global inference mechanisms at the thread level using the predictions of local classifiers ( . We follow the second approach, assuming a graph representation of the answer-thread, where nodes are comments and edges represent pairwise (similarity) relations between them.", "labels": [], "entities": [{"text": "answer classification", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.9333316385746002}]}, {"text": "Classification decisions are at the level of nodes and edges, and global inference is used to get the best label assignment to all comments.", "labels": [], "entities": []}, {"text": "http://alt.qcri.org/semeval2015/task3/ 3 http://alt.qcri.org/semeval2016/task3/ Our main contribution is to propose online models for learning the decisions jointly, incorporating the inference inside the joint learning algorithm.", "labels": [], "entities": []}, {"text": "Building on the ideas from papers coupling learning and inference for NLP structure prediction problems), we propose joint learning of two MaxEnt classifiers with stochastic gradient descent, integrating global inference based on loopy belief propagation.", "labels": [], "entities": [{"text": "NLP structure prediction", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7338109612464905}]}, {"text": "We also propose a joint model with global normalization, that is an instance of Fully Connected Conditional Random Fields.", "labels": [], "entities": []}, {"text": "We compare our joint models with the previous state of the art for the comment classification problem.", "labels": [], "entities": [{"text": "comment classification", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.8185029625892639}]}, {"text": "We find that the coupled learning-and-inference model is not competitive, probably due to the label bias problem.", "labels": [], "entities": []}, {"text": "On the contrary, the fully connected CRF model improves results significantly overall rivaling models, yielding the best results on the task to date.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, after discussing related work in Section 2, we introduce our joint models in Section 3.", "labels": [], "entities": []}, {"text": "We then describe our experimental settings in Section 4.", "labels": [], "entities": []}, {"text": "The experiments and analysis of results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we summarize our contributions with future directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experimental setting.", "labels": [], "entities": []}, {"text": "We first introduce the dataset we use, then we present the features and the models that we compare.", "labels": [], "entities": []}, {"text": "We experimented with the dataset from SemEval-2015 Task 3 on Answer Selection for Community Question Answering ( . The dataset contains question-answer threads from the Qatar Living forum.", "labels": [], "entities": [{"text": "Answer Selection for Community Question Answering", "start_pos": 61, "end_pos": 110, "type": "TASK", "confidence": 0.6449148654937744}, {"text": "Qatar Living forum", "start_pos": 169, "end_pos": 187, "type": "DATASET", "confidence": 0.9439968268076578}]}, {"text": "Each thread consists of a question followed by one or more (up to 143) comments.", "labels": [], "entities": []}, {"text": "The dataset is split into training, development and test sets, with 2,600, 300, and 329 questions, and 16,541, 1,645, and 1,976 answers, respectively.", "labels": [], "entities": []}, {"text": "Each comment in the dataset is annotated with one of the following labels, reflecting how well it answers the question: Good, Potential, Bad, Dialogue, Not English, and Other.", "labels": [], "entities": []}, {"text": "At SemEval-2015 Task 3, the latter four classes were merged into BAD at testing time, and the evaluation measure uses a macroaveraged F 1 over the three classes: Good, Potential, and BAD.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.6957483490308126}, {"text": "BAD", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9743061065673828}, {"text": "F 1", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.8597034215927124}, {"text": "BAD", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9933644533157349}]}, {"text": "Unfortunately, the Potential class was both the smallest (covering about 10% of the data), and also the noisiest and the hardest to predict; yet, its impact was magnified by the macro-averaged F 1 . Thus, subsequent work has further merged Potential under BAD ( , and has used for evaluation F 1 with respect to the Good category (or just accuracy).", "labels": [], "entities": [{"text": "F 1", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9103541672229767}, {"text": "BAD", "start_pos": 256, "end_pos": 259, "type": "METRIC", "confidence": 0.6967818140983582}, {"text": "accuracy", "start_pos": 339, "end_pos": 347, "type": "METRIC", "confidence": 0.9977397918701172}]}, {"text": "For our experiments below, we also report F 1 for the Good class and the overall accuracy.", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.98984095454216}, {"text": "Good class", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.6689970791339874}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.999572217464447}]}, {"text": "We further perform statistical significance tests using an approximate randomization test based on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9988753199577332}]}, {"text": "We used SIGF V.) with 10,000 iterations.", "labels": [], "entities": []}, {"text": "All results we report below are calculated in the test set, using parameters tuned on the development set.", "labels": [], "entities": []}, {"text": "Our main results are shown in, where we report accuracy (Acc) as well as precision (P), recall (R) and F 1 -score (F 1 ) for the good class.", "labels": [], "entities": [{"text": "accuracy (Acc)", "start_pos": 47, "end_pos": 61, "type": "METRIC", "confidence": 0.8426270037889481}, {"text": "precision (P)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9563169628381729}, {"text": "recall (R)", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9623203575611115}, {"text": "F 1 -score (F 1 )", "start_pos": 103, "end_pos": 120, "type": "METRIC", "confidence": 0.9653895795345306}]}, {"text": "The models are organized in four blocks.", "labels": [], "entities": []}, {"text": "On top, we see that the majority class baseline achieves accuracy of 50.5%, as the dataset is very well balanced between the classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9996010661125183}]}, {"text": "In block II, we find the results for the local classifiers, ICC ME and ICC P erc , which achieve very similar results.", "labels": [], "entities": [{"text": "ICC P erc", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.6991771459579468}]}, {"text": "They are comparable to MaxEnt in, where we report the best published results on this dataset; yet, our classifiers are trained on-line.", "labels": [], "entities": []}, {"text": "Block III in the table reports results for models that train two local MaxEnt classifiers and then perform thread-level inference using either graph-cut (LI M E\u2212GC ) or loopy BP (LI M E\u2212LBP ).", "labels": [], "entities": []}, {"text": "This yields improvements over the ICC models with the threadlevel inference in block II, which is consistent with the findings of (Joty et al., 2015); however, the difference in terms of accuracy is not statistically significant (p-value = 0.09).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9989732503890991}]}], "tableCaptions": [{"text": " Table 1: Results of all compared models on the test set. The best results are boldfaced.", "labels": [], "entities": []}, {"text": " Table 2: Comparison to the best published results on  the same datasets, as reported in (Joty et al., 2015).", "labels": [], "entities": []}, {"text": " Table 3: Results for different variants of the joint  CRF model on the test set.", "labels": [], "entities": []}]}