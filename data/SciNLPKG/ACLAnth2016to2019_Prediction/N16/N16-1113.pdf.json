{"title": [{"text": "A Novel Approach to Dropped Pronoun Translation", "labels": [], "entities": [{"text": "Dropped Pronoun Translation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6691856483618418}]}], "abstractContent": [{"text": "Dropped Pronouns (DP) in which pronouns are frequently dropped in the source language but should be retained in the target language are challenge in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7112279832363129}]}, {"text": "In response to this problem, we propose a semi-supervised approach to recall possibly missing pronouns in the translation.", "labels": [], "entities": []}, {"text": "Firstly, we build training data for DP generation in which the DPs are automatically labelled according to the alignment information from a parallel corpus.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9917596578598022}]}, {"text": "Secondly, we build a deep learning-based DP generator for input sentences in decoding when no corresponding references exist.", "labels": [], "entities": []}, {"text": "More specifically, the generation is two-phase: (1) DP position detection, which is modeled as a sequential labelling task with recurrent neural networks; and (2) DP prediction, which employs a multilayer perceptron with rich features.", "labels": [], "entities": [{"text": "DP position detection", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.6997824509938558}, {"text": "DP prediction", "start_pos": 163, "end_pos": 176, "type": "TASK", "confidence": 0.8202008008956909}]}, {"text": "Finally, we integrate the above outputs into our translation system to recall missing pronouns by both extracting rules from the DP-labelled training data and translating the DP-generated input sentences.", "labels": [], "entities": [{"text": "DP-labelled training data", "start_pos": 129, "end_pos": 154, "type": "DATASET", "confidence": 0.7070671319961548}]}, {"text": "Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9951714873313904}, {"text": "translation", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.963713526725769}, {"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9995470643043518}, {"text": "DP generation", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.8700189292430878}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.8727995157241821}]}], "introductionContent": [{"text": "In pro-drop languages, certain classes of pronouns can be omitted to make the sentence compact yet comprehensible when the identity of the pronouns can be inferred from the context (.", "labels": [], "entities": []}, {"text": "shows an example, in which Chinese is a pro-drop language, while English is not).", "labels": [], "entities": []}, {"text": "On the Chinese side, the subject pronouns { (you), (I)} and the object pronouns { (it), (you)} are omitted in the dialogue between Speakers A and B.", "labels": [], "entities": []}, {"text": "These omissions may not be problems for humans since people can easily recall the missing pronouns from the context.", "labels": [], "entities": []}, {"text": "However, this poses difficulties for Statistical Machine Translation (SMT) from pro-drop languages (e.g. Chinese) to non-pro-drop languages (e.g. English), since translation of such missing pronouns cannot be normally reproduced.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.8692037363847097}]}, {"text": "Generally, this phenomenon is more common in informal genres such as dialogues and conversations than others ().", "labels": [], "entities": []}, {"text": "We also validated this finding by analysing a large Chinese-English dialogue corpus which consists of 1M sentence pairs extracted from movie and TV episode subtitles.", "labels": [], "entities": []}, {"text": "We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing.", "labels": [], "entities": []}, {"text": "In response to this problem, we propose to find a general and replicable way to improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9598119854927063}]}, {"text": "The main challenge of this research is that training data for DP generation are scarce.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9906440675258636}]}, {"text": "Most works either apply manual annotation ( or use existing but small-scale resources such as the Penn Treebank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.994614988565445}]}, {"text": "In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9904017448425293}]}, {"text": "The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language).", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9791714549064636}]}, {"text": "To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring.", "labels": [], "entities": []}, {"text": "After building the training data for DP generation, we apply a supervised approach to build our DP generator.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9821749031543732}]}, {"text": "We divide the DP generation task into two phases: DP detection (from which position a pronoun is dropped), and DP prediction (which pronoun is dropped).", "labels": [], "entities": [{"text": "DP generation task", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.952219029267629}, {"text": "DP detection", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8200383484363556}, {"text": "DP prediction", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.764584094285965}]}, {"text": "Due to the powerful capacity of feature learning and representation learning, we model the DP detection problem as sequential labelling with Recurrent Neural Networks (RNNs) and model the prediction problem as classification with Multi-Layer Perceptron (MLP) using features at various levels: from lexical, through contextual, to syntax.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.895304262638092}, {"text": "DP detection", "start_pos": 91, "end_pos": 103, "type": "TASK", "confidence": 0.9693414866924286}]}, {"text": "Finally, we try to improve the translation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences.", "labels": [], "entities": [{"text": "translation of missing pronouns", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8496526032686234}]}, {"text": "More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a \"pronoun-complete\" translation model.", "labels": [], "entities": [{"text": "DP-inserted parallel corpus", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.6603272557258606}]}, {"text": "In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model.", "labels": [], "entities": []}, {"text": "This makes the input sentences more consistent with the additional pronoun-complete rule table.", "labels": [], "entities": []}, {"text": "To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (.", "labels": [], "entities": [{"text": "DP prediction", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8923811316490173}]}, {"text": "To validate the effect of the proposed approach, we carried out experiments on a Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.7362274726231893}]}, {"text": "Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points () using the additional translation model trained on the DP-inserted corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9991594552993774}, {"text": "DP-inserted corpus", "start_pos": 186, "end_pos": 204, "type": "DATASET", "confidence": 0.8467802703380585}]}, {"text": "Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 BLEU point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9992665648460388}]}, {"text": "Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points).", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9654950499534607}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9988501071929932}]}, {"text": "Generally, the contributions of this paper include the following: \u2022 We propose an automatic method to build a large-scale DP training corpus.", "labels": [], "entities": [{"text": "DP training", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.8983637392520905}]}, {"text": "Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; \u2022 Benefiting from representation learning, our deep learning-based generation models are able to avoid ignore the complex featureengineering work while still yielding encouraging results; \u2022 To decrease the negative effects on translation caused by inserting incorrect DPs, we force the SMT system to arbitrate between multiple ambiguous hypotheses from the DP predictions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 417, "end_pos": 420, "type": "TASK", "confidence": 0.973552942276001}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our approaches to building the DP corpus, DP generator and SMT integration.", "labels": [], "entities": [{"text": "DP corpus", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.7088448107242584}, {"text": "SMT integration", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.9910756051540375}]}, {"text": "Related work is described in Section 3.", "labels": [], "entities": []}, {"text": "The experimental results for both the DP generator and translation are reported in Section 4.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9418432712554932}]}, {"text": "Section 5 analyses some real examples which is followed by our conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first check whether our DP annotation strategy is reasonable.", "labels": [], "entities": []}, {"text": "To this end, we follow the strategy to automatically and manually label the source sides of the development and test data with their target sides.", "labels": [], "entities": []}, {"text": "The agreement between automatic labels and manual labels on DP prediction are 94% and 95% on development and test data and on DP generation are 92% and 92%, respectively.", "labels": [], "entities": [{"text": "DP prediction", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9660768508911133}, {"text": "DP generation", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.8681359887123108}]}, {"text": "This indicates that the automatic annotation strategy is relatively trustworthy.", "labels": [], "entities": []}, {"text": "We then measure the accuracy (in terms of words) of our generation models in two phases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9992706179618835}]}, {"text": "\"DP Detection\" shows the performance of our sequencelabelling model based on RNN.", "labels": [], "entities": [{"text": "DP Detection", "start_pos": 1, "end_pos": 13, "type": "TASK", "confidence": 0.7069196701049805}]}, {"text": "We only consider the tag for each word (pro-drop or not pro-drop before the current word), without considering the exact pronoun for DPs.", "labels": [], "entities": []}, {"text": "\"DP Prediction\" shows the performance of the MLP classifier in determining the exact DP based on detection.", "labels": [], "entities": []}, {"text": "Thus we consider both the detected and predicted pronouns.", "labels": [], "entities": []}, {"text": "lists the results of the above DP generation approaches.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.971544623374939}]}, {"text": "The F1 score of \"DP Detection\" achieves 88% and 86% on the Dev and Test set, respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9788612425327301}, {"text": "DP Detection", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.6712760627269745}, {"text": "Dev and Test set", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.6882918626070023}]}, {"text": "However, it has lower F1 scores of 66% and 65% for the final pronoun generation (\"DP Prediction\") on the development and test data, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9997535347938538}, {"text": "DP Prediction", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.8421266078948975}]}, {"text": "This indicates that predicting the exact DP in Chinese is a really difficult task.", "labels": [], "entities": [{"text": "predicting the exact DP", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.6183233186602592}]}, {"text": "Even though the DP prediction is not highly accurate, we still hypothesize that the DP generation models are reliable enough to be used for end-to-end machine translation.", "labels": [], "entities": [{"text": "DP prediction", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8172248899936676}, {"text": "machine translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.659612625837326}]}, {"text": "Note that we only show the results of 1-best DP generation here, but in the translation task, we use N -best generation candidates to recall more DPs.", "labels": [], "entities": [{"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.9043891131877899}]}, {"text": "In this section, we evaluate the end-to-end translation quality by integrating the DP generation results (Section 3.3).", "labels": [], "entities": []}, {"text": "summaries the results of translation performance with different sources of DP information.", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9721680879592896}]}, {"text": "\"Baseline\" uses the original input to feed the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9875861406326294}]}, {"text": "TM\" denotes using an additional translation model trained on the DPinserted training corpus, while \"+DP-gen.", "labels": [], "entities": [{"text": "TM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5334762930870056}, {"text": "DPinserted training corpus", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.8162046869595846}]}, {"text": "Input N\" denotes further completing the input sentences with the N -best pronouns generated from the DP generation model.", "labels": [], "entities": []}, {"text": "\"Oracle\" uses the input with manual (\"Manual\") or automatic (\"Auto\") insertion of DPs by considering the target set.", "labels": [], "entities": []}, {"text": "Taking \"Auto Oracle\" for example, we annotate the DPs via alignment information (supposing the reference is available) using the technique described in Section 2.1.", "labels": [], "entities": []}, {"text": "The baseline system uses the parallel corpus and input sentences without inserting/generating DPs.", "labels": [], "entities": []}, {"text": "It achieves 20.06 and 18.76 in BLEU score on the development and test data, respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9662557244300842}]}, {"text": "The BLEU scores are relatively low because 1) we have only one reference, and 2) dialogue machine translation is still a challenge for the current SMT approaches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9990326166152954}, {"text": "dialogue machine translation", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.7828638354937235}, {"text": "SMT", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.993280827999115}]}, {"text": "By using an additional translation model trained on the DP-inserted parallel corpus as described in Section 2.1, we improve the performance consistently on both development (+0.26) and test data (+0.61).", "labels": [], "entities": [{"text": "DP-inserted parallel corpus", "start_pos": 56, "end_pos": 83, "type": "DATASET", "confidence": 0.7152950366338094}]}, {"text": "This indicates that the inserted DPs are helpful for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9970172643661499}]}, {"text": "Thus, the gain in the \"+DP-ins TM\" is mainly from the improved alignment quality.", "labels": [], "entities": []}, {"text": "We can further improve translation performance by completing the input sentences with our DP gen-eration model as described in Section 2.2.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.972679078578949}]}, {"text": "We test N -best DP insertion to examine the performance, where N ={1, 2, 4, 6, 8}.", "labels": [], "entities": [{"text": "DP insertion", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.869943380355835}]}, {"text": "Working together with \"DP-ins.", "labels": [], "entities": []}, {"text": "TM\", 1-best generated input already achieves +0.43 and + 0.74 BLEU score improvements on development and test set, respectively.", "labels": [], "entities": [{"text": "TM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9014294743537903}, {"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9748037159442902}]}, {"text": "The consistency between the input sentences and the DPinserted parallel corpus contributes most to these further improvements.", "labels": [], "entities": [{"text": "DPinserted parallel corpus", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.7176164388656616}]}, {"text": "As N increases, the BLEU score grows, peaking at 21.61 and 20.34 BLEU points when N =6.", "labels": [], "entities": [{"text": "N", "start_pos": 3, "end_pos": 4, "type": "METRIC", "confidence": 0.974949300289154}, {"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9804719686508179}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9988270401954651}]}, {"text": "Thus we achieve a final improvement of 1.55 and 1.58 BLEU points on the development and test data, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9990615248680115}]}, {"text": "However, when adding more DP candidates, the BLEU score decreases by 0.97 and 0.51.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.977186769247055}]}, {"text": "The reason for this maybe that more DP candidates add more noise, which harms the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.9581232070922852}]}, {"text": "The oracle system uses the input sentences with manually annotated DPs rather than \"DP-gen.", "labels": [], "entities": []}, {"text": "The performance gap between \"Oracle\" and \"+DP-gen.", "labels": [], "entities": []}, {"text": "Input\" shows that there is still a large space (+4.22 or +3.17) for further improvement for the DP generation model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: List of features.", "labels": [], "entities": []}, {"text": " Table 3. As far as the DP  training corpus is concerned, we annotate the Chi- nese side of the parallel data using the approach de- scribed in Section 2.1. There are two different lan- guage models for the DP annotation (Section 2.1)  and translation tasks, respectively: one is trained on  the 2.13TB Chinese Web Page Collection Corpus 5  while the other one is trained on all extracted 7M  English subtitle data (", "labels": [], "entities": [{"text": "translation", "start_pos": 240, "end_pos": 251, "type": "TASK", "confidence": 0.9630266427993774}, {"text": "2.13TB Chinese Web Page Collection Corpus 5", "start_pos": 296, "end_pos": 339, "type": "DATASET", "confidence": 0.8937738793236869}, {"text": "7M  English subtitle data", "start_pos": 389, "end_pos": 414, "type": "DATASET", "confidence": 0.8150037676095963}]}, {"text": " Table 3: Statistics of corpora.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of DP generation quality.", "labels": [], "entities": [{"text": "DP generation", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9163371920585632}]}, {"text": " Table 5: Evaluation of DP translation quality.", "labels": [], "entities": [{"text": "DP translation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9475512206554413}]}]}