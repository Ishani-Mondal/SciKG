{"title": [{"text": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.6111693605780602}]}], "abstractContent": [{"text": "We propose multi-way, multilingual neural machine translation.", "labels": [], "entities": [{"text": "multilingual neural machine translation", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.6191796585917473}]}, {"text": "The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages.", "labels": [], "entities": []}, {"text": "This is made possible by having a single attention mechanism that is shared across all language pairs.", "labels": [], "entities": []}, {"text": "We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9649960994720459}]}, {"text": "In particular , we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation It has been shown that a deep (recurrent) neural network can successfully learn a complex mapping between variablelength input and output sequences on its own.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8182363708813986}]}, {"text": "Some of the earlier successes in this task have, for instance, been handwriting recognition () and speech recognition (.", "labels": [], "entities": [{"text": "handwriting recognition", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.9262113869190216}, {"text": "speech recognition", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9074388742446899}]}, {"text": "More recently, a general framework of encoderdecoder networks has been found to be effective at learning this kind of sequence-to-sequence mapping by using two recurrent neural networks (.", "labels": [], "entities": []}, {"text": "A basic encoder-decoder network consists of two recurrent networks.", "labels": [], "entities": []}, {"text": "The first network, called an encoder, maps an input sequence of variable length into a point in a continuous vector space, resulting in a fixed-dimensional context vector.", "labels": [], "entities": []}, {"text": "The other recurrent neural network, called a decoder, then generates a target sequence again of variable length starting from the context vector.", "labels": [], "entities": []}, {"text": "This approach however has been found to be inefficient in () when handling long sentences, due to the difficulty in learning a complex mapping between an arbitrary long sentence and a single fixed-dimensional vector.", "labels": [], "entities": []}, {"text": "In (), a remedy to this issue was proposed by incorporating an attention mechanism to the basic encoder-decoder network.", "labels": [], "entities": []}, {"text": "The attention mechanism in the encoder-decoder network frees the network from having to map a sequence of arbitrary length to a single, fixed-dimensional vector.", "labels": [], "entities": []}, {"text": "Since this attention mechanism was introduced to the encoder-decoder network for machine translation, neural machine translation, which is purely based on neural networks to perform full end-to-end translation, has become competitive with the existing phrase-based statistical machine translation in many language pairs).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7551830112934113}, {"text": "neural machine translation", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6848474343617758}, {"text": "phrase-based statistical machine translation", "start_pos": 252, "end_pos": 296, "type": "TASK", "confidence": 0.6287569776177406}]}, {"text": "Multilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language.", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation Existing machine translation", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.7841772522245135}]}, {"text": "This kind of mapping is strictly specific to a given language pair, and it is not trivial to extend this mapping to work on multiple pairs of languages.", "labels": [], "entities": []}, {"text": "A system based on neural machine translation, on the other hand, can be decomposed into two mod-ules.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.7737367153167725}]}, {"text": "The encoder maps a source sentence into a continuous representation, either a fixed-dimensional vector in the case of the basic encoder-decoder network or a set of vectors in the case of attentionbased encoder-decoder network.", "labels": [], "entities": []}, {"text": "The decoder then generates a target translation based on this source representation.", "labels": [], "entities": []}, {"text": "This makes it possible conceptually to build a system that maps a source sentence in any language to a common continuous representation space and decodes the representation into any of the target languages, allowing us to make a multilingual machine translation system.", "labels": [], "entities": [{"text": "multilingual machine translation", "start_pos": 229, "end_pos": 261, "type": "TASK", "confidence": 0.7000953356424967}]}, {"text": "This possibility is straightforward to implement and has been validated in the case of basic encoderdecoder networks (.", "labels": [], "entities": []}, {"text": "It is however not so, in the case of the attention-based encoder-decoder network, as the attention mechanism, or originally called the alignment function in (, is conceptually language pair-specific.", "labels": [], "entities": []}, {"text": "In (, the authors cleverly avoided this issue of language pair-specific attention mechanism by considering only a one-tomany translation, where each target language decoder embedded its own attention mechanism.", "labels": [], "entities": []}, {"text": "Also, we notice that both of these works have only evaluated their models on relatively small-scale tasks, making it difficult to assess whether multilingual neural machine translation can scale beyond lowresource language translation.", "labels": [], "entities": [{"text": "multilingual neural machine translation", "start_pos": 145, "end_pos": 184, "type": "TASK", "confidence": 0.6231823489069939}, {"text": "lowresource language translation", "start_pos": 202, "end_pos": 234, "type": "TASK", "confidence": 0.643871416648229}]}, {"text": "Multi-Way, Multilingual Neural Machine Translation In this paper, we first step back from the currently available multilingual neural translation systems proposed in ( and ask the question of whether the attention mechanism can be shared across multiple language pairs.", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.6550709903240204}, {"text": "multilingual neural translation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.7920275131861368}]}, {"text": "As an answer to this question, we propose an attention-based encoder-decoder network that admits a shared attention mechanism with multiple encoders and decoders.", "labels": [], "entities": []}, {"text": "We use this model for all the experiments, which suggests that it is indeed possible to share an attention mechanism across multiple language pairs.", "labels": [], "entities": []}, {"text": "The next question we ask is the following: in which scenario would the proposed multi-way, multilingual neural translation have an advantage over the existing, single-pair model?", "labels": [], "entities": [{"text": "multilingual neural translation", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.6707614064216614}]}, {"text": "Specifically, we consider a case of the translation between a lowresource language pair.", "labels": [], "entities": []}, {"text": "The experiments show that the proposed multi-way, multilingual model generalizes better than the single-pair translation model, when the amount of available parallel corpus is small.", "labels": [], "entities": []}, {"text": "Furthermore, we validate that this is not only due to the increased amount of target-side, monolingual corpus.", "labels": [], "entities": []}, {"text": "Finally, we train a single model with the proposed architecture on all the language pairs from the WMT'15; English, French, Czech, German, Russian and Finnish.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9052382111549377}]}, {"text": "The experiments show that it is indeed possible to train a single attention-based network to perform multi-way translation.", "labels": [], "entities": [{"text": "multi-way translation", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.6250917017459869}]}], "datasetContent": [{"text": "We evaluate the proposed multi-way, multilingual translation model on all the pairs available from WMT'15-English (En) \u2194 French (Fr), Czech (Cs), German (De), Russian (Ru) and Finnish (Fi)-, totalling ten directed pairs.", "labels": [], "entities": [{"text": "WMT'15-English", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9117987751960754}]}, {"text": "For each pair, we concatenate all the available parallel corpora from WMT'15 and use it as a training set.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9477307796478271}]}, {"text": "We use newstest-2013 as a development set and newstest-2015 as a test set, in all the pairs other than Fi-En.", "labels": [], "entities": [{"text": "Fi-En", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.9610991477966309}]}, {"text": "In the case of Fi-En, we use newsdev-2015 and newstest-2015 as a development set and test set, respectively.", "labels": [], "entities": [{"text": "Fi-En", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.9376179575920105}]}, {"text": "Data Preprocessing Each training corpus is tokenized using the tokenizer script from the Moses decoder.", "labels": [], "entities": []}, {"text": "The tokenized training corpus is cleaned fol-lowing the procedure in ( . Instead of using space-separated tokens, or words, we use sub-word units extracted by byte pair encoding, as recently proposed in ().", "labels": [], "entities": []}, {"text": "For each and every language, we include 30k sub-word symbols in a vocabulary.", "labels": [], "entities": []}, {"text": "See for the statistics of the final, preprocessed training corpora.", "labels": [], "entities": []}, {"text": "Evaluation Metric We mainly use BLEU as an evaluation metric using the multi-bleu script from Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9975897073745728}]}, {"text": "3 BLEU is computed on the tokenized text after merging the BPE-based sub-word symbols.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.999171257019043}]}, {"text": "We further look at the average log-probability assigned to reference translations by the trained model as an additional evaluation metric, as away to measure the model's density estimation performance free from any error caused by approximate decoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores where the target pair's parallel corpus is", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989907145500183}]}, {"text": " Table 3: (a) BLEU scores and (b) average log-probabilities for all the five languages from WMT'15.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9993500113487244}, {"text": "WMT'15", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9525729417800903}]}]}