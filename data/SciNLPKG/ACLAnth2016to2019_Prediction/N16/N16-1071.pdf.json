{"title": [{"text": "Vision and Feature Norms: Improving automatic feature norm learning through cross-modal maps", "labels": [], "entities": [{"text": "Feature Norms", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.6665031015872955}, {"text": "Improving automatic feature norm learning", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.6495636820793151}]}], "abstractContent": [{"text": "Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts.", "labels": [], "entities": []}, {"text": "Recent work has focused on text as the main source of information for automatic property extraction.", "labels": [], "entities": [{"text": "automatic property extraction", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.670290599266688}]}, {"text": "In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm and visual spaces.", "labels": [], "entities": [{"text": "property norm prediction", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.642308364311854}]}, {"text": "We also investigate the importance of having a complete feature norm dataset, for both training and testing.", "labels": [], "entities": []}, {"text": "Finally, we evaluate how these datasets and cross-modal maps can be used in an image retrieval task.", "labels": [], "entities": [{"text": "image retrieval task", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8025278846422831}]}], "introductionContent": [{"text": "Many cognitive theories of conceptual organisation assume that concepts are distributed representations over semantic primitives, often referred to as features or properties 1 ().", "labels": [], "entities": []}, {"text": "That is, we can understand the meaning of a concept through its properties.", "labels": [], "entities": []}, {"text": "For example, understanding the meaning of BANANA is closely related to understanding that it has properties such as is a fruit, is yellow, is long, is sweet, and knowing how these properties overlap with or differ from the properties of other concepts.", "labels": [], "entities": [{"text": "BANANA", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.7753018736839294}]}, {"text": "A number of property norm datasets, where humans were asked to list attributes of given concepts, have been collected to test this hypothesis.", "labels": [], "entities": []}, {"text": "After having been used to test models of conceptual representation in cognitive science for decades (), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.7288278937339783}]}, {"text": "More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics ().", "labels": [], "entities": []}, {"text": "Such models aim to addres the grounding problem) that distributional semantic models of language ( suffer from.", "labels": [], "entities": []}, {"text": "Property norms area valuable source of semantic information, and can potentially be applied to a variety of NLP tasks, but are expensive to obtain because they involve intensive human annotation.", "labels": [], "entities": []}, {"text": "The largest property norm dataset to date consists of just 638 concepts, and the most widely cited one presents properties for only 541 concepts).", "labels": [], "entities": []}, {"text": "If we are to use these datasets in large-scale semantic tasks, we would need to extend the currently available property norms by obtaining annotations for more than just a few hundred words.", "labels": [], "entities": []}, {"text": "The alternative to collecting more data through human annotation is to increase the coverage of property norms datasets by automatically inferring properties of concepts from easily accessible resources, such as textual data.", "labels": [], "entities": []}, {"text": "Considering the fact that concepts, as well as their properties, are in linguistic form, the task then becomes a bootstrapping one where we take advantage of the abundance of freely available textual corpora.", "labels": [], "entities": []}, {"text": "There are two strands of research that attempt to automatically obtain property norm data for new concepts.", "labels": [], "entities": []}, {"text": "One approach is to automatically generate feature norms from text corpora by mining text data fora set of generalised property patterns ().", "labels": [], "entities": []}, {"text": "Another avenue of research is inspired by and and tries to increase the coverage of feature norms through cross-modal mapping from linguistic information.", "labels": [], "entities": []}, {"text": "Here, we follow recent trends in multi-modal semantics and explore automatic property norm extraction from visual, rather than textual, data.", "labels": [], "entities": [{"text": "automatic property norm extraction", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.6238807216286659}]}, {"text": "Obtaining property norms from visual information makes intuitive sense: information contained in the property norm datasets can often be attributed to extra-linguistic modalities-a large proportion of relevant properties are visual, auditory or tactile, rather than linguistic (e.g. is round, makes noise, is yellow).", "labels": [], "entities": []}, {"text": "We show that such conceptual properties can be more accurately predicted through cross-modal mappings from raw perceptual information (i.e. image data) or multi-modal models (i.e. text and image data combined) rather than from purely textual information (Section 3).", "labels": [], "entities": []}, {"text": "Furthermore, we analyse the quality of human collected property norm datasets and conclude that these are sparse and incomplete, meaning that there will be a lot of property annotations missing fora given concept (e.g. has legs is not listed as a property of TORTOISE).", "labels": [], "entities": [{"text": "TORTOISE", "start_pos": 259, "end_pos": 267, "type": "DATASET", "confidence": 0.636988639831543}]}, {"text": "We show that having a complete dataset can drastically increase the performance of automatic feature prediction, resulting in a truer evaluation.", "labels": [], "entities": [{"text": "automatic feature prediction", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6489269534746805}]}, {"text": "Lastly, we demonstrate how property norm datasets could be used in an image retrieval task (Section 4), which opens up intriguing possibilities for retrieving concepts based on their visual properties.", "labels": [], "entities": [{"text": "image retrieval task", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7891692022482554}]}], "datasetContent": [{"text": "Following previous work) we use partial least squares regression (PLSR) to learn cross-modal maps to the property-norm space (PROPNORM) from the visual (VISUAL), linguistic (DISTRIB, SVD, EMBED) and multi-modal semantic spaces (VISUAL+DISTRIB, VISUAL+SVD, VISUAL+EMBED).", "labels": [], "entities": [{"text": "partial least squares regression", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.5964735075831413}]}, {"text": "At training time, we take advantage of the fact that we possess both visual/linguistic/multi-modal and property norm information for the concepts in MCRAE.", "labels": [], "entities": [{"text": "MCRAE", "start_pos": 149, "end_pos": 154, "type": "DATASET", "confidence": 0.8862518072128296}]}, {"text": "Let's consider the VISUAL\u2192PROPNORM setting as an example.", "labels": [], "entities": [{"text": "VISUAL\u2192PROPNORM", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7274102568626404}]}, {"text": "We use this cross-modal vocabulary to learn a mapping function between VISUAL and PROP-NORM: this function will learn to map visual dimensions to property dimensions.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.6383742690086365}]}, {"text": "During testing, we use the learnt function to map the visual information of a previously unseen concept (e.g. to the property norm space and obtain a predicted property vector for that concept.", "labels": [], "entities": []}, {"text": "Ideally, we want this predicted property vector to be closer to the goldstandard property vector for CAT than to any other property vector (i.e. the label of its nearest neighbour in PROPNORM to be CAT).", "labels": [], "entities": []}, {"text": "We use the standard evaluation metric for this task: average percentage correct at N (P@N).", "labels": [], "entities": [{"text": "average percentage correct", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.9611020485560099}]}, {"text": "This measures how many of the test instances were ranked within the top N highest ranked nearest neighbors (using the cosine measure).", "labels": [], "entities": []}, {"text": "All the results reported in use the zero-shot learning procedure-for each of the 541 concepts in MCRAE, we train a mapping on the remaining 540 concepts and record whether the correct label is retrieved among the top N neighbours-and are averaged over the entire dataset.", "labels": [], "entities": [{"text": "MCRAE", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.8961458206176758}]}, {"text": "We also compare to a random baseline, for which a concept's nearest neighbours list is obtained by randomly ranking the list of target words.", "labels": [], "entities": []}, {"text": "Since the cross-modal map allows us to obtain property vectors for any concept, we were also able to evaluate these semantic representations on a standard NLP task, such as the well known conceptual similarity and relatedness task.", "labels": [], "entities": []}, {"text": "The MEN test collection ( ) contains human similarity ratings for 3000 concept pairs.", "labels": [], "entities": [{"text": "MEN test collection", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9422746896743774}]}, {"text": "Performance on this dataset is usually measured by computing the Spearman \u03c1 s correlation between the ranking produced by the similarity scores of the learnt property vectors and that produced by the human-annotated concept similarity scores.", "labels": [], "entities": [{"text": "Spearman \u03c1 s correlation", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.874133288860321}]}, {"text": "Similarity between concept pairs is calculated using cosine similarity.", "labels": [], "entities": []}, {"text": "For each of the semantic spaces presented in we learn a cross-modal map to PROPNORM using all the concepts in MCRAE at training time.", "labels": [], "entities": [{"text": "MCRAE", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.9027151465415955}]}, {"text": "During testing, we predict property vectors for all concepts in MEN-NOUNS, a subset of the MEN dataset consisting of 1285 noun pairs that don't occur in MCRAE.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.8881975412368774}, {"text": "MCRAE", "start_pos": 153, "end_pos": 158, "type": "DATASET", "confidence": 0.923041045665741}]}], "tableCaptions": [{"text": " Table 3: Subspace of PROPNORM. Important to  note that MCRAE is not complete, meaning that even  though some properties are true of a given concept,  they have not been produced by the human partic- ipants (e.g. the is edible property for APPLE holds  the value 0).", "labels": [], "entities": []}, {"text": " Table 4: Zero-shot learning performance when map- ping to the property-norm space (PROPNORM)", "labels": [], "entities": []}, {"text": " Table 5: Performance (Spearman \u03c1 s correlation) of  various uni-modal and multi-modal semantic spaces  (column SS), together with that of the property vec- tors they predict (column \u2192PROPNORM) on a se- mantic relatedness task (MEN-NOUNS)", "labels": [], "entities": [{"text": "Spearman \u03c1 s correlation)", "start_pos": 23, "end_pos": 48, "type": "METRIC", "confidence": 0.8062945008277893}]}, {"text": " Table 7: Comparison of various datasets, according  to the number of concepts and properties covered, as  well as the pairs of (CONCEPT, property) contained", "labels": [], "entities": [{"text": "CONCEPT", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.8697552680969238}]}, {"text": " Table 8: Zeroshot learning performance for the vi- sion to norms cross-modal map on different training  and test sets", "labels": [], "entities": []}, {"text": " Table 9: Zero-shot learning performance when map- ping to the visual space (VISUAL)", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.6446390748023987}]}]}