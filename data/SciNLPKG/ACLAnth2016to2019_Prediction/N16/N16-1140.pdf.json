{"title": [{"text": "Integrating Morphological Desegmentation into Phrase-based Decoding", "labels": [], "entities": [{"text": "Integrating Morphological Desegmentation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8283190528551737}]}], "abstractContent": [{"text": "When translating into a morphologically complex language, segmenting the target language can reduce data sparsity, while introducing the complication of desegmenting the system output.", "labels": [], "entities": []}, {"text": "We present a method for decoder-integrated desegmentation, allowing features that consider the desegmented target, such as a word-level language model, to be considered throughout the entire search space.", "labels": [], "entities": []}, {"text": "Our results on a large-scale, English to Arabic translation task show significant improvement over the 1-best desegmentation baseline.", "labels": [], "entities": [{"text": "English to Arabic translation task", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7049350500106811}]}], "introductionContent": [{"text": "State-of-the-art systems for translating into morphologically complex languages, such as Arabic, employ morphological segmentation of the target language in order to reduce data sparsity and improve translation quality.", "labels": [], "entities": []}, {"text": "For example, the Arabic word ldwlthm \"for their country\" is segmented into the prefix + l+ \"for\", the stem dwlp \"country\" and the suffix + +hm \"their.\"", "labels": [], "entities": []}, {"text": "The segmentation sometimes involves performing orthographic normalizations, such as transforming the stem-final t top.", "labels": [], "entities": []}, {"text": "The result is not only a reduction in the number of word types, but also better token-to-token correspondence with the source language.", "labels": [], "entities": []}, {"text": "Morphological segmentation is typically performed as a pre-processing step before the training phase, which results in a model that translates the source language into segmented target language.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.890562504529953}]}, {"text": "Desegmentation is the process of transforming the segmented output into a readable word sequence, which can be performed using a table lookup combined with a small set of rules.", "labels": [], "entities": []}, {"text": "Desegmentation is usually applied to the 1-best output of the decoder.", "labels": [], "entities": []}, {"text": "However, this pipeline suffers from error propagation: errors made during decoding cannot be corrected, even when desegmentation results in an illegal or extremely unlikely word.", "labels": [], "entities": []}, {"text": "Two principal types of solutions have been proposed for this problem: rescoring and phrase-table desegmentation.", "labels": [], "entities": [{"text": "phrase-table desegmentation", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.8320688009262085}]}, {"text": "The rescoring approach desegments either an n-best list) or lattice (, and then re-ranks with features that consider the desegmented word sequence of each hypothesis.", "labels": [], "entities": []}, {"text": "Rescoring features include the score from an unsegmented target language model and contiguity indicators that flag target words that were translated from contiguous source tokens.", "labels": [], "entities": []}, {"text": "Rescoring widens the desegmentation pipeline, allowing desegmentation features to reduce the number of translation errors.", "labels": [], "entities": []}, {"text": "However, these features are calculated for only a subset of the search space, and the extra rescoring step complicates the training and translation processes.", "labels": [], "entities": [{"text": "translation", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.9598071575164795}]}, {"text": "Phrase-table desegmentation () also translates into a segmented target language, but alters training to perform word-boundary-aware phrase extraction.", "labels": [], "entities": [{"text": "Phrase-table desegmentation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8903181552886963}, {"text": "word-boundary-aware phrase extraction", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.61419344941775}]}, {"text": "The extracted phrases are constrained to contain only complete target words, without any dangling affixes.", "labels": [], "entities": []}, {"text": "With this restriction in place, the phrase table can be desegmented before decoding begins, allowing the decoder to track features over both the segmented and desegmented target.", "labels": [], "entities": []}, {"text": "This ensures that desegmentation features are integrated into the complete search space, and side-steps the complications of rescoring.", "labels": [], "entities": []}, {"text": "However, show experimentally that these benefits are not worth giving up phrase-pairs with dangling affixes, which are eliminated by wordboundary-aware phrase extraction.", "labels": [], "entities": [{"text": "wordboundary-aware phrase extraction", "start_pos": 133, "end_pos": 169, "type": "TASK", "confidence": 0.6165324250857035}]}, {"text": "We present a method for decoder-integrated desegmentation that combines the strengths of these two approaches.", "labels": [], "entities": [{"text": "decoder-integrated desegmentation", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.626006156206131}]}, {"text": "Like a rescoring approach, it places no restrictions on what morpheme sequences can appear in the target side of a phrase pair.", "labels": [], "entities": []}, {"text": "Like phrasetable desegmentation, its desegmentation features are integrated directly into decoding and considered throughout the entire search space.", "labels": [], "entities": [{"text": "phrasetable desegmentation", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8198725283145905}]}, {"text": "We achieve this by augmenting the decoder to desegment hypotheses on the fly, allowing the inclusion of an unsegmented language model and other features.", "labels": [], "entities": []}, {"text": "Our results on a large-scale, NIST-data English to Arabic translation task show significant improvements over the 1-best desegmentation baseline, and match the performance of the state-of-the-art lattice desegmentation approach of, while eliminating the complication and cost of its rescoring step.", "labels": [], "entities": [{"text": "NIST-data English to Arabic translation task", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.6524453361829122}]}, {"text": "Our approach is implemented as a single stateful feature function in Moses (, which we will submit back to the community.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the NIST 2012 dataset (1.49 million sentence pairs excluding UN pairs) to train an Englishto-Arabic system.", "labels": [], "entities": [{"text": "NIST 2012 dataset", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9808014432589213}]}, {"text": "The system is tuned with the NIST 2004 (1353 pairs) evaluation set and tested using NIST 2005 (1056 sentences) and the newswire portion of NIST 2008 (813 pairs) and NIST 2009 (586 pairs).", "labels": [], "entities": [{"text": "NIST 2004 (1353 pairs) evaluation set", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.920399010181427}, {"text": "NIST 2005", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.9487706124782562}, {"text": "NIST 2008", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.9286328852176666}, {"text": "NIST 2009", "start_pos": 165, "end_pos": 174, "type": "DATASET", "confidence": 0.9577465951442719}]}, {"text": "As there are multiple English reference translations provided for these evaluation sets, we use the first reference as our source text.", "labels": [], "entities": []}, {"text": "The Arabic part of the training set is morphologically segmented and tokenized by MADA 3.2 () using the Penn Arabic Treebank (PATB) segmentation scheme.", "labels": [], "entities": [{"text": "MADA 3.2", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.8144361972808838}, {"text": "Penn Arabic Treebank (PATB)", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.956056276957194}]}, {"text": "Variants of Alif and Ya characters are uniformly normalized.", "labels": [], "entities": []}, {"text": "We generate a desegmentation table from the Arabic side of the training data by collecting mappings of segmented forms to surface forms.", "labels": [], "entities": []}, {"text": "We align the parallel data with GIZA++ (, and decode with Moses (.", "labels": [], "entities": []}, {"text": "The decoder's log-linear model uses a standard feature set, including four phrase table scores, six features from a lexicalized distortion model, along with a phrase penalty and a distance-based distortion penalty.", "labels": [], "entities": []}, {"text": "KN-smoothed 5-gram language models are trained on both the segmented and unsegmented views of the target side of the parallel data.", "labels": [], "entities": []}, {"text": "We experiment with word penalties based on either morphemes or desegmented words.", "labels": [], "entities": []}, {"text": "The decoder uses Moses' default search parameters, except for the maximum phrase length, which is set to 8, and the translation table limit, which is set to 40.", "labels": [], "entities": []}, {"text": "The decoder's log-linear model is tuned with MERT (Och, 2003) using unsegmented Arabic reference translations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9676536917686462}]}, {"text": "When necessary, we desegment our 100-best-lists before MERT evaluates each hypothesis.", "labels": [], "entities": []}, {"text": "We evaluate with BLEU () measured on unsegmented Arabic, and test statistical significance with multeval (Clark et al., 2011) over 3 tuning replications.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9993540644645691}]}, {"text": "We test four systems that differ in their desegmentation approach.", "labels": [], "entities": [{"text": "desegmentation", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.971598207950592}]}, {"text": "The One-best baseline translates into segmented Arabic and desegments the decoder's 1-best output.", "labels": [], "entities": []}, {"text": "The Lattice system is the lattice-desegmentation approach of.", "labels": [], "entities": []}, {"text": "We implement our in-Decoder desegmenta- tion approach as a feature functions in Moses, testing scoring variants (delayed vs. optimistic), and word penalty variants (morpheme vs. word).", "labels": [], "entities": []}, {"text": "shows the results on three NIST test sets, each averaged over 3 tuning replications.", "labels": [], "entities": [{"text": "NIST test sets", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9612450997034708}]}, {"text": "The lattice approach is significantly better than the 1-best system, which in turn is significantly better than the unsegmented baseline.", "labels": [], "entities": []}, {"text": "Our Optimistic in-decoder approach with word penalty calculated on word tokens is significantly (p < 0.05) better than the 1-best approach, and effectively matches the quality of the more complex lattice approach.", "labels": [], "entities": []}, {"text": "All of the systems with word-level features improve over 1-best desegmentation, as their features penalize desegmentations resulting in illegal words or unlikely word sequences.", "labels": [], "entities": []}, {"text": "We see a small, consistent benefit from optimistic scoring.", "labels": [], "entities": []}, {"text": "Error analysis reveals that translations with many consecutive stems benefit the most from this variant, which makes sense, as our approximations would be exact in these cases.", "labels": [], "entities": []}, {"text": "Using a word penalty calculated on word tokens appears to work slightly better on average than one calculated on morphemes.", "labels": [], "entities": []}, {"text": "Typically, one would hope to surpass a rescoring approach with decoder integration; however, our lattice implementation fully searches its lattice, even if composition with the word-level language model would cause the lattice to explode in size.", "labels": [], "entities": []}, {"text": "That is, lattice desegmentation has an advantage, as it trades time-efficiency fora perfect search that ignores the complexity introduced by expanded n-gram context.", "labels": [], "entities": [{"text": "lattice desegmentation", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.78453728556633}]}, {"text": "A lattice beam search that dynamically calculates word-level language model scores while pruning away unlikely paths would provide a more fair, and more efficient, comparison point.", "labels": [], "entities": []}, {"text": "Lattice rescoring also involves many steps, requiring one to train and tune a complete segmented system with segmented references, then desegment lattices and compose them with a word LM, and then tune a lattice rescorer on unsegmented references.", "labels": [], "entities": [{"text": "Lattice rescoring", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7857975661754608}]}, {"text": "In contrast, our system is implemented as a single decoder feature function in Moses.", "labels": [], "entities": []}, {"text": "This one function replaces the lattice desegmentation, LM composition, and lattice rescoring steps, greatly simplifying the translation pipeline.", "labels": [], "entities": [{"text": "LM composition", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.776615709066391}]}], "tableCaptions": [{"text": " Table 1: Evaluation of the desegmentation methods using", "labels": [], "entities": []}]}