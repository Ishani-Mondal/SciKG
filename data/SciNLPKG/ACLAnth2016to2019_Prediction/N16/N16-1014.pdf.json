{"title": [{"text": "A Diversity-Promoting Objective Function for Neural Conversation Models", "labels": [], "entities": [{"text": "Neural Conversation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7366794049739838}]}], "abstractContent": [{"text": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don't know) regardless of the input.", "labels": [], "entities": []}, {"text": "We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks.", "labels": [], "entities": [{"text": "response generation tasks", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.7894759575525919}]}, {"text": "Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models.", "labels": [], "entities": [{"text": "Maximum Mutual Information (MMI)", "start_pos": 25, "end_pos": 57, "type": "METRIC", "confidence": 0.761781856417656}]}, {"text": "Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and inhuman evaluations.", "labels": [], "entities": [{"text": "MMI", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9464682936668396}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9993698000907898}]}], "introductionContent": [{"text": "Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 237, "end_pos": 254, "type": "TASK", "confidence": 0.7091995775699615}]}, {"text": "Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (), or using neural networks to rerank, or directly in the form of sequence-to-sequence (SEQ2SEQ) models).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 182, "end_pos": 219, "type": "TASK", "confidence": 0.793757438659668}]}, {"text": "SEQ2SEQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft.", "labels": [], "entities": []}, {"text": "to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies ( ) in away not possible with conventional SMT approaches ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9854690432548523}]}, {"text": "An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting.", "labels": [], "entities": [{"text": "response generation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7120290100574493}]}, {"text": "In practice, however, neural conversation models tend to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don't know or I'm OK (.", "labels": [], "entities": []}, {"text": "illustrates this phenomenon, showing top outputs from SEQ2SEQ models.", "labels": [], "entities": []}, {"text": "All the top-ranked responses are generic.", "labels": [], "entities": []}, {"text": "Responses that seem more meaningful or specific can also be found in the N-best lists, but rank much lower.", "labels": [], "entities": []}, {"text": "In part at least, this behavior can be ascribed to the relative frequency of generic responses like I don't know in conversational datasets, in contrast with the relative sparsity of more contentful alternative responses.", "labels": [], "entities": []}, {"text": "1 It appears that by optimizing for the likelihood of outputs given inputs, neural models assign high probability to \"safe\" responses.", "labels": [], "entities": []}, {"text": "This objective function, common in related tasks such as machine translation, maybe unsuited to generation tasks involving intrinsically diverse outputs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8200722932815552}]}, {"text": "Intuitively, it seems desirable to take into account not only the dependency of responses on messages, but also the inverse, the likelihood that a message will be provided to a given response.", "labels": [], "entities": []}, {"text": "We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in: Responses generated by a 4-layer SEQ2SEQ neural model trained on 20 million conversation pairs take from the OpenSubtitles dataset.", "labels": [], "entities": [{"text": "Maximum Mutual Information (MMI)", "start_pos": 46, "end_pos": 78, "type": "METRIC", "confidence": 0.7862089027961096}, {"text": "OpenSubtitles dataset", "start_pos": 210, "end_pos": 231, "type": "DATASET", "confidence": 0.9463027119636536}]}, {"text": "Decoding is implemented with beam size set to 200.", "labels": [], "entities": []}, {"text": "The top examples are the responses with the highest average probability loglikelihoods in the N-best list.", "labels": [], "entities": []}, {"text": "Lower-ranked, less-generic responses were manually chosen.", "labels": [], "entities": []}, {"text": "speech recognition (, as an optimization objective that measures the mutual dependence between inputs and outputs.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7602648437023163}]}, {"text": "Below, we present practical strategies for neural generation models that use MMI as an objective function.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7749786972999573}]}, {"text": "We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs.", "labels": [], "entities": [{"text": "MMI", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9327551126480103}]}], "datasetContent": [{"text": "Twitter Conversation Triple Dataset We used an extension of the dataset described in , which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month period from June through August 2012.", "labels": [], "entities": [{"text": "Twitter Conversation Triple Dataset", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.7251956760883331}]}, {"text": "For the purposes of our experiments, we limited context to the turn in the conversation immediately preceding the message.", "labels": [], "entities": []}, {"text": "In our LSTM models, we used a simple input model in which contexts and messages are concatenated to form the source input.: Performance on the Twitter dataset of 4-layer SEQ2SEQ models and MMI models.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8647167980670929}]}, {"text": "distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words.", "labels": [], "entities": []}, {"text": "For tuning and evaluation, we used the development dataset (2118 conversations) and the test dataset (2114 examples), augmented using information retrieval methods to create a multi-reference set, as described by . The selection criteria for these two datasets included a component of relevance/interestingness, with the result that dull responses will tend to be penalized in evaluation.", "labels": [], "entities": []}, {"text": "OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles (OSDb) dataset, a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters.", "labels": [], "entities": [{"text": "OpenSubtitles dataset", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.8749521970748901}, {"text": "OpenSubtitles (OSDb) dataset", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.7004797875881195}]}, {"text": "This dataset does not specify which character speaks each subtitle line, which prevents us from inferring speaker turns.", "labels": [], "entities": []}, {"text": "Following , we make the simplifying assumption that each line of subtitle constitutes a full speaker turn.", "labels": [], "entities": []}, {"text": "Our models are trained to predict the current turn given the preceding ones based on the assumption that consecutive turns belong to the same conversation.", "labels": [], "entities": []}, {"text": "This introduces a degree of noise, since consecutive lines may not appear in the same conversation or scene, and may not even bespoken by the same character.", "labels": [], "entities": []}, {"text": "This limitation potentially renders the OSDb dataset unreliable for evaluation purposes.", "labels": [], "entities": [{"text": "OSDb dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8713851571083069}]}, {"text": "For evaluation purposes, we therefore used data from the Internet Movie Script Database (IMSDB), which explicitly identifies which character speaks each line of the script.", "labels": [], "entities": [{"text": "Internet Movie Script Database (IMSDB)", "start_pos": 57, "end_pos": 95, "type": "DATASET", "confidence": 0.8893621563911438}]}, {"text": "This allowed us to identify consecutive message-response pairs spoken by different characters.", "labels": [], "entities": []}, {"text": "We randomly selected two subsets as development and test datasets, each containing 2k pairs, with source and target length restricted to the range of.", "labels": [], "entities": []}, {"text": "For parameter tuning and final evaluation, we used BLEU (), which was shown to correlate reasonably well with human judgment on the response generation task ( ).", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7435826063156128}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9991748929023743}, {"text": "response generation task", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.7563197513421377}]}, {"text": "In the case of the Twitter models, we used multireference BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9758803844451904}]}, {"text": "As the IMSDB data is too limited to support extraction of multiple references, only single reference BLEU was used in training and evaluating the OSDb models.", "labels": [], "entities": [{"text": "IMSDB data", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9270611703395844}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9956678152084351}]}, {"text": "We did not follow  in using perplexity as evaluation metric.", "labels": [], "entities": []}, {"text": "Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard SEQ2SEQ model in order to diversify the outputs.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9309444427490234}]}, {"text": "We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses.", "labels": [], "entities": []}, {"text": "The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2 in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance on the Twitter dataset of 4-layer SEQ2SEQ models and MMI models. distinct-1 and distinct-2 are  respectively the number of distinct unigrams and bigrams divided by total number of generated words.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7923103272914886}]}, {"text": " Table 3: Performance of the SEQ2SEQ baseline and two  MMI models on the OpenSubtitles dataset.", "labels": [], "entities": [{"text": "SEQ2SEQ baseline", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.8311382532119751}, {"text": "OpenSubtitles dataset", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9655669927597046}]}]}