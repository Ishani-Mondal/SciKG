{"title": [{"text": "Hierarchical Attention Networks for Document Classification", "labels": [], "entities": [{"text": "Hierarchical Attention Networks", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7844534317652384}, {"text": "Document Classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8300761282444}]}], "abstractContent": [{"text": "We propose a hierarchical attention network for document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.764592856168747}]}, {"text": "Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word-and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation.", "labels": [], "entities": []}, {"text": "Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.782931516567866}]}, {"text": "Visualiza-tion of the attention layers illustrates that the model selects qualitatively informative words and sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification is one of the fundamental task in Natural Language Processing.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8375145792961121}, {"text": "Natural Language Processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6349724729855856}]}, {"text": "The goal is to assign labels to text.", "labels": [], "entities": []}, {"text": "It has broad applications including topic labeling (, sentiment classification, and spam detection (.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.8002385199069977}, {"text": "sentiment classification", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.9712108671665192}, {"text": "spam detection", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.9314095079898834}]}, {"text": "Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7261238992214203}]}, {"text": "More recent approaches used deep learning, such as convolutional neural networks () and recurrent neural networks based on long short-term memory (LSTM)) to learn text representations.", "labels": [], "entities": []}, {"text": "pork belly = delicious . || scallops?", "labels": [], "entities": []}, {"text": "|| I don't even like scallops, and these were a-m-a-z-i-n-g . || fun and tasty cocktails.", "labels": [], "entities": []}, {"text": "|| next time I in Phoenix, I will go back here.", "labels": [], "entities": []}, {"text": "Although neural-network-based approaches to text classification have been quite effective, in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture.", "labels": [], "entities": [{"text": "text classification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7626854479312897}]}, {"text": "The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation.", "labels": [], "entities": []}, {"text": "Our primary contribution is anew neural architecture ( \u00a72), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure.", "labels": [], "entities": []}, {"text": "First, since documents have a hierarchical structure (words form sentences, sentences form a document), we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation.", "labels": [], "entities": []}, {"text": "Second, it is observed that different words and sentences in a documents are differentially informative.", "labels": [], "entities": []}, {"text": "Moreover, the impor-tance of words and sentences are highly context dependent, i.e. the same word or sentence maybe differentially important in different context ( \u00a73.5).", "labels": [], "entities": []}, {"text": "To include sensitivity to this fact, our model includes two levels of attention mechanisms () -one at the word level and one at the sentence level -that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document.", "labels": [], "entities": []}, {"text": "To illustrate, consider the example in, which is a short Yelp review where the task is to predict the rating on a scale from 1-5.", "labels": [], "entities": []}, {"text": "Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review.", "labels": [], "entities": []}, {"text": "Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis ).", "labels": [], "entities": []}, {"text": "The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets ( \u00a73).", "labels": [], "entities": []}, {"text": "Our model outperforms previous approaches by a significant margin.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data statistics: #s denotes the number of sentences (average and maximum per document), #w denotes the number of", "labels": [], "entities": []}, {"text": " Table 2: Document Classification, in percentage", "labels": [], "entities": [{"text": "Document Classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8689047694206238}]}]}