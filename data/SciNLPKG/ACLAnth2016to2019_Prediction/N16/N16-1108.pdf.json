{"title": [{"text": "Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement", "labels": [], "entities": [{"text": "Pairwise Word Interaction Modeling", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6801562160253525}, {"text": "Semantic Similarity Measurement", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.616294393936793}]}], "abstractContent": [{"text": "Textual similarity measurement is a challenging problem, as it requires understanding the semantics of input sentences.", "labels": [], "entities": [{"text": "Textual similarity measurement", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8241511185963949}]}, {"text": "Most previous neural network models use coarse-grained sentence modeling, which has difficulty capturing fine-grained word-level information for semantic comparisons.", "labels": [], "entities": []}, {"text": "As an alternative, we propose to explicitly model pairwise word interactions and present a novel similarity focus mechanism to identify important correspondences for better similarity measurement.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.6351999342441559}]}, {"text": "Our ideas are implemented in a novel neural network architecture that demonstrates state-of-the-art accuracy on three SemEval tasks and two answer selection tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9981116056442261}, {"text": "SemEval tasks", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.887708842754364}, {"text": "answer selection tasks", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.845205028851827}]}], "introductionContent": [{"text": "Given two pieces of text, measuring their semantic textual similarity (STS) remains a fundamental problem in language research and lies at the core of many language processing tasks, including question answering, query ranking (), and paraphrase generation.", "labels": [], "entities": [{"text": "measuring their semantic textual similarity (STS)", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.5813748016953468}, {"text": "question answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.8293337523937225}, {"text": "paraphrase generation", "start_pos": 235, "end_pos": 256, "type": "TASK", "confidence": 0.8970770239830017}]}, {"text": "Traditional NLP approaches, e.g., developing hand-crafted features, suffer from sparsity because of language ambiguity and the limited amount of annotated data available.", "labels": [], "entities": []}, {"text": "Neural networks and distributed representations can alleviate such sparsity, thus neural network-based models are widely used by recent systems for the STS problem (.", "labels": [], "entities": [{"text": "STS problem", "start_pos": 152, "end_pos": 163, "type": "TASK", "confidence": 0.9337174296379089}]}, {"text": "However, most previous neural network approaches are based on sentence modeling, which first maps each input sentence into a fixed-length vector and then performs comparisons on these representations.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7239445000886917}]}, {"text": "Despite its conceptual simplicity, researchers have raised concerns about this approach: Will fine-grained wordlevel information, which is crucial for similarity measurement, get lost in the coarse-grained sentence representations?", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.630571112036705}]}, {"text": "Is it really effective to \"cram\" whole sentence meanings into fixed-length vectors?", "labels": [], "entities": []}, {"text": "In contrast, we focus on capturing fine-grained word-level information directly.", "labels": [], "entities": []}, {"text": "Our contribution is twofold: First, instead of using sentence modeling, we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences.", "labels": [], "entities": [{"text": "pairwise word interaction modeling", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.6346675977110863}]}, {"text": "This is inspired by our own intuitions of how people recognize textual similarity: given two sentences sent 1 and sent 2 , a careful reader might look for corresponding semantic units, which we operationalize in our pairwise word interaction modeling technique (Sec. 5).", "labels": [], "entities": [{"text": "pairwise word interaction modeling", "start_pos": 216, "end_pos": 250, "type": "TASK", "confidence": 0.633985660970211}]}, {"text": "Second, based on the pairwise word interactions, we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement.", "labels": [], "entities": []}, {"text": "Since not all words are created equal, important words that can make more contributions deserve extra \"focus\" from the model (Sec. 6).", "labels": [], "entities": []}, {"text": "We conducted thorough evaluations on ten test sets from three SemEval STS competitions () and two answer selection tasks (.", "labels": [], "entities": [{"text": "SemEval STS competitions", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8889195720354716}, {"text": "answer selection tasks", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.935929516951243}]}, {"text": "We outperform the recent multiperspective convolutional neural networks of and demonstrate state-of-the-art accuracy on all five tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.998831570148468}]}, {"text": "In addition, we conducted ablation studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.710947722196579}]}], "datasetContent": [{"text": "We conducted five separate experiments on ten different datasets: three recent SemEval competitions and two answer selection tasks.", "labels": [], "entities": [{"text": "SemEval competitions", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8836683332920074}, {"text": "answer selection tasks", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9199636777242025}]}, {"text": "Note that the answer selection task, which is to rank candidate answer sentences based on their similarity to the questions, is essentially the similarity measurement problem.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.8977167904376984}]}, {"text": "The five experiments are as follows: 1.", "labels": [], "entities": []}, {"text": "Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval competition () and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set, and 4,927 for  The TrecQA data consists of 1,229 questions with 53,417 question-answer pairs in the TRAIN-ALL training set, 82 questions with 1,148 pairs in the development set, and 100 questions with 1,517 pairs in the test set.", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge (SICK)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8432427389281136}, {"text": "TrecQA data", "start_pos": 219, "end_pos": 230, "type": "DATASET", "confidence": 0.8591941595077515}, {"text": "TRAIN-ALL training set", "start_pos": 300, "end_pos": 322, "type": "DATASET", "confidence": 0.7794966499010721}]}, {"text": "For experiments on SICK, MSRVID, and STS2014, the training objective is to minimize the KL-divergence loss: where f is the ground truth, f \u03b8 is the predicted distribution with model weights \u03b8, and n is the number of training examples.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8341397643089294}, {"text": "KL-divergence loss", "start_pos": 88, "end_pos": 106, "type": "METRIC", "confidence": 0.9131309390068054}]}, {"text": "We used a hinge loss for the answer selection task on WikiQA and TrecQA data.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9179181059201559}, {"text": "WikiQA", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9473802447319031}, {"text": "TrecQA data", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.8037632703781128}]}, {"text": "The training objective is to minimize the following loss, summed over examples x, y gold : where y gold is the ground truth label, input x is the pair of sentences x = {S 1 , S 2 }, \u03b8 is the model weight vector, and the function f \u03b8 (x, y ) is the output of our model.", "labels": [], "entities": []}, {"text": "In all cases, we performed optimization using RMSProp) with backpropagation, with a learning rate fixed to 10 \u22124 . Settings.", "labels": [], "entities": []}, {"text": "For the SICK and MSRVID experiments, we used 300-dimension GloVe word embeddings ().", "labels": [], "entities": []}, {"text": "For the STS2014, WikiQA, and TrecQA experiments, we used 300-dimension PARAGRAM-SL999 embeddings from and the PARAGRAM-PHRASE embeddings from, trained on word pairs from the Paraphrase Database (PPDB) ().", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.9259037375450134}, {"text": "Paraphrase Database (PPDB)", "start_pos": 174, "end_pos": 200, "type": "DATASET", "confidence": 0.7244558513164521}]}, {"text": "We did not update word embeddings in all experiments.", "labels": [], "entities": []}, {"text": "We used the SICK development set for tuning and then applied exactly the same hyperparameters to all ten test sets.", "labels": [], "entities": [{"text": "SICK development set", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.8458218177159628}]}, {"text": "For the answer selection task (Wiki-QA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) and  selected the best development model based on MRR for final testing.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.9010700980822245}, {"text": "Wiki-QA", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.912102222442627}, {"text": "TrecQA", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.7124924063682556}, {"text": "Mean Average Precision (MAP)", "start_pos": 113, "end_pos": 141, "type": "METRIC", "confidence": 0.9693656663099924}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 146, "end_pos": 172, "type": "METRIC", "confidence": 0.9221465984980265}]}, {"text": "Our timing experiments were conducted on an Intel Xeon E5-2680 CPU.", "labels": [], "entities": []}, {"text": "Due to sentence length variations, for the SICK and MSRVID data we padded the sentences to 32 words; for the STS2014, WikiQA, and TrecQA data, we padded the sentences to 48 words..", "labels": [], "entities": [{"text": "MSRVID data", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.8787083327770233}, {"text": "STS2014", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.923995316028595}, {"text": "WikiQA", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.8397101759910583}, {"text": "TrecQA data", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.87218177318573}]}, {"text": "Our model outperforms previous neural network models, most of which are based on sentence modeling.", "labels": [], "entities": []}, {"text": "The ConvNet work and TreeLSTM work) achieve comparable accuracy; for example, their difference in Pearson's r is only 0.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992770552635193}, {"text": "Pearson's r", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.6984586914380392}]}, {"text": "In comparison, our model outperforms both by 1% in Pearson's r, over 1.1% in Spearman's \u03c1, and 2-3% in MSE.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.6713313659032186}, {"text": "MSE", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9144061207771301}]}, {"text": "Note that we used the same word embeddings, sparse distribution targets, and loss function as in and, thereby representing comparable experimental conditions..", "labels": [], "entities": []}, {"text": "Our model outperforms the work of, which already reports a Pearson's r score of over 0.9, STS2014 Results.", "labels": [], "entities": [{"text": "Pearson's r score", "start_pos": 59, "end_pos": 76, "type": "METRIC", "confidence": 0.9616968333721161}]}, {"text": "Systems in the competition are ranked by the weighted mean (the of-   WikiQA Results.", "labels": [], "entities": [{"text": "WikiQA Results", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.8735386729240417}]}, {"text": "We compared our model to competitive baselines prepared by  TrecQA Results.", "labels": [], "entities": [{"text": "TrecQA Results", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9196797013282776}]}, {"text": "This is the largest dataset in our experiments, with over 55,000 question-answer pairs.", "labels": [], "entities": []}, {"text": "Only recently have neural network approaches () started to show promising results on this decade-old dataset.", "labels": [], "entities": []}, {"text": "Previous approaches with probabilistic tree-edit techniques or tree kernels () have been successful since tree structure information per- 0.5110 0.5160 PV-Cnt ( 0.5976 0.6058 LCLR ( 0.5993 0.6086 CNN ( 0.6190 0.6281 CNN-Cnt ( 0.6520 0.6652 0.6930 0.7090 This work 0.7090 0.7234  mits a fine-grained focus on important words for similarity comparison purposes.", "labels": [], "entities": [{"text": "LCLR", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9017913341522217}, {"text": "CNN-Cnt", "start_pos": 216, "end_pos": 223, "type": "DATASET", "confidence": 0.715599536895752}]}, {"text": "Our approach essentially follows this intuition, but in a neural network setting with the use of our similarity focus layer.", "labels": [], "entities": []}, {"text": "Our model outperforms previous work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Description of STS2014 test sets.", "labels": [], "entities": [{"text": "STS2014 test sets", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.739688903093338}]}, {"text": " Table 3: Test results on SICK grouped as: (1) RNN  variants; (2) SemEval 2014 systems; (3) Sequential  LSTM variants; (4) Dependency and constituency  tree LSTMs. Evaluation metrics are Pearson's r,  Spearman's \u03c1, and mean squared error (MSE). Rows  in grey are neural network models.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 187, "end_pos": 198, "type": "METRIC", "confidence": 0.7417664329210917}, {"text": "mean squared error (MSE)", "start_pos": 219, "end_pos": 243, "type": "METRIC", "confidence": 0.9455874562263489}]}, {"text": " Table 4: Test results on MSRVID data.", "labels": [], "entities": [{"text": "MSRVID data", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9674587845802307}]}, {"text": " Table 5: Test results on all six test sets in STS2014.  We show results of the top three participating sys- tems at the competition in Pearson's r scores.", "labels": [], "entities": [{"text": "Pearson's r scores", "start_pos": 136, "end_pos": 154, "type": "METRIC", "confidence": 0.7492257803678513}]}, {"text": " Table 6: Test results on WikiQA data.", "labels": [], "entities": [{"text": "WikiQA data", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.975700169801712}]}, {"text": " Table 7: Test results on TrecQA data.", "labels": [], "entities": [{"text": "TrecQA data", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.8886147141456604}]}, {"text": " Table 10: Visualization of cosine values (multiplied  by 10) in the f ocusCube given two sentence pairs  in the SICK test set.", "labels": [], "entities": [{"text": "SICK test set", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.8005739748477936}]}]}