{"title": [{"text": "Deep LSTM based Feature Mapping for Query Classification", "labels": [], "entities": [{"text": "Query Classification", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7243635803461075}]}], "abstractContent": [{"text": "Traditional convolutional neural network (CNN) based query classification uses linear feature mapping in its convolution operation.", "labels": [], "entities": [{"text": "convolutional neural network (CNN) based query classification", "start_pos": 12, "end_pos": 73, "type": "TASK", "confidence": 0.6316957010163201}]}, {"text": "The recurrent neural network (RNN), differs from a CNN in representing word sequence with their ordering information kept explicitly.", "labels": [], "entities": []}, {"text": "We propose using a deep long-short-term-memory (DLSTM) based feature mapping to learn feature representation for CNN.", "labels": [], "entities": []}, {"text": "The DLSTM, which is a stack of LSTM units, has different order of feature representations at different depth of LSTM unit.", "labels": [], "entities": []}, {"text": "The bottom LSTM unit equipped with input and output gates, extracts the first order feature representation from current word.", "labels": [], "entities": []}, {"text": "To extract higher order nonlinear feature representation, the LSTM unit at higher position gets input from two parts.", "labels": [], "entities": []}, {"text": "First part is the lower LSTM unit's memory cell from previous word.", "labels": [], "entities": []}, {"text": "Second part is the lower LSTM unit's hidden output from current word.", "labels": [], "entities": []}, {"text": "In this way, the DLSTM captures the nonlinear nonconsecutive interaction within n-grams.", "labels": [], "entities": []}, {"text": "Using an architecture that combines a stack of the DLSTM layers with a tradition CNN layer, we have observed new state-of-the-art query classification accuracy on benchmark data sets for query classification.", "labels": [], "entities": [{"text": "query classification", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.6464556157588959}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.7909987568855286}, {"text": "query classification", "start_pos": 187, "end_pos": 207, "type": "TASK", "confidence": 0.8077943623065948}]}], "introductionContent": [{"text": "Convolutional neural networks (CNNs) have achieved significant improvements for query classification.", "labels": [], "entities": [{"text": "query classification", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.8571166396141052}]}, {"text": "CNNs capture the correlations of spatial or temporal structures with different resolutions using their temporal convolution operators.", "labels": [], "entities": []}, {"text": "A pooling strategy on these local correlations extracts invariant regularities.", "labels": [], "entities": []}, {"text": "However, CNNs use simple linear operations on ngram vectors that are formed by concatenating word vectors.", "labels": [], "entities": []}, {"text": "The linear operation together with the concatenation may not be sufficient to model the nonconsecutive dependency and interaction within the n-grams.", "labels": [], "entities": []}, {"text": "For example, in the query \"not a total loss\", nonconsecutive dependency \"not loss\" is the key information that is not well addressed by the linear operation with simple concatenation.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use deep long-shortterm-memory (DLSTM) based feature mapping to capture high order nonlinear feature representations.", "labels": [], "entities": []}, {"text": "LSTM) is one type of recurrent neural networks (RNNs) that have achieved remarkable performance in natural language processing and speech recognition).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7117746770381927}]}, {"text": "The DLSTM is a stack of LSTM units where different order of nonlinear feature representation is captured by LSTM units at different depth.", "labels": [], "entities": []}, {"text": "The bottom LSTM unit extracts the first order feature representation from current word.", "labels": [], "entities": []}, {"text": "The LSTM unit at the higher position captures the higher order feature representation relying on the outputs from LSTM units at lower position, specifically, the memory cell from lower LSTM unit at previous word position and the hidden output from lower LSTM unit at current word position.", "labels": [], "entities": []}, {"text": "Using DLSTM, linear feature mapping in traditional CNN can be obviously extended to nonlinear feature mapping.", "labels": [], "entities": [{"text": "linear feature mapping", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7000243862469991}]}, {"text": "Moreover, the memory cell together with different gates in LSTM unit are able to model the nonconsecutive feature interaction and information decaying based on context.", "labels": [], "entities": []}, {"text": "For example, in the query \"not so good\", the proposed DL-STM is expected to keep the information of \"not\" and \"good\" in the memory, and to decay the information about \"so\" via the forget gates.", "labels": [], "entities": []}, {"text": "Similar to CNNs where multiple convolution operations are used, we propose to stack different DLSTM feature mappings together to model multiple level nonlinear feature representations.", "labels": [], "entities": []}, {"text": "The bottom DL-STM layer takes the original word sequence as input.", "labels": [], "entities": []}, {"text": "The DLSTM layer at lower position fed its output to the adjacent higher DLSTM layer.", "labels": [], "entities": []}, {"text": "In the proposed models, the concatenation of the multiple level feature representations are further reduced by the pooling operation.", "labels": [], "entities": []}, {"text": "The prediction output is finally made based on the reduced feature representations.", "labels": [], "entities": []}, {"text": "We evaluated the proposed method on three benchmark data sets: Standford Sentiment Treebank dataset, TREC (Text Retrieval Conference) question type classification data set () and ATIS (Airline Travel Information Systems) dataset ().", "labels": [], "entities": [{"text": "Standford Sentiment Treebank dataset", "start_pos": 63, "end_pos": 99, "type": "DATASET", "confidence": 0.9364602267742157}, {"text": "TREC (Text Retrieval Conference) question type classification", "start_pos": 101, "end_pos": 162, "type": "TASK", "confidence": 0.660777038998074}, {"text": "ATIS (Airline Travel Information Systems) dataset", "start_pos": 179, "end_pos": 228, "type": "DATASET", "confidence": 0.6697973310947418}]}, {"text": "On Standford Sentiment Treebank dataset, our model obtains 51.9% accuracy on fine-grained classification and 88.7% accuracy on binary classification.", "labels": [], "entities": [{"text": "Standford Sentiment Treebank dataset", "start_pos": 3, "end_pos": 39, "type": "DATASET", "confidence": 0.9460430145263672}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994145631790161}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9991576671600342}]}, {"text": "The SVM based method uses a large amount of engineered features, and it outperforms LSTM and RNN based methods on TREC question type classification dataset.", "labels": [], "entities": [{"text": "TREC question type classification", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.6017853245139122}]}, {"text": "The DLSTM outperforms other neural network based methods without using engineered features.", "labels": [], "entities": []}, {"text": "On ATIS data, DLSTM achieves 97.9% F1 score, which is better than the previous best F1 score of 95.6% using the same data settings.", "labels": [], "entities": [{"text": "ATIS data", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.9585899710655212}, {"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9792744219303131}, {"text": "F1 score", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9874420166015625}]}], "datasetContent": [{"text": "We evaluate the proposed query classification models on sentence sentiment classification, question type categorization and query intent detection tasks.", "labels": [], "entities": [{"text": "query classification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7387394607067108}, {"text": "sentence sentiment classification", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.8394972880681356}, {"text": "query intent detection tasks", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.7460058629512787}]}, {"text": "For sentence sentiment classification, the Stanford Sentiment Treebank () is used.", "labels": [], "entities": [{"text": "sentence sentiment classification", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8677886525789896}, {"text": "Stanford Sentiment Treebank", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.909867525100708}]}, {"text": "In this dataset, 11855 English sentences are annotated at both sentence level and phrases level with fine-grained labels (very positive, positive, neutral, negative and very negative).", "labels": [], "entities": []}, {"text": "We use the provided data split, which has 8544 sentences for training, 1101 sentences for developing and 2210 sentences for testing.", "labels": [], "entities": []}, {"text": "This dataset also provides a binary classification variant that ignores the neutral sentences.", "labels": [], "entities": []}, {"text": "The binary classification task in this dataset has 6920 sentences for training, 872 sentences for developing and 1821 sentences for testing.", "labels": [], "entities": []}, {"text": "There are in total 17835 unique running words for fine-grained dataset and 16185 for binary version dataset.", "labels": [], "entities": []}, {"text": "For query intent detection, ATIS (airline travel information system) dataset () is used.", "labels": [], "entities": [{"text": "query intent detection", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9081900517145792}, {"text": "ATIS (airline travel information system) dataset", "start_pos": 28, "end_pos": 76, "type": "DATASET", "confidence": 0.5140505526214838}]}, {"text": "This dataset is mainly about the air travel domain with 26 different intents such as \"flight\", \"ground s ervice\" and \"city\".", "labels": [], "entities": []}, {"text": "There are 893 utterances for testing (ATIS-III, Nov93 and Dec94), and 4978 utterances for training (rest of ATIS-III and ATIS-II).", "labels": [], "entities": [{"text": "ATIS-III", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.630073070526123}, {"text": "Nov93 and Dec94", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.6951844890912374}, {"text": "ATIS-III", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.9463255405426025}, {"text": "ATIS-II", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.8636687397956848}]}, {"text": "There are 899 unique running words and 22 intents in the training data.", "labels": [], "entities": []}, {"text": "The question type classification task is to classify a question into a specific type, which is a very important step in question answering system.", "labels": [], "entities": [{"text": "question type classification", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7534507910410563}, {"text": "question answering", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.8604417741298676}]}, {"text": "In TREC (Text Retrieval Conference) data (), all the questions are divided into 6 categories, including \"human\", \"entity\", \"location\", \"description\", \"abbreviation\" and \"numeric\".", "labels": [], "entities": [{"text": "TREC (Text Retrieval Conference) data", "start_pos": 3, "end_pos": 40, "type": "DATASET", "confidence": 0.6927093097141811}]}, {"text": "The dataset in total has 5952 questions, 5452 of them for training, the rest for testing.", "labels": [], "entities": []}, {"text": "The vocabulary size of TREC dataset is 9592.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8203443288803101}]}, {"text": "Following previous work, we used word vectors pre-trained on large unannotated corpora to achieve better generalization capability.", "labels": [], "entities": []}, {"text": "In this paper, we used a publicly available 300 dimensional GloVe word vectors that are trained using Common Crawl with 840B tokens and 2.2M vocabulary size.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Standford Sentiment Treebank Classification accuracy", "labels": [], "entities": [{"text": "Standford Sentiment Treebank Classification", "start_pos": 10, "end_pos": 53, "type": "DATASET", "confidence": 0.8790230751037598}]}]}