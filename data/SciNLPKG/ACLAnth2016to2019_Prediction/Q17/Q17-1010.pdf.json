{"title": [{"text": "Enriching Word Vectors with Subword Information", "labels": [], "entities": [{"text": "Enriching Word Vectors", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.823036273320516}]}], "abstractContent": [{"text": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.", "labels": [], "entities": []}, {"text": "Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word.", "labels": [], "entities": []}, {"text": "This is a limitation, especially for languages with large vocabularies and many rare words.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew approach based on the skipgram model, where each word is represented as a bag of character n-grams.", "labels": [], "entities": []}, {"text": "A vector representation is associated to each character n-gram; words being represented as the sum of these representations.", "labels": [], "entities": []}, {"text": "Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data.", "labels": [], "entities": []}, {"text": "We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.", "labels": [], "entities": []}, {"text": "By comparing to recently proposed morphological word representations , we show that our vectors achieve state-of-the-art performance on these tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning continuous representations of words has along history in natural language processing).", "labels": [], "entities": []}, {"text": "These representations are typically derived from large unlabeled corpora using co-occurrence statistics.", "labels": [], "entities": []}, {"text": "A large body of work, known as distributional semantics, has studied the properties of these methods (Turney * The two first authors contributed equally. et al.,;).", "labels": [], "entities": []}, {"text": "In the neural network community, proposed to learn word embeddings using a feedforward neural network, by predicting a word based on the two words on the left and two words on the right.", "labels": [], "entities": []}, {"text": "More recently, proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently.", "labels": [], "entities": []}, {"text": "Most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing.", "labels": [], "entities": []}, {"text": "In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish.", "labels": [], "entities": []}, {"text": "For example, in French or Spanish, most verbs have more than forty different inflected forms, while the Finnish language has fifteen cases for nouns.", "labels": [], "entities": []}, {"text": "These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations.", "labels": [], "entities": []}, {"text": "Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.", "labels": [], "entities": []}, {"text": "In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors.", "labels": [], "entities": []}, {"text": "Our main contribution is to introduce an extension of the continuous skipgram model (), which takes into account subword information.", "labels": [], "entities": []}, {"text": "We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "In most experiments (except in Sec. 5.3), we compare our model to the C implementation 1 http://www.isthe.com/chongo/tech/comp/fnv 137 of the skipgram and cbow models from the word2vec 2 package.", "labels": [], "entities": [{"text": "word2vec 2 package", "start_pos": 176, "end_pos": 194, "type": "DATASET", "confidence": 0.7966498335202535}]}, {"text": "Except for the comparison to previous work (Sec. 5.3), we train our models on Wikipedia data.", "labels": [], "entities": []}, {"text": "We downloaded Wikipedia dumps in nine languages: Arabic, Czech, German, English, Spanish, French, Italian, Romanian and Russian.", "labels": [], "entities": []}, {"text": "We normalize the raw Wikipedia data using Matt Mahoney's pre-processing perl script.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8984348475933075}]}, {"text": "All the datasets are shuffled, and we train our models by doing five passes over them.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation between human judgement and  similarity scores on word similarity datasets. We  train both our model and the word2vec baseline on  normalized Wikipedia dumps. Evaluation datasets  contain words that are not part of the training set,  so we represent them using null vectors (sisg-).", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of our model and baselines on  word analogy tasks for Czech, German, English and  Italian. We report results for semantic and syntactic  analogies separately.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9776628017425537}, {"text": "word analogy", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.7598114907741547}]}, {"text": " Table 3: Spearman's rank correlation coefficient between human judgement and model scores for different  methods using morphology to learn word representations. We keep all the word pairs of the evaluation set  and obtain representations for out-of-vocabulary words with our model by summing the vectors of character  n-grams. Our model was trained on the same datasets as the methods we are comparing to (hence the two  lines of results for our approach).", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 21, "end_pos": 49, "type": "METRIC", "confidence": 0.7817073265711466}]}, {"text": " Table 5: Test perplexity on the language modeling  task, for 5 different languages. We compare to two  state of the art approaches: CLBL refers to the work  of", "labels": [], "entities": [{"text": "language modeling  task", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7986362179120382}]}]}