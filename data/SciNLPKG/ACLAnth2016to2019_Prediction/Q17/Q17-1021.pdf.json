{"title": [{"text": "Overcoming Language Variation in Sentiment Analysis with Social Attention", "labels": [], "entities": [{"text": "Overcoming Language Variation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7307037313779196}, {"text": "Sentiment Analysis", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9514832496643066}]}], "abstractContent": [{"text": "Variation in language is ubiquitous, particularly in newer forms of writing such as social media.", "labels": [], "entities": [{"text": "Variation in language", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8752402265866598}]}, {"text": "Fortunately, variation is not random; it is often linked to social properties of the author.", "labels": [], "entities": []}, {"text": "In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9561733901500702}]}, {"text": "The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways.", "labels": [], "entities": []}, {"text": "We formalize this idea in a novel attention-based neural network architecture , in which attention is divided among several basis models, depending on the author's position in the social network.", "labels": [], "entities": []}, {"text": "This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata.", "labels": [], "entities": []}, {"text": "This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9695003032684326}, {"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9348354339599609}]}], "introductionContent": [{"text": "Words can mean different things to different people.", "labels": [], "entities": []}, {"text": "Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (), gender), race), geography, and more ineffable characteristics such as political and cultural attitudes.", "labels": [], "entities": []}, {"text": "In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for characterizing variation in the lexicon (, orthography, and syntax (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8238129615783691}]}, {"text": "However, aside from the focused task of spelling normalization), there have been few attempts to make NLP systems more robust to language variation across speakers or writers.", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9157454073429108}]}, {"text": "One exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9552760720252991}, {"text": "topic classification", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8392674326896667}]}, {"text": "However, such demographic information is not directly available inmost datasets, and it is not yet clear whether predicted age and gender offer any improvements.", "labels": [], "entities": []}, {"text": "On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval), recommender systems (, and language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.7419850826263428}]}, {"text": "But personalization requires annotated data for each individual user-something that maybe possible in interactive settings such as information retrieval, but is not typically feasible in natural language processing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.727197140455246}]}, {"text": "We propose a middle ground between group-level demographic characteristics and personalization, by exploiting social network structure.", "labels": [], "entities": []}, {"text": "The sociological theory of homophily asserts that individuals are usually similar to their friends (.", "labels": [], "entities": []}, {"text": "This property has been demonstrated for language () as well as for the demographic properties targeted by, which are more likely to be shared by friends than by random pairs of individuals.", "labels": [], "entities": []}, {"text": "Social network information is available in a wide range of contexts, from social media ( to political speech () to historical texts.", "labels": [], "entities": []}, {"text": "Thus, social network homophily has the potential to provide a more general way to account for linguistic variation in NLP.", "labels": [], "entities": []}, {"text": "gives a schematic of the motivation for our approach.", "labels": [], "entities": []}, {"text": "The word 'sick' typically has a negative sentiment, e.g., 'I would like to believe he's sick rather than just mean and evil.'", "labels": [], "entities": []}, {"text": "1 However, in some communities the word can have a positive sentiment, e.g., the lyric 'this sick beat', recently trademarked by the musician Taylor Swift.", "labels": [], "entities": []}, {"text": "Given labeled examples of 'sick' in use by individuals in asocial network, we assume that the word will have a similar sentiment meaning for their near neighbors-an assumption of linguistic homophily that is the basis for this research.", "labels": [], "entities": []}, {"text": "Note that this differs from the assumption of label homophily, which entails that neighbors in the network will hold similar opinions, and will therefore produce similar document-level labels.", "labels": [], "entities": []}, {"text": "Linguistic homophily is a more generalizable claim, which could in principle be applied to any language processing task where author network information is available.", "labels": [], "entities": []}, {"text": "To scale this basic intuition to datasets with tens of thousands of unique authors, we compress the social network into vector representations of each author node, using an embedding method for large Charles Rangel, describing Dick Cheney 2 In the case of 'sick', speakers like Taylor Swift may employ either the positive and negative meanings, while speakers like Charles Rangel employ only the negative meaning.", "labels": [], "entities": []}, {"text": "In other cases, communities may maintain completely distinct semantics fora word, such as the term 'pants' in American and British English.", "labels": [], "entities": []}, {"text": "Thanks to Christopher Potts for suggesting this distinction and this example.", "labels": [], "entities": []}], "datasetContent": [{"text": "# Positive # Negative # Neutral #: Statistics of the SemEval Twitter sentiment datasets.", "labels": [], "entities": [{"text": "SemEval Twitter sentiment datasets", "start_pos": 53, "end_pos": 87, "type": "DATASET", "confidence": 0.7753995358943939}]}, {"text": "Applying the algorithm to, the authors within each triad would likely be closer to each other than to authors in the opposite triad.", "labels": [], "entities": []}, {"text": "We then incorporate these embeddings into an attention-based neural network model, called SOCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network.", "labels": [], "entities": [{"text": "ATTENTION", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9548006057739258}]}, {"text": "We apply SOCIAL ATTENTION to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks ().", "labels": [], "entities": [{"text": "ATTENTION", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9707051515579224}, {"text": "Twitter sentiment classification", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6590080459912618}, {"text": "SemEval Twitter sentiment analysis", "start_pos": 122, "end_pos": 156, "type": "TASK", "confidence": 0.854180708527565}]}, {"text": "We further adopt the system to Ciao product reviews (, training author embeddings using trust relationships between reviewers.", "labels": [], "entities": []}, {"text": "SOCIAL ATTEN-TION offers a 2-3% improvement over related neural and ensemble architectures in which the social information is ablated.", "labels": [], "entities": [{"text": "SOCIAL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.37849271297454834}, {"text": "ATTEN-TION", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9104790091514587}]}, {"text": "It also outperforms all prior published results on the SemEval Twitter test sets.", "labels": [], "entities": [{"text": "SemEval Twitter test sets", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.843282476067543}]}, {"text": "We employ the pretrained word embeddings used by, which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task.", "labels": [], "entities": []}, {"text": "The embeddings are learned using the structured skip-gram model ( , and the embedding dimension is set at 600, following.", "labels": [], "entities": []}, {"text": "We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.907747358083725}, {"text": "F1 score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9837919771671295}]}, {"text": "8 Competitive systems We consider five competitive Twitter sentiment classification methods.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7335520188013712}]}, {"text": "ties solely depend on the input values.", "labels": [], "entities": []}, {"text": "We adopt the summation of the pretrained word embeddings as the sentence-level input to learn the gating function.", "labels": [], "entities": []}, {"text": "The model architecture of random attention is nearly identical to SOCIAL ATTENTION: the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (\u22120.25, 0.25).", "labels": [], "entities": [{"text": "ATTENTION", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9342286586761475}]}, {"text": "Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier.", "labels": [], "entities": [{"text": "CNN", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9300925731658936}]}, {"text": "Finally, we include SOCIAL ATTENTION, the attention-based neural network method described in \u00a7 4.", "labels": [], "entities": [{"text": "ATTENTION", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8645645380020142}]}, {"text": "We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge: WE-BIS (,, and LSISLIF (.", "labels": [], "entities": [{"text": "SemEval 2015 Twitter sentiment analysis challenge", "start_pos": 64, "end_pos": 113, "type": "TASK", "confidence": 0.8791077037652334}, {"text": "WE-BIS", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.4883742928504944}]}, {"text": "UNITN achieves the best average F1 score on Test 2013-2015 sets among all the submitted systems.", "labels": [], "entities": [{"text": "UNITN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9410672783851624}, {"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9865681827068329}, {"text": "Test 2013-2015 sets", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9257866938908895}]}, {"text": "Finally, we republish results of NLSE (), a non-linear subspace embedding model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the SemEval Twitter sentiment  datasets.", "labels": [], "entities": [{"text": "SemEval Twitter sentiment  datasets", "start_pos": 28, "end_pos": 63, "type": "DATASET", "confidence": 0.8157469779253006}]}, {"text": " Table 2: Statistics of the author social networks used for  training author embeddings.", "labels": [], "entities": []}, {"text": " Table 3: Average F1 score on the SemEval test sets. The best results are in bold. Results are marked with * if they are  significantly better than CNN at p < 0.05.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9676344394683838}, {"text": "SemEval test sets", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8169302046298981}]}, {"text": " Table 4: Comparison of different social networks with  SOCIAL ATTENTION. The best results are in bold.", "labels": [], "entities": [{"text": "ATTENTION", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.8443611264228821}]}, {"text": " Table 7: Statistics of the Ciao product review datasets.", "labels": [], "entities": [{"text": "Ciao product review datasets", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.8070295602083206}]}, {"text": " Table 8: Average F1 score on the Ciao test set. The best  results are in bold. Results are marked with * and ** if  they are significantly better than CNN and random atten- tion respectively, at p < 0.05.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9700992107391357}, {"text": "Ciao test set", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9150752027829488}, {"text": "CNN", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.7463468313217163}]}]}