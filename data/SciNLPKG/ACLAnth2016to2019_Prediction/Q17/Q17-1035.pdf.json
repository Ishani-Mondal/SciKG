{"title": [{"text": "Joint Prediction of Word Alignment with Alignment Types", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.6491375118494034}]}], "abstractContent": [{"text": "Current word alignment models do not distinguish between different types of alignment links.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.701112911105156}]}, {"text": "In this paper, we provide anew proba-bilistic model for word alignment where word alignments are associated with linguistically motivated alignment types.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7784063518047333}]}, {"text": "We propose a novel task of joint prediction of word alignment and alignment types and propose novel semi-supervised learning algorithms for this task.", "labels": [], "entities": [{"text": "joint prediction of word alignment and alignment types", "start_pos": 27, "end_pos": 81, "type": "TASK", "confidence": 0.7081047892570496}]}, {"text": "We also solve a sub-task of predicting the alignment type given an aligned word pair.", "labels": [], "entities": []}, {"text": "In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment is an essential component in a statistical machine translation (SMT) system.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7464208006858826}, {"text": "statistical machine translation (SMT)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.7778959920008978}]}, {"text": "Soft alignments, or attention, are also an important component in neural machine translation (NMT) systems.", "labels": [], "entities": [{"text": "Soft alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6912066787481308}, {"text": "neural machine translation (NMT)", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.8464070955912272}]}, {"text": "The classic generative model approach to word alignment is based on IBM models 1-5 () and the HMM model ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8284608125686646}]}, {"text": "These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments.", "labels": [], "entities": []}, {"text": "Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (;).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8369730710983276}]}, {"text": "Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available fora handful of language pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9968218803405762}]}, {"text": "Semisupervised methods for word alignment combine hand-annotated word alignment data with parallel data without explicit word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8443924486637115}, {"text": "word alignment", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.6923474669456482}]}, {"text": "Even small amounts of hand-annotated word alignment data has been shown to improve the alignment and translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.6836079210042953}]}, {"text": "In this paper, we provide a novel semi-supervised word alignment model that adds alignment type information to word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7273937314748764}, {"text": "word alignments", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7245871871709824}]}, {"text": "Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7115967124700546}, {"text": "neural machine translation (NMT)", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.8476980527242025}]}, {"text": "However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9254375100135803}, {"text": "statistical word alignment", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.6038945317268372}]}, {"text": "Other than machine translation, word alignments are also important in the best performing models for NLP tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7292498648166656}, {"text": "word alignments", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7740868330001831}, {"text": "NLP tasks", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.8771331608295441}]}, {"text": "They play a central role in learning paraphrases in a source language by doing round-trips from source to target and back using word alignments (.", "labels": [], "entities": []}, {"text": "Aligments also form the basis for learning multi-lingual word embeddings and in the projection of syntactic and semantic annotations from one language to another).", "labels": [], "entities": []}, {"text": "Therefore, there is still a prominent role for word alignment in NLP; research into improvements in word alignment is a worthy goal.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.823631763458252}, {"text": "word alignment", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7792318761348724}]}, {"text": "Adding additional information such as part-ofspeech tags and syntactic parse information has yielded some improvements in word alignment quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7870623767375946}]}, {"text": "incorporated the partof-speech (POS) tags of the words in the sentence pair as a constraint on HMM-based word alignment.", "labels": [], "entities": [{"text": "HMM-based word alignment", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.7938673496246338}]}, {"text": "Additional constraints have also been injected into generative and discriminative models", "labels": [], "entities": [{"text": "generative", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.9721243977546692}]}], "datasetContent": [{"text": "For the experiments, we have used two datasets.", "labels": [], "entities": []}, {"text": "The first is the GALE Chinese-English Word Alignment and Tagging corpus which is released by LDC 1 . This dataset is annotated with gold alignment and alignment types (see Section 2 for more details).", "labels": [], "entities": [{"text": "GALE Chinese-English Word Alignment and Tagging", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8190641899903616}]}, {"text": "The second dataset is the Hong Kong parliament proceedings (HK Hansards) for which we do not have the gold alignment and alignment types.", "labels": [], "entities": [{"text": "Hong Kong parliament proceedings (HK Hansards)", "start_pos": 26, "end_pos": 72, "type": "DATASET", "confidence": 0.8751153647899628}]}, {"text": "We used 1 million sentences of the HK Hansards in the experiments to augment the training data.", "labels": [], "entities": [{"text": "HK Hansards", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8549906015396118}]}, {"text": "In the following sections, we describe three experiments.", "labels": [], "entities": []}, {"text": "First, we examine how effective the logistic regression classifier is for alignment type prediction.", "labels": [], "entities": [{"text": "alignment type prediction", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.8511137167612711}]}, {"text": "Second, we present our experiments for two tasks: word alignment and the joint prediction of word alignment and alignment types.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8330522775650024}]}, {"text": "Finally, we explain the machine translation experiment.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8048867583274841}]}, {"text": "We measure the performance of our models using precision, recall, and F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9997500777244568}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9995819926261902}, {"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9991118311882019}]}, {"text": "We also evaluated the performance of our models and the baseline models on two different tasks: (1) The traditional word alignment task and The joint prediction of word alignment and alignment types task.", "labels": [], "entities": [{"text": "word alignment task", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7967007954915365}]}, {"text": "The second task is harder as the model has to predict both word alignment and alignment types correctly.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.6854410320520401}]}, {"text": "Moreover, as the baseline IBM Model 1 and the baseline HMM cannot predict the alignment types, we can only make a comparison between our generative and discriminative models for the second task.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.8848791917165121}]}, {"text": "We initialized the translation probabilities of Model 1 uniformly over the word pairs that occur together in the same sentence pair.", "labels": [], "entities": []}, {"text": "We built an HMM similar to the one proposed by.", "labels": [], "entities": []}, {"text": "This model is referred to as HMM in this paper.", "labels": [], "entities": []}, {"text": "HMM was initialized with uniform transition probabilities and Model 1 translation probabilities.", "labels": [], "entities": []}, {"text": "Model 1 was trained for 5 iterations; it is followed by 5 iterations of HMM.", "labels": [], "entities": [{"text": "HMM", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8666031956672668}]}, {"text": "To handle unseen data when the model is applied to the test data, smoothing has been used.", "labels": [], "entities": []}, {"text": "We smooth translation probability p(f |e) by backingoff to a uniform probability 1/|V | where |V | is the source vocabulary size.", "labels": [], "entities": []}, {"text": "For smoothing alignment type probabilities p(h|f, e), we used the following linear interpolation:  Stanford POS tagger and then trained a model on these POS tags.", "labels": [], "entities": [{"text": "Stanford POS tagger", "start_pos": 99, "end_pos": 118, "type": "DATASET", "confidence": 0.8520946304003397}]}, {"text": "p(h) is the prior probability of alignment type h estimated over the gold training data using.", "labels": [], "entities": []}, {"text": "Both p(h|f, e) and p(h|t f , t e ) are smoothed with p(h) using linear interpolation.", "labels": [], "entities": []}, {"text": "To learn the hyper-parameters, we split the 20K LDC training data into two sets: a train set of 18K sentences and a 2K validation set.", "labels": [], "entities": [{"text": "LDC training data", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.6607478559017181}]}, {"text": "To learn p 0 and NULL emission probability, we performed a two-dimensional grid search varying p 0 in the set {0.05, 0.1, 0.2, 0.3, 0.4} and NULL emission probability in the set {1e-7, 5e-7, . .", "labels": [], "entities": []}, {"text": ".,1e-2, 5e-2, 1e-1}.", "labels": [], "entities": []}, {"text": "The tuned parameters that lead to the best result were achieved when p 0 = 0.3 and NULL emission probability was 5e-6.", "labels": [], "entities": [{"text": "NULL emission probability", "start_pos": 83, "end_pos": 108, "type": "METRIC", "confidence": 0.913035531838735}]}, {"text": "To tune the hyperparameters \u03bb 1 , \u03bb 2 and \u03bb 3 , we performed a twodimensional grid search.", "labels": [], "entities": []}, {"text": "The tuned parameters that lead to the best result was achieved when \u03bb 1 = 0.99, and \u03bb 3 =1e-15.", "labels": [], "entities": []}, {"text": "Hence, \u03bb 2 = 1 \u2212 \u03bb 1 \u2212 \u03bb 3 = 9.99e-11.", "labels": [], "entities": []}, {"text": "We then used these learned parameters in the experiments.", "labels": [], "entities": []}, {"text": "Finally, for HMM-based models, we smooth transition parameters p(i|i , I) by backing off to a uniform prior 1/I. joint prediction of word alignment and alignment types task, and (2) word alignment models followed by the discriminative classifier to predict alignment types. and tested on the 2K sentences used as held-out data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.6510670632123947}]}, {"text": "To see whether the improvement in F1-score by our generative model also improves the BLEU score, we aligned the 20K LDC data and 1 million sentences of the HK Hansards data using the augmented model and tested on 919 sentences of MTC part 4 (LDC2006T04).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9985266923904419}, {"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9752078354358673}, {"text": "HK Hansards data", "start_pos": 156, "end_pos": 172, "type": "DATASET", "confidence": 0.9442083835601807}, {"text": "MTC part 4 (LDC2006T04)", "start_pos": 230, "end_pos": 253, "type": "DATASET", "confidence": 0.9166468381881714}]}, {"text": "We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic.", "labels": [], "entities": []}, {"text": "We used Moses () with standard features, and tuned the weights with MERT).", "labels": [], "entities": [{"text": "MERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9806637167930603}]}, {"text": "An English 5-gram language model is trained using KenLM (Heafield, 2011) on the Gigaword corpus).", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9355482757091522}]}, {"text": "We give a comparison between HMM+Type+Gen model, our baseline HMM, GIZA++ HMM and standard GIZA++ (as used by Moses) in.", "labels": [], "entities": [{"text": "GIZA++ HMM", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.8376108805338541}]}, {"text": "We report the BLEU scores and TER computed using shows the performance of baseline HMM and HMM+Type+Gen model for two word alignment examples extracted from the test data, where squares indicate the gold standard alignments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9994632601737976}, {"text": "TER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9993891716003418}]}, {"text": "Numbers in the circles show the IDs of the predicted tags by the HMM+Type+Gen model, where ID of each tag is defined in.", "labels": [], "entities": []}, {"text": "The incorrectly predicted tags are shown with the * symbol.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of each alignment type in the annotated  training data", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9558812975883484}]}, {"text": " Table 3: Accuracy of the alignment type classifiers given  the alignment.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966312050819397}]}, {"text": " Table 4: Word alignment task results of the models  trained on 20K LDC data (22K LDC data for GIZA++)  and tested on 2K LDC test data.", "labels": [], "entities": [{"text": "Word alignment task", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.846549391746521}]}, {"text": " Table 1. Both p(h|f, e) and p(h|t f , t e )  are smoothed with p(h) using linear interpolation.", "labels": [], "entities": []}, {"text": " Table 6: Word alignment task results for the augmented  model.", "labels": [], "entities": [{"text": "Word alignment task", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8286611636479696}]}, {"text": " Table 7: Results using the augmented model for (1) joint  prediction of word alignment and alignment types task,  and (2) word alignment models followed by the discrim- inative classifier to predict alignment types.", "labels": [], "entities": [{"text": "prediction of word alignment", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.5736159756779671}]}, {"text": " Table 8: Word alignment task results, back-off using the  augmented model.", "labels": [], "entities": [{"text": "Word alignment task", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8346524039904276}]}, {"text": " Table 9: Results with back-off using the augmented  model for (1) joint prediction of word alignment and  alignment types task, and (2) word alignment models fol- lowed by the discriminative classifier to predict alignment  types.", "labels": [], "entities": [{"text": "joint prediction of word alignment", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.5491434812545777}]}, {"text": " Table 9. This confirms our success in improving  the performance of all the methods, compared to the  results in", "labels": [], "entities": []}, {"text": " Table 10.  We report the BLEU scores and TER computed us- ing", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9635952115058899}, {"text": "TER computed us- ing", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.9374621987342835}]}, {"text": " Table 11: Confusion matrix of the HMM+Type+Gen  model on the LDC test data. The vertical axis represents  the actual alignment type and the horizontal axis repre- sents the predicted alignment type.", "labels": [], "entities": [{"text": "LDC test data", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9245607654253641}]}]}