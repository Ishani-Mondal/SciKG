{"title": [{"text": "Automatically Tagging Constructions of Causation and Their Slot-Fillers", "labels": [], "entities": [{"text": "Automatically Tagging Constructions of Causation", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8189233303070068}]}], "abstractContent": [{"text": "This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case.", "labels": [], "entities": [{"text": "shallow semantic parsing", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.712242583433787}]}, {"text": "Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that causation can take on.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9072934985160828}]}, {"text": "We therefore base our approach on the concept of CONSTRUCTIONS from the linguistic paradigm known as CONSTRUCTION GRAMMAR (CxG).", "labels": [], "entities": [{"text": "GRAMMAR", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.7027177214622498}]}, {"text": "In CxG, a construction is a form/function pairing that can rely on arbitrary linguistic and semantic features.", "labels": [], "entities": []}, {"text": "Rather than codifying all aspects of each construc-tion's form, as some attempts to employ CxG in NLP have done, we propose methods that offload that problem to machine learning.", "labels": [], "entities": []}, {"text": "We describe two supervised approaches for tagging causal constructions and their arguments.", "labels": [], "entities": [{"text": "tagging causal constructions", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.9287408192952474}]}, {"text": "Both approaches combine automatically induced pattern-matching rules with statistical classifiers that learn the subtler parameters of the constructions.", "labels": [], "entities": []}, {"text": "Our results show that these approaches are promising: they significantly outperform na\u00a8\u0131vena\u00a8\u0131ve baselines for both construction recognition and cause and effect head matches.", "labels": [], "entities": [{"text": "construction recognition", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8739588260650635}]}], "introductionContent": [{"text": "Historically, shallow semantic parsing has focused on tagging predicates expressed by individual lexical units.", "labels": [], "entities": [{"text": "shallow semantic parsing", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.7427996595700582}, {"text": "tagging predicates expressed by individual lexical units", "start_pos": 54, "end_pos": 110, "type": "TASK", "confidence": 0.8773623619760785}]}, {"text": "While this paradigm has been fruitful, tying meaning to lexical units excludes some essential semantic relationships that cannot be captured in such a representation.", "labels": [], "entities": []}, {"text": "One domain that highlights the problem is causal relations.", "labels": [], "entities": []}, {"text": "Causation can be expressed in a tremen- 9.", "labels": [], "entities": []}, {"text": "Judy's comments were SO OFFENSIVE that I left.: Examples of causal language, reflecting the annotation scheme described in \u00a72.1 (with connectives in bold, CAUSES in small caps, and effects in italics).", "labels": [], "entities": [{"text": "SO", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.990757942199707}, {"text": "OFFENSIVE", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9218112826347351}]}, {"text": "dous variety of linguistic forms).", "labels": [], "entities": []}, {"text": "As exemplified in, possibilities include verbs, prepositions/conjunctions (3, 4, 5), adjectives (6), and much more complex expressions.", "labels": [], "entities": []}, {"text": "Some of these trickier cases can be handled as idiomatic multiword expressions (MWEs; 7).", "labels": [], "entities": []}, {"text": "Others (8, 9), however, are more structured than typical MWEs: they depend on particular configurations of syntactic relations and slot-fillers, placing them closer to the grammatical end of the continuum of lexicon and grammar.", "labels": [], "entities": []}, {"text": "This diversity presents a problem for most semantic parsers, which inherit the restrictions of the representational schemes they are based on.", "labels": [], "entities": []}, {"text": "Many semantic annotation schemes limit themselves to the argument structures of particular word classes.", "labels": [], "entities": []}, {"text": "For example, the Penn Discourse Treebank (PDTB; includes only conjunctions and adverbials as connectives,) and VerbNet () focus on verb arguments.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.9389692544937134}]}, {"text": "FrameNet () is less restrictive, allowing many parts of speech as triggers.", "labels": [], "entities": []}, {"text": "Most importantly, though, all these representations share the fundamental simplifying assumption that the basic linguistic carrier of meaning is the lexical unit.", "labels": [], "entities": []}, {"text": "Some (e.g., PDTB and FrameNet) allow MWEs as lexical units, and much work has been done on detecting and interpreting MWEs (see.", "labels": [], "entities": [{"text": "interpreting MWEs", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7720438241958618}]}, {"text": "But even these schemes overlook essential linguistic elements that encode meanings.", "labels": [], "entities": []}, {"text": "In example 9, for instance, a lexical unit approach would have to treat so as encoding the causal relationship, when in fact so merely intensifies the adjective; it is the combination of so and the finite clausal complement that indicates causality.", "labels": [], "entities": []}, {"text": "A more general approach can be found in the principles of CONSTRUCTION GRAMMAR (CxG;.", "labels": [], "entities": [{"text": "CONSTRUCTION", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.3664388358592987}, {"text": "GRAMMAR", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.8336887359619141}]}, {"text": "CxG posits that the fundamental units of language are CONSTRUC-TIONS -pairings of meanings with arbitrarily complex linguistic forms.", "labels": [], "entities": [{"text": "CxG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9290316104888916}]}, {"text": "These forms are often productive, consisting of some fixed elements combined with some open slots for semantic arguments.", "labels": [], "entities": []}, {"text": "The form/meaning pairings can be as simple as those in traditional lexical semantics.", "labels": [], "entities": []}, {"text": "The verb push, for instance, is paired with the meaning force to move, and the verb takes two linguistic arguments (subject and object) corresponding to the two semantic arguments (pusher and pushee).", "labels": [], "entities": []}, {"text": "But in CxG, the meaning-bearing forms can be much more complex: so X that Y is a single construction, paired with the meaning X to an extreme that causes Y.", "labels": [], "entities": []}, {"text": "The CxG paradigm can anchor semantic interpretations to any constellation of surface forms, making it potentially invaluable for computational semantics.", "labels": [], "entities": []}, {"text": "Even as it has grown in prominence in linguistics, however, CxG has received relatively little attention in NLP.", "labels": [], "entities": []}, {"text": "This is partly because the usual approach to operationalizing CxG is to rebuild the entire NLP pipeline to be \"constructions all the way down\" -to explicitly model the interactions and inheritance relationships between constructions that produce the final utterance and meaning.", "labels": [], "entities": []}, {"text": "Here, we take a different approach.", "labels": [], "entities": []}, {"text": "Instead of \"constructions all the way down,\" we propose a \"CON-STRUCTIONS ON TOP\" methodology: we use a conventional NLP pipeline for POS tagging, parsing, and soon, but add a layer for constructional phenomena that directly carry meaning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 134, "end_pos": 145, "type": "TASK", "confidence": 0.8442517817020416}, {"text": "parsing", "start_pos": 147, "end_pos": 154, "type": "TASK", "confidence": 0.7126357555389404}]}, {"text": "Rather than specifying by hand the constraints and properties that characterize each construction, we allow machine learning algorithms to learn these characteristics.", "labels": [], "entities": []}, {"text": "Causal relations present an ideal testbed for this approach.", "labels": [], "entities": []}, {"text": "As noted above, causal relations are realized in extremely diverse ways, demanding an operationalized concept of constructions.", "labels": [], "entities": []}, {"text": "Recognizing causal relations also requires a combination of linguistic analysis and broader world knowledge.", "labels": [], "entities": []}, {"text": "Additionally, causal relations are ubiquitous, both in our thinking and in our language (see, e.g.,.", "labels": [], "entities": []}, {"text": "Recognizing these relations is thus invaluable for many semantics-oriented applications, including textual entailment and question answering (especially for \"why\" questions).", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7301267385482788}, {"text": "question answering", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.857769101858139}]}, {"text": "They are especially helpful for domain-specific applications such as finance, politics, and biology (see, e.g.,, where extracting cause and effect relationships can help drive decision-making.", "labels": [], "entities": []}, {"text": "More general applications like machine translation and summarization, which ought to preserve stated causal relationships, can also benefit.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8143541216850281}, {"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9817326664924622}]}, {"text": "In the remainder of this paper, we suggest two related approaches for tagging causal constructions and their arguments.", "labels": [], "entities": [{"text": "tagging causal constructions", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.9102131326993307}]}, {"text": "We first review an annotation scheme for causal language and present anew corpus annotated using that scheme ( \u00a72).", "labels": [], "entities": []}, {"text": "We then define the task of tagging causal language, casting it as a construction recognition problem ( \u00a73).", "labels": [], "entities": [{"text": "tagging causal language", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8964495460192362}, {"text": "construction recognition", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.722774088382721}]}, {"text": "Because it is so hard to identify the relevant components of a construction (tenses, grammatical relations, etc.), the scheme and task do not explicitly include all of these elements.", "labels": [], "entities": []}, {"text": "We instead tag the words that participate in a causal construction as a proxy for that construction.", "labels": [], "entities": []}, {"text": "We leave it to annotators (when humans are annotating) or machine learning (during automated tagging) to assess when the full constellation of constructional elements is present.", "labels": [], "entities": []}, {"text": "Next, we present Causeway-L and Causeway-S, two versions of a pipeline for performing this task, and compare the two approaches.", "labels": [], "entities": [{"text": "Causeway-L", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.9260773658752441}, {"text": "Causeway-S", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9179455041885376}]}, {"text": "Both approaches use automatically induced patterns, either syntactic ( \u00a74.1) or lexical ( \u00a74.2), to find possible lexical triggers of causal constructions, as well as their likely arguments.", "labels": [], "entities": []}, {"text": "They then apply a mix of constructionspecific classifiers and construction-independent classifiers to determine when causal constructions are truly present.", "labels": [], "entities": []}, {"text": "We report on three sets of experiments ( \u00a75) assessing the two systems' performance, the impacts of various design features, and the effects of parsing errors.", "labels": [], "entities": []}, {"text": "The results indicate the viability of the approach, and point to further work needed to improve construction recognition ( \u00a76).", "labels": [], "entities": [{"text": "construction recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.9197025299072266}]}], "datasetContent": [{"text": "In this experiment, we measured the performance of Causeway-S, Causeway-L, and the baseline on the tasks of connective discovery and argument ID.", "labels": [], "entities": [{"text": "Causeway-S", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8983890414237976}, {"text": "Causeway-L", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.9109682440757751}, {"text": "connective discovery", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7911267280578613}, {"text": "argument ID", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7000661492347717}]}, {"text": "We also tried taking the union of each system's outputs with the baseline's.", "labels": [], "entities": []}, {"text": "Because of the small size of BECAUSE, we report averaged metrics from 20-fold cross-validation, with fold size measured by sentence count.", "labels": [], "entities": [{"text": "BECAUSE", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.747177004814148}]}, {"text": "All pipelines were run on the same folds.", "labels": [], "entities": []}, {"text": "For connective discovery, we report precision, recall, and F 1 for connectives, requiring connectives to match exactly.", "labels": [], "entities": [{"text": "connective discovery", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8426534533500671}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9995762705802917}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9996562004089355}, {"text": "F 1", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.989372044801712}]}, {"text": "In counting true positives and false negatives, the only gold-standard instances counted are those with both a cause and an effect, in keeping with the task definition.", "labels": [], "entities": []}, {"text": "For argument ID, we split out metrics by causes and effects.", "labels": [], "entities": [{"text": "argument ID", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8372199237346649}]}, {"text": "For each, we report: \u2022 percent agreement on exact spans \u2022 percent agreement on heads \u2022 the average Jaccard index for gold-standard vs. predicted spans, defined as J(A, B) = |A\u2229B| |A\u222aB| , where A and B are the sets of tokens in the two spans.", "labels": [], "entities": []}, {"text": "This metric reflects how well the argument spans overlap when they do not match exactly.", "labels": [], "entities": []}, {"text": "All argument metrics are reported only for correctly predicted connectives, as there is noway to automatically evaluate argument spans for false positives.", "labels": [], "entities": []}, {"text": "Note that as a result, argument ID scores are not directly comparable between pipelines -the scores represent how well argument ID works given the previous stage, rather than in an absolute sense.", "labels": [], "entities": []}, {"text": "We use the same metrics for Experiments 2 and 3.", "labels": [], "entities": []}, {"text": "Our second experiment explores the impact of various design choices by eliminating individual design elements.", "labels": [], "entities": []}, {"text": "We report results from using the global and connective-specific classifiers on their own, without the soft-voting ensemble.", "labels": [], "entities": []}, {"text": "(The MFS classifier is tested alone as part of Experiment 1.)", "labels": [], "entities": [{"text": "MFS classifier", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.5567541718482971}]}, {"text": "We also report results from the ensemble classifier without using any features that primarily reflect world knowledge: NER tags, WordNet hypernyms, and lemma skip-grams.", "labels": [], "entities": []}, {"text": "In our third experiment, we examined the effects of parse errors on our pipelines' performance.", "labels": [], "entities": []}, {"text": "We compared each pipeline's performance with and without gold-standard parses, using only the Penn Treebank portion of BECAUSE.", "labels": [], "entities": [{"text": "Penn Treebank portion of BECAUSE", "start_pos": 94, "end_pos": 126, "type": "DATASET", "confidence": 0.9038377523422241}]}, {"text": "For gold-standard runs, we used the Stanford dependency converter) to convert the gold parses into dependency format.", "labels": [], "entities": []}, {"text": "We report averaged results from 20-fold cross-validation on this subcorpus.", "labels": [], "entities": []}, {"text": "Results from Experiment 1 are shown in.", "labels": [], "entities": []}, {"text": "Our most important conclusion from these results is that a classifier can indeed learn to recognize many of the subtleties that distinguish causal constructions from their similar non-causal counterparts.", "labels": [], "entities": []}, {"text": "Even our end-to-end baseline offers moderate performance, but Causeway-L outperforms it at connective discovery by over 14 F 1 points, and Causeway-S outperforms it by 18 points.", "labels": [], "entities": [{"text": "Causeway-L", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.9036239981651306}, {"text": "connective discovery", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7919085323810577}, {"text": "F 1", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9082735478878021}, {"text": "Causeway-S", "start_pos": 139, "end_pos": 149, "type": "DATASET", "confidence": 0.8475009799003601}]}, {"text": "The design of the filter is a significant contributor here.", "labels": [], "entities": []}, {"text": "The MFS classifier alone substantially underperforms the voting classifier, particularly for the syntactic pipeline.", "labels": [], "entities": [{"text": "MFS classifier", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6396803557872772}]}, {"text": "The small connectives filter makes up some of the difference, but the full pipeline still beats the MFS filter by 4.4 points for the lexical system and 6.7 points for the syntax-based system.", "labels": [], "entities": []}, {"text": "When our pipelines are combined with the end-toend baseline, the results are better still, beating the baseline alone by 21.4 points.", "labels": [], "entities": []}, {"text": "This supports our hypothesis that causal construction recognition rests on a combination of both shallow and deep information.", "labels": [], "entities": [{"text": "causal construction recognition", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.8287308812141418}]}, {"text": "As expected, both pipelines show high recall but low precision for the connective discovery stage.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9995253086090088}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9993993043899536}, {"text": "connective discovery", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8385446965694427}]}, {"text": "(Much of the remaining gap in recall came simply from the long tail of constructions -about half of connective types never saw a pattern match.)", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9946644306182861}]}, {"text": "The filter does balance out precision and recall fora better F 1 . However, as the filter's steep drop in recall suggests, more work is needed to upweight positive instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9994000196456909}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9986578226089478}, {"text": "F 1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9744549095630646}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9993048906326294}]}, {"text": "Examining the classifier scores reveals that the filter is doing a good job of assigning low probability to negative instances: the vast majority of false pattern matches are clustered below a probability of 0.5, whereas the positives are peppered more evenly over the probability spectrum.", "labels": [], "entities": []}, {"text": "Unfortunately, the positives' probabilities are not clustered on the high end, as they should be.", "labels": [], "entities": []}, {"text": "Significant leverage could be gained just from improving classification for the connective to.", "labels": [], "entities": []}, {"text": "For both pipelines, this one connective accounted for 20-25% of end-to-end false positives and false negatives, and nearly half of all misclassifications by the filter.", "labels": [], "entities": []}, {"text": "Many of the remaining errors (about 40%) came from just a few simple but highly ambiguous/polysemous connectives, including if, for, and so.", "labels": [], "entities": []}, {"text": "For complex constructions (MWEs or more complex syntactic structures), Causeway-L achieved 42% F 1 and Causeway-S achieved 48%.", "labels": [], "entities": [{"text": "Causeway-L", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.8758447170257568}, {"text": "F 1", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9956821501255035}, {"text": "Causeway-S", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.8948392868041992}]}, {"text": "Overall, then, it seems that the classifier is doing well even at the cases that would challenge typical semantic parsing systems, but it needs some features that will allow it to upweight positive instances of a few challenging words.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7704745829105377}]}, {"text": "For argument ID, both techniques do reasonably well at recovering exact argument spans, and the H C and H E columns show that even when the exact spans do not match, the key content words are mostly correct.", "labels": [], "entities": [{"text": "argument ID", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.835847944021225}]}, {"text": "The low Jaccard indices, meanwhile, indicate that there is plenty of room for improvement in finding not just heads, but full spans.", "labels": [], "entities": [{"text": "Jaccard indices", "start_pos": 8, "end_pos": 23, "type": "METRIC", "confidence": 0.9056558310985565}]}, {"text": "Interestingly, effects seem to be harder to recover than causes.", "labels": [], "entities": []}, {"text": "The likely culprit is the difference in lengths between the two types of arguments.", "labels": [], "entities": [{"text": "lengths", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9519006013870239}]}, {"text": "The distribution of cause lengths is skewed toward low numbers, with a peak at 2 and a median of 5, while effects have a smoother peak at 5 with a median of 7 (.", "labels": [], "entities": []}, {"text": "The difference makes it harder for the system to guess full effect spans, and even for heads there are more plausible options.", "labels": [], "entities": []}, {"text": "The length disparity, in turn, is probably due to the fact that causes are likely to be subjects (19%) or nominal modifiers (13%), which skew short, whereas most effects are primary clauses (24%), complements (30%), or direct objects (12%), which are often more complex.", "labels": [], "entities": []}, {"text": "Results for Experiment 2 are shown in.", "labels": [], "entities": []}, {"text": "The results using single classifiers, rather than an: Results for Experiment 1.", "labels": [], "entities": []}, {"text": "SC and SE indicate exact span match for causes and effects, respectively; H C and H E indicate percentage accuracy for cause and effect heads; and J C and J E indicate cause and effect Jaccard indices.", "labels": [], "entities": [{"text": "SE", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.997907280921936}, {"text": "exact span match", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9464934666951498}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9638146758079529}]}, {"text": "\"SC filter\" indicates the filter for smaller overlapping connectives.", "labels": [], "entities": []}, {"text": "For the combinations of the baseline with Causeway, the union of the baseline's and Causeway's outputs was passed to the SC filter.", "labels": [], "entities": [{"text": "Causeway", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9837374687194824}, {"text": "Causeway", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9721013903617859}]}, {"text": "ensemble, uphold our design of combining multiple information sources.", "labels": [], "entities": []}, {"text": "Even the best non-ensemble filter, the per-connective filter in Causeway-S, underperforms its ensemble counterpart by 3.1 points.", "labels": [], "entities": [{"text": "Causeway-S", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.9654030203819275}]}, {"text": "Likewise, the results from removing worldknowledge features confirm that this knowledge significantly assists the classifier beyond what surfacelevel features alone can provide.", "labels": [], "entities": []}, {"text": "World knowledge features add 2.4 points for Causeway-L and 2.8 points for Causeway-S.  Results from Experiment 3 are shown in.", "labels": [], "entities": [{"text": "Causeway-L", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9433015584945679}, {"text": "Causeway-S.", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9364385604858398}]}, {"text": "As expected, Causeway-S improved significantly with gold-standard parses, whereas Causeway-L gets only a tiny boost.", "labels": [], "entities": [{"text": "Causeway-S", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.8715239763259888}, {"text": "Causeway-L", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9332377314567566}]}, {"text": "Surprisingly, Causeway-S did not improve from better TRegex matching of connectives per se.", "labels": [], "entities": []}, {"text": "In fact, all scores for connective matching from the first stage were worse with gold-standard parses.", "labels": [], "entities": [{"text": "connective matching", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8472521603107452}]}, {"text": "Instead, the improvement appears to come from argument identification: better parses made it easier to identify argument heads, which in turn made the many features based on those heads more reliable.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8304983377456665}]}, {"text": "This is supported by the high argument head accuracy with gold parses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9518042802810669}]}, {"text": "Further, when we ran the baseline on the PTB subcorpus with and without gold parses, we saw a similar improvement.", "labels": [], "entities": [{"text": "PTB subcorpus", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9892963469028473}]}, {"text": "Thus, although the limited data and the classifier's failure to upweight positives are still the primary handicaps, better parses would be somewhat helpful for at least the syntax-based approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Inter-annotator agreement results for the BE- CAUSE corpus. The difference between the two columns  is that for the left column, we counted two annotation  spans as a match if at least a quarter of the larger one  overlapped with the smaller; for the right column, we re- quired an exact match. \u03ba scores indicate Cohen's kappa.  Each \u03ba score was calculated only for spans that agreed  (e.g., degrees were only compared for matching connec- tive spans).", "labels": [], "entities": [{"text": "BE- CAUSE corpus", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.6634535863995552}]}, {"text": " Table 3: Percentages of the causal connectives in BE- CAUSE that would be partially or fully annotatable under  other annotation schemes. Connectives were grouped into  types by the sequence of connective lemmas.", "labels": [], "entities": [{"text": "BE- CAUSE", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8130386273066202}]}, {"text": " Table 6: Results for Experiment 1. S C and S E indicate exact span match for causes and effects, respectively; H C and  H E indicate percentage accuracy for cause and effect heads; and J C and J E indicate cause and effect Jaccard indices.  \"SC filter\" indicates the filter for smaller overlapping connectives. For the combinations of the baseline with Causeway,  the union of the baseline's and Causeway's outputs was passed to the SC filter.", "labels": [], "entities": [{"text": "exact span match", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.9429481426874796}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9777102470397949}, {"text": "Causeway", "start_pos": 354, "end_pos": 362, "type": "DATASET", "confidence": 0.9885990619659424}, {"text": "Causeway", "start_pos": 397, "end_pos": 405, "type": "DATASET", "confidence": 0.9733539819717407}]}, {"text": " Table 7: Results for Experiment 2. Unablated full-pipeline results from", "labels": [], "entities": []}, {"text": " Table 8: Results for Experiment 3.", "labels": [], "entities": []}]}