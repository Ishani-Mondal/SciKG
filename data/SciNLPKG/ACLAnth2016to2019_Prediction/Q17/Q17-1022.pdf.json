{"title": [{"text": "Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "We present ATTRACT-REPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources.", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.8988580703735352}]}, {"text": "ATTRACT-REPEL facilitates the use of constraints from mono-and cross-lingual resources, yielding semantically specialized cross-lingual vector spaces.", "labels": [], "entities": []}, {"text": "Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces fora plethora of different languages, facilitating semantic transfer from high-to lower-resource ones.", "labels": [], "entities": []}, {"text": "The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages.", "labels": [], "entities": []}, {"text": "We next show that ATTRACT-REPEL-specialized vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages.", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.8013876179854075}]}, {"text": "Finally, we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual DST models, which brings further performance improvements.", "labels": [], "entities": [{"text": "DST", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.8622723817825317}]}], "introductionContent": [{"text": "Word representation learning has become a research area of central importance in modern natural language processing.", "labels": [], "entities": [{"text": "Word representation learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8131881952285767}, {"text": "natural language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6621763408184052}]}, {"text": "The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations ().", "labels": [], "entities": []}, {"text": "Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity.", "labels": [], "entities": []}, {"text": "These models typically build on distributional ones by using human-or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections.", "labels": [], "entities": []}, {"text": "Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9564726948738098}]}, {"text": "We term this approach semantic specialization.", "labels": [], "entities": [{"text": "semantic specialization", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7873886227607727}]}, {"text": "In this paper we advance the semantic specialization paradigm in a number of ways.", "labels": [], "entities": [{"text": "semantic specialization", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8752193450927734}]}, {"text": "We introduce anew algorithm, ATTRACT-REPEL, that uses synonymy and antonymy constraints drawn from lexical resources to tune word vector spaces using linguistic information that is difficult to capture with conventional distributional training.", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.9186428189277649}]}, {"text": "Our evaluation shows that ATTRACT-REPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (  and).", "labels": [], "entities": []}, {"text": "We then deploy the ATTRACT-REPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations.", "labels": [], "entities": []}, {"text": "This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones.", "labels": [], "entities": []}, {"text": "In each case, the vast majority of each words' neighbors are meaningful synonyms/translations.", "labels": [], "entities": []}, {"text": "While there is a considerable amount of prior research on joint learning of cross-lingual vector spaces (see Section 2.2), to the best of our knowledge we are the first to apply semantic specialization to this problem.", "labels": [], "entities": []}, {"text": "We demonstrate its efficacy with state-ofthe-art results on the four languages in the Multilingual SimLex-999 dataset (.", "labels": [], "entities": [{"text": "Multilingual SimLex-999 dataset", "start_pos": 86, "end_pos": 117, "type": "DATASET", "confidence": 0.579781969388326}]}, {"text": "To show that our approach yields semantically informative vectors for lower-resource languages, we collect intrinsic evaluation datasets for Hebrew and Croatian and show that cross-lingual specialization significantly improves word vector quality in these two (comparatively) low-resource languages.", "labels": [], "entities": []}, {"text": "In the second part of the paper, we explore the use of ATTRACT-REPEL-specialized vectors in a downstream application.", "labels": [], "entities": []}, {"text": "One important motivation for training word vectors is to improve the lexical coverage of supervised models for language understanding tasks, e.g., question answering () or textual entailment (.", "labels": [], "entities": [{"text": "language understanding tasks", "start_pos": 111, "end_pos": 139, "type": "TASK", "confidence": 0.7758008142312368}, {"text": "question answering", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8471284210681915}]}, {"text": "In 1 Some (negative) effects of the distributional hypothesis do persist.", "labels": [], "entities": []}, {"text": "For example, nl_krieken (Dutch for cherries), is identified as a synonym for en_morning, presumably because the idiom 'het krieken van de dag' translates to 'the crack of dawn'.", "labels": [], "entities": []}, {"text": "2 Our approach is not suited for languages for which no lexical resources exist.", "labels": [], "entities": []}, {"text": "However, many languages have some coverage in cross-lingual lexicons.", "labels": [], "entities": []}, {"text": "For instance, BabelNet 3.7 automatically aligns WordNet to Wikipedia, providing accurate cross-lingual mappings between 271 languages.", "labels": [], "entities": []}, {"text": "In our evaluation, we demonstrate substantial gains for Hebrew and Croatian, both of which are spoken by less than 10 million people worldwide.", "labels": [], "entities": []}, {"text": "this work, we use the task of dialogue state tracking (DST) for extrinsic evaluation.", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7655761539936066}, {"text": "extrinsic evaluation", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7210405766963959}]}, {"text": "This task, which arises in the construction of statistical dialogue systems (, involves understanding the goals expressed by the user and updating the system's distribution over such goals as the conversation progresses and new information becomes available.", "labels": [], "entities": []}, {"text": "We show that incorporating our specialized vectors into a state-of-the-art neural-network model for DST improves performance on English dialogues.", "labels": [], "entities": [{"text": "DST", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9699684381484985}]}, {"text": "In the multilingual spirit of this paper, we produce new Italian and German DST datasets and show that using ATTRACT-REPEL-specialized vectors leads to even stronger gains in these two languages.", "labels": [], "entities": [{"text": "German DST datasets", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.6103829642136892}]}, {"text": "Finally, we show that our cross-lingual vectors can be used to train a single model that performs DST in all three languages, in each case outperforming the monolingual model.", "labels": [], "entities": [{"text": "DST", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9644361734390259}]}, {"text": "To the best of our knowledge, this is the first work on multilingual training of any component of a statistical dialogue system.", "labels": [], "entities": []}, {"text": "Our results indicate that multilingual training holds great promise for bootstrapping language understanding models for other languages, especially for dialogue domains where data collection is very resource-intensive.", "labels": [], "entities": [{"text": "bootstrapping language understanding", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.6494404574235281}]}, {"text": "All resources related to this paper are available at www.github.com/nmrksic/ attract-repel.", "labels": [], "entities": []}, {"text": "These include: 1) the ATTRACT-REPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work.", "labels": [], "entities": [{"text": "ATTRACT-REPEL source code", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.7943542897701263}, {"text": "Italian and German Dialogue State Tracking", "start_pos": 190, "end_pos": 232, "type": "TASK", "confidence": 0.5368628799915314}]}], "datasetContent": [{"text": "Spearman's rank correlation with the SimLex-999 dataset ( ) is used as the intrinsic evaluation metric throughout the experiments.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9516644477844238}]}, {"text": "Unlike other gold standard resources such as WordSim-353 () or MEN (), SimLex-999 consists of word pairs scored by annotators instructed to discern between semantic similarity and conceptual association, so that related but nonsimilar words (e.g. book and read) have a low rating.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9477560520172119}]}, {"text": "translated SimLex-999 to German, Italian and Russian, crowd-sourcing the similarity scores from native speakers of these languages.", "labels": [], "entities": [{"text": "similarity", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9497815370559692}]}, {"text": "We use this resource for multilingual intrinsic evaluation.", "labels": [], "entities": []}, {"text": "8 To investigate the portability of our approach to lower-resource languages, we used the same experimental setup to collect SimLex-999 datasets for Hebrew and Croatian.", "labels": [], "entities": [{"text": "SimLex-999 datasets", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.7156002223491669}]}, {"text": "For English vectors, we also report Spearman's correlation with SimVerb-3500 ( ), a semantic similarity dataset that focuses on verb pair similarity.", "labels": [], "entities": []}, {"text": "Monolingual and Cross-Lingual Specialization We start from distributional vectors for the SimLex languages: English, German, Italian and Russian.", "labels": [], "entities": []}, {"text": "For each language, we first perform semantic specialization of these spaces using: a) monolingual synonyms; b) monolingual antonyms; and c) the combination of both.", "labels": [], "entities": []}, {"text": "We then add cross-lingual synonyms and antonyms to these constraints and train a shared fourlingual vector space for these languages.", "labels": [], "entities": []}, {"text": "Hebrew and Croatian Wikipedias (which are used to induce their BabelNet constraints) currently consist of 203,867 / 172,824 articles, ranking them 40th / 42nd by size.", "labels": [], "entities": []}, {"text": "8 Leviant and Reichart (2015) also re-scored the original English SimLex.", "labels": [], "entities": [{"text": "English SimLex", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8509112596511841}]}, {"text": "We report results on their version, but also provide numbers for the original dataset for comparability.", "labels": [], "entities": []}, {"text": "The 999 word pairs and annotator instructions were translated by native speakers and scored by 10 annotators.", "labels": [], "entities": []}, {"text": "The interannotator agreement scores (Spearman's \u03c1) were 0.77 (pairwise) and 0.87 (mean) for Croatian, and 0.59 / 0.71 for Hebrew.", "labels": [], "entities": [{"text": "Spearman's \u03c1)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.6297973021864891}]}, {"text": "Comparison to Baseline Methods Both monoand cross-lingual specialization was performed using ATTRACT-REPEL and counter-fitting, in order to conclusively determine which of the two methods exhibited superior performance.", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.9664109945297241}]}, {"text": "Retrofitting and PARA-GRAM methods only inject synonymy, and their cost functions can be expressed using sub-components of counter-fitting and ATTRACT-REPEL cost functions.", "labels": [], "entities": [{"text": "PARA-GRAM", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8644043207168579}, {"text": "ATTRACT-REPEL", "start_pos": 143, "end_pos": 156, "type": "METRIC", "confidence": 0.8124013543128967}]}, {"text": "As such, the performance of the two investigated methods when they make use of synonymy (but not antonymy) constraints illustrates the performance range of the two preceding models.", "labels": [], "entities": []}, {"text": "Importance of Initial Vectors We use three different sets of initial word vectors: a) well-known distributional word vector collections (Section 4.1); b) distributional word vectors trained on the latest Wikipedia dumps; and c) word vectors randomly initialized using the XAVIER initialization.", "labels": [], "entities": []}, {"text": "The principal evaluation metric in our DST experiments is the joint goal accuracy, which represents the proportion of test set dialogue turns where all the search constraints expressed up to that point in the conversation were decoded correctly.", "labels": [], "entities": [{"text": "DST", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9581482410430908}, {"text": "joint goal accuracy", "start_pos": 62, "end_pos": 81, "type": "METRIC", "confidence": 0.6396531462669373}]}, {"text": "Our DST experiments investigate two propositions: 1.", "labels": [], "entities": [{"text": "DST", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8912796378135681}]}, {"text": "Intrinsic vs. Downstream Evaluation If monoand cross-lingual semantic specialization improves the semantic content of word vector collections according to intrinsic evaluation, we would expect the NBT model to perform higher-quality belief tracking when such improved vectors are deployed.", "labels": [], "entities": [{"text": "NBT", "start_pos": 197, "end_pos": 200, "type": "DATASET", "confidence": 0.9321193695068359}, {"text": "belief tracking", "start_pos": 233, "end_pos": 248, "type": "TASK", "confidence": 0.7084872275590897}]}, {"text": "We investigate the difference in DST performance for English, German and Italian when the NBT model employs the following word vector collections: 1) distributional word vectors; 2) monolingual semantically specialized vectors; and 3) monolingual subspaces of the cross-lingual semantically specialized EN-DE-IT-RU vectors.", "labels": [], "entities": [{"text": "DST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9824197292327881}, {"text": "NBT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9244201183319092}]}, {"text": "For each language, we also compare to the NBT performance achieved using the five state-of-the-art bilingual vector spaces we compared to in Section 5.3.", "labels": [], "entities": [{"text": "NBT", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6257074475288391}]}], "tableCaptions": [{"text": " Table 2: Linguistic constraint counts (in thousands). For  each language pair, the two figures show the number of in- jected synonymy and antonymy constraints. Monolingual  constraints (the diagonal elements) are underlined.", "labels": [], "entities": []}, {"text": " Table 3: Multilingual SimLex-999. The effect of using the COUNTER-FITTING and ATTRACT-REPEL procedures to  inject mono-and cross-lingual synonymy and antonymy constraints into the four collections of distributional word  vectors. Our best results set the new state-of-the-art performance for all four languages.", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9867145419120789}]}, {"text": " Table 4: Multilingual SimLex-999. The effect of  ATTRACT-REPEL (A-R) on alternative sets of starting  word vectors (Random = XAVIER initialization).", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9530267119407654}]}, {"text": " Table 5: SimLex-999 performance. Tying the SimLex languages into bilingual vector spaces with 16 different languages.  The first number in each row represents monolingual specialization. All but two of the bilingual spaces improved over  these baselines. The EN-FR vectors set a new high score of 0.754 on the original (English) SimLex-999.", "labels": [], "entities": []}, {"text": " Table 6: Bilingual semantic specialization for: a) Hebrew  and Croatian; and b) the original SimLex languages. Each  row shows how SimLex scores for that language improve  when its distributional vectors are tied into bilingual vector  spaces with the four high-resource languages.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of the intrinsic quality (SimLex-999)  of bilingual spaces produced by the ATTRACT-REPEL  method to those produced by five state-of-the-art methods  for constructing bilingual vector spaces.", "labels": [], "entities": []}]}