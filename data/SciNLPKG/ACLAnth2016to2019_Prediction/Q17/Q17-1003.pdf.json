{"title": [{"text": "Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction", "labels": [], "entities": [{"text": "Modeling Semantic Expectation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8187578916549683}, {"text": "Referent Prediction", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7388362288475037}]}], "abstractContent": [{"text": "Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content.", "labels": [], "entities": []}, {"text": "Prediction also affects perception and might be a key to robustness inhuman language processing.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9015851616859436}]}, {"text": "In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts.", "labels": [], "entities": [{"text": "human prediction", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.716590940952301}]}, {"text": "We find that script knowledge significantly improves model estimates of human predictions.", "labels": [], "entities": []}, {"text": "Ina second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.", "labels": [], "entities": []}], "introductionContent": [{"text": "Being able to anticipate upcoming content is a core property of human language processing) that has received a lot of attention in the psycholinguistic literature in recent years.", "labels": [], "entities": []}, {"text": "Expectations about upcoming words help humans comprehend language in noisy settings and deal with ungrammatical input.", "labels": [], "entities": []}, {"text": "In this paper, we use a computational model to address the question of how different layers of knowledge (linguistic knowledge as well as common-sense knowledge) influence human anticipation.", "labels": [], "entities": []}, {"text": "Here we focus our attention on semantic predictions of discourse referents for upcoming noun phrases.", "labels": [], "entities": [{"text": "semantic predictions of discourse referents for upcoming noun phrases", "start_pos": 31, "end_pos": 100, "type": "TASK", "confidence": 0.7750229438145956}]}, {"text": "This task is particularly interesting because it allows us to separate the semantic task of anticipating an intended referent and the processing of the actual surface form.", "labels": [], "entities": []}, {"text": "For example, in the context of I ordered a medium sirloin steak with fries.", "labels": [], "entities": []}, {"text": "Later, the waiter brought . .", "labels": [], "entities": []}, {"text": ", there is a strong expectation of a specific discourse referent, i.e., the referent introduced by the object NP of the preceding sentence, while the possible referring expression could be either the steak I had ordered, the steak, our food, or it.", "labels": [], "entities": []}, {"text": "Existing models of human prediction are usually formulated using the informationtheoretic concept of surprisal.", "labels": [], "entities": [{"text": "human prediction", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7306981384754181}]}, {"text": "In recent work, however, surprisal is usually not computed for DRs, which represent the relevant semantic unit, but for the surface form of the referring expressions, even though there is an increasing amount of literature suggesting that human expectations at different levels of representation have separable effects on prediction and, as a consequence, that the modelling of only one level (the linguistic surface form) is insufficient ().", "labels": [], "entities": []}, {"text": "The present model addresses this shortcoming by explicitly modelling and representing common-sense knowledge and conceptually separating the semantic (discourse referent) and the surface level (referring expression) expectations.", "labels": [], "entities": []}, {"text": "Our discourse referent prediction task is related to the NLP task of coreference resolution, but it substantially differs from that task in the following ways: 1) we use only the incrementally available left context, while coreference resolution uses the full text; 2) coreference resolution tries to identify the DR fora given target NP in context, while we look at the expectations of DRs based only on the context before the target NP is seen.", "labels": [], "entities": [{"text": "discourse referent prediction task", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7065223008394241}, {"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9295380413532257}, {"text": "coreference resolution", "start_pos": 223, "end_pos": 245, "type": "TASK", "confidence": 0.8565987944602966}, {"text": "coreference resolution", "start_pos": 269, "end_pos": 291, "type": "TASK", "confidence": 0.8839324116706848}]}, {"text": "The distinction between referent prediction and prediction of referring expressions also allows us to study a closely related question in natural language generation: the choice of a type of referring expression based on the predictability of the DR that is intended by the speaker.", "labels": [], "entities": [{"text": "referent prediction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8245158493518829}, {"text": "natural language generation", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.7094597220420837}]}, {"text": "This part of our work is inspired by a referent guessing experiment by, who showed that highly predictable referents were more likely to be realized with a pronoun than unpredictable referents, which were more likely to be realized using a full NP.", "labels": [], "entities": []}, {"text": "The effect they observe is consistent with a Gricean point of view, or the principle of uniform information density (see Section 5.1).", "labels": [], "entities": []}, {"text": "However, Tily and Piantadosi do not provide a computational model for estimating referent predictability.", "labels": [], "entities": [{"text": "estimating referent predictability", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.8264521757761637}]}, {"text": "Also, they do not include selectional preference or common-sense knowledge effects in their analysis.", "labels": [], "entities": []}, {"text": "We believe that script knowledge, i.e., commonsense knowledge about everyday event sequences, represents a good starting point for modelling conversational anticipation.", "labels": [], "entities": []}, {"text": "This type of common-sense knowledge includes temporal structure which is particularly relevant for anticipation in continuous language processing.", "labels": [], "entities": []}, {"text": "Furthermore, our approach can build on progress that has been made in recent years in methods for acquiring large-scale script knowledge; see Section 1.1.", "labels": [], "entities": []}, {"text": "Our hypothesis is that script knowledge maybe a significant factor inhuman anticipation of discourse referents.", "labels": [], "entities": []}, {"text": "Explicitly modelling this knowledge will thus allow us to produce more human-like predictions.", "labels": [], "entities": []}, {"text": "Script knowledge enables our model to generate anticipations about discourse referents that have already been mentioned in the text, as well as anticipations about textually new discourse referents which have been activated due to script knowledge.", "labels": [], "entities": []}, {"text": "By modelling event sequences and event participants, our model captures many more long-range dependencies than normal language models are able to.", "labels": [], "entities": []}, {"text": "As an example, consider the following two alternative text passages: We got seated, and had to wait for 20 minutes.", "labels": [], "entities": []}, {"text": "Then, the waiter brought the ...", "labels": [], "entities": []}, {"text": "We ordered, and had to wait for 20 minutes.", "labels": [], "entities": []}, {"text": "Then, the waiter brought the ...", "labels": [], "entities": []}, {"text": "Preferred candidate referents for the object position of the waiter brought the ... are instances of the food, menu, or bill participant types.", "labels": [], "entities": []}, {"text": "In the context of the alternative preceding sentences, there is a strong expectation of instances of a menu and a food participant, respectively.", "labels": [], "entities": []}, {"text": "This paper represents foundational research investigating human language processing.", "labels": [], "entities": [{"text": "human language processing", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6386802494525909}]}, {"text": "However, it also has the potential for application in assistant technology and embodied agents.", "labels": [], "entities": []}, {"text": "The goal is to achieve human-level language comprehension in realistic settings, and in particular to achieve robustness in the face of errors or noise.", "labels": [], "entities": []}, {"text": "Explicitly modelling expectations that are driven by common-sense knowledge is an important step in this direction.", "labels": [], "entities": []}, {"text": "In order to be able to investigate the influence of script knowledge on discourse referent expectations, we use a corpus that contains frequent reference to script knowledge, and provides annotations for coreference information, script events and participants (Section 2).", "labels": [], "entities": []}, {"text": "In Section 3, we present a large-scale experiment for empirically assessing human expectations on upcoming referents, which allows us to quantify at what points in a text humans have very clear anticipations vs. when they do not.", "labels": [], "entities": []}, {"text": "Our goal is to model human expectations, even if they turnout to be incorrect in a specific instance.", "labels": [], "entities": []}, {"text": "The experiment was conducted via Mechanical Turk and follows the methodology of Tily and Piantadosi.", "labels": [], "entities": []}, {"text": "In section 4, we describe our computational model that represents script knowledge.", "labels": [], "entities": []}, {"text": "The model is trained on the gold standard annotations of the corpus, because we assume that human comprehenders usually will have an analysis of the preceding discourse which closely corresponds to the gold standard.", "labels": [], "entities": []}, {"text": "We compare the prediction accuracy of this model to human predictions, as well as to two baseline models in Section 4.3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9483110904693604}]}, {"text": "One of them uses only structural linguistic features for predicting referents; the other uses general script-independent selectional preference features.", "labels": [], "entities": []}, {"text": "In Section 5, we test whether surprisal (as estimated from human guesses vs. computational models) can predict the type of referring expression used in the original texts in the corpus (pronoun vs. full referring expression).", "labels": [], "entities": []}, {"text": "This experiment also has wider implications with respect to the on-going discussion of whether the referring expression choice is dependent on predictability, as predicted by the uniform information density hy-(I) (1) P bather E wash to take a (bath) (2) P bath yesterday afternoon after working out . Once (I) (1) P bather got back home , (I) (1) P bather E enter bathroom to (my) (1) P bather (bathroom) (3) P bathroom and first quickly scrubbed the (bathroom tub) (4) P bathtub by [turning on] E turn water on the (water) (5) P water and rinsing (it) (4) P bathtub clean with a rag . After (I) (1) P bather finished , (I) (1) P bather E close drain the (tub) (4) P bathtub and began E fill water (it) (4) P bathtub with warm (water) (5) P water set at about 98 (degrees) (6) P temperature . Figure 1: An excerpt from a story in the InScript corpus.", "labels": [], "entities": [{"text": "InScript corpus", "start_pos": 835, "end_pos": 850, "type": "DATASET", "confidence": 0.8836781680583954}]}, {"text": "The referring expressions are in parentheses, and the corresponding discourse referent label is given by the superscript.", "labels": [], "entities": []}, {"text": "Referring expressions of the same discourse referent have the same color and superscript number.", "labels": [], "entities": []}, {"text": "Script-relevant events are in square brackets and colored in orange.", "labels": [], "entities": []}, {"text": "Event type is indicated by the corresponding subscript. pothesis.", "labels": [], "entities": []}, {"text": "The contributions of this paper consist of: \u2022 a large dataset of human expectations, in a variety of texts related to every-day activities.", "labels": [], "entities": []}, {"text": "\u2022 an implementation of the conceptual distinction between the semantic level of referent prediction and the type of a referring expression.", "labels": [], "entities": [{"text": "referent prediction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7701787948608398}]}, {"text": "\u2022 a computational model which significantly improves modelling of human anticipations.", "labels": [], "entities": [{"text": "modelling of human anticipations", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.758782371878624}]}, {"text": "\u2022 showing that script knowledge is a significant factor inhuman expectations.", "labels": [], "entities": []}, {"text": "\u2022 testing the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent.", "labels": [], "entities": []}], "datasetContent": [{"text": "We would like to test whether our model can produce accurate predictions and whether the model's guesses correlate well with human predictions for the referent cloze task.", "labels": [], "entities": []}, {"text": "In order to be able to evaluate the effect of script knowledge on referent predictability, we compare three models: our full Script model uses all of the features introduced in section 4.2; the Linguistic model relies only on the 'linguistic features' but not the script-specific ones; and the Base model includes all the shallow linguistic features.", "labels": [], "entities": [{"text": "referent predictability", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7362723648548126}]}, {"text": "The Base model differs from the linguistic model in that it does not model selectional preferences.", "labels": [], "entities": []}, {"text": "summarizes features used in different models.", "labels": [], "entities": []}, {"text": "The data set was randomly divided into training (70%), development (10%, 91 stories from 10 sce-39 narios), and test (20%, 182 stories from 10 scenarios) sets.", "labels": [], "entities": []}, {"text": "The feature weights were learned using L-BFGS ( to optimize the loglikelihood.", "labels": [], "entities": []}, {"text": "We calculated the percentage of correct DR predictions.", "labels": [], "entities": [{"text": "correct DR predictions", "start_pos": 32, "end_pos": 54, "type": "METRIC", "confidence": 0.7312731742858887}]}, {"text": "See for the averages across 10 scenarios.", "labels": [], "entities": []}, {"text": "We can see that the task appears hard for humans: their average performance reaches only 73% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9975649118423462}]}, {"text": "As expected, the Base model is the weakest system (the accuracy of 31%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9997672438621521}]}, {"text": "Modeling selectional preferences yields an extra 18% inaccuracy (Linguistic model).", "labels": [], "entities": []}, {"text": "The key finding is that incorporation of script knowledge increases the accuracy by further 13%, although still far behind human performance (62% vs. 73%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9996809959411621}]}, {"text": "Besides accuracy, we use perplexity, which we computed not only for all our models but also for human predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9992330074310303}]}, {"text": "This was possible as each task was solved by multiple humans.", "labels": [], "entities": []}, {"text": "We used unsmoothed normalized guess frequencies as the probabilities.", "labels": [], "entities": []}, {"text": "As we can see from, the perplexity scores are consistent with the accuracies: the script model again outperforms other methods, and, as expected, all the models are weaker than humans.", "labels": [], "entities": []}, {"text": "As we used two sets of script features, capturing different aspects of script knowledge, we performed extra ablation studies.", "labels": [], "entities": []}, {"text": "The experiments confirm that both feature sets were beneficial.", "labels": [], "entities": []}, {"text": "In the previous subsection, we demonstrated that the incorporation of selectional preferences and, perhaps more interestingly, the integration of automatically acquired script knowledge lead to improved accuracy in predicting discourse referents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9984328150749207}, {"text": "predicting discourse referents", "start_pos": 215, "end_pos": 245, "type": "TASK", "confidence": 0.9006173213322958}]}, {"text": "Now we turn to another question raised in the introduction: does incorporation of this knowledge make our predictions more human-like?", "labels": [], "entities": []}, {"text": "In other words, are we able to accurately estimate human expectations?", "labels": [], "entities": []}, {"text": "This includes not only being sufficiently accurate but also making the same kind of incorrect predictions.", "labels": [], "entities": []}, {"text": "In this evaluation, we therefore use human guesses collected during the referent cloze task as our target.", "labels": [], "entities": []}, {"text": "We then calculate the relative accuracy of each computational model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9986799359321594}]}, {"text": "As can be seen in, the Script model, at approx. 53% accuracy, is a lot more accurate in predicting human guesses than the Linguistic model and the Base model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992102384567261}, {"text": "predicting human guesses", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.9038520057996114}]}, {"text": "We can also  observe that the margin between the Script model and the Linguistic model is a lot larger in this evaluation than between the Base model and the Linguistic model.", "labels": [], "entities": []}, {"text": "This indicates that the model which has access to script knowledge is much more similar to human prediction behavior in terms of top guesses than the script-agnostic models.", "labels": [], "entities": []}, {"text": "Now we would like to assess if our predictions are similar as distributions rather than only yielding similar top predictions.", "labels": [], "entities": []}, {"text": "In order to compare the distributions, we use the Jensen-Shannon divergence (JSD), a symmetrized version of the KullbackLeibler divergence.", "labels": [], "entities": [{"text": "Jensen-Shannon divergence (JSD)", "start_pos": 50, "end_pos": 81, "type": "METRIC", "confidence": 0.5689241945743561}]}, {"text": "Intuitively, JSD measures the distance between two probability distributions.", "labels": [], "entities": [{"text": "JSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8773535490036011}]}, {"text": "A smaller JSD value is indicative of more similar distributions.", "labels": [], "entities": [{"text": "JSD", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5078762769699097}]}, {"text": "shows that the probability distributions resulting from the Script model are more similar to human predictions than those of the Linguistic and Base models.", "labels": [], "entities": []}, {"text": "In these experiments, we have shown that script knowledge improves predictions of upcoming referents and that the script model is the best among our models in approximating human referent predictions.", "labels": [], "entities": [{"text": "approximating human referent predictions", "start_pos": 159, "end_pos": 199, "type": "TASK", "confidence": 0.82310551404953}]}, {"text": "We ran four different logistic regression models.", "labels": [], "entities": []}, {"text": "These models all contained exactly the same set of linguistic predictors but differed in the estimates used for referent type surprisal and residual entropy.", "labels": [], "entities": []}, {"text": "One logistic regression model used surprisal estimates based on the human referent cloze task, while the three other models used estimates based on the three computational models (Base, Linguistic and Script).", "labels": [], "entities": []}, {"text": "For our experiment, we are interested in the choice of referring expression type for those occurrences of references, where a \"real choice\" is possible.", "labels": [], "entities": []}, {"text": "We therefore exclude for our analysis reported below all first mentions as well as all first and second person pronouns (because there is no optionality in how to refer to first or second person).", "labels": [], "entities": []}, {"text": "This subset contains 1345 data points.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracies (in %) and perplexities for different models and scenarios. The script model substantially out- performs linguistic and base models (with p < 0.001, significance tested with McNemar's test", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9975258708000183}, {"text": "McNemar's test", "start_pos": 195, "end_pos": 209, "type": "DATASET", "confidence": 0.7870543996493021}]}, {"text": " Table 4: Accuracies from ablation experiments.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.983223021030426}]}, {"text": " Table 5: Coefficients obtained from regression analysis for different models. Two NP types considered: full NP and  Pronoun/ProperNoun, with base class full NP. Significance: '***' < 0.001, '**' < 0.01, '*' < 0.05, and '.' < 0.1.", "labels": [], "entities": []}]}