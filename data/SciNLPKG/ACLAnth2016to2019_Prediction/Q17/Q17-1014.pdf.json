{"title": [{"text": "Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult.", "labels": [], "entities": []}, {"text": "We often rely on metrics of scholarly authority to find the prominent authors but these authority indices do not differentiate authority based on research topics.", "labels": [], "entities": []}, {"text": "We present Latent Topical-Authority Indexing (LTAI) for jointly modeling the topics, citations, and topical authority in a corpus of academic papers.", "labels": [], "entities": [{"text": "Latent Topical-Authority Indexing (LTAI)", "start_pos": 11, "end_pos": 51, "type": "METRIC", "confidence": 0.7350276559591293}]}, {"text": "Compared to previous models, LTAI differs in two main aspects.", "labels": [], "entities": []}, {"text": "First, it explicitly models the generative process of the citations, rather than treating the citations as given.", "labels": [], "entities": [{"text": "generative process of the citations", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8493593454360961}]}, {"text": "Second, it models each author's influence on citations of a paper based on the topics of the cited papers , as well as the citing papers.", "labels": [], "entities": []}, {"text": "We fit LTAI into four academic corpora: CORA, Arxiv Physics, PNAS, and Citeseer.", "labels": [], "entities": [{"text": "CORA", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.832745373249054}, {"text": "Arxiv", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8068573474884033}]}, {"text": "We compare the performance of LTAI against various base-lines, starting with the latent Dirichlet allocation , to the more advanced models including author-link topic model and dynamic author citation topic model.", "labels": [], "entities": []}, {"text": "The results show that LTAI achieves improved accuracy over other similar models when predicting words, citations and authors of publications.", "labels": [], "entities": [{"text": "LTAI", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7310937643051147}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9989728927612305}, {"text": "predicting words, citations and authors of publications", "start_pos": 85, "end_pos": 140, "type": "TASK", "confidence": 0.7217809781432152}]}], "introductionContent": [{"text": "With a corpus of scientific literature, we can observe the complex and intricate process of scientific progress.", "labels": [], "entities": []}, {"text": "We can learn the major topics in journal articles and conference proceedings, follow authors who are prolific and influential, and find papers that are highly cited.", "labels": [], "entities": []}, {"text": "The huge number of publications Citation Network", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we introduce the four academic corpora used to fit the LTAI, describe comparison models, and provide information about the evaluation metric and parameter settings for the LTAI 1 .  We experiment with four academic corpora: CORA (), Arxiv-Physics (, the Proceedings of the National Academy of Sciences (PNAS), and Citeseer (.", "labels": [], "entities": [{"text": "CORA", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.4666447639465332}, {"text": "Arxiv-Physics", "start_pos": 250, "end_pos": 263, "type": "METRIC", "confidence": 0.9438698291778564}]}, {"text": "CORA, Arxiv-Physics, and PNAS datasets contain abstracts only, and the locations of the citations within each paper are not preserved, whereas the Citeseer dataset contains the citation locations.", "labels": [], "entities": [{"text": "CORA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.969550609588623}, {"text": "Arxiv-Physics", "start_pos": 6, "end_pos": 19, "type": "METRIC", "confidence": 0.6140493750572205}, {"text": "PNAS datasets", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9131912589073181}, {"text": "Citeseer dataset", "start_pos": 147, "end_pos": 163, "type": "DATASET", "confidence": 0.9384869933128357}]}, {"text": "For CORA, Arxiv-Physics, and PNAS, we lemmatize words, remove stop words, and discard words that occur fewer than four times in the corpus.", "labels": [], "entities": [{"text": "CORA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7732458114624023}]}, {"text": "Table 1 describes the datasets in detail.", "labels": [], "entities": []}, {"text": "Note that we obtain citation data from the entire document, not only from the abstract.", "labels": [], "entities": []}, {"text": "Also, we consider withincorpus citation only, which leads to less than 13 average citation counts per document for all corpora.", "labels": [], "entities": []}, {"text": "We measured per-word log predictive probability on four datasets.", "labels": [], "entities": []}, {"text": "As shown in the graphs, our model performs better than LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.5593709945678711}]}, {"text": "We use MRR) to measure the predictive performance of the LTAI and the comparison models.", "labels": [], "entities": [{"text": "MRR", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9574733376502991}]}, {"text": "MRR is a widely used metric for evaluating link prediction tasks (Balog and de.", "labels": [], "entities": [{"text": "link prediction tasks", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7886429131031036}]}, {"text": "When the models output the correct answers as ranks, MRR is the inverse of the harmonic mean of such ranks.", "labels": [], "entities": [{"text": "MRR", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9981557726860046}]}, {"text": "We report the parameter values used for evaluations.", "labels": [], "entities": []}, {"text": "For all datasets, we set c \u2212 to 1.", "labels": [], "entities": []}, {"text": "To predict a citation, we set c + to 10,000, 100, 1,000, 10, and to predict authorship, we set c + to 1,000, 1,000, 10,000, 1,000 for CORA, Arxiv-Physics, PNAS, and Citeseer datasets.", "labels": [], "entities": [{"text": "CORA", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8996386528015137}, {"text": "Citeseer datasets", "start_pos": 165, "end_pos": 182, "type": "DATASET", "confidence": 0.8789933025836945}]}, {"text": "These values are obtained through exhaustive parameter analysis.", "labels": [], "entities": []}, {"text": "We set \u03b1 \u03b8 to 1, and \u03b1 \u03b2 to 0.1.", "labels": [], "entities": []}, {"text": "We fix the subsample sizes to 500 2 . For fair comparison, all the parameters that the LTAI and the baseline models share are set to have the same values, and for other parameters that uniquely belong to the baseline models, the values are exhaustively tuned as done in the LTAI.", "labels": [], "entities": [{"text": "LTAI", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.9173837304115295}, {"text": "LTAI", "start_pos": 274, "end_pos": 278, "type": "DATASET", "confidence": 0.9456868171691895}]}, {"text": "Finally, we note that all parameters are tuned using the training set, and test dataset is used only for the testing purpose.", "labels": [], "entities": []}, {"text": "We conduct the evaluation of the LTAI with three different quantitative tasks, along with one qualitative analysis.", "labels": [], "entities": []}, {"text": "In the first task, we check whether using citation and authorship information in the LTAI helps increase the word-level predictive performance.", "labels": [], "entities": []}, {"text": "In the second and third tasks, we measure the predictability of the LTAI regarding missing publication-publication linkage and authorpublication linkage.", "labels": [], "entities": []}, {"text": "With these two tasks, we compare the predictive power of the LTAI with other comparison models and use MRR as evaluation metric.", "labels": [], "entities": [{"text": "MRR", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9810649752616882}]}, {"text": "Finally, we observe famous researchers' topical authority scores generated by the LTAI and investigate how these scores capture notable academic characteristics of the researchers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets. From left to right, each column  shows the number of word tokens, number of docu- ments, number of authors, average citations per doc- ument (Avg C/D), and average citations per author  (Avg C/A).", "labels": [], "entities": [{"text": "average citations per doc- ument (Avg C/D)", "start_pos": 128, "end_pos": 170, "type": "METRIC", "confidence": 0.8639438102642695}]}, {"text": " Table 2: Authors with the highest h-index scores and their statistics from the CORA dataset. We show the  authors with their h-index, number of citations (# cite), and number of papers (# paper), representative topic,  and their topical authority (T Authority) of the corresponding topic. We show that while the authors have  the highest h-indices with lots of papers written and lots of citations earned, the topics that the authors exert  authority varies.", "labels": [], "entities": [{"text": "CORA dataset", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9823848307132721}, {"text": "T Authority)", "start_pos": 249, "end_pos": 261, "type": "METRIC", "confidence": 0.9045215845108032}]}, {"text": " Table 3: Authors who have a high authority score in  a computer vision topic.", "labels": [], "entities": []}, {"text": " Table 4: Authors who have a high authority score in  an artificial intelligence topic.", "labels": [], "entities": []}, {"text": " Table 5: Authors who have a high authority score in  a computer security topic.", "labels": [], "entities": []}]}