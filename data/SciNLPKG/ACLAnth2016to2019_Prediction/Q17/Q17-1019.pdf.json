{"title": [{"text": "Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "Pruning hypotheses during dynamic programming is commonly used to speedup inference in settings such as parsing.", "labels": [], "entities": []}, {"text": "Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search fora fast and accurate policy.", "labels": [], "entities": []}, {"text": "This poses a difficult machine learning problem, which we tackle with the LOLS algorithm.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9147505164146423}]}, {"text": "LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7337891459465027}]}, {"text": "We find that optimizing end-to-end performance in this way leads to a better Pareto frontier-i.e., parsers which are more accurate fora given runtime.", "labels": [], "entities": []}], "introductionContent": [{"text": "Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing) and machine translation (.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.788281112909317}, {"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8455087244510651}]}, {"text": "Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9976180195808411}]}, {"text": "For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.", "labels": [], "entities": []}, {"text": "Without heuristics, these parsers take minutes to process a single sentence.", "labels": [], "entities": []}, {"text": "To speedup inference, we will learn a pruning policy.", "labels": [], "entities": []}, {"text": "During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference process.", "labels": [], "entities": []}, {"text": "Our approach searches fora policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy \u2212 \u03bb \u00b7 runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9978845715522766}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9963647723197937}]}, {"text": "The parameter \u03bb \u2265 0 specifies the relative importance of runtime and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9984503984451294}]}, {"text": "By adjusting \u03bb, we obtain policies with different speed-accuracy tradeoffs.", "labels": [], "entities": []}, {"text": "For learning, we use Locally Optimal Learning to Search (LOLS) (), an algorithm for learning sequential decision-making policies, which accounts for the end-to-end performance of the entire decision sequence jointly.", "labels": [], "entities": []}, {"text": "Unfortunately, executing LOLS naively in our setting is prohibitive because it would run inference from scratch millions of times under different policies, training examples, and variations of the decision sequence.", "labels": [], "entities": []}, {"text": "Thus, this paper presents efficient algorithms for repeated inference, which are applicable to a wide variety of NLP tasks, including parsing, machine translation and sequence tagging.", "labels": [], "entities": [{"text": "parsing", "start_pos": 134, "end_pos": 141, "type": "TASK", "confidence": 0.9717851877212524}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7747336626052856}, {"text": "sequence tagging", "start_pos": 167, "end_pos": 183, "type": "TASK", "confidence": 0.6801882982254028}]}, {"text": "These algorithms, based on change propagation and dynamic programming, dramatically reduce time spent evaluating similar decision sequences by leveraging problem structure and sharing work among evaluations.", "labels": [], "entities": [{"text": "change propagation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7403079569339752}]}, {"text": "We evaluate our approach by learning pruning heuristics for constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8927882611751556}]}, {"text": "In this setting, our approach is the first to account for end-to-end performance of the pruning policy, without making independence assumptions about the reward function, as in prior work).", "labels": [], "entities": []}, {"text": "In the larger context of learning-to-search for structured prediction, our work is unusual in that it learns to control a dynamic programming algorithm (i.e., graphbased parsing) rather than a greedy algorithm (e.g., transition-based parsing).", "labels": [], "entities": [{"text": "learning-to-search for structured prediction", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.6572134345769882}]}, {"text": "Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.", "labels": [], "entities": [{"text": "Pareto frontier", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.7858220934867859}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.999441921710968}]}], "datasetContent": [{"text": "Reward functions and surrogates: Each user has a personal reward function.", "labels": [], "entities": []}, {"text": "In this paper, we choose to specify our true reward as accuracy \u2212 \u03bb \u00b7 runtime, where accuracy is given by labeled F 1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines 6 and 19 of Alg.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992172718048096}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9990959167480469}, {"text": "F 1 percentage", "start_pos": 114, "end_pos": 128, "type": "METRIC", "confidence": 0.9042283097902933}, {"text": "Alg.", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.9683502018451691}]}, {"text": "1, which is in practice proportional to seconds per sentence (correlation > 0.95) and is more replicable.", "labels": [], "entities": [{"text": "correlation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9747446775436401}]}, {"text": "We evaluate accordingly (on test data)-but during LOLS training we approximate these metrics.", "labels": [], "entities": []}, {"text": "We compare: \u2022 r CP (fast): Use change propagation ( \u00a75.2) to compute accuracy on a sentence as F 1 of just that sentence, and to approximate runtime as ||\u03b2|| 0 , 269 the number of constituents that were built.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9980682730674744}, {"text": "F 1", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9739714860916138}]}, {"text": "10 \u2022 r DP (faster): Use dynamic programming ( \u00a75.3) to approximate accuracy on a sentence as expected recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9956497550010681}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9705550074577332}]}, {"text": "11 This time we approximate runtime more crudely as ||m|| 0 , the number of nonzeros in the pruning mask for the sentence (i.e., the number of spans whose constituents the policy would be willing to keep if they were built).", "labels": [], "entities": []}, {"text": "We use these surrogates because they admit efficient rollout algorithms.", "labels": [], "entities": []}, {"text": "Less important, they preserve the training objective (1) as an average over sentences.", "labels": [], "entities": []}, {"text": "(Our true F 1 metric on a corpus cannot be computed in this way, though it could reasonably be estimated by averaging over minibatches of sentences in (1).)", "labels": [], "entities": [{"text": "F 1 metric", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8860795497894287}]}, {"text": "Controlled experimental design: Our baseline system is an adaptation of to learning-to-prune, as described in \u00a73 and \u00a76.", "labels": [], "entities": []}, {"text": "Our goal is to determine whether such systems can be improved by LOLS training.", "labels": [], "entities": []}, {"text": "We repeat the following design for both reward surrogates (r CP and r DP ) and for both grammars (coarse and fine).", "labels": [], "entities": []}, {"text": "x We start by training a number of baseline models by sweeping the asymmetric weighting parameter.", "labels": [], "entities": []}, {"text": "For the coarse grammar we train 8 such models, and for the fine grammar 12.", "labels": [], "entities": []}, {"text": "y For each baseline policy, we estimate a value of \u03bb for which that policy is optimal (among baseline policies) according to surrogate reward.", "labels": [], "entities": []}, {"text": "12 When using rCP, we speedup LOLS by doing \u2264 2n rollouts per sentence of length n.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8075684309005737}]}, {"text": "We sample these uniformly without replacement from the T possible rollouts ( \u00a75), and compensate by upweighting the resulting training examples by T /(2n).", "labels": [], "entities": []}, {"text": "11 Considering all nodes in the binarized tree, except for the root, width-1 constituents, and children of unary rules.", "labels": [], "entities": []}, {"text": "We estimate \u03bb by first fitting a parametric model yi = h(xi) ymax \u00b7 sigmoid(a \u00b7 log(xi + c) + b) to the baseline runtime-accuracy measurements on dev data (shown in green in) by minimizing mean squared error.", "labels": [], "entities": [{"text": "mean squared error", "start_pos": 189, "end_pos": 207, "type": "METRIC", "confidence": 0.8285351395606995}]}, {"text": "We then use the fitted curve's slope h to estimate each \u03bbi = h (xi), where xi is the runtime of baseline i.", "labels": [], "entities": []}, {"text": "The resulting choice of reward function y \u2212 \u03bbi \u00b7 x increases along the green arrow in, and is indeed maximized (subject toy \u2264 h(x), and in the region where h is concave) at x = xi.", "labels": [], "entities": []}, {"text": "As a sanity check, notice since \u03bbi is a derivative of the function y = h(x), its units are in units of y (accuracy) per unit of x (runtime), as appropriate for use in the expression y \u2212 \u03bbi \u00b7 x.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9933155179023743}]}, {"text": "Indeed, this procedure will construct the same reward function regardless of the units we use to express x.", "labels": [], "entities": []}, {"text": "Our specific parametric model h is a sigmoidal curve, with z For each baseline policy, we run LOLS with the same surrogate reward function (defined by \u03bb) for which that baseline policy was optimal.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9127341508865356}]}, {"text": "We initialize LOLS by setting \u03c0 0 to the baseline policy.", "labels": [], "entities": []}, {"text": "Furthermore, we include the baseline policy's weighted training set Q 0 in the at line 13.", "labels": [], "entities": [{"text": "weighted training set Q 0", "start_pos": 46, "end_pos": 71, "type": "METRIC", "confidence": 0.6321388840675354}]}, {"text": "shows that LOLS learns to improve on the baseline, as evaluated on development data.", "labels": [], "entities": []}, {"text": "{ But do these surrogate reward improvements also improve our true reward?", "labels": [], "entities": []}, {"text": "For each baseline policy, we use dev data to estimate a value of \u03bb for which that policy is optimal according to our true reward function.", "labels": [], "entities": []}, {"text": "We use blind test data to compare the baseline policy to its corresponding LOLS policy on this true reward function, testing significance with a paired permutation test.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9440678358078003}]}, {"text": "The improvements holdup, as shown in.", "labels": [], "entities": []}, {"text": "The rationale behind this design is that a user who actually wishes to maximize accuracy\u2212\u03bb\u00b7runtime, for some specific \u03bb, could reasonably start by choosing the best baseline policy for this reward function, and then try to improve that baseline by running LOLS with the same reward function.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9929109215736389}]}, {"text": "Our experiments show this procedure works fora range of \u03bb values.", "labels": [], "entities": []}, {"text": "In the real world, a user's true objective might instead be some nonlinear function of runtime and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9971615076065063}]}, {"text": "For example, when accuracy is \"good enough,\" it maybe more important to improve runtime, and vice-versa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9990856647491455}]}, {"text": "LOLS could be used with such a nonlinear reward function as well.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8005619645118713}]}, {"text": "In fact, a user does not even have to quantify their global preferences by writing down such a function.", "labels": [], "entities": []}, {"text": "Rather, they could select manually among the baseline policies, choosing one with an attractive speed-accuracy tradeoff, and then specify \u03bb to indicate a local direction of desired improvement (like the green arrows in, modifying this direction periodically as LOLS runs.", "labels": [], "entities": []}], "tableCaptions": []}