{"title": [{"text": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences.", "labels": [], "entities": []}, {"text": "We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations.", "labels": [], "entities": []}, {"text": "Such typological properties could be helpful in grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8761574923992157}]}, {"text": "While such a problem is usually regarded as unsu-pervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data.", "labels": [], "entities": []}, {"text": "The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (la-tent trees).", "labels": [], "entities": []}, {"text": "In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data.", "labels": [], "entities": []}, {"text": "2) Our system is robust even when the POS sequences include noise.", "labels": [], "entities": []}, {"text": "3) Our system on this task outperforms a grammar induction baseline by a large margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Descriptive linguists often characterize a human language by its typological properties.", "labels": [], "entities": []}, {"text": "For instance, English is an SVO-type language because its basic clause order is Subject-Verb-Object (SVO), and also a prepositional-type language because its adpositions normally precede the noun.", "labels": [], "entities": []}, {"text": "Identifying basic word order must happen early in the acquisition of syntax, and presumably guides the initial interpretation of sentences and the acquisition of a finer-grained grammar.", "labels": [], "entities": [{"text": "Identifying basic word order", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7381526231765747}]}, {"text": "In this paper, we propose a method for doing this.", "labels": [], "entities": []}, {"text": "While we focus on word order, one could try similar methods for other typological classifications (syntactic, morphological, or phonological).", "labels": [], "entities": [{"text": "word order", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.7037654370069504}]}, {"text": "The problem is challenging because the language's true word order statistics are computed from syntax trees, whereas our method has access only to a POS-tagged corpus.", "labels": [], "entities": []}, {"text": "Based on these POS sequences alone, we predict the directionality of each type of dependency relation.", "labels": [], "entities": []}, {"text": "We define the directionality to be areal number in: the fraction of tokens of this relation that are \"right-directed,\" in the sense that the child (modifier) falls to the right of its parent (head).", "labels": [], "entities": []}, {"text": "For example, the dobj relation points from a verb to its direct object (if any), so a directionality of 0.9-meaning that 90% of dobj dependencies are right-directed-indicates a dominant verb-object order.", "labels": [], "entities": []}, {"text": "(See for more such examples.)", "labels": [], "entities": []}, {"text": "Our system is trained to predict the relative frequency of rightward dependencies for each of 57 dependency types from the Universal Dependencies project (UD).", "labels": [], "entities": [{"text": "Universal Dependencies project (UD)", "start_pos": 123, "end_pos": 158, "type": "DATASET", "confidence": 0.6188643326361974}]}, {"text": "We assume that all languages draw on the same set of POS tags and dependency relations that is proposed by the UD project (see \u00a73), so that our predictor works across languages.", "labels": [], "entities": []}, {"text": "Why do this? has argued for using these directionality numbers in as fine-grained and robust typological descriptors.", "labels": [], "entities": []}, {"text": "We believe that these directionalities could also be used to help define an initializer, prior, or regularizer for tasks like grammar induction or syntax-based machine translation.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7185046225786209}, {"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7093397378921509}]}, {"text": "Finally, the vector of directionalities-or the feature vector that our method extracts in order to predict the directionalities-can be regarded as a language embedding computed from the POStagged corpus.", "labels": [], "entities": [{"text": "POStagged corpus", "start_pos": 186, "end_pos": 202, "type": "DATASET", "confidence": 0.9091947972774506}]}, {"text": "This language embedding maybe useful as an input to multilingual NLP systems, such as the cross-linguistic neural dependency parser of.", "labels": [], "entities": [{"text": "cross-linguistic neural dependency parser", "start_pos": 90, "end_pos": 131, "type": "TASK", "confidence": 0.6514249220490456}]}, {"text": "In fact, some multilingual NLP systems already condition on typological properties looked up in the World Atlas of Language Structures, or WALS (, and how they affect the directionality of Universal Dependencies relations.", "labels": [], "entities": [{"text": "World Atlas of Language Structures", "start_pos": 100, "end_pos": 134, "type": "DATASET", "confidence": 0.9400214910507202}]}, {"text": "we review in \u00a78.", "labels": [], "entities": []}, {"text": "However, WALS does not list all properties of all languages, and maybe somewhat inconsistent since it collects work by many linguists.", "labels": [], "entities": [{"text": "WALS", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9198614954948425}]}, {"text": "Our system provides an automatic alternative as well as a methodology for generalizing to new properties.", "labels": [], "entities": []}, {"text": "More broadly, we are motivated by the challenge of determining the structure of a language from its superficial features.", "labels": [], "entities": []}, {"text": "Principles & Parameters theory famously-if controversially-hypothesized that human babies are born with an evolutionarily tuned system that is specifically adapted to natural language, which can predict typological properties (\"parameters\") by spotting telltale configurations in purely linguistic input.", "labels": [], "entities": []}, {"text": "Here we investigate whether such a system can be tuned by gradient descent.", "labels": [], "entities": []}, {"text": "It is at least plausible that useful superficial features do exist: e.g., if nouns often precede verbs but rarely follow verbs, then the language maybe verb-final.", "labels": [], "entities": []}], "datasetContent": [{"text": "All previous experiments were conducted by crossvalidation on the 20 training languages.", "labels": [], "entities": []}, {"text": "We now train the system on all 20, and report results on the 17 blind test languages).", "labels": [], "entities": []}, {"text": "In our evaluation metric (1), R includes all 57 relation types that appear in training data, plus a special UNK type for: Accuracy on the simpler task of binary classification of relation directionality for each training language.", "labels": [], "entities": [{"text": "binary classification of relation directionality", "start_pos": 154, "end_pos": 202, "type": "TASK", "confidence": 0.6176796197891236}]}, {"text": "A detailed comparison shows that EC is significantly worse than UD and +GD, and that \u2205 is significantly worse than +GD (paired permutation test over the 20 languages, p < 0.05).", "labels": [], "entities": [{"text": "EC", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9621567726135254}]}, {"text": "The improvement from UD to +GD is insignificant, which suggests that this is an easier task where weak models might suffice.", "labels": [], "entities": [{"text": "GD", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9513919949531555}]}, {"text": "relations that appear only in test data.", "labels": [], "entities": []}, {"text": "The results range from good to excellent, with synthetic data providing consistent and often large improvements.", "labels": [], "entities": []}, {"text": "These results could potentially be boosted in the future by using an even larger and more diverse training set.", "labels": [], "entities": []}, {"text": "In principle, when evaluating on anyone of our 37 real languages, one could train a system on all of the other 36 (plus the galactic languages derived from them), not just 20.", "labels": [], "entities": []}, {"text": "Moreover, the Universal Dependencies collection has continued to grow beyond the 37 languages used here ( \u00a73).", "labels": [], "entities": [{"text": "Universal Dependencies collection", "start_pos": 14, "end_pos": 47, "type": "DATASET", "confidence": 0.8478213548660278}]}, {"text": "Finally, our current setup extracts only one training example from each (real or synthetic) language.", "labels": [], "entities": []}, {"text": "One could easily generate a variant of this example each time the language is visited during stochastic optimization, by bootstrap-resampling its training corpus (to add \"natural\" variation) or subsampling it (to train the predictor to work on smaller corpora).", "labels": [], "entities": []}, {"text": "In the case of a synthetic language, one could also generate a corpus of new trees each time the language is visited (by re-running the stochastic permutation procedure, instead of reusing the particular permutation released by the Galactic Dependencies project).: Our final comparison on the 17 test languages appears at left.", "labels": [], "entities": []}, {"text": "We ask whether the average expected loss on these 17 real target languages is reduced by augmenting the training pool of 20 UD languages with +20*21*21 GD languages.", "labels": [], "entities": []}, {"text": "For completeness, we extend the table with the cross-validation results on the training pool.", "labels": [], "entities": []}, {"text": "The \"Avg.\" lines report the average of 17 test or 37 training+testing languages.", "labels": [], "entities": []}, {"text": "We mark both \"+GD\" averages with \"*\" as they are significantly better than their \"UD\" counterparts (paired permutation test by language, p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Average expected loss over 20 UD languages, com- puted by 5-fold cross-validation. The first column indicates  whether we score using hand-engineered features (sH), neural  features (sN), or a combination (see end of  \u00a76.3). As a baseline,  the first line evaluates the EC (expected count) heuristic from   \u00a75. Within each column, we boldface the best (smallest) re- sult as well as all results that are not significantly worse (paired  permutation test by language, p < 0.05). A starred result is  significantly better than the other model in the same row.", "labels": [], "entities": [{"text": "EC (expected count)", "start_pos": 280, "end_pos": 299, "type": "METRIC", "confidence": 0.8927322030067444}]}, {"text": " Table 4: Cross-validation losses with different subsets of hand- engineered features from  \u00a76.3. \"\u2205\" is a baseline with no fea- tures (bias feature only), so it makes the same prediction for all  languages. \"conditional\" = \u03c0 w  t|s features, \"joint\" = \u03c0 w  t|s \u00b7 \u03c0 w  s fea- tures, \"PMI\" = \u03c0 w  t|s //\u03c0 w  t and \u03c0 w  t //\u03c0 w  t|s features, \"asymmetry\"  = \u03c0 w  t|s //\u03c0 \u2212w  t|s features, \"b\" are the features superscripted by b,  and \"t\" are the features with truncated window. \"+\" means con- catenation.The \"Length\" field refers to length thresholding (see   \u00a76.4). The system in the starred row is the one that we selected  for row 2 of Table 3.", "labels": [], "entities": []}, {"text": " Table 6: Accuracy on the simpler task of binary classification of  relation directionality. The most common relations are shown  first: the \"Rate\" column gives the average rate of the relation  across the 20 training languages (like the x coordinate in", "labels": [], "entities": []}, {"text": " Table 7: Accuracy on the simpler task of binary classification  of relation directionality for each training language. A detailed  comparison shows that EC is significantly worse than UD and  +GD, and that \u2205 is significantly worse than +GD (paired permu- tation test over the 20 languages, p < 0.05). The improvement  from UD to +GD is insignificant, which suggests that this is an  easier task where weak models might suffice.", "labels": [], "entities": [{"text": "EC", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9512410759925842}]}, {"text": " Table 8: Our final comparison on the 17 test languages appears  at left. We ask whether the average expected loss on these 17  real target languages is reduced by augmenting the training pool  of 20 UD languages with +20*21*21 GD languages. For com- pleteness, we extend the table with the cross-validation results  on the training pool. The \"Avg.\" lines report the average of 17  test or 37 training+testing languages. We mark both \"+GD\" av- erages with \"*\" as they are significantly better than their \"UD\"  counterparts (paired permutation test by language, p < 0.05).", "labels": [], "entities": []}]}