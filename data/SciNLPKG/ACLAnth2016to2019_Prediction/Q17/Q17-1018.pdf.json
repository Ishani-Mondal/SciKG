{"title": [{"text": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling", "labels": [], "entities": [{"text": "Sparse Coding of Neural Word Embeddings", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8563365836938223}, {"text": "Multilingual Sequence Labeling", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6771132051944733}]}], "abstractContent": [{"text": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6474922597408295}]}, {"text": "The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition fora variety of languages.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.739769458770752}, {"text": "named entity recognition", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6670835912227631}]}, {"text": "Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks.", "labels": [], "entities": []}, {"text": "The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.7640096545219421}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.837257444858551}]}], "introductionContent": [{"text": "Determining the linguistic structure of natural language texts based on rich hand-crafted features has a long-going history in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6489634613196055}]}, {"text": "The focus of traditional approaches has mostly been on building linguistic analyzers fora particular kind of analysis, which often leads to the incorporation of extensive linguistic and/or domain knowledge for defining the feature space.", "labels": [], "entities": []}, {"text": "Consequently, traditional models easily become language and/or task specific resulting in improper generalization properties.", "labels": [], "entities": []}, {"text": "A new research direction has emerged recently, that aims at building more general models that require far less feature engineering or none at all.", "labels": [], "entities": []}, {"text": "These advancements in natural language processing, pioneered by, followed by,, among others, employ a different philosophy.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.645570824543635}]}, {"text": "The objective of these works is to find representations for linguistic phenomena in an unsupervised manner by relying on large amounts of text.", "labels": [], "entities": []}, {"text": "Natural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words.", "labels": [], "entities": []}, {"text": "In our paper we empirically verify via rigorous experiments that turning these dense representations into a much sparser (yet denser than one-hot encoding) form can keep the most salient parts of word representations that are highly suitable for sequence models.", "labels": [], "entities": []}, {"text": "Furthermore, our experiments reveal that our proposed model performs substantially better than traditional feature-rich models in the absence of abundant training data.", "labels": [], "entities": []}, {"text": "Our proposed model also has the advantage of performing well on multiple sequence labeling tasks without any modification in the applied word representations thanks to the sparse features derived from continuous word representations.", "labels": [], "entities": []}, {"text": "Our work aims at introducing a novel sequence labeling model solely utilizing features derived from the sparse coding of continuous word embeddings.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6383205652236938}]}, {"text": "Even though sparse coding had previously been utilized in NLP prior to us, to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions: \u2022 We show that the proposed sparse representation is general as sequence labeling models trained on them achieve (near) state-of-the-art performances for both POS tagging and NER.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.6792376786470413}, {"text": "POS tagging", "start_pos": 373, "end_pos": 384, "type": "TASK", "confidence": 0.7778095602989197}]}, {"text": "\u2022 We show that the representation is general in the other sense, that it produces reasonable results for more than 40 treebanks for POS tagging, \u2022 rigorously compare different sparse coding approaches in conjunction with differently trained continuous word embeddings, \u2022 highlight the favorable generalization properties of our model in settings when access to a very limited training corpus is assumed, \u2022 release the sparse word representations determined for our experiments at https:// begab.github.io/sparse_embeds to ensure the replicability of our results and to foster further multilingual NLP research.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.8836714625358582}]}], "datasetContent": [{"text": "We rely on the SPArse Modeling Software 1 (SPAMS) () for performing sparse coding of distributed word representations.", "labels": [], "entities": [{"text": "SPArse Modeling Software 1 (SPAMS)", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.6415833575384957}]}, {"text": "For dictionary learning as formulated in Equation 1, one should choose m and \u03bb, controlling the number of the basis vectors and the regularization coefficient affecting the sparsity of \u03b1, respectively.", "labels": [], "entities": [{"text": "dictionary learning", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8437930941581726}]}, {"text": "Starting with m = 256 and doubling it at each iteration, our preliminary investigations showed a steady growth in the usefulness of sparse word representations as a function of m, plateauing at m = 1024.", "labels": [], "entities": []}, {"text": "We set m to that value for further experiments.", "labels": [], "entities": []}, {"text": "Even though it is reasonable to assume that languages share a common coarse set of linguistic categories, linguistic resources had their own notations for part-of-speech tags.", "labels": [], "entities": []}, {"text": "The first notable attempt to canonize the multiple tag sets was the Google universal part-of-speech tags introduced by in which the POS tags of various tagging schemes were mapped to 12 language-independent part-of-speech tags.", "labels": [], "entities": []}, {"text": "The recent initiative of universal dependencies (UD) aims to provide a unified notation for multiple linguistic phenomena, including part-of-speech tags as well.", "labels": [], "entities": [{"text": "universal dependencies (UD)", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6935053110122681}]}, {"text": "The POS tag set proposed for UD has 17 categories which partially overlap with those defined by.", "labels": [], "entities": [{"text": "POS tag set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7760517199834188}, {"text": "UD", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9044805765151978}]}, {"text": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 () shared tasks.", "labels": [], "entities": [{"text": "CoNLL-X format", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9140523076057434}, {"text": "CoNLL-2006/07 () shared tasks", "start_pos": 51, "end_pos": 80, "type": "DATASET", "confidence": 0.8693950374921163}]}, {"text": "The complete list of the treebanks included in our experiments is presented in.", "labels": [], "entities": []}, {"text": "We rely on the official scripts released by Petrov et al.", "labels": [], "entities": []}, {"text": "POS tags to the Google universal POS tags in order to obtain results comparable across languages.", "labels": [], "entities": []}, {"text": "For our experiments we used the original CoNLL-X train/test splits of the treebanks.", "labels": [], "entities": [{"text": "CoNLL-X train", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8089955449104309}]}, {"text": "A key factor for the efficiency of our proposed model resides in the coverage of word embeddings, i.e. the proportion of tokens/word forms for which distributed representation is determined.", "labels": [], "entities": []}, {"text": "depicts these coverage scores calculated over the merged training and test sets for the different languages.", "labels": [], "entities": []}, {"text": "reveals that a substantial amount of tokens has distributed representation defined for (around 90% for the majority of languages, except for Turkish where it is 5 point less).", "labels": [], "entities": []}, {"text": "Token coverages of the word embeddings are most likely affected by the morphological richness of the languages and the elaborateness of the corresponding Wikipedia articles used for training word embeddings.", "labels": [], "entities": [{"text": "Token coverages", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9040897786617279}]}, {"text": "Comparing word embeddings Our motivation for choosing polyglot word embeddings as input to sparse coding is that they are publicly available fora variety of languages.", "labels": [], "entities": []}, {"text": "However, distributed word representations trained in any other reasonable manner can serve as input to our approach.", "labels": [], "entities": []}, {"text": "In order to investigate if some of the popular word embedding techniques seem favorable for our algorithm, we conduct experiments using alternatively trained embeddings, i.e. skip-gram (SG), continuous bagof-words (CBOW) and Glove.", "labels": [], "entities": []}, {"text": "In order that the utility of different word embeddings not to be conflated with other factors, we train them on the same Wikipedia dumps used for training the polyglot word vectors.", "labels": [], "entities": []}, {"text": "We choose further hyperparameters identically to polyglot, i.e. we train 64 dimensional dense word representations using asymmetric context window of size 2 for both SG/CBOW 6 and Glove . demonstrates that POS tagging performance is quite insensitive to the choice of \u03bb unless it yields some extreme sparsity level (>99.5%).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 206, "end_pos": 217, "type": "TASK", "confidence": 0.8163127899169922}]}, {"text": "The average tagging performance over the 12 languages when relying on features based on polyglot SC is only 1.3 points below that of F R w+c (i.e. 94.4 versus 95.7).", "labels": [], "entities": [{"text": "tagging", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9598040580749512}]}, {"text": "Recall that F R w+c uses a feature-rich representation, whereas our proposed model uses only O(m) features, i.e. it is tied to the number of the basis vectors employed for sparse coding.", "labels": [], "entities": [{"text": "O", "start_pos": 93, "end_pos": 94, "type": "METRIC", "confidence": 0.9530171155929565}]}, {"text": "Furthermore, our model does not employ word identity features, nor does it rely on character-level features of words.", "labels": [], "entities": []}, {"text": "Analyzing the effects of window size Hyperparameters for training word representations can greatly impact their quality as also concluded by.", "labels": [], "entities": []}, {"text": "We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their performance in our model.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.813896656036377}]}, {"text": "According to applying context window sizes of 2 for training the word embeddings tend to Comparing dense and sparse representations Unless stated otherwise, we use \u03bb = 0.1 for the experiments below in accordance to.", "labels": [], "entities": []}, {"text": "Table 3 demonstrates that performances obtained by models using dense word representations as features are consistently inferior to those models relying on sparse word representations.", "labels": [], "entities": []}, {"text": "In, we can see that polyglot embeddings perform the best for dense representations as well.", "labels": [], "entities": []}, {"text": "When using dense features, the CBOW representation-based model tends to produce results better than by a 1.4 points margin on average compared to SG embeddings.", "labels": [], "entities": []}, {"text": "This performance gap between the two word2vec variants vanishes, however, when dense representations are replaced by their sparse counterparts.", "labels": [], "entities": []}, {"text": "Comparing the effects of training corpus size We also investigate the generalization characteristics of the proposed representation by training models that have access to substantially different amounts of training data per language.", "labels": [], "entities": []}, {"text": "We distinguish three scenarios, i.e. when using only the first 150, the first 1,500 and all the available training sentences from each corpus.", "labels": [], "entities": []}, {"text": "illustrates the average POS tagging accuracy over the 12 CoNLL-X datasets for different amounts of training data and models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7754962742328644}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9281123280525208}, {"text": "CoNLL-X datasets", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9624660015106201}]}, {"text": "The main difference in Eq.", "labels": [], "entities": []}, {"text": "1 and 3 is that the latter does not explicitly constrain D to be a member of the convex set of matrices comprising of column vectors having a pre-defined upper bound on their norm.", "labels": [], "entities": []}, {"text": "In order to implicitly control for the norms of the basis vectors for which a non-negativity constraint on the elements of \u03b1 (but no constraint on D) is imposed.", "labels": [], "entities": []}, {"text": "When using the objective functions introduced by , we use the default \u03c4 = 10 \u22125 value.", "labels": [], "entities": []}, {"text": "Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i \u2208 {1, 3, 4}.", "labels": [], "entities": []}, {"text": "We applied \u03bb = 0.05 for SC-1 and \u03bb = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf. the left of).", "labels": [], "entities": []}, {"text": "The right of further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.7708924114704132}, {"text": "CoNLL-X treebanks", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9466944932937622}]}, {"text": "Although mentions nonnegativity as a desired property of word representations for cognitive plausibility, reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.7 points below that of SC-3 approach.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 185, "end_pos": 196, "type": "TASK", "confidence": 0.6226413249969482}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.5625282526016235}]}, {"text": "The average performances when applying SC-1 and SC-3 are nearly identical with a 0.18 point difference between the two.", "labels": [], "entities": [{"text": "SC-1", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8707139492034912}]}, {"text": "It is instructive to analyze the patterns different sparse coding approaches exhibit.", "labels": [], "entities": []}, {"text": "Even though the objective functions used by the different approaches are similar, decompositions obtained by them convey rather different sparsity structures.", "labels": [], "entities": []}, {"text": "illustrates that there exist substantial variation in the length of the basis vectors obtained by SC-3 and SC-4 both within and across languages.", "labels": [], "entities": []}, {"text": "However, SC-1 produces practically no variation in the length of the basis vectors comprising D due to the constraint present in the objective function it employs.", "labels": [], "entities": []}, {"text": "shows similar differences about the relative frequency of basis vectors taking part in the reconstruction of word embeddings.", "labels": [], "entities": []}, {"text": "shows a strong correlation between the 2 norm of basis vectors and the relative number of times a non-zero coefficient is assigned to them in \u03b1 for SC-3 and SC-4 but not for SC-1.", "labels": [], "entities": []}, {"text": "It can be further noted from  of the basis vectors determined by SC-3 and SC-4 are often orders of magnitude larger than those determined by SC-1.", "labels": [], "entities": []}, {"text": "This effect, however, can be naturally mitigated by increasing \u03c4 . Overall, the different approaches convey comparable POS tagging accuracies but different decompositions due to the differences in the objective functions they employ.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.8128688633441925}]}, {"text": "Experiments described below are conducted using the objective function in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 74, "end_pos": 76, "type": "DATASET", "confidence": 0.8859241008758545}]}, {"text": "1.  For POS tagging we also experiment with UD v1. treebanks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.8995916247367859}, {"text": "UD v1. treebanks", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.7201195508241653}]}, {"text": "We used the default train-test splits of the treebanks not utilizing the development sets for fine tuning performance on any of the languages during our experiments.", "labels": [], "entities": []}, {"text": "We omitted the Japanese treebank as words in it are stripped off due to licensing issues.", "labels": [], "entities": [{"text": "Japanese treebank", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.9284733235836029}]}, {"text": "Also there is no polyglot vector released for Old Church Slavonic and Gothic.", "labels": [], "entities": []}, {"text": "Even though polyglot word representations are released for Arabic, it was of no practical use as it contained unvocalized surface forms of tokens in contrast to the vocalized forms in UD v.1.2.", "labels": [], "entities": []}, {"text": "For this reason, we discarded the Arabic treebank as less than 30% of its tokens could be associated with a representation.", "labels": [], "entities": [{"text": "Arabic treebank", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8833992183208466}]}, {"text": "By omitting these 4 languages from our experiments we are finally left with 33 treebanks for 29 languages.", "labels": [], "entities": []}, {"text": "We note that for Ancient Greek treebanks (grc*) we use word embeddings trained on Modern Greek.", "labels": [], "entities": []}, {"text": "We should add that there are 4 languages (related to 6 treebanks) for which polyglot word vectors are accessible, however, the Wikipedia dumps used for training them are not distributed.", "labels": [], "entities": []}, {"text": "For this reason, Brown clustering-based baselines are missing for the affected treebanks.", "labels": [], "entities": []}, {"text": "We report our results on UD v1.2 in.", "labels": [], "entities": [{"text": "UD v1.2", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.7396497428417206}]}, {"text": "Recall that the default behavior of our sparse codingbased models (SC in) is that they do not handle word identity as an explicit feature.", "labels": [], "entities": []}, {"text": "We now investigate how much contribution word identity features convey on their own and also when used in conjunction with sparse coding-derived features.", "labels": [], "entities": []}, {"text": "For this end we introduce a simple linear chain CRF model generating features solely on the identity of the current word and the ones surrounding it (WI in).", "labels": [], "entities": []}, {"text": "Likewise, we define a model that relies on WI and SC features simultaneously (WI+SC).", "labels": [], "entities": []}, {"text": "Table 5 reveals that SC outperforms WI by a large margin and that combining the two feature sets together yields some further improvements over SC scores.", "labels": [], "entities": [{"text": "SC", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.8602741360664368}]}, {"text": "We also present in the state-of-the-art results of the bidirectional LSTM models by for comparative purposes.", "labels": [], "entities": []}, {"text": "Note that the authors reported results only on a subset of UD v1.2 (i.e. treebanks with at least 60k tokens), for which reason we can include their results on 21 treebanks.", "labels": [], "entities": []}, {"text": "Out of these 21 UD v1.2 treebanks there are 15 and 20 cases, respectively, for which SC and WI+SC produces better results than bi-LSTM w . Only FR w+c and bi-LSTM w+c , models which enjoy the additional benefit of employing character-level features besides word-level ones, are capable of outperforming SC and WI+SC.", "labels": [], "entities": []}, {"text": "Besides the POS tagging experiments, we investigated if the very same features as the ones applied for POS tagging can be utilized in a different sequence labeling task, namely named entity recognition.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.760729044675827}, {"text": "POS tagging", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.8150173127651215}, {"text": "named entity recognition", "start_pos": 177, "end_pos": 201, "type": "TASK", "confidence": 0.6095538139343262}]}, {"text": "In order to evaluate our approach, we obtained the English, Spanish and Dutch datasets from the 2002 and 2003 CoNLL shared tasks on multilingual Named Entity Recognition.", "labels": [], "entities": [{"text": "multilingual Named Entity Recognition", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.5333966165781021}]}, {"text": "We use the train-test splits provided by the or- includes our NER results obtained using different word embedding representations as input for sparse coding and different levels of sparsity.", "labels": [], "entities": []}, {"text": "Similar to our POS tagging experiments, using polyglot SC vectors tend to perform best for NER as well.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.7675399482250214}, {"text": "NER", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9536511301994324}]}, {"text": "However, a substantial difference compared to the POS tagging results is that NER performances   do not degrade even for extreme levels of sparsity.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.7104480564594269}, {"text": "NER", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9330858588218689}]}, {"text": "Also, the sparse coding-based models perform much better when compared to the FR w+c baseline.", "labels": [], "entities": [{"text": "FR w+c baseline", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.8405659675598145}]}, {"text": "In, we compare the effectiveness of models relying on sparse and dense word representations for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9121107459068298}]}, {"text": "In order not to fine-tune hyperparameters fora particular experiment, similarly to our previous choices m and \u03bb are set to 1024 and 0.1, respectively.", "labels": [], "entities": []}, {"text": "Results in are inline with those reported in for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8452233076095581}]}], "tableCaptions": [{"text": " Table 2: Treebanks used for POS tagging experiments  from the CoNLL 2006/07 shared task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8886070251464844}, {"text": "CoNLL 2006/07 shared task", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.9571910699208578}]}, {"text": " Table 3: Performances of sparse and dense word representations for POS tagging over the 12 CoNLL-X datasets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.8587453365325928}, {"text": "CoNLL-X datasets", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9437719881534576}]}, {"text": " Table 4: Comparison of models based on different amount of training data. Bold numbers indicate the best results for a  given training regime (i.e. either training on 150/1,500/all training sentences). polyglot SC uses m = 1024, \u03bb = 0.1.", "labels": [], "entities": []}, {"text": " Table 5: Per token POS tagging accuracies for 33 UD treebanks. For sparse coding SPAMS is used on polyglot  vectors with \u03bb = 0.1 and m = 1024. Results in bold are better than any of bi-LSTM w , FR w and Brown models  (i.e. the baselines using features based on words only). Average is calculated over the 20 highlighted treebanks for  which there are results in every column. The bi-LSTM results are from Plank et al. (2016).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.5628025978803635}]}, {"text": " Table 6: Comparison of the performance of sparse and  dense word representations for NER.", "labels": [], "entities": []}]}