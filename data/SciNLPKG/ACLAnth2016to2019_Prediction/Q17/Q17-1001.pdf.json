{"title": [{"text": "Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels", "labels": [], "entities": [{"text": "Evaluating Visual Representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8628884553909302}, {"text": "Topic Understanding", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8337970674037933}]}], "abstractContent": [{"text": "Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes.", "labels": [], "entities": [{"text": "indexing, summarizing, and analyzing large document collections", "start_pos": 51, "end_pos": 114, "type": "TASK", "confidence": 0.6945479909578959}]}, {"text": "However, promoting end-user understanding of topics remains an open research problem.", "labels": [], "entities": []}, {"text": "We compare labels generated by users given four topic visualization techniques-word lists, word lists with bars, word clouds, and network graphs-against each other and against automatically generated labels.", "labels": [], "entities": []}, {"text": "Our basis of comparison is participant ratings of how well labels describe documents from the topic.", "labels": [], "entities": []}, {"text": "Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics' documents.", "labels": [], "entities": []}, {"text": "Although all visual-izations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualiza-tions obscure.", "labels": [], "entities": []}, {"text": "Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms .", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 188, "end_pos": 202, "type": "TASK", "confidence": 0.6797570139169693}]}], "introductionContent": [], "datasetContent": [{"text": "We select a corpus that does not assume domain expertise: 7, 156 New York Times articles from January 2007.", "labels": [], "entities": []}, {"text": "We model the corpus using an LDA () implementation in Mallet () with domain-specific stopwords and standard hyperparameter settings.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9660506248474121}]}, {"text": "Our simple setup is by design: our goal is to emulate the \"off the shelf\" behavior of conventional topic modeling tools used by novice users.", "labels": [], "entities": []}, {"text": "Instead of improving the quality of the model using asymmetric priors () or bigrams ), our topic model has topics of variable quality, allowing us to explore the relationship between topic quality and our task measures.", "labels": [], "entities": []}, {"text": "Automatic labels are generated from representative Wikipedia article titles using a technique similar to.", "labels": [], "entities": []}, {"text": "We first index Wikipedia using Apache Lucene.", "labels": [], "entities": []}, {"text": "To label a topic, we query Wikipedia with the top twenty topic words to retrieve fifty articles.", "labels": [], "entities": []}, {"text": "These articles' titles comprise our candidate set of labels.", "labels": [], "entities": []}, {"text": "We then represent each article using its TF-IDF vector and calculate the centroid (average TF-IDF) of the retrieved articles.", "labels": [], "entities": []}, {"text": "To rank and choose the most representative of the set, we calculate the cosine similarity between the centroid TF-IDF vector and the TF-IDF vector of each of the articles.", "labels": [], "entities": []}, {"text": "We choose the title of the article with the maximum cosine similarity to the centroid.", "labels": [], "entities": []}, {"text": "Unlike Lau et al., we do not include the topic words or Wikipedia title n-grams derived from our label set, as these labels are typically not the best candidates.", "labels": [], "entities": []}, {"text": "Although other automatic labeling techniques exist, we choose this one as it is representative of general techniques.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the labeling phase: number of tasks completed, the average and standard deviation (in paren- theses) for time spent per task in seconds, and the average and standard deviation for self-reported confidence on a  5-point Likert scale for each of the twelve conditions.", "labels": [], "entities": [{"text": "standard deviation (in paren- theses)", "start_pos": 85, "end_pos": 122, "type": "METRIC", "confidence": 0.8210801780223846}]}]}