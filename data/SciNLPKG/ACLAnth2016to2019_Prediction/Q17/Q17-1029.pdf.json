{"title": [], "abstractContent": [{"text": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing.", "labels": [], "entities": [{"text": "neural transition-based constituent parsing", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.6110238060355186}]}, {"text": "The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively.", "labels": [], "entities": []}, {"text": "Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its construction.", "labels": [], "entities": []}, {"text": "To mitigate both issues, we propose a novel parsing system based on in-order traversal over syntactic trees, designing a set of transition actions to find a compromise between bottom-up constituent information and top-down lookahead information.", "labels": [], "entities": []}, {"text": "Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F 1 on the WSJ benchmark.", "labels": [], "entities": [{"text": "psycholinguistically motivated constituent parsing", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.6462491303682327}, {"text": "91.8 F 1", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.7957374453544617}, {"text": "WSJ benchmark", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9629944562911987}]}, {"text": "Furthermore, the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking, which are the best results on the WSJ benchmark.", "labels": [], "entities": [{"text": "F 1", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9786612093448639}, {"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9608397483825684}, {"text": "WSJ benchmark", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.9474007487297058}]}], "introductionContent": [{"text": "Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences.", "labels": [], "entities": [{"text": "Transition-based constituent parsing", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6596162915229797}]}, {"text": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing and top-down parsing).", "labels": [], "entities": []}, {"text": "The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree.", "labels": [], "entities": []}, {"text": "The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree.", "labels": [], "entities": [{"text": "bottom-up parsing", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.5432305932044983}]}, {"text": "For example, given the sentence in, a bottom-up shift-reduce parser takes the action sequence in(a) to build the output, where the word sequence \"The little boy\" is first read, and then an NP recognized for the word sequence.", "labels": [], "entities": []}, {"text": "After the system reads the verb \"likes\" and its subsequent NP, a VP is recognized.", "labels": [], "entities": []}, {"text": "The full order of recognition for the tree nodes is . When making local decisions, rich information is available from readily built partial trees (, which contributes to local disambiguation.", "labels": [], "entities": []}, {"text": "However, there is lack of top-down guidance from lookahead information, which can be useful.", "labels": [], "entities": []}, {"text": "In addition, binarization must be applied to trees, as shown in(b), to ensure a constant number of actions (), and to take advantage of lexical head information.", "labels": [], "entities": []}, {"text": "However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages.", "labels": [], "entities": []}, {"text": "On the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree.", "labels": [], "entities": []}, {"text": "Given the sentence in, a top-down shift-reduce parser takes the action sequence in(b) to build the output, where an S is first made and then an NP is generated.", "labels": [], "entities": []}, {"text": "After that, the system makes a decision to read the word sequence \"The little boy\" to complete the NP.", "labels": [], "entities": [{"text": "NP", "start_pos": 99, "end_pos": 101, "type": "DATASET", "confidence": 0.5982559323310852}]}, {"text": "The full order of recognition for the tree nodes is . The top-down lookahead guidance contributes to non-local disambiguation.", "labels": [], "entities": []}, {"text": "However, it is difficult to generate a constituent before its sub constituents have been realized, since no explicit features can be extracted from their subtree structures.", "labels": [], "entities": []}, {"text": "Thanks to the use of recurrent neural networks, which make it possible to represent a sentence globally before syntactic tree construction, seminal work of neural top-down parsing directly generates bracketed constituent trees using sequence-to-sequence models ( (c) in-order system: Action sequences of three types of transition constituent parsing system.", "labels": [], "entities": [{"text": "transition constituent parsing", "start_pos": 319, "end_pos": 349, "type": "TASK", "confidence": 0.7570167779922485}]}, {"text": "Details of the action system are introduced in Section 2.1, Section 2.2 and Section 3, respectively.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel transition system for constituent parsing, mitigating issues of both bottom-up and top-down systems by finding a compromise between bottom-up constituent information and top-down lookahead information.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8169625401496887}]}, {"text": "The process of the proposed constituent parsing can be regarded as in-order traversal over a tree.", "labels": [], "entities": []}, {"text": "Given the sentence in, the system takes the action sequence in(c) to build the output.", "labels": [], "entities": []}, {"text": "The system reads the word \"The\" and then projects an NP, which is based on bottom-up evidence.", "labels": [], "entities": [{"text": "NP", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9557111859321594}]}, {"text": "After this, based on the projected NP, the system reads the word sequence \"little boy\", with top-down guidance from NP.", "labels": [], "entities": []}, {"text": "Similarly, based on the completed constituent \"(NP The little boy)\", the system projects an S, with the bottom-up evidence.", "labels": [], "entities": []}, {"text": "With the Sand the word \"likes\", the system projects an VP, which can serve as top-down guidance.", "labels": [], "entities": []}, {"text": "The full order of recognition for the tree nodes is . Compared to post-order traversal, in-order traversal can potentially resolve non-local ambiguity better by top-down guidance.", "labels": [], "entities": []}, {"text": "Compared to pre-order traversal, in-order traversal can potentially resolve local ambiguity better by bottom-up evidence.", "labels": [], "entities": []}, {"text": "Furthermore, in-order traversal is psycholinguistically motivated).", "labels": [], "entities": []}, {"text": "Empirically, a human reader comprehends sentences by giving lookahead guesses for constituents.", "labels": [], "entities": []}, {"text": "For example, when reading a word \"likes\", a human reader could guess that it could be a start of a constituent VP, instead of waiting to read the object \"red tomatoes\", which is the procedure of a bottom-up system.", "labels": [], "entities": []}, {"text": "We compare our system with the two baseline systems (i.e. a top-down system and a bottomup system) under the same neural transition-based framework of . Our final models outperform both of the bottom-up and top-down transition-based constituent parsing by achieving a 91.8 F 1 in English and a 86.1 F 1 in Chinese for greedy fully-supervised parsing, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 273, "end_pos": 276, "type": "METRIC", "confidence": 0.9694636464118958}, {"text": "F 1", "start_pos": 299, "end_pos": 302, "type": "METRIC", "confidence": 0.9495038092136383}]}, {"text": "Furthermore, our final model obtains a 93.6 F 1 with supervised reranking) and a 94.2 F 1 with semi-supervised reranking, achieving the state-of-the-art results on constituent parsing on the English benchmark.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.6256963610649109}, {"text": "English benchmark", "start_pos": 191, "end_pos": 208, "type": "DATASET", "confidence": 0.9043244123458862}]}, {"text": "By converting to Stanford dependencies, our final model achieves the state-ofthe-art results on dependency parsing by obtaining a 96.2% UAS and a 95.2% LAS.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8269784152507782}, {"text": "UAS", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9921796321868896}, {"text": "LAS", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9952280521392822}]}, {"text": "To our knowledge, we are the first to systematically compare top-down and bottom-up constituent parsing under the same neural framework.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7244724929332733}]}, {"text": "We release our code at https://github.com/LeonCrashCode/ InOrderParser.", "labels": [], "entities": [{"text": "LeonCrashCode", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9566584825515747}]}], "datasetContent": [{"text": "Following the same reranking setting of  and, we obtain 100 samples from our bottom-up, top-down, and in-order model (Sec- shows the development results of the three parsing systems.", "labels": [], "entities": []}, {"text": "The bottom-up system performs slightly better than the top-down system.", "labels": [], "entities": []}, {"text": "The inorder system outperforms both the bottom-up and the top-down system.", "labels": [], "entities": []}, {"text": "shows the parsing results on the English test dataset.", "labels": [], "entities": [{"text": "English test dataset", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9103890061378479}]}, {"text": "We find that the bottom-up parser and the top-down parser have similar results under the greedy setting, and the in-order parser outperforms both of them.", "labels": [], "entities": []}, {"text": "Also, with supervised reranking, the in-order parser achieves the best results.", "labels": [], "entities": []}, {"text": "English constituent results We compare our models with previous work, as shown in Table 4.", "labels": [], "entities": []}, {"text": "With the fully-supervise setting , the inorder parser outperforms the state-of-the-art discrete parser (, the state-of-the-art neural parsers (Cross and Huang, Here, we only consider the work of a single model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Development results (%) on WSJ 22.", "labels": [], "entities": [{"text": "Development", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9427595734596252}, {"text": "WSJ 22", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9647520184516907}]}, {"text": " Table 4: Final results (%) on WSJ Section 23.", "labels": [], "entities": [{"text": "WSJ Section 23", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9132354855537415}]}, {"text": " Table 5: Stanford Dependency accuracy (%) on  WSJ Section 23.  \u2020 means graph-based parsing. \"-re\"  means fully-supervised reranking and \"-sre\" means  semi-supervised reranking.", "labels": [], "entities": [{"text": "Stanford", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.6787522435188293}, {"text": "Dependency", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.503516435623169}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6388941407203674}, {"text": "WSJ Section 23", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.962139626344045}]}, {"text": " Table 6: Final results on test set of CTB.", "labels": [], "entities": [{"text": "CTB", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6554508805274963}]}, {"text": " Table 7: Dependency accuracy (%) on CTB test set.   \u2020 means graph-based parsing. \"-re\" means super- vised reranking.", "labels": [], "entities": [{"text": "Dependency", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9145992994308472}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7283567786216736}, {"text": "CTB test set", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9588111837704977}]}, {"text": " Table 8: Comparison on different phrases types.", "labels": [], "entities": []}]}