{"title": [{"text": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation", "start_pos": 9, "end_pos": 48, "type": "TASK", "confidence": 0.6275445222854614}, {"text": "Enabling Zero-Shot Translation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6074841817220052}]}], "abstractContent": [{"text": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.8380309243996938}]}, {"text": "Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language.", "labels": [], "entities": []}, {"text": "Using a shared wordpiece vocabulary , our approach enables Multilingual NMT systems using a single model.", "labels": [], "entities": []}, {"text": "On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English\u2192French and surpasses state-of-the-art results for English\u2192German.", "labels": [], "entities": [{"text": "WMT'14 benchmarks", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9305864572525024}]}, {"text": "Similarly, a single multilingual model surpasses state-of-the-art results for French\u2192English and German\u2192English on WMT'14 and WMT'15 benchmarks, respectively.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.9684630632400513}, {"text": "WMT'15 benchmarks", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.8853423893451691}]}, {"text": "On production corpora , multilingual models of up to twelve language pairs allow for better translation of many individual pairs.", "labels": [], "entities": []}, {"text": "Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training , showing that transfer learning and zero-shot translation is possible for neural translation.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.6884768456220627}, {"text": "neural translation", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7413285672664642}]}, {"text": "Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "End-to-end Neural Machine Translation (NMT)) is an approach to machine translation that has rapidly gained adoption in many large-scale settings (.", "labels": [], "entities": [{"text": "End-to-end Neural Machine Translation (NMT))", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7413393088749477}, {"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7690364420413971}]}, {"text": "Almost all such systems are built fora single language pair -so far there has not been a sufficiently simple and efficient way to handle multiple language pairs using a single model without making significant changes to the basic NMT architecture.", "labels": [], "entities": []}, {"text": "In this paper we introduce a simple method to translate between multiple languages using a single model, taking advantage of multilingual data to improve NMT for all languages involved.", "labels": [], "entities": []}, {"text": "Our method requires no change to the traditional NMT model architecture.", "labels": [], "entities": []}, {"text": "Instead, we add an artificial token to the input sequence to indicate the required target language, a simple amendment to the data only.", "labels": [], "entities": []}, {"text": "All other parts of the system -encoder, decoder, attention, and shared wordpiece vocabulary as described in -stay exactly the same.", "labels": [], "entities": []}, {"text": "This method has several attractive benefits: \u2022 Simplicity: Since no changes are made to the architecture of the model, scaling to more languages is trivial -any new data is simply added, possibly with over-or under-sampling such that all languages are appropriately represented, and used with anew token if the target language changes.", "labels": [], "entities": []}, {"text": "Since no changes are made to the training procedure, the mini-batches for training are just sampled from the overall mixed-language training data just like for the single-language case.", "labels": [], "entities": []}, {"text": "Since no a-priori decisions about how to allocate parameters for different languages are made, the system adapts automatically to use the total number of param-eters efficiently to minimize the global loss.", "labels": [], "entities": []}, {"text": "A multilingual model architecture of this type also simplifies production deployment significantly since it can cut down the total number of models necessary when dealing with multiple languages.", "labels": [], "entities": []}, {"text": "Note that at Google, we support a total of over 100 languages as source and target, so theoretically 100 2 models would be necessary for the best possible translations between all pairs, if each model could only support a single language pair.", "labels": [], "entities": []}, {"text": "Clearly this would be problematic in a production environment.", "labels": [], "entities": []}, {"text": "Even when limiting to translating to/from English only, we still need over 200 models.", "labels": [], "entities": []}, {"text": "Finally, batching together many requests from potentially different source and target languages can significantly improve efficiency of the serving system.", "labels": [], "entities": []}, {"text": "In comparison, an alternative system that requires language-dependent encoders, decoders or attention modules does not have any of the above advantages.", "labels": [], "entities": []}, {"text": "\u2022 Low-resource language improvements: Ina multilingual NMT model, all parameters are implicitly shared by all the language pairs being modeled.", "labels": [], "entities": []}, {"text": "This forces the model to generalize across language boundaries during training.", "labels": [], "entities": []}, {"text": "It is observed that when language pairs with little available data and language pairs with abundant data are mixed into a single model, translation quality on the low resource language pair is significantly improved.", "labels": [], "entities": [{"text": "translation", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.9602404832839966}]}, {"text": "\u2022 Zero-shot translation: A surprising benefit of modeling several language pairs in a single model is that the model can learn to translate between language pairs it has never seen in this combination during training (zero-shot translation) -a working example of transfer learning within neural translation models.", "labels": [], "entities": [{"text": "Zero-shot translation", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.7323956787586212}, {"text": "zero-shot translation", "start_pos": 218, "end_pos": 239, "type": "TASK", "confidence": 0.7412766218185425}]}, {"text": "For example, a multilingual NMT model trained with Portuguese\u2192English and English\u2192Spanish examples can generate reasonable translations for Portuguese\u2192Spanish although it has not seen any data for that language pair.", "labels": [], "entities": []}, {"text": "We show that the quality of zero-shot language pairs can easily be improved with little additional data of the language pair in question (a fact that has been previously confirmed fora related approach which is discussed in more detail in the next section).", "labels": [], "entities": []}, {"text": "In the remaining sections of this paper we first discuss related work and explain our multilingual system architecture in more detail.", "labels": [], "entities": []}, {"text": "Then, we go through the different ways of merging languages on the source and target side in increasing difficulty (many-to-one, one-to-many, many-to-many), and discuss the results of a number of experiments on WMT benchmarks, as well as on some of Google's large-scale production datasets.", "labels": [], "entities": []}, {"text": "We present results from transfer learning experiments and show how implicitly-learned bridging (zero-shot translation) performs in comparison to explicit bridging (i.e., first translating to a common language like English and then translating from that common language into the desired target language) as typically used in machine translation systems.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.7593488395214081}, {"text": "machine translation", "start_pos": 324, "end_pos": 343, "type": "TASK", "confidence": 0.7089695334434509}]}, {"text": "We describe visualizations of the new system inaction, which provide early evidence of shared semantic representations (interlingua) between languages.", "labels": [], "entities": []}, {"text": "Finally we also show some interesting applications of mixing languages with examples: code-switching on the source side and weighted target language mixing, and suggest possible avenues for further exploration.", "labels": [], "entities": [{"text": "weighted target language mixing", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.6123747229576111}]}], "datasetContent": [{"text": "In this section, we apply our proposed method to train multilingual models in several different configurations.", "labels": [], "entities": []}, {"text": "Since we can have models with either single or multiple source/target languages we test three interesting cases for mapping languages: 1) many to one, 2) one to many, and 3) many to many.", "labels": [], "entities": []}, {"text": "As already discussed in Section 2, other models have been used to explore some of these cases already, but for completeness we apply our technique to these interesting use cases again to give a full picture of the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "We will also show results and discuss benefits of bringing together many (un)related languages in a single large-scale model trained on production data.", "labels": [], "entities": []}, {"text": "Finally, we will present our findings on zero-shot translation where the model learns to translate between pairs of languages for which no explicit parallel examples existed in the training data, and show results of experiments where adding additional data improves zero-shot translation quality further.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8767762780189514}]}, {"text": "For WMT, we train our models on the WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8138927817344666}, {"text": "WMT", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9760969281196594}]}, {"text": "The combination of newstest2012 and newstest2013 is used as the development set.", "labels": [], "entities": []}, {"text": "In addition to WMT, we also evaluate the multilingual approach on some Google-internal large-scale production datasets representing a wide spectrum of languages with very distinct linguistic properties: En\u2194Japanese(Ja), En\u2194Korean(Ko), En\u2194Es, and En\u2194Pt.", "labels": [], "entities": [{"text": "WMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8323952555656433}]}, {"text": "These datasets are two to three orders of magnitude larger than the WMT datasets.", "labels": [], "entities": [{"text": "WMT datasets", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9485357403755188}]}, {"text": "Our training protocols are mostly identical to those described in.", "labels": [], "entities": []}, {"text": "We find that some multilingual models take a little more time to train than single language pair models, likely because each language pair is seen only fora fraction of the training process.", "labels": [], "entities": []}, {"text": "We use larger batch sizes with a slightly higher initial learning rate to speedup the convergence of these models.", "labels": [], "entities": []}, {"text": "We evaluate our models using the standard BLEU score metric and to make our results comparable to previous work, we report tokenized BLEU score as computed by the multi-bleu.pl script, which can be downloaded from the public implementation of Moses.", "labels": [], "entities": [{"text": "BLEU score metric", "start_pos": 42, "end_pos": 59, "type": "METRIC", "confidence": 0.9732118248939514}, {"text": "BLEU score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9809623062610626}]}, {"text": "To test the influence of varying amounts of training data per language pair we explore two strategies when building multilingual models: a) where we oversample the data from all language pairs to be of the same size as the largest language pair, and b) where we mix the data as is without any change.", "labels": [], "entities": []}, {"text": "The wordpiece model training is done after the optional oversampling taking into account all the changed data ratios.", "labels": [], "entities": []}, {"text": "For the WMT models we report results using both of these strategies.", "labels": [], "entities": [{"text": "WMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.7394369840621948}]}, {"text": "For the production models, we always balance the data such that the ratios are equal.", "labels": [], "entities": []}, {"text": "One benefit of the way we share all the components of the model is that the mini-batches can contain data from different language pairs during training and inference, which are typically just random samples from the final training and test data distributions.", "labels": [], "entities": []}, {"text": "This is a simple way of preventing \"catastrophic forgetting\" -tendency for knowledge of previously learned task(s) (e.g. language pair A) to be abruptly forgotten as information relevant to the current task (e.g. language pair B) is incorporated.", "labels": [], "entities": []}, {"text": "Other approaches to multilingual translation require complex update scheduling mechanisms to prevent this effect).", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.7304219305515289}]}, {"text": "This section shows the result of combining 12 production language pairs having a total of 3B parameters (255M per single model) into a single multilingual.", "labels": [], "entities": []}, {"text": "We find that the multilingual models are on average worse than the single models (about 5.6% to 2.5% relative depending on size, however, some actually get better) and as expected the average difference gets smaller when going to larger multilingual models.", "labels": [], "entities": []}, {"text": "It should be noted that the largest multilingual model we have trained has still about five times less parameters than the combined single models.", "labels": [], "entities": []}, {"text": "The multilingual model also requires only roughly 1/12-th of the training time (or computing resources) to converge compared to the combined single models (total training time for all our models is still in the order of weeks).", "labels": [], "entities": []}, {"text": "Another important point is that since we only train fora little longer than a standard single model, the individual language pairs can see as little as 1/12-th of the data in comparison to their single language pair models but still produce satisfactory results.", "labels": [], "entities": []}, {"text": "In summary, multilingual NMT enables us to group languages with little loss in quality while having the benefits of better training efficiency, smaller number of models, and easier productionization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Many to One: BLEU scores on for single lan- guage pair and multilingual models. : no oversampling", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9989928603172302}]}, {"text": " Table 1. For all ex- periments the multilingual models outperform the  baseline single systems despite the above mentioned  disadvantage with respect to the number of param- eters available per language pair. One possible hy- pothesis explaining the gains is that the model has  been shown more English data on the target side,  and that the source languages belong to the same  language families, so the model has learned useful  generalizations.", "labels": [], "entities": []}, {"text": " Table 2: One to Many: BLEU scores for single language  pair and multilingual models. : no oversampling", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9989961981773376}]}, {"text": " Table 3: Many to Many: BLEU scores for single language  pair and multilingual models. : no oversampling", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9989663362503052}]}, {"text": " Table 4. We find that  the multilingual models are on average worse than  the single models (about 5.6% to 2.5% relative de- pending on size, however, some actually get better)  and as expected the average difference gets smaller  when going to larger multilingual models. It should  be noted that the largest multilingual model we have  trained has still about five times less parameters than  the combined single models.", "labels": [], "entities": []}, {"text": " Table 4: Large-scale experiments: BLEU scores for single  language pair and multilingual models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9983969330787659}]}, {"text": " Table 6: Spanish\u2192Japanese BLEU scores for explicit and  implicit bridging using the 12-language pair large-scale  model from Table 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9947847723960876}]}, {"text": " Table 7: BLEU scores for English\u2194{Belarusian, Russian,  Ukrainian} models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987431168556213}]}, {"text": " Table 8: Gradually mixing target languages Ja/Ko.", "labels": [], "entities": []}]}