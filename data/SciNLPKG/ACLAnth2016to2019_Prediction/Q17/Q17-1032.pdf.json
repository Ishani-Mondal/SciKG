{"title": [{"text": "Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition in an n-gram Lattice", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew model for acquiring comprehensive multiword lexicons from large corpora based on competition among n-gram candidates.", "labels": [], "entities": []}, {"text": "In contrast to the standard approach of simple ranking by association measure , in our model n-grams are arranged in a lattice structure based on subsumption and overlap relationships, with nodes inhibiting other nodes in their vicinity when they are selected as a lexical item.", "labels": [], "entities": []}, {"text": "We show how the configuration of such a lattice can be optimized tractably, and demonstrate using annotations of sampled n-grams that our method consistently outperforms alternatives by at least 0.05 F-score across several corpora and languages.", "labels": [], "entities": [{"text": "F-score", "start_pos": 200, "end_pos": 207, "type": "METRIC", "confidence": 0.9962233304977417}]}], "introductionContent": [{"text": "Despite over 25 years of research in computational linguistics aimed at acquiring multiword lexicons using corpora statistics, and growing evidence that speakers process language primarily in terms of memorized sequences, the individual word nonetheless stubbornly remains the de facto standard processing unit for most research in modern NLP.", "labels": [], "entities": []}, {"text": "The potential of multiword knowledge to improve both the automatic processing of language as well as offer new understanding of human acquisition and usage of language is the primary motivator of this work.", "labels": [], "entities": []}, {"text": "Here, we present an effective, expandable, and tractable new approach to comprehensive multiword lexicon acquisition.", "labels": [], "entities": [{"text": "multiword lexicon acquisition", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.7634583115577698}]}, {"text": "Our aim is to find a middle ground between standard MWE acquisition approaches based on association measures) and more sophisticated statistical models) that do not scale to large corpora, the main source of the distributional information in modern NLP systems.", "labels": [], "entities": [{"text": "MWE acquisition", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.9745595455169678}]}, {"text": "A central challenge in building comprehensive multiword lexicons is paring down the huge space of possibilities without imposing restrictions which disregard a major portion of the multiword vocabulary of a language: allowing for diversity creates significant redundancy among statistically promising candidates.", "labels": [], "entities": []}, {"text": "The lattice model proposed here addresses this primarily by having the candidatescontiguous and non-contiguous n-gram typescompete with each other based on subsumption and overlap relations to be selected as the best (i.e., most parsimonious) explanation for statistical irregularities.", "labels": [], "entities": []}, {"text": "We test this approach across four large corpora in three languages, including two relatively freeword-order languages (Croatian and Japanese), and find that this approach consistency outperforms alternatives, offering scalability and many avenues for future enhancement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity.", "labels": [], "entities": []}, {"text": "In English, we follow in using a 890M token filtered portion of the ICWSM blog corpus () tagged with the Tree Tagger (.", "labels": [], "entities": [{"text": "ICWSM blog corpus", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9314627647399902}]}, {"text": "To facilitate a comparison with, which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus), using the standard CLAWS-derived POS tags for the corpus.", "labels": [], "entities": [{"text": "ICWSM", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9551761150360107}, {"text": "British National Corpus", "start_pos": 137, "end_pos": 160, "type": "DATASET", "confidence": 0.9537511269251505}, {"text": "CLAWS-derived POS tags", "start_pos": 182, "end_pos": 204, "type": "DATASET", "confidence": 0.8246654272079468}]}, {"text": "Lemmatization included removing all inflectional marking from both words and POS tags.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.7690014839172363}]}, {"text": "For English, gaps are identified using the same POS regex used in, which includes simple nouns and portions thereof, up to a maximum of 4 words.", "labels": [], "entities": []}, {"text": "The other two languages we include in our evaluation are Croatian and Japanese.", "labels": [], "entities": []}, {"text": "Relative to English, both languages have freer word order: we were interested in probing the challenges associated with using an n-gram approach to FS identification in such languages.", "labels": [], "entities": [{"text": "FS identification", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.9889406561851501}]}, {"text": "For Croatian, we used the 1.2-billion-token fhrWaC corpus), a filtered version of the Croatian web corpus hrWaC, which is POS-tagged and lemmatized using the tools of Agi\u00b4c.", "labels": [], "entities": [{"text": "fhrWaC corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.7736035585403442}, {"text": "Agi\u00b4c", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.9152706861495972}]}, {"text": "Similar to English, the POS regex for Croatian includes simple nouns, adjectives and pronouns, but also other elements that regularly appear inside FS, including both adverbs and copulas.", "labels": [], "entities": []}, {"text": "For Japanese, we used a subset of the 100M-page web corpus of, which was roughly the same token length as the English corpus.", "labels": [], "entities": []}, {"text": "We segmented and POS-tagged the corpus with MeCab () using the UNIDIC morphological dictionary).", "labels": [], "entities": [{"text": "UNIDIC morphological dictionary", "start_pos": 63, "end_pos": 94, "type": "DATASET", "confidence": 0.839132825533549}]}, {"text": "The POS regex for Japanese covers the same basic nominal structures as English, but also includes case markers and adverbials.", "labels": [], "entities": [{"text": "POS regex", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.5557689666748047}]}, {"text": "Though our processing of Japanese includes basic lemmatization related to superficial elements like the choice of writing script and politeness markers, many elements (such as case marking) which are removed by lemmatization in Croatian are segmented into independent morphological units in the MeCab output, making the task somewhat different for the two languages.", "labels": [], "entities": [{"text": "MeCab output", "start_pos": 295, "end_pos": 307, "type": "DATASET", "confidence": 0.8967332541942596}]}, {"text": "introduced a method for evaluating FS extraction without a reference lexicon or direct annotation of the output of a model.", "labels": [], "entities": [{"text": "FS extraction", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9666808843612671}]}, {"text": "Instead, n-grams are sampled after applying the frequency threshold and then annotated as being either an FS or not.", "labels": [], "entities": [{"text": "FS", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9642171263694763}]}, {"text": "Benefits of this style of evaluation include replicability, the diversity of FS, and the ability to calculate a true F-score.", "labels": [], "entities": [{"text": "replicability", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9401811361312866}, {"text": "FS", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.8164260387420654}, {"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9228003025054932}]}, {"text": "We use the annotation of 2000 n-grams in the ICWSM corpus from that earlier work, and applied the same annotation methodology to the other three corpora: after training and based on written guidelines derived from the definitions of Wray, three native-speaker, educated annotators judged 500 contiguous n-grams and another 500 gapped n-grams for each corpus.", "labels": [], "entities": [{"text": "ICWSM corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9779202938079834}]}, {"text": "Other than the inclusion of new languages, our test sets differ from in two ways.", "labels": [], "entities": []}, {"text": "One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness.", "labels": [], "entities": [{"text": "reliability", "start_pos": 201, "end_pos": 212, "type": "METRIC", "confidence": 0.9672057032585144}]}, {"text": "As such we entirely excluded from our test set n-grams which just one annotator marked as FS.", "labels": [], "entities": [{"text": "FS", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9978495836257935}]}, {"text": "contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (\"Pre\") and after (\"Post\").", "labels": [], "entities": [{"text": "Fleiss' Kappa scores", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.913962701956431}]}, {"text": "The second change is that for the main evaluation we collapsed gapped and contiguous n-grams into a single test set.", "labels": [], "entities": []}, {"text": "The rationale is that the number of positive gapped examples is too low to provide a reliable independent F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9880637526512146}]}, {"text": "Our primary comparison is with the heuristic LPR model of, which is scalable to large corpora and includes gapped n-grams.", "labels": [], "entities": []}, {"text": "For the BNC, we also benchmark against the DP-seg model of with recommended settings, and the LocalMaxs algorithm of da Silva and Lopes (1999) using SCP; neither of these methods scale to the larger corpora.", "labels": [], "entities": [{"text": "BNC", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7922936677932739}]}, {"text": "Because these other approaches only generate sequential multiword units, we use only the sequential part of the BNC test set for this evaluation.", "labels": [], "entities": [{"text": "BNC test set", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9279036124547323}]}, {"text": "All comparison approaches have themselves been previously compared against a wide range of association measures.", "labels": [], "entities": []}, {"text": "As such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n-grams according to the measure used in our lattice (minLPR) as well as PMI and raw frequency.", "labels": [], "entities": [{"text": "PMI", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.8339989185333252}]}, {"text": "For each of these association measures we rank all n-grams above the frequency threshold and build a lexicon equal to the size of the lexicon produced by our model.", "labels": [], "entities": []}, {"text": "We created small development sets for each corpus and used them to do a thorough testing of parameter settings.", "labels": [], "entities": []}, {"text": "Although it is generally possible to increase precision by increasing C, we found that across corpora we always obtained near-optimal results with C = 4, so to demonstrate the usefulness of the lattice technique as an entirely off-the-shelf tool, we present the results using identical settings for all four corpora.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.998417854309082}]}, {"text": "We treat covering as a fundamental part of the Lattice model, but to investigate the efficacy of other node interactions within the model we present results with overlap and clearing node interactions turned off.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for test sets", "labels": [], "entities": []}, {"text": " Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based  ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al.", "labels": [], "entities": [{"text": "FS identification", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9781304001808167}, {"text": "Countrank", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9843127727508545}, {"text": "LPRseg", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.8933912515640259}]}, {"text": " Table 4: Statistics for the lexicons created by our lattice method", "labels": [], "entities": []}]}