{"title": [{"text": "Phrase Table Induction Using In-Domain Monolingual Data for Domain Adaptation in Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase Table Induction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8799782395362854}, {"text": "Statistical Machine Translation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.6799159248669943}]}], "abstractContent": [{"text": "We present anew framework to induce an in-domain phrase table from in-domain monolin-gual data that can be used to adapt a general-domain statistical machine translation system to the targeted domain.", "labels": [], "entities": [{"text": "general-domain statistical machine translation", "start_pos": 123, "end_pos": 169, "type": "TASK", "confidence": 0.6054238379001617}]}, {"text": "Our method first compiles sets of phrases in source and target languages separately and generates candidate phrase pairs by taking the Cartesian product of the two phrase sets.", "labels": [], "entities": []}, {"text": "It then computes inexpensive features for each candidate phrase pair and filters them using a supervised clas-sifier in order to induce an in-domain phrase table.", "labels": [], "entities": []}, {"text": "We experimented on the language pair English-French, both translation directions, in two domains and obtained consistently better results than a strong baseline system that uses an in-domain bilingual lexicon.", "labels": [], "entities": []}, {"text": "We also conducted an error analysis that showed the induced phrase tables proposed useful translations , especially for words and phrases unseen in the parallel data used to train the general-domain baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "In phrase-based statistical machine translation (SMT), translation models are estimated over a large amount of parallel data.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 3, "end_pos": 53, "type": "TASK", "confidence": 0.7338674494198391}]}, {"text": "In general, using more data leads to a better translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9678201079368591}]}, {"text": "When no specific domain is targeted, general-domain 1 parallel data from various domains maybe used to As in, in this paper, we use the term general-domain instead of the commonly used out-of-domain because we assume that the parallel data may contain some indomain sentence pairs.", "labels": [], "entities": []}, {"text": "train a general-purpose SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9871275424957275}]}, {"text": "However, it is well-known that, in training a system to translate texts from a specific domain, using in-domain parallel data can lead to a significantly better translation quality.", "labels": [], "entities": []}, {"text": "Indeed, when only general-domain parallel data are used, it is unlikely that the translation model can learn expressions and their translations specific to the targeted domain.", "labels": [], "entities": []}, {"text": "Such expressions will then remain untranslated in the in-domain texts to translate.", "labels": [], "entities": []}, {"text": "So far, in-domain parallel data have been harnessed to cover domain-specific expressions and their translations in the translation model.", "labels": [], "entities": []}, {"text": "However, even if we can assume the availability of a large quantity of general-domain parallel data, at least for resource-rich language pairs, finding in-domain parallel data specific to a particular domain remains challenging.", "labels": [], "entities": []}, {"text": "In-domain parallel data may not exist for the targeted language pairs or may not be available at hand to train a good translation model.", "labels": [], "entities": []}, {"text": "In order to circumvent the lack of in-domain parallel data, this paper presents anew method to adapt an existing SMT system to a specific domain by inducing an in-domain phrase table, i.e., a set of phrase pairs associated with features for decoding, from indomain monolingual data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9866047501564026}]}, {"text": "As we review in Section 2, most of the existing methods for inducing phrase tables are not designed, and may not perform as expected, to induce a phrase table fora specific domain for which only limited resources are available.", "labels": [], "entities": []}, {"text": "Instead of relying on large quantity of parallel data or highly comparable corpora, our method induces an in-domain phrase table from unaligned indomain monolingual data through a three-step pro-cedure: phrase collection, phrase pair scoring, and phrase pair filtering.", "labels": [], "entities": [{"text": "phrase collection", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.7538271844387054}, {"text": "phrase pair scoring", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.7406164209047953}, {"text": "phrase pair filtering", "start_pos": 247, "end_pos": 268, "type": "TASK", "confidence": 0.7090187271436056}]}, {"text": "Incorporating our induced indomain phrase table into an SMT system achieves substantial improvements in translating in-domain texts over a strong baseline system, which uses an in-domain bilingual lexicon.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9718964099884033}, {"text": "translating in-domain texts", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.8542947371800741}]}, {"text": "To achieve this improvement, our proposed method for inducing an in-domain phrase table addresses several limitations of previous work by: \u2022 dealing with source and target phrases of arbitrary length collected from in-domain monolingual data, \u2022 proposing translations for not only unseen source phrases, but also those already seen in the general-domain parallel data, and \u2022 making use of potentially many features computed from the monolingual data, as well as from the parallel data, in order to score and filter the candidate phrase pairs.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first review previous work in Section 2, highlighting the main weaknesses of existing methods for inducing a phrase table for domain adaptation, and our motivation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.75282222032547}]}, {"text": "In Section 3, we then present our phrase table induction method with all the necessary steps: phrase collection (Section 3.1), computing features of each phrase pair (Section 3.2), and pruning the induced phrase tables to keep their size manageable.", "labels": [], "entities": [{"text": "phrase collection", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7860745191574097}]}, {"text": "In Section 4, we describe our experiments to evaluate the impact of the induced phrase tables in translating in-domain texts.", "labels": [], "entities": [{"text": "translating in-domain texts", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.8773693641026815}]}, {"text": "Following the description of the data (Section 4.1), we explain the tools and parameters used to induce the phrase tables (Section 4.2), our SMT systems (Section 4.3), and present additional baseline systems (Section 4.4).", "labels": [], "entities": [{"text": "SMT", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.9722363948822021}]}, {"text": "Our experimental results are given in Section 4.5.", "labels": [], "entities": []}, {"text": "Section 5.1 analyzes the error distribution of the translations produced by an SMT system using our induced phrase table, followed by translation examples to further illustrate its impact in Section 5.2.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9746102690696716}]}, {"text": "Finally, Section 6 concludes this work and proposes some possible improvements to our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section demonstrates the impact of the induced phrase tables in translating in-domain texts in three configurations.", "labels": [], "entities": [{"text": "translating in-domain texts", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.8609632054964701}]}, {"text": "In the first configuration (Conf. 1), we evaluated whether our induced phrase table improves the translation of in-domain texts over the vanilla SMT system which used only one phrase table trained from general-domain parallel data.", "labels": [], "entities": [{"text": "translation of in-domain texts", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.8361909538507462}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9268549084663391}]}, {"text": "We then evaluated, in the second configuration (Conf. 2), whether our induced phrase table is also beneficial when used in an SMT system that already incorporates an in-domain bilingual lexicon that could be created manually or induced by some of the methods mentioned in Section 2.", "labels": [], "entities": [{"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9839975833892822}]}, {"text": "Finally, we evaluated in complementary experiments (Conf.", "labels": [], "entities": []}, {"text": "3) whether our induced phrase table can also offer useful information to improve translation quality even when used in combination with another standard phrase table generated from in-domain parallel data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on train, development, and test data.", "labels": [], "entities": []}, {"text": " Table 6: Results (BLEU and METEOR) with an induced phrase table (IPT). The Moses du and vanilla Moses systems  use only one phrase table trained from", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9982567429542542}, {"text": "METEOR", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9516267776489258}]}, {"text": " Table 7: Percentage of the source tokens: comparison of  the translations generated with (w/) or without (w/o) our  gen-lex induced phrase table (Conf. 1).", "labels": [], "entities": []}]}