{"title": [{"text": "Context Gates for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.7637836138407389}]}], "abstractContent": [{"text": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.8510031998157501}]}, {"text": "We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency.", "labels": [], "entities": []}, {"text": "Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context.", "labels": [], "entities": []}, {"text": "Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations.", "labels": [], "entities": []}, {"text": "To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words.", "labels": [], "entities": []}, {"text": "In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts.", "labels": [], "entities": []}, {"text": "Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9994064569473267}]}], "introductionContent": [{"text": "Neural machine translation (NMT)) has made significant progress in the past several years.", "labels": [], "entities": [{"text": "Neural machine translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8086172044277191}]}, {"text": "Its goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.8941963613033295}]}, {"text": "One great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in statistical machine translation (SMT)).", "labels": [], "entities": [{"text": "statistical machine translation (SMT))", "start_pos": 165, "end_pos": 203, "type": "TASK", "confidence": 0.7863386025031408}]}, {"text": "The encoderdecoder architecture is widely employed ( input j\u00af \u0131nni\u00e1n qi\u00e1n li\u02c7angli\u02c7angy\u00f9 e gu\u02c7angd\u00afgu\u02c7angd\u00af ong g\u00af aox\u00af \u0131n j` \u0131sh\u00f9 ch\u02c7anp\u02c7\u0131nch\u02c7anp\u02c7ch\u02c7anp\u02c7\u0131n ch\u00af uk\u02c7ouuk\u02c7ou 37.6y`6y`\u0131 m\u011biyu\u00e1n NMT in the first two months of this year , the export of new high level technology product was UNK -billion us dollars src china 's guangdong hi -tech exports hit 58 billion dollars tgt china 's export of high and new hi -tech exports of the export of the export of the export of the export of the export of the export of the export of the export of \u00b7 \u00b7 \u00b7: Source and target contexts are highly correlated to translation adequacy and fluency, respectively.", "labels": [], "entities": []}, {"text": "src and tgt denote halving the contributions from the source and target contexts when generating the translation, respectively.", "labels": [], "entities": []}, {"text": "2014;, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word-by-word from the vector representation.", "labels": [], "entities": []}, {"text": "The representation of the source sentence and the representation of the partially generated target sentence (translation) at each position are referred to as source context and target context, respectively.", "labels": [], "entities": []}, {"text": "The generation of a target word is determined jointly by the source context and target context.", "labels": [], "entities": []}, {"text": "Several techniques in NMT have proven to be very effective, including gating) and attention () which can model long-distance dependencies and complicated align-ment relations in the translation process.", "labels": [], "entities": []}, {"text": "Using an encoder-decoder framework that incorporates gating and attention techniques, it has been reported that the performance of NMT can surpass the performance of traditional SMT as measured by BLEU score (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9831656217575073}, {"text": "BLEU score", "start_pos": 197, "end_pos": 207, "type": "METRIC", "confidence": 0.9721987843513489}]}, {"text": "Despite this success, we observe that NMT usually yields fluent but inadequate translations.", "labels": [], "entities": []}, {"text": "We attribute this to a stronger influence of target context on generation, which results from a stronger language model than that used in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9888879656791687}]}, {"text": "One question naturally arises: what will happen if we change the ratio of influences from the source or target contexts?", "labels": [], "entities": []}, {"text": "shows an example in which an attentionbased NMT system () generates a fluent yet inadequate translation (e.g., missing the translation of \"gu\u02c7angd\u00afgu\u02c7angd\u00af ong\").", "labels": [], "entities": []}, {"text": "When we halve the contribution from the source context, the result further loses its adequacy by missing the partial translation \"in the first two months of this year\".", "labels": [], "entities": []}, {"text": "One possible explanation is that the target context takes a higher weight and thus the system favors a shorter translation.", "labels": [], "entities": []}, {"text": "In contrast, when we halve the contribution from the target context, the result completely loses its fluency by repeatedly generating the translation of \"ch\u00af uk\u02c7ouuk\u02c7ou\" (i.e., \"the export of\") until the generated translation reaches the maximum length.", "labels": [], "entities": []}, {"text": "Therefore, this example indicates that source and target contexts in NMT are highly correlated to translation adequacy and fluency, respectively.", "labels": [], "entities": []}, {"text": "In fact, conventional NMT lacks effective control on the influence of source and target contexts.", "labels": [], "entities": []}, {"text": "At each decoding step, NMT treats the source and target contexts equally, and thus ignores the different needs of the contexts.", "labels": [], "entities": []}, {"text": "For example, content words in the target sentence are more related to the translation adequacy, and thus should depend more on the source context.", "labels": [], "entities": []}, {"text": "In contrast, function words in the target sentence are often more related to the translation fluency (e.g., \"of\" after \"is fond\"), and thus should depend more on the target context.", "labels": [], "entities": []}, {"text": "In this work, we propose to use context gates to control the contributions of source and target contexts on the generation of target words (decoding) in NMT.", "labels": [], "entities": []}, {"text": "Context gates are non-linear gating units which can dynamically select the amount of context information in the decoding process.", "labels": [], "entities": []}, {"text": "Specifically, at each decoding step, the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts.", "labels": [], "entities": []}, {"text": "In this way, the system can balance the adequacy and fluency of the translation with regard to the generation of a word at each position.", "labels": [], "entities": []}, {"text": "Experimental results show that introducing context gates leads to an average improvement of +2.3 BLEU points over a standard attention-based NMT system ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9991214871406555}]}, {"text": "An interesting finding is that we can replace the GRU units in the decoder with conventional RNN units and in the meantime utilize context gates.", "labels": [], "entities": []}, {"text": "The translation performance is comparable with the standard NMT system with GRU, but the system enjoys a simpler structure (i.e., uses only a single gate and half of the parameters) and a faster decoding (i.e., requires only half the matrix computations for decoding).", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9575241804122925}, {"text": "GRU", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8383383750915527}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation of translation quality measured by case-insensitive BLEU score. \"GroundHog  (vanilla)\" and \"GroundHog (GRU )\" denote attention-based NMT (", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9542244076728821}, {"text": "BLEU score", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9662416875362396}, {"text": "GroundHog (GRU )\"", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.6257014498114586}]}, {"text": " Table 4: Evaluation of alignment quality. The lower  the score, the better the alignment quality.", "labels": [], "entities": []}, {"text": " Table 5: Analysis of the model architectures measured in BLEU scores. \"Gating Scalar\" denotes the model  proposed by", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9948840737342834}]}]}