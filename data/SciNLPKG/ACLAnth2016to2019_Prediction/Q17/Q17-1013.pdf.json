{"title": [], "abstractContent": [{"text": "This paper presents a novel hybrid genera-tive/discriminative model of word segmenta-tion based on nonparametric Bayesian methods.", "labels": [], "entities": []}, {"text": "Unlike ordinary discriminative word seg-mentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new \"words\", and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services.", "labels": [], "entities": []}, {"text": "Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al.", "labels": [], "entities": []}, {"text": "(2001) and unsupervised word segmentation (NPYLM; Mochihashi et al.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6622105240821838}]}, {"text": "(2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM; Suzuki and Isozaki (2008)).", "labels": [], "entities": [{"text": "JESS-CM", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.902027428150177}]}, {"text": "We confirmed that it can appropriately segment non-standard texts like those in Twitter and Weibo and has nearly state-of-the-art accuracy on standard datasets in Japanese, Chinese, and Thai.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9993482232093811}]}], "introductionContent": [{"text": "For any unsegmented language, especially East Asian languages such as Chinese, Japanese and Thai, word segmentation is almost an inevitable first step in natural language processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7159893065690994}, {"text": "natural language processing", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6891644398371378}]}, {"text": "In fact, it is becoming increasingly important lately because of the growing interest in processing user-generated media, such as Twitter and blogs.", "labels": [], "entities": []}, {"text": "Texts in such media are often written in a colloquial style that contains many new words and expressions that are not present in any existing dictionaries.", "labels": [], "entities": []}, {"text": "Since such words are theoretically infinite in number, we need to leverage unsupervised learning to automatically identify them in corpora.", "labels": [], "entities": []}, {"text": "For this purpose, ordinary supervised learning is clearly unsatisfactory; even hand-crafted dictionaries will not suffice because functional expressions more complex than simple nouns need to be recognized through their relationship with other words in text, which also might be unknown in advance.", "labels": [], "entities": []}, {"text": "Previous studies of this issue used character and word information in the framework of supervised learning).", "labels": [], "entities": []}, {"text": "However, they (1) did not explicitly model new words, or (2) did not give a seamless combination with discriminative classifiers (e.g., they just used a threshold to discriminate between known and unknown words).", "labels": [], "entities": []}, {"text": "In contrast, unsupervised word segmentation methods () use nonparametric Bayesian generative models for word generation to infer the \"words\" only from observations of raw input strings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7627917528152466}, {"text": "word generation", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7230216413736343}]}, {"text": "These methods work quite well and have been used not only for tokenization but also for machine translation (, speech recognition (, and even robotics (.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.9726192951202393}, {"text": "machine translation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.8010912537574768}, {"text": "speech recognition", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7829305231571198}]}, {"text": "However, from a practical point of view, such purely unsupervised approaches do not suffice.", "labels": [], "entities": []}, {"text": "Since they only aim to maximize the probability of the language model on the observed set of strings, they sometimes yield word segmentations that are Figure 1: Excerpt of Weibo tweets.", "labels": [], "entities": [{"text": "word segmentations", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7236409187316895}]}, {"text": "It contains many \"unknown\" words such as novel proper nouns, terms from local dialects, etc., that cannot be covered by ordinary labeled data or dictionaries.", "labels": [], "entities": []}, {"text": "different from human standards on low frequency words.", "labels": [], "entities": []}, {"text": "To solve this problem, this paper describes a novel combination of a nonparametric Bayesian generative model (NPYLM;) and a discriminative classifier (CRF;).", "labels": [], "entities": []}, {"text": "This combination is based on a semisupervised framework called JESS-CM, and it requires a nontrivial exchange of information between these two models.", "labels": [], "entities": [{"text": "JESS-CM", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9044758081436157}]}, {"text": "In this approach, the generative and discriminative models will \"teach each other\" and yield a novel log-linear model for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7583338022232056}]}, {"text": "Experiments on standard datasets of Chinese, Japanese, and Thai indicate that this hybrid model achieves nearly state-of-the-art accuracy on standard corpora, and, thanks to our nonparametric Bayesian model of infinite vocabulary it can accurately segment non-standard texts like those in Twitter and Weibo (the Chinese equivalent of Twitter) without any human intervention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9987452030181885}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces NPYLM which will be leveraged in the framework of JESS-CM, described in Section 3.", "labels": [], "entities": [{"text": "JESS-CM", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9185259342193604}]}, {"text": "Section 4 introduces our model, NPYCRF, and the necessary exchange of information, while Section 5 is devoted to experiments on datasets in Chinese, Japanese, and Thai.", "labels": [], "entities": [{"text": "NPYCRF", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7613429427146912}]}, {"text": "We analyze the results and discuss future directions of research on semisupervised learning in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on several corpora of unsegmented languages: Japanese, Chinese, and Thai.", "labels": [], "entities": []}, {"text": "The corpora included standard corpora as well as text from Twitter and its equivalent, Weibo, in Chinese.", "labels": [], "entities": []}, {"text": "Chinese show IV (in-vocabulary) and OOV (out-of-vocabulary) precision and Fmeasure, computed against segmented tokens.", "labels": [], "entities": [{"text": "OOV", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9969047904014587}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.8717123866081238}, {"text": "Fmeasure", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9981772899627686}]}, {"text": "The results for standard newspaper text indicate that NPYCRF is basically comparable in performance to state-of-the-art supervised neural networks) that require hand tuning of hyperparameters or model architectures.", "labels": [], "entities": []}, {"text": "shows some of the learned words in the testset of the Bakeoff MSR corpus.", "labels": [], "entities": [{"text": "Bakeoff MSR corpus", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.8975731333096822}]}, {"text": "As shown in Table 3, NPYCRF also yields higher precision than supervised learning on non-standard text like Weibo, which is the main objective for this study.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9986666440963745}, {"text": "Weibo", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.9585577249526978}]}, {"text": "Contrary to ordinary supervised learning, we can see that NPYCRF effectively learns many \"new words\" from the large amount of unlabeled data thanks to the generative model, while observing human standards of segmentation by the discriminative model.", "labels": [], "entities": []}, {"text": "Note that in Weibo segmentation, complete supervision is not available in practice.", "labels": [], "entities": [{"text": "Weibo segmentation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7331291139125824}]}, {"text": "In fact, we realized that the Weibo segmentations were given automatically by an existing classifier, and contain many inappropriate segmentations, while NPYCRF finds much \"better\" segmentations.", "labels": [], "entities": [{"text": "Weibo segmentations", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7104750871658325}, {"text": "NPYCRF", "start_pos": 154, "end_pos": 160, "type": "DATASET", "confidence": 0.876134991645813}]}, {"text": "compares the results of CRF, NPYLM, and NPYCRF with the gold segmentation.", "labels": [], "entities": [{"text": "NPYCRF", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.8702590465545654}]}, {"text": "While proverbs like \"4c9b< jDc \" (wide vision without action) are correctly captured from the unlabeled data by NPYLM, it is sometimes broken by CRF through integration.", "labels": [], "entities": [{"text": "NPYLM", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9169452786445618}]}, {"text": "In another case, the name of a person is properly connected because of the information provided by the CRF.", "labels": [], "entities": []}, {"text": "This comparison shows that there is still room for improvement in NPYCRF.", "labels": [], "entities": [{"text": "NPYCRF", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.8556142449378967}]}, {"text": "Section 6 discusses future research directions for improvements.", "labels": [], "entities": []}, {"text": "Japanese and Thai shows an example of the analysis of Japanese Twitter text.", "labels": [], "entities": []}, {"text": "Shaded words are those that are not contained in labeled data (BCCWJ core) but were found by NPYCRF.", "labels": [], "entities": [{"text": "BCCWJ core", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8077841997146606}, {"text": "NPYCRF", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.933906078338623}]}, {"text": "Many segmentations, including new words, are correct.", "labels": [], "entities": []}, {"text": "We expect NPYCRF would perform better with more unlabeled data that are easily obtained.", "labels": [], "entities": [{"text": "NPYCRF", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.7728312611579895}]}, {"text": "show the segmentation accuracies of the Twitter data in Japanese and novel data in Thai.", "labels": [], "entities": []}, {"text": "While there are no publicly available results for these data (the InterBEST testset is closed during competition), NPYCRF achieved better accuracies than vanilla supervised segmentation based on CRF.", "labels": [], "entities": [{"text": "InterBEST testset", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.8682305216789246}]}, {"text": "Considering that many new words were found in, for example, we believe NPYCRF is quite competitive thanks to its ability to learn the infinite vocabulary, which it inherits from NPYLM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies of Bakeoff MSR dataset in Chinese.  \"Filtered\" are the results with a simple post-hoc filter de- scribed in Sun et al. (2009).", "labels": [], "entities": [{"text": "Bakeoff MSR dataset", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.7709372838338217}]}, {"text": " Table 3: Accuracies on Leiden Weibo corpus in Chinese.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8942828178405762}, {"text": "Leiden Weibo corpus", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.9553727904955546}]}, {"text": " Table 4: Accuracies for Twitter text in Japanese.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9951115250587463}]}, {"text": " Table 5: Accuracies for InterBEST novel dataset in Thai.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9925353527069092}, {"text": "InterBEST novel dataset", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.6231015225251516}]}]}