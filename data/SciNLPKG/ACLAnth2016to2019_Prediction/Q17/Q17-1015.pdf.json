{"title": [{"text": "Pushing the Limits of Translation Quality Estimation", "labels": [], "entities": [{"text": "Translation Quality Estimation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.9051152666409811}]}], "abstractContent": [{"text": "Translation quality estimation is a task of growing importance in NLP, due to its potential to reduce post-editing human effort in dis-ruptive ways.", "labels": [], "entities": [{"text": "Translation quality estimation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8736199140548706}]}, {"text": "However, this potential is currently limited by the relatively low accuracy of existing systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991390705108643}]}, {"text": "In this paper, we achieve remarkable improvements by exploiting syn-ergies between the related tasks of word-level quality estimation and automatic post-editing.", "labels": [], "entities": [{"text": "word-level quality estimation", "start_pos": 104, "end_pos": 133, "type": "TASK", "confidence": 0.7108685374259949}]}, {"text": "First, we stack anew, carefully engineered, neural model into a rich feature-based word-level quality estimation system.", "labels": [], "entities": []}, {"text": "Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16: a word-level F MULT 1 score of 57.47% (an absolute gain of +7.95% over the current state of the art), and a Pearson correlation score of 65.56% for sentence-level HTER prediction (an absolute gain of +13.36%).", "labels": [], "entities": [{"text": "WMT16", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9235184192657471}, {"text": "word-level F MULT 1 score", "start_pos": 120, "end_pos": 145, "type": "METRIC", "confidence": 0.7756561040878296}, {"text": "Pearson correlation score", "start_pos": 226, "end_pos": 251, "type": "METRIC", "confidence": 0.9503971338272095}, {"text": "HTER prediction", "start_pos": 281, "end_pos": 296, "type": "TASK", "confidence": 0.7658495604991913}]}], "introductionContent": [{"text": "The goal of quality estimation (QE) is to evaluate a translation system's quality without access to reference translations (.", "labels": [], "entities": [{"text": "quality estimation (QE)", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.6837710440158844}]}, {"text": "This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed.", "labels": [], "entities": []}, {"text": "QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs.", "labels": [], "entities": []}, {"text": "The increasing interest in this problem from an industrial angle comes as no surprise (.", "labels": [], "entities": []}, {"text": "In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation).", "labels": [], "entities": [{"text": "word-level QE", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.4332653433084488}, {"text": "OK", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9453959465026855}, {"text": "BAD", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.7401456236839294}]}, {"text": "Past approaches to this problem include linear classifiers with handcrafted features), often combined with feature selection, recurrent neural networks (de, and systems that combine linear and neural models (.", "labels": [], "entities": []}, {"text": "We start by proposing a \"pure\" QE system ( \u00a73) consisting of anew, carefully engineered neural model (NEURALQE), stacked into a linear feature-rich classifier.", "labels": [], "entities": []}, {"text": "Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system.", "labels": [], "entities": []}, {"text": "A second contribution of this paper is bringing in the related task of automatic post-editing (APE;), which aims to au-: Example from the WMT16 word-level QE training set.", "labels": [], "entities": [{"text": "automatic post-editing (APE", "start_pos": 71, "end_pos": 98, "type": "METRIC", "confidence": 0.5938558951020241}, {"text": "WMT16 word-level QE training set", "start_pos": 138, "end_pos": 170, "type": "DATASET", "confidence": 0.8577769279479981}]}, {"text": "Shown are the English source sentence, the German translation (MT), its manual post-edition (PE), and the conversion to word quality labels made with the TERCOM tool (QE).", "labels": [], "entities": [{"text": "manual post-edition (PE)", "start_pos": 72, "end_pos": 96, "type": "METRIC", "confidence": 0.6403539776802063}, {"text": "TERCOM", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9245449304580688}]}, {"text": "Words labeled as OK are shown in green, and those labeled as BAD are shown in red.", "labels": [], "entities": [{"text": "BAD", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9680383801460266}]}, {"text": "We also show the HTER (fraction of edit operations to produce PE from MT) computed by TERCOM.", "labels": [], "entities": [{"text": "HTER", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9950653910636902}, {"text": "TERCOM", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9025512933731079}]}, {"text": "tomatically correct the output of machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8226120173931122}]}, {"text": "We show that a variant of the APE system of Junczys-Dowmunt and, trained on a large amount of artificial \"roundtrip translations,\" is extremely effective when adapted to predict word-level quality labels (yielding APEQE, \u00a74).", "labels": [], "entities": [{"text": "APEQE", "start_pos": 214, "end_pos": 219, "type": "METRIC", "confidence": 0.5748307704925537}]}, {"text": "We further show that the pure and the APEbased QE system are highly complementary ( \u00a75): a stacked combination of LINEARQE, NEURALQE, and APEQE boosts the scores even further, leading to anew state of the art on the WMT15 and WMT16 datasets.", "labels": [], "entities": [{"text": "LINEARQE", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9912357330322266}, {"text": "NEURALQE", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9076454639434814}, {"text": "APEQE", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9198113679885864}, {"text": "WMT15", "start_pos": 216, "end_pos": 221, "type": "DATASET", "confidence": 0.9737207293510437}, {"text": "WMT16 datasets", "start_pos": 226, "end_pos": 240, "type": "DATASET", "confidence": 0.8616304397583008}]}, {"text": "For the latter, we achieve an F MULT 1 score of 57.47%, which represents an absolute improvement of +7.95% over the previous best system.", "labels": [], "entities": [{"text": "F", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9986578226089478}, {"text": "MULT 1 score", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.929691751797994}]}, {"text": "Finally, we provide a simple word-to-sentence conversion to adapt our system to sentence-level QE.", "labels": [], "entities": []}, {"text": "This results in anew state of the art for humantargeted translation error rate (HTER) prediction, where we obtain a Pearson's r correlation score of 65.56% (+13.36% absolute gain), and for sentence ranking, which achieves a Spearman's \u03c1 correlation score of 65.92% (+17.62%).", "labels": [], "entities": [{"text": "humantargeted translation error rate (HTER) prediction", "start_pos": 42, "end_pos": 96, "type": "TASK", "confidence": 0.6673675626516342}, {"text": "Pearson's r correlation score", "start_pos": 116, "end_pos": 145, "type": "METRIC", "confidence": 0.8962823867797851}, {"text": "Spearman's \u03c1 correlation score", "start_pos": 224, "end_pos": 254, "type": "METRIC", "confidence": 0.6573023080825806}]}, {"text": "We complement our findings with error analysis that highlights the synergies between pure and APE-based QE systems.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.453729584813118}]}], "datasetContent": [{"text": "For developing and evaluating our systems, we use the datasets listed in.", "labels": [], "entities": []}, {"text": "These datasets have been used in the QE and APE tasks in).", "labels": [], "entities": [{"text": "APE tasks", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.3786650151014328}]}, {"text": "1 They span two language pairs (English-Spanish and English-German) and two different domains (news translations and information technology).", "labels": [], "entities": [{"text": "news translations", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.741878479719162}]}, {"text": "We used the standard train, development and test splits.", "labels": [], "entities": []}, {"text": "Each split contains the source and automatically translated sentences (which we use as inputs), the manu-ally post-edited sentences (output for the APE task), and a sequence of OK/BAD quality labels, one per each translated word (output for the word-level QE task); see.", "labels": [], "entities": [{"text": "BAD", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.8634541034698486}]}, {"text": "Besides these datasets, for training the APE system we make use of artificial roundtrip translations; this will be detailed in \u00a74.", "labels": [], "entities": [{"text": "APE", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.47143039107322693}]}, {"text": "For all experiments, we report the official evaluation metrics of each dataset's year.", "labels": [], "entities": []}, {"text": "For WMT15, the official metric for the word-level QE task is the F 1 score of the BAD labels (F BAD 1 ).", "labels": [], "entities": [{"text": "WMT15", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6513514518737793}, {"text": "F 1 score", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.98625514904658}, {"text": "BAD labels (F BAD 1 )", "start_pos": 82, "end_pos": 103, "type": "METRIC", "confidence": 0.8107616305351257}]}, {"text": "For WMT16, it is the product of the F 1 scores for the OK and BAD labels (denoted F MULT 1 ).", "labels": [], "entities": [{"text": "WMT16", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7135950326919556}, {"text": "F 1 scores", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9530368645985922}, {"text": "BAD", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9474526047706604}, {"text": "F MULT 1 )", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.8585310280323029}]}, {"text": "For sentencelevel QE, we report the Pearson's r correlation for HTER prediction and the Spearman's \u03c1 correlation score for sentence ranking.", "labels": [], "entities": [{"text": "sentencelevel QE", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6258562207221985}, {"text": "Pearson's r correlation", "start_pos": 36, "end_pos": 59, "type": "METRIC", "confidence": 0.9308811128139496}, {"text": "HTER prediction", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.791636735200882}, {"text": "Spearman's \u03c1 correlation score", "start_pos": 88, "end_pos": 118, "type": "METRIC", "confidence": 0.6838370740413666}]}, {"text": "From post-edited sentences to quality labels.", "labels": [], "entities": []}, {"text": "In the datasets above, the word quality labels are obtained automatically by aligning the translated and the post-edited sentence with the TERCOM software tool) , with the default settings (tokenized, case insensitive, exact matching only, shifts disabled).", "labels": [], "entities": []}, {"text": "This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence.", "labels": [], "entities": [{"text": "HTER", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.993368923664093}]}, {"text": "As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and insertions (redundant words in the translation).", "labels": [], "entities": []}, {"text": "Words in the MT output that need to be edited are marked by the BAD quality labels.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8447890281677246}, {"text": "BAD quality labels", "start_pos": 64, "end_pos": 82, "type": "METRIC", "confidence": 0.9569236636161804}]}, {"text": "The fact that the quality labels are automatically obtained from the post-edited sentences is not just an artifact of these datasets, but a procedure that is highly convenient for developing QE systems in an industrial setting.", "labels": [], "entities": []}, {"text": "Manually annotating word-level quality labels is time-consuming and expensive; on the other hand, post-editing translated sentences is commonly part of the workflow of crowd-sourced and professional translation services.", "labels": [], "entities": []}, {"text": "Thus, getting quality labels for free from sentences that have already been post-edited is a much more realistic and sustainable process.", "labels": [], "entities": []}, {"text": "This observation suggests that we can tackle word-level QE in two ways: 1.", "labels": [], "entities": [{"text": "word-level QE", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.4800712764263153}]}, {"text": "Pure QE: run the TER alignment tool (i.e. TER-COM) on the post-edited data, and then train a QE system directly on the generated quality labels; 2.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7318784296512604}]}, {"text": "APE-based QE: train an APE system on the original post-edited data, and at runtime use the TER aligment tool to convert the automatically post-edited sentences to quality labels.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6098646521568298}]}, {"text": "From a machine learning pespective, QE is a sequence labeling problem (i.e., whose output sequence has a fixed length and a small number of labels), while APE is a sequence-to-sequence problem (where the output is of variable length and spans a large vocabulary).", "labels": [], "entities": [{"text": "APE", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.842327892780304}]}, {"text": "Therefore, we can regard APE-based QE as a \"projection\" of a more complex and fine-grained output (APE) into a simpler output space (QE).", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6360097229480743}]}, {"text": "APE-based QE systems have the potential for being more powerful since they are trained with this finer-grained information (provided there is enough training data to make them generalize well).", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.592102438211441}]}, {"text": "We report results in \u00a74 confirming this hypothesis.", "labels": [], "entities": []}, {"text": "Our system architecture, described in full detail in the following sections, consists of state of the art pure QE and APE-based QE systems, which are then combined to yield anew, more powerful, QE system.", "labels": [], "entities": [{"text": "APE-based", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.5935068726539612}]}], "tableCaptions": [{"text": " Table 1: Datasets used in this work.", "labels": [], "entities": []}, {"text": " Table 2: Features used in the LINEARQE system (see Martins et al., 2016 for a detailed description). Features marked  with  *  are included in the WMT16 baseline system. Those marked with  \u2020 were proposed by Kreutzer et al. (2015).", "labels": [], "entities": [{"text": "LINEARQE", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8892749547958374}, {"text": "WMT16 baseline system", "start_pos": 148, "end_pos": 169, "type": "DATASET", "confidence": 0.9257672230402628}]}, {"text": " Table 3:  Performance on the WMT15 (En-Es) and  WMT16 (En-De) development sets of several configu- rations of LINEARQE. We report the official metric for  these shared tasks, F BAD", "labels": [], "entities": [{"text": "WMT15", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.882179319858551}, {"text": "WMT16 (En-De) development sets", "start_pos": 49, "end_pos": 79, "type": "DATASET", "confidence": 0.6914817194143931}, {"text": "LINEARQE", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.7922806143760681}, {"text": "F BAD", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.7647909224033356}]}, {"text": " Table 5: Performance of the pure QE systems on the  WMT15 datasets. The best performing system in the  WMT15 competition was byEsp\u00ec a-Gomis et al. (2015),  followed by Kreutzer et al. (2015)'s QUETCH+, which  is also an ensemble of a linear and a neural system.", "labels": [], "entities": [{"text": "WMT15 datasets", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9860266149044037}, {"text": "WMT15", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.8129842281341553}]}, {"text": " Table 6: Performance of the pure QE systems on the  WMT16 datasets. The best performing system in the  WMT16 competition was by Martins et al. (2016), fol- lowed by a linear system developed by the same team  (Unbabel-Linear).", "labels": [], "entities": [{"text": "WMT16 datasets", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9875871539115906}, {"text": "WMT16", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.7854213118553162}]}, {"text": " Table 7: TER scores on the official WMT15 and WMT16  test sets for the APE task. Lower is better.", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9994972944259644}, {"text": "WMT15", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9435927271842957}, {"text": "WMT16  test sets", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.8741893172264099}, {"text": "APE task", "start_pos": 72, "end_pos": 80, "type": "TASK", "confidence": 0.8742907345294952}]}, {"text": " Table 8: Performance of APE-based QE systems on the  WMT15 development and test sets.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7416049540042877}, {"text": "WMT15 development and test sets", "start_pos": 54, "end_pos": 85, "type": "DATASET", "confidence": 0.9169676899909973}]}, {"text": " Table 9: Performance of APE-based QE systems on the  WMT16 development and test sets.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7320774495601654}, {"text": "WMT16 development and test sets", "start_pos": 54, "end_pos": 85, "type": "DATASET", "confidence": 0.9066840529441833}]}, {"text": " Table 10: Performance of the several word-level QE sys- tems on the WMT15 development and test datasets. The  baseline is the best participating system in WMT15, from Esp\u00ec a-Gomis et al. (2015).", "labels": [], "entities": [{"text": "WMT15 development and test datasets", "start_pos": 69, "end_pos": 104, "type": "DATASET", "confidence": 0.7821604967117309}, {"text": "WMT15", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.8678838610649109}]}, {"text": " Table 11: Performance of the several word-level QE sys- tems on the WMT16 development and test datasets. The  baseline is the best participating system in WMT16, from  the Unbabel team (Martins et al., 2016).", "labels": [], "entities": [{"text": "WMT16 development and test datasets", "start_pos": 69, "end_pos": 104, "type": "DATASET", "confidence": 0.795529055595398}, {"text": "WMT16", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.8675392270088196}]}, {"text": " Table 12: Performance of our sentence-level QE systems on the WMT15 an WMT16 datasets, as measured by the  WMT16 official evaluation script. The baselines are the best WMT15-16 systems in the HTER prediction track (Bicici  et al., 2015; Kozlova et al., 2016) and in the sentence ranking track", "labels": [], "entities": [{"text": "WMT15", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9503190517425537}, {"text": "WMT16 datasets", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.8778273165225983}, {"text": "WMT16 official evaluation script", "start_pos": 108, "end_pos": 140, "type": "DATASET", "confidence": 0.9392862468957901}, {"text": "HTER prediction", "start_pos": 193, "end_pos": 208, "type": "TASK", "confidence": 0.4887141287326813}]}]}