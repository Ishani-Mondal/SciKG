{"title": [{"text": "Domain-Targeted, High Precision Knowledge Extraction", "labels": [], "entities": [{"text": "High Precision Knowledge Extraction", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.5812686756253242}]}], "abstractContent": [{"text": "Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject,predicate,object) statements about the world, in support of a downstream question-answering (QA) application.", "labels": [], "entities": []}, {"text": "Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8698378205299377}]}, {"text": "To address these, we have created a domain-targeted, high precision knowledge extraction pipeline, leveraging Open IE, crowdsourcing, and a novel canonical schema learning algorithm (called CASI), that produces high precision knowledge targeted to a particular domain in our case, elementary science.", "labels": [], "entities": []}, {"text": "To measure the KB's coverage of the target do-main's knowledge (its \"comprehensiveness\" with respect to science) we measure recall with respect to an independent corpus of domain text, and show that our pipeline produces output with over 80% precision and 23% recall with respect to that target, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9989739656448364}, {"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.9971422553062439}, {"text": "recall", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.9969546794891357}]}, {"text": "We have made the KB publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "While there have been substantial advances in knowledge extraction techniques, the availability of high precision, general knowledge about the world, remains elusive.", "labels": [], "entities": [{"text": "knowledge extraction", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7438286393880844}]}, {"text": "Specifically, our goal is a large, high precision body of (subject,predicate,object) statements relevant to elementary science, to support a downstream QA application task.", "labels": [], "entities": []}, {"text": "Although there are several impressive, existing resources that can contribute to our endeavor, e.g.,,, WordNet,),), FreeBase (, and ReVerb-15M), their applicability is limited by both \u2022 limited coverage of general knowledge (e.g., FreeBase and NELL primarily contain knowledge about Named Entities; WordNet uses only a few (< 10) semantic relations) \u2022 low precision (e.g., many ConceptNet assertions express idiosyncratic rather than general knowledge) Our goal in this work is to create a domain-targeted knowledge extraction pipeline that can overcome these limitations and output a high precision KB of triples relevant to our end task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9580665826797485}, {"text": "FreeBase", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.954955518245697}, {"text": "precision", "start_pos": 356, "end_pos": 365, "type": "METRIC", "confidence": 0.9958410859107971}]}, {"text": "Our approach leverages existing techniques of open information extraction (Open IE) and crowdsourcing, along with a novel schema learning algorithm.", "labels": [], "entities": [{"text": "open information extraction (Open IE)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.7819416267531258}]}, {"text": "There are three main contributions of this work.", "labels": [], "entities": []}, {"text": "First, we present a high precision extraction pipeline able to extract (subject,predicate,object) tuples relevant to a domain with precision in excess of 80%.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9636058211326599}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9984947443008423}]}, {"text": "The input to the pipeline is a corpus, a sensedisambiguated domain vocabulary, and a small set of entity types.", "labels": [], "entities": []}, {"text": "The pipeline uses a combination of text filtering, Open IE, Turker annotation on samples, and precision prediction to generate its output.", "labels": [], "entities": [{"text": "text filtering", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.745013028383255}, {"text": "precision prediction", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.8858201801776886}]}, {"text": "Second, we present a novel canonical schema induction method (called CASI) that identifies clusters of similar-meaning predicates, and maps them to the most appropriate general predicate that captures that canonical meaning.", "labels": [], "entities": []}, {"text": "Open IE, used in the early part of our pipeline, generates triples containing a large number of predicates (expressed as verbs or verb phrases), but equivalences and generalizations among them are not captured.", "labels": [], "entities": []}, {"text": "Synonym dictionaries, paraphrase databases, and verb taxonomies can help identify these relationships, but only partially so because the meaning of a verb often shifts as its subject and object vary, something that these resources do not explicitly model.", "labels": [], "entities": []}, {"text": "To address this challenge, we have developed a corpus-driven method that takes into account the subject and object of the verb, and thus can learn argument-specific mapping rules, e.g., the rule \"(x:Animal,found in,y:Location) \u2192 (x:Animal,live in,y:Location)\" states that if some animal is found in a location then it also means the animal lives in the location.", "labels": [], "entities": []}, {"text": "Note that 'found in' can have very different meaning in the schema \"(x:Substance,found in,y:Material).", "labels": [], "entities": []}, {"text": "The result is a KB whose general predicates are more richly populated, still with high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9968847632408142}]}, {"text": "Finally, we contribute the science KB itself as a resource publicly available 2 to the research community.", "labels": [], "entities": []}, {"text": "To measure how \"complete\" the KB is with respect to the target domain (elementary science), we use an (independent) corpus of domain text to characterize the target science knowledge, and measure the KB's recall at high (>80%) precision over that corpus (its \"comprehensiveness\" with respect to science).", "labels": [], "entities": [{"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9977158308029175}, {"text": "precision", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.9959502220153809}]}, {"text": "This measure is similar to recall at the point P=80% on the PR curve, except measured against a domain-specific sample of data that reflects the distribution of the target domain knowledge.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9986989498138428}]}, {"text": "Comprehensiveness thus gives us an approximate notion of the completeness of the KB for (tuple-expressible) facts in our target domain, something that has been lacking in earlier KB construction research.", "labels": [], "entities": [{"text": "KB construction", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.8358376324176788}]}, {"text": "We show that our KB has comprehensiveness (recall of domain facts at >80% precision) of 23% with respect to science, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources.", "labels": [], "entities": [{"text": "comprehensiveness", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.994441568851471}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9921086430549622}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9966391324996948}]}, {"text": "We are making the KB publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we will focus on usefulness and correctness of our canonical schema induction method.", "labels": [], "entities": [{"text": "correctness", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9658553004264832}]}, {"text": "The parameters of the ILP model (see Equation 1) i.e., \u03bb 1 . .", "labels": [], "entities": []}, {"text": "\u03bb 5 and \u03b4 are tuned based on sample accuracy of individual feature sources and using a small schema mapping problem with schemas applicable to vocabulary types Animal and Body-Part.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8330888152122498}]}, {"text": "Further, with O(n 3 ) transitivity constraints we could not successfully solve a single ILP problem with 100 schemas within a time limit of 1 hour.", "labels": [], "entities": []}, {"text": "Whereas, when we rewrite them with O(n 2 ) constraints as explained in Section 4.3, we could solve 443 ILP sub-problems within 6 minutes with average runtime per ILP being 800 msec.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Precision and coverage of tuple-expressible elementary science knowledge by existing resources vs. our KB.  Precision estimates are within +/-3% with 95% confidence interval.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9842409491539001}, {"text": "Precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.991378903388977}]}, {"text": " Table 4: Evaluation of KB at different stages of extrac- tion. Precision estimates are within +/-3% with 95% con- fidence interval.", "labels": [], "entities": [{"text": "Precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9980134963989258}, {"text": "con- fidence interval", "start_pos": 110, "end_pos": 131, "type": "METRIC", "confidence": 0.9225627779960632}]}, {"text": " Table 6: CASI canonicalizes five times more schemas than AMIE*, and also achieves a small (9%) increase in preci- sion, demonstrating how additional knowledge resources can help the canonicalization process (Section 4.2). Precision  estimates are within +/-4% with 95% confidence interval.", "labels": [], "entities": [{"text": "CASI canonicalizes", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8551344573497772}, {"text": "Precision", "start_pos": 223, "end_pos": 232, "type": "METRIC", "confidence": 0.9812483787536621}]}]}