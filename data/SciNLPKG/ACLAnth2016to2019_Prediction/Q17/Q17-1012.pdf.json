{"title": [], "abstractContent": [{"text": "Sequential LSTMs have been extended to model tree structures, giving competitive results fora number of tasks.", "labels": [], "entities": []}, {"text": "Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes.", "labels": [], "entities": []}, {"text": "This is different from sequential LSTMs, which contain references to input words for each node.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node.", "labels": [], "entities": []}, {"text": "In addition, enabled by head lexicaliza-tion, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure.", "labels": [], "entities": []}, {"text": "Experiments show that both extensions give better representations of tree structures.", "labels": [], "entities": []}, {"text": "Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.9265960057576498}, {"text": "TREC question type classification task", "start_pos": 112, "end_pos": 150, "type": "TASK", "confidence": 0.8313106060028076}]}], "introductionContent": [{"text": "Both sequence structured and tree structured neural models have been applied to NLP problems.", "labels": [], "entities": []}, {"text": "Seminal work uses convolutional neural networks, recurrent neural networks) and recursive neural networks) for sequence and tree modeling.", "labels": [], "entities": [{"text": "sequence and tree modeling", "start_pos": 111, "end_pos": 137, "type": "TASK", "confidence": 0.6038535237312317}]}, {"text": "Long short-term memory (LSTM) networks have significantly improved accuracies in a variety of sequence tasks) compared to vanilla recurrent neural networks.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9914721846580505}]}, {"text": "Addressing diminishing gradients effectively, they have been extended to tree structures, achieving promising results for tasks such as syntactic language modeling ( , sentiment analysis () and relation extraction.", "labels": [], "entities": [{"text": "syntactic language modeling", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.6805880864461263}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.9004198014736176}, {"text": "relation extraction", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.9215142726898193}]}, {"text": "Depending on the node type, typical tree structures in NLP can be categorized to constituent trees and dependency trees.", "labels": [], "entities": []}, {"text": "A salient difference between the two types of tree structures is in the node.", "labels": [], "entities": []}, {"text": "While dependency tree nodes are input words themselves, constituent tree nodes represent syntactic constituents.", "labels": [], "entities": []}, {"text": "Only leaf nodes in constituent trees correspond to words.", "labels": [], "entities": []}, {"text": "Though LSTM structures have been developed for both types of trees above, we investigate constituent trees in this paper.", "labels": [], "entities": []}, {"text": "There are three existing methods for constituent tree LSTMs (, which make essentially the same extension from sequence structured LSTMs.", "labels": [], "entities": []}, {"text": "We take the method of  as our baseline.", "labels": [], "entities": []}, {"text": "shows the sequence structured LSTM of Hochreiter and and the treestructured LSTM of , illustrating the input (x), cell (c) and hidden (h) nodes at a certain time step t.", "labels": [], "entities": []}, {"text": "The most important difference between(a) and(b) is the branching factor.", "labels": [], "entities": []}, {"text": "While a cell in the sequence structure LSTM depends on the single previous hidden node, a cell in the tree-structured LSTM depends on a left hidden node and aright hidden node.", "labels": [], "entities": []}, {"text": "Such tree-structured extensions of the sequence structured LSTM assume that the constituent tree is binarized, building hidden nodes from the input words in the bottom-up direction.", "labels": [], "entities": []}, {"text": "The leaf node structure is shown in(c).", "labels": [], "entities": []}, {"text": "A second salient difference between the two types of LSTMs is the modeling of input words.", "labels": [], "entities": []}, {"text": "While each cell in the sequence structure LSTM directly depends on its corresponding input word ((a)), only leaf cells in the tree structure LSTM directly depend on corresponding input words ().", "labels": [], "entities": []}, {"text": "This corresponds well to the constituent tree structure, where there is no direct association between non-leaf constituent nodes and input words.", "labels": [], "entities": []}, {"text": "However, it leaves the tree structure a degraded version of a perfect binary-branching variation of the sequence-structure LSTM, with one important source of information (i.e. words) missing in forming a cell).", "labels": [], "entities": []}, {"text": "We fill this gap by proposing an extension to the tree LSTM model, injecting lexical information into every node in the tree.", "labels": [], "entities": []}, {"text": "Our method takes inspiration from work on head-lexicalization, which shows that each node in a constituent tree structure is governed by ahead word.", "labels": [], "entities": []}, {"text": "As shown in, the headword for the verb phrase \"visited Mary\" is \"visited\", and the headword of the adverb phrase \"this afternoon\" is \"afternoon\".", "labels": [], "entities": []}, {"text": "Research has shown that headword information can significantly improve the performance of syntactic parsing).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7645408511161804}]}, {"text": "Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in  Traditional head-lexicalization relies on specific rules, typically extracting heads from constituent treebanks according to certain grammar formalisms.", "labels": [], "entities": []}, {"text": "For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent, which is language-and formalism-dependent.", "labels": [], "entities": []}, {"text": "Based on such head lexicalization, we further make a bidirectional extension of the tree structured LSTM, propagating information in the top-down direction as well as the bottom-up direction.", "labels": [], "entities": []}, {"text": "This is analogous to the bidirectional extension of sequence structured LSTMs, which are commonly used for NLP tasks such as speech recognition (, sentiment analysis () and machine translation tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7373114973306656}, {"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8738344311714172}, {"text": "machine translation tasks", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.8573963642120361}]}, {"text": "Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of . We achieve the best reported results for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8184950053691864}, {"text": "accuracies", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9748460650444031}, {"text": "sentiment classification", "start_pos": 245, "end_pos": 269, "type": "TASK", "confidence": 0.9348842203617096}]}, {"text": "Interestingly, the head lexical information that is learned automatically from the sentiment treebank consists of both syntactic head information and key sentiment word information.", "labels": [], "entities": []}, {"text": "This shows the advantage of automatic head-finding as compared with rule-based head lexicalization.", "labels": [], "entities": []}, {"text": "We make our code available under GPL at https://github.com/ zeeeyang/lexicalized_bitreelstm.", "labels": [], "entities": []}], "datasetContent": [{"text": "The effectiveness of our model is tested mainly on a sentiment classification task and a question type classification task.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.9156400362650553}, {"text": "question type classification task", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.7094104960560799}]}], "tableCaptions": [{"text": " Table 1: Test set accuracies for sentiment classifica- tion tasks.", "labels": [], "entities": [{"text": "sentiment classifica- tion tasks", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.9009306788444519}]}, {"text": " Table 2: TREC question type classification results.", "labels": [], "entities": [{"text": "TREC question type classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8250373154878616}]}, {"text": " Table 3: Averaged training time over 30 iterations.", "labels": [], "entities": [{"text": "Averaged", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.974254846572876}]}, {"text": " Table 4: Effect of model size.", "labels": [], "entities": []}, {"text": " Table 7: Reranking results on WSJ test set.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9594894647598267}, {"text": "WSJ test set", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9715062578519186}]}]}