{"title": [{"text": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets", "labels": [], "entities": [{"text": "Replicability Analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8999672830104828}]}], "abstractContent": [{"text": "With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups.", "labels": [], "entities": []}, {"text": "However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions.", "labels": [], "entities": []}, {"text": "In this paper we propose a Replicability Analysis framework fora statistically sound analysis of multiple comparisons between algorithms for NLP tasks.", "labels": [], "entities": [{"text": "Replicability Analysis", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9060006439685822}]}, {"text": "We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging , cross-domain sentiment classification and word similarity prediction.", "labels": [], "entities": [{"text": "multi-domain dependency parsing", "start_pos": 195, "end_pos": 226, "type": "TASK", "confidence": 0.6509229143460592}, {"text": "POS tagging", "start_pos": 241, "end_pos": 252, "type": "TASK", "confidence": 0.7461248636245728}, {"text": "cross-domain sentiment classification", "start_pos": 255, "end_pos": 292, "type": "TASK", "confidence": 0.7309393684069315}, {"text": "word similarity prediction", "start_pos": 297, "end_pos": 323, "type": "TASK", "confidence": 0.8281435569127401}]}], "introductionContent": [{"text": "The field of Natural Language Processing (NLP) is going through the data revolution.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.7729737559954325}]}, {"text": "With the persistent increase of the heterogeneous web, for the first time inhuman history, written language from multiple languages, domains, and genres is now abundant.", "labels": [], "entities": []}, {"text": "Naturally, the expectations from NLP algorithms also grow and evaluating anew algorithm on as many languages, domains, and genres as possible is becoming a de-facto standard.", "labels": [], "entities": []}, {"text": "For example, the phrase structure parsers of and were mostly evaluated on the Wall Street Journal Penn Treebank, consisting of written, edited English text of economic news.", "labels": [], "entities": [{"text": "phrase structure parsers", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.623314638932546}, {"text": "Wall Street Journal Penn Treebank", "start_pos": 78, "end_pos": 111, "type": "DATASET", "confidence": 0.9554376721382141}]}, {"text": "In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (, and additional challenges, such as the shared task on parsing multiple English Web domains, are continuously proposed.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8060034215450287}, {"text": "CoNLL 2006-2007 shared tasks", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.8905003517866135}, {"text": "multilingual dependency parsing", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.5589169859886169}, {"text": "parsing multiple English Web domains", "start_pos": 209, "end_pos": 245, "type": "TASK", "confidence": 0.8798999905586242}]}, {"text": "Despite the growing number of evaluation tasks, the analysis toolbox employed by NLP researchers has remained quite stable.", "labels": [], "entities": []}, {"text": "Indeed, inmost experimental NLP papers, several algorithms are compared on a number of datasets where the performance of each algorithm is reported together with per-dataset statistical significance figures.", "labels": [], "entities": []}, {"text": "However, with the growing number of evaluation datasets, it becomes more challenging to draw comprehensive conclusions from such comparisons.", "labels": [], "entities": []}, {"text": "This is because although the probability of drawing an erroneous conclusion from a single comparison is small, with multiple comparisons the probability of making one or more false claims maybe very high.", "labels": [], "entities": []}, {"text": "The goal of this paper is to provide the NLP community with a statistical analysis framework, which we term Replicability Analysis, which will allow us to draw statistically sound conclusions in evaluation setups that involve multiple comparisons.", "labels": [], "entities": [{"text": "Replicability Analysis", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7951176464557648}]}, {"text": "The classical goal of replicability analysis is to examine the consistency of findings across studies in order to address the basic dogma of science, that a find-471 ing is more convincingly true if it is replicated in at least one more study).", "labels": [], "entities": [{"text": "replicability analysis", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9772988855838776}]}, {"text": "We adapt this goal to NLP, where we wish to ascertain the superiority of one algorithm over another across multiple datasets, which may come from different languages, domains, and genres.", "labels": [], "entities": []}, {"text": "Finding that one algorithm outperforms another across domains gives a sense of consistency to the results and positive evidence that the better performance is not specific to a selected setup.", "labels": [], "entities": [{"text": "consistency", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9792187809944153}]}, {"text": "In this work we address two questions: (1) Counting: For how many datasets does a given algorithm outperform another? and (2) Identification: What are these datasets?", "labels": [], "entities": [{"text": "Identification", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.9543383121490479}]}, {"text": "When comparing two algorithms on multiple datasets, NLP papers often answer informally the questions we address in this work.", "labels": [], "entities": []}, {"text": "In some cases this is done without any statistical analysis, by simply declaring better performance of a given algorithm for datasets where its performance measure is better than that of another algorithm, and counting these datasets.", "labels": [], "entities": []}, {"text": "In other cases answers are based on the p-values from statistical tests performed for each dataset: declaring better performance for datasets with p-value below the significance level (e.g. 0.05) and counting these datasets.", "labels": [], "entities": []}, {"text": "While it is clear that the first approach is not statistically valid, it seems that our community is not aware of the fact that the second approach, which may seem statistically sound, is not valid as well.", "labels": [], "entities": []}, {"text": "This may lead to erroneous conclusions, which result in adopting new (and probably complicated) algorithms, while they are not better than previous (probably more simple) ones.", "labels": [], "entities": []}, {"text": "In this work, we demonstrate this problem and show that it becomes more severe as the number of evaluation sets grows, which seems to be the current trend in NLP.", "labels": [], "entities": []}, {"text": "We adopt a known general statistical methodology for addressing the counting (question (1)) and identification (question (2)) problems, by choosing the tests and procedures which are valid for 2 \"Replicability\" is sometimes referred to as \"reproducibility\".", "labels": [], "entities": []}, {"text": "In recent NLP work the term reproducibility was used when trying to get identical results on the same data.", "labels": [], "entities": []}, {"text": "In this paper, we adopt the meaning of \"replicability\" and its distinction from \"reproducibility\" from Peng (2011) and and refer to replicability analysis as the effort to show that a finding is consistent over different datasets from different domains or languages, and is not idiosyncratic to a specific scenario.", "labels": [], "entities": [{"text": "replicability analysis", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.9199638664722443}]}, {"text": "situations encountered in NLP problems, and giving specific recommendations for such situations.", "labels": [], "entities": []}, {"text": "Particularly, we first demonstrate (Section 3) that the current prominent approach in the NLP literature, identifying the datasets for which the difference between the performance of the algorithms reaches a predefined significance level according to some statistical significance test, does not guarantee to bound the probability to make at least one erroneous claim.", "labels": [], "entities": []}, {"text": "Hence this approach is error-prone when the number of participating datasets is large.", "labels": [], "entities": []}, {"text": "We thus propose an alternative approach (Section 4).", "labels": [], "entities": []}, {"text": "For question (1), we adopt the approach of to replicability analysis of multiple studies, based on the partial conjunction framework of.", "labels": [], "entities": [{"text": "replicability analysis", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9758196473121643}]}, {"text": "This analysis comes with a guarantee that the probability of overestimating the true number of datasets with effect is upper bounded by a predefined constant.", "labels": [], "entities": []}, {"text": "For question (2), we motivate a multiple testing procedure which guarantees that the probability of making at least one erroneous claim on the superiority of one algorithm over another is upper bounded by a predefined constant.", "labels": [], "entities": []}, {"text": "In Sections 5 and 6 we demonstrate how to apply the proposed frameworks to two synthetic data toy examples and four NLP applications: multidomain dependency parsing, multilingual POS tagging, cross-domain sentiment classification, and word similarity prediction with word embedding models.", "labels": [], "entities": [{"text": "multidomain dependency parsing", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.6873726050059}, {"text": "POS tagging", "start_pos": 179, "end_pos": 190, "type": "TASK", "confidence": 0.6954151540994644}, {"text": "cross-domain sentiment classification", "start_pos": 192, "end_pos": 229, "type": "TASK", "confidence": 0.7367891867955526}, {"text": "word similarity prediction", "start_pos": 235, "end_pos": 261, "type": "TASK", "confidence": 0.7894516587257385}]}, {"text": "Our results demonstrate that the current practice in NLP for addressing our questions is error-prone, and illustrate the differences between it and the proposed statistically sound approach.", "labels": [], "entities": []}, {"text": "We hope that this work will encourage our community to increase the number of standard evaluation setups per task when appropriate (e.g. including additional languages and domains), possibly paving the way to hundreds of comparisons per study.", "labels": [], "entities": []}, {"text": "This is due to two main reasons.", "labels": [], "entities": []}, {"text": "First, replicability analysis is a statistically sound framework that allows a researcher to safely draw valid conclusions with well defined statistical guarantees.", "labels": [], "entities": [{"text": "replicability analysis", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.9913595020771027}]}, {"text": "Moreover, this framework provides a means of summarizing a large number of experiments with a handful of easily interpretable numbers (e.g., see).", "labels": [], "entities": []}, {"text": "This allows researchers to report results over a large number of comparisons in a concise manner, delving into details of particular comparisons when necessary.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recall that the number of datasets where algorithm A outperforms algorithm B (denoted with kin Definition 1) is the true number of false null hypotheses in our problem.", "labels": [], "entities": []}, {"text": "proposed to estimate k to be the largest u for which is rejected.", "labels": [], "entities": []}, {"text": "Specifically, the estimator\u02c6kestimator\u02c6 estimator\u02c6k is defined as follows: where p and \u03b1 is the desired upper bound on the probability to overestimate the true k.", "labels": [], "entities": []}, {"text": "It is guaranteed that P( \u02c6 k > k) \u2264 \u03b1 as long as the p\u2212value combination method used for constructing p u/N is valid for the given dependency across the test statistics.", "labels": [], "entities": []}, {"text": "When\u02c6kWhen\u02c6 When\u02c6k is based on p u/N Bonf erroni it is denoted with\u02c6k with\u02c6 with\u02c6k Bonf erroni ; when it is based on p u/N Fisher , it is denoted with\u02c6kwith\u02c6 with\u02c6k Fisher . A crucial practical consideration, when choosing between\u02c6kbetween\u02c6 between\u02c6k Bonf erroni and\u02c6kand\u02c6 and\u02c6k Fisher , is the assumed dependency between the datasets.", "labels": [], "entities": []}, {"text": "As discussed in Section 4.1, p u/N Fisher is recommended when the participating datasets are assumed to be independent; when this assumption cannot be made, only p u/N Bonf erroni is appropriate.", "labels": [], "entities": []}, {"text": "As th\u00ea k estimators are based on the respective p u/N s, the same considerations hold when choosing between them.", "labels": [], "entities": []}, {"text": "With th\u00ea k estimators, one can answer the counting question of Section 1, reporting that algorithm A is better than algorithm B in at least\u02c6kleast\u02c6 least\u02c6k out of N datasets with a confidence level of 1 \u2212 \u03b1.", "labels": [], "entities": []}, {"text": "Regarding the identification question, a natural approach would be to declare th\u00ea k datasets with the smallest p\u2212values as those for which the effect holds.", "labels": [], "entities": [{"text": "identification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9595153331756592}]}, {"text": "However, with\u02c6kwith\u02c6 with\u02c6k Fisher this approach does not guarantee control over type I errors.", "labels": [], "entities": []}, {"text": "In contrast, for\u02c6k for\u02c6 for\u02c6k Bonf erroni , the above approach comes with such guarantees, as described in the next section.", "labels": [], "entities": []}, {"text": "As demonstrated in Section 3.2, identifying the datasets with p\u2212value below the nominal significance level and declaring them as those where algorithm A is better than B may lead to a very high number of erroneous claims.", "labels": [], "entities": []}, {"text": "A variety of methods exist for addressing this problem.", "labels": [], "entities": []}, {"text": "A classical and very simple method for addressing this problem is named the Bonferroni's procedure, which compensates for the increased probability of making at least one type I error by testing each individual hypothesis at a significance level of \u03b1 = \u03b1/N , where \u03b1 is the predefined bound on this probability and N is the number of hypotheses tested.", "labels": [], "entities": []}, {"text": "6 While Bonferroni's procedure is valid for any dependency among the p\u2212values, the probability of detecting a true effect using this procedure is often very low, because of its strict p\u2212value threshold.", "labels": [], "entities": []}, {"text": "Many other procedures controlling the above or other error criteria, and having less strict p\u2212value thresholds, have been proposed.", "labels": [], "entities": []}, {"text": "Below we advocate one of these methods: the Holm procedure.", "labels": [], "entities": []}, {"text": "This is a simple p\u2212value based procedure that is concordant with the partial conjunction analysis when p u/N Bonf erroni is used in that analysis.", "labels": [], "entities": []}, {"text": "Importantly for NLP applications, Holm controls the probability of making at least one type I error for any type of dependency between the participating datasets (see a demonstration in Section 6).", "labels": [], "entities": []}, {"text": "Let \u03b1 be the desired upper bound on the probability that at least one false rejection occurs, let p (1) \u2264 p (2) \u2264 . .", "labels": [], "entities": []}, {"text": "\u2264 p (N ) be the ordered p\u2212values and let the associated hypotheses be H (1) . .", "labels": [], "entities": []}, {"text": "H (N ) . The Holm procedure for identifying the datasets with a significant effect is given below.", "labels": [], "entities": []}, {"text": "list of null hypotheses; the corresponding datasets are those we return in response to the identification question of Section 1.", "labels": [], "entities": []}, {"text": "Note that the Holm procedure rejects a subset of hypotheses with p-value below \u03b1.", "labels": [], "entities": []}, {"text": "Each p-value is compared to a threshold which is smaller or equal to \u03b1 and depends on the number of evaluation datasets N.", "labels": [], "entities": []}, {"text": "The dependence of the thresholds on N can be intuitively explained as follows: the probability of making one or more erroneous claims may increase with N, as demonstrated in Section 3.2.", "labels": [], "entities": []}, {"text": "Therefore, in order to bound this probability by a pre-specified level \u03b1, the thresholds for p-values should depend on N.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 summarizes the replicability analysis results  while Table 2 -5 present task specific performance  measures and p\u2212values.", "labels": [], "entities": [{"text": "replicability analysis", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9386849105358124}]}, {"text": " Table 2: UAS results for multi-domain dependency parsing. p\u2212values are in parentheses.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.679558515548706}, {"text": "multi-domain dependency parsing", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6505819459756216}]}, {"text": " Table 3: Multilingual POS tagging accuracy for the MIM- ICK and the Char\u2192Tag models.  *  indicates languages  identified by the Holm procedure with \u03b1 = 0.05 .", "labels": [], "entities": [{"text": "Multilingual POS tagging", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.4307469626267751}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9375015497207642}, {"text": "MIM- ICK", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.7289192875226339}]}, {"text": " Table 5: Spearman's \u03c1 values for the best performing pre- dict model (W2V-CBOW) of (", "labels": [], "entities": []}]}