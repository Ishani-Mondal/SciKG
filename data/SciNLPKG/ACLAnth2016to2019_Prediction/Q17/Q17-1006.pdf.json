{"title": [], "abstractContent": [{"text": "We present a probabilistic model of phono-tactics, the set of well-formed phoneme sequences in a language.", "labels": [], "entities": []}, {"text": "Unlike most computational models of phonotactics (Hayes and Wilson, 2008; Goldsmith and Riggle, 2012), we take a fully generative approach, model-ing a process where forms are built up out of subparts by phonologically-informed structure building operations.", "labels": [], "entities": []}, {"text": "We learn an inventory of subparts by applying stochastic memo-ization (Johnson et al., 2007; Goodman et al., 2008) to a generative process for phonemes structured as an and-or graph, based on concepts of feature hierarchy from generative phonology (Clements, 1985; Dresher, 2009).", "labels": [], "entities": []}, {"text": "Subparts are combined in away that allows tier-based feature interactions.", "labels": [], "entities": []}, {"text": "We evaluate our models' ability to capture phonotactic distributions in the lexicons of 14 languages drawn from the WOLEX corpus (Graff, 2012).", "labels": [], "entities": [{"text": "WOLEX corpus (Graff, 2012)", "start_pos": 116, "end_pos": 142, "type": "DATASET", "confidence": 0.9185543656349182}]}, {"text": "Our full model robustly assigns higher probabilities to held-out forms than a sophisticated N-gram model for all languages.", "labels": [], "entities": []}, {"text": "We also present novel analyses that probe model behavior in more detail.", "labels": [], "entities": []}], "introductionContent": [{"text": "People have systematic intuitions about which sequences of sounds would constitute likely or unlikely words in their language: Although blick is not an English word, it sounds like it could be, while bnick does not.", "labels": [], "entities": []}, {"text": "Such intuitions reveal that speakers are aware of the restrictions on sound sequences which can makeup possible morphemes in their language-the phonotactics of the language.", "labels": [], "entities": []}, {"text": "Phonotactic restrictions mean that each language uses only a subset of the logically, or even articulatorily, possible strings of phonemes.", "labels": [], "entities": []}, {"text": "Admissible phoneme combinations, on the other hand, typically recur in multiple morphemes, leading to redundancy.", "labels": [], "entities": []}, {"text": "It is widely accepted that phonotactic judgments maybe gradient: the nonsense word blick is better as a hypothetical English word than bwick, which is better than bnick ().", "labels": [], "entities": []}, {"text": "To account for such graded judgements, there have been a variety of probabilistic (or, more generally, weighted) models proposed to handle phonotactic learning and generalization over the last two decades (see and below for review).", "labels": [], "entities": []}, {"text": "However, inspired by optimality-theoretic approaches to phonology, the most linguistically informed and successful such models have been constraint-basedformulating the problem of phonotactic generalization in terms of restrictions that penalize illicit combinations of sounds (e.g., ruling out * bn-).", "labels": [], "entities": [{"text": "phonotactic generalization", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.717144250869751}]}, {"text": "In this paper, by contrast, we adopt a generative approach to modeling phonotactic structure.", "labels": [], "entities": []}, {"text": "Our approach harkens back to early work on the sound structure of lexical items which made use of morpheme structure rules or conditions.", "labels": [], "entities": []}, {"text": "Such approaches explicitly attempted to model the 73 redudancy within the set of allowable lexical forms in a language.", "labels": [], "entities": []}, {"text": "We adopt a probabilistic version of this idea, conceiving of the phonotactic system as the component of the linguistic system which generates the phonological form of lexical items such as words and morphemes.", "labels": [], "entities": []}, {"text": "Our system learns inventories of reusable phonotactically licit structures from existing lexical items, and assembles new lexical items by combining these learned phonotactic patterns using phonologically plausible structurebuilding operations.", "labels": [], "entities": []}, {"text": "Thus, instead of modeling phonotactic generalizations in terms of constraints, we treat the problem as a problem of learning language specific inventories of phonological units and language specific biases on how these phones are likely to be combined.", "labels": [], "entities": []}, {"text": "Although there have been a number of earlier generative models of phonotactic structure (see Section 4) these models have mostly used relatively simplistic or phonologically implausible representations of phones and phonological structure-building.", "labels": [], "entities": []}, {"text": "By contrast, our model is built around three representational assumptions inspired by the generative phonology literature.", "labels": [], "entities": []}, {"text": "First, we capture sparsity in the space of feature-specifications of phonemes by using feature dependency graphs-an idea inspired by work on feature geometries and the contrastive hierarchy.", "labels": [], "entities": []}, {"text": "Second, our system can represent phonotactic generalizations not only at the level of fully specified segments, but also allows the storage and reuse of subsegments, inspired by the autosegments and class nodes of autosegmental phonology.", "labels": [], "entities": []}, {"text": "Finally, also inspired by autosegmental phonology, we make use of a structure-building operation which is senstitive to tier-based contextual structure.", "labels": [], "entities": []}, {"text": "To model phonotactic learning, we make use of tools from Bayesian nonparametric statistics.", "labels": [], "entities": []}, {"text": "In particular, we make use of the notion of lexical memoization (?;-the idea that languagespecific generalizations can be captured by the storage and reuse of frequent patterns from a linguisti-1 Ultimately, we conceive of phonotactics as the module of phonology which generates the underlying forms of lexical items, which are then subject to phonological transformations (i.e., transductions).", "labels": [], "entities": []}, {"text": "In this work, however, we do not attempt to model transformations from underlying to surface forms.", "labels": [], "entities": []}, {"text": "In our case, this amounts to the idea that an inventory of segments and subsegments can be acquired by a learner that stores and reuses commonly occuring segments in particular, phonologically relevant contexts.", "labels": [], "entities": []}, {"text": "In short, we view the problem of learning the phoneme inventory as one of concentrating probability mass on the segments which have been observed before, and the problem of phonotactic generalization as learning which (sub-)segments are likely in particular tierbased phonological contexts.", "labels": [], "entities": [{"text": "phonotactic generalization", "start_pos": 173, "end_pos": 199, "type": "TASK", "confidence": 0.7231544256210327}]}], "datasetContent": [{"text": "Here we evaluate some of the design decisions of our model and compare it to a baseline N-gram model and to a widely-used constraint-based model, BLICK.", "labels": [], "entities": [{"text": "BLICK", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.7157378792762756}]}, {"text": "In order to probe model behavior, we also present evaluations on artificial data, and a sampling of \"representative forms\" preferred by one model as compared to another.", "labels": [], "entities": []}, {"text": "Our model consists of structure-building operations over a learned inventory of subsegments.", "labels": [], "entities": []}, {"text": "If our model can exploit more repeated structure in phonological forms than the N-gram model or constraintbased models, then it should assign higher probabilities to forms.", "labels": [], "entities": []}, {"text": "The log probability of a form under a model corresponds to the description length of that form under the model; if a model assigns a higher log probability to a form, that means the model is capable of compressing the form more than other models.", "labels": [], "entities": []}, {"text": "Therefore, we compare models on their ability to assign high probabilities to phonological forms, as in Goldsmith and Riggle (2012).", "labels": [], "entities": []}, {"text": "We are interested in discovering the extent to which each model component described above-feature dependency graphs (Section 2.1), class node structure (Section 2.2), and tier-based conditioning (Section 2.4)-contributes to the ability of the model to explain wordforms.", "labels": [], "entities": []}, {"text": "To evaluate the contribution of feature dependency graphs, we compare our models with a baseline N-gram model, which represents phonemes as atomic units.", "labels": [], "entities": []}, {"text": "For this N-gram model, we use a Hierarchical Dirichlet Process with n = 3.", "labels": [], "entities": []}, {"text": "To evaluate feature dependency graphs with and without articulated class node structure, we compare models using the graph shown in (the minimal structure required to produce wellformed phonemes) to models with the graph shown in, which includes phonologically motivated \"class nodes\".", "labels": [], "entities": []}, {"text": "To evaluate tier-based conditioning, we compare models with the conditioning described in Sections 2.4 and 3.3 to models where all decisions are conditioned on the full featural specification of the previous n \u2212 1 phonemes.", "labels": [], "entities": []}, {"text": "This allows us to isolate improvements due to tier-based conditioning beyond improvements from the feature hierarchy.", "labels": [], "entities": []}, {"text": "These feature dependency graphs differ from those in the exposition in Section 2 in that they do not include a MANNER feature; but rather treat vowel as a possible value of MANNER.", "labels": [], "entities": []}, {"text": "Here we test whether the different model configurations described above assign high probability to held-out forms.", "labels": [], "entities": []}, {"text": "This tests the models' ability to generalize beyond their training data.", "labels": [], "entities": []}, {"text": "We train each model on 2500 randomly selected wordforms from a WOLEX dictionary, and compute posterior predictive probabilities for the remaining wordforms from the final state of the model.: Average log posterior predictive probability of a held-out form.", "labels": [], "entities": [{"text": "WOLEX dictionary", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9171724319458008}]}, {"text": "\"ngram\" is the DP Backoff 3-gram model.", "labels": [], "entities": []}, {"text": "\"flat\" models use the feature dependency graph in.", "labels": [], "entities": []}, {"text": "\"cl. node\" models use the graph in.", "labels": [], "entities": []}, {"text": "See text for motivations of these graphs.", "labels": [], "entities": []}, {"text": "\"no tiers\" models condition each decision on the previous phoneme, rather than on tiers of previous features.", "labels": [], "entities": []}, {"text": "Asterisks indicate statistical significance according to a t-test comparing with the scores under the N-gram model.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.7508851587772369}]}, {"text": "* = p < .05; ** = p < .001.", "labels": [], "entities": []}, {"text": "shows the average probability of a heldout word under our models and under the N-gram model for one model run.", "labels": [], "entities": []}, {"text": "For all languages, we get a statistically significant increase in probabilities by adopting the autosegmental model with class nodes and tier-based conditioning.", "labels": [], "entities": []}, {"text": "Model variants without either component do not significantly outperform the N-gram model except in Chinese.", "labels": [], "entities": []}, {"text": "The combination of class nodes and tier-based conditioning results in model improvements beyond the contributions of the individual features.", "labels": [], "entities": []}, {"text": "Our model outperforms the N-gram model in predicting held-out forms, but it remains to be shown that this performance is due to capturing the kinds of linguistic intuitions discussed in Section 2.", "labels": [], "entities": []}, {"text": "An alternative possibility is that the Autosegmental Ngram model, which has many more parameters than a plain N-gram model, can simply learn a more accurate model of any sequence, even if that sequence has none of the structure discussed above.", "labels": [], "entities": []}, {"text": "To evaluate this possibility, we compare the performance of our model in predicting held-out linguistic forms to its performance in predicting held-out forms from artificial lexicons which expressly do not have the The mean standard deviation perform of log probabilities over 50 runs of the full model ranged from .09 for Amharic to .23 for Dutch.", "labels": [], "entities": []}, {"text": "linguistic structure we are interested in.", "labels": [], "entities": []}, {"text": "If the autosegmental model outperforms the Ngram model even on artificial data with no phonological structure, then its performance on the real linguistic data in Section 5.3 might be overfitting.", "labels": [], "entities": []}, {"text": "On the other hand, if the autosegmental model does better on real data but not artificial data, then we can conclude that it is picking upon some real distinctive structure of that data.", "labels": [], "entities": []}, {"text": "For each real lexicon Lr , we generate an artificial lexicon La by training a DP 3-gram model on Lr and forward-sampling |L r | forms.", "labels": [], "entities": []}, {"text": "Additionally, the forms in La are constrained to have the same distribution over lengths as the forms in Lr . The resulting lexicons have no tier-based or featural interactions except as they appear by chance from the N-gram model trained on these lexica.", "labels": [], "entities": []}, {"text": "For each La we then train our models on the first 2500 forms and score the probabilities of the held-out forms, the same procedure as in Section 5.3.", "labels": [], "entities": []}, {"text": "We ran this procedure for all the lexicons shown in.", "labels": [], "entities": []}, {"text": "For all but one lexicon, we find that the autosegmental models do not significantly outperform the N-gram models on artificial data.", "labels": [], "entities": []}, {"text": "The exception is Mandarin Chinese, where the average log probability of an artificial form is \u221213.81 under the N-gram model and \u221213.71 under the full autosegmental model.", "labels": [], "entities": []}, {"text": "The result suggests that the anomalous behavior of Mandarin Chinese in Section 5.3 maybe due to overfitting.", "labels": [], "entities": [{"text": "Section 5.3", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.8032512068748474}]}, {"text": "When exposed to data that explicitly does not have autosegmental structure, the model is not more accurate than a plain sequence model for almost all languages.", "labels": [], "entities": []}, {"text": "But when exposed to real linguistic data, the model is more accurate.", "labels": [], "entities": []}, {"text": "This result provides evidence that the generative model developed in Section 2 captures true distributional properties of lexicons that are absent in N-gram distributions, such as featural and tier-based interactions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average log posterior predictive probability of  an English form of fixed length under BLICK and our  models.", "labels": [], "entities": [{"text": "BLICK", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.7647489309310913}]}]}