{"title": [{"text": "Cross-Lingual Syntactic Transfer with Limited Resources", "labels": [], "entities": [{"text": "Cross-Lingual Syntactic Transfer", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6883522272109985}]}], "abstractContent": [{"text": "We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available.", "labels": [], "entities": [{"text": "cross-lingual syntactic transfer of dependency parsers", "start_pos": 46, "end_pos": 100, "type": "TASK", "confidence": 0.7524255166451136}]}, {"text": "This method makes use of three steps: 1) a method for deriving cross-lingual word clusters , which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015).", "labels": [], "entities": []}, {"text": "Experiments show improvements over the state-of-the-art in several languages used in previous work, in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 208, "end_pos": 223, "type": "DATASET", "confidence": 0.9809526205062866}]}, {"text": "Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9938546121120453}]}, {"text": "We conclude with results on 38 datasets from the Universal Dependencies corpora.", "labels": [], "entities": [{"text": "Universal Dependencies corpora", "start_pos": 49, "end_pos": 79, "type": "DATASET", "confidence": 0.8436604340871176}]}], "introductionContent": [{"text": "Creating manually-annotated syntactic treebanks is an expensive and time consuming task.", "labels": [], "entities": []}, {"text": "Recently there has been a great deal of interest in cross-lingual syntactic transfer, where a parsing model is trained for some language of interest, using only treebanks in other languages.", "labels": [], "entities": [{"text": "cross-lingual syntactic transfer", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.7012830575307211}]}, {"text": "There is a clear motivation for this in building parsing models for languages for which treebank data is unavailable.", "labels": [], "entities": []}, {"text": "Methods * On leave at Google Inc.", "labels": [], "entities": []}, {"text": "for syntactic transfer include annotation projection methods (, learning of delexicalized models on universal treebanks, treebank translation ( and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries).", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7555255889892578}, {"text": "annotation projection", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.737145721912384}, {"text": "treebank translation", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.7560370862483978}]}, {"text": "This paper considers the problem of cross-lingual syntactic transfer with limited resources of monolingual and translation data.", "labels": [], "entities": [{"text": "cross-lingual syntactic transfer", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6552402575810751}]}, {"text": "Specifically, we use the Bible corpus of as a source of translation data, and Wikipedia as a source of monolingual data.", "labels": [], "entities": [{"text": "Bible corpus", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8754550218582153}]}, {"text": "We deliberately limit ourselves to the use of Bible translation data because it is available fora very broad set of languages: the data from Christodouloupoulos and Steedman (2014) includes data from 100 languages.", "labels": [], "entities": [{"text": "Bible translation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7129816710948944}]}, {"text": "The Bible data contains a much smaller set of sentences (around 24,000) than other translation corpora, for example Europarl (), which has around 2 million sentences per language pair.", "labels": [], "entities": [{"text": "Bible data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9230815172195435}, {"text": "Europarl", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.9689025282859802}]}, {"text": "This makes it a considerably more challenging corpus to work with.", "labels": [], "entities": []}, {"text": "Similarly, our choice of Wikipedia as the source of monolingual data is motivated by the availability of Wikipedia data in a very broad set of languages.", "labels": [], "entities": []}, {"text": "We introduce a set of simple but effective methods for syntactic transfer, as follows: \u2022 We describe a method for deriving crosslingual clusters, where words from different languages with a similar syntactic or semantic role are grouped in the same cluster.", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8897443115711212}]}, {"text": "These clusters can then be used as features in a shiftreduce dependency parser.", "labels": [], "entities": []}, {"text": "\u2022 We describe a method for transfer of lexical information from the target language into source language treebanks, using word-to-word translation dictionaries derived from parallel corpora.", "labels": [], "entities": [{"text": "transfer of lexical information from the target language", "start_pos": 27, "end_pos": 83, "type": "TASK", "confidence": 0.7841909527778625}]}, {"text": "Lexical features from the target language can then be integrated in parsing.", "labels": [], "entities": []}, {"text": "\u2022 We describe a method that integrates the above two approaches with the density-driven approach to annotation projection described by.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6917177587747574}]}, {"text": "Experiments show that our model outperforms previous work on a set of European languages from the Google universal treebank).", "labels": [], "entities": [{"text": "Google universal treebank", "start_pos": 98, "end_pos": 123, "type": "DATASET", "confidence": 0.6635600725809733}]}, {"text": "We achieve 80.9% average unlabeled attachment score (UAS) on these languages; in comparison the work of, and have a UAS of 75.4%, 76.3% and 77.8%, respectively.", "labels": [], "entities": [{"text": "average unlabeled attachment score (UAS)", "start_pos": 17, "end_pos": 57, "type": "METRIC", "confidence": 0.7900437584945134}, {"text": "UAS", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9871728420257568}]}, {"text": "All of these previous works make use of the much larger Europarl () corpus to derive lexical representations.", "labels": [], "entities": [{"text": "Europarl () corpus", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.9527809421221415}]}, {"text": "When using Europarl data instead of the Bible, our approach gives 83.9% accuracy, a 1.7% absolute improvement over.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9920754432678223}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.999713122844696}]}, {"text": "Finally, we conduct experiments on 38 datasets (26 languages) in the universal dependencies v1.3 (Nivre et al., 2016) corpus.", "labels": [], "entities": []}, {"text": "Our method has an average unlabeled dependency accuracy of 74.8% for these languages, more than 6% higher than the method of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9442532658576965}]}, {"text": "Thirteen datasets (10 languages) have accuracies higher than 80.0%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9989509582519531}]}], "datasetContent": [{"text": "This section first describes the experimental settings, then reports results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Sizes of the monolingual datasets for each of our  languages. All numbers are in millions.", "labels": [], "entities": []}, {"text": " Table 4: Statistics for the Bible, sampled Europarl (EU- S) and Europarl datasets. Each individual Bible text file  from Christodouloupoulos and Steedman (2014) consists  of 24720 sentences, except for English datasets, where  two translations into English are available, giving dou- ble the amount of data. Each text file from the sampled  Europarl datasets consists of 25K sentences and Europarl  has approximately 2 million sentences per language pair.", "labels": [], "entities": [{"text": "Europarl (EU- S)", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.892428477605184}, {"text": "Europarl datasets", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9583359360694885}, {"text": "Europarl datasets", "start_pos": 342, "end_pos": 359, "type": "DATASET", "confidence": 0.8737407326698303}, {"text": "Europarl", "start_pos": 390, "end_pos": 398, "type": "DATASET", "confidence": 0.9075073003768921}]}, {"text": " Table 5: Performance of different models in this paper;  first the baseline model, then models trained using the  methods described in sections  \u00a73.1-3.3. All results make  use of the Bible as a source of translation data. All differ- ences in UAS and LAS are statistically significant with  p < 0.001 using McNemar's test, with the exception of  \"de\" UAS/LAS Baseline vs. 3.1 (i.e., 49.7 vs 51.6 UAS  and 59.1 vs 59.7 LAS are not significant differences).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 309, "end_pos": 323, "type": "DATASET", "confidence": 0.8109517892201742}]}, {"text": " Table 6: Results for our method using different sources of translation data. \"Density\" refers to the method of", "labels": [], "entities": []}, {"text": " Table 7: Comparison of our work using the Bible and Europarl data, with previous work: MX14 (Ma and Xia, 2014),  LA16 (Lacroix et al., 2016), ZB15 (Zhang and Barzilay, 2015), GCY16 (Guo et al., 2016), AMB 16 (Ammar et al.,  2016b), and RC15 (", "labels": [], "entities": [{"text": "Bible and Europarl data", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.7942117750644684}, {"text": "GCY16", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.6896560192108154}, {"text": "AMB 16 (Ammar et al.,  2016b)", "start_pos": 202, "end_pos": 231, "type": "DATASET", "confidence": 0.8676857617166307}]}, {"text": " Table 8: The final results based on automatic part of  speech tags. RC15 refers to the best performing model  of Rasooli and Collins (2015).", "labels": [], "entities": []}, {"text": " Table 9: Results for the density driven method (", "labels": [], "entities": []}, {"text": " Table 10: Precision, recall and f-score of different depen- dency relations on the English development data of the  Google universal treebank. The major columns show the  dependency labels (\"dep.\"), frequency (\"freq.\"), the base- line delexicalized model (\"delex\"), and our method using  the Bible and Europarl (\"EU\") as translation data. The  rows are sorted by frequency.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9960236549377441}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9990825653076172}, {"text": "f-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9617887735366821}, {"text": "Google universal treebank", "start_pos": 117, "end_pos": 142, "type": "DATASET", "confidence": 0.7435784339904785}, {"text": "Europarl (\"EU\")", "start_pos": 303, "end_pos": 318, "type": "DATASET", "confidence": 0.7920548468828201}]}, {"text": " Table 11: Accuracy of unlabeled dependencies by POS  of the modifier word, for three groups of languages for  the universal dependencies experiments in", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9685310125350952}, {"text": "POS", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9409091472625732}]}, {"text": " Table 9: G1  (languages with UAS \u2265 80), G2 (languages with 70 \u2264  UAS < 80), G3 (languages with UAS < 70). The rows  are sorted by frequency in the G1 languages.", "labels": [], "entities": []}, {"text": " Table 12: Precision, recall and f-score of unlabeled dependency attachment for different POS tags as head for three  groups of languages for the universal dependencies experiments in", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9951629638671875}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.998647153377533}, {"text": "f-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9711703062057495}]}, {"text": " Table 9: G1 (languages with UAS \u2265 80), G2  (languages with 70 \u2264 UAS < 80), G3 (languages with UAS < 70). The rows are sorted by frequency in the G1  languages.", "labels": [], "entities": []}, {"text": " Table 13: Precision, recall and f-score for different dependency labels for three groups of languages for the universal  dependencies experiments in", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9982206225395203}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9985640645027161}, {"text": "f-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.955392062664032}]}, {"text": " Table 9: G1 (languages with UAS \u2265 80), G2 (languages with 70 \u2264 UAS < 80), G3  (languages with UAS < 70). The rows are sorted by frequency in the G1 languages.", "labels": [], "entities": []}]}