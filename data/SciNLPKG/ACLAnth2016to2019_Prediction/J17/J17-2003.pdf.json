{"title": [{"text": "Statistical Models for Unsupervised, Semi-Supervised, and Supervised Transliteration Mining under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings: unsupervised, semi-supervised, and supervised translitera-tion mining.", "labels": [], "entities": []}, {"text": "The model interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise).", "labels": [], "entities": []}, {"text": "The model is trained on noisy unlabeled data using the EM algorithm.", "labels": [], "entities": []}, {"text": "During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs.", "labels": [], "entities": []}, {"text": "After training, the unlabeled data is disambiguated based on the posterior probabilities of the two sub-models.", "labels": [], "entities": []}, {"text": "We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8761571049690247}]}, {"text": "For three out of four language pairs, our system outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task.", "labels": [], "entities": [{"text": "NEWS 2010 shared task", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.6193247884511948}]}, {"text": "On word pairs extracted from parallel corpora with fewer than 2% transliteration pairs, our system achieves up to 86.7% F-measure with 77.9% precision and 97.8% recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9954046010971069}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9992985725402832}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9984419941902161}]}, {"text": "* Much of the research presented here was conducted while the authors were at", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration converts a word from a source script into a target script.", "labels": [], "entities": []}, {"text": "The English words Alberto and Doppler, for example, can be written in Arabic script as /Albrtw and /dwblr, respectively, and are examples of transliteration.", "labels": [], "entities": [{"text": "Doppler", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.7880257964134216}]}, {"text": "Automatic transliteration is useful in many NLP applications such as crosslanguage information retrieval, statistical machine translation, building of comparable corpora, terminology extraction, and so forth.", "labels": [], "entities": [{"text": "crosslanguage information retrieval", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.729695330063502}, {"text": "statistical machine translation", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.7658542195955912}, {"text": "terminology extraction", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.9116673171520233}]}, {"text": "Most transliteration systems are trained on a list of transliteration pairs which consist of a word and its transliteration.", "labels": [], "entities": []}, {"text": "However, manually labeled transliteration pairs are only available fora few language pairs.", "labels": [], "entities": []}, {"text": "Therefore it is attractive to extract transliteration pairs automatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance.", "labels": [], "entities": []}, {"text": "This extraction process is called transliteration mining.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8972658514976501}]}, {"text": "There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs.", "labels": [], "entities": []}, {"text": "Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair.", "labels": [], "entities": []}, {"text": "Pairs with an edit distance below a given threshold are extracted ().", "labels": [], "entities": []}, {"text": "Supervised transliteration mining systems) make use of an initial list of transliteration pairs that is automatically aligned at the character level.", "labels": [], "entities": []}, {"text": "The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs.", "labels": [], "entities": []}, {"text": "Word pairs with a probability greater than a certain threshold are classified as transliteration pairs.", "labels": [], "entities": []}, {"text": "Similarly to supervised approaches, semi-supervised systems) also use a list of transliteration pairs for training.", "labels": [], "entities": []}, {"text": "However, here the list is generally small.", "labels": [], "entities": []}, {"text": "The systems thus do not solely rely on it to mine transliteration pairs.", "labels": [], "entities": []}, {"text": "They use both the list of transliteration pairs and unlabeled data for training.", "labels": [], "entities": []}, {"text": "We are only aware of two unsupervised systems (requiring no labeled data).", "labels": [], "entities": []}, {"text": "One of them was proposed by Fei.", "labels": [], "entities": []}, {"text": "He extracts named entity pairs from a bilingual corpus, converts all words into Latin script by romanization, and classifies them into transliterations and non-transliterations based on the edit distance.", "labels": [], "entities": []}, {"text": "This system still requires a named entity tagger to generate the candidate pairs, a list of mapping rules to convert non-Latin scripts to Latin, and labeled data to optimize parameters.", "labels": [], "entities": []}, {"text": "The only previous system that requires no such resources is that of . They extract transliteration pairs by iteratively filtering a list of candidate pairs.", "labels": [], "entities": []}, {"text": "The downsides of their method are inefficiency and inflexibility.", "labels": [], "entities": []}, {"text": "It requires about 100 EM runs with 100 iterations each, and it is unclear how to extend it for semi-supervised and supervised settings.", "labels": [], "entities": []}, {"text": "In this article, we present anew approach to transliteration mining that is fully unsupervised like the system of Sajjad, Fraser, and . It is based on a principled model which is both efficient and accurate and can be used in three different training settings-unsupervised, semi-supervised, and supervised learning.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9545964896678925}]}, {"text": "Our method directly learns character correspondences between two scripts from a noisy unlabeled list of word pairs which contains both transliterations and non-transliterations.", "labels": [], "entities": []}, {"text": "When such a list is extracted from an aligned bilingual corpus, for instance, it contains, apart from transliterations, also both translations and misalignments, which we will call non-transliterations.", "labels": [], "entities": []}, {"text": "Our statistical model interpolates a transliteration sub-model and a nontransliteration sub-model.", "labels": [], "entities": []}, {"text": "The intuition behind using two sub-models is that the transliteration pairs and non-transliteration pairs, which makeup the unlabeled training data, have rather different characteristics and need to be modeled separately.", "labels": [], "entities": []}, {"text": "Transliteration word pairs show a strong dependency between source and target characters, whereas the characters of non-transliteration pairs are unrelated.", "labels": [], "entities": []}, {"text": "Hence we use one sub-model for transliterations that jointly generate the source and target strings with a joint source channel model, and a second sub-model for nontransliterations that generate the two strings independently of each other using separate source and target character sequence models whose probabilities are multiplied.", "labels": [], "entities": []}, {"text": "The overall model is trained with the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.8167916893959045}]}, {"text": "Only the parameters of the transliteration model and the interpolation weight are learned during EM training, whereas the parameters of the non-transliteration model are kept fixed after initialization.", "labels": [], "entities": []}, {"text": "At test time, a word pair is classified as a transliteration if the posterior probability of transliteration is higher than the posterior probability of non-transliteration.", "labels": [], "entities": []}, {"text": "For the semi-supervised system, we modify EM training by adding anew step, which we call the S-step.", "labels": [], "entities": []}, {"text": "The S-step takes the probability estimates from one EM iteration on the unlabeled data and uses them as a backoff distribution in smoothing probabilities which were estimated from labeled data.", "labels": [], "entities": []}, {"text": "The smoothed probabilities are then used in the next E-step.", "labels": [], "entities": []}, {"text": "In this way, we constrain the parameters learned by EM to values that are close to those estimated from the labeled data.", "labels": [], "entities": []}, {"text": "In the supervised approach, we set the weight of the non-transliteration sub-model during EM training to zero, because all training word pairs are transliterations here.", "labels": [], "entities": []}, {"text": "In test mode, the supervised mining model uses both sub-models and estimates a proper interpolation weight with EM on the test data.", "labels": [], "entities": [{"text": "EM", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9955106973648071}]}, {"text": "We evaluate our system on the data sets available from the NEWS 2010 shared task on transliteration mining, which we call NEWS10 herein.", "labels": [], "entities": [{"text": "NEWS 2010 shared task", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.9369080364704132}, {"text": "transliteration mining", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7400901913642883}, {"text": "NEWS10", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.9640958905220032}]}, {"text": "On three out of four language pairs, our unsupervised transliteration mining system performs better than all semi-supervised and supervised systems that participated in NEWS10.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7121379673480988}]}, {"text": "We also evaluate our unsupervised system on parallel corpora of English/Hindi and English/Arabic texts and show that it is able to effectively mine transliteration pairs from data with only 2% transliteration pairs.", "labels": [], "entities": []}, {"text": "The unigram version of the unsupervised and semi-supervised systems was published in.", "labels": [], "entities": []}, {"text": "In that paper, we proposed a supervised version of our transliteration mining system and also extended it to higher orders of character n-grams.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8256634473800659}]}, {"text": "Together with this article we also release data and source code as described below.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: r We present a statistical model for unsupervised transliteration mining, which is very efficient and accurate.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8092203736305237}]}, {"text": "It models unlabeled data consisting of transliterations and non-transliterations.", "labels": [], "entities": []}, {"text": "r We show that our unsupervised system can easily be extended to both semi-supervised and supervised learning scenarios.", "labels": [], "entities": []}, {"text": "r We present a detailed analysis of our system using different types of corpora, with various learning strategies and with different n-gram orders.", "labels": [], "entities": []}, {"text": "We show that if labeled data are available, it is superior to building a semi-supervised system rather than an unsupervised system or a supervised system.", "labels": [], "entities": []}, {"text": "r We make our transliteration mining tool, which is capable of unsupervised, semi-supervised, and supervised learning, freely available to the research community at http://alt.qcri.org/\u223chsajjad/ software/transliteration_mining/.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.852853000164032}]}, {"text": "r We also provide the transliteration mining gold standard data that we created from English/Arabic and English/Hindi parallel corpora for use by other researchers.", "labels": [], "entities": []}], "datasetContent": [{"text": "The unsupervised system is trained on the cross-product list only.", "labels": [], "entities": []}, {"text": "The semisupervised system is trained on the cross-product list and the seed data, and the supervised system is trained only on the seed data.", "labels": [], "entities": []}, {"text": "The multigram probabilities are uniformly initialized with the inverse of the number of all possible multigrams of the source and target language characters.", "labels": [], "entities": []}, {"text": "The prior probability of non-transliteration \u03bb is initialized with 0.5.", "labels": [], "entities": []}, {"text": "In test mode, the trained model is applied to the test data.", "labels": [], "entities": []}, {"text": "If the training data is identical to the test data, then the value of \u03bb estimated during training is used attest time.", "labels": [], "entities": []}, {"text": "If they are different, as in the case of the supervised system, we reestimate \u03bb on the test data.", "labels": [], "entities": []}, {"text": "Word pairs whose posterior probability of transliteration is above 0.5 are classified as transliterations.", "labels": [], "entities": []}, {"text": "shows the result of our unsupervised transliteration mining system on the NEWS10 data set in comparison with the best unsupervised and (semi-)supervised systems presented at NEWS10 (S BEST ) and the best (semi-)supervised results reported overall on this data set (GR, DBN).", "labels": [], "entities": [{"text": "NEWS10 data set", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9934419790903727}, {"text": "NEWS10", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.9573362469673157}, {"text": "BEST", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.41518333554267883}]}], "tableCaptions": [{"text": " Table 2  Comparison of our unsupervised system OUR with the state-of-the-art unsupervised,  semi-supervised, and supervised systems, where S Best is the best NEWS10 system, SJD is the  unsupervised system of", "labels": [], "entities": [{"text": "OUR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.4385574162006378}]}, {"text": " Table 3  Results of our unsupervised, semi-supervised, and supervised transliteration mining systems  trained on the cross-product list and using the unigram, bigram, and trigram models for  transliteration and non-transliteration. The bolded values show the best precision, recall, and  F-measure for each language pair.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7703171670436859}, {"text": "precision", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.9995114803314209}, {"text": "recall", "start_pos": 276, "end_pos": 282, "type": "METRIC", "confidence": 0.9989153146743774}, {"text": "F-measure", "start_pos": 289, "end_pos": 298, "type": "METRIC", "confidence": 0.999220609664917}]}, {"text": " Table 8  Transliteration mining results of the heuristic-based system SJD and the unsupervised unigram  system OUR trained and tested on the word-aligned list of the English/Hindi and English/  Arabic parallel corpus. TP, FN, TN, and FP represent true positive, false negative, true negative,  and false positive, respectively.", "labels": [], "entities": [{"text": "Transliteration mining", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8233662247657776}, {"text": "FP", "start_pos": 235, "end_pos": 237, "type": "METRIC", "confidence": 0.9723652005195618}]}, {"text": " Table 9  Results of the unsupervised, semi-supervised, and supervised mining systems trained on the  word-aligned list and tested on the cross-product list of the English/Hindi parallel corpus.  The bolded values show the best precision, recall, and F-measure for the unigram, bigram, and  trigram systems.", "labels": [], "entities": [{"text": "English/Hindi parallel corpus", "start_pos": 164, "end_pos": 193, "type": "DATASET", "confidence": 0.5872276902198792}, {"text": "precision", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9993919134140015}, {"text": "recall", "start_pos": 239, "end_pos": 245, "type": "METRIC", "confidence": 0.9990074038505554}, {"text": "F-measure", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.9992590546607971}]}, {"text": " Table 10  Results of the unsupervised, semi-supervised, and supervised mining systems trained on the  word-aligned list and tested on the cross-product list of the English/Arabic parallel corpus.  The bolded values show the best precision, recall, and F-measure for the unigram, bigram, and  trigram systems.", "labels": [], "entities": [{"text": "English/Arabic parallel corpus", "start_pos": 165, "end_pos": 195, "type": "DATASET", "confidence": 0.5949567794799805}, {"text": "precision", "start_pos": 230, "end_pos": 239, "type": "METRIC", "confidence": 0.9993852376937866}, {"text": "recall", "start_pos": 241, "end_pos": 247, "type": "METRIC", "confidence": 0.9989451766014099}, {"text": "F-measure", "start_pos": 253, "end_pos": 262, "type": "METRIC", "confidence": 0.9992853999137878}]}, {"text": " Table 13  Types of errors made by the unsupervised transliteration mining system on the English/Arabic  language pair. The numbers are based on randomly selected 100 word pairs that were wrongly  classified by the mining system.", "labels": [], "entities": []}]}