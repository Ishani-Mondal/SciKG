{"title": [{"text": "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce HyperLex-a data set and evaluation resource that quantifies the extent of the semantic category membership, that is, type-of relation, also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs.", "labels": [], "entities": []}, {"text": "Cognitive psychology research has established that typicality and category/class membership are computed inhuman semantic memory as a gradual rather than binary relation.", "labels": [], "entities": [{"text": "typicality and category/class membership", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.5647186040878296}]}, {"text": "Nevertheless, most NLP research and existing large-scale inventories of concept category membership (WordNet, DBPedia, etc.) treat category membership and LE as binary.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8682442903518677}]}, {"text": "To address this, we asked hundreds of native English speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform.", "labels": [], "entities": []}, {"text": "Our results confirm that category membership and LE are indeed more gradual than binary.", "labels": [], "entities": [{"text": "category membership", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6456667333841324}, {"text": "LE", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8551405668258667}]}, {"text": "We then compare these human judgments with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves.", "labels": [], "entities": []}, {"text": "We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most native speakers of English, in almost all contexts and situations, would agree that dogs, cows, or cats are animals, and that tables or pencils are not.", "labels": [], "entities": []}, {"text": "However, for certain concepts, membership of the animal category is less clear-cut.", "labels": [], "entities": []}, {"text": "Whether lexical concepts such as dinosaur, human being, or amoeba are considered animals seems to depend on the context in which such concepts are described, the perspective of the speaker or listener, and even the formal scientific knowledge of the interlocutors.", "labels": [], "entities": []}, {"text": "Despite this indeterminacy, when communicating, humans intuitively reason about such relations between concepts and categories.", "labels": [], "entities": []}, {"text": "Indeed, the ability to quickly perform inference over such networks and arrive at coherent knowledge representations is crucial for human language understanding.", "labels": [], "entities": [{"text": "human language understanding", "start_pos": 132, "end_pos": 160, "type": "TASK", "confidence": 0.6423525313536326}]}, {"text": "The Princeton WordNet lexical database) is perhaps the best-known attempt to formally represent such a semantic network.", "labels": [], "entities": [{"text": "Princeton WordNet lexical database", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.9251162111759186}]}, {"text": "In WordNet, concepts are organized in a hierarchical fashion, in an attempt to replicate observed aspects of human semantic memory.", "labels": [], "entities": []}, {"text": "One of the fundamental relations between concepts in WordNet is the so-called TYPE-OF or hyponymy-hypernymy relation that exists between category concepts such as animal and their constituent members such as cat or dog.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9259548187255859}, {"text": "TYPE-OF", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9716268181800842}]}, {"text": "The type-of relation is particularly important in language understanding because it underlines the lexical entailment (LE) relation.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7185067981481552}]}, {"text": "Simply put, an instantiation of a member concept such as a cat entails the existence of an animal.", "labels": [], "entities": []}, {"text": "This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.", "labels": [], "entities": []}, {"text": "Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing, inter alia).", "labels": [], "entities": [{"text": "language understanding", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7273195534944534}, {"text": "automatic detection and modeling of lexical entailment", "start_pos": 70, "end_pos": 124, "type": "TASK", "confidence": 0.8180665118353707}]}, {"text": "The ability to effectively detect and model both lexical and phrasal entailment in a human-like way maybe critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8890619874000549}, {"text": "information retrieval", "start_pos": 178, "end_pos": 199, "type": "TASK", "confidence": 0.8157954216003418}, {"text": "information extraction", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.8575789034366608}, {"text": "text summarization and generation", "start_pos": 229, "end_pos": 262, "type": "TASK", "confidence": 0.7968092262744904}]}, {"text": "For instance, in order to answer a question such as \"Which mammal has a strong bite?\", a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not.", "labels": [], "entities": []}, {"text": "Although inspired to some extent by theories of human semantic memory, large-scale inventories of semantic concepts, such as WordNet, typically make many simplifying assumptions, particularly regarding the nature of the type-of relation, and consequently the effect of LE.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9397939443588257}]}, {"text": "In WordNet, for instance, all semantic relations are represented in a binary way (i.e., concept X entails Y) rather than gradual (e.g., X entails Y to a certain degree).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9395388960838318}]}, {"text": "However, since at least the pioneering experiments on prototypes, it has been known that, fora given semantic category, certain member concepts are consistently understood as more central to the category than others (even when controlling for clearly confounding factors such as frequency).", "labels": [], "entities": []}, {"text": "In other words, WordNet and similar resources fail to capture the fact that category membership is a gradual semantic phenomenon.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9455841779708862}, {"text": "category membership", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6947656869888306}]}, {"text": "This limitation of WordNet also characterizes much of the LE research in NLP, as we discuss later in Section 3.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9385503530502319}]}, {"text": "To address these limitations, the present work is concerned with graded lexical entailment: the degree of the LE relation between two concepts on a continuous scale.", "labels": [], "entities": []}, {"text": "Thanks to the availability of crowdsourcing technology, we conduct a variant of the seminal behavioral data collection by, but on a massive scale.", "labels": [], "entities": []}, {"text": "To do so, we introduce the idea of graded or soft LE, and design a human rating task for (X, Y) concept pairs based on the following question: To what degree is X a type of Y?.", "labels": [], "entities": []}, {"text": "We arrive at a data set with 2,616 concept pairs, each rated by at least 10 human raters, scored by the degree to which they exhibit typicality and semantic category membership and, equivalently, LE.", "labels": [], "entities": [{"text": "LE", "start_pos": 196, "end_pos": 198, "type": "METRIC", "confidence": 0.9971117973327637}]}, {"text": "Using this data set, HyperLex, we investigate two questions: r (Q1) Do we observe the same effects of typicality, graded membership, and graded lexical entailment inhuman judgments as observed by Rosch?", "labels": [], "entities": []}, {"text": "Do humans intuitively distinguish between central and non-central members of a category/class?", "labels": [], "entities": []}, {"text": "Do humans distinguish between full and partial membership in a class as discussed by r (Q2) Is the current LE modeling and representation methodology as applied in NLP research and technology sufficient to accurately capture graded lexical entailment automatically?", "labels": [], "entities": [{"text": "LE modeling and representation", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7877176702022552}]}, {"text": "What is the gap between current automatic systems and human performance in the graded LE task?", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "We define and discuss graded LE in Section 2.", "labels": [], "entities": [{"text": "LE", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.599632978439331}]}, {"text": "In Section 3, we survey benchmarking resources from the literature that pertain to semantic category membership, LE identification, or evaluation, and motivate the need fora new, more expressive resource.", "labels": [], "entities": [{"text": "semantic category membership", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.667411744594574}, {"text": "LE identification", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8760062754154205}]}, {"text": "In Section 4, we describe the design and development of HyperLex, and outline the various semantic dimensions (such as POS usage, hypernymy levels, and concreteness levels) along which these concept pairs are designed to vary.", "labels": [], "entities": []}, {"text": "This allows us to address Q1 in Section 5, where we present a series of qualitative analyses of the data gathered and collated into HyperLex.", "labels": [], "entities": [{"text": "HyperLex", "start_pos": 132, "end_pos": 140, "type": "DATASET", "confidence": 0.9295216202735901}]}, {"text": "High inter-annotator agreement scores (pairwise and mean Spearman's \u03c1 correlations around 0.85 on the entire data set; similar correlations on noun and verb subsets) indicate that participants found it unproblematic to rate consistently the graded LE relation for the full range of concepts.", "labels": [], "entities": [{"text": "mean Spearman's \u03c1 correlations", "start_pos": 52, "end_pos": 82, "type": "METRIC", "confidence": 0.662465500831604}]}, {"text": "These analyses reveal that the data in HyperLex enhance, rather than contradict or undermine, the information in WordNet, in the sense that hyponymy-hypernymy pairs receive highest average ratings in HyperLex compared with all other WordNet relations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9565005302429199}]}, {"text": "We also show that participants are able to capture the implicit asymmetry of the graded LE relation by examining ratings of (X, Y) and reversed (Y, X) pairs.", "labels": [], "entities": []}, {"text": "Most importantly, our analysis shows that the effects of typicality, vagueness, and gradual nature of LE are indeed captured inhuman judgments.", "labels": [], "entities": []}, {"text": "For instance, graded LE scores indicate that humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray.", "labels": [], "entities": []}, {"text": "In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches.", "labels": [], "entities": [{"text": "LE detection or measurement", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.9078500866889954}]}, {"text": "This review covers: (i) distributional models relying on the distributional inclusion hypothesis and semantic generality computations (); (ii) multi-modal approaches ( ); (iii) WordNet-based approaches; and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (;, inter alia), others developed to better capture the asymmetric LE relation).", "labels": [], "entities": []}, {"text": "Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [) so that HyperLex can be used for supervised learning.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9392451643943787}]}, {"text": "We therefore evaluate several prominent supervised LE architectures (, inter alia).", "labels": [], "entities": []}, {"text": "Although we observe interesting differences in the models, our findings indicate clearly that none of the currently available models or approaches accurately model the relation of graded LE reflected inhuman subjects.", "labels": [], "entities": []}, {"text": "This study therefore calls for new paradigms and solutions capable of capturing the gradual nature of semantic relations such as hypernymy in hierarchical semantic networks.", "labels": [], "entities": []}, {"text": "In Section 8, we turn to the future and discuss potential applications of the graded LE concept and HyperLex.", "labels": [], "entities": []}, {"text": "We conclude in Section 9 by summarizing the key aspects of our contribution.", "labels": [], "entities": []}, {"text": "HyperLex offers robust, data-driven insight into how humans perceive the concepts of typicality and graded membership within the graded LE relation.", "labels": [], "entities": []}, {"text": "We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because the work in NLP and human language understanding focuses on the ungraded version of the LE relation, we briefly survey main ungraded LE evaluation protocols in Section 3.1.1, followed by an overview of benchmarking LE evaluation sets in For instance, given the lexical relation classification scheme of, LE or CLASS-INCLUSION is only one of the 10 high-level relation classes.", "labels": [], "entities": [{"text": "human language understanding", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6217191914717356}]}, {"text": "6 From the SimLex-999 guidelines: \"Two words are syonymys if they have very similar meanings.", "labels": [], "entities": []}, {"text": "Synonyms represent the same type or category (...) you are asked to compare word pairs and to rate how similar they are...\"", "labels": [], "entities": []}, {"text": "Synonymy and LE capture different aspects of meaning regarding semantic hierarchies/taxonomies: For example, whereas the pair (mouse, rat) receives a score of 7.78 in SimLex-999 (on the scale 0-10), the same pair has a graded LE score of 2.22 in HyperLex.", "labels": [], "entities": []}, {"text": "We show that none of the existing evaluation protocols coupled with existing evaluation sets enables a satisfactory evaluation of the capability of statistical models to capture graded LE.", "labels": [], "entities": []}, {"text": "As opposed to existing evaluation sets, by collecting human judgments through a crowdsourcing study our new HyperLex evaluation set also enables qualitative linguistic analysis on how humans perceive and rate graded lexical entailment.", "labels": [], "entities": [{"text": "HyperLex evaluation set", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.7737157543500265}]}, {"text": "Evaluation protocols for the lexical entailment or type-of relation in NLP, based on the classical definition of ungraded LE, maybe roughly clustered as follows: (i) Entailment Directionality.", "labels": [], "entities": [{"text": "Entailment Directionality", "start_pos": 166, "end_pos": 191, "type": "TASK", "confidence": 0.7155254483222961}]}, {"text": "Given two words (X, Y) that are known to stand in a lexical entailment relation, the system has to predict the relation directionality, that is, which word is the hypernym and which word is the hyponym.", "labels": [], "entities": []}, {"text": "More formally, the following mapping is defined by the directionality function f dir : f dir simply maps to 1 when Y is the hypernym, and to \u22121 otherwise.", "labels": [], "entities": []}, {"text": "(ii) Entailment Detection.", "labels": [], "entities": [{"text": "Entailment Detection", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.9402396082878113}]}, {"text": "The system has to predict whether there exists a lexical entailment relation between two words, or the words stand in some other relation (synonymy, meronymy-holonymy, causality, no relation, etc.).", "labels": [], "entities": []}, {"text": "A more detailed overview of lexical relations is available in related work ().", "labels": [], "entities": []}, {"text": "The following mapping is defined by the detection function f det : f det simply maps to 1 when (X, Y) stand in a lexical entailment relation, irrespective to the actual directionality of the relation, and to 0 otherwise.", "labels": [], "entities": []}, {"text": "(iii) Entailment Detection and Directionality.", "labels": [], "entities": [{"text": "Entailment Detection", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.9641154110431671}]}, {"text": "This recently proposed evaluation protocol () combines (i) and (ii).", "labels": [], "entities": []}, {"text": "The system first has to detect whether there exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym).", "labels": [], "entities": []}, {"text": "The following mapping is defined by the joint detection and directionality function f det+dir : f det+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to \u22121 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation.", "labels": [], "entities": []}, {"text": "These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013).", "labels": [], "entities": []}, {"text": "The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms.", "labels": [], "entities": []}, {"text": "The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are semantically more general than the characteristics forming the intension 7 of its hyponyms (e.g., bark or has tail for the concept dog).", "labels": [], "entities": []}, {"text": "In other words, superordinate concepts such as animal or appliance are semantically less informative than their hyponyms, which is also reflected in less specific contexts for hypernyms.", "labels": [], "entities": []}, {"text": "Unsupervised (distributional) models of lexical entailment were instigated by the early work of Hearst (1992) on prototypicality patterns (e.g., the pattern \"X such as Y\" indicates that Y is a hyponym of X).", "labels": [], "entities": []}, {"text": "The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations) with an asymmetric similarity measure optimized for entailment).", "labels": [], "entities": []}, {"text": "Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (.", "labels": [], "entities": []}, {"text": "An overview of the supervised techniques also discussing their main shortcomings is provided by; a thorough discussion of differences between unsupervised and supervised entailment models is provided by.", "labels": [], "entities": []}, {"text": "In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) maybe straightforwardly translated into binary decision problems: (1) distinguishing between hypernyms and hyponyms; (2) distinguishing between lexical entailment and other relations.", "labels": [], "entities": []}, {"text": "HyperLex, on the other hand, targets a different type of evaluation.", "labels": [], "entities": [{"text": "HyperLex", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9037290215492249}]}, {"text": "The graded entailment function f graded defines the following mapping: f graded outputs the strength of the lexical entailment relation s \u2208 R + 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y).", "labels": [], "entities": []}, {"text": "From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y.", "labels": [], "entities": []}, {"text": "From the relational similarity viewpoint (), it also measures the prototypicality of the pair (X, Y) for the LE relation.", "labels": [], "entities": []}, {"text": "Introduced by, the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities.", "labels": [], "entities": [{"text": "BLESS evaluation set", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.7195444703102112}]}, {"text": "A total of 175 concepts were extracted from the McRae feature norms data set (, and the remaining 25 were selected manually by the authors.", "labels": [], "entities": [{"text": "McRae feature norms data set", "start_pos": 48, "end_pos": 76, "type": "DATASET", "confidence": 0.9566838860511779}]}, {"text": "These concepts were then paired to 8,625 The terms intension and extension assume classical intensional and extensional definitions of a concept), only the LE subset is used.", "labels": [], "entities": []}, {"text": "Note that the original BLESS data are always presented with the hyponym first, so gold annotations are implicitly provided here.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.965771496295929}]}, {"text": "Second, for detection evaluations), the pairs from the LE subset are taken as positive pairs, and all the remaining pairs are considered negative pairs.", "labels": [], "entities": []}, {"text": "That way, the evaluation data effectively measure a model's ability to predict the positive LE relation.", "labels": [], "entities": []}, {"text": "Another evaluation data set based on BLESS was introduced by.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9918783903121948}]}, {"text": "Following the standard annotation scheme, it comprises 7,429 noun pairs in total, and 1,880 LE pairs in particular, covering a wider range of relations than BLESS (i.e., the data set now includes synonymy and antonymy pairs).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.9535514116287231}]}, {"text": "Adaptations of the original BLESS evaluation set were proposed recently.", "labels": [], "entities": [{"text": "BLESS evaluation set", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8163641095161438}]}, {"text": "First, relying on its LE subset, created another data set called WBLESS ( ) consisting of 1,976 concept pairs in total.", "labels": [], "entities": [{"text": "WBLESS", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7973860502243042}]}, {"text": "Only (X, Y) pairs where Y is the hypernym are annotated as positive examples.", "labels": [], "entities": []}, {"text": "It also contains reversed LE pairs (i.e., X is the hypernym), cohyponymy pairs, meronymy-holonymy pairs, and randomly matched nouns balanced across different lexical relations; all are annotated as negative examples.", "labels": [], "entities": []}, {"text": "Because of its construction, WBLESS is used solely for experiments on LE detection.", "labels": [], "entities": [{"text": "WBLESS", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.7982358336448669}, {"text": "LE detection", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.8852240741252899}]}, {"text": "created another data set in a similar fashion, consisting of 5,835 noun pairs, targeting co-hyponymy detection.", "labels": [], "entities": [{"text": "co-hyponymy detection", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7279646396636963}]}, {"text": "For the combined detection and directionality evaluation, a variant evaluation set called BiBLESS was proposed ( ).", "labels": [], "entities": [{"text": "BiBLESS", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.964758038520813}]}, {"text": "It is built on WBLESS, but now explicitly distinguishes direction in LE pairs.", "labels": [], "entities": [{"text": "WBLESS", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9268683195114136}]}, {"text": "Examples of concept pairs in all BLESS variants can be found in.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9254426956176758}]}, {"text": "A majority of alternative ungraded LE evaluation sets briefly discussed here have a structure very similar to BLESS and its variants..", "labels": [], "entities": [{"text": "BLESS", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9678810834884644}]}, {"text": "Based on the original data set of, this evaluation set () contains 3,772 word pairs in total.", "labels": [], "entities": []}, {"text": "The structure is similar to BLESS: 1,068 pairs are labeled as positive examples (i.e., 1 or entails iff X entails Y), and 2,704 labeled as negative examples, including the reversed positive pairs.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.8772037029266357}]}, {"text": "The assignment of binary labels is described in detail by.", "labels": [], "entities": []}, {"text": "The class sizes are not balanced, and because of its design, although each pair is unique, 30 high-frequent nouns occur in each pair in the data set.", "labels": [], "entities": []}, {"text": "Note that Example pairs from BLESS data set variants.", "labels": [], "entities": [{"text": "BLESS data set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8325144648551941}]}, {"text": "We compare the performance of prominent models and frameworks focused on modeling lexical entailment on our new HyperLex evaluation set now measuring the strength of the lexical entailment relation.", "labels": [], "entities": [{"text": "HyperLex evaluation set", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.9334509571393331}]}, {"text": "Because of the evident similarity of the graded evaluation with standard protocols in the semantic similarity (i.e., synonymy detection) literature (, inter alia), we adopt the same evaluation set-up.", "labels": [], "entities": [{"text": "synonymy detection)", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8125002880891165}]}, {"text": "Each evaluated model assigns a score to each pair of words measuring the strength of lexical entailment relation between them.", "labels": [], "entities": []}, {"text": "As in prior work on intrinsic evaluations with word pair scoring evaluation sets, all reported scores are Spearman's \u03c1 correlations between the ranks derived from the scores of the evaluated models and the human scores provided in HyperLex.", "labels": [], "entities": [{"text": "HyperLex", "start_pos": 231, "end_pos": 239, "type": "DATASET", "confidence": 0.870378315448761}]}, {"text": "In this work, we evaluate off-the-shelf unsupervised models and insightful baselines on the entire HyperLex.", "labels": [], "entities": [{"text": "HyperLex", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9514578580856323}]}, {"text": "We also report on preliminary experiments exploiting provided data splits for supervised learning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Example word pairs from HyperLex. The order of words in each pair is fixed, for example, the pair  chemistry / science should be read as \"Is CHEMISTRY a type of SCIENCE?\"", "labels": [], "entities": []}, {"text": " Table 4  A comparison of HyperLex IAA with several prominent crowdsourced semantic similarity/  relatedness evaluation benchmarks that also provide scores for word pairs. Numbers in  parentheses refer to the total number of word pairs in each evaluation set.", "labels": [], "entities": []}, {"text": " Table 5  Inter-annotator agreements, measured by average pairwise Spearman's \u03c1 correlation over  different fine-grained lexical relations extracted from WordNet.", "labels": [], "entities": [{"text": "pairwise Spearman's \u03c1 correlation", "start_pos": 58, "end_pos": 91, "type": "METRIC", "confidence": 0.6446925580501557}, {"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.967012345790863}]}, {"text": " Table 6. We might draw several preliminary insights based on  the presented lists. There is an evident prototyping effect present in human judgments:  Concepts such as cat, monkey, or cow are more typical instances of the class animal than  the more peculiar instances, such as mongoose or snail, according to HyperLex annotators.  Instances of the class sport also seem to be sorted accordingly, as higher scores are  assigned to arguably more prototypical sports such as basketball, volleyball, or soccer, and  less prototypical sports such as racquetball or wrestling are assigned lower scores.  Nonetheless, the majority of hyp-N pairs (X, animal) or (X, sport), where X is a  hyponym of animal/sport according to WordNet (WN), are indeed assigned reasonably  high graded LE scores. It suggests that humans are able to: (1) judge the LE relation  consistently and decide that a concept indeed stands in a type-of relation with another  concept, and (2) grade the LE relation by assigning more strength to more prototypical  class instances. Similar patterns are visible with other class instances from", "labels": [], "entities": [{"text": "WordNet (WN)", "start_pos": 719, "end_pos": 731, "type": "DATASET", "confidence": 0.9029079228639603}]}, {"text": " Table 6  Graded LE scores for instances of several prominent taxonomical categories/classes represented in  HyperLex (i.e., the categories are the word Y in each (X, Y, s) graded LE triplet).", "labels": [], "entities": [{"text": "Graded LE scores", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8982894420623779}, {"text": "HyperLex", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9081816673278809}]}, {"text": " Table 7  Average HyperLex scores across all pairs, and noun and verb pairs representing finer-grained  semantic relations extracted from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9527541995048523}]}, {"text": " Table 8  A selection of scored (X, Y) word pairs from HyperLex holding the hyp-1, hyp-2, and hyp-3  relation according to WordNet along with the HyperLex score for the actual (X, Y) pair (scr) and  the HyperLex score for the reversed (Y, X) pair (i.e., rhyp-N relations): rscr. The reported  percentages on top refer to the ratio of (X, Y) pairs for each relation where scr > rscr.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9660285115242004}]}, {"text": " Table 9  A total number of concept pairs in each of the four coarse-grained groups based on the concepts'  concreteness ratings: Both concepts are concrete (USF concreteness rating \u2265 4) \u2192 G 1 ; both abstract  \u2192 G 2 ; one concrete and one abstract concept with a difference in concreteness \u2264 1 \u2192 G 3 or > 1 \u2192  G 4 . rhyp-N pairs are not counted.", "labels": [], "entities": [{"text": "USF", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.860236406326294}]}, {"text": " Table 14  Results in the ungraded LE directionality task (precision) using a subset of 940 HyperLex pairs  converted to the ungraded directionality data set. Graded LE results (Spearman's \u03c1 correlation) on  the same subset are also provided for comparison purposes, using the best model configurations  from", "labels": [], "entities": [{"text": "LE directionality task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7728737394014994}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9990106821060181}, {"text": "Spearman's \u03c1 correlation)", "start_pos": 178, "end_pos": 203, "type": "METRIC", "confidence": 0.6961843609809876}]}, {"text": " Table 15  Results on the two TEST sets of HyperLex splits with a selection of unsupervised LE models.  Lower scoring model variants are not shown.", "labels": [], "entities": [{"text": "TEST sets", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.6797544211149216}]}, {"text": " Table 16  The effects of lexical memorization on the output of regression models when dealing with typical  hypernymy concepts higher in the taxonomy (e.g., animal, plant): HYPERLEX denotes the score  assigned to the pair by humans in HyperLex, and OLS and RIDGE refer to the predicted output of  the two tested regression models. We use SGNS-DEPS with concatenation (  X \u2295  Y, see", "labels": [], "entities": [{"text": "HYPERLEX", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9966768026351929}, {"text": "OLS", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.9771275520324707}, {"text": "RIDGE", "start_pos": 258, "end_pos": 263, "type": "METRIC", "confidence": 0.982107937335968}]}]}