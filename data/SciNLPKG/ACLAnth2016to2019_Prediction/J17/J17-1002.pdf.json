{"title": [{"text": "A Game-Theoretic Approach to Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7947466770807902}]}], "abstractContent": [{"text": "This article presents anew model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as anode on a graph whose edges represent word relations and senses are represented as classes.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6818395455678304}]}, {"text": "The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose.", "labels": [], "entities": []}, {"text": "We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices.", "labels": [], "entities": []}, {"text": "With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence.", "labels": [], "entities": [{"text": "word sense disambiguation problem", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.706857480108738}]}, {"text": "The model is based on two ideas: Similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them.", "labels": [], "entities": []}, {"text": "The article provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example.", "labels": [], "entities": [{"text": "word sense disambiguation problem", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.7411819621920586}]}, {"text": "The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems.", "labels": [], "entities": []}, {"text": "The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is the task of identifying the intended meaning of a word based on the context in which it appears.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD) is the task of identifying the intended meaning of a word based on the context in which it appears", "start_pos": 0, "end_pos": 130, "type": "Description", "confidence": 0.7332825601100922}]}, {"text": "It has been studied since the beginnings of Natural Language Processing (NLP)) and today it is still a central topic of this discipline.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP))", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7857615152994791}]}, {"text": "This because it is important for many NLP tasks such as text understanding), text entailment (, machine translation (), opinion mining), sentiment This problem is illustrated in these sentences: r There is a financial institution near the riverbank.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7867786586284637}, {"text": "text entailment", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7452429234981537}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.6987680196762085}, {"text": "opinion mining", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.7312166690826416}]}, {"text": "r They were troubled by insects while playing cricket.", "labels": [], "entities": []}, {"text": "In these two sentences 1 the meaning of the words bank and cricket can be misinterpreted by a centrality algorithm that tries to find the most important node in the graph composed of all the possible senses of the words in the sentence.", "labels": [], "entities": []}, {"text": "This because the meanings of the words financial and institution tend to shift the meaning of the word bank toward its financial meaning and not toward its naturalistic meaning.", "labels": [], "entities": []}, {"text": "The same behavior can be observed for the word cricket, which is shifted by the word insect toward its insect meaning and not toward its game meaning.", "labels": [], "entities": []}, {"text": "In our work, the disambiguation task is performed imposing a stronger importance on the relations between the words bank and river for the first sentence and between cricket and play for the second; exploiting proximity relations.", "labels": [], "entities": []}, {"text": "Our approach is based on the principle that the senses of the words that share a strong relation must be similar.", "labels": [], "entities": []}, {"text": "The idea of assigning a similar class to similar objects has been implemented in a different way by, within a Markov random field framework.", "labels": [], "entities": []}, {"text": "They have shown that it is beneficial in combinatorial optimization problems.", "labels": [], "entities": [{"text": "combinatorial optimization", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7656986117362976}]}, {"text": "In our case, this idea can preserve the textual coherence-a characteristic that is missing in many state-of-the-art systems.", "labels": [], "entities": []}, {"text": "In particular, it is missing in systems in which the words are disambiguated independently.", "labels": [], "entities": []}, {"text": "On the contrary, our approach disambiguates all the words in a text concurrently, using an underlying structure of interconnected links, which models the interdependence between the words.", "labels": [], "entities": []}, {"text": "In so doing, we model the idea that the meaning for any word depends at least implicitly on the combined meaning of all the interacting words.", "labels": [], "entities": []}, {"text": "In our study, we model these interactions by developing a system in which it is possible to map lexical items onto concepts exploiting contextual information in away in which collocated words influence each other simultaneously, imposing constraints in order to preserve the textual coherence.", "labels": [], "entities": []}, {"text": "For this reason, we have decided to use a powerful tool, derived from game theory: the non-cooperative game (see Section 4).", "labels": [], "entities": []}, {"text": "In our system, the nodes of the graph are interpreted as players, in the game theoretic sense (see Section 4), that play a game with the other words in the graph in order to maximize their utility; constraints are defined as similarity measures among the senses of two words that are playing a game.", "labels": [], "entities": []}, {"text": "The concept of utility has been used in different ways in the game theory literature; in general, it refers to the satisfaction that a player derives from the outcome of a game.", "labels": [], "entities": []}, {"text": "From our point of view, increasing the utility of a word means increasing the textual coherence in a distributional semantics perspective.", "labels": [], "entities": []}, {"text": "In fact, it has been shown that collocated words tend to have a determined meaning (.", "labels": [], "entities": []}, {"text": "Game theoretic frameworks have been used in different ways to study language use) and evolution, but to the best of our knowledge, our method is the first attempt to use it in a specific NLP task.", "labels": [], "entities": []}, {"text": "This choice is motivated by the fact that game theoretic models are able to perform a consistent labeling of the data, taking into account contextual information.", "labels": [], "entities": []}, {"text": "These features are of great importance for an unsupervised or semi-supervised algorithm, which tries to perform a WSD task, because, by assumption, the sense of a word is given by the context in which it appears.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 114, "end_pos": 122, "type": "TASK", "confidence": 0.9187451601028442}]}, {"text": "Within a game theoretic framework we are able to cast the WSD problem as a continuous optimization problem, exploiting contextual information in a dynamic way.", "labels": [], "entities": [{"text": "WSD problem", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9096487760543823}]}, {"text": "Furthermore, no supervision is required and the system can adapt easily to different contextual domains, which is exactly what is required fora WSD algorithm.", "labels": [], "entities": []}, {"text": "The additional reason for the use of a consistent labeling system relies on the fact that it is able to deal with semantic drifts.", "labels": [], "entities": []}, {"text": "In fact, as shown in the two example sentences, concentrating the disambiguation task of a word on highly collocated words, taking into account proximity (or even syntactic) information, allows the meaning interpretation to be guided only towards senses that are strongly related to the word that has to be disambiguated.", "labels": [], "entities": []}, {"text": "In this article, we provide a detailed discussion about the motivation behind our approach and a full evaluation of our algorithm, comparing it with state-of-the-art systems in WSD tasks.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 177, "end_pos": 186, "type": "TASK", "confidence": 0.9320330321788788}]}, {"text": "Ina previous work we used a similar algorithm in a semisupervised scenario, casting the WSD task as a graph transduction problem.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 88, "end_pos": 96, "type": "TASK", "confidence": 0.8141287565231323}]}, {"text": "Now we have extended that work, making the algorithm fully unsupervised.", "labels": [], "entities": []}, {"text": "Furthermore, in this article we provide a complete evaluation of the algorithm extending our previous works , exploiting proximity relations among words.", "labels": [], "entities": []}, {"text": "An important feature of our approach is that it is versatile.", "labels": [], "entities": []}, {"text": "In fact, the method can adapt to different scenarios and to different tasks, and it is possible to use it as unsupervised or semi-supervised.", "labels": [], "entities": []}, {"text": "The semi-supervised approach, presented in Tripodi,, is a bootstrapping graph-based method, which propagates, over the graph, the information from labeled nodes to unlabeled.", "labels": [], "entities": []}, {"text": "In this article, we also provide anew semi-supervised version of the approach, which can exploit the evidence from sense tagged corpora or the most frequent sense heuristic and does not require labeled nodes to propagate the labeling information.", "labels": [], "entities": []}, {"text": "We tested our approach on different data sets from WSD and entity-linking tasks in order to find the similarity measures that perform better, and evaluated our approach against unsupervised, semi-supervised, and supervised state-of-the-art systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8783071637153625}]}, {"text": "The results of this evaluation show that our method performs well and can be considered as a valid alternative to current models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe how the parameters of the presented method have been found and how it has been tested and compared with state-of-the-art systems 12 in Section 6.1 and Section 6.2, respectively.", "labels": [], "entities": []}, {"text": "We describe the data sets used for the tuning and for the evaluation of our model and the different settings used to test it.", "labels": [], "entities": []}, {"text": "The results of our experiments using WordNet as knowledge base are described in Section 6.2.1, where two different implementations of the system are proposed-the unsupervised and the supervised.", "labels": [], "entities": [{"text": "Section 6.2.1", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.8628131151199341}]}, {"text": "In Section 6.2.1 we compare our results with state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Finally, the results of the experiments using BabelNet as knowledge base, related to WSD and entity disambiguation, are described in Section 6.2.2.", "labels": [], "entities": [{"text": "WSD", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.964163064956665}, {"text": "entity disambiguation", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7163452953100204}]}, {"text": "The results are provided as F1, computed according to the following equation: F1 is a measure that determines the weighted harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.998813271522522}, {"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9991762042045593}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9974516034126282}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9913205504417419}]}, {"text": "Precision is defined as the number of correct answers divided by the number of provided answers; and recall is defined as the number of correct answers divided by the total number of answers to be provided.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9883865714073181}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9994925260543823}]}, {"text": "We evaluated our algorithm with three fine-grained data sets: Senseval-2 English allwords (S2)), Senseval-3 English all-words (S3) (Snyder and Palmer", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  The Prisoner's Dilemma.", "labels": [], "entities": [{"text": "Prisoner's Dilemma", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6623883644739786}]}, {"text": " Table 2  Results as F1 for S10. The first result with a statistically significant difference from the best  (bold result) is marked with * (\u03c7 2 , p < 0.05).", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9976396560668945}]}, {"text": " Table 3  Results as F1 for S15. The first result with a statistically significant difference from the best  (bold result) is marked with * (\u03c7 2 , p < 0.05).", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9977834820747375}, {"text": "S15", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8810443878173828}]}, {"text": " Table 4  Detailed results as F1 for the four data sets studied with tf-idf and mdice as measures. The results  show the performance of our unsupervised (uns) and semi-supervised (ssup) system and the  results obtained using the most frequent sense heuristic (MFS). Detailed information about the  performance of the systems on different parts of speech are provided: nouns (N), verbs (V),  adjectives (A), and adverbs (R).", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9874846339225769}]}, {"text": " Table 5  Comparison with state-of-the-art algorithms: unsupervised (unsup.), semisupervised (semi sup.),  and supervised (sup.). MFS refers to the MFS heuristic computed on SemCor on each data set  and Best refers to the best supervised system for each competition. The results are provided as  F1 and the first result with a statistically significant difference from the best of each data set is  marked with * (\u03c7 2 , p < 0.05", "labels": [], "entities": []}, {"text": " Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (\u03c7 2 , p < 0.05).", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8660650253295898}, {"text": "entity linking", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7899149358272552}, {"text": "F1", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9988921284675598}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.999646782875061}, {"text": "KORE50", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.9323552250862122}]}]}