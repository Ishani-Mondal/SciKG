{"title": [{"text": "Representation of Linguistic Form and Function in Recurrent Neural Networks\u00b4Akos", "labels": [], "entities": [{"text": "Representation of Linguistic Form", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8697499632835388}]}], "abstractContent": [{"text": "We present novel methods for analyzing the activation patterns of recurrent neural networks from a linguistic point of view and explore the types of linguistic structure they learn.", "labels": [], "entities": []}, {"text": "As a case study, we use a standard standalone language model, and a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings: The VISUAL pathway is trained on predicting the representations of the visual scene corresponding to an input sentence, and the TEXTUAL pathway is trained to predict the next word in the same sentence.", "labels": [], "entities": [{"text": "predicting the representations of the visual scene corresponding to an input sentence", "start_pos": 214, "end_pos": 299, "type": "TASK", "confidence": 0.7725702474514643}, {"text": "TEXTUAL", "start_pos": 309, "end_pos": 316, "type": "METRIC", "confidence": 0.9474122524261475}]}, {"text": "We propose a method for estimating the amount of contribution of individual tokens in the input to the final prediction of the networks.", "labels": [], "entities": []}, {"text": "Using this method, we show that the VISUAL pathway pays selective attention to lexical categories and grammatical functions that carry semantic information, and learns to treat word types differently depending on their grammatical function and their position in the sequential structure of the sentence.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.9110811948776245}]}, {"text": "In contrast, the language models are comparatively more sensitive to words with a syntactic function.", "labels": [], "entities": []}, {"text": "Further analysis of the most informative n-gram contexts for each model shows that in comparison with the VISUAL pathway, the language models react more strongly to abstract contexts that represent syntactic constructions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) were introduced by as a connectionist architecture with the ability to model the temporal dimension.", "labels": [], "entities": []}, {"text": "They have proved popular for modeling language data as they learn representations of words and larger linguistic units directly from the input data, without feature engineering.", "labels": [], "entities": []}, {"text": "Variations of the RNN architecture have been applied in several NLP domains such as parsing () and machine translation (, as well as in computer vision applications such as image generation () and object segmentation ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9731440544128418}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.8302665650844574}, {"text": "image generation", "start_pos": 173, "end_pos": 189, "type": "TASK", "confidence": 0.7445961833000183}, {"text": "object segmentation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7708063125610352}]}, {"text": "RNNs are also important components of systems integrating vision and language-for example, image and video captioning ().", "labels": [], "entities": [{"text": "image and video captioning", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.642787829041481}]}, {"text": "These networks can represent variable-length linguistic expressions by encoding them into a fixed-size low-dimensional vector.", "labels": [], "entities": []}, {"text": "The nature and the role of the components of these representations are not directly interpretable as they area complex, nonlinear function of the input.", "labels": [], "entities": []}, {"text": "There have recently been numerous efforts to visualize deep models such as convolutional neural networks in the domain of computer vision, but much less so for variants of RNNs and for language processing.", "labels": [], "entities": [{"text": "language processing", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7367995381355286}]}, {"text": "The present article develops novel methods for uncovering abstract linguistic knowledge encoded by the distributed representations of RNNs, with a specific focus on analyzing the hidden activation patterns rather than word embeddings and on the syntactic generalizations that models learn to capture.", "labels": [], "entities": []}, {"text": "In the current work we apply our methods to a specific architecture trained on specific tasks, but also provide pointers about how to generalize the proposed analysis to other settings.", "labels": [], "entities": []}, {"text": "As our case study we picked the IMAGINET model introduced by.", "labels": [], "entities": [{"text": "IMAGINET", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.5064916014671326}]}, {"text": "It is a multi-task, multi-modal architecture consisting of two gatedrecurrent unit (GRU) () pathways and a shared word embedding matrix.", "labels": [], "entities": []}, {"text": "One of the GRUs (VISUAL) is trained to predict image vectors given image descriptions, and the other pathway (TEXTUAL) is a language model, trained to sequentially predict each word in the descriptions.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.5998180508613586}, {"text": "TEXTUAL", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9893437623977661}]}, {"text": "This particular architecture allows a comparative analysis of the hidden activation patterns between networks trained on two different tasks, while keeping the training data and the word embeddings fixed.", "labels": [], "entities": []}, {"text": "Recurrent neural language models akin to TEXTUAL, which are trained to predict the next symbol in a sequence, are relatively well understood, and there have been some attempts to analyze their internal states.", "labels": [], "entities": [{"text": "TEXTUAL", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.7333634495735168}]}, {"text": "In constrast, VISUAL maps a complete sequence of words to a representation of a corresponding visual scene and is a less commonly encountered, but more interesting, model from the point of view of representing meaning conveyed via linguistic structure.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.9444939494132996}]}, {"text": "For comparison, we also consider a standard standalone language model.", "labels": [], "entities": []}, {"text": "We report a thorough quantitative analysis to provide a linguistic interpretation of the networks' activation patterns.", "labels": [], "entities": []}, {"text": "We present a series of experiments using a novel method we call omission score to measure the importance of input tokens to the final prediction of models that compute distributed representations of sentences.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a more global measure for estimating the informativeness of various types of n-gram contexts for each model.", "labels": [], "entities": []}, {"text": "These techniques can be applied to various RNN architectures such as recursive neural networks and convolutional neural networks.", "labels": [], "entities": []}, {"text": "Our experiments show that the VISUAL pathway in general pays special attention to syntactic categories that carry semantic content, and particularly to nouns.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 30, "end_pos": 36, "type": "TASK", "confidence": 0.961891233921051}]}, {"text": "More surprisingly, this pathway also learns to treat word types differently depending on their grammatical function and their position in the sequential structure of the sentence.", "labels": [], "entities": []}, {"text": "In contrast, the TEXTUAL pathway and the standalone language model are especially sensitive to the local syntactic characteristics of the input sentences.", "labels": [], "entities": [{"text": "TEXTUAL", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.7611924409866333}]}, {"text": "Further analysis of the most informative n-gram contexts for each model shows that whereas the VISUAL pathway is mostly sensitive to lexical (i.e., token n-gram) contexts, the language models react more strongly to abstract contexts (i.e., dependency relation n-grams) that represent syntactic constructions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report a series of experiments in which we explore the kinds of linguistic regularities the networks learn from word-level input.", "labels": [], "entities": []}, {"text": "In Section 4.1 we introduce omission score, a metric to measure the contribution of each token to the prediction of the networks, and in Section 4.2 we analyze how omission scores are distributed over dependency relations and part-of-speech categories.", "labels": [], "entities": []}, {"text": "In Section 4.3 we investigate the extent to which the importance of words for the different networks depends on the words themselves, their sequential position, and their grammatical function in the sentences.", "labels": [], "entities": []}, {"text": "Finally, in Section 4.4 we systematically compare the types of n-gram contexts that trigger individual dimensions in the hidden layers of the networks, and discuss their level of abstractness.", "labels": [], "entities": []}, {"text": "In all these experiments we report our findings based on the IMAGINET model, and whenever appropriate compare it with our two other models LM and SUM.", "labels": [], "entities": [{"text": "IMAGINET", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8876328468322754}]}, {"text": "For all the experiments, we trained the models on the training portion of the MSCOCO imagecaption data set (), and analyzed the representations of the sentences in the validation set corresponding to 5000 randomly chosen images.", "labels": [], "entities": [{"text": "MSCOCO imagecaption data set", "start_pos": 78, "end_pos": 106, "type": "DATASET", "confidence": 0.9113453477621078}]}, {"text": "The target image representations were extracted from the pre-softmax layer of the 16-layer CNN of Simonyan and Zisserman (2015).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Proportion of variance in omission scores explained by linear regression.", "labels": [], "entities": []}]}