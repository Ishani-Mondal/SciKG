{"title": [{"text": "AutoExtend: Combining Word Embeddings with Semantic Resources", "labels": [], "entities": []}], "abstractContent": [{"text": "We present AutoExtend, a system that combines word embeddings with semantic resources by learning embeddings for non-word objects like synsets and entities and learning word embeddings that incorporate the semantic information from the resource.", "labels": [], "entities": []}, {"text": "The method is based on encoding and decoding the word embeddings and is flexible in that it can take any word embeddings as input and does not need an additional training corpus.", "labels": [], "entities": []}, {"text": "The obtained embeddings live in the same vector space as the input word embeddings.", "labels": [], "entities": []}, {"text": "A sparse tensor formalization guarantees efficiency and parallelizability.", "labels": [], "entities": []}, {"text": "We use WordNet, GermaNet, and Freebase as semantic resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9693055748939514}, {"text": "Freebase", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9521397352218628}]}, {"text": "AutoExtend achieves state-of-the-art performance on Word-in-Context Similarity and Word Sense Disambiguation tasks.", "labels": [], "entities": [{"text": "Word-in-Context Similarity", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6559051275253296}, {"text": "Word Sense Disambiguation tasks", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6906792297959328}]}], "introductionContent": [{"text": "Unsupervised methods for learning word embeddings are widely used in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.7980874677499136}]}, {"text": "The only data these methods need as input are very large corpora.", "labels": [], "entities": []}, {"text": "However, in addition to corpora, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9447801113128662}]}, {"text": "We will simply refer to these as resources.", "labels": [], "entities": []}, {"text": "In this article, we present AutoExtend, a method for enriching these valuable resources with embeddings for non-word objects they describe; for example, AutoExtend enriches WordNet with embeddings for synsets.", "labels": [], "entities": []}, {"text": "The word embeddings and the new non-word embeddings live in the same vector space.", "labels": [], "entities": []}, {"text": "Many NLP applications benefit if non-word objects described by resources-such as synsets in WordNet-are also available as embeddings.", "labels": [], "entities": []}, {"text": "For example, in sentiment analysis, showed the superiority of sensebased features over word-based features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9621121883392334}]}, {"text": "Generally, the arguments for the utility of embeddings for words carryover to the utility of embeddings for non-word objects like synsets in WordNet.", "labels": [], "entities": []}, {"text": "We demonstrate this by improved performance thanks to AutoExtend embeddings for non-word objects in experiments on Word-in-Context Similarity, Word Sense Disambiguation (WSD), and several other tasks.", "labels": [], "entities": [{"text": "Word-in-Context Similarity", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.6388018578290939}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 143, "end_pos": 174, "type": "TASK", "confidence": 0.7363418688376745}]}, {"text": "To extend a resource with AutoExtend, we first formalize it as a graph in which (i) objects of the resource (both word objects and non-word objects) are nodes and (ii) edges describe relations between nodes.", "labels": [], "entities": []}, {"text": "These relations can be of an additive or a similarity nature.", "labels": [], "entities": []}, {"text": "Additive relations capture the basic intuition of the offset calculus) as we will discuss in detail in Section 2.", "labels": [], "entities": []}, {"text": "Similarity relations simply define similar nodes.", "labels": [], "entities": []}, {"text": "We then define various constraints based on these relations.", "labels": [], "entities": []}, {"text": "For example, one of our constraints states that the embeddings of two synsets related by the similarity relation should be close.", "labels": [], "entities": []}, {"text": "Finally, we select the set of embeddings that minimizes the learning objective.", "labels": [], "entities": []}, {"text": "The advantage of our approach is that it decouples (i) the learning of word embeddings on the one hand and (ii) the extension of these word embeddings to non-word objects in a resource on the other hand.", "labels": [], "entities": []}, {"text": "If someone identifies a better way of learning word embeddings, AutoExtend immediately can extend these embeddings to similarly improved embeddings for non-word objects.", "labels": [], "entities": []}, {"text": "We do not rely on any specific properties of word embeddings that make them usable in some resources but not in others.", "labels": [], "entities": []}, {"text": "The main contributions of this article are as follows.", "labels": [], "entities": []}, {"text": "We present AutoExtend, a flexible method that extends word embeddings to embeddings of non-word objects.", "labels": [], "entities": []}, {"text": "We demonstrate the generality and flexibility of AutoExtend by running experiments on three different resources: WordNet (Fellbaum 1998),, and GermaNet (Hamp,).", "labels": [], "entities": [{"text": "WordNet (Fellbaum 1998)", "start_pos": 113, "end_pos": 136, "type": "DATASET", "confidence": 0.8883760094642639}, {"text": "GermaNet (Hamp", "start_pos": 143, "end_pos": 157, "type": "DATASET", "confidence": 0.7037584185600281}]}, {"text": "AutoExtend does not require manually labeled corpora.", "labels": [], "entities": [{"text": "AutoExtend", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9405731558799744}]}, {"text": "In fact, it does not require any corpora.", "labels": [], "entities": []}, {"text": "All we need as input is a set of word embeddings and a resource that can be formally modeled as a graph in the way described above.", "labels": [], "entities": []}, {"text": "We show that AutoExtend achieves state-of-the-art performance on several tasks including WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.6899844408035278}]}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the AutoExtend model.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the three resources we use in our experiments and how we model them.", "labels": [], "entities": []}, {"text": "We evaluate the embeddings of word and non-word objects in Section 4 using the tasks of WSD, Entity Linking, Word Similarity, Word-in-Context Similarity, and Synset Alignment.", "labels": [], "entities": [{"text": "WSD", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.5041343569755554}, {"text": "Entity Linking", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.6653306931257248}, {"text": "Word Similarity", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7122987508773804}, {"text": "Word-in-Context Similarity", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.7083393186330795}, {"text": "Synset Alignment", "start_pos": 158, "end_pos": 174, "type": "TASK", "confidence": 0.9252487421035767}]}, {"text": "Finally, we give an overview of related work in Section 5 and present our conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate AutoExtend embeddings on the following tasks: WSD, Entity Linking, Word-in-Context Similarity, Word Similarity, and Synset Alignment.", "labels": [], "entities": [{"text": "WSD", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.5872122049331665}, {"text": "Entity Linking", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.6621581614017487}, {"text": "Word-in-Context Similarity", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6803610026836395}, {"text": "Word Similarity", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.7372494339942932}, {"text": "Synset Alignment", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.9217126369476318}]}, {"text": "Our results depend directly on the quality of the underlying word embeddings.", "labels": [], "entities": []}, {"text": "We would expect even better evaluation results as word representation learning methods improve.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8287164171536764}]}, {"text": "Using anew and improved set of underlying embeddings in AutoExtend is simple: It is a simple switch of the input file that contains the word embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Number of similarity relations by part-of-speech.", "labels": [], "entities": []}, {"text": " Table 2  Number of items in different resources and after the intersection with word2vec vectors (w2v).  The analogs of synsets in Freebase are entities (\"e:\") and types (\"t:\").", "labels": [], "entities": []}, {"text": " Table 4  WSD and Entity Linking accuracy for different feature sets and systems. Best result in each  column in bold. Results of development sets are italic. Results significantly worse than the best  (bold) result in each column are marked  \u2020 for \u03b1 = 0.05 and  \u2021 for \u03b1 = 0.10 (one-tailed Z-test).", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7938982248306274}, {"text": "Entity Linking", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.5648285746574402}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9273903965950012}]}, {"text": " Table 7  Spearman correlation (\u03c1 \u00d7 100). Best result per column in bold. Results of development sets are  italic. Results significantly worse or better than the baseline (line 1) in each column are marked  \u2020  for \u03b1 = 0.05 and  \u2021 for \u03b1 = 0.10 (one-tailed Z-test).", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7370133399963379}]}, {"text": " Table 8  Accuracy of development and test set on the Synset Alignment task. Best result per column in  bold. Results of development set are italic. Results significantly worse than the best result are  marked  \u2020 for \u03b1 = 0.05 (one-tailed Z-test) and  \u2021 for \u03b1 = 0.10 (one-tailed Z-test).", "labels": [], "entities": [{"text": "Synset Alignment task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.9158105055491129}]}, {"text": " Table 9  Optimal weighting of the three constraints (word, lexeme, similarity) for different tasks.", "labels": [], "entities": []}]}