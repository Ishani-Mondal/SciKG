{"title": [{"text": "Framing QA as Building and Ranking Intersentence Answer Justifications", "labels": [], "entities": [{"text": "Framing QA", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6624502539634705}]}], "abstractContent": [{"text": "We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8401037693023682}]}, {"text": "Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information.", "labels": [], "entities": []}, {"text": "We then jointly rank answers and their justifications using a reranking perceptron that treats justification quality as a latent variable.", "labels": [], "entities": []}, {"text": "We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches.", "labels": [], "entities": []}, {"text": "Our best configuration answers 44% of the questions correctly, where the top justifications for 57% of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer.", "labels": [], "entities": []}, {"text": "We include a detailed characterization of the justification quality for both our method and a strong baseline, and show that information aggregation is key to addressing the information need in complex questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "To encourage an emphasis on the task of explainable inference for question answering (QA), Clark (2015) introduced the Aristo challenge, a QA task focused on developing methods of automated inference capable of passing standardized elementary school science exams while also providing human-readable explanations (or justifications) for those answers.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8496213912963867}]}, {"text": "Science exams are an important proving ground for QA because inference is often required to arrive at a correct answer, and, commonly, incorrect answers that are high semantic associates of either the question or correct answer are included to \"lure\" students (or automated methods) away from the correct response.", "labels": [], "entities": [{"text": "QA", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9723178148269653}]}, {"text": "Adding to the challenge, not only is inference required for science exams, but several kinds of inference are present.", "labels": [], "entities": []}, {"text": "In an analysis of three years of standardized science exams, identified three main categories of questions based on the methods likely required to answer them correctly.", "labels": [], "entities": []}, {"text": "Examples of these questions can be seen in, highlighting that 65% of questions require some form of inference to be answered correctly.", "labels": [], "entities": []}, {"text": "We propose a QA approach that jointly addresses answer extraction and justification.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.894380122423172}]}, {"text": "We believe that justifying why the QA algorithm believes an answer is correct is, in many cases, a critical part of the QA process.", "labels": [], "entities": []}, {"text": "For example, in the medical domain, a user would not trust a system that recommends invasive procedures without giving a justification as to why (e.g., \" found procedure X healed 90% of patients with heart disease who also had secondary pulmonary complications\").", "labels": [], "entities": []}, {"text": "A QA tool is clearly more useful when its human user can identify both when it functions correctly and when it delivers an incorrect or misleading result-especially in situations where incorrect results carry a high cost.", "labels": [], "entities": []}, {"text": "To address these issues, here we reframe QA from the task of scoring (or reranking) answers to a process of generating and evaluating justifications for why a particular answer candidate is correct.", "labels": [], "entities": [{"text": "QA from the task of scoring (or reranking) answers to a process of generating and evaluating justifications for why a particular answer candidate is correct", "start_pos": 41, "end_pos": 197, "type": "Description", "confidence": 0.7560646843027186}]}, {"text": "We focus on answering science exam questions, where many questions require complex inferences, and where building and evaluating answer justifications is challenging.", "labels": [], "entities": []}, {"text": "In particular, we construct justifications by aggregating Categories of questions and their relative frequencies as identified by.", "labels": [], "entities": []}, {"text": "Retrieval-based questions (including is-a, dictionary definition, and property identification questions) tend to be answerable using information retrieval methods over structured knowledge bases, including taxonomies and dictionaries.", "labels": [], "entities": [{"text": "dictionary definition", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6616782993078232}, {"text": "property identification questions", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.7683287759621938}]}, {"text": "More complex general inference questions make use of either simple inference rules that apply to a particular situation, a knowledge of causality, or a knowledge of simple processes (such as solids melt when heated).", "labels": [], "entities": []}, {"text": "Difficult model-based reasoning questions require a domain-specific model of how a process works, like how gravity causes planets to orbit stars, in order to be correctly answered.", "labels": [], "entities": []}, {"text": "Note that we do not include diagram questions, as they require specialized spatial reasoning that is beyond the scope of this work.", "labels": [], "entities": []}, {"text": "multiple sentences from a number of textual knowledge bases (e.g., study guides, science dictionaries), which, in combination, aim to explain the answer.", "labels": [], "entities": []}, {"text": "We then rank these candidate justifications based on a number of measures designed to assess how well-integrated, relevant, and on-topic a given justification is, and select the answer that corresponds to the highest-ranked justification.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2  Focus word decomposition of an example question, suggesting the question is primarily about  measuring the speed of walking, and not about turtles or paths. (Correct answer: \"a stopwatch  and meter stick.\") For a given word: Conc refers to the psycholinguistic concreteness score, Tag  refers to the focus word category (FOCUS signifies a focus word, EX an example word, ATYPE  an answer-type word, and ST a stop word), Score refers to the focus word score, and Weight  refers to the normalized focus word scores.", "labels": [], "entities": [{"text": "Focus word decomposition", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6668359239896139}, {"text": "ATYPE", "start_pos": 381, "end_pos": 386, "type": "METRIC", "confidence": 0.9492741823196411}, {"text": "Score", "start_pos": 430, "end_pos": 435, "type": "METRIC", "confidence": 0.990345299243927}, {"text": "Weight", "start_pos": 472, "end_pos": 478, "type": "METRIC", "confidence": 0.9720240235328674}]}, {"text": " Table 5  Performance as a function of justification length in sentences (or, the number of graphlets in a  TAG) for two models: one aware of connection-type, and one that is not. Bold font indicates the  best score in a given column for each model group.", "labels": [], "entities": []}, {"text": " Table 6  Performance of the baseline and best-performing TAG models, both separately and in  combination. TAG justifications of different short lengths were found to best combine in single  classifiers (denoted with a +), where models that combine the CR baseline or long (3G) TAG  justifications best combined using voting ensembles (denoted with a \u222a). Bold font indicates the  best score in a given column for each model group. Asterisks indicate that a score is significantly  better than the highest-performing baseline model (* signifies p < 0.05, ** signifies p < 0.01). The  dagger indicates that a score is significantly higher than the score in the line number indicated in  superscript (p < 0.01). All significance tests were implemented using one-tailed non-parametric  bootstrap resampling using 10,000 iterations.", "labels": [], "entities": []}, {"text": " Table 8  At least one justification performance for both CR and TAG models, reflecting the highest rating  attained by at least one of the top six justifications for a given question.", "labels": [], "entities": []}, {"text": " Table 9  Performance of the Latent Ranking Neural Network models. Models with CR include the  candidate retrieval scores as input, models with TAG use the features from the best performing  TAG model (1G CT +2G CT ), and models with embeddings include an average embedding for each  of the questions, the answers, and the text from which the justification graphlet was derived.  Significance tests were performed using bootstrap resampling with 10,000 iterations, but none of  the differences between the neural network models and the CR baseline were significant.", "labels": [], "entities": []}, {"text": " Table 10  Precision@1 by grade level.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.7669767737388611}]}, {"text": " Table 11  Most useful knowledge resources for justifications classified as \"good.\"", "labels": [], "entities": []}, {"text": " Table 12  Proportion of good justifications with a given number of connecting word categories (Q, A, X) for  both correct and incorrect answers", "labels": [], "entities": []}, {"text": " Table 13  A summary of the inference type necessary for incorrectly answered questions. The summary is  broken down into three categories: incorrectly answered questions with a good justification in  the top six, incorrectly answered questions without a good justification in the top six, as well as  the overall proportions across these two conditions.", "labels": [], "entities": []}, {"text": " Table 14  A summary of the classes of the errors made by the system. On any given question, more than  one error may have been made. The summary is broken down into three categories: incorrectly  answered questions with a good justification in the top six, incorrectly answered questions  without a good justification in the top six, as well as the overall proportions across these two  conditions.", "labels": [], "entities": []}]}