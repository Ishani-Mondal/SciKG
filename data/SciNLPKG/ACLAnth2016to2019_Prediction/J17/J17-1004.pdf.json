{"title": [{"text": "Argumentation Mining in User-Generated Web Discourse", "labels": [], "entities": [{"text": "Argumentation Mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7468940913677216}]}], "abstractContent": [{"text": "The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people's argumentation.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.9532827436923981}]}, {"text": "In this article, we go beyond the state of the art in several ways.", "labels": [], "entities": []}, {"text": "(i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse.", "labels": [], "entities": []}, {"text": "(ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study.", "labels": [], "entities": []}, {"text": "(iii) We create anew gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components.", "labels": [], "entities": []}, {"text": "We offer the data, source codes, and annotation guidelines to the community under free licenses.", "labels": [], "entities": []}, {"text": "Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.9755707383155823}]}], "introductionContent": [{"text": "The art of argumentation has been studied since the early work of Aristotle, dating back to the 4th century BCE (Aristotle and Kennedy.", "labels": [], "entities": []}, {"text": "It has been exhaustively examined from different perspectives, such as philosophy, psychology, communication studies, cognitive science, formal and informal logic, linguistics, computer science, educational research, and many others.", "labels": [], "entities": []}, {"text": "Ina recent and critically wellacclaimed study, Mercier and Sperber (2011) even claim that argumentation is what drives humans to perform reasoning.", "labels": [], "entities": []}, {"text": "From the pragmatic perspective, argumentation can be seen as a verbal activity oriented towards the realization of a goal) or more in detail as a verbal, social, and rational activity aimed at convincing a reasonable critic of the acceptability of a standpoint by putting forward a constellation of one or more propositions to justify this standpoint.", "labels": [], "entities": []}, {"text": "Analyzing argumentation from the computational linguistics point of view has very recently led to anew field called argumentation mining).", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.8488669097423553}]}, {"text": "Despite the lack of an exact definition, researchers within this field usually focus on analyzing discourse on the pragmatics level and applying a certain argumentation theory to model and analyze textual data 1 at hand.", "labels": [], "entities": []}, {"text": "Our motivation for argumentation mining stems from a practical information seeking perspective from the user-generated content on the Web.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9416059255599976}]}, {"text": "For example, when users search for information in user-generated Web content to facilitate their personal decision-making related to controversial topics, they lack tools to overcome the current information overload.", "labels": [], "entities": []}, {"text": "One particular use-case example dealing with a forum post discussing private versus public schools is shown in.", "labels": [], "entities": []}, {"text": "Here, the lengthy text on the left-hand side is transformed into an argument gist on the right-hand side by (i) analyzing argument components and (ii) summarizing their content.", "labels": [], "entities": []}, {"text": "shows another use-case example, in which users search for reasons that underpin certain standpoints in a given controversy (which is homeschooling in this case).", "labels": [], "entities": []}, {"text": "In general, the output of automatic argument analysis performed on the large scale in Web data can provide users with analyzed arguments to a given topic of interest, find the evidence for the given controversial standpoint, or help to reveal flaws in argumentation of others.", "labels": [], "entities": [{"text": "automatic argument analysis", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6400034129619598}]}, {"text": "Satisfying these information needs cannot be directly tackled by current methods (e.g., opinion mining, question answering, 2 or summarization 3 ), and requires novel approaches within the argumentation mining field.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8418844044208527}, {"text": "question answering, 2", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.8380947709083557}, {"text": "argumentation mining", "start_pos": 189, "end_pos": 209, "type": "TASK", "confidence": 0.9274486303329468}]}, {"text": "Although user-generated Web content has already been considered in argumentation mining, many limitations and research gaps can be identified in the existing research.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9011363685131073}]}, {"text": "First, the scope of the current approaches is restricted to a particular domain or register-for example, hotel reviews (), Tweets related to local riot events (), student essays (Stab and Gurevych 2014a), airline passenger rights and consumer protection, or renewable energy sources ().", "labels": [], "entities": [{"text": "airline passenger rights and consumer protection", "start_pos": 205, "end_pos": 253, "type": "TASK", "confidence": 0.6162683020035425}]}, {"text": "Second, not all related works are tightly connected to argumentation theories, resulting in a gap between the substantial research in argumentation itself and its adaptation in natural language processing (NLP) applications.", "labels": [], "entities": []}, {"text": "Third, as an emerging research area, argumentation mining still suffers from alack of labeled corpora, which is crucial for designing, training, and evaluating the algorithms.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9686538279056549}]}, {"text": "Although some works have dealt with creating new data sets, the reliability (in terms of inter-annotator agreement) of the", "labels": [], "entities": [{"text": "reliability", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9968254566192627}]}], "datasetContent": [{"text": "This section presents experiments conducted on the annotated corpora introduced in Section 4.", "labels": [], "entities": []}, {"text": "We put the main focus on identifying argument components in the discourse.", "labels": [], "entities": []}, {"text": "To comply with machine learning terminology, in this section we will use the term We also experimented with classification of persuasive documents, as introduced in Annotation Study 1 (section 4.3).", "labels": [], "entities": []}, {"text": "This task can be seen as standard document-level two-class text classification.", "labels": [], "entities": [{"text": "document-level two-class text classification", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.5612659007310867}]}, {"text": "Using SVM (Cortes and Vapnik 1995) with Sequential Minimal Optimization (Platt 1999), polynomial kernel, and n-gram baseline features, we obtained 0.69 Macro-F 1 score.", "labels": [], "entities": []}, {"text": "We also used a rich feature set (a large part of features that will be discussed in Section 5.1) but the system did not beat the baseline, therefore we do not report on this experiment in detail.", "labels": [], "entities": []}, {"text": "However, we expect that in a real-world scenario of automatically analyzing argument components in user-generated content, the first step of assessing on-topic persuasiveness (or external relevance) is essential.", "labels": [], "entities": []}, {"text": "domain as an equivalent to a topic (remember that our data set includes six different topics; see Section 4.1).", "labels": [], "entities": []}, {"text": "We evaluate three different scenarios.", "labels": [], "entities": []}, {"text": "First, we report ten-fold cross-validation over a random ordering of the entire data set.", "labels": [], "entities": []}, {"text": "Second, we deal with in-domain ten-fold crossvalidation for each of the six domains.", "labels": [], "entities": []}, {"text": "Third, in order to evaluate the domain portability of our approach, we train the system on five domains and test on the remaining one for all six domains (which we report as cross-domain validation).", "labels": [], "entities": []}, {"text": "Because the smallest annotation unit is a token and the argument components do not overlap, we approach identification of argument components as a sequence labeling problem.", "labels": [], "entities": []}, {"text": "We use the BIO encoding, so each token belongs to one of the following 11 classes: O (not apart of any argument component), Backing-B, Backing-I, Claim-B, Claim-I, Premise-B, Premise-I, Rebuttal-B, Rebuttal-I, Refutation-B, Refutation-I. This is the minimal encoding that is able to distinguish two adjacent argument components of the same type.", "labels": [], "entities": []}, {"text": "In our data, 48% of all adjacent argument components of the same type are direct neighbors (there are no \"O\" tokens in between).", "labels": [], "entities": []}, {"text": "We report Macro-F 1 score and F 1 scores for each of the 11 classes as the main evaluation metric.", "labels": [], "entities": [{"text": "Macro-F 1 score", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7694681485493978}, {"text": "F 1 scores", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9878462155659994}]}, {"text": "This evaluation is performed on the token level, and for each token the predicted label must exactly match the gold data label (classification of tokens into 11 classes).", "labels": [], "entities": []}, {"text": "As instances for the sequence-labeling model, we chose sentences rather than tokens.", "labels": [], "entities": []}, {"text": "During our initial experiments, we observed that building a sequence labeling model for recognizing argument components as sequences of tokens is too fine-grained, as a In our approach to annotation of controversies, this would mean that the overall standpoint of the author is neutral but she presents arguments for both sides of the controversy.", "labels": [], "entities": []}, {"text": "30 This simplification can be seen as a limitation of our model, as argumentation mining in some related works is a form of structured predictions of elements in discourse where the explicit notion of relation between argument components is crucial for argument \"parsing,\" for example, in the work by envisioned in their earlier survey paper (Peldszus and Stede 2013a), or by Stab and Gurevych (2014b).", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.853002518415451}, {"text": "argument \"parsing", "start_pos": 253, "end_pos": 270, "type": "TASK", "confidence": 0.676620086034139}]}, {"text": "It is thus possible that in a general argumentative discourse, the same proposition can play two different roles in two arguments, similarly to the approach of.", "labels": [], "entities": []}, {"text": "This phenomena was discussed as divergent structures by and later elaborated on by Freeman (2011, page 16).", "labels": [], "entities": []}, {"text": "single token does not convey enough information that could be encoded as features fora machine learner.", "labels": [], "entities": []}, {"text": "However, as discussed in Section 4.4.5, the annotations were performed on data pre-segmented to sentences and annotating tokens was necessary only when the sentence segmentation was wrong or one sentence contained multiple argument components.", "labels": [], "entities": []}, {"text": "Our corpus consists of 3,899 sentences, from which 2,214 sentences (57%) contain no argument components.", "labels": [], "entities": []}, {"text": "From the remaining ones, only 50 sentences (1%) have more than one argument component.", "labels": [], "entities": []}, {"text": "Although in 19 cases (0.5%) the sentence contains a Claim-Premise pair, which is an important distinction from the argumentation perspective, given the overall small number of such occurrences, we simplify the task by treating each sentence as if it has either one argument component or none.", "labels": [], "entities": []}, {"text": "The approximation with sentence-level units is explained in the example in.", "labels": [], "entities": []}, {"text": "In order to evaluate the expected performance loss using this approximation, we used an oracle that always predicts the correct label for the unit (sentence) and evaluated it against the true labels (recall that the evaluation against the true gold labels is done always on token level).", "labels": [], "entities": []}, {"text": "We lose only about 10% of Macro-F 1 score (0.906) and only about 2% of accuracy (0.984).", "labels": [], "entities": [{"text": "Macro-F 1 score", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.7271941105524699}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9997518658638}]}, {"text": "This performance is still acceptable, while allowing us to model sequences where the minimal unit is a sentence.", "labels": [], "entities": []}, {"text": "shows the distribution of the classes in the gold data Toulmin, where the labeling was already mapped to the sentences.", "labels": [], "entities": [{"text": "gold data Toulmin", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.7502364714940389}]}, {"text": "The minimal presence of rebuttal and refutation (four classes account for only 3.4% of the data) makes this data set very unbalanced.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Topic and register distribution in the gold standard Toulmin corpus.", "labels": [], "entities": [{"text": "gold standard Toulmin corpus", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.8141026794910431}]}, {"text": " Table 3  Gold standard Toulmin corpus statistics.", "labels": [], "entities": [{"text": "Toulmin corpus statistics", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8649675448735555}]}, {"text": " Table 4  Inter-annotator agreement (Krippendorff's \u03b1 U ) across various registers, topics, and  argument components. Bold values emphasize \u03b1 U \u2265 0.50. Joint logos is a joint \u03b1 U for all  argument components in the logos dimension (claim, premise, backing, rebuttal, refutation).  HS = homeschooling; RS = redshirting; PIS = prayer in schools; SSE = single sex education;  MS = mainstreaming; PPS = private vs. public schools.", "labels": [], "entities": []}, {"text": " Table 5  Correlations between \u03b1 U and various measures on different data subsets. SC = full sentence  coverage; DL = document length; APL = average paragraph length; ASL = average sentence  length; ARI, C-L (Coleman-Liau), Flesch, LIX = readability measures. Bold numbers denote  statistically significant correlation (p < 0.05).", "labels": [], "entities": [{"text": "APL", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9809251427650452}, {"text": "ASL = average sentence  length", "start_pos": 167, "end_pos": 197, "type": "METRIC", "confidence": 0.7131869971752167}, {"text": "ARI", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.9519385099411011}, {"text": "Flesch, LIX", "start_pos": 224, "end_pos": 235, "type": "METRIC", "confidence": 0.7990007201830546}, {"text": "statistically significant correlation", "start_pos": 281, "end_pos": 318, "type": "METRIC", "confidence": 0.7311785419782003}]}, {"text": " Table 6  Probabilistic confusion matrix between all annotators.", "labels": [], "entities": []}, {"text": " Table 8  Class distribution of the gold data Toulmin corpus approximated to the sentence-level boundaries.", "labels": [], "entities": [{"text": "gold data Toulmin corpus", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.7530414462089539}]}, {"text": " Table 10  Results of classification of argument components in the in-domain cross-validation scenario.  Macro-F 1 scores reported, bold numbers denote the best results. HS = homeschooling, MS =  mainstreaming; PIS = prayer in schools; PPS = private vs. public schools; RS = redshirting; SSE =  single sex education. Results in the aggregated row are computed from an aggregated confusion  matrix over all domains. The differences between the best feature set combination (01234) and  others are statistically significant (p < 0.001; paired exact Liddell's test).", "labels": [], "entities": []}, {"text": " Table 11  Results of classification of argument components in the cross-domain scenario. Macro-F 1 scores  reported, bold numbers denote the best results. HS = homeschooling; MS = mainstreaming;  PIS = prayer in schools; PPS = private vs. public schools; RS = redshirting; SSE = single sex  education. Results in the aggregated row are computed from an aggregated confusion matrix over  all domains. The differences between the best feature set combination (4) and others are  statistically significant (p < 0.001; paired exact Liddell's test).", "labels": [], "entities": []}, {"text": " Table 13  Additional metrics to evaluate the performance of argument component identification applied to  the results of 10-fold cross-validation scenario", "labels": [], "entities": [{"text": "argument component identification", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.6593371629714966}]}]}