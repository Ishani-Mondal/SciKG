{"title": [{"text": "Adapting to Learner Errors with Minimal Supervision under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": []}], "abstractContent": [{"text": "This article considers the problem of correcting errors made by English as a Second Language writers from a machine learning perspective, and addresses an important issue of developing an appropriate training paradigm for the task, one that accounts for error patterns of non-native writers using minimal supervision.", "labels": [], "entities": [{"text": "correcting errors made by English as a Second Language writers", "start_pos": 38, "end_pos": 100, "type": "TASK", "confidence": 0.888503909111023}]}, {"text": "Existing training approaches present a trade-off between large amounts of cheap data offered by the native-trained models and additional knowledge of learner error patterns provided by the more expensive method of training on annotated learner data.", "labels": [], "entities": []}, {"text": "We propose a novel training approach that draws on the strengths offered by the two standard training paradigms-of training either on native or on annotated learner data-and that outperforms both of these standard methods.", "labels": [], "entities": []}, {"text": "Using the key observation that parameters relating to error regularities exhibited by non-native writers are relatively simple, we develop models that can incorporate knowledge about error regularities based on a small annotated sample but that are otherwise trained on native English data.", "labels": [], "entities": []}, {"text": "The key contribution of this article is the introduction and analysis of two methods for adapting the learned models to error patterns of non-native writers; one method that applies to generative classifiers and a second that applies to discriminative classifiers.", "labels": [], "entities": []}, {"text": "Both methods demonstrated state-of-the-art performance in several text correction competitions.", "labels": [], "entities": [{"text": "text correction competitions", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.8800106048583984}]}, {"text": "In particular, Volume 43, Number 4 the Illinois system that implements these methods ranked at the top in two recent CoNLL shared tasks on error correction.", "labels": [], "entities": [{"text": "error correction", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.630610853433609}]}, {"text": "1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We now present experiments demonstrating that the adaptation approaches with minimal supervision proposed in this work outperform the standard methods of training on native data and when training on annotated learner data alone.", "labels": [], "entities": []}, {"text": "In the following sections, we describe the data sets, and present key adaptation experiments on three error types.", "labels": [], "entities": []}, {"text": "Section 5 presents language-specific adaptation.", "labels": [], "entities": [{"text": "language-specific adaptation", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.7280324399471283}]}, {"text": "Depending on what training data source is used, we refer to the models as follows:  Various metrics have been proposed and used in error correction.", "labels": [], "entities": [{"text": "error correction", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.6494273543357849}]}, {"text": "These can be broken down roughly into metrics that compute the accuracy of the system and those that use the F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9992048144340515}, {"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9910959005355835}]}, {"text": "The HOO competitions adopted F1, which can take into account both precision and recall of the systems; the M2 scorer used in CoNLL is also F-based but can take into account phrase-based edits.", "labels": [], "entities": [{"text": "HOO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8520225286483765}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9984563589096069}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9991652965545654}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9977582097053528}, {"text": "CoNLL", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9031215310096741}]}, {"text": "Overall, accuracy and F-measure are equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995576739311218}, {"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9892874956130981}, {"text": "tuning", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9641380906105042}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9080387353897095}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9913508892059326}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9950927495956421}]}, {"text": "In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall).", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.8772954940795898}, {"text": "F0.5", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9986255168914795}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999704897403717}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9994876384735107}]}, {"text": "Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure.", "labels": [], "entities": [{"text": "error correction", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.6704048663377762}, {"text": "F1", "start_pos": 200, "end_pos": 202, "type": "METRIC", "confidence": 0.9566717743873596}, {"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9991284012794495}, {"text": "F-measure", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.9847404360771179}]}, {"text": "proposed an I-measure that is accuracy-based and proposed a variation of the BLEU metric used in Machine Translation (called GLEU).", "labels": [], "entities": [{"text": "accuracy-based", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9908122420310974}, {"text": "BLEU metric", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9776626229286194}, {"text": "Machine Translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7782199680805206}]}, {"text": "Further, and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) also compare the outputs of the systems in the CoNLL shared task against human judgments and show that the need for new metrics is motivated by alack of correlation between F1 and human judgments.", "labels": [], "entities": [{"text": "F1", "start_pos": 236, "end_pos": 238, "type": "METRIC", "confidence": 0.9456284046173096}]}, {"text": "However, the newly proposed GLEU metric does not fare much better in terms of correlation with human judgments than the F-measure and the I-measure.", "labels": [], "entities": [{"text": "GLEU metric", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.772344172000885}, {"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9043086767196655}]}, {"text": "Developing anew, more appropriate metric is an involved issue that is beyond the scope of this work.", "labels": [], "entities": []}, {"text": "We are not dealing with phrase-based edits here, so we follow the evaluation metrics based on precision, recall, and F1 that were used in the shared tasks on grammar correction.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.999427318572998}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9982707500457764}, {"text": "F1", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.999721109867096}, {"text": "grammar correction", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7411569207906723}]}, {"text": "One thing that can be observed is that the performance numbers are quite low, especially for preposition mistakes.", "labels": [], "entities": []}, {"text": "One reason that is related to this is the learner baseline, or the performance of non-native writers, which is quite high to begin with.", "labels": [], "entities": []}, {"text": "Typically, fewer than 10% of instances are erroneous (one would expect that a higher baseline [i.e., a lower error rate] would imply a more difficult learning task).", "labels": [], "entities": []}, {"text": "However, although the preposition baseline is lower than for the other mistakes, the performance is also the lowest, which seems counterintuitive.", "labels": [], "entities": []}, {"text": "We attribute this to the large confusion set (12 prepositions), and to the fact that, typically, preposition usage is highly ambiguous (multiple prepositions can be licensed in a given context), whereas the FCE annotation does not allow for multiple acceptable answers.", "labels": [], "entities": [{"text": "FCE", "start_pos": 207, "end_pos": 210, "type": "DATASET", "confidence": 0.8674564957618713}]}, {"text": "De Felice (2008) investigated preposition performance when going beyond the top match and found that performance would improve when looking at the top n candidate corrections.", "labels": [], "entities": []}, {"text": "Given that preposition usage is highly variable, we conduct further evaluation using the top three choices provided by the classifiers).", "labels": [], "entities": []}, {"text": "In the top three evaluation, a classifier's prediction is counted as \"correct\" if the gold label is among any of the top three candidates selected by the classifier.", "labels": [], "entities": []}, {"text": "F1 performance increases for all the models when top three choices are considered.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9812699556350708}]}, {"text": "However, adapted models fare much better: Precision increases substantially from 31.27 to 72.70 for AP-adapted and from 31.42 to 59.87 for NB-adapted models, whereas it only improves slightly from 26.07 to 31.84, and from 28.47 to 44.13 for their respective native-trained counterparts.", "labels": [], "entities": [{"text": "Precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.999419093132019}]}, {"text": "These improvements suggest that it is not just the first candidate that is better but the overall ranking is better in adapted models.", "labels": [], "entities": []}, {"text": "The top three evaluation thus provides additional evidence that knowledge of error patterns available in the adapted models is extremely important.", "labels": [], "entities": []}, {"text": "Whether presenting the top three candidates to the user is reasonable depends on the evaluation of how good the top three candidates are.", "labels": [], "entities": []}, {"text": "Thus, we also wish to assess top three candidates directly because the top three F1 measure values presented are computed fora gold standard that allows only a single correct answer, when in fact there maybe multiple possible correct answers.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9806050658226013}]}, {"text": "More specifically, given that preposition usage is variable, we would like to know how often the top three candidates include multiple valid options.", "labels": [], "entities": []}, {"text": "To this end, for each adapted model, we selected a sample of 200 preposition instances where the classifier's top choice was different from the gold label.", "labels": [], "entities": []}, {"text": "These were manually annotated by one of the authors, a native English speaker, who was given a preposition in a context (where by \"context\" we consider the entire sentence where the preposition appears) and was asked to specify how many of the top three choices are acceptable in a given context.", "labels": [], "entities": []}, {"text": "Results of the evaluation are shown in.", "labels": [], "entities": []}, {"text": "Column 1 shows the number of correct predictions among the top three candidates (between 0 and 3).", "labels": [], "entities": []}, {"text": "Each cell shows the percentage of cases with this number of correct predictions among the top three.", "labels": [], "entities": []}, {"text": "We found that for the majority of the cases, at least one of the top three candidates is valid; row 1 shows that only 6.3% and 14.3% of the examples for AP-and NB-adapted models, respectively, did not have a single correct candidate among the top three choices.", "labels": [], "entities": []}, {"text": "More importantly, 14.3% of examples for each of the classifiers have two or three valid preposition choices among the top three candidates (last two rows in the table).", "labels": [], "entities": []}, {"text": "We believe that these numbers confirm earlier findings with respect to the highly variable preposition usage and thus justify presenting multiple options to the user, at least for preposition mistakes.", "labels": [], "entities": []}, {"text": "Finally, it is interesting to note that AP-adapted has a higher percentage of correct prepositions in the top three evaluation than NB-adapted, even though NB-adapted does better in the top 1 evaluation.", "labels": [], "entities": []}, {"text": "We believe that this indicates that AP-adaptation is better than NBadaptation at ranking the appropriate candidates.", "labels": [], "entities": [{"text": "AP-adaptation", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.7374975681304932}, {"text": "NBadaptation", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.6719594597816467}]}, {"text": "This result is also consistent with those in that show that the precision (and F-score) of AP-adapted models in the top three evaluation improves from 31.27 to 72.70, whereas for NB-adapted models the improvement is more modest (from 31.42 to 59.87).", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9997391104698181}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9992992877960205}]}, {"text": "Lastly, as the results show, whereas precision numbers increase markedly, the recall does not.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.999234676361084}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9997180104255676}]}, {"text": "This happens because the top three evaluation is performed on models that were optimized with respect to top one evaluation.", "labels": [], "entities": []}, {"text": "It is also possible to optimize models based on top three evaluation, in which case both precision and recall would increase, resulting (most likely) in improved F1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9995354413986206}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9988032579421997}, {"text": "F1 scores", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9795344471931458}]}, {"text": "However, the evaluation and additional annotation that we perform already demonstrate how multiple preposition usage underestimates top one evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Confusion matrix for article errors used in the artificial errors method. Based on the training data  from the FCE corpus. The left column shows the correct article. Each row shows the author's  article choices for that label and Prob(source|label). The numbers next to the targets show the  count of the label (or source) in the data set.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.9805594086647034}]}, {"text": " Table 2  Confusion matrix for preposition errors used in the artificial errors method. Based on the  training data from the FCE corpus. The left column shows the correct preposition. Each row  shows the author's preposition choices for that label and Prob(source|label). The numbers next  to the targets show the count of the label (or source) in the data set. The table is based on the  confusion matrix for 12 prepositions but only rows corresponding to eight most frequent  prepositions are shown to make the table readable.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9854761064052582}, {"text": "Prob", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.936189591884613}]}, {"text": " Table 3  Confusion matrix for verb agreement errors used in the artificial errors method. Based on the  training data from the FCE corpus. S and INF stand for third person singular and bare verb  form, respectively. The left column shows the correct verb. Each row shows the author's verb  choices for that label and Prob(source|label). The numbers next to the targets show the count of  the label (or source) in the data set.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.9841714799404144}, {"text": "INF", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9813025593757629}]}, {"text": " Table 4  Artificial errors. Percentage of preposition examples converted into artificial mistakes in  training, using the inflation method with different inflation rates.", "labels": [], "entities": []}, {"text": " Table 5  Examples of adapted candidate priors used to adapt NB (Prob(candidate label|source (author's  choice))) for two of the author's choices-on and by. Global prior denotes the probability of the  candidate in the standard model and is based on the relative frequency of the candidate in native  training data. Adapted priors are dependent on the author's preposition. Adapted priors for the  author's choice are very high. Other candidates are given higher priors if they often appear as  corrections for the author's choice (shown in bold). Based on preposition errors in the FCE  training data.", "labels": [], "entities": [{"text": "FCE  training data", "start_pos": 583, "end_pos": 601, "type": "DATASET", "confidence": 0.9455692966779073}]}, {"text": " Table 7  Statistics on articles, prepositions, and verbs in the ESL data set. Error rate denotes the  percentage of examples (articles, prepositions, or verbs in the data) that are mistakes.", "labels": [], "entities": [{"text": "ESL data set", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9658547242482504}, {"text": "Error rate", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9927121996879578}]}, {"text": " Table 8  Key adaptation experiments. Models are trained on WikiNYT data or Web1T. Two types of  adapted models are shown: those that use 100 files (10%) of the FCE training data and those that  use all of the learner training data for error statistics. Inflation rate is optimized on the  development set. All differences are statistically significant (McNemar's test, p < 0.0001).", "labels": [], "entities": [{"text": "WikiNYT data", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8719688057899475}, {"text": "FCE training data", "start_pos": 161, "end_pos": 178, "type": "DATASET", "confidence": 0.8756671945254008}, {"text": "Inflation", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9846449494361877}, {"text": "McNemar's test", "start_pos": 354, "end_pos": 368, "type": "METRIC", "confidence": 0.531056672334671}]}, {"text": " Table 9  Adaptation using the preposition error correction task as an example. Models are trained on  different sizes of native (WikiNYT) and learner data suing the AP algorithm. Each cell shows the  F1 score. The same results are presented in", "labels": [], "entities": [{"text": "preposition error correction task", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6741865202784538}, {"text": "F1 score", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9801024198532104}]}, {"text": " Table 10  Prepositions: Top three evaluation. Models are trained on WikiNYT data or Web1T. Adaptation  uses 100 files (10%) of the FCE training data.", "labels": [], "entities": [{"text": "WikiNYT data", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9358899891376495}, {"text": "FCE training data", "start_pos": 132, "end_pos": 149, "type": "DATASET", "confidence": 0.8801687359809875}]}, {"text": " Table 11  Prepositions: Top three evaluation. Column 1 shows the number of correct predictions among  the top three candidates (between 0 and 3). Each cell shows the percentage of cases with this  number of correct predictions among the top three.", "labels": [], "entities": []}, {"text": " Table 12  Number of examples and errors for the 11 languages used in the experiments on  language-specific adaptation.", "labels": [], "entities": [{"text": "language-specific adaptation", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.6954046189785004}]}, {"text": " Table 13  Adapting NB with data from the target language background and other language backgrounds.  Numbers in the parentheses show the relative improvement of using adapted models vs. the  native-trained models. All improvements of adapted models vs. native-trained models are  statistically significant (McNemar's test, p < 0.001).", "labels": [], "entities": []}, {"text": " Table 14  Target-language adaptation. Adapting NB with data from the target language background and  other language backgrounds. Results are presented by first-language background where  languages are ordered by the amount of target error data available. Bold is used to indicate  results where target-language adaptation is better than other-language adaptation.", "labels": [], "entities": [{"text": "Target-language adaptation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.8068535923957825}]}, {"text": " Table 17  Distance between error distributions vs. benefit from using target priors compared to priors  averaged on other language data. Relative advantage reflects the relative improvement that we get  between priors estimated on target data vs. other data.", "labels": [], "entities": [{"text": "Relative", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9912270903587341}]}]}