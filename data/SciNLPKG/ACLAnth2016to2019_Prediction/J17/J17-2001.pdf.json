{"title": [{"text": "A Comprehensive Analysis of Bilingual Lexicon Induction under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license", "labels": [], "entities": [{"text": "Bilingual Lexicon Induction", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6052245795726776}]}], "abstractContent": [{"text": "Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages.", "labels": [], "entities": [{"text": "Bilingual lexicon induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8580383459726969}]}, {"text": "In this article we present the most comprehensive analysis of bilingual lexicon induction to date.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7383039196332296}]}, {"text": "We present experiments on a wide range of languages and data sizes.", "labels": [], "entities": []}, {"text": "We examine translation into English from 25 foreign languages:, and Welsh.", "labels": [], "entities": []}, {"text": "We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.649322380622228}]}, {"text": "Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6886158486207327}]}, {"text": "We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction.", "labels": [], "entities": []}, {"text": "We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity.", "labels": [], "entities": []}, {"text": "We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora.", "labels": [], "entities": []}, {"text": "Additionally, we introduce a novel discriminative approach to bilingual lexicon induction.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7173990805943807}]}, {"text": "Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence.", "labels": [], "entities": [{"text": "translation equivalence", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.8676661550998688}]}, {"text": "When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g., using minimum reciprocal rank).", "labels": [], "entities": []}, {"text": "We also directly compare our model's performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al.", "labels": [], "entities": [{"text": "matching canonical correlation analysis (MCCA)", "start_pos": 98, "end_pos": 144, "type": "TASK", "confidence": 0.527506947517395}]}, {"text": "Our algorithm achieves an accuracy of 42% versus MCCA's 15%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996689558029175}, {"text": "MCCA", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.48141050338745117}]}], "introductionContent": [{"text": "In natural language processing, translations are typically learned from parallel corpora, which are sentence-aligned bilingual texts ().", "labels": [], "entities": []}, {"text": "In contrast, bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7579487164815267}]}, {"text": "These monolingual corpora can range from being completely unrelated topics to being comparable corpora that contain related information (such as Wikipedia articles on the same subject, but written independently in two languages) but are not translations of each other.", "labels": [], "entities": []}, {"text": "Being able to learn translations from monolingual text is potentially very useful for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8198612570762634}]}, {"text": "For many language pairs, we often only have access to small bilingual resources.", "labels": [], "entities": []}, {"text": "When a machine translation system has access to limited parallel corpora and to incomplete bilingual dictionaries, therefore, there are likely to be many unknown (out-of-vocabulary, or OOV) words in the texts that we would like it to translate.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7216023504734039}]}, {"text": "Being able to mine translations for these OOV words from monolingual corpora means that we could potentially produce some translation for every word in our text, achieving perfect model coverage (but not perfect accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9983534812927246}]}, {"text": "Bilingual lexicon induction uses monolingual or comparable corpora to identify pairs of translated words; a small seed dictionary is also typically assumed.", "labels": [], "entities": [{"text": "Bilingual lexicon induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7939532001813253}]}, {"text": "The quality of induced word translations could be evaluated by using the induction algorithm to expand the coverage of translation models extracted from parallel corpora, by translating OOV words, and then checking whether the induced translations improved the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 261, "end_pos": 263, "type": "TASK", "confidence": 0.9676043391227722}]}, {"text": "However, most prior work in bilingual lexicon induction has treated it as a standalone task, without actually integrating induced translations into end-to-end machine translation.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.742373506228129}]}, {"text": "Instead, it has been evaluated by holding out a portion of the bilingual dictionary and evaluating how well the algorithm learns the translations of the held-out words.", "labels": [], "entities": []}, {"text": "To discover translated words across languages, past work has proposed a variety of monolingual distributional similarity metrics as signals of translation equivalence.", "labels": [], "entities": []}, {"text": "These signals include contextual similarity, temporal similarity, and orthographic similarity.", "labels": [], "entities": []}, {"text": "Most prior work has used unsupervised methods (like rank combination) to aggregate these types of orthogonal signals).", "labels": [], "entities": []}, {"text": "Surprisingly, no past research has used supervised approaches to combine diverse monolingually derived signals for bilingual lexicon induction.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6778416633605957}]}, {"text": "The field of machine learning has shown repeatedly that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 163, "end_pos": 194, "type": "TASK", "confidence": 0.6510570347309113}]}, {"text": "For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary, and that same dictionary maybe used for estimating the parameters of a model to combine monolingual signals.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7389458566904068}]}, {"text": "In this setting, bilingual lexicon induction is critical for translating source words that do not appear in the parallel data or dictionary.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7525078256924947}]}, {"text": "We make several contributions with this article.", "labels": [], "entities": []}, {"text": "First, we present a discriminative model of bilingual lexicon induction that significantly outperforms previous models.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6701791783173879}]}, {"text": "Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence.", "labels": [], "entities": [{"text": "translation equivalence", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.8676661550998688}]}, {"text": "When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion The earliest work in bilingual lexicon induction by and used the surrounding context of a given word as a clue to its translation.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 204, "end_pos": 231, "type": "TASK", "confidence": 0.6109241445859274}]}, {"text": "The key to using contextual similarity as a signal of translation equivalence is to find a mapping between the vector space of one language and the vector space of another language.", "labels": [], "entities": []}, {"text": "To accomplish this, originally proposed creating two co-occurrence matrices for the source and target languages, where the co-occurrence between a pair of words is defined as follows: Where f (i, j) is defined as the number of times words i and j, in the same language, occur in the same context in a large monolingual corpus (Rapp uses a context window of 11 words), and f (i) is the total number of times word i appears in the same corpus.", "labels": [], "entities": []}, {"text": "In this original formulation, no bilingual information was used to find the mappings between the vector spaces of the two languages.", "labels": [], "entities": []}, {"text": "Instead, after computing the two co-occurrence matrices for the two languages, iteratively randomly permutes the word order of the matrix for one of the languages and calculates the similarity between the two matrices.", "labels": [], "entities": [{"text": "similarity", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9643691182136536}]}, {"text": "The permutation is optimal when the similarity between the matrices is maximal, which is when the ordered words in the two matrices are most likely to be translations of one another.", "labels": [], "entities": []}, {"text": "Results are given fora set of 100 English and German word translation pairs.", "labels": [], "entities": [{"text": "English and German word translation", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6330282390117645}]}, {"text": "Later formulations of the problem, including and, used small seed dictionaries to project word-based context vectors from the vector space of one language into the vector space of the other language.", "labels": [], "entities": []}, {"text": "That is, each position in contextual vector v corresponds to a word in the source vocabulary, 2 and vectors v are computed for each source word in the test set.", "labels": [], "entities": []}, {"text": "calculate the ith position of word w's context vector, v w i , as where TF i,w is the number of times i and w co-occur (in this case, defined as appearing in the same sentence), and: where maxn is the maximum frequency of any of the words in the corpus, and f i is the frequency of word i.", "labels": [], "entities": [{"text": "TF", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9903618097305298}]}, {"text": "Rapp (1999) uses the same projection method as but uses log-likelihood ratios instead of TF \u00b7 IDF.", "labels": [], "entities": [{"text": "TF", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9818748831748962}, {"text": "IDF", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.6149937510490417}]}, {"text": "Once source and target language contextual vectors are built, each position in the source language vectors is projected onto the target side using a seed bilingual dictionary.", "labels": [], "entities": []}, {"text": "Finally, contextual similarities are calculated.", "labels": [], "entities": []}, {"text": "That is, each projected vector is compared, using any vector comparison method, with the context vector of each target word.", "labels": [], "entities": []}, {"text": "Word pairs with high Example of projecting contextual vectors over a seed bilingual lexicon.", "labels": [], "entities": []}, {"text": "In monolingual text, Spanish crecer appears in the context of the words empleo, extranjero, etc.", "labels": [], "entities": []}, {"text": "A context vector is built and projected across a seed dictionary.", "labels": [], "entities": []}, {"text": "Context vectors for English words (policy, expand, etc.) are collected and then compared against the projected context vector for Spanish crecer (which can be glossed as grow).", "labels": [], "entities": []}, {"text": "Words with similar context vectors are likely to be translations of one another.", "labels": [], "entities": []}, {"text": "contextual similarity are likely to be translations.", "labels": [], "entities": []}, {"text": "This method of projecting contextual vectors is illustrated in.", "labels": [], "entities": []}, {"text": "We use the vector space approach of to compute similarity between words in the source and target languages.", "labels": [], "entities": []}, {"text": "More formally, assume that (s 1 , s 2 , . .", "labels": [], "entities": []}, {"text": "s N ) and (t 1 , t 2 , . .", "labels": [], "entities": []}, {"text": "t M ) are (arbitrarily indexed) source and target vocabularies, respectively.", "labels": [], "entities": []}, {"text": "A source word f is represented with an N-dimensional vector and a target word e is represented with an M-dimensional vector (see).", "labels": [], "entities": []}, {"text": "The component values of the vector representing a word correspond to how often each of the words in that vocabulary appear within a two-word window on either side of the given word.", "labels": [], "entities": []}, {"text": "These counts are collected using monolingual corpora.", "labels": [], "entities": []}, {"text": "After the values have been computed, a contextual vector f is projected onto the English vector space using translations in a given bilingual dictionary to map the component values into their appropriate English vector positions.", "labels": [], "entities": []}, {"text": "This sparse projected vector is compared with the vectors representing all English words, e.", "labels": [], "entities": []}, {"text": "Each word pair is assigned a contextual similarity score c( f, e) based on the similarity between e and the projection off . Various means of computing the component values and vector similarity measures have been proposed in literature (e.g.,.", "labels": [], "entities": [{"text": "contextual similarity score c", "start_pos": 29, "end_pos": 58, "type": "METRIC", "confidence": 0.7181469202041626}]}, {"text": "Following, we compute the value of the kth component off 's contextual vector, f k , as follows: where n f,k and n k are the number of times s k appears in the context off and in the entire corpus, respectively, and n is the maximum number of occurrences of any word in the data.", "labels": [], "entities": []}, {"text": "Intuitively, the more frequently s k appears with f i and the less common it is in the corpus in general, the higher its component value.", "labels": [], "entities": []}, {"text": "After projecting each component of the source language contextual vectors into the English vector space, we are left with M-dimensional source word contextual vectors, F context , and correspondingly ordered M-dimensional target word contextual vectors, E context , for all words in the vocabulary of each language.", "labels": [], "entities": []}, {"text": "We use cosine similarity to measure the similarity between each pair of contextual vectors: shows example ranked lists using contextual similarity to rank English words for several Spanish words.", "labels": [], "entities": []}, {"text": "For example, contextual similarity ranks the English words enjoyed and contained highly as candidate translations of Spanish alcanzaron.", "labels": [], "entities": []}, {"text": "These incorrect English words tend to appear in similar contexts as the correct English translation, reached.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed a set of experiments to systematically explore the following research questions: To what extent are the different signals of translation equivalence orthogonal to each other?", "labels": [], "entities": []}, {"text": "Are certain signals better than others at ranking translations?", "labels": [], "entities": [{"text": "ranking translations", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6785685420036316}]}, {"text": "Does this vary based on language or part of speech?", "labels": [], "entities": []}, {"text": "How accurately do they individually rank translation candidates fora variety of languages?", "labels": [], "entities": []}, {"text": "How can we effectively combine them in order to rank translation candidates?", "labels": [], "entities": []}, {"text": "How much does the performance vary per language?", "labels": [], "entities": []}, {"text": "To what extent does performance depend on the size of the seed bilingual dictionary, and on the size of the monolingual corpora?", "labels": [], "entities": []}, {"text": "Does bilingual lexicon induction make more accurate predictions for words with certain properties like being highly bursty?", "labels": [], "entities": []}, {"text": "How well does our discriminative model compare to the sophisticated generative model MCCA?", "labels": [], "entities": [{"text": "generative model MCCA", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.6290931502978007}]}, {"text": "First, we describe our evaluation metric, data, and experimental set-up.", "labels": [], "entities": []}, {"text": "We then present our findings.", "labels": [], "entities": []}, {"text": "We measure performance using accuracy in the top-k ranked translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9991186261177063}]}, {"text": "We define top-k accuracy over some set of ranked lists L as follows: where I lk is an indicator function that is 1 if and only if a correct item is included in the top-k elements of list l.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9832339286804199}]}, {"text": "That is, top-k accuracy is the proportion of ranked lists in a set of ranked lists for which a correct item is included anywhere in the highest k ranked elements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9470914602279663}]}, {"text": "The denominator |L| is the number of words in a test set fora language.", "labels": [], "entities": []}, {"text": "The numerator indicates how many of the words had at least one correct translation in the top-k translations posited for the word.", "labels": [], "entities": []}, {"text": "Top-k accuracy increases ask increases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9806910753250122}, {"text": "ask", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9988985061645508}]}, {"text": "A translation counts as correct if it appears in our bilingual dictionary for the language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6  Statistics for the data used in our experiments.", "labels": [], "entities": []}, {"text": " Table 9  Measure of the correlation (orthogonality) between signals. For each of 24 languages,  we randomly select 1,000 source language words and compute the Spearman rank correlation  coefficient across pairwise ranked lists of translation candidates generated by each of eight  signals of translation equivalence. We average coefficients within each language. The results here  show the mean of the correlation coefficient between all pairs of signals across the 24 languages.  crawls-cont  wiki-cont  -0.15  wiki-cont  temporal  -0.14  -0.19  temporal  orthography  -0.28  -0.31  -0.28  orth.  topic  -0.15  -0.14  -0.13  -0.30 topic  frequency  0.01  0.13  0.02  -0.18  0.13  freq.  burstiness  -0.10  0.06  -0.07  0.06  0.11  0.28 burst.  idf  0.06  0.10  -0.12  -0.01  0.00  0.49  0.14", "labels": [], "entities": [{"text": "Spearman rank correlation  coefficient", "start_pos": 160, "end_pos": 198, "type": "METRIC", "confidence": 0.6580563634634018}, {"text": "temporal  -0.14  -0.19  temporal  orthography  -0.28  -0.31  -0.28  orth.  topic  -0.15  -0.14  -0.13  -0.30 topic  frequency  0.01  0.13  0.02  -0.18  0.13  freq.  burstiness  -", "start_pos": 524, "end_pos": 702, "type": "METRIC", "confidence": 0.732630112932788}]}, {"text": " Table 10  Percent of time when each translation signal ranks a correct translation the highest out of all of  the translation signals. This percentage is calculated for 1,000 randomly chosen words with  dictionary entries for each of the 24 languages.", "labels": [], "entities": []}, {"text": " Table 11  Analysis of signals by POS tag. This table shows the percent of time when each translation signal  ranks a correct translation highest out of all of the translation signals. The results are subdivided  based on part of speech. The average row is identical to the average per-language result given in  Table 10.", "labels": [], "entities": []}, {"text": " Table 13  Comparison of bilingual lexicon induction accuracies using (1) matching canonical correlation  analysis (MCCA), (2) our supervised discriminative model using only contextual and  orthographic features, and (3) our supervised discriminative model using our complete feature  set. Accuracy is measured as the percent of test set translations that are correctly matched by  each model's full bipartite matching.", "labels": [], "entities": [{"text": "matching canonical correlation  analysis (MCCA)", "start_pos": 74, "end_pos": 121, "type": "METRIC", "confidence": 0.759526925427573}, {"text": "Accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9970608353614807}]}]}