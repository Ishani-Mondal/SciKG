{"title": [{"text": "Discourse Structure in Machine Translation Evaluation under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.7813266416390737}]}], "abstractContent": [{"text": "Llu\u00eds M` arquez \u2020 ALT, QCRI, HBKU, Qatar Foundation Preslav Nakov \u2020 ALT, QCRI, HBKU, Qatar Foundation In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation.", "labels": [], "entities": [{"text": "QCRI", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9443151950836182}, {"text": "HBKU, Qatar Foundation Preslav Nakov \u2020 ALT", "start_pos": 29, "end_pos": 71, "type": "DATASET", "confidence": 0.7598040960729122}, {"text": "HBKU", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.7578323483467102}, {"text": "machine translation evaluation", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.8316151897112528}]}, {"text": "We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST).", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 139, "end_pos": 172, "type": "TASK", "confidence": 0.6986817320187887}]}, {"text": "Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment level and at the system level.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.8505171736081442}]}, {"text": "This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could betaken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DISCOTK party.", "labels": [], "entities": [{"text": "WMT-14 winning combined metric DISCOTK party", "start_pos": 220, "end_pos": 264, "type": "DATASET", "confidence": 0.7893343170483907}]}, {"text": "We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation.", "labels": [], "entities": [{"text": "RST parse", "start_pos": 106, "end_pos": 115, "type": "TASK", "confidence": 0.881179928779602}, {"text": "machine translation evaluation", "start_pos": 126, "end_pos": 156, "type": "TASK", "confidence": 0.8596018155415853}]}, {"text": "In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.", "labels": [], "entities": [{"text": "RST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9429616928100586}]}], "introductionContent": [{"text": "From its foundations, Statistical Machine Translation (SMT) as afield had two defining characteristics.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.9018565018971761}]}, {"text": "First, translation was modeled as a generative process at the sentence level.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9812318682670593}]}, {"text": "Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (.", "labels": [], "entities": []}, {"text": "Although modern SMT systems switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9920639395713806}]}, {"text": "One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.995464026927948}]}, {"text": "For instance, in SMT we find systems based on syntax (, hierarchical structures, and semantic roles (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9912290573120117}]}, {"text": "However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese-English and German-English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8410546183586121}]}, {"text": "Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information.", "labels": [], "entities": []}, {"text": "The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level.", "labels": [], "entities": []}, {"text": "Research in this direction has also been the focus of the two editions of the).", "labels": [], "entities": []}, {"text": "Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9579087197780609}, {"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9909052848815918}]}, {"text": "Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 135, "end_pos": 151, "type": "METRIC", "confidence": 0.9591831266880035}]}, {"text": "The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information.", "labels": [], "entities": []}, {"text": "BLEU ( is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9842153191566467}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9724263548851013}]}, {"text": "BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9770964980125427}]}, {"text": "However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output, and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9954851865768433}]}, {"text": "Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning.", "labels": [], "entities": []}, {"text": "Several alternatives emerged to overcome these limitations, most notably TER () and METEOR (.", "labels": [], "entities": [{"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9945268034934998}, {"text": "METEOR", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9696531295776367}]}, {"text": "Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing, syntax (, semantic roles (, and, most recently, discourse).", "labels": [], "entities": []}, {"text": "Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9923074245452881}]}, {"text": "To the best of our knowledge, semantic relations between clauses in a sentence and between sentences in a text have not been seriously explored.", "labels": [], "entities": []}, {"text": "However, clauses and sentences rarely stand on their own in a well-written text; rather, the logical relationship between them carries significant information that allows the text to express a meaning as a whole.", "labels": [], "entities": []}, {"text": "Each clause follows smoothly from the ones before it and leads into the ones that come afterward.", "labels": [], "entities": []}, {"text": "This logical relationship between clauses forms a coherence structure.", "labels": [], "entities": []}, {"text": "In discourse analysis, we seek to uncover this coherence structure underneath the text.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7279701828956604}]}, {"text": "Several formal theories of discourse have been proposed to describe the coherence structure ().", "labels": [], "entities": []}, {"text": "Rhetorical Structure Theory (RST; is perhaps the most influential of these in computational linguistics, where it is used either to parse the text in language understanding or to plan a coherent text in language generation ().", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7837110817432403}, {"text": "language understanding", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.7068134695291519}, {"text": "language generation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7303581833839417}]}, {"text": "RST describes coherence using discourse relations between parts of a text and postulates a hierarchical tree structure called discourse tree.", "labels": [], "entities": [{"text": "RST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9569142460823059}]}, {"text": "For example, in the next section shows discourse trees for three different translations of a source sentence.", "labels": [], "entities": []}, {"text": "Modeling discourse brings together the usage of higher-level linguistic information and the exploration of relations between clauses and sentences in a text, which makes it a very attractive goal for MT and its evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 200, "end_pos": 202, "type": "TASK", "confidence": 0.9965518712997437}]}, {"text": "We believe that the semantic and pragmatic information captured in the form of discourse trees (i) can yield better (b) A higher quality system translation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data sets we used in our experiments, the interpolation approach we applied to combine our discourse-based metrics with pre-existing evaluation metrics, and all the correlation measures we used for evaluation.", "labels": [], "entities": []}, {"text": "For the correlation at the system level, we first produce a score for each of the systems according to the quality of their translations based on the evaluation metrics and on the human judgments.", "labels": [], "entities": []}, {"text": "Then, we calculate the correlation between the scores for the participating systems using a target metric's scores and the human scores.", "labels": [], "entities": []}, {"text": "We do this based on system ranks induced by the scores (using Spearman's rank correlation) or based on the scores themselves (using Pearson correlation).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 132, "end_pos": 151, "type": "METRIC", "confidence": 0.9274848699569702}]}, {"text": "Note that, following WMT, we calculate the correlation score separately for each language pair, and then we average the resulting correlations to obtain the final score.", "labels": [], "entities": [{"text": "WMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.48805010318756104}, {"text": "correlation score", "start_pos": 43, "end_pos": 60, "type": "METRIC", "confidence": 0.9677790403366089}]}, {"text": "In order to produce a system-level score based on the pairwise sentence-level human judgments, we need to aggregate these judgments, which we do based on the ratio of wins (ignoring ties), as defined for the official ranking at WMT12 where win(s i ) and loss(s i ) are the number of wins and losses, respectively, of system s i against any other system in the segment-level pairwise human judgments.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 228, "end_pos": 233, "type": "DATASET", "confidence": 0.9055374264717102}]}, {"text": "This is the WMT12 official metric for system-level evaluation.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7347536087036133}]}, {"text": "To calculate it, we first convert the raw scores assigned to each system to ranks, and then we use the following formula (Spearman 1904): where d i is the difference between the ranks for system i, and n is the number of systems being evaluated.", "labels": [], "entities": []}, {"text": "Note that this formula requires that there be no ties in the ranks of the systems (based on the automatic metric or based on the human judgments), which was indeed the case.", "labels": [], "entities": []}, {"text": "Spearman's rank correlation ranges between \u22121 and 1.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.8856929838657379}]}, {"text": "However, unlike Kendall's \u03c4, here the sign does not matter, and high absolute values indicate better performance.", "labels": [], "entities": []}, {"text": "In our experiments, we used the official script from WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.5898399353027344}]}, {"text": "In some experiments, we also report Pearson correlation (Pearson 1895), which was the official system-level score at WMT14.", "labels": [], "entities": [{"text": "Pearson correlation (Pearson 1895)", "start_pos": 36, "end_pos": 70, "type": "METRIC", "confidence": 0.8861999114354452}, {"text": "WMT14", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.9261300563812256}]}, {"text": "This is a more general correlation coefficient than Spearman's and does not require that all n ranks be distinct integers.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 23, "end_pos": 46, "type": "METRIC", "confidence": 0.9550313949584961}]}, {"text": "It is defined as follows: where H is the vector of the human scores of all participating systems, M is the vector of the corresponding scores as predicted by the given metric, and \u00af H and \u00af M are the means for H and M, respectively.", "labels": [], "entities": []}, {"text": "The Pearson correlation value ranges between \u22121 and 1, where higher absolute score is better.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8479299247264862}, {"text": "absolute score", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.9432657957077026}]}, {"text": "We used the official WMT14 scoring tool to calculate it.", "labels": [], "entities": [{"text": "WMT14 scoring", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.6839519441127777}]}, {"text": "In this section, we show the utility of discourse information for machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.88325035572052}]}, {"text": "We present the evaluation results at the system level and at the segment level, using our two basic discourse-based metrics, which we refer to as DR and DR-LEX (Section 2.1).", "labels": [], "entities": []}, {"text": "In our experiments, we combine DR and DR-LEX with other evaluation metrics in two different ways: using uniform linear interpolation (at the system level and at the segment level), and using a tuned linear interpolation for the segment-level.", "labels": [], "entities": [{"text": "DR", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.8582378625869751}]}, {"text": "We only present the average results overall language pairs.", "labels": [], "entities": []}, {"text": "For clarity, in our tables we show results divided into three evaluation groups: Group I contains our discourse-based evaluation metrics, DR, and DR-LEX.", "labels": [], "entities": [{"text": "DR", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.9224525094032288}]}, {"text": "Group II includes the publicly available MT evaluation metrics that participated in the WMT12 metrics task, excluding those that did not have results for all language pairs).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9015094339847565}, {"text": "WMT12 metrics task", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.5774932702382406}]}, {"text": "More precisely, they are SPEDE07PP, AMBER, METEOR, TERRORCAT, SIMPBLEU, XENERRCATS, WORDBLOCKEC, BLOCKERRCATS, and POSF.", "labels": [], "entities": [{"text": "SPEDE07PP", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.6220242381095886}, {"text": "AMBER", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9250789880752563}, {"text": "METEOR", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9845543503761292}, {"text": "TERRORCAT", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9910286068916321}, {"text": "XENERRCATS", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9503529667854309}, {"text": "BLOCKERRCATS", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9949878454208374}]}, {"text": "Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (, NIST (Doddington 2002), ROUGE (Lin 2004), and TER ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.9426665604114532}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9990972280502319}, {"text": "NIST (Doddington 2002)", "start_pos": 114, "end_pos": 136, "type": "DATASET", "confidence": 0.8155127882957458}, {"text": "ROUGE", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9961298704147339}, {"text": "TER", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9963864088058472}]}, {"text": "We calculated the metrics in this group using Asiya.", "labels": [], "entities": []}, {"text": "In particular, we used the following Asiya versions of TER and ROUGE: TERP-A and ROUGE-W.", "labels": [], "entities": [{"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9535213708877563}, {"text": "ROUGE", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9767703413963318}, {"text": "TERP-A", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9590924382209778}, {"text": "ROUGE-W", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9525702595710754}]}, {"text": "For each metric in groups II and III, we present the system-level and segment-level results for the original metric as well as for the linear interpolation of that metric with DR and with DR-LEX.", "labels": [], "entities": []}, {"text": "The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic.", "labels": [], "entities": []}, {"text": "For the segment-level evaluation, we further indicate which interpolated results yield statistically significant improvement over the original metric.", "labels": [], "entities": []}, {"text": "Note that testing statistical significance is not trivial in our case because we have a complex correlation score for which the assumptions that standard tests make are not met.", "labels": [], "entities": []}, {"text": "We thus resorted to a non-parametric randomization framework, which is commonly used in NLP research.", "labels": [], "entities": []}, {"text": "shows the system-level experimental results for WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.6183298826217651}]}, {"text": "We can see that DR is already competitive by itself: On average, it has a correlation of 0.807, which is very close to the BLEU and the TER scores from group II (0.810 and 0.812, respectively).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9991576671600342}, {"text": "TER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9983882904052734}]}, {"text": "Moreover, DR yields improvements when combined with 13 of the 15 metrics, with a resulting correlation higher than those of the two individual metrics being combined.", "labels": [], "entities": [{"text": "DR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9979945421218872}]}, {"text": "This fact suggests that DR contains information that is complementary to that used by most of the other metrics.", "labels": [], "entities": []}, {"text": "From the results presented in the previous sections, we can conclude that discourse structure is an important information source, which is not entirely correlated to other information sources considered so far, and thus should betaken into account when designing future metrics for automatic evaluation of machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 306, "end_pos": 332, "type": "TASK", "confidence": 0.8074316382408142}]}, {"text": "In this section we show how the simple combination of DR-based metrics with a selection of other existing strong MT evaluation metrics can lead to a very competitive evaluation metric, DISCOTK party (), which we presented at the metrics task of WMT14.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9051100313663483}, {"text": "DISCOTK", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.8139678835868835}, {"text": "WMT14", "start_pos": 245, "end_pos": 250, "type": "DATASET", "confidence": 0.9288434386253357}]}, {"text": "ASIYA (Gim\u00e9nez and M` arquez 2010a) is a suite for MT evaluation that provides a large set of metrics using different levels of linguistic information.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9859089553356171}]}, {"text": "We used the 12 individual metrics from ASIYA's ULC (Gim\u00e9nez and M` arquez 2010b), which was  the best performing metric both at the system level and at the segment level at the WMT08 and WMT09 metrics tasks.", "labels": [], "entities": [{"text": "ASIYA's ULC", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.6442484756310781}, {"text": "WMT08 and WMT09 metrics", "start_pos": 177, "end_pos": 200, "type": "DATASET", "confidence": 0.6852531060576439}]}, {"text": "From the original ULC, we replaced METEOR by the four newer variants METEOR-ex (exact match), METEOR-st (+stemming), METEOR-sy (+synonymy lookup), and METEOR-pa (+paraphrasing) in ASIYA's terminology (Denkowski and Lavie 2011).", "labels": [], "entities": [{"text": "METEOR-ex (exact match)", "start_pos": 69, "end_pos": 92, "type": "METRIC", "confidence": 0.7307543814182281}, {"text": "METEOR-st", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.7061278223991394}, {"text": "METEOR-sy", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.7139843106269836}]}, {"text": "We also added to the mix TERp-A (a variant of TER with paraphrasing), BLEU, NIST, and ROUGE-W, fora total of 18 individual metrics.", "labels": [], "entities": [{"text": "TERp-A", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9956098198890686}, {"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9652305245399475}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.999281108379364}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8126493096351624}, {"text": "ROUGE-W", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9962952733039856}]}, {"text": "The metrics in this set use diverse linguistic information, including lexical-, syntactic-, and semantic-oriented individual metrics.", "labels": [], "entities": []}, {"text": "Regarding the discourse metrics, we used five variants, including DR and DR-LEX described in Section 2, and three more constrained variants oriented to match words between trees only if they occur under the same substructure types (e.g., the same nuclearity type).", "labels": [], "entities": []}, {"text": "These variants are designed by introducing structural modifications in the discourse trees.", "labels": [], "entities": []}, {"text": "A detailed description can be found in.", "labels": [], "entities": []}, {"text": "We tuned the relative weights of the previous 23 individual metrics (18 ASIYA+ 5 discourse) following the same maximum entropy learning framework described in Section 3.2.", "labels": [], "entities": []}, {"text": "As the training set, we used the simple concatenation of WMT11, WMT12, and WMT13.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.8891141414642334}, {"text": "WMT12", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.7937410473823547}, {"text": "WMT13", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9421147108078003}]}, {"text": "DISCOTK party was the best-performing metric at WMT14 both at the segment and at the system level, among a set of 16 and 20 participants, respectively.", "labels": [], "entities": [{"text": "DISCOTK party", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9327086806297302}, {"text": "WMT14", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.86899334192276}]}, {"text": "shows a comparison at the segment level of our tuned metric DISCOTK party to the best rivaling metric at WMT14, for each individual language pair, using Kendall's \u03c4.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.9786663055419922}]}, {"text": "Note that this best rival differs across language pairs, for example, for FR-EN, HI-EN, and CS-EN it is BEER, for DE-EN it is UPC-STOUT, and for RU-EN it is REDCOMBSENT.", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.6083731651306152}, {"text": "BEER", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9979910850524902}, {"text": "REDCOMBSENT", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.905487060546875}]}, {"text": "We can see that our metric outperforms this best rival for four of the language pairs, with statistically significant differences.", "labels": [], "entities": []}, {"text": "The only exception is HI-EN, where the best rival performs slightly better, not statistically significantly.", "labels": [], "entities": []}, {"text": "System translations for Hindi-English were of extremely low quality, and were very hard to discourse-parse accurately.", "labels": [], "entities": []}, {"text": "The linguistically heavy components of our DISCOTK party (discourse parsing, syntactic parsing, semantic role labeling, etc.) may suffer from the common ungrammaticality of the translation hypotheses for HI-EN, whereas other, less linguistically heavy metrics seem to be more robust in such cases.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6969652771949768}, {"text": "syntactic parsing", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7333679050207138}, {"text": "semantic role labeling", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.6153388420740763}]}, {"text": "We show in the weights for the individual metrics combined in DISCOTK party after tuning on the combined WMT11+12+13 data set.", "labels": [], "entities": [{"text": "DISCOTK party", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9254741072654724}, {"text": "WMT11+12+13 data set", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.9046644653592791}]}, {"text": "The horizontal axis displays all the individual metrics involved in the combination.", "labels": [], "entities": []}, {"text": "DR-NOLEX to DR-LEX 1.1 ) metrics are the metric variants based on discourse trees.", "labels": [], "entities": [{"text": "DR-NOLEX", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8474748134613037}]}, {"text": "Note that all metric scores are passed through a min-max normalization step to put them in the same scale before tuning their relative weights.", "labels": [], "entities": []}, {"text": "We can see that most of the metrics involved in the metric combination play a significant role, the most important ones being TERp-A, METEOR-pa (paraphrases), and ROUGE-W.", "labels": [], "entities": [{"text": "TERp-A", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9969846606254578}, {"text": "METEOR-pa", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9899193048477173}, {"text": "ROUGE-W", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9806308746337891}]}, {"text": "Some metrics accounting for syntactic and semantic information also get assigned relatively high weights (DP-Or*, CP-STM-4, and DR-Orp*).", "labels": [], "entities": []}, {"text": "Interestingly, all five variants of our discourse metric received moderately high weights, with the four variants using lexical information (DR-LEX's) being more important.", "labels": [], "entities": []}, {"text": "In particular, DR-LEX 1 has the fourth highest absolute weight in the overall combination.", "labels": [], "entities": []}, {"text": "This confirms again the importance of discourse information in machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.8854413827260336}]}, {"text": "Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9879751801490784}]}, {"text": "16 A common argument is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9923442006111145}]}, {"text": "Thus, there is consensus that discourse-informed MT evaluation metrics are needed in order to advance MT research.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.900838166475296}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9965749382972717}]}, {"text": "The need to consider discourse phenomena in MT evaluation was also emphasized earlier by the Framework for Machine Translation Evaluation in ISLE (FEMTI), which defines quality models (i.e., desired MT system qualities and their metrics) based on the intended context of use.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9866626858711243}, {"text": "Machine Translation Evaluation in ISLE (FEMTI)", "start_pos": 107, "end_pos": 153, "type": "TASK", "confidence": 0.7830810099840164}, {"text": "MT system", "start_pos": 199, "end_pos": 208, "type": "TASK", "confidence": 0.8994181752204895}]}, {"text": "The suitability requirement of MT system in the FEMTI comprises discourse aspects including readability, comprehensibility, coherence, and cohesion.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9770613312721252}, {"text": "FEMTI", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8330585360527039}]}, {"text": "In Section 4, we have suggested some simple ways to create such metrics, and we have also shown that they yield better correlation with human judgments.", "labels": [], "entities": []}, {"text": "Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.9552568793296814}]}, {"text": "Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources.", "labels": [], "entities": []}, {"text": "Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT, and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9616816639900208}, {"text": "MT metric comparisons", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.8191220164299011}, {"text": "Machine Translation or WMT", "start_pos": 219, "end_pos": 245, "type": "TASK", "confidence": 0.7872397527098656}, {"text": "NIST Metrics for Machine Translation Challenge", "start_pos": 255, "end_pos": 301, "type": "TASK", "confidence": 0.7372685869534811}]}, {"text": "For example, at WMT15, 11 research teams submitted 46 metrics to be compared).", "labels": [], "entities": [{"text": "WMT15", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9127529859542847}]}, {"text": "Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge.", "labels": [], "entities": []}, {"text": "This reflects the general trend in the field.", "labels": [], "entities": []}, {"text": "For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Number of systems (sys), unique non-tied translation pairs (pairs), and unique sentences for  which such pairs exist (sent) for the different language pairs, for the human evaluation of the  WMT11-WMT14 metric shared tasks. These statistics show what we use for training; the  numbers for testing are higher, as explained in the text.", "labels": [], "entities": [{"text": "WMT11-WMT14 metric shared tasks", "start_pos": 201, "end_pos": 232, "type": "TASK", "confidence": 0.7199642956256866}]}, {"text": " Table 2  Results on WMT12 at the system-level (calculated on 6 systems for CS-EN, 16 for DE-EN, 12 for  ES-EN, and 15 for FR-EN). Spearman's correlation with human judgments.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.5570693612098694}, {"text": "FR-EN", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9781008958816528}]}, {"text": " Table 3  Results on WMT12 at the segment-level (calculated on 11,021 pairs for CS-EN, 11,934 for DE-EN,  9,796 for ES-EN, and 11,594 for FR-EN): untuned and tuned versions. Kendall's \u03c4 with human  judgments. Improvements over the baseline are shown in bold, and statistically significant  improvements are marked with  *  *  and  *  for p-value <0.01 and p-value <0.05, respectively.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8981652855873108}, {"text": "FR-EN", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.870154857635498}]}, {"text": " Table 4  Results on WMT11 at the segment-level (calculated on 3,695 pairs for CS-EN, 8,950 for DE-EN,  5,974 for ES-EN, and 6,337 for FR-EN): tuning on the entire WMT12. Kendall's \u03c4 with human  judgments. Improvements over the baseline are shown in bold, and statistically significant  improvements are marked with  *  *  for p-value <0.01.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8816497325897217}, {"text": "FR-EN", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.928125262260437}, {"text": "WMT12", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.92470782995224}]}, {"text": " Table 5  Comparing our tuned metric to the best rivaling metric at WMT14, for each individual language  pair (this best rival differs across language pairs) at the segment-level using Kendall's \u03c4.  Statistically significant improvements are marked with  *  *  for p-value < 0.01.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9668629169464111}]}, {"text": " Table 6  System-level Spearman (\u03c1) and Pearson (r) correlation results for the ablation study over the  DR-LEX metric across the WMT{11,12,13} data sets and overall.", "labels": [], "entities": [{"text": "Spearman (\u03c1)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9733364880084991}, {"text": "Pearson (r) correlation", "start_pos": 40, "end_pos": 63, "type": "METRIC", "confidence": 0.9496938824653626}, {"text": "ablation", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9592298269271851}, {"text": "WMT{11,12,13} data sets", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.846965511639913}]}, {"text": " Table 7  System-level Spearman (\u03c1) and Pearson (r) correlation results for the ablation study over the  DR-LEX metric across language pairs for the WMT{11,12,13} data sets.", "labels": [], "entities": [{"text": "Spearman (\u03c1)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9709182679653168}, {"text": "Pearson (r) correlation", "start_pos": 40, "end_pos": 63, "type": "METRIC", "confidence": 0.9529591798782349}, {"text": "ablation", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9505510330200195}, {"text": "WMT{11,12,13} data sets", "start_pos": 149, "end_pos": 172, "type": "DATASET", "confidence": 0.8291090428829193}]}, {"text": " Table 8  Kendall's (\u03c4) segment level correlation with human judgments on WMT12 obtained by the  pairwise preference kernel learning. Results are presented for each language pair and overall.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8758866190910339}]}]}