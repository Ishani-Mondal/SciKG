{"title": [{"text": "String Kernels for Native Language Identification: Insights from Behind the Curtains", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6995187004407247}]}], "abstractContent": [{"text": "The most common approach in text mining classification tasks is to rely on features like words, part-of-speech tags, stems, or some other high-level linguistic features.", "labels": [], "entities": [{"text": "text mining classification tasks", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8816253691911697}]}, {"text": "Recently, an approach that uses only character p-grams as features has been proposed for the task of native language identification (NLI).", "labels": [], "entities": [{"text": "native language identification (NLI)", "start_pos": 101, "end_pos": 137, "type": "TASK", "confidence": 0.8087180952231089}]}, {"text": "The approach obtained state-of-the-art results by combining several string kernels using multiple kernel learning.", "labels": [], "entities": []}, {"text": "Despite the fact that the approach based on string kernels performs so well, several questions about this method remain unanswered.", "labels": [], "entities": []}, {"text": "First, it is not clear why such a simple approach can compete with far more complex approaches that take words, lemmas, syntactic information, or even semantics into account.", "labels": [], "entities": []}, {"text": "Second, although the approach is designed to be language independent, all experiments to date have been on English.", "labels": [], "entities": []}, {"text": "This work is an extensive study that aims to systematically present the string kernel approach and to clarify the open questions mentioned above.", "labels": [], "entities": []}, {"text": "A broad set of native language identification experiments were conducted to compare the string kernels approach with other state-of-the-art methods.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.7039638161659241}]}, {"text": "The empirical results obtained in all of the experiments conducted in this work indicate that the proposed approach achieves state-of-the-art performance in NLI, reaching an accuracy that is 1.7% above the top scoring system of the 2013 NLI Shared Task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9992764592170715}, {"text": "2013 NLI Shared Task", "start_pos": 232, "end_pos": 252, "type": "DATASET", "confidence": 0.666856586933136}]}, {"text": "Furthermore, the results obtained on both the Arabic and the Norwegian corpora demonstrate that the proposed approach is language independent.", "labels": [], "entities": [{"text": "Norwegian corpora", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9146559238433838}]}, {"text": "In the Arabic native language identification task, string kernels show an increase of more than 17% over the best accuracy reported so far.", "labels": [], "entities": [{"text": "Arabic native language identification task", "start_pos": 7, "end_pos": 49, "type": "TASK", "confidence": 0.6690115690231323}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9907795190811157}]}, {"text": "The results of string kernels on Norwegian native language identification are also significantly better than the state-of-the-art approach.", "labels": [], "entities": [{"text": "Norwegian native language identification", "start_pos": 33, "end_pos": 73, "type": "TASK", "confidence": 0.5587083622813225}]}, {"text": "In addition, in a cross-corpus experiment, the proposed approach shows that it can also be topic independent, improving the state-of-the-art system by 32.3%.", "labels": [], "entities": []}, {"text": "To gain additional insights about the string kernels approach, the features selected by the classifier as being more discriminating are analyzed in this work.", "labels": [], "entities": []}, {"text": "The analysis also offers information about localized language transfer effects, since the features used by the proposed model are p-grams of various lengths.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6910195350646973}]}, {"text": "The features captured by the model typically include stems, function words, and word prefixes and suffixes, which have the potential to generalize over purely word-based features.", "labels": [], "entities": []}, {"text": "By analyzing the discriminating features, this article offers insights into two kinds of language transfer effects, namely, word choice (lexical transfer) and morphological differences.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.6987604349851608}, {"text": "word choice (lexical transfer)", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.7144444286823273}]}, {"text": "The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using words as basic units is natural in textual analysis tasks such as text categorization, authorship identification, or plagiarism detection.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8154084384441376}, {"text": "plagiarism detection", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.7950747311115265}]}, {"text": "Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (.", "labels": [], "entities": []}, {"text": "By avoiding to explicitly consider features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral.", "labels": [], "entities": []}, {"text": "In this context, a state-of-the-art machine learning system for native language identification (NLI) that works at the character level is presented in this article.", "labels": [], "entities": [{"text": "native language identification (NLI)", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.8252045412858328}]}, {"text": "NLI is the task of identifying the native language of a writer, based on a text they have written in a language other than their mother tongue.", "labels": [], "entities": []}, {"text": "This is an interesting sub-task in forensic linguistic applications such as plagiarism detection and authorship identification, where the native language of an author is just one piece of the puzzle ().", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7305263727903366}, {"text": "authorship identification", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.8946234881877899}]}, {"text": "NLI can also play a key role in second language acquisition applications where NLI techniques are used to identify language transfer patterns that help teachers and students focus feedback and learning on particular areas of interest (Rozovskaya and Roth 2010; Jarvis and Crossley 2012).", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.659129281838735}]}, {"text": "The system based on string kernels described in this article was initially designed to participate in the 2013 NLI Shared Task and obtained third place (Popescu and Ionescu 2013).", "labels": [], "entities": [{"text": "2013 NLI Shared Task", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.7161896824836731}]}, {"text": "The approach was further extended by to combine several string kernels via multiple kernel learning (MKL) (Gonen and Alpaydin 2011).", "labels": [], "entities": []}, {"text": "The system revealed some interesting facts about the kinds of string kernels and kernel learning methods that worked well for NLI.", "labels": [], "entities": []}, {"text": "For instance, one of the best performing kernels is the (histogram) intersection kernel, which is inspired from computer vision.", "labels": [], "entities": []}, {"text": "Another kernel based on Local Rank Distance is inspired from biology (Ionescu 2013; Dinu, Ionescu, and Tomescu 2014).", "labels": [], "entities": []}, {"text": "Two kernel classifiers are alternatively used for the learning task, namely, kernel ridge regression (KRR) and kernel discriminant analysis (KDA).", "labels": [], "entities": [{"text": "kernel discriminant analysis (KDA", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.6669577062129974}]}, {"text": "Interestingly, these kernel classifiers give better results than the widely used support vector machines (SVM).", "labels": [], "entities": []}, {"text": "Ionescu, conducted several experiments to demonstrate state-of-the-art performance of the system based on string kernels.", "labels": [], "entities": []}, {"text": "However, the system was only used for the task of identifying the native language of a person from text written in English.", "labels": [], "entities": [{"text": "identifying the native language of a person from text written in English", "start_pos": 50, "end_pos": 122, "type": "TASK", "confidence": 0.832840621471405}]}, {"text": "Furthermore, a study about the language transfer patterns captured by the system was not given.", "labels": [], "entities": [{"text": "language transfer patterns captured", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.7974628433585167}]}, {"text": "The current work aims to clarify these points and to give an extended presentation and evaluation of the string kernels approach.", "labels": [], "entities": []}, {"text": "The first goal of this work is to demonstrate that the system is indeed languageindependent, as claimed in Ionescu,, and to show that it can achieve state-of-the-art performance on a consistent basis.", "labels": [], "entities": []}, {"text": "To fulfill this goal, the system is evaluated on several corpora of text documents written in three different languages, namely, Arabic, English, and Norwegian.", "labels": [], "entities": []}, {"text": "Having a general characterbased approach that works well across languages could be useful, for example, in a streamlined intelligence application that is required to work efficiently on a wide range of languages for which NLP tools or language experts might not be readily available, or where the user does not desire a complex customization for each language.", "labels": [], "entities": []}, {"text": "On the other hand, kernels based on a different kind of information (for example, syntactic and semantic information) can be combined with string kernels via MKL to improve accuracy in some specific situations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9965338706970215}]}, {"text": "Indeed, others have obtained better NLI results by using ensemble models based on several features such as context-free grammar production rules, Stanford dependencies, function words, and part-of-speech p-grams, than using models based on each of these features alone.", "labels": [], "entities": []}, {"text": "Combing string kernels with other kind of information is not covered in the present work.", "labels": [], "entities": []}, {"text": "The second goal of this work is to investigate which properties of the simple kernelbased approach are the main driver behind the higher performance than more complex approaches that take words, lemmas, syntactic information, or even semantics into account.", "labels": [], "entities": []}, {"text": "The language transfer analysis provided in Section 6 shows that there are generalizations to the kinds of mistakes that certain non-native speakers make that can be captured by p-grams of different lengths.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6942113190889359}]}, {"text": "Interestingly, using a range of p-grams implicitly generates a very large number of features including (but not limited to) stop words, stems of content words, word suffixes, entire words, and even p-grams of short words.", "labels": [], "entities": []}, {"text": "Rather than doing feature selection before the training step, which is the usual NLP approach, the kernel classifier, aided by regularization, selects the most relevant features during training.", "labels": [], "entities": []}, {"text": "With enough training samples, the kernel classifier does a better job of selecting the right features from a very high feature space.", "labels": [], "entities": []}, {"text": "This maybe one reason for why the string kernel approach works so well.", "labels": [], "entities": []}, {"text": "To gain additional insights into why this technique works so well, the features selected by the classifier as being most discriminating are analyzed in this work.", "labels": [], "entities": []}, {"text": "The analysis of the discriminant features also provides some information (useful in the context of SLA) about localized language transfer effects, namely, word choice (lexical transfer) and morphological differences.", "labels": [], "entities": [{"text": "SLA", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9614567160606384}, {"text": "word choice (lexical transfer)", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.7108234415451685}]}, {"text": "The article is organized as follows.", "labels": [], "entities": []}, {"text": "Work related to native language identification and to methods that work at the character level is presented in Section 2.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6306588848431905}]}, {"text": "Section 3 presents several similarity measures for strings, including string kernels and Local Rank Distance.", "labels": [], "entities": []}, {"text": "The learning methods used in the experiments are described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents details about the experiments.", "labels": [], "entities": []}, {"text": "Section 6 discusses the discriminant features of the string kernels approach and the observed language transfer effects.", "labels": [], "entities": []}, {"text": "Finally, conclusions are drawn in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main purpose of the experiments is to show that techniques that work at the character level can obtain state-of-the-art performance in NLI and are truly languageindependent.", "labels": [], "entities": []}, {"text": "The secondary purpose is to determine what kind of kernels and which kernel method works best in the context of NLI.", "labels": [], "entities": []}, {"text": "This section describes the results on the TOEFL11 corpus and, for the sake of completion, it also includes results for the 2013 Closed NLI Shared Task.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9544399678707123}, {"text": "completion", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9541638493537903}, {"text": "2013 Closed NLI Shared Task", "start_pos": 123, "end_pos": 150, "type": "DATASET", "confidence": 0.8254937648773193}]}, {"text": "In the closed shared task the goal is to predict the native language of testing examples, restricted to learning only from the training and the development data.", "labels": [], "entities": []}, {"text": "The additional information from prompts or the English language proficiency level were not used in the approach presented here.", "labels": [], "entities": []}, {"text": "The regularization parameters were tuned on the development set.", "labels": [], "entities": []}, {"text": "In this case, the systems were trained on the entire training set.", "labels": [], "entities": []}, {"text": "A 10-fold cross-validation (CV) procedure was done on the training and the development sets.", "labels": [], "entities": []}, {"text": "The folds were provided along with the TOEFL11 corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9534511566162109}]}, {"text": "Finally, the results of the proposed systems are also reported on the NLI Shared Task test set.", "labels": [], "entities": [{"text": "NLI Shared Task test set", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.8821087002754211}]}, {"text": "For testing, the systems were trained on both the training set and the development set.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The results presented in show that string kernels can reach state-of-the-art accuracy levels for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9986262321472168}]}, {"text": "Overall, it seems that KDA is able to obtain better results than KRR.", "labels": [], "entities": [{"text": "KDA", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.5429282784461975}, {"text": "KRR", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8804444670677185}]}, {"text": "The intersection kernel alone is able to obtain slightly better results than the presence bits kernel.", "labels": [], "entities": []}, {"text": "The kernel based on LRD provides significantly lower accuracy rates, but it is able to improve the performance when it is combined with the blended p-grams presence bits kernel.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9810642004013062}]}, {"text": "In fact, most of the kernel combinations give better results than each of their components.", "labels": [], "entities": []}, {"text": "The best kernel combination is that of the presence bits kernel and the intersection kernel.", "labels": [], "entities": []}, {"text": "Results are quite similar when they are combined either by summing them up or by kernel alignment.", "labels": [], "entities": []}, {"text": "The best performance on the test set (85.3%) is obtained by the system that combines these two kernels via kernel alignment and learns using KDA.", "labels": [], "entities": [{"text": "KDA", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.8563437461853027}]}, {"text": "This system is 1.7 percentage points better than the state-of-theart system of Jarvis, Bestgen, and Pepper (2013) based on SVM and word features, this being the top scoring system in the 2013 NLI Shared Task.", "labels": [], "entities": []}, {"text": "It is also 2.6 percentage points better than the state-of-the-art system based on string kernels of Popescu and Ionescu  Accuracy rates on TOEFL11 corpus (English L2) of various classification systems based on string kernels compared with other state-of-the-art approaches.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9700151681900024}, {"text": "TOEFL11 corpus (English L2)", "start_pos": 139, "end_pos": 166, "type": "DATASET", "confidence": 0.8830771048863729}]}, {"text": "The best accuracy rates on each set of experiments are highlighted in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992768168449402}]}, {"text": "The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.", "labels": [], "entities": [{"text": "kernel alignment", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7660481035709381}]}, {"text": "points below the top scoring system of Jarvis,.", "labels": [], "entities": [{"text": "scoring", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.5512703657150269}, {"text": "Jarvis", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9470030665397644}]}, {"text": "The empirical results obtained in this experiment demonstrate that the approach proposed in this article can reach state-of-the-art accuracy levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9989763498306274}]}, {"text": "The results presented in are obtained on the data released for the 2013 NLI Shared Task.", "labels": [], "entities": [{"text": "2013 NLI Shared Task", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.8965380042791367}]}, {"text": "The documents provided for the task were already tokenized.", "labels": [], "entities": []}, {"text": "Access to the raw text documents was not provided during the task.", "labels": [], "entities": []}, {"text": "As the TOEFL11 corpus was later released through LDC, 3 the raw text documents also became available for use.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.967672735452652}]}, {"text": "It is particularly interesting to evaluate the various string kernel systems on the raw text documents, in order to determine whether the string kernels approach is indeed neutral to any linguistic theory.", "labels": [], "entities": []}, {"text": "The process of breaking a stream of text up into tokens is not an easy task for some languages and it is often based on linguistic theories.", "labels": [], "entities": [{"text": "breaking a stream of text up into tokens", "start_pos": 15, "end_pos": 55, "type": "TASK", "confidence": 0.7839696928858757}]}, {"text": "Even if tokenization is fairly straightforward in languages that use spaces to separate words, there are many edge cases including contractions and hyphenated words that require linguistic knowledge.", "labels": [], "entities": []}, {"text": "Therefore, analyzing the results of the string kernels approach when tokenization is not being used at all will reveal if the linguistic knowledge helps to improve the performance or not.", "labels": [], "entities": []}, {"text": "reports the results on the raw text documents of various strings kernels combined either with KRR and KDA.", "labels": [], "entities": []}, {"text": "An important remark is that the tokenization process can influence the average word length in the corpus.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.9754706025123596}]}, {"text": "Therefore, the range of p-grams used for computing the blended string kernels was adjusted based on the development set for the raw text documents.", "labels": [], "entities": []}, {"text": "All p-grams in the range 3-9 were evaluated and the best results were obtained when all the p-grams with length in the range 5-9 were used.", "labels": [], "entities": []}, {"text": "Not surprisingly, the range of p-grams for the raw text documents is fairly close to the range of p-grams that worked best for the tokenized text.", "labels": [], "entities": []}, {"text": "The results on the ICLEv2 corpus using a 5-fold cross-validation procedure are summarized in.", "labels": [], "entities": [{"text": "ICLEv2 corpus", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9194362461566925}]}, {"text": "To adequately compare the results with a state-of-the-art system, the same 5-fold cross-validation procedure used by was also used in this experiment.", "labels": [], "entities": []}, {"text": "shows that the results obtained by the presence bits kernel and by the intersection kernel are systematically better than the state-of-the-art system Accuracy rates on ICLEv2 corpus (English L2) of various classification systems based on string kernels compared with a state-of-the-art approach.", "labels": [], "entities": [{"text": "ICLEv2 corpus (English L2)", "start_pos": 168, "end_pos": 194, "type": "DATASET", "confidence": 0.8886299928029379}]}, {"text": "The accuracy rates are reported using a 5-fold cross-validation (CV) procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994620680809021}]}, {"text": "The best accuracy rate is highlighted in bold. of.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9693566262722015}]}, {"text": "Although both KRR and KDA produce accuracy rates that are better than the state-of-the-art accuracy rate, it seems that KRR is slightly better in this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9976467490196228}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9987517595291138}, {"text": "KRR", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.7782068848609924}]}, {"text": "Again, the idea of combining kernels seems to produce more robust systems.", "labels": [], "entities": []}, {"text": "The best systems are based on combining the presence bits kernel either with the kernel based on LRD or the intersection kernel.", "labels": [], "entities": []}, {"text": "Overall, the reported accuracy rates are higher than the state-of-the-art accuracy rate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.990661084651947}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990492463111877}]}, {"text": "The best performance (91.3%) is achieved by the KRR classifier based on combining the presence bits kernel with the kernel based on LRD.", "labels": [], "entities": []}, {"text": "This represents a 1.2 percentage point improvement over the state-ofthe-art accuracy rate of.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9903038740158081}]}, {"text": "Two more systems are able to obtain accuracy rates greater than 91.0%.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9797845780849457}]}, {"text": "These are the KRR classifier based on the presence bits kernel (91.2%) and the KDA classifier based on the sum of the presence bits kernel and the intersection kernel (91.0%).", "labels": [], "entities": []}, {"text": "The overall results on the ICLEv2 corpus show that the string kernels approach can reach state-of-the-art accuracy levels, although a paired-sample Student's t-test indicated that the results are not significantly different.", "labels": [], "entities": [{"text": "ICLEv2 corpus", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8755864500999451}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9981959462165833}]}, {"text": "It is worth mentioning the purpose of this experiment was to use the same approach determined to work well in the TOEFL11 corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9586820304393768}]}, {"text": "To serve this purpose, the range of p-grams was not tuned on this data set.", "labels": [], "entities": []}, {"text": "Furthermore, other classifiers were not tested in this experiment.", "labels": [], "entities": []}, {"text": "Nevertheless, better results can probably be obtained by adding these aspects into the equation.", "labels": [], "entities": []}, {"text": "The presence bits kernel and the intersection kernel are evaluated on the TOEFL11-Big corpus using the 10-fold CV procedure.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.9791637361049652}]}, {"text": "The folds are chosen according to, in order to fairly compare the string kernels with their state-of-the-art ensemble model.", "labels": [], "entities": []}, {"text": "The kernel based on LRD is not evaluated in this experiment on purpose, because it obtained lower accuracy rates in the TOEFL11 and ICLE experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9989998936653137}, {"text": "TOEFL11", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9049660563468933}, {"text": "ICLE", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.4635538160800934}]}, {"text": "The second reason for not including LRD is that it is more computationally expensive than the presence bits kernel or the intersection kernel.", "labels": [], "entities": []}, {"text": "As in the previous experiments, both KRR and KDA were tested on TOEFL11-Big, but KDA was not trained using all the training samples because it runs out of memory.", "labels": [], "entities": [{"text": "TOEFL11-Big", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9060821533203125}]}, {"text": "The machine used for this experiment has 256 GB of RAM, and it seems that training KDA on 78, 000 samples requires slightly more memory.", "labels": [], "entities": [{"text": "RAM", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9679077863693237}]}, {"text": "However, a KDA classifier can successfully be trained on 75, 000 samples on the same machine.", "labels": [], "entities": [{"text": "KDA classifier", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7691859006881714}]}, {"text": "Therefore, the results of KDA presented in this section are obtained by training it on a random sample of 75, 000 samples selected during each fold.", "labels": [], "entities": [{"text": "KDA", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.894447386264801}]}, {"text": "The results of KDA might be slightly different (but probably less than 1 percentage point) if the entire available training data would be used.", "labels": [], "entities": [{"text": "KDA", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8120808601379395}]}, {"text": "The regularization parameters of KRR and KDA were tuned such that an accuracy of roughly 98% is obtained on the training set.", "labels": [], "entities": [{"text": "KRR", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.6976364850997925}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9996801614761353}]}, {"text": "In other words, the classifiers are allowed to disregard around 2% of the training documents to prevent overfitting.", "labels": [], "entities": []}, {"text": "From this perspective, the disregarded training documents are not considered helpful for the NLI task.", "labels": [], "entities": []}, {"text": "Preliminary results showed that trying to fit more of the training data leads to lower accuracy rates (not more than 1-2 percentage points, though) in the CV procedure.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9818021059036255}]}, {"text": "The results obtained by the presence bits kernel and by the intersection kernel are presented in.", "labels": [], "entities": []}, {"text": "The two kernels are also combined together.", "labels": [], "entities": []}, {"text": "When considering the individual kernels, KDA seems to work much better than KRR, even if it does not use the entire available training data set.", "labels": [], "entities": []}, {"text": "For instance, the KDA classifier based on the intersection kernel attains the same accuracy as the state-of-the-art ensemble model of, whereas KRR based on the same kernel reaches an accuracy of only 82.3%, which is 2.3 percentage points lower than the state-of-the-art (84.6%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9987651109695435}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9975817203521729}]}, {"text": "On the other hand, KRR overturns the results when the two kernels are combined through MKL.", "labels": [], "entities": [{"text": "MKL", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8476231098175049}]}, {"text": "Indeed, KRR based on the sum of\u02c6kof\u02c6 of\u02c6k Accuracy rates on TOEFL11-Big (English L2) corpus of various classification systems based on string kernels compared with a state-of-the-art approach.", "labels": [], "entities": [{"text": "KRR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.7128331065177917}, {"text": "Accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9985727071762085}, {"text": "TOEFL11-Big (English L2) corpus", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.9070654511451721}]}, {"text": "The accuracy rates are reported using a 10-fold cross-validation (CV) procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994484782218933}]}, {"text": "The best accuracy rate is highlighted in bold. of samples in the TOEFL11-Big data set can tell a different story, since 0.1% means roughly 90 samples.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9873155653476715}, {"text": "TOEFL11-Big data set", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9794296224912008}]}, {"text": "From this point of view, KRR based on the kernel combination assigns the correct labels for more than 170 extra samples compared to the state-ofthe-art system.", "labels": [], "entities": []}, {"text": "Moreover, these improvements are obtained without using any feature engineering, which represents an advantage over the standard NLP approaches.", "labels": [], "entities": []}, {"text": "To draw a conclusion on the empirical results, the idea of combining kernels through MKL proves to be extremely helpful for boosting the performance of the classification system.", "labels": [], "entities": []}, {"text": "KDA seems to be more stable with respect to the type of kernel, whereas KRR gives the top accuracy only when the kernels are combined.", "labels": [], "entities": [{"text": "KDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7992565035820007}, {"text": "KRR", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7837317585945129}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9654536843299866}]}, {"text": "In this experiment, various systems based on KRR or KDA are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9711657464504242}, {"text": "TOEFL11-Big corpus", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.9714345633983612}]}, {"text": "The state-of-the-art ensemble model () was trained and tested in exactly the same manner.", "labels": [], "entities": []}, {"text": "The goal of this cross-corpus evaluation is to show that the string kernel approach does not perform well simply because of potential topic bias in the corpora.", "labels": [], "entities": []}, {"text": "Again, the kernel based on LRD was not included in this experiment because it is more computationally expensive.", "labels": [], "entities": []}, {"text": "Therefore, only the presence bits kernel and the intersection kernel were evaluated on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9775389432907104}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The same regularization parameters determined to work well on the TOEFL11 development set were used.", "labels": [], "entities": [{"text": "TOEFL11 development set", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.9742231965065002}]}, {"text": "The most interesting fact is that all the proposed systems are at least 30 percentage points better than the state-of-the-art system.", "labels": [], "entities": []}, {"text": "Considering that the TOEFL11-Big corpus contains 87, 000 samples, the 30 percentage point improvement is significant without any doubt.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9468928277492523}]}, {"text": "Diving into details, we can see that the results obtained by KRR are higher than those obtained by KDA.", "labels": [], "entities": [{"text": "KRR", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.6408544182777405}, {"text": "KDA", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.880038857460022}]}, {"text": "However, both methods perform very well compared with the state-of-the-art.", "labels": [], "entities": []}, {"text": "Again, kernel combinations are better than each of their individual kernels alone.", "labels": [], "entities": []}, {"text": "Accuracy rates on TOEFL11-Big corpus (English L2) of various classification systems based on string kernels compared with a state-of-the-art approach.", "labels": [], "entities": [{"text": "TOEFL11-Big corpus (English L2)", "start_pos": 18, "end_pos": 49, "type": "DATASET", "confidence": 0.9427690704663595}]}, {"text": "The systems are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9680631160736084}, {"text": "TOEFL11-Big corpus", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9811041355133057}]}, {"text": "The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9583332538604736}]}, {"text": "The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.", "labels": [], "entities": [{"text": "kernel alignment", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7660481035709381}]}, {"text": "It is important to mention that the significant performance increase is not due to the learning method (KRR or KDA), but rather due to the string kernels that work at the character level.", "labels": [], "entities": []}, {"text": "It is not only the case that string kernels are language independent, but for the same reasons they can also be topic independent.", "labels": [], "entities": []}, {"text": "The topics (prompts) from TOEFL11 are different from the topics from TOEFL11-Big, and it becomes clear that a method that uses words as features is strongly affected by topic variations, because the distribution of words per topic can be completely different.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9048855900764465}, {"text": "TOEFL11-Big", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8996887803077698}]}, {"text": "But mistakes that reveal the native language can be captured by character p-grams that can appear more often even in different topics.", "labels": [], "entities": []}, {"text": "The results indicate that this is also the case of the approach based on string kernels, which seems to be more robust to such topic variations of the data set.", "labels": [], "entities": []}, {"text": "The best system has an accuracy rate that is 32.3 percentage points better than the state-of-the-art system of.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9853256344795227}]}, {"text": "Overall, the empirical results indicate that the string kernels approach can achieve significantly better results than other stateof-the-art approaches.", "labels": [], "entities": []}, {"text": "The string kernels approach is evaluated on Arabic data in order to demonstrate that the approach is language independent by showing that it can obtain state-of-the-art results. are the first (and only) to show NLI results on Arabic data.", "labels": [], "entities": []}, {"text": "In their evaluation, the 10-fold CV procedure was used to evaluate an SVM model based on several types of features including CFG rules, function words, and part-of-speech p-grams.", "labels": [], "entities": []}, {"text": "To directly compare the string kernel systems with the approach of, the same folds should have been used.", "labels": [], "entities": []}, {"text": "Because the folds are not publicly available, two alternative solutions are adopted to fairly compare the two NLI approaches.", "labels": [], "entities": []}, {"text": "First of all, each classification system based on string kernels is evaluated by repeating the 10-fold CV procedure for 20 times and averaging the resulted accuracy rates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9988337159156799}]}, {"text": "The folds are randomly selected at each trial.", "labels": [], "entities": []}, {"text": "This helps to reduce the amount of accuracy variation introduced by using a different partition of the data set for the 10-fold CV procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.998745322227478}]}, {"text": "To provide an idea of the amount of variation in each trial, the standard deviations for the computed average accuracy rates are also reported.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 110, "end_pos": 124, "type": "METRIC", "confidence": 0.9422605633735657}]}, {"text": "Second of all, the leave-one-out (LOO) cross-validation procedure is also adopted because it involves a predefined partitioning of the data set.", "labels": [], "entities": []}, {"text": "Furthermore, the LOO procedure can easily be performed on a small data set, such as the subset of 329 samples of the ALC.", "labels": [], "entities": [{"text": "LOO", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8485729098320007}]}, {"text": "Thus, the LOO procedure is more suitable for this NLI experiment, because it is straightforward to compare newly developed systems with the previous state-of-theart systems. were kindly asked to re-evaluate their system using the LOO CV procedure and they readily provided additional results, included in.", "labels": [], "entities": []}, {"text": "Lastly, a studentized bootstrap procedure based on 200 iterations was employed to provide confidence intervals with a confidence level of 95%.", "labels": [], "entities": []}, {"text": "The bootstrap procedure is suitable for small data sets such as the ALC subset.", "labels": [], "entities": []}, {"text": "presents the results of the classification systems based on string kernels in contrast to the results of the SVM model based on several combined features.", "labels": [], "entities": []}, {"text": "The results clearly indicate the advantage of using an approach that works at the character level.", "labels": [], "entities": []}, {"text": "When the 10-fold CV procedure is used, the average accuracy rates of the systems based on string kernels range between 45.9% and 56.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9992795586585999}]}, {"text": "The lowest scores are obtained by LRD, whereas the presence bits kernel and the intersection kernel obtain better results.", "labels": [], "entities": [{"text": "LRD", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.907151997089386}]}, {"text": "Even so, the kernel based on LRD is far better than the model of.", "labels": [], "entities": []}, {"text": "Combining the kernels through MKL again proves to be a good idea.", "labels": [], "entities": [{"text": "MKL", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.868177592754364}]}, {"text": "Certainly, the top scoring system in the 10-fold CV experiment is the KRR based on Accuracy rates on ALC subset (Arabic L2) of various classification systems based on string kernels compared with a state-of-the-art approach.", "labels": [], "entities": [{"text": "KRR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.7372984290122986}, {"text": "Accuracy rates", "start_pos": 83, "end_pos": 97, "type": "METRIC", "confidence": 0.9796185195446014}]}, {"text": "The accuracy rates are reported using a studentized bootstrap procedure and two cross-validation (CV) procedures, one based on 10 folds and one based on leave-one-out (LOO).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992743134498596}]}, {"text": "The 10-fold CV procedure was repeated for 20 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9990928173065186}]}, {"text": "The standard deviations for the computed average accuracy rates are also given.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9687072038650513}]}, {"text": "The bootstrap procedure is based on 200 iterations, and the reported confidence intervals are based on a 95% confidence level.", "labels": [], "entities": []}, {"text": "The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9583332538604736}]}, {"text": "the sum of the presence bits kernel and the intersection kernel.", "labels": [], "entities": []}, {"text": "Its accuracy (56.8%) is 15.8 percentage points above the accuracy of the SVM based on combined features (41.0%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997299313545227}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9994521737098694}, {"text": "SVM", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7657535672187805}]}, {"text": "An important remark is that the standard deviations computed over the 20 trials are between 1.1% and 1.6% for all systems, which means the amount of accuracy variation is too small to have an influence on the overall conclusion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9982277750968933}]}, {"text": "Furthermore, all the results obtained using the 10-fold CV procedure are consistent with the results obtained using the leave-one-out CV procedure or the bootstrap procedure.", "labels": [], "entities": []}, {"text": "The accuracy rates are generally lower when bootstrap is used for evaluation, because the bootstrap procedure is a rather more pessimistic way of estimating the performance compared to the CV procedures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993306398391724}]}, {"text": "With no exception, the models reach better accuracy rates when the LOO procedure is used, most likely because there are more samples available for training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9991974234580994}]}, {"text": "When the LOO CV procedure is used, the systems based on string kernels attain accuracy rates that range between 47.1% and 59.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9993422627449036}]}, {"text": "Better accuracy rates are obtained when the kernels are combined.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9992765784263611}]}, {"text": "When\u02c6kWhen\u02c6 When\u02c6k 0/1 3\u22125 and\u02c6kand\u02c6 and\u02c6k \u2229 3\u22125 are summed together and the training is performed by KRR, the best accuracy rate (59.3%) is obtained, which brings an improvement of 17.7 percentage points over accuracy of the SVM model (41.6%).", "labels": [], "entities": [{"text": "KRR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.6795586347579956}, {"text": "accuracy rate", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.991085410118103}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9995043277740479}]}, {"text": "Regarding the classification systems, KRR obtains better results than KDA for every kernel type, even if the differences are not that high.", "labels": [], "entities": []}, {"text": "Still, both KRR and KDA give results that are much better than the SVM of Malmasi and Dras (2014a).", "labels": [], "entities": [{"text": "KRR", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.49931037425994873}, {"text": "KDA", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.5000868439674377}]}, {"text": "In conclusion, each and every classification systems based on string kernels is significantly better that the SVM model of Malmasi and Dras (2014a) on the Arabic data set.", "labels": [], "entities": [{"text": "Arabic data set", "start_pos": 155, "end_pos": 170, "type": "DATASET", "confidence": 0.8251693844795227}]}, {"text": "Pepper (2012) conducted an extensive set of experiments on the ASK corpus.", "labels": [], "entities": [{"text": "ASK corpus", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8622272908687592}]}, {"text": "The 10-fold CV procedure and the leave-one-out CV procedure were alternatively used to evaluate various LDA models based on linguistic features.", "labels": [], "entities": []}, {"text": "Pepper used only 100 essays per native language and formed several subsets each of five languages.", "labels": [], "entities": []}, {"text": "All the subsets included four languages in common, namely, English, German, Polish, and Russian.", "labels": [], "entities": []}, {"text": "The fifth language was different in each subset.", "labels": [], "entities": []}, {"text": "The three subsets that included Spanish, Dutch, and Serbo-Croatian as the fifth language have also been used in the present work in order to compare the results of string kernels with the LDA model of.", "labels": [], "entities": []}, {"text": "For a direct comparison, the same 100 essays per native language should have been used, but this information is not available.", "labels": [], "entities": []}, {"text": "For this reason, the string kernels are evaluated by repeating the LOO CV procedure 20 times, and in each trial 100 essays per native language are randomly selected from the 200 essays available for each language.", "labels": [], "entities": [{"text": "LOO", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.8788952827453613}]}, {"text": "reports the average accuracy rates along with their standard deviations which give some indication about the amount of variation in each trial.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9968356490135193}]}, {"text": "The results obtained on the subset that includes Spanish along with English, German, Polish, and Russian are given in the second column.", "labels": [], "entities": []}, {"text": "Ina similar way, the results obtained on the subset that includes Dutch as the fifth language are listed in the third column, while the results obtained on the subset that includes Serbo-Croatian as the fifth language are given in the last column.", "labels": [], "entities": []}, {"text": "The best results obtained by Pepper (2012) using the LOO CV procedure are listed in the first row.", "labels": [], "entities": [{"text": "LOO", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8642775416374207}]}, {"text": "An important remark is that Pepper (2012) reported much lower accuracy rates when the 10-fold CV procedure was used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993667006492615}]}, {"text": "Because of this, the comparison is carried out only by using the LOO CV procedure, which also helps to reduce the amount of variation in the 20 trials.", "labels": [], "entities": [{"text": "LOO", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9253671765327454}]}, {"text": "However, it must be mentioned the results of string kernels are only 2 percentage points lower when the 10-fold CV procedure is used, but any further details of that experiment are not reported here.", "labels": [], "entities": []}, {"text": "The empirical results presented in are consistent with the results on the ALC corpus.", "labels": [], "entities": [{"text": "ALC corpus", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.8256682455539703}]}, {"text": "Indeed, all the systems based on string kernels are significantly better than the state-of-the-art approach, in all three experiments.", "labels": [], "entities": []}, {"text": "KDA and KRR seem to produce fairly similar results, but there is an interesting difference between the two classifiers that stands out.", "labels": [], "entities": [{"text": "KDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8294006586074829}]}, {"text": "This difference refers to the fact that KRR produces better results when the sum of\u02c6kof\u02c6 of\u02c6k 0/1 5\u22128 and\u02c6kand\u02c6 and\u02c6k \u2229 5\u22128 is used, whereas KDA produces better results when the kernel Accuracy rates on three subsets of five languages of the ASK corpus (Norwegian L2) of various classification systems based on string kernels compared with a state-of-the-art approach.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9972468614578247}, {"text": "ASK corpus (Norwegian L2)", "start_pos": 242, "end_pos": 267, "type": "DATASET", "confidence": 0.8715736369291941}]}, {"text": "All subsets include samples of English, German, Polish, and Russian (EGPR).", "labels": [], "entities": []}, {"text": "The fifth language is different in each subset.", "labels": [], "entities": []}, {"text": "The first subset includes Spanish (SP), the second one includes Dutch (DU), and the last subset includes Serbo-Croatian (SC) as the fifth language.", "labels": [], "entities": []}, {"text": "The accuracy rates for each subset are reported using the leave-one-out cross-validation procedure, which was repeated 20 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the documents (100 per language).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9989659786224365}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9955541491508484}]}, {"text": "The standard deviations for the computed average accuracy rates are also given.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9687072038650513}]}, {"text": "The best accuracy rate for each subset of five languages is highlighted in bold.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9846949577331543}]}, {"text": "67.7% \u00b1 1.4 69.1% \u00b1 1.9 68.0% \u00b1 1.3 based on LRD is also added into the sum.", "labels": [], "entities": [{"text": "LRD", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.949802815914154}]}, {"text": "When the kernels are used independently, it can be observed that LRD produces accuracy rates that are nearly 5 percentage points below the accuracy rates of the intersection kernel and the presence bits kernel, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9987590312957764}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9984182119369507}]}, {"text": "Nevertheless, the kernel based on LRD is still significantly better than the LDA model used by.", "labels": [], "entities": []}, {"text": "For instance, when Spanish is used as the fifth language, all the string kernels, including the one based on LRD, attain accuracy rates that are at least 10 percentage points better than LDA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9990173578262329}]}, {"text": "In all three evaluation sets, the best results are obtained when kernels are combined together.", "labels": [], "entities": []}, {"text": "The best accuracy on the first subset (67.8%) is given by the KRR based on the kernel combination of\u02c6kof\u02c6 of\u02c6k . This time, the accuracy improvement over the state-of-the-art LDA model is 13.5 percentage points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993658661842346}, {"text": "KRR", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7919241189956665}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9993416666984558}]}, {"text": "The best accuracy on the second subset of five languages (69.1%) is obtained by the KDA based on the combination of all three string kernels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992451667785645}, {"text": "KDA", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.7172449827194214}]}, {"text": "The accuracy improvement over the LDA model is 14.1 percentage points, which is consistent with the improvement demonstrated on the other two subsets that include Spanish and Serbo-Croatian, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996337890625}]}, {"text": "Even if the string kernels are significantly better than the LDA model, the two distinct approaches seem to agree on the difficulty of each subset.", "labels": [], "entities": []}, {"text": "Indeed, both approaches produce better results when Dutch is used as the fifth language and worse results when Spanish is the fifth language.", "labels": [], "entities": []}, {"text": "This could indicate that Dutch native speakers writing in Norwegian can be more easily distinguished than Spanish speakers writing in Norwegian.", "labels": [], "entities": []}, {"text": "Although one might expect Dutch, German, and English speakers to be more similar (and therefore potentially more difficult to distinguish) than, say, English or Spanish speakers writing in Norwegian, this does not appear to be the case, given that the overall performance is better when Dutch is added to the mix of English, German, Polish, and Russian.", "labels": [], "entities": []}, {"text": "What can be said for sure is that the approach based on string kernels attains a significant performance improvement over the LDA based on linguistic features.", "labels": [], "entities": []}, {"text": "The standard deviations computed over the 20 trials are less than 2.2% for all systems, which indicates that the amount of accuracy variation is small enough to support the conclusion that the string kernel approach works better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9987803101539612}]}], "tableCaptions": [{"text": " Table 1  Summary of corpora used in the experiments.", "labels": [], "entities": []}, {"text": " Table 2  Distribution of the documents per native language in the ALC subset.", "labels": [], "entities": []}, {"text": " Table 4. When the various kernels are mixed together, the  results are nearly identical to the results obtained on tokenized text documents. Overall,  the results are better when the string kernels are computed on the raw text, although  the improvements are not considerably different, since the accuracy difference is less  than 1 percentage point in all cases. However, the improvement could be the result of  removing tokenization or the results of using a different range of p-grams. To find which  of these two hypotheses is responsible for the performance gain, the presence bits kernel  was computed on the raw text documents using a range of 5-8 p-grams. The accuracy  obtained by KRR and\u02c6kand\u02c6 and\u02c6k", "labels": [], "entities": [{"text": "accuracy", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.9968380928039551}, {"text": "accuracy", "start_pos": 669, "end_pos": 677, "type": "METRIC", "confidence": 0.9970325231552124}]}, {"text": " Table 6  Accuracy rates on ICLEv2 corpus (English L2) of various classification systems based on string  kernels compared with a state-of-the-art approach. The accuracy rates are reported using a 5-fold  cross-validation (CV) procedure. The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "ICLEv2 corpus (English L2)", "start_pos": 28, "end_pos": 54, "type": "DATASET", "confidence": 0.894701341787974}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.998565137386322}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9988190531730652}]}, {"text": " Table 7. The two kernels are also combined together. When considering  the individual kernels, KDA seems to work much better than KRR, even if it does not  use the entire available training data set. For instance, the KDA classifier based on the  intersection kernel attains the same accuracy as the state-of-the-art ensemble model  of", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9990742206573486}]}, {"text": " Table 7  Accuracy rates on TOEFL11-Big (English L2) corpus of various classification systems based on  string kernels compared with a state-of-the-art approach. The accuracy rates are reported using a  10-fold cross-validation (CV) procedure. The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "TOEFL11-Big (English L2) corpus", "start_pos": 28, "end_pos": 59, "type": "DATASET", "confidence": 0.9096534848213196}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9987953901290894}, {"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9989190101623535}]}, {"text": " Table 8  Accuracy rates on TOEFL11-Big corpus (English L2) of various classification systems based  on string kernels compared with a state-of-the-art approach. The systems are trained on the  TOEFL11 corpus and tested on the TOEFL11-Big corpus. The best accuracy rate is highlighted in  bold. The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9676533341407776}, {"text": "TOEFL11-Big corpus (English L2)", "start_pos": 28, "end_pos": 59, "type": "DATASET", "confidence": 0.9111213882764181}, {"text": "TOEFL11 corpus", "start_pos": 194, "end_pos": 208, "type": "DATASET", "confidence": 0.9441498816013336}, {"text": "TOEFL11-Big corpus", "start_pos": 227, "end_pos": 245, "type": "DATASET", "confidence": 0.9495945274829865}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9981828331947327}]}, {"text": " Table 9  Accuracy rates on ALC subset (Arabic L2) of various classification systems based on string  kernels compared with a state-of-the-art approach. The accuracy rates are reported using a  studentized bootstrap procedure and two cross-validation (CV) procedures, one based on  10 folds and one based on leave-one-out (LOO). The 10-fold CV procedure was repeated for  20 times and the results were averaged to reduce the accuracy variation introduced by randomly  selecting the folds. The standard deviations for the computed average accuracy rates are also  given. The bootstrap procedure is based on 200 iterations, and the reported confidence intervals  are based on a 95% confidence level. The best accuracy rate is highlighted in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9989363551139832}, {"text": "accuracy", "start_pos": 425, "end_pos": 433, "type": "METRIC", "confidence": 0.9946322441101074}, {"text": "accuracy", "start_pos": 538, "end_pos": 546, "type": "METRIC", "confidence": 0.9209587574005127}, {"text": "accuracy", "start_pos": 707, "end_pos": 715, "type": "METRIC", "confidence": 0.99813312292099}]}, {"text": " Table 10  Accuracy rates on three subsets of five languages of the ASK corpus (Norwegian L2) of various  classification systems based on string kernels compared with a state-of-the-art approach. All  subsets include samples of English, German, Polish, and Russian (EGPR). The fifth language is  different in each subset. The first subset includes Spanish (SP), the second one includes Dutch  (DU), and the last subset includes Serbo-Croatian (SC) as the fifth language. The accuracy rates  for each subset are reported using the leave-one-out cross-validation procedure, which was  repeated 20 times and the results were averaged to reduce the accuracy variation introduced by  randomly selecting the documents (100 per language). The standard deviations for the  computed average accuracy rates are also given. The best accuracy rate for each subset of  five languages is highlighted in bold.", "labels": [], "entities": [{"text": "ASK corpus (Norwegian L2)", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.8857574065526327}, {"text": "accuracy", "start_pos": 475, "end_pos": 483, "type": "METRIC", "confidence": 0.9989421963691711}, {"text": "accuracy", "start_pos": 645, "end_pos": 653, "type": "METRIC", "confidence": 0.9962233304977417}, {"text": "accuracy", "start_pos": 782, "end_pos": 790, "type": "METRIC", "confidence": 0.8906264305114746}, {"text": "accuracy", "start_pos": 822, "end_pos": 830, "type": "METRIC", "confidence": 0.9945945143699646}]}, {"text": " Table 11 are similar to those presented in Table 10. The extra  number of documents per native language (200 instead of 100) seems to compensate for  having to discriminate between seven languages instead of five. When the 10-fold CV  procedure is used, the average accuracy rates of the systems based on string kernels  range between 60.5% and 68.2%. The lowest scores are obtained by LRD, while the  presence bits kernel and the intersection kernel obtain better results. Combining the  kernels through MKL proves to work again as a robust approach that usually improves  performance. It must be noted that combining the LRD kernel with either one of the  other kernels ( \u02c6  k", "labels": [], "entities": [{"text": "accuracy", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9987144470214844}]}]}