{"title": [], "abstractContent": [{"text": "Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6681715150674185}, {"text": "word sense induction", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.6896591186523438}]}, {"text": "But this seems to be a much easier task for some lemmas than others.", "labels": [], "entities": []}, {"text": "Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability.", "labels": [], "entities": []}, {"text": "Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.", "labels": [], "entities": []}, {"text": "We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster.", "labels": [], "entities": []}, {"text": "We test two ways of measuring clusterability: (1) existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and (2) the idea that if a lemma is more clusterable, two clusterings based on two different \"views\" of the same data points will be more congruent.", "labels": [], "entities": []}, {"text": "The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations.", "labels": [], "entities": []}, {"text": "We apply automatic clustering to the manual annotations.", "labels": [], "entities": []}, {"text": "We use manual annotations because we want the representations of the instances that we cluster to be as informative and \"clean\" as possible.", "labels": [], "entities": []}, {"text": "We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-(1) clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering.", "labels": [], "entities": []}], "introductionContent": [{"text": "In computational linguistics, the field of word sense disambiguation (WSD)-where a computer selects the appropriate sense from an inventory fora word in a given context-has received considerable attention.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)-", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.8286414444446564}]}, {"text": "Initially, most work focused on manually constructed inventories such as WordNet) but there has subsequently been a great deal of work on the related field of word sense induction (WSI); prior to disambiguation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9560828804969788}, {"text": "word sense induction (WSI)", "start_pos": 159, "end_pos": 185, "type": "TASK", "confidence": 0.8114911516507467}]}, {"text": "This article concerns the phenomenon of word meaning and current practice in the fields of WSD and WSI.", "labels": [], "entities": [{"text": "word meaning", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.7158877104520798}, {"text": "WSD", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9285396337509155}]}, {"text": "Computational approaches to determining word meaning in context have traditionally relied on a fixed sense inventory produced by humans or by a WSI system that groups token instances into hard clusters.", "labels": [], "entities": [{"text": "determining word meaning in context", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.8224370360374451}]}, {"text": "Either sense inventory can then be applied to tag sentences on the premise that there will be one best-fitting sense for each token instance.", "labels": [], "entities": []}, {"text": "However, word meanings do not always take the form of discrete senses but vary on a continuum between clear-cut ambiguity and vagueness.", "labels": [], "entities": []}, {"text": "For example, the noun crane is a clear-cut case of ambiguity between lifting device and bird, whereas the exact meaning of the noun thing can only be retrieved via the context of use rather than via a representation in the mental lexicon of speakers.", "labels": [], "entities": []}, {"text": "Cases of polysemy such as the verb paint, which can mean painting a picture, decorating a room, or painting a mural on a house, lie somewhere between these two poles.", "labels": [], "entities": []}, {"text": "Tuggy highlights the fact that boundaries between these different categories are blurred.", "labels": [], "entities": [{"text": "Tuggy", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.885267972946167}]}, {"text": "Although specific context clearly plays a role) some lemmas are inherently much harder to partition than others.", "labels": [], "entities": []}, {"text": "There are recent attempts to address some of these issues by using alternative characterizations of word meaning that do not involve creating a partition of usages into senses, and by asking WSI systems to produce soft or graded clusterings where tokens can belong to a mixture of the clusters.", "labels": [], "entities": []}, {"text": "However, these approaches do not overtly consider the location of a lemma on the continuum, but doing so should help in determining an appropriate representation.", "labels": [], "entities": []}, {"text": "Whereas the broad senses of the noun crane could easily be represented by a hard clustering, this would not make any sense for the noun thing; meanwhile, the verb paint might benefit from a more graded representation.", "labels": [], "entities": []}, {"text": "In this article, we propose the notion of partitionability of a lemma, that is, the ease with which usages can be grouped into senses.", "labels": [], "entities": []}, {"text": "We exploit data from annotation studies to explore the partitionability of different lemmas and see whereon the ambiguityvagueness cline a lemma is.", "labels": [], "entities": []}, {"text": "This should be useful in helping to determine the appropriate computational representation fora word's meanings-for example, whether a hard clustering will suffice, whether a soft clustering would be more appropriate, or whether a clustering representation does not make sense.", "labels": [], "entities": []}, {"text": "To our knowledge, there has been no study on detecting partitionability of word senses.", "labels": [], "entities": [{"text": "partitionability of word senses", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7280460745096207}]}, {"text": "We operationalize partitionability as clusterability, a measure of how much structure there is in the data and therefore how easy it is to cluster, and test to what extent clusterability can predict partitionability.", "labels": [], "entities": []}, {"text": "For deriving a gold estimate of partitionability, we turn to the Usage Similarity (hereafter Usim) data set, for which annotators have rated the similarity of should be computationally easier to cluster) because the structure in the data is more obvious, so any reasonable algorithm should be able to partition the data to reflect that structure.", "labels": [], "entities": [{"text": "Usim) data set", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.785376712679863}]}, {"text": "We contrast the performance of the three intraclust measures and the inter-clust measure with a simplistic baseline that relies on the amount of overlapping items in a soft clustering of the instance data, since such a baseline would be immediately available if one applied soft clustering to all lemmas.", "labels": [], "entities": []}, {"text": "We show that when controlling for polysemy, our indicators of higher clusterability tend to correlate with our two gold standard partitionability estimates.", "labels": [], "entities": []}, {"text": "In particular, clusterability tends to correlate positively with higher inter-annotator agreement and negatively with a greater proportion of mid-range judgments on a graded scale of instance similarity.", "labels": [], "entities": []}, {"text": "Although all our measures show some positive results, it is the intraclust measures (particularly two of these) that are most promising.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments reported here, we test both intra-clust and inter-clust clusterability measures.", "labels": [], "entities": []}, {"text": "All clusterability results are computed on the basis of LEXSUB and CLLS data.", "labels": [], "entities": [{"text": "LEXSUB", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9468008279800415}, {"text": "CLLS data", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.7285790741443634}]}, {"text": "The clusterings that we use for the intra-clust measures are k-means clusterings.", "labels": [], "entities": []}, {"text": "We use k-means because this is how these measures have been defined in the machine learning literature; as k-means is a widely used clustering, this is not an onerous restriction.", "labels": [], "entities": []}, {"text": "The similarity between sentences used by k-means is defined in Section 4.2.", "labels": [], "entities": []}, {"text": "The k-means method needs the number k of clusters as input; we determine this number for each lemma by a simple graph-partitioning method that groups all instances that have a minimum number of substitutes in common (Section 4.3).", "labels": [], "entities": []}, {"text": "The graph-partitioning method is also used for the inter-clust approach, since it provides the simplest partitioning of the data and determines the number of partitions (clusters) automatically.", "labels": [], "entities": []}, {"text": "In addition to the intra-clust and inter-clust clusterability measures, we test a baseline measure based on degree of overlap in an overlapping clustering (Section 4.4).", "labels": [], "entities": []}, {"text": "We compare the clusterability ratings to two gold standard partitionability estimates, both of which are derived from Usim (Section 4.1).", "labels": [], "entities": []}, {"text": "We perform two experiments to measure how well clusterability tracks partitionability (Section 4.6).", "labels": [], "entities": []}, {"text": "In we give an overview of the whole processing pipeline, from the input data to the clusterability estimation.", "labels": [], "entities": []}, {"text": "The graphs built for each lemma from the LEXSUB and CLLS 12 Cliques are computed directly from a graph, not from the COMPS.", "labels": [], "entities": [{"text": "LEXSUB", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.7614036202430725}, {"text": "CLLS 12 Cliques", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.8247542977333069}, {"text": "COMPS", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.9418617486953735}]}, {"text": "13 Note that two different COMPS and CLIQUES can share substitutes (translations or paraphrases).", "labels": [], "entities": []}, {"text": "Substitutes serve to determine the distance of the instances.", "labels": [], "entities": []}, {"text": "If the distance is high, two instances are not linked in the graph despite their shared substitutes.", "labels": [], "entities": []}, {"text": "gives an overview of the expected directions.", "labels": [], "entities": []}, {"text": "We perform two sets of experiments, which differ in the way in which we control for polysemy.", "labels": [], "entities": []}, {"text": "Partitionability estimates as well as clusterability predictions can be expected to be influenced by polysemy.", "labels": [], "entities": []}, {"text": "Polysemy has an influence on inter-annotator agreement in that agreement is lower with higher attested polysemy ().", "labels": [], "entities": []}, {"text": "The number of clusters also influences all our measures of clusterability.", "labels": [], "entities": []}, {"text": "note that V and pF are influenced by polysemy.", "labels": [], "entities": []}, {"text": "Also, all intra-clust clusterability measures are influenced by k.", "labels": [], "entities": []}, {"text": "Variance ratio and worst pair ratio both improve monotonically with k because the distance of points from the center mass of their cluster decreases as the number of clusters rises (this affects the within-cluster variance W(C) and width(C)).", "labels": [], "entities": [{"text": "Variance ratio", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9801283478736877}, {"text": "worst pair ratio", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.8160064419110616}, {"text": "width(C))", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9428642839193344}]}, {"text": "Separability is always lowest fork = n (number of data points), and almost always second-lowest fork = n \u2212 1.", "labels": [], "entities": [{"text": "Separability", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9449892044067383}]}, {"text": "The first set of experiments measures correlation using Spearman's \u03c1 between a ranking of partitionability estimates and a ranking of clusterability predictions.", "labels": [], "entities": []}, {"text": "We do not perform correlation across all lemmas but control for polysemy by grouping lemmas into polysemy bands, and performing correlations only on lemmas with a polysemy within the bounds of the same band.", "labels": [], "entities": []}, {"text": "Let k be the number of clusters for lemma l, which is the number of COMPS for all clusterability metrics other than nc s , and the number of CLIQUES for nc s . For the cluster congruence metrics (V and pF), we take the average number of clusters fora lemma in both LEXSUB and CLLS.", "labels": [], "entities": [{"text": "COMPS", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.991775631904602}]}, {"text": "Then we define three polysemy bands: r mid: 4.3 \u2264 k < 6.6 r high: 6.6 \u2264 k < 9 Note that none of the intra-clust clusterability measures are applicable fork = 1, so in cases where the number of COMPS is one, the lemma is excluded from analysis.", "labels": [], "entities": []}, {"text": "In these cases the clustering algorithm itself decides that the instances are not easy to partition.", "labels": [], "entities": []}, {"text": "The second set of experiments performs linear regression to link partitionability to clusterability, using the degree of polysemy k as an additional independent variable.", "labels": [], "entities": []}, {"text": "As we expect polysemy to interfere with all clusterability measures, we are interested not so much in polysemy as a separate variable but in the interaction polysemy \u00d7 clusterability.", "labels": [], "entities": []}, {"text": "This lets us test experimentally whether our prediction that polysemy influences clusterability is borne out in the data.", "labels": [], "entities": []}, {"text": "As the second set of experiments does not break the lemmas into polysemy bands, we have a single, larger set of data points undergoing analysis, which gives us a stronger basis for assessing significance.", "labels": [], "entities": []}, {"text": "In this section we provide our main results evaluating the various clusterability measures against our gold-standard estimates.", "labels": [], "entities": []}, {"text": "Section 5.1 discusses the evaluation via correlation with Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "In Section 5.2 we present the regression experiments.", "labels": [], "entities": []}, {"text": "In Section 5.3 we provide examples and lemma rankings by two of our best performing metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Sentences for post.n from LEXSUB.", "labels": [], "entities": [{"text": "post.n from LEXSUB", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.7008041938145956}]}, {"text": " Table 3  Average Usim judgments for post.n.", "labels": [], "entities": [{"text": "post.n", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.943468451499939}]}, {"text": " Table 7  The macro-averaged correlation of each clusterability metric with the Usim gold-standard  rankings Uiaa and Umid: All correlations are in the expected direction. Also, the proportion  (prop.) of trials from", "labels": [], "entities": [{"text": "Usim gold-standard", "start_pos": 80, "end_pos": 98, "type": "DATASET", "confidence": 0.8542022109031677}]}, {"text": " Table 12  Values of clusterability metrics for the examples fire.v and solid.a.", "labels": [], "entities": []}]}