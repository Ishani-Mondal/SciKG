{"title": [{"text": "Representing Meaning with a Combination of Logical and Distributional Models", "labels": [], "entities": [{"text": "Representing Meaning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.913960725069046}]}], "abstractContent": [{"text": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements.", "labels": [], "entities": []}, {"text": "Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning.", "labels": [], "entities": []}, {"text": "Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches.", "labels": [], "entities": []}, {"text": "It has therefore been argued that the two are complementary.", "labels": [], "entities": []}, {"text": "We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks.", "labels": [], "entities": []}, {"text": "In this article, we focus on the three components of a practical system: 1 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently.", "labels": [], "entities": [{"text": "Logical representation", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8343812227249146}, {"text": "knowledge base construction", "start_pos": 171, "end_pos": 198, "type": "TASK", "confidence": 0.6277812222639719}]}, {"text": "To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations.", "labels": [], "entities": []}, {"text": "In particular we focus on the SICK data set, where we achieve state-of-the-art results.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9065397580464681}]}, {"text": "We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9037140409151713}]}], "introductionContent": [{"text": "Computational semantics studies mechanisms for encoding the meaning of natural language in a machine-friendly representation that supports automated reasoning and that, ideally, can be automatically acquired from large text corpora.", "labels": [], "entities": [{"text": "Computational semantics", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8352232277393341}]}, {"text": "Effective semantic representations and reasoning tools give computers the power to perform complex applications like question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8907846808433533}]}, {"text": "But applications of computational semantics are very diverse and pose differing requirements on the underlying representational formalism.", "labels": [], "entities": []}, {"text": "Some applications benefit from a detailed representation of the structure of complex sentences.", "labels": [], "entities": []}, {"text": "Some applications require the ability to recognize near-paraphrases or degrees of similarity between sentences.", "labels": [], "entities": []}, {"text": "Some applications require inference, either exact or approximate.", "labels": [], "entities": []}, {"text": "Often, it is necessary to handle ambiguity and vagueness in meaning.", "labels": [], "entities": []}, {"text": "Finally, we frequently want to learn knowledge relevant to these applications automatically from corpus data.", "labels": [], "entities": []}, {"text": "There is no single representation for natural language meaning at this time that fulfills all of these requirements, but there are representations that fulfill some of them.", "labels": [], "entities": []}, {"text": "Logic-based representations, like first-order logic, represent many linguistic phenomena like negation, quantifiers, or discourse entities.", "labels": [], "entities": []}, {"text": "Some of these phenomena (especially negation scope and discourse entities over paragraphs) cannot be easily represented in syntax-based representations like Natural Logic (.", "labels": [], "entities": []}, {"text": "In addition, firstorder logic has standardized inference mechanisms.", "labels": [], "entities": []}, {"text": "Consequently, logical approaches have been widely used in semantic parsing where it supports answering complex natural language queries requiring reasoning and data aggregation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.757708877325058}, {"text": "answering complex natural language queries", "start_pos": 93, "end_pos": 135, "type": "TASK", "confidence": 0.7986063599586487}]}, {"text": "But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems.", "labels": [], "entities": []}, {"text": "And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic and probabilities).", "labels": [], "entities": []}, {"text": "Distributional models use contextual similarity to predict the graded semantic similarity of words and phrases, and to model polysemy.", "labels": [], "entities": []}, {"text": "But at this point, fully representing structure and logical form using distributional models of phrases and sentences is still an open problem.", "labels": [], "entities": []}, {"text": "Also, current distributional representations do not support logical inference that captures the semantics of negation, logical connectives, and quantifiers.", "labels": [], "entities": []}, {"text": "Therefore, distributional models and logical representations of natural language meaning are complementary in their strengths, as has frequently been remarked.", "labels": [], "entities": []}, {"text": "Our aim has been to construct a general-purpose natural language understanding system that provides in-depth representations of sentence meaning amenable to automated inference, but that also allows for flexible and graded inferences involving word meaning.", "labels": [], "entities": []}, {"text": "Therefore, our approach combines logical and distributional methods.", "labels": [], "entities": []}, {"text": "Specifically, we use first-order logic as a basic representation, providing a sentence representation that can be easily interpreted and manipulated.", "labels": [], "entities": []}, {"text": "However, we also use distributional information fora more graded representation of words and short phrases, providing information on near-synonymy and lexical entailment.", "labels": [], "entities": []}, {"text": "Uncertainty and gradedness at the lexical and phrasal level should inform inference at all levels, so we rely on probabilistic inference to integrate logical and distributional semantics.", "labels": [], "entities": []}, {"text": "Thus, our system has three main components, all of which present interesting challenges.", "labels": [], "entities": []}, {"text": "For logic-based semantics, one of the challenges is to adapt the representation to the assumptions of the probabilistic logic.", "labels": [], "entities": []}, {"text": "For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules.", "labels": [], "entities": []}, {"text": "In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference . Our approach has previously been described in Garrette, Erk, and Mooney (2011) and.", "labels": [], "entities": [{"text": "probabilistic inference", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7149782180786133}]}, {"text": "We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al., SICK and FraCas in Beltagy and Erk) and semantic textual similarity, and we are investigating applications to question answering.", "labels": [], "entities": [{"text": "RTE-1", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.7759586572647095}, {"text": "question answering", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.8545832335948944}]}, {"text": "We have demonstrated the modularity of the system by testing both MLNs () and Probabilistic Soft Logic as probabilistic inference engines (.", "labels": [], "entities": []}, {"text": "The primary aim of the current article is to describe our complete system in detailall the nuts and bolts necessary to bring together the three distinct components of our approach-and to showcase some of the difficult problems that we face in all three areas, along with our current solutions.", "labels": [], "entities": []}, {"text": "The secondary aim of this article is to show that it is possible to take this general approach and apply it to a specific task-here, textual entailment ()-adding task-specific aspects to the general framework in such away that the model achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "We chose the task of textual entailment because it utilizes the strengths of both logical and distributional representations.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7139193713665009}]}, {"text": "We specifically use the SICK dataset () because it was designed to focus on lexical knowledge rather than world knowledge, matching the focus of our system.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7380284368991852}]}, {"text": "Our system is flexible with respect to the sources of lexical and phrasal knowledge it uses, and in this article we utilize PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013) and WordNet, along with distributional models.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.9555841684341431}]}, {"text": "But we are specifically interested in distributional models, in particular, in how well they can predict lexical and phrasal entailment.", "labels": [], "entities": []}, {"text": "Our system provides a unique framework for evaluating distributional models on recognizing textual entailment (RTE) because the overall sentence representation is handled by the logic, so we can zoom in on the performance of distributional models at predicting lexical and phrasal entailment.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.7320172588030497}, {"text": "predicting lexical and phrasal entailment", "start_pos": 250, "end_pos": 291, "type": "TASK", "confidence": 0.8146146416664124}]}, {"text": "The evaluation of distributional models on RTE is the third aim of our article.", "labels": [], "entities": [{"text": "RTE", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.703142523765564}]}, {"text": "We build a lexical entailment classifier that exploits both task-specific features as well as distributional information, and present an in-depth evaluation of the distributional components.", "labels": [], "entities": []}, {"text": "We now provide a brief sketch of our framework).", "labels": [], "entities": []}, {"text": "Our framework is three components.", "labels": [], "entities": []}, {"text": "The first is the logical form, which is the primary meaning representation fora sentence.", "labels": [], "entities": []}, {"text": "The second is the distributional information, which is encoded in the form of weighted logical rules (first-order formulas).", "labels": [], "entities": []}, {"text": "For example, in its simplest form, our approach can use the distributional similarity of the words grumpy and sad as the weight on a rule that says if x is grumpy, then there is a chance that x is also sad: \u2200x.grumpy(x) \u2192 sad(x) | f (sim( grumpy, sad)) where grumpy and sad are the vector representations of the words grumpy and sad, sim is a distributional similarity measure, like cosine, and f is a function that maps the similarity score to an MLN weight.", "labels": [], "entities": []}, {"text": "A more principled, and in fact, superior, choice is to use an asymmetric similarity measure to compute the weight, as we discuss subsequently.", "labels": [], "entities": []}, {"text": "The third component is inference.", "labels": [], "entities": []}, {"text": "We draw inferences over the weighted rules using MLNs (), a Statistical Relational Learning technique that combines logical and statistical knowledge in one uniform framework, and provides a mechanism for coherent probabilistic inference.", "labels": [], "entities": []}, {"text": "MLNs represent uncertainty in terms of weights on the logical rules, as in this example: \u2200x. ogre(x) \u21d2 grumpy(x) | 1.5 \u2200x, y.", "labels": [], "entities": []}, {"text": "( friend(x, y) \u2227 ogre(x)) \u21d2 ogre(y) | 1.1 (1) which states that there is a chance that ogres are grumpy, and friends of ogres tend to be ogres too.", "labels": [], "entities": []}, {"text": "Markov logic uses such weighted rules to derive a probability distribution over possible worlds through an undirected graphical model.", "labels": [], "entities": []}, {"text": "This probability distribution over possible worlds is then used to draw inferences.", "labels": [], "entities": []}, {"text": "We publish a data set of the lexical and phrasal rules that our system queries when running on SICK, along with gold standard annotations.", "labels": [], "entities": []}, {"text": "The training and testing sets are extracted from the SICK training and testing sets, respectively.", "labels": [], "entities": [{"text": "SICK training and testing sets", "start_pos": 53, "end_pos": 83, "type": "DATASET", "confidence": 0.6368397891521453}]}, {"text": "The total number of rules (training + testing) is 12,510-only 10,211 are unique with 3,106 entailing rules, 177 contradictions, and 6,928 neutral.", "labels": [], "entities": []}, {"text": "This is a valuable resource for testing lexical entailment systems, containing a variety of entailment relations (hypernymy, synonymy, antonymy, etc.) that are actually useful in an end-to-end RTE system.", "labels": [], "entities": []}, {"text": "In addition to providing further details on the approach introduced in Garrette, Erk, and Mooney (2011) and (including improvements that improve the scalability of MLN inference  and adapt logical constructs for probabilistic inference), this article makes the following new contributions: r We show how to represent the RTE task as an inference problem in probabilistic logic (Sections 4.1, 4.2), arguing for the use of a closed-word assumption (Section 4.3).", "labels": [], "entities": [{"text": "RTE task", "start_pos": 321, "end_pos": 329, "type": "TASK", "confidence": 0.9257353842258453}]}, {"text": "r Contradictory RTE sentence pairs are often only contradictory given some assumption about entity coreference.", "labels": [], "entities": [{"text": "entity coreference", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.6872886121273041}]}, {"text": "For example, An ogre is not snoring and An ogre is snoring are not contradictory unless we assume that the two ogres are the same.", "labels": [], "entities": []}, {"text": "Handling such coreferences is important to detecting many cases of contradiction (Section 4.4).", "labels": [], "entities": []}, {"text": "r We use multiple parses to reduce the impact of misparsing (Section 4.5).", "labels": [], "entities": []}, {"text": "r In addition to distributional rules, we add rules from existing databases, in particular WordNet (Princeton University 2010) and the paraphrase collection PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013) (Section 5.3).", "labels": [], "entities": [{"text": "WordNet (Princeton University 2010)", "start_pos": 91, "end_pos": 126, "type": "DATASET", "confidence": 0.9101060231526693}]}, {"text": "r We provide a logic-based alignment to guide generation of distributional rules (Section 5.1).", "labels": [], "entities": []}, {"text": "r We provide a data set of all lexical and phrasal rules needed for the SICK data set.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.8491246700286865}]}, {"text": "This is a valuable resource for testing lexical entailment systems on entailment relations that are actually useful in an end-to-end RTE system (Section 5.1).", "labels": [], "entities": []}, {"text": "r We evaluate a state-of-the-art compositional distributional approach on the task of phrasal entailment (Section 5.2.5).", "labels": [], "entities": [{"text": "phrasal entailment", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7627366185188293}]}, {"text": "r We propose a simple weight learning approach to map rule weights to MLN weights (Section 6.3).", "labels": [], "entities": []}, {"text": "r The question \"Do supervised distributional methods really learn lexical inference relations?\"", "labels": [], "entities": []}, {"text": "() has been studied before on a variety of lexical entailment data sets.", "labels": [], "entities": []}, {"text": "For the first time, we study it on data from an actual RTE data set and show that distributional information is useful for lexical entailment (Section 7.1).", "labels": [], "entities": [{"text": "RTE data set", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8365815877914429}, {"text": "lexical entailment", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7369210422039032}]}, {"text": "r report that for the SICK data set used in SemEval 2014, the best result was achieved by systems that did not compute a sentence representation in a compositional manner.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8800601561864217}, {"text": "SemEval 2014", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.5631971657276154}]}, {"text": "We present a model that performs deep compositional semantic analysis and achieves state-of-the-art performance (Section 7.2).", "labels": [], "entities": [{"text": "compositional semantic analysis", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6755402088165283}]}], "datasetContent": [{"text": "This section evaluates our system.", "labels": [], "entities": []}, {"text": "First, we evaluate several lexical and phrasal distributional systems on the rules that we collected using modified Robinson resolution.", "labels": [], "entities": []}, {"text": "This includes an in-depth analysis of different types of distributional information within the entailment rule classifier.", "labels": [], "entities": []}, {"text": "Second, we use the best configuration we find in the first step as a knowledge base and evaluate our system on the RTE task using the SICK data set.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 115, "end_pos": 123, "type": "TASK", "confidence": 0.6920832991600037}, {"text": "SICK data set", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.8475238879521688}]}, {"text": "The SICK data set, which is described in Section 2, consists of 5,000 pairs for training and 4,927 for testing.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8659300804138184}]}, {"text": "Pairs are annotated for RTE and STS tasks.", "labels": [], "entities": [{"text": "RTE", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7668058276176453}]}, {"text": "We use the RTE annotations of the data set.", "labels": [], "entities": [{"text": "RTE annotations of the data set", "start_pos": 11, "end_pos": 42, "type": "DATASET", "confidence": 0.8035866916179657}]}, {"text": "This section evaluates different components of the system, and finds a configuration of our system that achieves state-of-the-art results on the SICK RTE data set.", "labels": [], "entities": [{"text": "SICK RTE data set", "start_pos": 145, "end_pos": 162, "type": "DATASET", "confidence": 0.817284032702446}]}, {"text": "We evaluate the following system components.", "labels": [], "entities": []}, {"text": "The component logic is our basic MLN-based logic system that computes two inference probabilities (Section 4.1).", "labels": [], "entities": []}, {"text": "This includes the changes to the logical form to handle the domain closure assumption (Section 4.2), the inference algorithm for query formulas (Section 6.1), and the inference optimization (Section 6.2).", "labels": [], "entities": [{"text": "inference optimization", "start_pos": 167, "end_pos": 189, "type": "TASK", "confidence": 0.7829103767871857}]}, {"text": "The component cws deals with the problem that the closed-world assumption raises for negation in the hypothesis (Section 4.3), and coref is coreference resolution to identify contradictions (Section 4.4).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.852158784866333}]}, {"text": "The component multiparse signals the use of two parsers, the top C&C parse and the top EasyCCG parse (Section 4.5).", "labels": [], "entities": [{"text": "EasyCCG", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9232452511787415}]}, {"text": "The remaining components add entailment rules.", "labels": [], "entities": []}, {"text": "The component eclassif adds the rules from the best performing entailment rule classifier trained in Section 7.1.", "labels": [], "entities": []}, {"text": "This is the system with all features included.", "labels": [], "entities": []}, {"text": "The ppdb component adds rules from PPDB paraphrase collection (Section 5.3).", "labels": [], "entities": [{"text": "PPDB paraphrase collection", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7271341880162557}]}, {"text": "The wlearn component learns a scaling factor for ppdb rules, and another scaling factor for the eclassif rules that maps the classification confidence scores to MLN weights (Section 6.3).", "labels": [], "entities": []}, {"text": "Without weight learning, the scaling factor for ppdb is set to 1, and all eclassif rules are used as hard rules (infinite weight).", "labels": [], "entities": []}, {"text": "The wlearn log component is similar to wlearn but uses Equation (11), which first transforms a rule weight to its log odds.", "labels": [], "entities": [{"text": "Equation", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9890407919883728}]}, {"text": "The wn component adds rules from WordNet (Section 5.3).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9641611576080322}]}, {"text": "In addition, we have a few handcoded rules (Section 5.3).", "labels": [], "entities": []}, {"text": "Like wn, the components hyp and mem repeat information that is used as features for entailment rules classification but is not always picked up by the classifier.", "labels": [], "entities": [{"text": "entailment rules classification", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7613097826639811}]}, {"text": "As the classifier sometimes misses hypernyms, hyp marks all hypernymy rules as entailing (so this component is subsumed by wn), as well as all rules where the left-hand side and the right-hand side are the same.", "labels": [], "entities": []}, {"text": "(The latter step becomes necessary after splitting long rules derived by our modified Robinson resolution; some of the pieces may have equal left-hand and right-hand sides.)", "labels": [], "entities": []}, {"text": "The mem component memorizes all entailing rules seen in the training set of eclassif.", "labels": [], "entities": []}, {"text": "Sometimes inference takes along time, so we set a 2-minute timeout for each inference run.", "labels": [], "entities": []}, {"text": "If inference does not finish processing within the time limit, we terminate the process and return an error code.", "labels": [], "entities": []}, {"text": "About 1% of the data set times out.", "labels": [], "entities": []}, {"text": "7.2.1 Ablation Experiment without eclassif.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9498743414878845}]}, {"text": "Because eclassif has the most impact on the system's accuracy, and when enabled suppresses the contribution of the other components, we evaluate the other components first without eclassif.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9986554384231567}]}, {"text": "In the following section, we add the eclassif rules.", "labels": [], "entities": []}, {"text": "summarizes the results of this experiment.", "labels": [], "entities": []}, {"text": "The results show that each component plays a role in improving the system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9970927238464355}]}, {"text": "Our best accuracy without eclassif is 80.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995384216308594}]}, {"text": "Without handling the problem of negated hypotheses (logic alone), P(\u00acH|T) is almost always 1 and this additional inference becomes useless, resulting in an inability to distinguish between neutral and contradiction.", "labels": [], "entities": []}, {"text": "Adding cwa significantly improves accuracy because the resulting system has P(\u00acH|T) equal to 1 only for contradictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9990978240966797}]}, {"text": "Each rule set (ppdb, wn, handcoded) improves accuracy by reducing the number of false negatives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9991918206214905}]}, {"text": "We also note that applying weight learning (wlearn) to find a global scaling factor for PPDB rules makes them more useful.", "labels": [], "entities": []}, {"text": "The learned scaling factor is 3.0.", "labels": [], "entities": []}, {"text": "When the knowledge base is lacking other sources, weight learning assigns a high scaling factor to PPDB, giving it more influence throughout.", "labels": [], "entities": []}, {"text": "When eclassif is added in the following section, weight learning assigns PPDB a low scaling factor because eclassif already includes a large set of useful rules, such that only the highest weighted PPDB rules contribute significantly to the final inference.", "labels": [], "entities": []}, {"text": "The last component tested is the use of multiple parses (multiparse).", "labels": [], "entities": []}, {"text": "Many of the false negatives are due to misparses.", "labels": [], "entities": []}, {"text": "Using two different parses reduces the impact of the misparses, improving the system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.99608314037323}]}, {"text": "In this experiment, we first use eclassif as a knowledge base, then incrementally add the other system components.", "labels": [], "entities": []}, {"text": "First, we note that adding eclassif to the knowledge base KB significantly improves the accuracy from 73.4% to 83.0%.", "labels": [], "entities": [{"text": "knowledge base KB", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.8084946672121683}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996850490570068}]}, {"text": "This is higher than what ppdb and wn achieved without eclassif.", "labels": [], "entities": []}, {"text": "Adding handcoded still improves the accuracy somewhat.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995050430297852}]}, {"text": "Adding multiparse improves accuracy, but interestingly, not as much as in the previous experiment (without eclassif).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9993703961372375}]}, {"text": "The improvement on the test set decreases from 1.6% to 0.7%.", "labels": [], "entities": []}, {"text": "Therefore, the rules in eclassif help reduce the impact of misparses.", "labels": [], "entities": []}, {"text": "Here is an example to show how: T: An ogre is jumping over a wall, H: An ogre is jumping over the fence which in logic are:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Features used in the Phrasal Entailment Classifier, along with types and counts.", "labels": [], "entities": [{"text": "Phrasal Entailment Classifier", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.7019059062004089}]}, {"text": " Table 3  Cross-validation accuracy on entailment on all rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9885721206665039}]}, {"text": " Table 4  Cross-validation accuracy on entailment on lexical rules only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9929156303405762}]}, {"text": " Table 5  Cross-validation accuracy on entailment on phrasal rules only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9843395948410034}]}, {"text": " Table 6  Cross-validation accuracy on entailment on lexical rules for Asym evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9919424057006836}, {"text": "Asym evaluation", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.8794420957565308}]}, {"text": " Table 7  Ablation experiment for the system components without eclassif.", "labels": [], "entities": []}, {"text": " Table 8  Ablation experiment for the system components with eclassif, and the best performing  configuration.", "labels": [], "entities": []}]}