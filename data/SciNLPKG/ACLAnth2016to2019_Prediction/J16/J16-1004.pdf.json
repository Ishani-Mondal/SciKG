{"title": [{"text": "Online Learning for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8551115393638611}]}], "abstractContent": [{"text": "We present online learning techniques for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.8339401582876841}]}, {"text": "The availability of large training data sets that grow constantly overtime is becoming more and more frequent in the field of SMT-for example, in the context of translation agencies or the daily translation of government proceedings.", "labels": [], "entities": [{"text": "SMT-for", "start_pos": 126, "end_pos": 133, "type": "TASK", "confidence": 0.9955787062644958}, {"text": "translation agencies", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.8960435390472412}, {"text": "translation of government proceedings", "start_pos": 195, "end_pos": 232, "type": "TASK", "confidence": 0.8125243484973907}]}, {"text": "When new knowledge is to be incorporated in the SMT models, the use of batch learning techniques require very time-consuming estimation processes over the whole training set that may take days or weeks to be executed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9934337139129639}]}, {"text": "By means of the application of online learning, new training samples can be processed individually in real time.", "labels": [], "entities": []}, {"text": "For this purpose, we define a state-of-the-art SMT model composed of a set of submodels, as well as a set of incremental update rules for each of these submodels.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9916519522666931}]}, {"text": "To test our techniques, we have studied two well-known SMT applications that can be used in translation agencies: post-editing and interactive machine translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.991733193397522}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7209669649600983}]}, {"text": "In both scenarios, the SMT system collaborates with the user to generate high-quality translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9835101366043091}]}, {"text": "These user-validated translations can be used to extend the SMT models by means of online learning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9908074736595154}]}, {"text": "Empirical results in the two scenarios under consideration show the great impact of frequent updates in the system performance.", "labels": [], "entities": []}, {"text": "The time cost of such updates was also measured, comparing the efficiency of a batch learning SMT system with that of an online learning system, showing that online learning is able to work in real time whereas the time cost of batch retraining soon becomes infeasible.", "labels": [], "entities": [{"text": "batch learning SMT", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.4859041968981425}]}, {"text": "Empirical results also showed that the performance of online learning is comparable to that of batch learning.", "labels": [], "entities": []}, {"text": "Moreover, the proposed techniques were able to learn from previously estimated models or from scratch.", "labels": [], "entities": []}, {"text": "We also propose two new measures to predict the effectiveness of online learning in SMT tasks.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 84, "end_pos": 93, "type": "TASK", "confidence": 0.9421132802963257}]}, {"text": "The translation system with online learning capabilities presented here is implemented in the open-source Thot toolkit for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.978213906288147}]}], "introductionContent": [{"text": "Multiplicity of languages is inherent to modern society.", "labels": [], "entities": []}, {"text": "Phenomena such as globalization and technological development have extraordinarily increased the need for translating information from one language to another.", "labels": [], "entities": []}, {"text": "One possibility to deal with this growing demand of translations is the use of machine translation (MT) techniques.", "labels": [], "entities": [{"text": "translations", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.9713371992111206}, {"text": "machine translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.841455090045929}]}, {"text": "MT can be formalized under a statistical point of view as the process of finding the sentence of maximum probability in the target language given the source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9859370589256287}]}, {"text": "Statistical MT (SMT) requires the availability of parallel texts to estimate the statistical models involved in the translation.", "labels": [], "entities": [{"text": "Statistical MT (SMT)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8374312996864319}]}, {"text": "It is also important that such parallel texts belong to the same domain the system will be used for.", "labels": [], "entities": []}, {"text": "These kinds of texts are referred to as indomain corpora in the domain adaptation literature.", "labels": [], "entities": []}, {"text": "However, in-domain corpora are often not available in real translation scenarios, forcing us to estimate the system models by means of large out-of-domain texts, such as Parliament proceedings.", "labels": [], "entities": []}, {"text": "Unfortunately, this results in a significant degradation in the translation quality ().", "labels": [], "entities": []}, {"text": "There are many real translation scenarios in which new training data is inherently generated overtime (e.g., translation agencies or the daily translation of government proceedings).", "labels": [], "entities": [{"text": "translation agencies", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.8899917006492615}]}, {"text": "The newly generated training data could be used to mitigate the problem of data scarcity.", "labels": [], "entities": []}, {"text": "However, this situation poses new challenges in the SMT framework, because the vast majority of the SMT systems described in the literature makes use of the well-known batch learning paradigm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9946101903915405}, {"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.984477162361145}]}, {"text": "In the batch learning paradigm, the training of the SMT system and the translation process are carried out in separate stages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9938163757324219}]}, {"text": "This implies that all training samples must be available before training takes place, preventing the statistical models to be extended when the system starts generating translations.", "labels": [], "entities": []}, {"text": "To solve this problem, the online learning paradigm can be applied.", "labels": [], "entities": []}, {"text": "Online learning is a machine learning task that is structured in a series of trials, where each trial has four steps: (1) the learning algorithm receives an instance, (2) a label for the instance is predicted, (3) the true label for the instance is presented, and (4) the learning algorithm uses the true label to update its parameters.", "labels": [], "entities": [{"text": "Online learning", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7558512091636658}]}, {"text": "In this paradigm, the training and prediction stages are no longer separated.", "labels": [], "entities": [{"text": "prediction", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.8845031261444092}]}, {"text": "Online learning fits nicely in typical computer-assisted translation (CAT) applications used in translation agencies.", "labels": [], "entities": [{"text": "computer-assisted translation (CAT)", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8382947206497192}]}, {"text": "This is because in such applications the system translation for each source sentence is validated by a human expert and thus can be used to produce new training pairs.", "labels": [], "entities": []}, {"text": "One possible CAT implementation consists of post-editing (PE) the output of an MT system.", "labels": [], "entities": [{"text": "CAT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9702641367912292}, {"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9371433258056641}]}, {"text": "In this implementation, the MT system generates an initial translation that is corrected by the user without further system intervention.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9671561121940613}]}, {"text": "Another instance of CAT is interactive machine translation (IMT), where the user generates each translation in a series of interactions with the system.", "labels": [], "entities": [{"text": "CAT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9792163968086243}, {"text": "machine translation (IMT)", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8542823016643524}]}, {"text": "Scientific and commercial interest in CAT applications has greatly increased during recent years, capturing the attention of internationally renowned research groups and translation companies.", "labels": [], "entities": [{"text": "CAT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.987505316734314}]}, {"text": "A good example of this is the work carried out in the TransType (Foster, Isabelle, and Plamondon 1997) and TransType-II) research projects, where the IMT paradigm was developed, and the CasMaCat () and MateCat () projects, where a substantial part of the effort was focused on developing adaptive learning techniques for CAT.", "labels": [], "entities": [{"text": "CAT", "start_pos": 321, "end_pos": 324, "type": "TASK", "confidence": 0.9532355070114136}]}, {"text": "Literature also offers demonstrations of CAT applications) as well as studies involving real users showing the potential benefits of CAT).", "labels": [], "entities": []}, {"text": "In this work we propose online learning techniques for SMT useful to efficiently update the statistical models used by the system, avoiding the necessity of executing costly retraining processes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9955765008926392}]}, {"text": "The properties of the proposed techniques will be tested in the two CAT scenarios we have mentioned.", "labels": [], "entities": [{"text": "CAT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.979121744632721}]}, {"text": "As noted earlier, in such scenarios there is a human translator that supervises each system translation.", "labels": [], "entities": []}, {"text": "However, it is important to remark that our proposed techniques can also work in scenarios where there are no human experts involved.", "labels": [], "entities": []}, {"text": "One example of this can be found in fully automatic translation tasks where the initial models can be extended from new blocks of training data obtained from different sources, such as parliamentary proceedings.", "labels": [], "entities": []}, {"text": "The rest of the article is organized as follows: Section 2 describes the statistical foundations of SMT and its adaptation to the PE and IMT scenarios.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9967846870422363}, {"text": "PE and IMT", "start_pos": 130, "end_pos": 140, "type": "TASK", "confidence": 0.5509831309318542}]}, {"text": "Section 3 explains the online learning techniques proposed here, including the definition of a log-linear SMT model as well as a set of incremental update rules for each one of its components.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9871727228164673}]}, {"text": "The content of Section 3 is complemented by Appendix A, which presents an alternative incremental update rule for word-alignment models.", "labels": [], "entities": [{"text": "Appendix A", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9702507853507996}]}, {"text": "Experimental results as well as their discussion are shown in Sections 4 and 5, respectively.", "labels": [], "entities": []}, {"text": "Section 6 describes related work on online learning.", "labels": [], "entities": []}, {"text": "The work conclusions are given in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments that we carried out to test our proposed online learning techniques.", "labels": [], "entities": []}, {"text": "Our experiments were focused on the PE and IMT scenarios, because they fit nicely into the online learning paradigm.", "labels": [], "entities": [{"text": "PE and IMT", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.6113741596539816}]}, {"text": "Our experiments use the log-linear SMT model with online learning capabilities described in Section 3.4.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9891616106033325}]}, {"text": "The IMT experiments reported here combine this log-linear SMT model with stochastic error correction models following the technique introduced in.", "labels": [], "entities": [{"text": "IMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.927134096622467}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9925075173377991}]}, {"text": "This technique uses word graphs to avoid retranslating the source sentence at each interaction of the IMT process.", "labels": [], "entities": [{"text": "IMT process", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8860583007335663}]}, {"text": "The incremental language and phrase-based models involved in the interactive translation process were generated and accessed by means of the open source Thot toolkit.", "labels": [], "entities": []}, {"text": "The specific functionality used in this experimentation has been made freely available in anew version of toolkit.", "labels": [], "entities": []}, {"text": "We did not consider the use of the Moses decoder () in our experiments because it is not prepared to work in the IMT framework and it does not implement the incremental version of the EM algorithm (it implements the stepwise version, which is unstable in online learning settings, according to).", "labels": [], "entities": []}, {"text": "However, translation quality results reported in show that Thot is competitive with Moses for corpora of different complexities.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9654176831245422}]}, {"text": "We evaluated our techniques by simulating real users.", "labels": [], "entities": []}, {"text": "Because the different corpora used in the experiments contained source and target translations, we used the latter to simulate the reference translations that the user has in mind for each source sentence.", "labels": [], "entities": []}, {"text": "This paper studies the application of the online learning paradigm to SMT; therefore the experimentation follows the learning process structured as a sequence of trials that was described in Section 3.1.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9948586225509644}]}, {"text": "During this sequence of trials, online learning systems experience some sort of learning curve as they gain knowledge after each training sample presentation.", "labels": [], "entities": []}, {"text": "Given that such a learning curve is an important issue when designing online learning algorithms, some of the results reported here include plots with the evolution of cumulative error measures.", "labels": [], "entities": []}, {"text": "It is also interesting to clarify the way in which the different corpora described in Section 4.1 have been used throughout the experimentation.", "labels": [], "entities": []}, {"text": "One factor that has influenced the decisions in this regard is the high computational cost of batch retraining.", "labels": [], "entities": [{"text": "batch retraining", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7962069511413574}]}, {"text": "Batch retraining is present in different experiments reported in this work because it provides a valuable reference when assessing the performance of online learning.", "labels": [], "entities": [{"text": "Batch retraining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7874389588832855}]}, {"text": "In such experiments, we have defined specific subsets of the training corpora in order to speedup the experiments.", "labels": [], "entities": []}, {"text": "More specifically, the first 10,000 sentences of the XRCE and Europarl corpora have been used.", "labels": [], "entities": [{"text": "XRCE", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.940144956111908}, {"text": "Europarl corpora", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9122924506664276}]}, {"text": "The decisions regarding the use of the different corpora in the experiments can be summarized as follows.", "labels": [], "entities": []}, {"text": "The training sets of the XRCE and Europarl corpora were used to measure the convergence properties of the incremental EM algorithm (Section 4.4).", "labels": [], "entities": [{"text": "XRCE", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9292113780975342}, {"text": "Europarl corpora", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9648449420928955}]}, {"text": "The above-mentioned subset of the training corpora was used to study the impact of the update frequency in the results (Section 4.5), to compare the performance of batch and online learning, and to analyze the influence of sentence ordering in the system performance (Section 4.7).", "labels": [], "entities": []}, {"text": "Finally, in the experiments to test the capability of our online learning techniques to learn from previously estimated models (Section 4.8), we used the training and development sets of the XRCE and Europarl corpora to initialize the system models, and the test sets to obtain translation results.", "labels": [], "entities": [{"text": "XRCE", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.937371015548706}, {"text": "Europarl corpora", "start_pos": 200, "end_pos": 216, "type": "DATASET", "confidence": 0.9138359725475311}]}, {"text": "For the system trained with the Europarl corpus, the experimentation is complemented with translation results using the in-domain EMEA corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9937305748462677}, {"text": "EMEA corpus", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9133503437042236}]}, {"text": "The standard estimation procedure for current phrase-based models relies on the generation of word alignment matrices.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.6795858144760132}]}, {"text": "As it was explained in Section 3.5, in our proposal such alignment matrices are generated by means of HMM-based word alignment models that are incrementally updated from user feedback.", "labels": [], "entities": [{"text": "HMM-based word alignment", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.6253477136294047}]}, {"text": "For this purpose, we need to replace the batch EM algorithm by the incremental EM algorithm.", "labels": [], "entities": []}, {"text": "Given the great importance of generating word alignments in the estimation of phrase-based models (see Section 3.5.3), we carried out experiments to compare the convergence rates of batch and incremental EM algorithms for HMM-based word alignment models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7137083560228348}, {"text": "HMM-based word alignment", "start_pos": 222, "end_pos": 246, "type": "TASK", "confidence": 0.766851544380188}]}, {"text": "shows the normalized log-likelihood that is obtained when executing up to five training epochs 6 of the batch and incremental versions of the EM algorithm (common training schemes in state-of-the-art SMT systems frequently execute five EM training epochs to train the different word-alignment models).", "labels": [], "entities": [{"text": "SMT", "start_pos": 200, "end_pos": 203, "type": "TASK", "confidence": 0.9743949174880981}]}, {"text": "Plots were obtained for the XRCE and Europarl training corpora and the three translation directions (from English to Spanish, French, and German).", "labels": [], "entities": [{"text": "XRCE", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.787446141242981}, {"text": "Europarl training corpora", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.93714572985967}]}, {"text": "However, in the figure only the XRCE English to Spanish) and the Europarl English to Spanish) results are reported 7 (very similar results were obtained for the other language pairs).", "labels": [], "entities": [{"text": "XRCE English to Spanish", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.7727690041065216}, {"text": "Europarl English to Spanish", "start_pos": 65, "end_pos": 92, "type": "DATASET", "confidence": 0.9306950569152832}]}, {"text": "According to the results presented in, the incremental EM algorithm is able to obtain a greater normalized log-likelihood than that obtained by the batch EM algorithm for the two corpora under consideration.", "labels": [], "entities": []}, {"text": "In addition to this, such a greater log-likelihood can be obtained with fewer EM training epochs.", "labels": [], "entities": []}, {"text": "These observed results are due to the fact that the incremental EM algorithm executes complete E and M steps for each training sample, resulting in a much greater rate of model updates per each training epoch.", "labels": [], "entities": []}, {"text": "Note that, according to Equation, only one training epoch of the incremental EM algorithm is performed when training HMM-based alignment models (i.e., each training sample is processed only once by the learning algorithm and discarded afterwards).", "labels": [], "entities": []}, {"text": "This contrasts with the conventional batch training scheme, in which a few training epochs (typically five) are executed.", "labels": [], "entities": []}, {"text": "Hence, to fairly compare batch learning with our proposed online learning strategy, we should observe the relationship between the normalized log-likelihood of the incremental EM algorithm at the first training epoch and that of the fifth training epoch of the batch algorithm.", "labels": [], "entities": []}, {"text": "According to the values shown in, we can appreciate a very small degradation in the log-likelihood (<1% for the Europarl corpus) or no degradation at all.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9935562014579773}]}, {"text": "Because the difference in the log-likelihood between batch and incremental EM algorithms is negligible, we consider that the update rule for HMM-based alignment models given by Equation (24) is able to obtain word-alignment models comparable to those that can be obtained using batch learning.", "labels": [], "entities": []}, {"text": "This claim will be supported with additional empirical evidence in Section 4.6.", "labels": [], "entities": []}, {"text": "Nevertheless, it is possible to take advantage of the better convergence properties of the incremental EM algorithm by slightly modifying the conditions imposed by the online learning framework adopted in this paper (see Section 3.1).", "labels": [], "entities": []}, {"text": "In such a framework, only the last sample presented so far to the learning algorithm can be used to modify the model parameters at each trial.", "labels": [], "entities": []}, {"text": "This constraint can be slightly relaxed, allowing us to define alternative update rules for the HMM alignment models that execute more than one EM algorithm iteration over each sample.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.7302571535110474}]}, {"text": "One example of such alternative update rules is described in Appendix A. Empirical results also given in the same appendix show that the obtained log-likelihood and the evaluation measures can be marginally improved with respect to the strict observation of the online learning framework.", "labels": [], "entities": []}, {"text": "Finally, it is worth pointing out that according to the results presented in, incremental EM could be suitable to replace batch EM in a batch-learning scenario.", "labels": [], "entities": []}, {"text": "However, one disadvantage of applying incremental EM to a batch-learning task is the necessity of storing the sufficient statistics for the whole data set: s 1 , s 2 , ..., s M . For large data sets, the sufficient statistics may not fit in memory.", "labels": [], "entities": []}, {"text": "Nevertheless, this information can be stored on disk and accessed efficiently, because the algorithm reads the data in a sequential manner.", "labels": [], "entities": []}, {"text": "By contrast, this disadvantage is totally removed when incremental EM is applied in an online learning scenario, since the sufficient statistics for each training sample are discarded at the end of each trial, or after a finite number of trials for the alternative update rule described in Appendix A.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  XRCE corpus statistics for three different language pairs.", "labels": [], "entities": [{"text": "XRCE corpus statistics", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7736559311548868}]}, {"text": " Table 2  Europarl corpus statistics for three different language pairs.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.981576532125473}]}, {"text": " Table 3  Statistics for a subset of the EMEA corpus selected for testing purposes. Figures are shown for  three different language pairs. RRR and UNF measures have been calculated using the Europarl  training set as the out-of-domain corpus.", "labels": [], "entities": [{"text": "EMEA corpus", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9406891167163849}, {"text": "RRR", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.946582555770874}, {"text": "UNF", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.8864030241966248}, {"text": "Europarl  training set", "start_pos": 191, "end_pos": 213, "type": "DATASET", "confidence": 0.981452743212382}]}, {"text": " Table 4  BLEU, WER, and KSMR results for the XRCE test corpora using conventional (batch learning  without retraining) and online SMT systems. Both systems used MERT to adjust log-linear  weights. The average online learning time (LT) in seconds is shown for the online system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994292855262756}, {"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9964300990104675}, {"text": "KSMR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9452159404754639}, {"text": "XRCE test corpora", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.8840479652086893}, {"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9514716863632202}, {"text": "MERT", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9783616662025452}, {"text": "online learning time (LT)", "start_pos": 210, "end_pos": 235, "type": "METRIC", "confidence": 0.7544435063997904}]}, {"text": " Table 5  KSMR results of the comparison of our system with online learning and three different  state-of-the-art IMT conventional systems. The experiments were executed on the XRCE corpora.  Best results are shown in bold.", "labels": [], "entities": [{"text": "XRCE corpora", "start_pos": 177, "end_pos": 189, "type": "DATASET", "confidence": 0.9775323867797852}]}, {"text": " Table 6  BLEU, WER, and KSMR results for the Europarl test corpora using conventional (batch learning  without retraining) and online SMT systems. Log-linear weights were adjusted via MERT. The  average online learning time (LT) in seconds is shown for the online system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993879795074463}, {"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9970550537109375}, {"text": "KSMR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9524337649345398}, {"text": "Europarl test corpora", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9866446256637573}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9577170014381409}, {"text": "MERT", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.8522326946258545}, {"text": "online learning time (LT)", "start_pos": 204, "end_pos": 229, "type": "METRIC", "confidence": 0.7558298806349436}]}, {"text": " Table 7  BLEU, WER, and KSMR for the EMEA test corpora using conventional (batch learning without  retraining), online, and online from scratch SMT systems, with models estimated by means of  the Europarl training corpora. Log-linear weights were trained via MERT. The average online  learning time (LT) in seconds is shown for the online system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994959831237793}, {"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.997591495513916}, {"text": "KSMR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9858507513999939}, {"text": "EMEA test corpora", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9331042567888895}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9389939904212952}, {"text": "Europarl training corpora", "start_pos": 197, "end_pos": 222, "type": "DATASET", "confidence": 0.9412891666094462}, {"text": "online  learning time (LT)", "start_pos": 278, "end_pos": 304, "type": "METRIC", "confidence": 0.7556999623775482}]}]}