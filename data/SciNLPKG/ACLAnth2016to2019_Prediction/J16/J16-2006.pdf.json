{"title": [{"text": "When the Whole Is Less Than the Sum of Its Parts: How Composition Affects PMI Values in Distributional Semantic Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantic models, deriving vector-based word representations from patterns of word usage in corpora, have many useful applications (Turney and Pantel 2010).", "labels": [], "entities": []}, {"text": "Recently, there has been interest in compositional distributional models, which derive vectors for phrases from representations of their constituent words (Mitchell and Lapata 2010).", "labels": [], "entities": []}, {"text": "Often, the values of distri-butional vectors are pointwise mutual information (PMI) scores obtained from raw co-occurrence counts.", "labels": [], "entities": []}, {"text": "In this article we study the relation between the PMI dimensions of a phrase vector and its components in order to gain insights into which operations an adequate composition model should perform.", "labels": [], "entities": []}, {"text": "We show mathematically that the difference between the PMI dimension of a phrase vector and the sum of PMIs in the corresponding dimensions of the phrase's parts is an independently interpretable value, namely, a quantification of the impact of the context associated with the relevant dimension on the phrase's internal cohesion, as also measured by PMI.", "labels": [], "entities": []}, {"text": "We then explore this quantity empirically, through an analysis of adjective-noun composition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dimensions of a word vector in distributional semantic models contain a function of the co-occurrence counts of the word with contexts of interest.", "labels": [], "entities": []}, {"text": "A popular and effective option (Bullinaria and Levy 2012) is to transform counts into pointwise mutual information (PMI) scores, which are given, for any word a and context c, by PMI(a, c) = log( P(a|c) P(a) ).", "labels": [], "entities": []}, {"text": "There are various proposals on deriving phrase representations * Center for Mind/Brain Sciences, University of Trento, Palazzo Fedrigotti, corso Bettini 31, 38068 Rovereto (TN), Italy.", "labels": [], "entities": []}, {"text": "E-mails: denis.paperno@unitn.it; marco.baroni@unitn.it.", "labels": [], "entities": []}, {"text": "1 PMI is also used to measure the tendency of two phrase constituents to be combined in a particular syntactic configuration (e.g., to assess the degree of lexicalization of the phrase).", "labels": [], "entities": []}, {"text": "We use PMI(ab) to refer by composing word vectors, ranging from simple, parameter-free vector addition to fully supervised deep-neural-network-based systems.", "labels": [], "entities": []}, {"text": "We focus hereon the models illustrated in; see for the original model references.", "labels": [], "entities": []}, {"text": "As an empirical test case, we consider adjective-noun composition.", "labels": [], "entities": [{"text": "adjective-noun composition", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.7170439958572388}]}], "datasetContent": [{"text": "We focus on adjective-noun (AN) phrases as a representative case.", "labels": [], "entities": []}, {"text": "We used 2.8 billion tokens comprising ukWaC, Wackypedia, and British National Corpus, 2 extracting the 12.6K ANs that occurred at least 1K times.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9874752163887024}, {"text": "Wackypedia", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9275069832801819}, {"text": "British National Corpus", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.9175128738085429}]}, {"text": "We collected sentence-internal co-occurrence counts with the 872 nouns 3 occurring at least 150K times in the corpus used as contexts.", "labels": [], "entities": []}, {"text": "PMI values were computed by standard maximum-likelihood estimation.", "labels": [], "entities": []}, {"text": "We separated a random subset of 6K ANs to train composition models.", "labels": [], "entities": []}, {"text": "We consider two versions of the corresponding constituent vectors as input to composition: plain PMI vectors (with zero co-occurrence rates conventionally converted to 0 instead of \u2212\u221e) and positive PMI (positive PMI) vectors (all non-positive PMI values converted to 0).", "labels": [], "entities": []}, {"text": "The latter transformation is common in the literature.", "labels": [], "entities": []}, {"text": "Model parameters were estimated using DISSECT, whose training objective is to approximate corpus-extracted phrase vectors, a criterion especially appropriate for our purposes.", "labels": [], "entities": [{"text": "DISSECT", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.7880752086639404}]}, {"text": "We report results based on the 1.8 million positive PMI dimensions of the 4.7K phrase vectors that were not used for training.", "labels": [], "entities": []}, {"text": "On average a phrase had non-zero co-occurrence with 84.8% of the context nouns, over half of which gave positive PMI values.", "labels": [], "entities": [{"text": "PMI", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9001818895339966}]}, {"text": "We focus on positive dimensions because negative association values are harder to interpret and noisier; furthermore, \u2212\u221e cases must beset to some arbitrary value, and most practical applications set all negative values to 0 anyway (PPMI).", "labels": [], "entities": []}, {"text": "We also repeated the experiments including negative observed values, with a similar pattern of results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Mean squared error of different models' predictions, trained on PMI (left) vs. PPMI vectors  (right).", "labels": [], "entities": [{"text": "Mean squared error", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9735101262728373}, {"text": "PMI", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.7104044556617737}]}]}