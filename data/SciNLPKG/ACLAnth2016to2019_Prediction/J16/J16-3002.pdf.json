{"title": [{"text": "Towards Accurate and Efficient Chinese Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Chinese Part-of-Speech Tagging", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.608964075644811}]}], "abstractContent": [{"text": "From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.7208788096904755}, {"text": "Chinese language processing", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.609445333480835}]}, {"text": "Paradigmatic lexical relations are explicitly captured byword clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger.", "labels": [], "entities": []}, {"text": "Syntagmatic lexical relations are implicitly captured by syntactic parsing in the constituency formalism, and are utilized via system combination.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7743017077445984}]}, {"text": "Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.9800795515378317}]}, {"text": "Our linguistically motivated, hybrid approaches yield a relative error reduction of 18% in total over state-of-the-art baselines.", "labels": [], "entities": []}, {"text": "Despite the effectiveness to boost accuracy, computationally expensive parsers make hybrid systems inappropriate for many realistic NLP applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9964105486869812}]}, {"text": "In this article, we are also concerned with improving tagging efficiency attest time.", "labels": [], "entities": [{"text": "tagging", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9673088788986206}]}, {"text": "In particular, we explore unlabeled data to transfer the predictive power of hybrid models to simple sequence models.", "labels": [], "entities": []}, {"text": "Specifically, hybrid systems are utilized to create large-scale pseudo training data for cheap models.", "labels": [], "entities": []}, {"text": "Experimental results illustrate that the re-compiled models not only achieve high accuracy with respect to per token classification, but also serve as a front-end to a parser well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.998765230178833}]}], "introductionContent": [{"text": "In grammar, a part-of-speech (POS) is a linguistic category of words, generally defined by the syntactic or morphological behavior of the word in question.", "labels": [], "entities": []}, {"text": "Automatically assigning POS tags to words plays an important role in parsing, word sense disambiguation, as well as many other NLP applications.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.974257230758667}, {"text": "word sense disambiguation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7306809226671854}]}, {"text": "Many successful tagging algorithms developed for English have been applied to many other languages as well.", "labels": [], "entities": []}, {"text": "In some cases, the methods work well without large modifications, such as for German.", "labels": [], "entities": []}, {"text": "But a number of augmentations and changes become necessary when dealing with highly inflected or agglutinative languages, as well as analytic languages, of which Chinese is the focus of this article.", "labels": [], "entities": []}, {"text": "The Chinese language is characterized by the lack of formal parser to implicitly capture syntagmatic relations and propose a simple yet effective stacking model to combine the complementary strengths of sequential taggers and parsers.", "labels": [], "entities": []}, {"text": "We conduct experiments on the CTB and Chinese Gigaword.", "labels": [], "entities": [{"text": "CTB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.972009003162384}, {"text": "Chinese Gigaword", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9007603526115417}]}, {"text": "We implement a discriminative sequential classification model for POS tagging that achieves state-of-the-art accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8232439458370209}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9966765642166138}]}, {"text": "Experiments show that this model is significantly improved byword cluster features inaccuracy across a wide range of conditions.", "labels": [], "entities": []}, {"text": "This confirms the importance of the paradigmatic relations.", "labels": [], "entities": []}, {"text": "We then present a comparative study of our tagger and a constituency parser and a dependency parser, and show that the combination of heterogeneous models can significantly improve tagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.891669750213623}]}, {"text": "Our experiments show that stacking is a very effective method to combine the complementary strengths of heterogeneous models.", "labels": [], "entities": [{"text": "stacking", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9823338985443115}]}, {"text": "This demonstrates the importance of the syntagmatic relations.", "labels": [], "entities": []}, {"text": "Cluster-based features and the stacking model result in a relative error reduction of 18% in terms of the word classification accuracy.", "labels": [], "entities": [{"text": "stacking", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9569756388664246}, {"text": "error", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9117791652679443}, {"text": "word classification", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7190340310335159}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.6962482929229736}]}, {"text": "Although predictive powers of hybrid systems are significantly better than individual systems, they are not suitable for large-scale real word applications that have stringent time requirements.", "labels": [], "entities": []}, {"text": "The best performing model is slow and large, and fast and compact models are less accurate, because either they are not expressive enough or they overfit to the limited training data.", "labels": [], "entities": []}, {"text": "To improve POS tagging efficiency without loss of accuracy, we explore unlabeled data to transfer the predictive power of complex, inefficient models to simple, efficient models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9412414133548737}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9957723021507263}]}, {"text": "Specifically, hybrid systems are utilized to create large-scale pseudo training data for cheap sequence models.", "labels": [], "entities": []}, {"text": "For the SR-HMM tagger, pseudo training data are able to estimate finer-grained latent variables, and for the discriminative tagger, tagging accuracy can be improved by extending the context for feature extraction.", "labels": [], "entities": [{"text": "SR-HMM tagger", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.6517422646284103}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9497108459472656}, {"text": "feature extraction", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7191373109817505}]}, {"text": "Experiments on the CTB and Gigaword demonstrate that unlabeled data are effective to transfer the predictive power of hybrid models to simple models, including both latent variable generative models and global linear classifiers.", "labels": [], "entities": [{"text": "CTB", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9809102416038513}, {"text": "Gigaword", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.8499077558517456}]}, {"text": "On one hand, the precision in terms of word classification is improved to 95.34%, which is equivalent to the parser-integrated hybrid model.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995121955871582}, {"text": "word classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7444625794887543}]}, {"text": "On the other hand, re-compiled models are adapted based on parsing results, and as a result the ability to capture syntagmatic lexical relations is improved, too.", "labels": [], "entities": []}, {"text": "Different from the purely supervised sequence models, recompiled models also serve as a front-end to a parser well.", "labels": [], "entities": []}, {"text": "Our study has been partially published in and.", "labels": [], "entities": []}, {"text": "For this iteration, we re-implement all models, and therefore experimental results are not exactly the same.", "labels": [], "entities": []}, {"text": "We also release our implementation for research purposes.", "labels": [], "entities": []}, {"text": "The related resources can be downloaded at www.icst.pku.edu.cn/ lcwm/lexer.", "labels": [], "entities": []}], "datasetContent": [{"text": "5.5.1 Reducing Hybrid Models to SR-HMMs.", "labels": [], "entities": []}, {"text": "With the increase of (pseudo) training data, a SR-HMM may learn better latent variables to subcategorize POS tags, which could significantly improve a purely supervised SR-HMM.", "labels": [], "entities": []}, {"text": "In our experiments, SR-HMM models are trained with six, seven, and eight iterations of split, merge, smooth.", "labels": [], "entities": []}, {"text": "shows the performance of the re-trained SR-HMMs.", "labels": [], "entities": []}, {"text": "The first column is the number of sentences of pseudo sentences, and the second column lists the number of words.", "labels": [], "entities": []}, {"text": "The pseudo sentences are selected from the Xinhua news section of the Chinese Gigaword.", "labels": [], "entities": [{"text": "Xinhua news section of the Chinese Gigaword", "start_pos": 43, "end_pos": 86, "type": "DATASET", "confidence": 0.8573649781090873}]}, {"text": "We can clearly see that the idea to leverage unlabeled data to transfer the predictive Tagging accuracies (%) of re-compiled SR-HMM models on the development data.", "labels": [], "entities": [{"text": "predictive Tagging", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.5746093392372131}]}, {"text": "\"I-x\" denotes the number (x) of split-merge-smooth iterations for training.", "labels": [], "entities": []}, {"text": "Bold identifies best performance results.", "labels": [], "entities": []}, {"text": "ability of the hybrid model works.", "labels": [], "entities": []}, {"text": "Self-training can also slightly improve a SR-HMM.", "labels": [], "entities": []}, {"text": "Our auxiliary experiments show that self-training is not as effective as our structure compilation method.", "labels": [], "entities": [{"text": "structure compilation", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7227154821157455}]}, {"text": "With the increase of training iterations, finer-grained latent variables are estimated and they can enhance tagging.", "labels": [], "entities": [{"text": "tagging", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.9611061215400696}]}, {"text": "Note that the training procedure on the purely supervised setting obtains the best tagging results at iteration 6.", "labels": [], "entities": []}, {"text": "More training data, even if it is not perfect, can improve the generative learning process.", "labels": [], "entities": [{"text": "generative learning", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.9624399244785309}]}, {"text": "The table also presents the performance with respect to DEC/DEG disambiguation.", "labels": [], "entities": [{"text": "DEC/DEG disambiguation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.6352938562631607}]}, {"text": "The results suggest that finer-grained latent variables lead to better long-range disambiguation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Training, development, and test data on CTB 6.0.", "labels": [], "entities": [{"text": "CTB 6.0", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.953447699546814}]}, {"text": " Table 2  Tagging accuracies on the development data. LGLM 1 and LGLM 2 denote first-and second-order  global linear model respectively.", "labels": [], "entities": []}, {"text": " Table 3  Tagging accuracies (%) relative to word frequency.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9761124849319458}, {"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.5771836042404175}]}, {"text": " Table 4  Tagging accuracies (%) relative to length. The length is defined as one plus the number of words  that are dominated by the target word.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9566222429275513}, {"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.4424353241920471}]}, {"text": " Table 5  Tagging F1 scores relative to POS types.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9779428839683533}, {"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.8762125372886658}]}, {"text": " Table 6  Tagging accuracies (%) with different feature configurations.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9754534363746643}]}, {"text": " Table 7  Tagging accuracies (%) relative to sizes of training data. Size = number of sentences in the  labeled training corpus. Bold identifies best performance at the given size.", "labels": [], "entities": []}, {"text": " Table 9  The tagging recall (%) of OOV words.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9200180768966675}]}, {"text": " Table 10  Tagging F1 scores of relative to word classes.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9612173438072205}, {"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8147774338722229}]}, {"text": " Table 11  Tagging accuracies (%) of the IV and OOV words.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9568467140197754}, {"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.5968453884124756}]}, {"text": " Table 12  Parsing accuracies (%) on the development data.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.832039475440979}, {"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.5205813646316528}]}, {"text": " Table 13  Tagging accuracies (%) of different stacking models on the development data.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9334208965301514}, {"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.4740109145641327}]}, {"text": " Table 14  F1 score of the DEC/DEG prediction and parsing performance of different stacking models on  the development data.  DEC DEG  LP  LR  LF", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9994848966598511}, {"text": "DEC/DEG prediction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6711024194955826}, {"text": "DEC DEG  LP  LR  LF", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.9116491317749024}]}, {"text": " Table 15  Tagging accuracies (%) of re-compiled SR-HMM models on the development data. \"I-x\" denotes  the number (x) of split-merge-smooth iterations for training. Bold identifies best performance  results.", "labels": [], "entities": []}, {"text": " Table 16  Tagging accuracies (%) of re-compiled LGLM 1 and LGLM 2 models on the development data.  The beam size is set to 4. \"win=x\" denotes the window size (x) of word uni-/bigrams for feature  extraction. Bold identifies best performance results.", "labels": [], "entities": [{"text": "feature  extraction", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7343518137931824}]}, {"text": " Table 17  Tagging accuracies (%) relative to beam width on the development data. The LGLM 2 model is  applied.", "labels": [], "entities": []}, {"text": " Table 20  Tagging accuracies (%) on the test data.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9597389698028564}, {"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.6610969305038452}]}]}