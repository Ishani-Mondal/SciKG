{"title": [{"text": "Optimization for Statistical Machine Translation: A Survey", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.863438069820404}]}], "abstractContent": [{"text": "In statistical machine translation (SMT), the optimization of the system parameters to maximize translation accuracy is now a fundamental part of virtually all modern systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8330481847127279}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9525209069252014}]}, {"text": "In this article, we survey 12 years of research on optimization for SMT, from the seminal work on discriminative models (Och and Ney 2002) and minimum error rate training (Och 2003), to the most recent advances.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9935846924781799}]}, {"text": "Starting with a brief introduction to the fundamentals of SMT systems, we follow by covering a wide variety of optimization algorithms for use in both batch and online optimization.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9969167709350586}]}, {"text": "Specifically, we discuss losses based on direct error minimization, maximum likelihood, maximum margin, risk minimization, ranking, and more, along with the appropriate methods for minimizing these losses.", "labels": [], "entities": [{"text": "direct error minimization", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6053281227747599}, {"text": "maximum margin", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.61469766497612}, {"text": "risk minimization", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7553596198558807}]}, {"text": "We also cover recent topics, including large-scale optimization, non-linear models, domain-dependent optimization, and the effect of MT evaluation measures or search on optimization.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.8980990946292877}]}, {"text": "Finally, we discuss the current state of affairs in MT optimization, and point out some unresolved problems that will likely be the target of further research in optimization for MT.", "labels": [], "entities": [{"text": "MT optimization", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.9947081506252289}, {"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.9895007014274597}]}], "introductionContent": [{"text": "Machine translation (MT) has long been both one of the most promising applications of natural language processing technology and one of the most elusive.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9153421878814697}]}, {"text": "However, over approximately the past decade, huge gains in translation accuracy have been achieved (), and commercial systems deployed for hundreds of language pairs are being used by hundreds of millions of users.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9560911655426025}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8530802726745605}]}, {"text": "There are many reasons for these advances in the accuracy and coverage of MT, but among them two particularly stand out: statistical machine translation (SMT) techniques that make it possible to learn statistical models from data, and massive increases in the amount of data available to learn SMT models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9990098476409912}, {"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9543834924697876}, {"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9672667384147644}, {"text": "statistical machine translation (SMT)", "start_pos": 121, "end_pos": 158, "type": "TASK", "confidence": 0.835315614938736}, {"text": "SMT models", "start_pos": 294, "end_pos": 304, "type": "TASK", "confidence": 0.9039713442325592}]}, {"text": "* 8916-5 Takayama-cho, Ikoma, Nara, Japan.", "labels": [], "entities": [{"text": "* 8916-5 Takayama-cho, Ikoma, Nara", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.7801705045359475}]}, {"text": "E-mail: neubig@is.naist.jp.", "labels": [], "entities": []}, {"text": "* * 6-10-1 Roppongi, Minato-ku, Tokyo, Japan.", "labels": [], "entities": [{"text": "Roppongi, Minato-ku, Tokyo", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.8680542349815369}]}, {"text": "E-mail: tarow@google.com.", "labels": [], "entities": []}, {"text": "This work was mostly done while the second author was affiliated with the National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan.", "labels": [], "entities": [{"text": "National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289", "start_pos": 74, "end_pos": 192, "type": "DATASET", "confidence": 0.7629519634776645}]}, {"text": "Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9928215146064758}]}, {"text": "The first was the pioneering work of, who proposed the idea of SMT, and described methods for estimation of the parameters used in translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9948630928993225}]}, {"text": "In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus.", "labels": [], "entities": [{"text": "word-based generative translation", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6332732141017914}]}, {"text": "The second major advance in SMT is the discriminative training framework proposed by and , who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9957998394966125}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9904865622520447}, {"text": "accuracy", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.9946738481521606}]}, {"text": "By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9248586893081665}, {"text": "MT", "start_pos": 191, "end_pos": 193, "type": "TASK", "confidence": 0.9707895517349243}, {"text": "POS tagging", "start_pos": 250, "end_pos": 261, "type": "TASK", "confidence": 0.8378869593143463}, {"text": "parsing", "start_pos": 265, "end_pos": 272, "type": "TASK", "confidence": 0.8785468935966492}]}, {"text": "However, within the general framework of structured prediction, MT stands apart in many ways, and as a result requires a number of unique design decisions not necessary in other frameworks (as summarized in).", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7028732746839523}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.989863932132721}]}, {"text": "The first is the search space that must be considered.", "labels": [], "entities": []}, {"text": "The search space in MT is generally too large to expand exhaustively, so it is necessary to decide which subset of all the possible hypotheses should be used in optimization.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9515823125839233}]}, {"text": "In addition, the evaluation of MT accuracy is not straightforward, with automatic evaluation measures for MT still being researched to this day.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9873199462890625}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.970198392868042}, {"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9896031022071838}]}, {"text": "From the optimization perspective, even once we have chosen an automatic evaluation metric, it is not necessarily the case that it can be decomposed for straightforward integration with structured learning algorithms.", "labels": [], "entities": []}, {"text": "Given this evaluation measure, it is necessary to incorporate it into a loss function to target.", "labels": [], "entities": []}, {"text": "The loss function should be closely related to the final evaluation objective, while allowing for the use of efficient optimization algorithms.", "labels": [], "entities": []}, {"text": "Finally, it is necessary to choose an optimization algorithm.", "labels": [], "entities": []}, {"text": "In many cases it is possible to choose a standard algorithm from other fields, but there are also algorithms that have been tailored towards the unique challenges posed by MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 172, "end_pos": 174, "type": "TASK", "confidence": 0.9736883044242859}]}, {"text": "In this article, we survey the state of the art in machine translation optimization in a comprehensive and systematic fashion, covering a wide variety of topics, with a unified set of terminology.", "labels": [], "entities": [{"text": "machine translation optimization", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8529436389605204}]}, {"text": "In Section 2, we first provide definitions of the problem of machine translation, describe briefly how models are built, how features are defined, and how translations are evaluated, and finally define the optimization setting.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.814070463180542}]}, {"text": "In Section 3, we next describe a variety of loss functions that have been targeted in machine translation optimization.", "labels": [], "entities": [{"text": "machine translation optimization", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.8890274167060852}]}, {"text": "In Section 4, we explain the selection of oracle translations, a non-trivial process that directly affects the optimization results.", "labels": [], "entities": []}, {"text": "In Section 5, we describe batch optimization algorithms, starting with the popular minimum error rate training, and continuing with other approaches using likelihood, margin, rank loss, or risk as objectives.", "labels": [], "entities": [{"text": "batch optimization", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7207238972187042}, {"text": "likelihood", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9688870906829834}, {"text": "margin", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.8233065009117126}]}, {"text": "In Section 6, we describe online learning algorithms, first explaining the relationship between corpus-level optimization and sentence-level optimization, and then moving onto algorithms based on perceptron, margin, or likelihood-based objectives.", "labels": [], "entities": []}, {"text": "In Section 7, we describe the recent advances in scaling training of MT systems up to large amounts of data through parallel computing, and in Section 8, we cover a number of other topics in MT optimization such as non-linear models, domain adaptation, and the relationship between MT evaluation and optimization.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9568569660186768}, {"text": "MT optimization", "start_pos": 191, "end_pos": 206, "type": "TASK", "confidence": 0.9919294714927673}, {"text": "domain adaptation", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.7762308716773987}, {"text": "MT evaluation", "start_pos": 282, "end_pos": 295, "type": "TASK", "confidence": 0.9650419354438782}]}, {"text": "Finally, we conclude in Section 9, overviewing the methods described, making a brief note about which methods seethe most use in actual systems, and outlining some of the unsolved problems in the optimization of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.9770708084106445}]}], "datasetContent": [{"text": "Once we have a machine translation system that can produce translations, we next must perform evaluation to judge how good the generated translations actually are.", "labels": [], "entities": []}, {"text": "As the final consumer of machine translation output is usually a human, the most natural form of evaluation is manual evaluation by human annotators.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7836449940999349}]}, {"text": "However, because human evaluation is expensive and time-consuming, in recent years there has been a shift to automatic calculation of the quality of MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9914358258247375}]}, {"text": "In general, automatic evaluation measures use a set of data consisting of N input , each of which having a reference translation E = e (i) N i=1 that was created by a human translator.", "labels": [], "entities": []}, {"text": "The input F is automatically translated using a ma- , which are then compared to the corresponding references.", "labels": [], "entities": []}, {"text": "The closer the MT output is to the reference, the better it is deemed to be, according to automatic evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9632296562194824}]}, {"text": "In addition, as there are often many ways to translate a particular sentence, it is also possible to perform evaluation with multiple references created by different translators.", "labels": [], "entities": []}, {"text": "There has also been some work on encoding a huge number of references in a lattice, created either by hand (Dreyer and Marcu 2012) or by automatic paraphrasing.", "labels": [], "entities": []}, {"text": "One major distinction between optimization measures is whether they are calculated on the corpus level or the sentence level.", "labels": [], "entities": []}, {"text": "Corpus-level measures are calculated by taking statistics over the whole corpus, whereas sentence-level measures are calculated by measuring sentence-level accuracy, and defining the corpus-level accuracy as the average of the sentence-level accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.8526346683502197}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.8117621541023254}]}, {"text": "All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view.", "labels": [], "entities": []}, {"text": "The most commonly used MT evaluation measure BLEU () is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9071643650531769}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.6995405554771423}]}, {"text": "Of course, there have been many other evaluation measures proposed since BLEU, with TER () and METEOR (Banerjee and Lavie 2005) being among the most widely used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9901899695396423}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9980238676071167}, {"text": "METEOR", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9920104742050171}]}, {"text": "The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9956721067428589}]}, {"text": "We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU.", "labels": [], "entities": [{"text": "MT optimization", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.9957834780216217}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9956684112548828}]}, {"text": "BLEU is defined as the geometric mean of n-gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9883993864059448}, {"text": "precisions", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.8775618672370911}]}, {"text": "For a single reference sentence e and a corresponding system output\u00eaoutput\u02c6output\u00ea, we can define c n (\u02c6 e) as the number of n-grams in\u00eain\u02c6in\u00ea, and m n (e, \u02c6 e) as the number of n-grams in\u00eain\u02c6in\u00ea that match e c n (\u02c6 e) = |{g n \u2208 \u02c6 e}| Here, {g n \u2208 \u02c6 e} and {g n \u2208 e} are multisets that can contain identical n-grams more than once, and \u2229 is an operator for multisets that allows for consideration of multiple instances of the same n-gram.", "labels": [], "entities": []}, {"text": "3 Note that the total count fora candidate n-gram is clipped to be no more than the count in the reference translation.", "labels": [], "entities": []}, {"text": "If we have a corpus of reference sets R = {e (1) , . .", "labels": [], "entities": []}, {"text": ", e (N) }, where each sentence has M references e (i) = {e (i) 1 , . .", "labels": [], "entities": []}, {"text": ", e (i) M }, the BLEU score of the corresponding system outputs E = {\u00ea (1) , . .", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9816741049289703}]}, {"text": ", \u02c6 e (N) } can be defined as where the first term corresponds to geometric mean of the n-gram precisions, and the second term BP(E, \u02c6 E) is the brevity penalty.", "labels": [], "entities": [{"text": "BP", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.9942055344581604}]}, {"text": "The brevity penalty is necessary here because evaluation of precision favors systems that output only the words and phrases that have high accuracy, and avoids outputting more difficult-to-translate content that might not match the reference.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9967928528785706}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.955020546913147}]}, {"text": "The brevity penalty prevents this by discounting outputs that are shorter than the reference where\u02dcewhere\u02dc where\u02dce (i) is defined as the longest reference with a length shorter than or equal t\u00f4 e (i) . 2.5.2 BLEU+1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9981379508972168}]}, {"text": "One thing to notice here is that BLEU is calculated by taking statistics over the entire corpus, and thus it is a corpus-level measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988828301429749}]}, {"text": "There is nothing inherently preventing us from calculating BLEU on a single sentence, but in the single-sentence case it is common for the number of matches of higher order n-grams to become zero, resulting in a BLEU score of zero for the entire sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9899735450744629}, {"text": "BLEU score", "start_pos": 212, "end_pos": 222, "type": "METRIC", "confidence": 0.9848551452159882}]}, {"text": "One common solution to this problem is the use of a smoothed version of BLEU, commonly referred to as BLEU+1 ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9896392822265625}, {"text": "BLEU+1", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9415810108184814}]}, {"text": "In BLEU+1, we add one to the numerators and denominators of each n-gram of order greater than one where \u03b4(\u00b7) is a function that takes a value of 1 when the corresponding statement is true.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9891125559806824}]}, {"text": "We can then re-define a sentence-level BLEU using these smoothed counts and the corpus-level evaluation can be re-defined as the average of sentence level evaluations It has also been noted, however, that the average of sentence-level BLEU+1 is not a very accurate approximation of corpus-level BLEU, but by adjusting the smoothing heuristics it is possible to achieve a more accurate approximation (Nakov, Guzman, and Vogel 2012).", "labels": [], "entities": []}, {"text": "In the entirety of this article, we have assumed that optimization for MT aims to reduce MT error defined using an evaluation measure, generally BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9923704862594604}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9703279137611389}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9947898387908936}]}, {"text": "However, as mentioned in Section 2.5, evaluation of MT is an active research field, and there are many alternatives in addition to BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9174185991287231}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9953561425209045}]}, {"text": "Thus, it is of interest whether changing the measure used in optimization can affect the overall quality of the translations achieved, as measured by human evaluators.", "labels": [], "entities": []}, {"text": "There have been a few comprehensive studies on the effect of the metric used in optimization on human assessments of the generated translations).", "labels": [], "entities": []}, {"text": "These studies showed the rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9970917701721191}, {"text": "BLEU-optimized", "start_pos": 195, "end_pos": 209, "type": "METRIC", "confidence": 0.982267439365387}]}, {"text": "Since this result, however, there have been other reports stating that systems optimized using other metrics such as TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT () achieve superior results to BLEU-optimized systems.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.98634934425354}, {"text": "MEANT", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.9740279316902161}]}, {"text": "There have also been attempts to directly optimize not automatic, but human evaluation measures of translation quality.", "labels": [], "entities": []}, {"text": "However, the cost of performing this sort of human-in-the-loop optimization is prohibitive, so propose a method that re-uses partial hypotheses in evaluation.", "labels": [], "entities": []}, {"text": "also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide than generating new reference sentences.", "labels": [], "entities": []}, {"text": "Finally, there is also some work on optimizing multiple evaluation metrics atone time.", "labels": [], "entities": []}, {"text": "The easiest way to do so is to simply use the linear interpolation of two or more metrics as the error function (: where L is the number of error functions, and \u03c1 i is a manually set interpolation coefficient for its respective error function.", "labels": [], "entities": []}, {"text": "There are also more sophisticated methods based on the idea of optimizing towards Pareto-optimal hypotheses (), which achieve errors lower than all other hypotheses on at least one evaluation measure, pareto(E, E ) = { \u02c6 E \u2208 E : \u2200 E \u2208E \u2203 i error i (E, \u02c6 E) < error i (E, E )} (79) To incorporate this concept of Pareto optimality into optimization, the Pareto-optimal set is defined on the sentence level, and ranking loss (Section 3.5) is used to ensure that the Pareto-optimal hypotheses achieve a higher score than those that are not Pareto optimal.", "labels": [], "entities": [{"text": "pareto", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9800300002098083}]}, {"text": "This method has also been extended to take advantage of ensemble decoding, where multiple parameter settings are used simultaneously in decoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Percent of WMT systems using each optimization method, including all systems, or systems that  achieved the best results on at least one language pair.", "labels": [], "entities": [{"text": "WMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9012760519981384}]}]}