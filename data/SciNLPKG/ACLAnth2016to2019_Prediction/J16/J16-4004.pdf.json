{"title": [{"text": "RELPRON: A Relative Clause Evaluation Data Set for Compositional Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "This article introduces RELPRON, a large data set of subject and object relative clauses, for the evaluation of methods in compositional distributional semantics.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9829833507537842}]}, {"text": "RELPRON targets an intermediate level of grammatical complexity between content-word pairs and full sentences.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5493472218513489}]}, {"text": "The task involves matching terms, such as \"wisdom,\" with representative properties, such as \"quality that experience teaches.\"", "labels": [], "entities": []}, {"text": "A unique feature of RELPRON is that it is built from attested properties, but without the need for them to appear in relative clause format in the source corpus.", "labels": [], "entities": []}, {"text": "The article also presents some initial experiments on RELPRON, using a variety of composition methods including simple baselines, arithmetic operators on vectors, and finally, more complex methods in which argument-taking words are represented as tensors.", "labels": [], "entities": []}, {"text": "The latter methods are based on the Categorial framework, which is described in detail.", "labels": [], "entities": [{"text": "Categorial framework", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.7873351871967316}]}, {"text": "The results show that vector addition is difficult to beat-in line with the existing literature-but that an implementation of the Categorial framework based on the Practical Lexical Function model is able to match the performance of vector addition.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8605379164218903}, {"text": "vector addition", "start_pos": 233, "end_pos": 248, "type": "TASK", "confidence": 0.7170820385217667}]}, {"text": "The article finishes with an in-depth analysis of RELPRON, showing how results vary across subject and object relative clauses, across different head nouns, and how the methods perform on the subtasks necessary for capturing relative clause Computational Linguistics Volume 42, Number 4 semantics, as well as providing a qualitative analysis highlighting some of the more common errors.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.621475875377655}]}, {"text": "Our hope is that the competitive results presented here, in which the best systems are on average ranking one out of every two properties correctly fora given term, will inspire new approaches to the RELPRON ranking task and other tasks based on linguistically interesting constructions.", "labels": [], "entities": [{"text": "RELPRON ranking task", "start_pos": 200, "end_pos": 220, "type": "TASK", "confidence": 0.6407652298609415}]}], "introductionContent": [{"text": "The field of compositional distributional semantics ( integrates distributional semantic representations of words) with formal methods for composing word representations into larger phrases and sentences.", "labels": [], "entities": []}, {"text": "In recent years a number of composition methods have been proposed, including simple arithmetic operations on distributional word vectors, multi-linear operations involving higher-order representations of argument-taking words such as verbs and adjectives (, and composition of distributed word vectors learned with neural networks.", "labels": [], "entities": []}, {"text": "To compare such approaches it is important to have high-quality data sets for evaluating composed phrase representations at different levels of granularity and complexity.", "labels": [], "entities": []}, {"text": "Existing evaluation data sets fall largely into two categories.", "labels": [], "entities": []}, {"text": "Some data sets focus on two to three word phrases consisting of content words only (i.e., no closedclass function words), including subject-verb, verb-object, subject-verb-object, and adjective-noun combinations.", "labels": [], "entities": []}, {"text": "Such data sets have been essential for evaluating the first generation of compositional distributional models, but the relative grammatical simplicity of the phrases-in particular, the absence of function words-leaves open a wide range of compositional phenomena in natural language against which models must be evaluated.", "labels": [], "entities": []}, {"text": "Other data sets, including those used in recent SemEval and *SEM Shared Tasks (, focus on pairs of full sentences, some as long as 20 words or more.", "labels": [], "entities": []}, {"text": "The evaluation is based on a numeric similarity measure for each sentence pair.", "labels": [], "entities": []}, {"text": "These data sets represent a more realistic task for language technology applications, but reducing sentence similarity to a single value makes it difficult to identify how a model performs on subparts of a sentence, or on specific grammatical constructions, masking the areas where models need improvement.", "labels": [], "entities": []}, {"text": "In the domain of syntactic parsing, a call has been made for \"grammatical construction-focused\" parser evaluation), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7969014048576355}, {"text": "grammatical construction-focused\" parser evaluation", "start_pos": 62, "end_pos": 113, "type": "TASK", "confidence": 0.6840395390987396}, {"text": "accuracy", "start_pos": 249, "end_pos": 257, "type": "METRIC", "confidence": 0.9951450228691101}]}, {"text": "We make an analogous call here, fora wider range of compositional phenomena to be investigated in compositional distributional semantics in the near future.", "labels": [], "entities": []}, {"text": "This article begins to answer that call by presenting RELPRON, a data set of noun phrases consisting of a noun modified by a relative clause.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9830185174942017}]}, {"text": "For example, building that hosts premieres is a noun phrase containing a subject relative clause (a relative clause with an extracted subject), and in our data set describes a theater; while person that helicopter saves contains an object relative clause, and in our data set describes a survivor.", "labels": [], "entities": []}, {"text": "The RELPRON data set is primarily concerned with the analysis of a particular type of closed-class function word, namely, relative pronouns (that, in our examples); function words have traditionally been of greater concern than content words for formal semantics, and we address how this focus can be extended to distributional semantics.", "labels": [], "entities": [{"text": "RELPRON data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8164812922477722}]}, {"text": "Relative clauses, although still fairly short and therefore more manageable than full sentences, are nevertheless more grammatically complex than the short phrases in previous data sets, because of the relative pronoun and the long-distance relationship between the verb and the extracted argument-known as the head noun of the relative clause (building, person).", "labels": [], "entities": []}, {"text": "The aim of RELPRON is to expand the variety of phrase types on which compositional distributional semantic methods can be evaluated, thus helping to build these methods up step-by-step to the full task of compositional sentence representations.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.5467181205749512}, {"text": "compositional sentence representations", "start_pos": 205, "end_pos": 243, "type": "TASK", "confidence": 0.6462591886520386}]}, {"text": "RELPRON is a large (1,087 relative clauses), corpus-based, naturalistic data set, including both subject and object relative clauses that modify a variety of concrete and abstract nouns.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5671913027763367}]}, {"text": "The relative clauses are matched with terms and represent distinctive properties of those terms, such as wisdom: quality that experience teaches, and bowler: player that dominates batsman.", "labels": [], "entities": []}, {"text": "A unique feature of RELPRON is that it is built from attested properties of terms, but without the properties needing to appear in relative clause form in the source corpus.", "labels": [], "entities": []}, {"text": "This article also presents some initial experiments on RELPRON, using a variety of composition methods.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.5194222331047058}]}, {"text": "We find that a simple arithmetic vector operation provides a challenging baseline, inline with existing literature evaluating such methods, but we are able to match this baseline using a more sophisticated method similar to the Practical Lexical Function (PLF) model of.", "labels": [], "entities": []}, {"text": "We hope that the compositional methods presented here will inspire new approaches.", "labels": [], "entities": []}, {"text": "The remainder of the article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides some motivation for the RELPRON data set and the canonical ranking task that we imagine RELPRON being used for, as well as a description of existing data sets designed to test compositional distributional models.", "labels": [], "entities": [{"text": "RELPRON data set", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8032414416472117}]}, {"text": "Section 3 describes the data set itself, and provides a detailed description of how it was built.", "labels": [], "entities": []}, {"text": "Section 4 surveys some of the existing compositional methods that have been applied in distributional semantics, including a short historical section describing work from cognitive science in the 1980s and 1990s.", "labels": [], "entities": []}, {"text": "Section 4.2 provides a fairly detailed description of the Categorial framework, which provides the basis for the more complex composition methods that we have tested, as well as a description of existing implementations of the framework.", "labels": [], "entities": []}, {"text": "Section 5 provides further details of the composition methods, including lower-level details of how the vectors and tensors were built.", "labels": [], "entities": []}, {"text": "Section 6 presents our results on the development and test portions of RELPRON, comparing different composition methods and different methods of building the vectors and tensors (including count-based vectors vs. neural embeddings).", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.5904941558837891}]}, {"text": "Finally, Section 7 provides an in-depth analysis of showing how results vary across subject and object relative clauses, across the different head nouns, and how the methods perform on the subtasks necessary for capturing relative pronoun semantics, as well as providing a qualitative analysis highlighting some of the more common errors.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most existing evaluation data sets for compositional distributional semantics have been based on small, fixed syntactic contexts, typically adjective-noun or subject-verbobject.", "labels": [], "entities": []}, {"text": "introduce an evaluation that has been adopted by a number of other researchers.", "labels": [], "entities": []}, {"text": "The task is to predict human similarity judgments on subject-verb phrases, where the similarity rating depends on word sense disambiguation in context.", "labels": [], "entities": [{"text": "predict human similarity judgments", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.5907915830612183}]}, {"text": "For example, horse run is more similar to horse gallop than to horse dissolve, whereas colors run is more similar to colors dissolve than to colors gallop.", "labels": [], "entities": []}, {"text": "Compositional models are evaluated on how well their similarity judgments correlate with the human judgments.", "labels": [], "entities": []}, {"text": "introduce a phrase similarity data set consisting of adjective-noun, verb-object, and noun-noun combinations, where the task again is to produce similarity ratings that correlate well with human judgments.", "labels": [], "entities": []}, {"text": "For example, large number and great majority have high similarity in the gold-standard data, whereas further evidence and low cost have low similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9914303421974182}, {"text": "gold-standard data", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.879181832075119}, {"text": "similarity", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9593217968940735}]}, {"text": "This data set formed the basis of a shared task at the GEMS 2011 workshop.  and introduce analogous tasks for subject-verb-object triples.", "labels": [], "entities": [{"text": "GEMS 2011 workshop.", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.818605363368988}]}, {"text": "In the disambiguation task, people try door is more similar to people test door than to people judge door.", "labels": [], "entities": []}, {"text": "In the similarity task, medication achieve result and drug produce effect have high similarity ratings, whereas author write book and delegate buy land have low ratings.", "labels": [], "entities": [{"text": "similarity", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9711970686912537}]}, {"text": "Sentence pairs with mid-similarity ratings tend to have high relatedness but are not mutually substitutable, such as team win match and people play game.", "labels": [], "entities": []}, {"text": "Other evaluations have made use of the contrast between compositional and noncompositional phrases, based on the intuition that a successful compositional model should produce phrase vectors similar to the observed context vectors for compositional phrases, but diverge from the observed context vectors for phrases known to be non-compositional.", "labels": [], "entities": []}, {"text": "introduce an evaluation based on compound nouns like climate change (compositional) and gravy train (noncompositional).", "labels": [], "entities": []}, {"text": "evaluate models on a data set involving intersective adjectives in phrases like white towel (compositional) and non-intersective adjectives in phrases like black hole and false floor (non-compositional).", "labels": [], "entities": []}, {"text": "The data from the shared task of the ACL 2011 Distributional Semantics and Compositionality workshop (Biemann and Giesbrecht 2011) are also relevant in this context.", "labels": [], "entities": [{"text": "ACL 2011 Distributional Semantics and Compositionality workshop", "start_pos": 37, "end_pos": 100, "type": "TASK", "confidence": 0.6238398935113635}]}, {"text": "Also relevant for compositional distributional semantics are larger data sets for full sentence similarity and entailment, including those from recent SemEval and *SEM Shared Tasks ().", "labels": [], "entities": []}, {"text": "The SemEval Semantic Textual Similarity tasks make use of a number of paraphrase corpora that include text from news articles, headlines, tweets, image captions, video descriptions, student work, online discussion forums, and statistical machine translation.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8687779456377029}, {"text": "statistical machine translation", "start_pos": 226, "end_pos": 257, "type": "TASK", "confidence": 0.7020475467046102}]}, {"text": "These corpora include sentences of widely varying length and quality.", "labels": [], "entities": []}, {"text": "The SICK data set () is based on image and video description corpora, with named entities removed and other normalization steps applied, in order to remove phenomena outside the scope of compositional distributional semantic models.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8676691253980001}]}, {"text": "Additional sentences are added based on linguistic transformations that vary the meaning of the original sentence in a controlled fashion.", "labels": [], "entities": []}, {"text": "However, this data set is not designed for targeted evaluation of models on specific compositional linguistic phenomena, especially because many of the sentence pairs exhibit significant lexical overlap; for example, An airplane is in the air, An airplane is flying in the air.", "labels": [], "entities": []}, {"text": "Some recent data sets have extended the range of linguistic phenomena that maybe evaluated.", "labels": [], "entities": []}, {"text": "test the ability of compositional models to detect semantic similarity in the presence of lexical and syntactic variation, for example, among the sentences Pfizer buys Rinat, Pfizer paid several hundred million dollars for Rinat, and Pfizer's acquisition of Rinat.", "labels": [], "entities": []}, {"text": "introduce a data set comparing nouns to noun phrases, including determiners as function words; for example, duel is compared to two opponents (similar), various opponents (dissimilar), and two engineers (dissimilar).", "labels": [], "entities": []}, {"text": "introduce a data set involving full sentences from a limited grammar, where determiners and word order vary; for example, A man plays a guitar is compared to A guitar plays a man and The man plays a guitar.", "labels": [], "entities": []}, {"text": "This section describes the evaluation of the ranking task on RELPRON and presents the results of the various composition methods.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.4464642107486725}]}, {"text": "To evaluate a composition method, composed vectors are produced for all properties in the data set.", "labels": [], "entities": []}, {"text": "For each term, the properties are ranked according to cosine similarity with the term vector.", "labels": [], "entities": []}, {"text": "Evaluation is based on MAP, which is defined as: where N is the number of terms in the data set, and AP(t) is the Average Precision (AP) for term t, defined as: where Pt is the number of correct properties for term tin the data set, M is the total number of properties in the data set, Prec(k) is the precision at rank k, and rel(k) is an indicator function equal to one if the property at rank k is a correct property fort, and zero otherwise.", "labels": [], "entities": [{"text": "AP", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9813111424446106}, {"text": "Average Precision (AP)", "start_pos": 114, "end_pos": 136, "type": "METRIC", "confidence": 0.9671857237815857}, {"text": "Prec", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.984317421913147}, {"text": "precision", "start_pos": 301, "end_pos": 310, "type": "METRIC", "confidence": 0.990043044090271}]}, {"text": "shows the results for the baseline methods, using Count, Count-SVD, and SkipGram vectors.", "labels": [], "entities": [{"text": "Count", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9775975346565247}]}, {"text": "The highest MAP score for all three types of vectors is achieved with vector addition, adding the vectors for head noun, verb, and argument, and the SkipGram vectors achieve the best result overall, with a MAP of 0.496.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9369626343250275}, {"text": "SkipGram", "start_pos": 149, "end_pos": 157, "type": "DATASET", "confidence": 0.9426866173744202}, {"text": "MAP", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.9972078204154968}]}, {"text": "This is already a challenging score to beat, because it represents a system that on average places a correct property at every second position in the top-ranked properties for each term.", "labels": [], "entities": []}, {"text": "Elementwise multiplication performs almost as well for Count vectors, but very poorly for Count-SVD and Skip-Gram vectors, which is consistent with previous findings on elementwise multiplication in dense vector spaces).", "labels": [], "entities": []}, {"text": "We will therefore use vector addition as our primary arithmetic vector operation for the rest of the article.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7428074777126312}]}, {"text": "Moreover, we will use Skip-Gram vectors for our main Categorial framework-based experiments.", "labels": [], "entities": []}, {"text": "In addition to poor performance on polysemous terms (Section 7.2), we observed two common sources of error on RELPRON.", "labels": [], "entities": []}, {"text": "The first is lexical overlap between terms and properties, a type of confounder that we deliberately included in the data set (see Section 3.5).", "labels": [], "entities": []}, {"text": "For example, the top two ranked properties for batter for all three methods are player that walks batter (pitcher) and player that strikes batter (pitcher); and the top ranked property for balance for all three methods is document that has balance (account).", "labels": [], "entities": []}, {"text": "FPLF, despite its more sophisticated modeling of the relative pronoun, suffers from lexical overlap about as much as SPLF and Addition.", "labels": [], "entities": [{"text": "FPLF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9059010148048401}]}, {"text": "However, we note that the RPTensor method is able to overcome this problem to a small extent, with the lexically confounding properties ranked a few positions below the top.", "labels": [], "entities": []}, {"text": "The second source of error is that the methods often assign high similarity to properties that are plausible descriptions fora term, but not annotated as gold positives.", "labels": [], "entities": []}, {"text": "For example, the top-ranked property for lease for the RPTensor method is document that government sells (bond).", "labels": [], "entities": []}, {"text": "The highest-ranked properties for intellectual for SPLF include person that promotes idea (advocate) and person that analyzes ontology (philosopher).", "labels": [], "entities": [{"text": "SPLF", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.9675474166870117}]}, {"text": "This phenomenon speaks to the difficulty of collecting distinctive properties during the manual annotation phase; future work may involve re-annotation of the data set for additional positives.", "labels": [], "entities": []}, {"text": "At present, the effect is to artificially lower the MAP ceiling, which affects all methods equally.", "labels": [], "entities": [{"text": "MAP ceiling", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.8971571922302246}]}, {"text": "To address this issue, we performed another type of evaluation, this time treating properties as queries and ranking terms by their similarity to a property.", "labels": [], "entities": []}, {"text": "We used Mean Reciprocal Rank (MRR) as the evaluation measure, because there is only one correct term per property.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 8, "end_pos": 34, "type": "METRIC", "confidence": 0.94027246038119}]}, {"text": "MRR is given by the formula: where M is the number of properties in the data set and rank(p) is the rank at which the correct term t is found for property p.", "labels": [], "entities": []}, {"text": "The reasoning behind the MRR evaluation is that even if a property is applicable to more than one term, there is always a single term which is most similar to it.", "labels": [], "entities": [{"text": "MRR evaluation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9138140380382538}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For all three methods, the correct term appears on average at approximately rank 2.", "labels": [], "entities": []}, {"text": "The MRR evaluation is also another use of RELPRON that can be exploited in future work.", "labels": [], "entities": [{"text": "MRR evaluation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7869997918605804}, {"text": "RELPRON", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9838431477546692}]}], "tableCaptions": [{"text": " Table 3  Total number of terms and properties by head noun in RELPRON.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.4863993227481842}]}, {"text": " Table 4  Total number of subject and object properties by head noun in RELPRON.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.6000919342041016}]}, {"text": " Table 5  MAP scores of Lexical, Arithmetic, and Frobenius algebra methods on the RELPRON  development set using Count, Count-SVD, and Skip-Gram vectors, and relational verb matrices.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9362691044807434}, {"text": "RELPRON  development set", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.84380571047465}]}, {"text": " Table 6  MAP scores of composition methods on the RELPRON development and test sets using  Skip-Gram vectors.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8880510926246643}, {"text": "RELPRON development and test sets", "start_pos": 51, "end_pos": 84, "type": "DATASET", "confidence": 0.8475947260856629}]}, {"text": " Table 7  MAP scores by grammatical function of extracted element in relative clause, on development set.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6351532936096191}]}, {"text": " Table 8  MAP scores by head noun on development set. AP is calculated over all properties for each  term, and mean AP calculated for each head noun. Head nouns are ordered from left to right by  increasing concreteness.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5350217819213867}, {"text": "AP", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9980409741401672}, {"text": "mean AP", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9154468178749084}]}, {"text": " Table 9  Average proportion of top ten ranked properties that have the correct head noun, on  development set.", "labels": [], "entities": []}, {"text": " Table 10  MAP scores within head nouns on development set. Only the properties with the correct head  noun are ranked for each term.", "labels": [], "entities": [{"text": "MAP", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.49547243118286133}]}]}