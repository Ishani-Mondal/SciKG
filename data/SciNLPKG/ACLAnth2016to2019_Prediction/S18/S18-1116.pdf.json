{"title": [{"text": "CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery", "labels": [], "entities": [{"text": "Hypernym Discovery", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7978324592113495}]}], "abstractContent": [{"text": "This report describes the system developed by the CRIM team for the hypernym discovery task at SemEval 2018.", "labels": [], "entities": [{"text": "hypernym discovery task at SemEval 2018", "start_pos": 68, "end_pos": 107, "type": "TASK", "confidence": 0.8678921560446421}]}, {"text": "This system exploits a combination of supervised projection learning and unsupervised pattern-based hypernym discovery.", "labels": [], "entities": [{"text": "pattern-based hypernym discovery", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.708208163579305}]}, {"text": "It was ranked first on the 3 sub-tasks for which we submitted results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of the hypernym discovery task at SemEval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms.", "labels": [], "entities": [{"text": "hypernym discovery task at SemEval 2018", "start_pos": 16, "end_pos": 55, "type": "TASK", "confidence": 0.800516684850057}]}, {"text": "A query can be either a concept (e.g. cocktail or epistemology) or a named entity (e.g. Craig Anderson or City of Whitehorse).", "labels": [], "entities": []}, {"text": "Two types of data were provided to train the systems: a large unlabeled text corpus and a small training set of examples comprising a query and its hypernyms.", "labels": [], "entities": []}, {"text": "More details on this task maybe found in the task description paper).", "labels": [], "entities": []}, {"text": "The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches: an unsupervised, pattern-based approach and a supervised, projection learning approach.", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8548485040664673}]}, {"text": "These two approaches are described in Sections 2 and 3, then Section 4 describes our hybrid system and Section 5 presents our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted 2 runs on 3 of the 5 sub-tasks: 1A (general), 2A (medical), and 2B (music).", "labels": [], "entities": []}, {"text": "The system outputs its top 15 predictions in all cases.", "labels": [], "entities": []}, {"text": "The difference between the 2 runs is that for run 1, we used data augmentation (see Section 3.4) to train the supervised system -the same unsupervised output was used for both runs.", "labels": [], "entities": []}, {"text": "We also submitted one run for cross-evaluation (training on 1A, but testing on 2A or 2B).", "labels": [], "entities": []}, {"text": "First, we added the queries and candidates of 2A or 2B to those of 1A be-fore training embeddings on the corpus of 1A.", "labels": [], "entities": []}, {"text": "These embeddings were used to train the supervised model on the 1A training data.", "labels": [], "entities": [{"text": "1A training data", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.5902325014273325}]}, {"text": "We then combined the predictions of the supervised and unsupervised models on test set 2A/2B.", "labels": [], "entities": []}, {"text": "A summary of our system's results is shown in.", "labels": [], "entities": []}, {"text": "This table shows the mean average precision (MAP), mean reciprocal rank (MRR) and precision at rank 1 (P@1) of our system and those of the 2 strongest baselines which were computed by the task organizers.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 21, "end_pos": 49, "type": "METRIC", "confidence": 0.8852494458357493}, {"text": "mean reciprocal rank (MRR)", "start_pos": 51, "end_pos": 77, "type": "METRIC", "confidence": 0.9146255056063334}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9982531666755676}]}, {"text": "The first is a supervised baseline and the second is based on the most frequent hypernyms in the training data.", "labels": [], "entities": []}, {"text": "For more details, see).", "labels": [], "entities": []}, {"text": "Our hybrid system was ranked 1st on all three sub-tasks for which we submitted runs.", "labels": [], "entities": []}, {"text": "As shown in, the scores obtained using this system are much higher than the strongest baselines for this task.", "labels": [], "entities": []}, {"text": "Furthermore, it is likely that we could improve our scores on 2A and 2B, since we only tuned the system on 1A.", "labels": [], "entities": []}, {"text": "If we compare runs 1 and 2 of our hybrid system, we see that data augmentation improved our scores slightly on 1A and 2B, and increased them by several points on 2A.", "labels": [], "entities": []}, {"text": "Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup, so training our system on general-purpose data produced better results on a domain-specific test set than a strong, supervised baseline trained on the domain-specific data.", "labels": [], "entities": []}, {"text": "also shows the scores we would have obtained on the test set if we had used only the unsupervised (pattern-based) or supervised (projection learning) parts of our system.", "labels": [], "entities": []}, {"text": "Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task, and even outperformed the supervised baseline on 2A.", "labels": [], "entities": []}, {"text": "Combining the outputs of the 2 systems improves the best score of either system on all test sets, sometimes by as much as 10 points.", "labels": [], "entities": []}, {"text": "Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only (compare runs 1 and 2), although our tests on the , and 2B.", "labels": [], "entities": []}, {"text": "The runs we submitted are the hybrid runs.", "labels": [], "entities": []}, {"text": "The supervised and unsupervised runs were produced by using our 2 sub-systems separately.", "labels": [], "entities": []}, {"text": "BaselineSUP is a strong, supervised baseline and BaselineMFH is the most-frequent-hypernym baseline.", "labels": [], "entities": []}, {"text": "Cross-evaluation results were obtained by training the supervised system on 1A and evaluating on 2A or 2B.", "labels": [], "entities": [{"text": "1A", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9038210511207581}]}, {"text": "trial set suggested it would also have a positive effect on our 1A scores.", "labels": [], "entities": [{"text": "1A scores", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9667524993419647}]}, {"text": "Given this observation, we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system.", "labels": [], "entities": []}, {"text": "One possible explanation is that adding the synthetic examples makes the errors of the supervised system more different from those of the unsupervised system, and that this in turn makes the ensemble method more beneficial, but we haven't looked into this.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our system's results on test sets 1A, 2A", "labels": [], "entities": []}, {"text": " Table 2: Results of ablation tests on test set 1A. The  baseline is our supervised system (run 1).", "labels": [], "entities": []}]}