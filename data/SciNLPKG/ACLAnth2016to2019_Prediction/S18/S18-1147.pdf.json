{"title": [{"text": "SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings", "labels": [], "entities": [{"text": "Neural Hypernym Discovery", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6949883699417114}]}], "abstractContent": [{"text": "This paper describes a hypernym discovery system for our participation in the SemEval-2018 Task 9, which aims to discover the best (set of) candidate hypernyms for input concepts or entities, given the search space of a pre-defined vocabulary.", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7492685317993164}, {"text": "SemEval-2018 Task 9", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7701740662256876}]}, {"text": "We introduce a neu-ral network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases.", "labels": [], "entities": []}, {"text": "The evaluated models include convolutional neural network, long-short term memory network, gated recurrent unit and recurrent convolutional neural network.", "labels": [], "entities": []}, {"text": "We also explore different embedding methods, including word embedding and sense embedding for better performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hypernym-hyponym relationship is an is-a semantic relation between terms as shown in Table 1.", "labels": [], "entities": []}, {"text": "Various natural language processing (NLP) tasks, especially those semantically intensive ones aiming for inference and reasoning with generalization capability, such as question answering () and textual entailment (, can benefit from identifying semantic relations between words beyond synonymy.", "labels": [], "entities": [{"text": "question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8139738440513611}, {"text": "textual entailment", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.6865604370832443}]}, {"text": "The hypernym discovery task) aims to discover the most appropriate hypernym(s) for input concepts or entities from a pre-defined corpus.", "labels": [], "entities": [{"text": "hypernym discovery task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8225417534510294}]}, {"text": "A relevant well-known scenario is hypernym detection, which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8091416358947754}]}, {"text": "A hypernym detection system should be capable of learning taxonomy and lexical semantics, including pattern-based methods) and graph-based approaches.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7866251766681671}]}, {"text": "However, our concerned task, hypernym discovery, is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work.", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8776874244213104}]}, {"text": "The other challenge is representation for terms, including words and phrases, where the phrase embedding could not be obtained byword embeddings directly.", "labels": [], "entities": []}, {"text": "A simple method is to average the inner word embeddings to form the phrase embedding.", "labels": [], "entities": []}, {"text": "However, this is too coarse since each word might share different weights.", "labels": [], "entities": []}, {"text": "Current systems like) commonly discover hypernymic relations by exploiting linear transformation matrix in embedding space, where the embedding should contain words and phrases, resulting to be parameter-exploded and hard to train.", "labels": [], "entities": []}, {"text": "Besides, these systems might be insufficient to obtain the deep relationships between terms.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following experiments, besides the neural networks, we also simply average the embeddings of an input phrase as our baseline to discover the relationship of terms and their corresponding hypernyms for comparison (denoted as term embedding averaging, TEA).", "labels": [], "entities": [{"text": "TEA", "start_pos": 257, "end_pos": 260, "type": "METRIC", "confidence": 0.8411468863487244}]}], "tableCaptions": [{"text": " Table 2: Gold standard evaluation on general-purpose subtask.", "labels": [], "entities": []}, {"text": " Table 3: Gold standard evaluation on domain-specific subtask. \"Embed\" is short for \"Embedding\".", "labels": [], "entities": []}]}