{"title": [{"text": "FEUP at SemEval-2018 Task 5: An Experimental Study of a Question Answering System", "labels": [], "entities": [{"text": "FEUP at SemEval-2018 Task 5", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.7210267186164856}, {"text": "Question Answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7047238498926163}]}], "abstractContent": [{"text": "We present the approach developed at the Faculty of Engineering of the University of Porto to participate in SemEval-2018 Task 5: Counting Events and Participants within Highly Ambiguous Data covering a very long tail.", "labels": [], "entities": [{"text": "SemEval-2018 Task", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.8854296505451202}]}, {"text": "1 The work described here presents the experimental system developed to extract entities from news articles for the sake of Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7555859982967377}]}, {"text": "We propose a supervised learning approach to enable the recognition of two different types of entities: Locations and Participants.", "labels": [], "entities": []}, {"text": "We also discuss the use of distance-based algorithms (using Levenshtein distance and Q-grams) for the detection of documents' closeness based on the entities extracted.", "labels": [], "entities": []}, {"text": "For the experiments, we also used a multi-agent system that improved the performance.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate the performance of various thresholds on near document detection by applying the metrics: Precision, Recall, and Accuracy.", "labels": [], "entities": [{"text": "near document detection", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6025808254877726}, {"text": "Precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.999038577079773}, {"text": "Recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9961591958999634}, {"text": "Accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9991580247879028}]}, {"text": "For each pair of news articles, we have calculated the similarity between their elements: title (T), list of participants (Part), and list of locations (Loc).", "labels": [], "entities": [{"text": "similarity", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9777354001998901}, {"text": "title (T)", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9274878948926926}]}, {"text": "For the sake of comparison, we have used two distance algorithms: Levenshtein (L)) and Qgrams (Q).", "labels": [], "entities": [{"text": "Levenshtein", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.934384286403656}]}, {"text": "We defined two scenarios (SS1, SS2), differing in the weights of the document elements as follows:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Categorizing each word on a sentence", "labels": [], "entities": []}, {"text": " Table 7: Recognizing Participants -Results", "labels": [], "entities": [{"text": "Recognizing Participants", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9104424715042114}]}, {"text": " Table 8: Recognizing Location -Results", "labels": [], "entities": [{"text": "Recognizing Location -", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.832968016465505}]}, {"text": " Table 9: Near Document Detection Results by Thresh- old", "labels": [], "entities": [{"text": "Near Document Detection", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8552233576774597}, {"text": "Thresh- old", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.709494968255361}]}]}