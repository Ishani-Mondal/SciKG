{"title": [{"text": "Peperomia at SemEval-2018 Task 2: Vector Similarity Based Approach for Emoji Prediction", "labels": [], "entities": [{"text": "Emoji Prediction", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.775222510099411}]}], "abstractContent": [{"text": "This paper describes our participation in Se-mEval 2018 Task 2: Multilingual Emoji Prediction , in which participants are asked to predict a tweet's most associated emoji from 20 emojis.", "labels": [], "entities": [{"text": "Multilingual Emoji Prediction", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.6066714723904928}]}, {"text": "Instead of regarding it as a 20-class classification problem we regard it as a text similarity problem.", "labels": [], "entities": [{"text": "20-class classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6866230070590973}]}, {"text": "We propose a vector similarity based approach for this task.", "labels": [], "entities": []}, {"text": "First the distributed representation (tweet vector) for each tweet is generated, then the similarity between this tweet vector and each emoji's embedding is evaluated.", "labels": [], "entities": []}, {"text": "The most similar emoji is chosen as the predicted label.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach performs comparably with the classification approach and shows its advantage in classifying emojis with similar semantic meaning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Participants for SemEval 2018 Task 2 ( are asked to predict the most likely associated emoji given the tweet.", "labels": [], "entities": [{"text": "SemEval 2018 Task 2", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.853085920214653}]}, {"text": "For simplicity purposes, each tweet contains one and only one emoji, which belongs to the 20 most frequent emojis.", "labels": [], "entities": []}, {"text": "We participate in its subtask 1: Emoji Prediction in English.", "labels": [], "entities": [{"text": "Emoji Prediction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7634054124355316}]}, {"text": "With the wide-spread use on many social platforms, emoji has attracted more and more attention of researchers recently.", "labels": [], "entities": []}, {"text": "explored whether emoji renderings or differences across platforms gave rise to diverse interpretations of emoji.", "labels": [], "entities": []}, {"text": "For the same emoji, the sender and the receiver may have different interpretations of its meaning.", "labels": [], "entities": []}, {"text": "This misinterpretation occurs when joint perceptual experience of sender and receiver lacks or the platforms' rendering style differs.", "labels": [], "entities": []}, {"text": "Some efforts have been devoted to studying emoji through its distributed representation.", "labels": [], "entities": []}, {"text": "trained emoji embeddings with a skip-gram model through millions of tweets, and explored the similarity and relatedness among these embeddings in various languages.", "labels": [], "entities": []}, {"text": "Their results suggested that the overall semantic of emoji was preserved across languages, but some emojis were interpretated differently due to users' socio-geographical differences.", "labels": [], "entities": []}, {"text": "trained emoji embeddings with their short descriptions and demonstrated that emoji embeddings trained through this way were beneficial to sentiment analysis task.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.9601283371448517}]}, {"text": "We believe that the key to better classify emojis is understanding their meaning, since people intend a particular meaning when they send an emoji.", "labels": [], "entities": [{"text": "classify emojis", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.821600466966629}]}, {"text": "People view the same characters during the exchange of plain text.", "labels": [], "entities": []}, {"text": "Unlike plain text, emoji is not definite enough and doesn't have a general acknowledgement of how we should use it.", "labels": [], "entities": []}, {"text": "It is common for different readers to have different interpretations of the same emoji, which naturally results in different ways of using emoji.", "labels": [], "entities": []}, {"text": "investigated a wide range of emoji usage and showed that emojis served at least two very different purposes: content and function words or multimodal affective markers.", "labels": [], "entities": []}, {"text": "Word embeddings ( are continuous distributed representations of words, with two good properties: 1.", "labels": [], "entities": []}, {"text": "take word's semantic meaning into account, 2.", "labels": [], "entities": []}, {"text": "distances between words are interpretable and can be measured using cosine distance.", "labels": [], "entities": []}, {"text": "Based on such previous work, we proposed our vector similarity based approach for emoji prediction: first the neural network model is trained to generate a 300-d 1 vector, which is considered as the overall sentence vector of the tweet.", "labels": [], "entities": [{"text": "emoji prediction", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.9334059357643127}]}, {"text": "Then this tweet vector's semantic similarity with each emoji's pre-trained embedding is evaluated.", "labels": [], "entities": []}, {"text": "The predicted label is the one with the highest similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system is implemented using Keras and the code is available on github . We use the official evaluation metric macro f1, which evaluates both precision and recall of each class regardless of its sample num.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.999189555644989}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9986252784729004}]}, {"text": "Three groups of experiments are achieved to evaluate our approach and models.", "labels": [], "entities": []}, {"text": "To compare the vector similarity based approach with the classification approach, we implement the above 2-layered LSTM model and BiLSTM model with the same experiment settings for both approaches.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9605933427810669}]}, {"text": "To figure out which model structure is better, we compare the 2-layered LSTM model and BiLSTM model's performance on both approaches.", "labels": [], "entities": []}, {"text": "We also test the loss functions loss 1 and loss 2 's effects on 2-layered LSTM model.", "labels": [], "entities": []}, {"text": "Next, we will describe the key experiment settings.", "labels": [], "entities": []}, {"text": "More detailed model settings can be found in.", "labels": [], "entities": []}, {"text": "Text Preprocessing: The whole tweet is lowercased.", "labels": [], "entities": []}, {"text": "We split it into token sequence using Keras' default tokenizer, which split a sentence by spaces and following punctuations: !\"#$%&() * +,-./:;<=>?@[\\]^_'~\\t\\n{ |}.", "labels": [], "entities": []}, {"text": "Long sequences are truncated and short ones are padded with 0s from the head to meet fixed length 20.", "labels": [], "entities": []}, {"text": "Embedding Layer: The embedding layer is set to be trainable.", "labels": [], "entities": []}, {"text": "It is initialized by looking up from a pre-trained twitter embedding matrix (), <UNK> is initialized as 0.", "labels": [], "entities": []}, {"text": "Output Layer: For classification approach, the output layer's unit num is 20 (same with the num  of emoji classes).", "labels": [], "entities": []}, {"text": "For vector similarity based approach, the output layer's unit num is 300 (same with the size of the pre-trained emoji embedding).", "labels": [], "entities": []}, {"text": "Training Loss: For our vector similarity approach, loss 1 and loss 2 described in section 2 are tested separately.", "labels": [], "entities": []}, {"text": "For classification approach, categorial_cross_entropy is used.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9717926979064941}]}], "tableCaptions": [{"text": " Table 2: Experiment Results. CL for classification ap- proach, VS for vector similarity based approach, 2-lstm  for 2-layered LSTM model. The results marked with  asterisk (*) are our submissions for final evaluation.", "labels": [], "entities": [{"text": "VS", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9862654805183411}]}]}