{"title": [], "abstractContent": [{"text": "Neural word embeddings models (such as those built with word2vec) are known to have stability problems: when retraining a model with the exact same hyperparameters, words neighborhoods may change.", "labels": [], "entities": []}, {"text": "We propose a method to estimate such variation, based on the overlap of neighbors of a given word in two models trained with identical hyperparam-eters.", "labels": [], "entities": []}, {"text": "We show that this inherent variation is not negligible, and that it does not affect every word in the same way.", "labels": [], "entities": []}, {"text": "We examine the influence of several features that are intrinsic to a word, corpus or embedding model and provide a methodology that can predict the variability (and as such, reliability) of a word representation in a semantic vector space.", "labels": [], "entities": [{"text": "reliability", "start_pos": 174, "end_pos": 185, "type": "METRIC", "confidence": 0.9853579998016357}]}], "introductionContent": [{"text": "Word embeddings are dense representations of the meaning of words that are efficient and easy to use.", "labels": [], "entities": []}, {"text": "Embeddings training methods such as word2vec (, are based on neural networks methods that imply random processes (initialization of the network, sampling, etc.).", "labels": [], "entities": []}, {"text": "As such, they display stability problems meaning that retraining a model with the exact same hyperparameters will give different word representations, with a word possibly having different nearest neighbors from one model to the other.", "labels": [], "entities": []}, {"text": "Benchmarks test sets such as WordSim-353 () are commonly used to evaluate word embeddings since they provide a fast and easy way to quickly evaluate a model.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.962712824344635}]}, {"text": "However, the instability of word embeddings is not detected by these test sets since only selected pairs of words are evaluated.", "labels": [], "entities": []}, {"text": "A model showing instability could get very similar performance results when evaluated on such benchmarks.", "labels": [], "entities": []}, {"text": "Hyperparameters selected when training word embeddings impact the semantic representation of a word.", "labels": [], "entities": []}, {"text": "Among these hyperparameters we find some hyperparameters internal to the system such as the architecture used, the size of the context window or the dimensions of the vectors as well as some external hyperparameters such as the corpus used for training.", "labels": [], "entities": []}, {"text": "In this work, we adopt a corpus linguistics approach in a similar way to, and meaning that observing the semantic representation of a word consists in observing the nearest neighbors of this word.", "labels": [], "entities": []}, {"text": "Corpus tools such as Sketch Engine () use embeddings trained on several corpora 1 to provide users with most similar words as a lexical semantic information on a target word.", "labels": [], "entities": []}, {"text": "In order to make accurate observations, it thus seems important to understand the stability of these embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we measure the variation that exists between several models trained with the same hyperparameters in terms of nearest neighbors for all words in a corpus.", "labels": [], "entities": []}, {"text": "A word having the same nearest neighbors across several models is considered stable.", "labels": [], "entities": []}, {"text": "Based on a set of selected features, we also attempt to predict the stability of a word.", "labels": [], "entities": []}, {"text": "Such a prediction is interesting to understand what features have an impact on a word representation variability.", "labels": [], "entities": []}, {"text": "It could also be used to certify the reliability of the given semantic representation of a word without having to retrain several models to make sure the representation is accurate.", "labels": [], "entities": []}, {"text": "This will be a useful method to give more reliability to observations made in corpus linguistics using word em-beddings.", "labels": [], "entities": [{"text": "reliability", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9803192019462585}]}, {"text": "It can also help choosing the right hyperparameters or refine a model (e.g. by removing selected semantic classes).", "labels": [], "entities": []}, {"text": "We examine the influence of several features that are intrinsic to a word, a corpus or a model: part of speech (henceforth POS), degree of polysemy, frequency of a word, distribution of the contexts of a word, position and environment of a vector in the semantic space.", "labels": [], "entities": []}, {"text": "We train a multilinear regression model using these features and predict up to 48% of the variance.", "labels": [], "entities": []}, {"text": "This experiment was conducted on 3 different corpora with similar results.", "labels": [], "entities": []}, {"text": "We first explain how we measure the variation of a model.", "labels": [], "entities": []}, {"text": "We then present the models used in this work and we finally describe our predictive model.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure the variation fora word between two embedding models, we used an approach similar to by measuring the nearest neighbors overlap for words common to the two models.", "labels": [], "entities": []}, {"text": "More precisely the variation score of a word varnn across two models M 1 and M 2 is measured as: represents the N words having the closest cosine similarity score with word win a distributional model M.", "labels": [], "entities": [{"text": "variation score", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.974664717912674}]}, {"text": "In the experiments presented here we selected N = 25.", "labels": [], "entities": []}, {"text": "To chose the value of N, we selected two models and computed the variation with different values of N across the entire vocabulary, 50 and 100).", "labels": [], "entities": []}, {"text": "We then computed the correlation coefficient between scores for all the N values and found that the highest average correlation value was for N = 25.", "labels": [], "entities": [{"text": "correlation coefficient between scores", "start_pos": 21, "end_pos": 59, "type": "METRIC", "confidence": 0.9202410727739334}, {"text": "correlation", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.8745360374450684}]}, {"text": "The variation was computed only for open classes (adverbs, adjectives, verbs and nouns).", "labels": [], "entities": []}, {"text": "This variation measure presents both advantages and inconvenients.", "labels": [], "entities": []}, {"text": "The fact that this measure is cost-effective and intuitive makes it very convenient to use.", "labels": [], "entities": []}, {"text": "It is also strongly related to the way we observe word embeddings in a corpuslinguistics approach (i.e. by observing a few nearest neighbors).", "labels": [], "entities": []}, {"text": "However we are aware that this measure assess only apart of what has changed from one model to the other based on the number of neighbors observed.", "labels": [], "entities": []}, {"text": "This measure may also be sensible to complex effects and phenomena in high-dimensional vector spaces such as hubness, with some words being more \"popular\" nearest neighbors than others.", "labels": [], "entities": []}, {"text": "Although we could indeed identify such hubs in our vector spaces, they were limited to a small cluster of words (such as surnames for the BNC) and did not interfere with our measure of stability for all other areas of the lexicon.", "labels": [], "entities": [{"text": "BNC", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.8953481316566467}]}, {"text": "The compared models were trained using the standard word2vec 2 with the default hyperparameters (architecture Skip-Gram with negative sampling rate of 5, window size set to 5, vectors dimensions set to 100, negative sampling rate set to 10 -3 and number of iterations set to 5).", "labels": [], "entities": []}, {"text": "Additionally, min-count was set to 100.", "labels": [], "entities": [{"text": "min-count", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.904884934425354}]}, {"text": "Models were trained on 3 different corpora: ACL (NLP scientific articles from the ACL anthology 3 ), BNC (written part of the British National Corpus 4 ) and PLOS (biology scientific articles from the PLOS archive collections 5 ).", "labels": [], "entities": [{"text": "ACL (NLP scientific articles from the ACL anthology", "start_pos": 44, "end_pos": 95, "type": "DATASET", "confidence": 0.62139364083608}, {"text": "BNC", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.5666757225990295}, {"text": "British National Corpus 4", "start_pos": 126, "end_pos": 151, "type": "DATASET", "confidence": 0.9260135143995285}, {"text": "PLOS archive collections", "start_pos": 201, "end_pos": 225, "type": "DATASET", "confidence": 0.8946497241655985}]}, {"text": "All corpora are the same size (about 100 million words) but they are from different types (the BNC is a generic corpus while PLOS and ACL are specialized corpora) and different domains.", "labels": [], "entities": []}, {"text": "Corpora were lemmatized and POS-tagged using the Talismane toolkit.", "labels": [], "entities": []}, {"text": "Every word is associated to its POS for all subsequent experiments.", "labels": [], "entities": [{"text": "POS", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.940683901309967}]}, {"text": "For each corpus, we trained 5 models using the exact same hyperparameters mentioned above; they only differ because of the inherent randomness of word2vec's technique.", "labels": [], "entities": []}, {"text": "We then made 10 pairwise comparisons of models per corpus, computing the variation score for every word.: Mean variation score and standard deviations for each corpus (5 models trained per corpus).", "labels": [], "entities": [{"text": "variation score", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.9627422988414764}, {"text": "Mean variation score", "start_pos": 106, "end_pos": 126, "type": "METRIC", "confidence": 0.962259570757548}]}, {"text": "reports the results of the comparisons.", "labels": [], "entities": []}, {"text": "For each corpus we indicate the mean variation score, i.e. the variation averaged overall words and the 10 pairwise comparisons.", "labels": [], "entities": [{"text": "mean variation score", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.8816630045572916}]}, {"text": "The variation is very similar from one corpus to the other.", "labels": [], "entities": []}, {"text": "Standard deviation is low (average of 0.04) across the 10 pairs of models, meaning that the variation is equally distributed among the comparisons made for each corpus.", "labels": [], "entities": [{"text": "Standard deviation", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9359902143478394}]}, {"text": "The standard deviation across words is much higher (average of 0.08), which indicates that there are important differences in variation from one word to the other within the same category of models.", "labels": [], "entities": [{"text": "standard", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9804767370223999}]}, {"text": "Variation scores fora given word can be zero (all 25 nearest neighbors are identical, although their order can vary) or as high as 0.68 (only a third of the nearest neighbors are found in both models).", "labels": [], "entities": []}, {"text": "Based on the average variation score across the 5 models, we had a closer look at words varying the most and the least in each corpus.", "labels": [], "entities": []}, {"text": "We identified semantic clusters that remained stable across models.", "labels": [], "entities": []}, {"text": "E.g., in the BNC that was the case for temporal expressions (am, pm, noon).", "labels": [], "entities": [{"text": "BNC", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.8877888917922974}]}, {"text": "For all 3 corpora we identified closed classes of co-hyponyms, e.g. family members in the BNC (wife, grandmother, sister...), linguistic preprocessing in ACL (parsing, lemmatizing, tokenizing...) and antibiotics in PLOS (puromycin, blasticidin, cefotaxime...).", "labels": [], "entities": [{"text": "BNC", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.6498973369598389}]}, {"text": "For ACL and PLOS we also noticed that words belonging to the transdisciplinary scientific lexicon remained stable (conjunctive adverbs such as nevertheless, moreover, furthermore and scientific processes such as hypothethize, reason, describe).", "labels": [], "entities": []}, {"text": "Among words displaying high variation we found a large number of tagging errors and proper nouns.", "labels": [], "entities": []}, {"text": "We also identified some common features for other words displaying a high variation.", "labels": [], "entities": []}, {"text": "E.g. highly polysemic words (sign in ACL, make in the BNC) and generic adjectives, i.e. adjectives than can modifiy almost any noun (special in ACL, current in PLOS and whole in the BNC), tend to vary more.", "labels": [], "entities": [{"text": "BNC", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9568522572517395}, {"text": "BNC", "start_pos": 182, "end_pos": 185, "type": "DATASET", "confidence": 0.9492467045783997}]}], "tableCaptions": [{"text": " Table 1: Mean variation score and standard deviations  for each corpus (5 models trained per corpus).", "labels": [], "entities": [{"text": "Mean variation score", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9476655324300131}]}]}