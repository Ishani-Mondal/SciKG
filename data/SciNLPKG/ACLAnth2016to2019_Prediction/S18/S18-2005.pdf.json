{"title": [{"text": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "labels": [], "entities": [{"text": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7418120175600051}]}], "abstractContent": [{"text": "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases.", "labels": [], "entities": []}, {"text": "Past work on examining inappropriate biases has largely focused on just individual systems.", "labels": [], "entities": []}, {"text": "Further, there is no benchmark dataset for examining inappropriate biases in systems.", "labels": [], "entities": []}, {"text": "Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders.", "labels": [], "entities": [{"text": "Equity Evaluation Corpus (EEC)", "start_pos": 40, "end_pos": 70, "type": "DATASET", "confidence": 0.6542072047789892}]}, {"text": "We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 'Affect in Tweets'.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7885818183422089}]}, {"text": "We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender.", "labels": [], "entities": []}, {"text": "We make the EEC freely available.", "labels": [], "entities": [{"text": "EEC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8595110774040222}]}], "introductionContent": [{"text": "Automatic systems have had a significant and beneficial impact on all walks of human life.", "labels": [], "entities": []}, {"text": "So much so that it is easy to overlook their potential to benefit society by promoting equity, diversity, and fairness.", "labels": [], "entities": []}, {"text": "For example, machines do not take bribes to do their jobs, they can determine eligibility fora loan without being influenced by the color of the applicant's skin, and they can provide access to information and services without discrimination based on gender or sexual orientation.", "labels": [], "entities": []}, {"text": "Nonetheless, as machine learning systems become more human-like in their predictions, they can also perpetuate human biases.", "labels": [], "entities": []}, {"text": "Some learned biases maybe beneficial for the downstream application (e.g., learning that humans often use some insect names, such as spider or cockroach, to refer to unpleasant situations).", "labels": [], "entities": []}, {"text": "Other biases can be inappropriate and result in negative experiences for some groups of people.", "labels": [], "entities": []}, {"text": "Examples include, loan eligibility and crime recidivism prediction systems that negatively assess people belonging to a certain pin/zip code (which may disproportionately impact people of a certain race) and resum\u00e9 sorting systems that believe that men are more qualified to be programmers than women (.", "labels": [], "entities": [{"text": "loan eligibility and crime recidivism prediction", "start_pos": 18, "end_pos": 66, "type": "TASK", "confidence": 0.7592291732629141}, {"text": "resum\u00e9 sorting", "start_pos": 208, "end_pos": 222, "type": "TASK", "confidence": 0.923920601606369}]}, {"text": "Similarly, sentiment and emotion analysis systems can also perpetuate and accentuate inappropriate human biases, e.g., systems that consider utterances from one race or gender to be less positive simply because of their race or gender, or customer support systems that prioritize a call from an angry male over a call from the equally angry female.", "labels": [], "entities": [{"text": "sentiment and emotion analysis", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.8460626304149628}]}, {"text": "Predictions of machine learning systems have also been shown to be of higher quality when dealing with information from some groups of people as opposed to other groups of people.", "labels": [], "entities": []}, {"text": "For example, in the area of computer vision, gender classification systems perform particularly poorly for darker skinned females.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7781296968460083}, {"text": "gender classification", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7324670851230621}]}, {"text": "Natural language processing (NLP) systems have been shown to be poor in understanding text produced by people belonging to certain races ().", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7283398807048798}]}, {"text": "For NLP systems, the sources of the bias often include the training data, other corpora, lexicons, and word embeddings that the machine learning algorithm may leverage to build its prediction model.", "labels": [], "entities": []}, {"text": "Even though there is some recent work highlighting such inappropriate biases (such as the work mentioned above), each such past work has largely focused on just one or two systems and resources.", "labels": [], "entities": []}, {"text": "Further, there is no benchmark dataset for examining inappropriate biases in natural language systems.", "labels": [], "entities": []}, {"text": "In this paper, we describe how we compiled a dataset of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders.", "labels": [], "entities": []}, {"text": "We will refer to it as the Equity Evaluation Corpus (EEC).", "labels": [], "entities": [{"text": "Equity Evaluation Corpus (EEC)", "start_pos": 27, "end_pos": 57, "type": "DATASET", "confidence": 0.6702834069728851}]}, {"text": "We used the EEC as a supplementary test set in a recent shared task on predicting sentiment and emotion intensity in tweets, SemEval-2018 Task 1: Affect in Tweets ( . In particular, we wanted to test a hypothesis that a system should equally rate the intensity of the emotion expressed by two sentences that differ only in the gender/race of a person mentioned.", "labels": [], "entities": [{"text": "EEC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8100190758705139}, {"text": "predicting sentiment and emotion intensity", "start_pos": 71, "end_pos": 113, "type": "TASK", "confidence": 0.7002646327018738}]}, {"text": "Note that here the term system refers to the combination of a machine learning architecture trained on a labeled dataset, and possibly using additional language resources.", "labels": [], "entities": []}, {"text": "The bias can originate from any or several of these parts.", "labels": [], "entities": []}, {"text": "We were thus able to use the EEC to examine 219 sentiment analysis systems that took part in the shared task.", "labels": [], "entities": [{"text": "EEC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9668605923652649}, {"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9123557507991791}]}, {"text": "We compare emotion and sentiment intensity scores that the systems predict on pairs of sentences in the EEC that differ only in one word corresponding to race or gender (e.g., 'This man made me feel angry' vs. 'This woman made me feel angry').", "labels": [], "entities": [{"text": "EEC", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9550654292106628}]}, {"text": "We find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.", "labels": [], "entities": []}, {"text": "We also find that the bias maybe different depending on the particular affect dimension that the natural language system is trained to predict.", "labels": [], "entities": []}, {"text": "Despite the work we describe here and what others have proposed in the past, it should be noted that there are no simple solutions for dealing with inappropriate human biases that percolate into machine learning systems.", "labels": [], "entities": []}, {"text": "It seems difficult to ever be able to identify and quantify all of the inappropriate biases perfectly (even when restricted to the scope of just gender and race).", "labels": [], "entities": []}, {"text": "Further, any such mechanism is liable to be circumvented, if one chooses to do so.", "labels": [], "entities": []}, {"text": "Nonetheless, as developers of sentiment analysis systems, and NLP systems more broadly, we cannot absolve ourselves of the ethical implications of the systems we build.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9310493469238281}]}, {"text": "Even if it is unclear how we should deal with the inappropriate biases in our systems, we should be measuring such biases.", "labels": [], "entities": []}, {"text": "The Equity Evaluation Corpus is not meant to be a catch-all for all inappropriate biases, but rather just one of the several ways by which we can examine the fairness of sentiment analysis systems.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8944038450717926}]}, {"text": "We make the corpus freely available so that both developers and users can use it, and build on it.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or lower) sentiment intensity scores to sentences involving a particular race or gender.", "labels": [], "entities": []}, {"text": "There are several ways in which such a dataset maybe compiled.", "labels": [], "entities": []}, {"text": "We present below the choices that we made.", "labels": [], "entities": []}, {"text": "Sentences with emotion words: 1.", "labels": [], "entities": []}, {"text": "<Person> feels <emotional state word>.", "labels": [], "entities": []}, {"text": "The situation makes <person> feel <emotional state word>.", "labels": [], "entities": []}, {"text": "I made <person> feel <emotional state word>.", "labels": [], "entities": []}, {"text": "<Person> made me feel <emotional state word>.", "labels": [], "entities": []}, {"text": "<Person> found himself/herself in a/an <emotional situation word> situation.", "labels": [], "entities": []}, {"text": "<Person> told us all about the recent <emotional situation word> events.", "labels": [], "entities": []}, {"text": "The conversation with <person> was <emotional situation word>.", "labels": [], "entities": []}, {"text": "1,200 Sentences with no emotion words: 8.", "labels": [], "entities": []}, {"text": "I saw <person> in the market.", "labels": [], "entities": []}, {"text": "I talked to <person> yesterday.", "labels": [], "entities": []}, {"text": "<Person> goes to the school in our neighborhood.", "labels": [], "entities": []}, {"text": "<Person> has two children.", "labels": [], "entities": []}, {"text": "60  We decided to use sentences involving at least one race-or gender-associated word.", "labels": [], "entities": []}, {"text": "The sentences were intended to be short and grammatically simple.", "labels": [], "entities": []}, {"text": "We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems.", "labels": [], "entities": []}, {"text": "We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building.", "labels": [], "entities": []}, {"text": "The templates are divided into two groups.", "labels": [], "entities": []}, {"text": "The first type (templates 1-7) includes emotion words.", "labels": [], "entities": []}, {"text": "The purpose of this set is to have sentences expressing emotions.", "labels": [], "entities": []}, {"text": "The second type (templates 8-11) does not include any emotion words.", "labels": [], "entities": []}, {"text": "The purpose of this set is to have nonemotional (neutral) sentences.", "labels": [], "entities": []}, {"text": "The templates include two variables: <person> and <emotion word>.", "labels": [], "entities": []}, {"text": "We generate sentences from the template by instantiating each variable with one of the pre-chosen values that the variable can take.", "labels": [], "entities": []}, {"text": "Each of the eleven templates includes the variable <person>.", "labels": [], "entities": []}, {"text": "<person> can be instantiated by any of the following noun phrases: \u2022 Common African American female or male first names; Common European American female or male first names; \u2022 Noun phrases referring to females, such as 'my daughter'; and noun phrases referring to males, such as 'my son'.", "labels": [], "entities": []}, {"text": "For our study, we chose ten names of each kind from the study by (see).", "labels": [], "entities": []}, {"text": "The full lists of noun phrases representing females and males, used in our study, are shown in.: Pairs of noun phrases representing a female or a male person used in this study.", "labels": [], "entities": []}, {"text": "The second variable, <emotion word>, has two variants.", "labels": [], "entities": []}, {"text": "Templates one through four include a variable for an emotional state word.", "labels": [], "entities": []}, {"text": "The emotional state words correspond to four basic emotions: anger, fear, joy, and sadness.", "labels": [], "entities": []}, {"text": "Specifically, for each of the emotions, we selected five words that convey that emotion in varying intensities.", "labels": [], "entities": []}, {"text": "These words were taken from the categories in the Roget's Thesaurus corresponding to the four emotions: category #900 Resentment (for anger), category #860 Fear (for fear), category #836 Cheerfulness (for joy), and category #837 Dejection (for sadness).", "labels": [], "entities": [{"text": "Roget's Thesaurus", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.7865971724192301}]}, {"text": "Templates five through seven include emotion words describing a situation or event.", "labels": [], "entities": []}, {"text": "These words were also taken from the same thesaurus categories listed above.", "labels": [], "entities": []}, {"text": "The full lists of emotion words (emotional state words and emotional situation/event words) are shown in.", "labels": [], "entities": []}, {"text": "We generated sentences from the templates by replacing <person> and <emotion word> variables with the values they can take.", "labels": [], "entities": []}, {"text": "In total, 8,640 sentences were generated with the various combinations of <person> and <emotion word> values across the eleven templates.", "labels": [], "entities": []}, {"text": "We manually exam-: Emotion words used in this study.", "labels": [], "entities": []}, {"text": "ined the sentences to make sure they were grammatically well-formed.", "labels": [], "entities": []}, {"text": "Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., 'My daughter feels devastated' and 'My son feels devastated').", "labels": [], "entities": [{"text": "EEC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9289737343788147}]}, {"text": "We refer to the full set of 8,640 sentences as Equity Evaluation Corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Analysis of gender bias: Summary results  for 219 submissions from 50 teams on the Equity Eval- uation Corpus (including both sentences with emotion  words and sentences without emotion words).", "labels": [], "entities": [{"text": "Equity Eval- uation Corpus", "start_pos": 93, "end_pos": 119, "type": "DATASET", "confidence": 0.5640390396118165}]}, {"text": " Table 6: Analysis of gender bias: Summary results  for 219 submissions from 50 teams on the subset of  sentences from the Equity Evaluation Corpus that do  not contain any emotion words.", "labels": [], "entities": []}, {"text": " Table 7: Analysis of race bias: Summary results for  219 submissions from 50 teams on the Equity Evalu- ation Corpus (including both sentences with emotion  words and sentences without emotion words).", "labels": [], "entities": [{"text": "Summary", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9255309700965881}, {"text": "Equity Evalu- ation Corpus", "start_pos": 91, "end_pos": 117, "type": "DATASET", "confidence": 0.56688894033432}]}]}