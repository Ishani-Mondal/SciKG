{"title": [{"text": "Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories", "labels": [], "entities": [{"text": "Improved Entity Linking on Question Answering", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.6544725050528845}]}], "abstractContent": [{"text": "The first stage of every knowledge base question answering approach is to link entities in the input question.", "labels": [], "entities": [{"text": "knowledge base question answering", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6028713062405586}]}, {"text": "We investigate entity linking in the context of a question answering task and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity.", "labels": [], "entities": [{"text": "entity linking in the context of a question answering task", "start_pos": 15, "end_pos": 73, "type": "TASK", "confidence": 0.635057932138443}, {"text": "entity mention detection", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6960497796535492}, {"text": "entity disambiguation", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.7454381883144379}]}, {"text": "We use the Wikidata knowledge base and available question answering datasets to create benchmarks for entity linking on question answering data.", "labels": [], "entities": [{"text": "Wikidata knowledge base", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.964120884736379}, {"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.688053086400032}, {"text": "entity linking on question answering data", "start_pos": 102, "end_pos": 143, "type": "TASK", "confidence": 0.7225164075692495}]}, {"text": "Our approach outperforms the previous state-of-the-art system on this data, resulting in an average 8% improvement of the final score.", "labels": [], "entities": []}, {"text": "We further demonstrate that our model delivers a strong performance across different entity categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge base question answering (QA) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base (KB) in order to retrieve the correct answer.", "labels": [], "entities": [{"text": "Knowledge base question answering (QA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.722193969147546}]}, {"text": "The first stage for every QA approach is entity linking (EL), that is the identification of entity mentions in the question and linking them to entities in KB.", "labels": [], "entities": [{"text": "entity linking (EL)", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8253226995468139}]}, {"text": "In, two entity mentions are detected and linked to the knowledge base referents.", "labels": [], "entities": []}, {"text": "This step is crucial for QA since the correct answer must be connected via some path over KB to the entities mentioned in the question.", "labels": [], "entities": [{"text": "QA", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9093358516693115}]}, {"text": "The state-of-the-art QA systems usually rely on off-the-shelf EL systems to extract entities from the question (.", "labels": [], "entities": []}, {"text": "Multiple EL systems are freely available and can be readily applied what are taylor swift's albums ? Taylor Swift Q462 album Q24951125, etc.", "labels": [], "entities": [{"text": "Taylor Swift Q462 album Q24951125", "start_pos": 101, "end_pos": 134, "type": "DATASET", "confidence": 0.8646230816841125}]}, {"text": "for question answering (e.g. DBPedia Spotlight 1 , AIDA 2 ).", "labels": [], "entities": [{"text": "question answering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.813547283411026}, {"text": "DBPedia Spotlight 1", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9512634873390198}]}, {"text": "However, these systems have certain drawbacks in the QA setting: they are targeted at long well-formed documents, such as news texts, and are less suited for typically short and noisy question data.", "labels": [], "entities": []}, {"text": "Other EL systems focus on noisy data (e.g. S-MART,), but are not openly available and hence limited in their usage and application.", "labels": [], "entities": []}, {"text": "Multiple error analyses of QA systems point to entity linking as a major external source of error.", "labels": [], "entities": []}, {"text": "The QA datasets are normally collected from the web and contain very noisy and diverse data, which poses a number of challenges for EL.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7961956262588501}, {"text": "EL", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.8397218585014343}]}, {"text": "First, many common features used in EL systems, such as capitalization, are not meaningful on noisy data.", "labels": [], "entities": []}, {"text": "Moreover, a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7298895865678787}]}, {"text": "The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems.", "labels": [], "entities": [{"text": "QA data", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8096660673618317}]}, {"text": "In this paper, we present an approach that tackles the challenges listed above: we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end-to-end differentiable.", "labels": [], "entities": [{"text": "entity mention detection", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.787238876024882}, {"text": "entity disambiguation", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.7132909446954727}]}, {"text": "This ensures that any token n-gram can be considered as a potential entity mention, which is important to be able to link entities of different categories, such as movie titles and organization names.", "labels": [], "entities": []}, {"text": "To overcome the noise in the data, we automatically learn features over a set of contexts of different granularity levels.", "labels": [], "entities": []}, {"text": "Each level of granularity is handled by a separate component of the model.", "labels": [], "entities": []}, {"text": "A token-level component extracts higher-level features from the whole question context, whereas a character-level component builds lower-level features for the candidate n-gram.", "labels": [], "entities": []}, {"text": "Simultaneously, we extract features from the knowledge base context of the candidate entity: character-level features are extracted for the entity label and higher-level features are produced based on the entities surrounding the candidate entity in the knowledge graph.", "labels": [], "entities": []}, {"text": "This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked.", "labels": [], "entities": []}, {"text": "Contributions The two main contributions of our work are: (i) We construct two datasets to evaluate EL for QA and present a set of strong baselines: the existing EL systems that were used as a building block for QA before and a model that uses manual features from the previous work on noisy data.", "labels": [], "entities": []}, {"text": "(ii) We design and implement an entity linking system that models contexts of variable granularity to detect and disambiguate entity mentions.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to present a unified end-to-end neural model for entity linking for noisy data that operates on different context levels and does not rely on manual features.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7388094365596771}]}, {"text": "Our architecture addresses the challenges of entity linking on question answering data and outperforms state-of-the-art EL systems.", "labels": [], "entities": [{"text": "entity linking on question answering data", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.7736648768186569}]}, {"text": "Code and Datasets Our system can be applied on any QA dataset.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8527382612228394}]}, {"text": "The complete code as well as the scripts that produce the evaluation data can be found here: https://github.com/UKPLab/ starsem2018-entity-linking.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9800865650177002}]}], "datasetContent": [{"text": "We compile two new datasets for entity linking on questions that we derive from publicly available question answering data:) and GraphQuestions (.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8012185990810394}]}, {"text": "WebQSP contains questions that were originally collected for the WebQuestions dataset from web search logs).", "labels": [], "entities": [{"text": "WebQSP", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9430239200592041}, {"text": "WebQuestions dataset from web search logs", "start_pos": 65, "end_pos": 106, "type": "DATASET", "confidence": 0.9251764218012491}]}, {"text": "They were manually annotated with SPARQL queries that can be executed to retrieve the correct answer to each question.", "labels": [], "entities": []}, {"text": "Additionally, the annotators have also selected the main entity in the question that is central to finding the answer.", "labels": [], "entities": []}, {"text": "The annotations and the query use identifiers from the Freebase knowledge base.", "labels": [], "entities": [{"text": "Freebase knowledge base", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.9823325673739115}]}, {"text": "We extract all entities that are mentioned in the question from the SPARQL query.", "labels": [], "entities": []}, {"text": "For the main entity, we also store the correct span in the text, as annotated in the dataset.", "labels": [], "entities": []}, {"text": "In order to be able to use Wikidata in our experiments, we translate the Freebase identifiers to Wikidata IDs.", "labels": [], "entities": []}, {"text": "The second dataset, GraphQuestions, was created by collecting manual paraphrases for automatically generated questions ().", "labels": [], "entities": []}, {"text": "The dataset is meant to test the ability of the system to understand different wordings of the same question.", "labels": [], "entities": []}, {"text": "In particular, the paraphrases include various references to the same entity, which creates a challenge for an entity linking system.", "labels": [], "entities": []}, {"text": "The following  are three example questions from the dataset that contain a mention of the same entity: (1) a. what is the rank of marvel's iron man?", "labels": [], "entities": []}, {"text": "b. iron-man has held what ranks?", "labels": [], "entities": []}, {"text": "c. tony stark has held what ranks?", "labels": [], "entities": []}, {"text": "GraphQuestions does not contain main entity annotations, but includes a SPARQL query structurally encoded in JSON format.", "labels": [], "entities": [{"text": "GraphQuestions", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8537311553955078}]}, {"text": "The queries were constructed manually by identifying the entities in the question and selecting the relevant KB relations.", "labels": [], "entities": []}, {"text": "We extract gold entities for each question from the SPARQL query and map them to Wikidata.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9388026595115662}]}, {"text": "We split the WebQSP training set into train and development subsets to optimize the neural model.", "labels": [], "entities": [{"text": "WebQSP training set", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.9399677912394205}]}, {"text": "We use the GraphQuestions only in the evaluation phase to test the generalization power of our model.", "labels": [], "entities": []}, {"text": "The sizes of the constructed datasets in terms of the number of questions and the number of entities are reported in.", "labels": [], "entities": []}, {"text": "In both datasets, each question contains at least one correct entity mention.", "labels": [], "entities": []}, {"text": "We use precision, recall and F1 scores to evaluate and compare the approaches.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9996668100357056}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9994469285011292}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9996110796928406}]}, {"text": "We follow and and define the scores on a per-entity basis.", "labels": [], "entities": []}, {"text": "Since there are no mention boundaries for the gold entities, an extracted entity is considered correct if it is present in the set of the gold entities for the given question.", "labels": [], "entities": []}, {"text": "We compute the metrics in the micro and macro setting.", "labels": [], "entities": []}, {"text": "The macro values are computed per entity class and averaged afterwards.", "labels": [], "entities": []}, {"text": "For the WebQSP dataset, we additionally perform a separate evaluation using only the information on the main entity.", "labels": [], "entities": [{"text": "WebQSP dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9754664897918701}]}, {"text": "The main entity has the information on the boundary offsets of the correct mentions and therefore for this type of evaluation, we enforce that the extracted mention has to over-: Best configuration for the VCG model lap with the correct mention.", "labels": [], "entities": [{"text": "VCG model lap", "start_pos": 206, "end_pos": 219, "type": "DATASET", "confidence": 0.9208781123161316}]}, {"text": "QA systems need at least one entity per question to attempt to find the correct answer.", "labels": [], "entities": []}, {"text": "Thus, evaluating using the main entity shows how the entity linking system fulfills this minimum requirement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results on the WEBQSP devel- opment dataset (all entities)", "labels": [], "entities": [{"text": "WEBQSP devel- opment dataset", "start_pos": 36, "end_pos": 64, "type": "DATASET", "confidence": 0.7334243178367614}]}, {"text": " Table 3: Best configuration for the VCG model", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results on the WEBQSP test dataset, the m prefix stands for macro", "labels": [], "entities": [{"text": "WEBQSP test dataset", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9207712809244791}]}, {"text": " Table 6: Ablation experiments for the VCG model on WEBQSP", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963634610176086}, {"text": "WEBQSP", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9365987181663513}]}]}