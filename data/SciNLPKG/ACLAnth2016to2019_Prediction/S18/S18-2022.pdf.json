{"title": [{"text": "Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds", "labels": [], "entities": [{"text": "Entity Typing", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.6541601270437241}, {"text": "Adaptive Classification Thresholds", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.6907325784365336}]}], "abstractContent": [{"text": "Fine-grained entity typing is the task of assigning fine-grained semantic types to entity mentions.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7023351192474365}, {"text": "assigning fine-grained semantic types to entity mentions", "start_pos": 42, "end_pos": 98, "type": "TASK", "confidence": 0.6957241765090397}]}, {"text": "We propose a neural architecture which learns a distributional semantic representation that leverages a greater amount of semantic context-both document and sentence level information-than prior work.", "labels": [], "entities": []}, {"text": "We find that additional context improves performance, with further improvements gained by utilizing adaptive classification thresholds.", "labels": [], "entities": []}, {"text": "Experiments show that our approach without reliance on hand-crafted features achieves the state-of-the-art results on three benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity typing is the task of detecting the type (e.g., person, location, or organization) of a named entity in natural language text.", "labels": [], "entities": [{"text": "Named entity typing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6018693149089813}, {"text": "detecting the type (e.g., person, location, or organization) of a named entity in natural language text", "start_pos": 35, "end_pos": 138, "type": "TASK", "confidence": 0.762743700118292}]}, {"text": "Entity type information has shown to be useful in natural language tasks such as question answering (), knowledge-base population, and coreference resolution).", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8651912212371826}, {"text": "coreference resolution", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.9296606183052063}]}, {"text": "Motivated by its application to downstream tasks, recent work on entity typing has moved beyond standard coarse types towards finer-grained semantic types with richer ontologies (;.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.789677619934082}]}, {"text": "Rather than assuming an entity can be uniquely categorized into a single type, the task has been approached as a multi-label classification problem: e.g., in \"...", "labels": [], "entities": []}, {"text": "Monopoly is played in 114 countries.", "labels": [], "entities": [{"text": "Monopoly", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8719764351844788}]}, {"text": "...\"), \"Monopoly\" is considered both a game as well as a product.", "labels": [], "entities": []}, {"text": "The state-of-the-art approach () for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.694675400853157}]}, {"text": "These representations are then combined  with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold.", "labels": [], "entities": []}, {"text": "While this approach outperforms previous approaches which only use sparse binary features () or distributed representations, it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, ignoring their mutual connection; (2) the attention on context is computed solely upon the context, considering no alignment to the entity; (3) document-level contexts which could be useful in classification are not exploited; and (4) hand-crafted features heavily rely on system or human annotations.", "labels": [], "entities": []}, {"text": "To overcome these drawbacks, we propose a neural architecture) which learns more context-aware representations by using a better attention mechanism and taking advantage of semantic discourse information available in both the document as well as sentence-level contexts.", "labels": [], "entities": []}, {"text": "Fur-ther, we find that adaptive classification thresholds leads to further improvements.", "labels": [], "entities": [{"text": "adaptive classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6271343529224396}]}, {"text": "Experiments demonstrate that our approach, without any reliance on hand-crafted features, outperforms prior work on three benchmark datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on three publicly available datasets.", "labels": [], "entities": []}, {"text": "2 shows the statistics of these datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 3: Results on the OntoNotes dataset.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9499837160110474}]}, {"text": " Table 4: Results on the BBN dataset.", "labels": [], "entities": [{"text": "BBN dataset", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9822790622711182}]}, {"text": " Table 5: Results on the FIGER dataset.", "labels": [], "entities": [{"text": "FIGER dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.7894418239593506}]}]}