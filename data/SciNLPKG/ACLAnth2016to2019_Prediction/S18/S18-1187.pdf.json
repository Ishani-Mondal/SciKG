{"title": [{"text": "YUN-HPCC at SemEval-2018 Task 12: The Argument Reasoning Comprehension Task Using a Bi-directional LSTM with Attention Model", "labels": [], "entities": [{"text": "Argument Reasoning Comprehension Task", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7450217232108116}]}], "abstractContent": [{"text": "An argument is divided into two parts, the claim and the reason.", "labels": [], "entities": []}, {"text": "To obtain a clearer conclusion, some additional explanation is required.", "labels": [], "entities": []}, {"text": "In this task, the explanations are called warrants.", "labels": [], "entities": []}, {"text": "This paper introduces a bi-directional long short term memory (Bi-LSTM) with an attention model to select a correct warrant from two to explain an argument.", "labels": [], "entities": []}, {"text": "We address this question as a question-answering system.", "labels": [], "entities": []}, {"text": "For each warrant, the model produces a probability that it is correct.", "labels": [], "entities": []}, {"text": "Finally , the system chooses the highest correct probability as the answer.", "labels": [], "entities": []}, {"text": "Ensemble learning is used to enhance the performance of the model.", "labels": [], "entities": []}, {"text": "Among all of the participants, we ranked 15th on the test results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reasoning is an important part of human logical thinking.", "labels": [], "entities": []}, {"text": "It gives us the ability to draw fresh conclusions from some of the known points.", "labels": [], "entities": []}, {"text": "Argument is the basis for reasoning.", "labels": [], "entities": []}, {"text": "Except for the argument's claim and reason, usually, it needs some additional information.", "labels": [], "entities": []}, {"text": "Therefore, what we know is the additional information and arguments reason.", "labels": [], "entities": []}, {"text": "The claim also needs warrants for an explanation.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "Obviously, A is a reasonable explanation.", "labels": [], "entities": [{"text": "A", "start_pos": 11, "end_pos": 12, "type": "METRIC", "confidence": 0.9954646229743958}]}, {"text": "The task is to get the reader to find a reasonable explanation for the known messages and claims in the two warrants.", "labels": [], "entities": []}, {"text": "Due to the small number of alternative warrants, this problem can be considered to be a binary classification problem.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6941880285739899}]}, {"text": "This idea can be used as the baseline model.", "labels": [], "entities": []}, {"text": "However, for system scalability and effectiveness, we treat this problem as the regression problem of probability prediction.", "labels": [], "entities": [{"text": "probability prediction", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7566692233085632}]}, {"text": "The idea calculates the probability for each warrant that it is correct.", "labels": [], "entities": []}, {"text": "Because of the diversity of natural language expression, there are many ways in which the same meaning can be expressed.", "labels": [], "entities": []}, {"text": "Thus, this approach can be better to address this situation).", "labels": [], "entities": []}, {"text": "Another benefit of addressing the problem in this way is to make the problem similar inform to the multi-choice question-answering system.", "labels": [], "entities": []}, {"text": "The question-answering system is a classic problem of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6552907625834147}]}, {"text": "Many methods and models can be used for reference.", "labels": [], "entities": []}, {"text": "The traditional question-answering system is based on semantic and statistical methods).", "labels": [], "entities": []}, {"text": "This method requires an enormous background knowledge base.", "labels": [], "entities": []}, {"text": "In addition, it is not very effective for nonstandard language expression.", "labels": [], "entities": []}, {"text": "The state-of-the-art methods are usually based on neural networks.", "labels": [], "entities": []}, {"text": "The trained word embedding can fully express the semantics and knowledge.", "labels": [], "entities": []}, {"text": "Therefore, the new method is usually better than the traditional statistical-based method.", "labels": [], "entities": []}, {"text": "In this paper, we proposed a bi-directional L-STM with an attention model.", "labels": [], "entities": []}, {"text": "The model uses a bi-LSTM network to encode the original word embedding.", "labels": [], "entities": []}, {"text": "Then, the semantic outputs are fed into the dense decoder with an attention mechanism.", "labels": [], "entities": []}, {"text": "Due to the uncertainty of a single model, ensemble learning is used to enhance the performance of the model.", "labels": [], "entities": []}, {"text": "The remainder of the paper consists of 3 parts.", "labels": [], "entities": []}, {"text": "The second part introduces the proposed model in detail, and the implementation is presented in the third part, while the last part presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment contains three parts.", "labels": [], "entities": []}, {"text": "The first part is the selection and preprocess of experimental data.", "labels": [], "entities": [{"text": "preprocess", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9337920546531677}]}, {"text": "The second part is the implementation details.", "labels": [], "entities": []}, {"text": "The third part is to show and analyze the results.", "labels": [], "entities": []}, {"text": "The training corpus of the word vector, the training set of the model and the test set must be selected and processed.", "labels": [], "entities": []}, {"text": "As mentioned above, reading comprehension focuses more on semantic understanding (), so GoogleNews is a good choice.", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7414077818393707}]}, {"text": "Because news reports use more cautious words and more rigorous grammar.", "labels": [], "entities": []}, {"text": "The mainstream word vector training tools are Word2Vec or GloVe.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9705874919891357}]}, {"text": "According to previous experimental results of related tasks, Word2Vec trained vector of words significantly better than GloVe ().", "labels": [], "entities": [{"text": "GloVe", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.7418000102043152}]}, {"text": "Thus, in this experiment, Word2vec was chosen to train the word vector.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9356611967086792}]}, {"text": "The form of the task data is complicated.", "labels": [], "entities": []}, {"text": "It is more difficult to obtain the data by artificial generation or online acquisition.", "labels": [], "entities": []}, {"text": "Thus, the training data and test data are given by the official data set.", "labels": [], "entities": [{"text": "official data set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.8771141370137533}]}, {"text": "Each row of the test data set is divided into several sections, including the id, topic, additional information, reason, claim, warrant0, warrant1 and label.", "labels": [], "entities": []}, {"text": "For each row of data, it is processed into two test data of the model.", "labels": [], "entities": []}, {"text": "Each training data contains four parts.", "labels": [], "entities": []}, {"text": "They are fact, warrant, claim and label.", "labels": [], "entities": []}, {"text": "Here, fact is the original topic, with additional information of reason.", "labels": [], "entities": []}, {"text": "Warrant, claim and label are not changed.", "labels": [], "entities": [{"text": "Warrant", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8522645235061646}, {"text": "label", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.8929317593574524}]}, {"text": "Because there are two warrants in one line, it generates two training data.", "labels": [], "entities": []}, {"text": "This approach is similar to a question-answer system, where claim is the question, and warrant is the answer.", "labels": [], "entities": []}, {"text": "Additionally, model is used to predict whether this answer is correct for the question.", "labels": [], "entities": []}, {"text": "Because English words have some special forms, such as past tense, past participle, abbreviation and soon, the lemmatisation is needed).", "labels": [], "entities": [{"text": "abbreviation", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.9613538980484009}]}], "tableCaptions": [{"text": " Table 5: Results of Ensemble Learning.", "labels": [], "entities": []}, {"text": " Table 4: Results of Parameter Tuning of Single Model.", "labels": [], "entities": [{"text": "Parameter Tuning", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.8832756876945496}]}]}