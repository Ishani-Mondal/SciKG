{"title": [{"text": "DUTH at SemEval-2018 Task 2: Emoji Prediction in Tweets", "labels": [], "entities": [{"text": "DUTH", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6091461777687073}, {"text": "Emoji Prediction in Tweets", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7916117608547211}]}], "abstractContent": [{"text": "This paper describes the approach that was developed for SemEval 2018 Task 2 (Multilin-gual Emoji Prediction) by the DUTH Team.", "labels": [], "entities": [{"text": "SemEval 2018 Task 2 (Multilin-gual Emoji Prediction)", "start_pos": 57, "end_pos": 109, "type": "TASK", "confidence": 0.7271503143840365}, {"text": "DUTH Team", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.8665300011634827}]}, {"text": "First, we employed a combination of pre-processing techniques to reduce the noise of tweets and produce a number of features.", "labels": [], "entities": []}, {"text": "Then, we built several N-grams, to represent the combination of word and emojis.", "labels": [], "entities": []}, {"text": "Finally, we trained our system with a tuned LinearSVC classifier.", "labels": [], "entities": []}, {"text": "Our approach in the leaderboard ranked 18th amongst 48 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emojis are used in everyday life to express words or feelings of microblogging users.", "labels": [], "entities": []}, {"text": "They are commonly placed at the end of a sentence or alone.", "labels": [], "entities": []}, {"text": "In this paper, we show how our emoji prediction framework was applied to SemEval-2018 Task 2 (Multilingual Emoji Prediction) (, specifically on Subtask 1 (Emoji Prediction in English).", "labels": [], "entities": [{"text": "emoji prediction", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8594375848770142}, {"text": "SemEval-2018 Task 2 (Multilingual Emoji Prediction)", "start_pos": 73, "end_pos": 124, "type": "TASK", "confidence": 0.5704977996647358}]}, {"text": "In the last few years, many studies concentrated on emoji prediction and analysis.", "labels": [], "entities": [{"text": "emoji prediction and analysis", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.8314369171857834}]}, {"text": "The prediction of emojis, the connection of emojis and words, and their separation from content-based tweet messages, based on Long ShortTerm Memory networks (LSTMs), was examined by.", "labels": [], "entities": []}, {"text": "The combination of emojis and sentiment was investigated by, who developed the first emoji sentiment lexicon and created a sentiment map of the 751 most frequently used emojis.", "labels": [], "entities": []}, {"text": "The study of tested several skip-gram word embedding models to measure the difference in performance between machine-learning models and human annotation.", "labels": [], "entities": []}, {"text": "analyzed the viability of a trained classifier to differentiate between those emojis utilized as semantic substance words and those utilized as paralinguistic or emotional multimodal markers.", "labels": [], "entities": []}, {"text": "investigated the hypothesis of previous works that emojis in their regular textual contexts would generously reduce and lead to miscommunication, but they found no such evidence; the potential for miscommunication appeared to be the same.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the architecture of our system and the dataset.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss the various parameters that were used to fine-tune the system, and present the performance of our framework.", "labels": [], "entities": []}, {"text": "In Section 4, we layout our main conclusions and research issues for further investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training and testing datasets were provided by the organizers.", "labels": [], "entities": []}, {"text": "The training set contained approximately 500, 000 tweets, where each tweet contained a single emoji, before they removed it and set it as class label.", "labels": [], "entities": []}, {"text": "That emoji is used as the class label for the particular tweet.", "labels": [], "entities": []}, {"text": "We extracted various statistics for the dataset as it can be seen in.", "labels": [], "entities": []}, {"text": "Some class labels contain more sentences per tweet, like label 10 ( ) and 0 ( ).", "labels": [], "entities": []}, {"text": "We also observe that the emoji has on average much fewer hashtags per tweet, while the emoji has much more.", "labels": [], "entities": []}, {"text": "All the other emojis range within reasonable limits.", "labels": [], "entities": []}, {"text": "The emojis with labels 7 ( ) and 3 ( ) are expressed using more words on average, while the emojis 10 ( ) and 11 ( ) are expressed with fewer words.", "labels": [], "entities": []}, {"text": "All the above observations are important to understand the dataset and how people are using each emoji.", "labels": [], "entities": []}, {"text": "One can use these statistics in order to create more features and test them to seethe changes in classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.9186185598373413}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8922959566116333}]}, {"text": "For example, one can count the words of each new sentence for classification, and compare them with the ones derived from the training dataset.", "labels": [], "entities": [{"text": "classification", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9622774124145508}]}, {"text": "In our study, we compared several machine learning algorithms (Ridge, Logistic Regression, Passive-Aggressive, and Linear SVC), and three different word to vector representations (tf-idf Vectorizer, count Vectorizer, and hashing Vectorizer).", "labels": [], "entities": []}, {"text": "The macro F-measure score was computed for 10-folds cross-validation on the training set and on the trial set while using the training set for training.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9350698590278625}]}, {"text": "We employed word n-grams and character n-grams (n ranging from 1 to 4), with the latter ones performing poorly.", "labels": [], "entities": []}, {"text": "In this section, we describe the different classifiers and vectorizers used and present our results.: Results per classifier and vectorizer using 10-fold unigrams.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our system with the macro F-measure score.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.884779691696167}]}, {"text": "The macro F-measure score gives equal weight to each emoji category, regardless of its class size.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9030455648899078}]}, {"text": "The F-measure per emoji class is the harmonic mean of the precision and recall of the class: The macro-average F-measure score is obtained by taking the average of F-measure values across emoji classes: where M is the total number of classes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9668925404548645}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9990221261978149}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9955947995185852}]}, {"text": "In we present the macro F-measure score of tfidfVectorizer combined with LinearSVC classification algorithm.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9199141561985016}]}, {"text": "In the first column, the results of 10-folds cross validation on the training set are presented.", "labels": [], "entities": []}, {"text": "In the second column we present the results when training with the training data and testing with the trial data.", "labels": [], "entities": []}, {"text": "As it can be seen, four-grams performance on trial data has the highest value, but trigrams perform better on 10-folds cross-validation.", "labels": [], "entities": []}, {"text": "This is the reason we used trigrams to train our model for the submitted runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics per emoji.", "labels": [], "entities": []}, {"text": " Table 2: Results per classifier and vectorizer using 10-fold unigrams.", "labels": [], "entities": []}, {"text": " Table 3: F-measure results for word N-grams.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9948968291282654}]}]}