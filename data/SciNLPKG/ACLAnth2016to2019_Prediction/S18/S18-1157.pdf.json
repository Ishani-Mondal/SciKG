{"title": [{"text": "THU NGN at SemEval-2018 Task 10: Capturing Discriminative Attributes with MLP-CNN model", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.8912042578061422}]}], "abstractContent": [{"text": "Existing semantic models are capable of identifying the semantic similarity of words.", "labels": [], "entities": []}, {"text": "However , it's hard for these models to discriminate between a word and another similar word.", "labels": [], "entities": []}, {"text": "Thus, the aim of SemEval-2018 Task 10 is to predict whether a word is a discrimina-tive attribute between two concepts.", "labels": [], "entities": [{"text": "SemEval-2018 Task 10", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8719142079353333}]}, {"text": "In this task, we apply a multilayer perceptron (MLP)-convolutional neural network (CNN) model to identify whether an attribute is discriminative.", "labels": [], "entities": []}, {"text": "The CNNs are used to extract low-level features from the inputs.", "labels": [], "entities": []}, {"text": "The MLP takes both the flatten CNN maps and inputs to predict the labels.", "labels": [], "entities": []}, {"text": "The evaluation F-score of our system on the test set is 0.629 (ranked 15th), which indicates that our system still needs to be improved.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9101060628890991}]}, {"text": "However, the behaviours of our system in our experiments provide useful information , which can help to improve the collective understanding of this novel task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating the similarity of words is an important task in semantic modeling.", "labels": [], "entities": [{"text": "Evaluating the similarity of words", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.850230050086975}, {"text": "semantic modeling", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8850441575050354}]}, {"text": "There have been different approaches based on corpus statistics) and ontology (.", "labels": [], "entities": []}, {"text": "After an effective word representation proposed by mikolov et al, word similarity can be evaluated based on word embedding weights (.", "labels": [], "entities": []}, {"text": "Usually higher cosine similarity of word embedding vectors indicates higher semantic similarity.", "labels": [], "entities": []}, {"text": "However, existing semantic methods are not capable of discriminating similar words between each other without additional information.", "labels": [], "entities": []}, {"text": "For example, it is easy for these models to tell \"dog\" and \"puppy\" is similar, but they can't tell the differences between each other.", "labels": [], "entities": []}, {"text": "It limits the use of these models to mine such fine-grained semantic information from texts.", "labels": [], "entities": []}, {"text": "Thus, the SemEval-2018 Task 10 is proposed to determine whether an attribute can help to discriminate between two words(.", "labels": [], "entities": []}, {"text": "One can express semantic differences between concepts by referring to attributes associated with those concepts.", "labels": [], "entities": []}, {"text": "The differences between concepts can usually be identified by the presence or absence of specific attributes.", "labels": [], "entities": []}, {"text": "For example, the attributes \"red\" and \"yellow\" are discriminative for concepts \"apple\" and \"banana\", while \"sweet\" or \"fruit\" are not discriminative.", "labels": [], "entities": []}, {"text": "Capturing such discriminative attributes can be regarded as a binary classification task: given two words and an attribute, predict whether the attribute is a difference between the two words.", "labels": [], "entities": []}, {"text": "Existing methods to capture discriminative attributes are mainly based on dictionary).", "labels": [], "entities": []}, {"text": "In recent years, CNN have been successfully applied to text classification task.", "labels": [], "entities": [{"text": "CNN", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7649486064910889}, {"text": "text classification task", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8924600879351298}]}, {"text": "In order to address this task, we develop a system based on MLP-CNN model.", "labels": [], "entities": []}, {"text": "Firstly, the input words will be converted into dense vectors using the combination of different word embeddings.", "labels": [], "entities": []}, {"text": "Then the CNN layers are used to extract features from these vectors.", "labels": [], "entities": []}, {"text": "Finally, a MLP classifier is used to predict binary labels based on both embedding and CNN features.", "labels": [], "entities": []}, {"text": "The experimental results show that our model outperforms several baseline neural model, and the additional features can improve the model performance.", "labels": [], "entities": []}, {"text": "Our system still has room for development according to the experimental analysis.", "labels": [], "entities": []}, {"text": "The behaviours of our system in our experiments can help to further fix and extend our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we use is constructed based on the approach proposed by and the initial source of data is provided by.", "labels": [], "entities": []}, {"text": "The entire dataset contains 17,547 samples for training, 2,722 for validation and 2,340 for testing.", "labels": [], "entities": []}, {"text": "The training set is automatically generated, while the validation and test set are manually refined.", "labels": [], "entities": []}, {"text": "The models will be evaluated by F1-measure, as is standard in a binary classification task.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9989132881164551}]}, {"text": "In our network, the kernel sizes of CNN are set to 3.", "labels": [], "entities": []}, {"text": "The dimensions of feature maps v 2 and v 3 are set to 256, and the dimensions of dense layers are 300.", "labels": [], "entities": []}, {"text": "The dropout rate of both embedding weights and CNN is set to 0.2.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9691000878810883}, {"text": "CNN", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8672001361846924}]}, {"text": "The training batch size is set to 50.", "labels": [], "entities": []}, {"text": "We use Adam as the optimizer for network training, which takes 10 epochs per time.", "labels": [], "entities": []}, {"text": "We train our model for 10 times and average their predictions on the test set.", "labels": [], "entities": []}, {"text": "The experimental results on the test and validation set are shown in.", "labels": [], "entities": []}, {"text": "For comparison, we also present several baseline models here.", "labels": [], "entities": []}, {"text": "Our official submission is the MLP-CNN model with ensemble techniques.", "labels": [], "entities": []}, {"text": "Our F-score is 62.9 (ranked 15th) in the evaluation phase.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9992020726203918}]}, {"text": "From the evaluation results, we can see that our model outperforms these baseline models.", "labels": [], "entities": []}, {"text": "It shows that our network architecture can learn more semantic information from the words and attributes.", "labels": [], "entities": []}, {"text": "However, our system needs to be improved compared with the top system (75.0 of F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9979047775268555}]}, {"text": "In addition, the testing results are much lower than validation results.", "labels": [], "entities": []}, {"text": "Some detailed information will be analysis in the next section.", "labels": [], "entities": []}, {"text": "73.4: Performance evaluation of our system and several baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance evaluation of our system and sev- eral baselines.", "labels": [], "entities": []}, {"text": " Table 2: Comparisons of using different pre-trained  embedding.", "labels": [], "entities": []}, {"text": " Table 3: Influence of word features on the test set.", "labels": [], "entities": []}]}