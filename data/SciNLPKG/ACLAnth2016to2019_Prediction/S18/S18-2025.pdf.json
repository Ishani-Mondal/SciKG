{"title": [], "abstractContent": [{"text": "Existing methods of hypernymy detection mainly rely on statistics over a big corpus, either mining some co-occurring patterns like \"animals such as cats\" or embedding words of interest into context-aware vectors.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8790257573127747}]}, {"text": "These approaches are therefore limited by the availability of a large enough corpus that can coverall terms of interest and provide sufficient contextual information to represent their meaning.", "labels": [], "entities": []}, {"text": "In this work, we propose anew paradigm, HYPERDEF, for hypernymy detection expressing word meaning by encoding word definitions, along with context driven representation.", "labels": [], "entities": [{"text": "hypernymy detection expressing word meaning", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.8750648498535156}]}, {"text": "This has two main benefits: (i) Definitional sentences express (sense-specific) corpus-independent meanings of words, hence definition-driven approaches enable strong generalization-once trained, the model is expected to work well in open-domain testbeds; (ii) Global context from a large corpus and definitions provide complementary information for words.", "labels": [], "entities": []}, {"text": "Consequently, our model, HYPERDEF, once trained on task-agnostic data, gets state-of-the-art results in multiple benchmarks 1 .", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.764164388179779}]}], "introductionContent": [{"text": "Language understanding applications like textual entailment (), question answering () and relation extraction (, benefit from the identification of lexical entailment relations.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6817850172519684}, {"text": "question answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8635603785514832}, {"text": "relation extraction", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8597866892814636}, {"text": "identification of lexical entailment relations", "start_pos": 130, "end_pos": 176, "type": "TASK", "confidence": 0.7546942949295044}]}, {"text": "Lexical inference encompasses several semantic relations, with hypernymy being one of the prevalent), an i.e., \"Is-A\" relation that holds fora pair of terms 2 (x, y) for specific terms' senses.", "labels": [], "entities": [{"text": "Lexical inference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8594098687171936}]}, {"text": "Two families of approaches have been studied for identifying term hypernymy.", "labels": [], "entities": [{"text": "identifying term hypernymy", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.7125548720359802}]}, {"text": "(i) Pattern match-ing exploits patterns such as \"animals such as cats\" to indicate a hypernymy relation from \"cat\" to \"animal\").", "labels": [], "entities": [{"text": "Pattern match-ing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.926560640335083}]}, {"text": "However, it requires the co-occurrence of the two terms in the same sentence, which limits the recall of this method; (ii) Term representation learning depends on a vector embedding of each term, where each entry in the vector expresses an explicit context feature () or a latent semantic (.", "labels": [], "entities": [{"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9928880333900452}, {"text": "Term representation learning", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.9214195013046265}]}, {"text": "Both approaches hinge on acquiring contextaware term meaning in a large corpus.", "labels": [], "entities": []}, {"text": "The generalization of these corpus-based representation learning paradigms, however, is limited due to the domain specificity of the training data.", "labels": [], "entities": []}, {"text": "For example, an IT corpus hardly mentions \"apple\" as a fruit.", "labels": [], "entities": []}, {"text": "Furthermore, the surrounding context of a term may not convey subtle differences in term meaning -\"he\" and \"she\" have highly similar context that may not reveal the important difference between them.", "labels": [], "entities": []}, {"text": "Moreover, rare words are poorly expressed by their sparse global context and, more generally, these methods would not generalize to the low resource language setting.", "labels": [], "entities": []}, {"text": "Humans can easily determine the hypernymy relation between terms even for words they have not been exposed to a lot, given a definition of it in terms of other words.", "labels": [], "entities": []}, {"text": "For example, one can imagine a \"teaching\" scenario that consists of defining a term, potentially followed by a few examples of the term usage in text.", "labels": [], "entities": []}, {"text": "Motivated by these considerations and the goal of eventually develop an approach that could generalize to unseen words and even to the low resource languages scenario, we introduce the following hypernymy detection paradigm, HYPER-DEF, where we augment distributional contextual models with that of learning terms representations from their definitions.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7247492074966431}, {"text": "HYPER-DEF", "start_pos": 225, "end_pos": 234, "type": "METRIC", "confidence": 0.7768406867980957}]}, {"text": "This paradigm has an important advantage in its powerful generalization, as definitions are agnostic to specific domains and benchmarks, and are equally available for words regardless of their frequency in a given data set.", "labels": [], "entities": []}, {"text": "Consequently, the task of identifying the relation between two terms is enhanced by the knowledge of the terms' definitions.", "labels": [], "entities": []}, {"text": "Our model can be applied to any new terms in any domain, given some context of the term usage and their domainagnostic definitions.", "labels": [], "entities": []}, {"text": "Moreover, given our learning approach -we learn also the notion of lexical entailment between terms -we can generalize to any lexical relation between terms.", "labels": [], "entities": []}, {"text": "Technically, we implement HYPERDEF by modifying the AttentiveConvNet ( , a top-performing system on a textual entailment benchmark, to model the input (x, d x ; y, d y ), where d i (i = x, y) is the definition of term i.", "labels": [], "entities": []}, {"text": "In contrast to earlier work which mostly built separate representations for terms x and y, HYPERDEF instead directly models the representation for each pair in {(x, y), (x, d y ), (d x , y), (d x , d y )}, and then accumulates the four-way representations to form an overall representation for the input.", "labels": [], "entities": []}, {"text": "In our experiments, we train HYPERDEF on a task-agnostic annotated dataset, Wordnet, and test it on abroad array of open-domain hypernymy detection datasets.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9702123999595642}]}, {"text": "The results show the outstanding performance and strong generalization of the HY-PERDEF model.", "labels": [], "entities": [{"text": "HY-PERDEF", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.7371255159378052}]}, {"text": "Overall, our contributions are as follows: \u2022 To our knowledge, this is the first work in hypernymy detection that makes use of term definitions.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8860748410224915}]}, {"text": "Definitions provide complementary knowledge to distributional context, so that our model better tolerates unseen words, rare words and words with biased sense distribution.", "labels": [], "entities": []}, {"text": "\u2022 HYPERDEF accounts for word sense when inferring the hypernymy relation.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9810224175453186}, {"text": "word sense", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.6873464733362198}]}, {"text": "This differs from much of the literature, which usually derives sense-unaware representative vectors for terms -earlier approaches would say 'yes' if the relation holds for some combination of the terms' senses.", "labels": [], "entities": []}, {"text": "\u2022 HYPERDEF has strong generalization capability -once trained on a task-agnostic definition dataset, it can be used in different testbeds, and shows state-of-the-art results.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.7000893950462341}]}], "datasetContent": [{"text": "First, we use four widely-explored datasets: BLESS (Baroni and Lenci, 2011), EVALution (,).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9968717694282532}]}, {"text": "They were constructed either using knowledge resources (e.g. WordNet, Wikipedia), crowd-sourcing or both.", "labels": [], "entities": []}, {"text": "The instance sizes of hypernymy and \"other\" relation types are detailed in.", "labels": [], "entities": []}, {"text": "We also report \"#OOV pair\", the proportions of unseen term pairs in above four datasets regarding the training set of HYPERDEF, i.e., wn train in Section 4.1.", "labels": [], "entities": [{"text": "OOV pair", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9820487201213837}, {"text": "HYPERDEF", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.6024688482284546}]}, {"text": "We notice that most term pairs in BLESS and Lenci/Benotto datasets are unseen in wn train.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9240477681159973}, {"text": "Lenci/Benotto datasets", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.620739758014679}]}, {"text": "First, we extract the term's all sense definitions from WordNet based on the term string.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9823797941207886}]}, {"text": "For a few instances which contain terms not covered by WordNet, such as proper noun \"you\", \"everybody\" etc, we set definitions the same as the term strings (this preprocessing does not influence results, just for making the system uniformed).: Statistics of four benchmarks.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.965411901473999}]}, {"text": "\"#OOV pair\": the proportions of unseen term pairs regarding the training set (i.e., wn train in Section 4.1) of HYPER-DEF.", "labels": [], "entities": [{"text": "OOV", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9897524118423462}]}, {"text": "Then, we apply the pre-trained HYPERDEF model on the test sets of the four benchmarks, discriminating hypernymy from \"other\" relations.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.949029803276062}]}, {"text": "AP and AP@100 are reported.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6260156035423279}, {"text": "AP", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.6691551804542542}]}, {"text": "As WordNet sorts sense definitions by sense frequency, we test the term pairs in two ways: (i) Only choose the top-1 sense definition to denote a term, reported as \"HYPERDEF T opDef \"; (ii) Keep all sense definitions for those terms, then test on all sense combinations and pick the highest probability as the term pair score, reported as\"HYPERDEF AllDef \".", "labels": [], "entities": [{"text": "HYPERDEF T opDef", "start_pos": 165, "end_pos": 181, "type": "METRIC", "confidence": 0.8366307814915975}, {"text": "HYPERDEF AllDef", "start_pos": 339, "end_pos": 354, "type": "METRIC", "confidence": 0.8663893342018127}]}, {"text": "We compare HYPERDEF with baselines: (i) Best-Unsuper.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.977294921875}]}, {"text": "The best unsupervised method in (, implemented by similarity measurement over weighted dependencybased context vectors; (ii) Concat-SVM ().", "labels": [], "entities": []}, {"text": "An SVM model with RBF kernel is trained on concatenation of unspecialized concept embeddings (); (iii) DUAL-T (.", "labels": [], "entities": [{"text": "DUAL-T", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.5288023352622986}]}, {"text": "Using dual tensors, DUAL-T transforms unspecialized embeddings into asymmetrically specialized representations -sets of specialized vectors -which are next used to predict whether the asymmetric relation holds between the concepts; (iv) HyperScore (.", "labels": [], "entities": [{"text": "HyperScore", "start_pos": 237, "end_pos": 247, "type": "DATASET", "confidence": 0.9199381470680237}]}, {"text": "It uses a large-scale hypernymy pair set to guide the learning of hierarchical word embeddings in hypernymy-structured space.", "labels": [], "entities": []}, {"text": "clearly demonstrates the superiority of our HYPERDEF models over other systems.", "labels": [], "entities": []}, {"text": "The three baselines Concat-SVM, DUAL-T and HyperScore are more inline with HYPERDEF since they did supervised learning overlarge numbers of annotated pairs.", "labels": [], "entities": []}, {"text": "HYPERDEF integrates term definitions, which is shown effective in improving the performance across different testbeds.", "labels": [], "entities": []}, {"text": "In addition, HYPERDEF AllDef consistently outperforms HYPERDEF T opDef . This makes sense as HYPERDEF T opDef maybe misled by incorrect definitions.", "labels": [], "entities": [{"text": "HYPERDEF AllDef", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.7197884619235992}]}, {"text": "In addition, the superiority of HY-PERDEF AllDef clearly supports the effectiveness of HYPERDEF in dealing with polysemy cases.", "labels": [], "entities": [{"text": "HY-PERDEF AllDef", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.6100868433713913}, {"text": "HYPERDEF", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.8467814326286316}]}, {"text": "Above four benchmarks are relatively small and contain common words mostly.", "labels": [], "entities": []}, {"text": "In real-world applications, there is a need to figure out the hypernymy relation between common nouns and proper nouns.", "labels": [], "entities": []}, {"text": "Taking \"(Champlin, city)\" for example, \"Champlin\" is not covered by WordNet vocabulary, thus uncovered by wn train -the training data of our HYPERDEF model.", "labels": [], "entities": []}, {"text": "Motivated, we further test HYPERDEF on the following dataset.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9801725745201111}]}, {"text": "Similarly, HypeNet dataset has \"random split\" and \"lexical split\" as well; their sizes are list in.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9555597901344299}]}, {"text": "HypeNet contains lots of locations, e.g., (Champlin, city), and organizations, e.g., and.", "labels": [], "entities": [{"text": "HypeNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8404155969619751}]}, {"text": "We first try to extract definitions for those terms from WordNet, if fail, then we extract from Wikipedia pages, treating the first sentence as a definition.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9767613410949707}]}, {"text": "We play HYPERDEF in two different ways, one testing its \"pre-trained\" model on HypeNet's test data, the other -\"specialized\" -training HYPER-DEF on HypeNet's training data then test on HypeNet's test data like other baselines did.", "labels": [], "entities": [{"text": "HypeNet's test data", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.8163747638463974}, {"text": "HypeNet's training data", "start_pos": 148, "end_pos": 171, "type": "DATASET", "confidence": 0.7020802944898605}, {"text": "HypeNet's test data", "start_pos": 185, "end_pos": 204, "type": "DATASET", "confidence": 0.8354463428258896}]}, {"text": "shows: (i) If trained on the specific training data of HypeNet, our system HYPER-DEF can get state of the art performance.", "labels": [], "entities": [{"text": "HypeNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9084332585334778}]}, {"text": "This indicates the superiority of our model over baseline systems.", "labels": [], "entities": []}, {"text": "(i) Our pretrained HYPERDEF model performs less satisfactorily.", "labels": [], "entities": [{"text": "HYPERDEF", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8142656087875366}]}, {"text": "Only the result on \"Lex.", "labels": [], "entities": [{"text": "Lex", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.7501505017280579}]}, {"text": "split\" is relatively close to the outstanding baselines.", "labels": [], "entities": [{"text": "split", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.60835200548172}]}, {"text": "This makes sense as baseline systems are specified by the HypeNet training set while our pretrained model comes from a different domain.", "labels": [], "entities": [{"text": "HypeNet training set", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9117128849029541}]}, {"text": "We studied the dataset and found following problems.", "labels": [], "entities": []}, {"text": "Two error sources are observed.", "labels": [], "entities": []}, {"text": "(i) Wrong definition.", "labels": [], "entities": []}, {"text": "For example, the system obtains the definition \"a substance or treatment with no active therapeutic effect\" for the term \"Placebo\" in the pair; however, a successful detection requires mining an-  Benotto Weeds Model AP AP@100 AP AP@100 AP AP@100 AP AP@100 Best-Unsuper ( . .097 . .", "labels": [], "entities": [{"text": "AP AP@100 AP AP@100 AP AP@100 AP AP@100 Best-Unsuper", "start_pos": 217, "end_pos": 269, "type": "METRIC", "confidence": 0.5508882508558386}]}, {"text": "other definition -\"are an alternative rock band, formed in London, England in 1994 by singerguitarist Brian Molko and guitarist-bassist Stefan Olsdal\" which depicts the article title \"Placebo (band)\".", "labels": [], "entities": []}, {"text": "This is a common problem due to the ambiguity of entity mentions.", "labels": [], "entities": []}, {"text": "To relieve this, we plan to refine the definition retrieval by more advanced entity linking techniques, or retrieve all highly related definitions and test as in polysemy cases (recall that in we showed HYPERDEF has more robust performance while addressing polysemy terms); (ii) Misleading information in definitions.", "labels": [], "entities": [{"text": "definition retrieval", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.776513546705246}]}, {"text": "Our system predicts \"1\" for the pair (Aurangabad, India); we analyze the definition of \"Aurangabad\": is a city in the Aurangabad district of Maharashtra state in India.", "labels": [], "entities": []}, {"text": "We intentionally removed the phrase \"in India\", and then the system predicts \"0\".", "labels": [], "entities": []}, {"text": "This demonstrates that definitions indeed provide informative knowledge about terms, but a system must be intelligent to avoid being misled; (iii) We miss a common embedding space to initialize single words and (multi-word) entities.", "labels": [], "entities": []}, {"text": "To generalize well to new entities, the model has to presume the new entities and the known terms lie in the same representation space.", "labels": [], "entities": []}, {"text": "However, most pretrained embedding sets cover pretty limited entities.", "labels": [], "entities": []}, {"text": "To learn uniformed word and entity embeddings, we may need to combine unstructured text corpus, semi-structured data (e.g., Wikipedia) and structured knowledge bases together.", "labels": [], "entities": []}, {"text": "We will advance this data preprocessing component -the access of term definitions and term representationsin our released system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tune HyperDef on wn dev", "labels": [], "entities": []}, {"text": " Table 3: Statistics of four benchmarks. \"#OOV pair\":  the proportions of unseen term pairs regarding the  training set (i.e., wn train in Section 4.1) of HYPER- DEF.", "labels": [], "entities": [{"text": "OOV pair", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9759688675403595}, {"text": "HYPER- DEF", "start_pos": 155, "end_pos": 165, "type": "TASK", "confidence": 0.4720864991346995}]}, {"text": " Table 4: System comparison on BLESS, EVALution, Benotto and Weeds datasets", "labels": [], "entities": [{"text": "BLESS", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.994945228099823}, {"text": "EVALution", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.6626670360565186}, {"text": "Weeds datasets", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.7662701308727264}]}, {"text": " Table 5: Statistics of HypeNet dataset. \"#OOV pair\" is  for \"test\" regarding the \"wn train\" of HYPERDEF.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9034948348999023}, {"text": "OOV pair", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9792886078357697}]}, {"text": " Table 6: System comparison on HypeNet test", "labels": [], "entities": []}]}