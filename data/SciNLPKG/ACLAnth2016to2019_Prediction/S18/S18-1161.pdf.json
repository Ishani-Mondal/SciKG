{"title": [], "abstractContent": [{"text": "In this paper we report an unsupervised method aimed to identify whether an attribute is discriminative for two words (which are treated as concepts, in our particular case).", "labels": [], "entities": []}, {"text": "To this end, we use geometrically inspired vector operations underlying unsupervised decision functions.", "labels": [], "entities": []}, {"text": "These decision functions operate on state-of-the-art neural word embeddings of the attribute and the concepts.", "labels": [], "entities": []}, {"text": "The main idea can be described as follows: if attribute q discriminates concept a from concept b, then q is excluded from the feature set shared by these two concepts: the intersection.", "labels": [], "entities": []}, {"text": "That is, the membership q \u2208 (a \u2229 b) does not hold.", "labels": [], "entities": []}, {"text": "As a, b, q are represented with neural word embeddings, we tested vector operations allowing us to measure membership, i.e. fuzzy set operations (t-norm, for fuzzy intersection, and t-conorm, for fuzzy union) and the similarity between q and the convex cone described by a and b.", "labels": [], "entities": []}], "introductionContent": [{"text": "There exist nowadays a number of arithmetic vector operations for computing word relationships interpreted as linguistic regularities.", "labels": [], "entities": [{"text": "computing word relationships interpreted as linguistic regularities", "start_pos": 66, "end_pos": 133, "type": "TASK", "confidence": 0.632913772548948}]}, {"text": "Avery popular setting is solving word analogies, which is mainly used to evaluate the quality of word embeddings (.", "labels": [], "entities": [{"text": "solving word analogies", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7563645541667938}]}, {"text": "Recently other alternatives to solve word analogies have been proposed, including supervised methods (.", "labels": [], "entities": []}, {"text": "Solving word analogies requires three word arguments, and a fourth one is inferred.", "labels": [], "entities": []}, {"text": "Such an inference raises from the similarity between common or similar contexts shared by the two pairs of words.", "labels": [], "entities": []}, {"text": "Thus, given words \"queen\", \"woman\", \"king\", \"man\", the following arithmetic operation holds for their corresponding embeddings x (\u00b7) : x king \u2212 x man + x woman = x queen . In this work, we explore similar approaches for Discriminative Attribute Identification (DAI).", "labels": [], "entities": [{"text": "Discriminative Attribute Identification (DAI)", "start_pos": 220, "end_pos": 265, "type": "TASK", "confidence": 0.8054949641227722}]}, {"text": "This task requires tree word arguments a, b, q, and a binary label y \u2208 {0, 1} is inferred from them ().", "labels": [], "entities": []}, {"text": "Such a label indicates whether the third word, q, is identified as a discriminative (semantic) attribute between words (concepts) a, b.", "labels": [], "entities": []}, {"text": "We observed that the task of identifying discriminative attributes between words, represented via word embeddings, evokes that of solving word analogies.", "labels": [], "entities": []}, {"text": "We propose geometrically inspired vector operations on word embeddings x a , x b , x q \u2208 Rn of the words a, b, q, respectively.", "labels": [], "entities": []}, {"text": "The output of each of these operations is in turn operated by a unsupervised decision function aimed to predict the label y.", "labels": [], "entities": []}, {"text": "The decision functions are based on the reasoning given originally in) for solving word analogies.", "labels": [], "entities": [{"text": "solving word analogies", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.6972003777821859}]}, {"text": "Under this reasoning, the important thing is to look for those items shared by the objects compared, and verify whether the item of interest is included among them.", "labels": [], "entities": []}, {"text": "In other words, in the case of DAI, if we are asked whether x q , the attribute embedding, discriminates x a from x b , then an idea is to verify whether the attribute is contained in the set shared by the two concepts in question, i.e. does the set operation q \u2208 (a \u2229 b) hold?", "labels": [], "entities": []}, {"text": "Our hypothesis, is that x q discriminates x a from x b if the result of such an operation is false in terms of the subspace delimited by x a and x b , i.e. a convex cone.", "labels": [], "entities": []}, {"text": "Thus, a number of vector operations and decision functions were tested as different vector versions of this set operation on state-of-the-art neural word embeddings.", "labels": [], "entities": []}, {"text": "The proposed method does not rely on language or knowledge resources (i.e. knowledge bases and graphs, PoS or any kind of taggers, etc.).", "labels": [], "entities": []}, {"text": "Furthermore, with the help of the geometrical insight that our method provides, we also discuss the possibilities of it for being used to study measures of how concepts can be generated from attributes in the sense of vector space modeling of natural language.", "labels": [], "entities": []}, {"text": "Thus, this study can be considered, e.g., for designing semantically driven word embedding methods or to explore alternatives for building knowledge resource applications.", "labels": [], "entities": []}, {"text": "Our results showed that the proposed approach hold coherence with respect to the semantic notions proposed in the DAI task.", "labels": [], "entities": []}, {"text": "This approach reached 0.622 of F-measure in predicting discriminative attributes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9992885589599609}, {"text": "predicting discriminative attributes", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.8597282568613688}]}], "datasetContent": [{"text": "For our experiments, we computed our decision functions f (\u00b7) on tuples of the form {x a , x b , x q , y}.).", "labels": [], "entities": []}, {"text": "We also explored embeddings tanking into account external knowledge.", "labels": [], "entities": []}, {"text": "This is the case of ConceptNet embeddings).", "labels": [], "entities": []}, {"text": "DBW2V embeddings are W2V embeddings enriched by using syntactic dependencies and Conceptnet are embeddings enriched with both syntactic dependencies and knowledge graphs).", "labels": [], "entities": [{"text": "DBW2V", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9062269926071167}]}, {"text": "We trained W2V and FastText by using the Wikipedia dataset 2 . In the case of Glove 3 , ConceptNet 4 and DBW2V we downloaded pretrained embeddings from authors' websites.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9641566872596741}, {"text": "DBW2V", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.942230761051178}]}, {"text": "For Word2Vec and FastText we trained models of 200, 300, 400, 500 and 1000 dimensions.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9593839049339294}, {"text": "FastText", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8368299603462219}]}, {"text": "In our results we only report the dimensionality that performed best.", "labels": [], "entities": []}, {"text": "As our approach is unsupervised, we report experiments on the validation dataset available on the competition's repository . We can see in that the arcone operation defined in (3) provided the best results for all word embedding methods.", "labels": [], "entities": []}, {"text": "Our general best result was obtained by using Glove embeddings of 300 dimensions.", "labels": [], "entities": []}, {"text": "We expected a good result from these embeddings as they specifically learn from mutual information statistics of word pairs.", "labels": [], "entities": []}, {"text": "This enables Glove to encode feature contrasts, which also allows it for being the state-of-the-art method in word analogy tasks.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8446186780929565}]}, {"text": "During the competition we submitted our best configuration as unique run (Glove 300d and arcone operation with \u03b4 = 0.4), which gave us F 1 = 0.60 (place 19/26).", "labels": [], "entities": [{"text": "F 1", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9905072748661041}]}, {"text": "Regarding to the threshold \u03b4 of the decision functions f (\u00b7), we tested a set of values \u03b4 \u2208 {0.0, 0.1, 0.4, 0.7, 1.0}.", "labels": [], "entities": []}, {"text": "Our best result was obtained when \u03b4 = 0.4 for almost all embedding methods, excepting DBW2V.", "labels": [], "entities": [{"text": "DBW2V", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9809627532958984}]}, {"text": "This means that the convex parameter \u03bb can vary 60% around the maximum (0.5) in order to consider that an attribute q is shared by (or to generate) both concepts a, b.", "labels": [], "entities": []}, {"text": "Thus, by evaluating \u03b4 in (4) we see either that x q is too biased towards x a if it holds that \u03bb < 0.4(0.5) = 0.2, or that x q is too biased towards x b if it holds that \u03bb > 0.3 + 0.5 = 0.8.", "labels": [], "entities": []}, {"text": "In these cases we can say that q is discriminative for the concepts a, b as it is an attribute only (or mainly) of one of them.: Best results for all the word embedding and vector operation methods on the validation dataset.", "labels": [], "entities": []}, {"text": "Given that it was needed \u03b4 = 0.7 for DBW2V, we inferred that these embeddings allowed much less bias from the center of the cone and \u03bb must be within 30% of its maximum in order to decide that an attribute is shared by two concepts.", "labels": [], "entities": [{"text": "DBW2V", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9492405652999878}]}, {"text": "In other words, with DBW2V, it is more difficult to distinguish whether the attribute q is discriminative of a, b because it is allowed to be distant from both them even when it can be discriminative.", "labels": [], "entities": [{"text": "DBW2V", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9706253409385681}]}, {"text": "This condition allows for much more feature overlapping and therefore the ranking on bottom of these embeddings can be explained.", "labels": [], "entities": []}, {"text": "Notice that the Euclidean version of the cone vector operation was the second best method for all word embedding methods.", "labels": [], "entities": []}, {"text": "In fact, no difference was registered greater than 0.7% between cone and arcone operations.", "labels": [], "entities": []}, {"text": "The fuzzy approach did not show noticeable results.", "labels": [], "entities": []}, {"text": "The variation of both, the threshold and the compensation parameter.", "labels": [], "entities": [{"text": "compensation", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.9830152988433838}]}], "tableCaptions": [{"text": " Table 1: Best results for all the word embed- ding and vector operation methods on the valida- tion dataset.", "labels": [], "entities": [{"text": "valida- tion dataset", "start_pos": 88, "end_pos": 108, "type": "DATASET", "confidence": 0.5505284145474434}]}]}