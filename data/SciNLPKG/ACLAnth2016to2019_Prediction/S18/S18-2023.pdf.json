{"title": [{"text": "Hypothesis Only Baselines in Natural Language Inference", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6988568107287089}]}], "abstractContent": [{"text": "We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI).", "labels": [], "entities": [{"text": "diagnosing Natural Language Inference (NLI)", "start_pos": 42, "end_pos": 85, "type": "TASK", "confidence": 0.8103114451680865}]}, {"text": "Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution.", "labels": [], "entities": []}, {"text": "Yet, through experiments on ten distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets.", "labels": [], "entities": []}, {"text": "Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Though datasets for the task of Natural Language Inference (NLI) may vary in just about every aspect (size, construction, genre, label classes), they generally share a common structure: each instance consists of two fragments of natural language text (a context, also known as a premise, and a hypothesis), and a label indicating the entailment relation between the two fragments (e.g., ENTAILMENT, NEUTRAL, CONTRADICTION).", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.8310032784938812}]}, {"text": "Computationally, the task of NLI is to predict an entailment relation label (output) given a premise-hypothesis pair (input), i.e., to determine whether the truth of the hypothesis follows from the truth of the premise (.", "labels": [], "entities": []}, {"text": "When these NLI datasets are constructed to facilitate the training and evaluation of natural language understanding (NLU) systems , it is tempting to claim that systems achieving high accuracy on such datasets have successfully \"understood\" natural language or at least a logical relationship between a premise and hypothesis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9712240695953369}]}, {"text": "While this paper does not attempt to (b) Figure 1: (1a) shows atypical NLI model that encodes the premise and hypothesis sentences into a vector space to classify the sentence pair.", "labels": [], "entities": []}, {"text": "(1b) shows our hypothesis-only baseline method that ignores the premise and only encodes the hypothesis sentence.", "labels": [], "entities": []}, {"text": "prescribe the sufficient conditions of such a claim, we argue for an obvious necessary, or at least desired condition: that interesting natural language inference should depend on both premise and hypothesis.", "labels": [], "entities": []}, {"text": "In other words, a baseline system with access only to hypotheses can be said to perform NLI only in the sense that it is understanding language based on prior background knowledge.", "labels": [], "entities": []}, {"text": "If this background knowledge is about the world, this maybe justifiable as an aspect of natural language understanding, if not in keeping with the spirit of NLI.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6993728081385294}]}, {"text": "But if the \"background knowledge\" consists of learned statistical irregularities in the data, this may not be ideal.", "labels": [], "entities": []}, {"text": "Here we explore the question: do NLI datasets contain statistical irregularities that allow hypothesis-only models to outperform the datasets specific prior?", "labels": [], "entities": []}, {"text": "We present the results of a hypothesis-only baseline across ten NLI-style datasets and advocate for its inclusion in future dataset reports.", "labels": [], "entities": []}, {"text": "We find that this baseline can perform above the majority-class prior across most of the ten examined datasets.", "labels": [], "entities": []}, {"text": "We examine whether: (1) hypotheses contain statistical irregularities within each entailment class that are \"giveaways\" to a welltrained hypothesis-only model, (2) the way in which an NLI dataset is constructed is related to how prone it is to this particular weakness, and (3) the majority baselines might not be as indicative of \"the difficulty of the task\" ( as previously thought.", "labels": [], "entities": []}, {"text": "We are not the first to consider the inherent difficulty of NLI datasets.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.7164749801158905}]}, {"text": "For example, MacCartney (2009) used a simple bag-of-words model to evaluate early iterations of Recognizing Textual Entailment (RTE) challenge sets.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) challenge sets", "start_pos": 96, "end_pos": 147, "type": "TASK", "confidence": 0.6824105642735958}]}, {"text": "1 Concerns have been raised previously about the hypotheses in the Stanford Natural Language Inference (SNLI) dataset specifically, such as by  and in unpublished work.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 67, "end_pos": 117, "type": "DATASET", "confidence": 0.6520491018891335}]}, {"text": "Here, we survey of large number of existing NLI datasets under the lens of a hypothesis-only model.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.6889835298061371}]}, {"text": "Concurrently, Tsuchiya (2018) and similarly trained an NLI classifier with access limited to hypotheses and discovered similar results on three of the ten datasets that we study.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collect ten NLI datasets and categorize them into three distinct groups based on the methods by which they were constructed.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.7675737738609314}]}, {"text": "The flooring is a horizontal surface Elicited SNLI 550K 3 An animal is jumping to catch an object", "labels": [], "entities": [{"text": "SNLI 550K", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.6952316761016846}]}], "tableCaptions": [{"text": " Table 2: NLI accuracies on each dataset. Columns 'Hyp-Only' and 'MAJ' indicates the accuracy of the hypothesis- only model and the majority baseline. |\u2206| and \u2206% indicate the absolute difference in percentage points and the  percentage increase between the Hyp-Only and MAJ. Blue numbers indicate that the hypothesis-model outper- forms MAJ. In the right-most section, 'Baseline' indicates the original baseline on the test when the dataset was  released and 'SOTA' indicates current state-of-the-art results. MNLI-1 is the matched version and MNLI-2 is the  mismatched for MNLI. The names of datasets are italicized if containing \u2264 10K labeled examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9992927312850952}, {"text": "SOTA", "start_pos": 460, "end_pos": 464, "type": "METRIC", "confidence": 0.9465811848640442}, {"text": "MNLI-1", "start_pos": 510, "end_pos": 516, "type": "DATASET", "confidence": 0.9030437469482422}, {"text": "MNLI", "start_pos": 574, "end_pos": 578, "type": "DATASET", "confidence": 0.9310612082481384}]}, {"text": " Table 3: Accuracies on FN+ for each class label.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9986289739608765}, {"text": "FN", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8912063241004944}]}, {"text": " Table 4: NLI accuracies on the SPR development data;  each property appears in 956 hypotheses.", "labels": [], "entities": [{"text": "SPR development data", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.8908531268437704}]}]}