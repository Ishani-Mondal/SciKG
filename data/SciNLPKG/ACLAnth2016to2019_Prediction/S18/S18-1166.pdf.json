{"title": [], "abstractContent": [{"text": "The \"Capturing Discriminative Attributes\" sharedtask is the tenth task, conjoint with Se-mEval2018.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes\" sharedtask", "start_pos": 5, "end_pos": 52, "type": "TASK", "confidence": 0.9028196692466736}]}, {"text": "The task is to predict if a word can capture distinguishing attributes of one word from another.", "labels": [], "entities": []}, {"text": "We use GloVe word embedding , pre-trained on openly sourced corpus for this task.", "labels": [], "entities": []}, {"text": "A base representation is initially established over varied dimensions.", "labels": [], "entities": []}, {"text": "These representations are evaluated based on validation scores over two models, first on an SVM based classifier and second on a one dimension CNN model.", "labels": [], "entities": []}, {"text": "The scores are used to further develop the representation with vector combinations, by considering various distance measures.", "labels": [], "entities": []}, {"text": "These measures correspond to offset vectors which are concatenated as features, mainly to improve upon the F1score, with the best accuracy.", "labels": [], "entities": [{"text": "F1score", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9943398833274841}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9982329607009888}]}, {"text": "The features are then further tuned on the validation scores, to achieve highest F1score.", "labels": [], "entities": [{"text": "F1score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9967971444129944}]}, {"text": "Our evaluation narrowed down to two representations, classified on CNN models , having a total dimension length of 1204 & 1203 for the final submissions.", "labels": [], "entities": [{"text": "CNN models", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9618536531925201}]}, {"text": "Of the two, the latter feature representation delivered our best F1score of 0.658024 (as per result 1 .)", "labels": [], "entities": [{"text": "F1score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9979294538497925}]}], "introductionContent": [{"text": "As famously quoted by firth \"You shall know a word by the company it keeps\" that is, the semantic information embedded in a representation can only be described by the words surrounding it.", "labels": [], "entities": [{"text": "firth", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.893172562122345}]}, {"text": "This can only get you somewhere when, company itself is unambiguous and a representation goes through capturing \"hypothetically\" every sense of the word over a corpus.", "labels": [], "entities": []}, {"text": "The capturing discriminative attributes sharedtask, conducted with SemEval(2018) is a task proposed by alicia kerbs and denis paperno.", "labels": [], "entities": [{"text": "capturing discriminative attributes sharedtask", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8435352742671967}]}, {"text": "It describes, how lexical similarity may not be enough to access qualitatively, the semantic information fora multitude of tasks.", "labels": [], "entities": []}, {"text": "Wherein they propose that, with this task, a Results/Evaluation under the team name \"AmritaNLP\" system can be modelled for effectively extracting certain semantic differences in the words for understanding the sense embedded within them.", "labels": [], "entities": []}, {"text": "This is provided as a proof of concept dataset for this sharedtask, where a certain word is used to check if it can distinguish between a pair of words.", "labels": [], "entities": []}, {"text": "The dataset in itself seems simple where, in the training set a label information for the two classes, positive or negative are provided making this a binary classification task.", "labels": [], "entities": []}, {"text": "The three words that are provided in each instance are given in the order as, a pivot word followed by a compare word and ending with a attribute or feature word, that mayor may not be associated with the pivot word.", "labels": [], "entities": []}, {"text": "Based on the last word it is decided, if that attribute word actually is a distinguishing feature that is able to discriminate the pivot word from that of the compare word.", "labels": [], "entities": []}, {"text": "e.g (apple,banana,red) here apple is the pivot word, banana the compare word and red, the word which decides if this is a feature that can be associate with apple to distinguish it from banana.", "labels": [], "entities": []}, {"text": "This is a rather oversimplified example to a human, as from a very young age we are taught to distinguish objects based on visual aid, which simplifies the task for us as we have embedded subconsciously to differentiate the fruits mainly based on their color or size.", "labels": [], "entities": []}, {"text": "This information is seldom used to describe the fruits when illustrated in written form, thus lacking that visual form of information fora machine to make this judgment call, making it that much more difficult to take an informed decision.", "labels": [], "entities": []}, {"text": "Their work is based on a method, that was presented by for prediction of distinguishing feature with use of image as reference for visual discrimination attribute identification task, more prominently it was related to capturing of lexical information using offset vectors.", "labels": [], "entities": [{"text": "visual discrimination attribute identification task", "start_pos": 131, "end_pos": 182, "type": "TASK", "confidence": 0.6917154312133789}]}], "datasetContent": [{"text": "The dataset in the sharedtask2018 () is divided into three sets namely train test and validation.", "labels": [], "entities": []}, {"text": "The training set contains automatically generated examples which are not manually curated.", "labels": [], "entities": []}, {"text": "Whereas, the test and validation set are manually verified examples which include just over 5000 instances.", "labels": [], "entities": []}, {"text": "The test set instances are made keeping in consideration that feature word overlap between the words in train and test are minimal.", "labels": [], "entities": []}, {"text": "The validation set is similar to that of the test set and is used for parameter tuning of the models.", "labels": [], "entities": []}, {"text": "There are in total 17782 instances in the training set, 2722 in the validation set and 2340 in the test set.", "labels": [], "entities": []}, {"text": "With the automated nature of the data, the training set is noisier in comparison to that of the validation and test set.", "labels": [], "entities": []}, {"text": "In the dataset, positive examples are annotated with the label '1', signifying that the attribute/feature word is a positive association only to the pivot word in the order presented and not vice verse.", "labels": [], "entities": []}, {"text": "e.g. (airplane,helicopter,wings) here 'wings' is an attribute only associated to 'airplane', whereas (helicopter,airplane,wings) is an invalid entry.", "labels": [], "entities": []}, {"text": "The combination of (helicopter,airplane) in this order will only be added if the concept 'helicopter' has a feature that airplane does not have in this set.", "labels": [], "entities": []}, {"text": "On the other hand, the negative examples are annotated with label '0' at the end.", "labels": [], "entities": []}, {"text": "These are considered when the attribute/feature words are either similar to both pivot and the compare word or are dissimilar to them, e.g. (Tractor,scooter,wheels), (Spider,elephant,legs) e.t.c.", "labels": [], "entities": []}, {"text": "In the training dataset, there is a total of 508 unique concepts (pivot) words, of which 375 words have positive attributes and 505 of these have negative attributes, seeing the big contrast between the two labeled attributes we can infer that not every concept word has an equal proportion of labeled instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Validation accuracy for varied dimension GloVe representation using SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9782888889312744}]}, {"text": " Table 3: Validation accuracy for varied dimension GloVe representation using CNN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9841340184211731}, {"text": "CNN", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9190803170204163}]}, {"text": " Table 4: Various feature representation taken for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.9299789667129517}]}, {"text": " Table 5: Validation accuracy of 300 dimension, GloVe representations on 840B common crawl tokens using CNN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9354279041290283}, {"text": "CNN", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.918742299079895}]}]}