{"title": [{"text": "CTSys at SemEval-2018 Task 3: Irony in Tweets", "labels": [], "entities": [{"text": "CTSys", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8980633020401001}, {"text": "Irony", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9397158026695251}]}], "abstractContent": [{"text": "The objective of this paper is to provide a description fora system built as our participation in SemEval-2018 Task 3 on Irony detection in English tweets.", "labels": [], "entities": [{"text": "Irony detection in English tweets", "start_pos": 121, "end_pos": 154, "type": "TASK", "confidence": 0.8056055068969726}]}, {"text": "This system classifies a tweet as either ironic or non-ironic through a supervised learning approach.", "labels": [], "entities": []}, {"text": "Our approach is to implement three feature models, and to then improve the performance of the supervised learning classification of tweets by combining many data features and using a voting system on four different classifiers.", "labels": [], "entities": []}, {"text": "We describe the process of pre-processing data, extracting features, and running different types of classifiers against our feature set.", "labels": [], "entities": []}, {"text": "In the competition, our system achieved an F1-score of 0.4675, ranking 35th in subtask A, and an F1-score score of 0.3014 ranking 22th in subtask B.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9997621178627014}, {"text": "F1-score score", "start_pos": 97, "end_pos": 111, "type": "METRIC", "confidence": 0.9796201288700104}]}], "introductionContent": [{"text": "Irony detection in text has extended to different data forms (tweets, reviews, TV series dialogues), our domain of data in this task is a Twitter corpus provided by SemEval2018 organizers.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7370589077472687}]}, {"text": "Here, irony detection refers to computational approaches to predict if a given text is sarcastic.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.9484705924987793}]}, {"text": "This problem is hard because of the nuanced ways in which irony maybe expressed.", "labels": [], "entities": []}, {"text": "The most difficult part of the problem mentioned is the process of feature engineering, because it defines the parameters and the relationships and dependencies between semantic meanings, and gives us the numerical model that the classifier would proceed to work on, thus being crucial to the soundness and efficiency of the system.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.758296400308609}]}, {"text": "This led us to dive into deeper questions, such as the nature of tweets, and how we are dealing with aversion of the English language that is not directly workable.", "labels": [], "entities": []}, {"text": "We need to perform preprocessing to deal with annotations and hashtags.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9761310815811157}]}, {"text": "Another question is how to analyze irony in English language and derive a rule-based approach that can be implemented to better understand the semantics of ironic text.", "labels": [], "entities": []}, {"text": "The problem, as described by the SemEval-2018 task organizers, addresses both the binary distinction between irony and non-irony, as well as different types of irony.", "labels": [], "entities": []}], "datasetContent": [{"text": "The used dataset in this assignment is the one provided in SemEval-2018 task 3.", "labels": [], "entities": [{"text": "SemEval-2018 task 3", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.696686863899231}]}, {"text": "It consists of 3,842 tweets in total.", "labels": [], "entities": []}, {"text": "The tweets were collected by searching Twitter for the hashtags #irony, #sarcasm and #not.", "labels": [], "entities": []}, {"text": "The dataset was presented in two phases: 1-Training data: already labeled tweets used to train the classifiers.", "labels": [], "entities": []}, {"text": "Each tweet was provided with a binary classification label and an index.", "labels": [], "entities": []}, {"text": "2-Testing data: unlabeled tweets to test the classifiers against.", "labels": [], "entities": []}, {"text": "For each instance in the test data, participants submitted a predicted label.", "labels": [], "entities": []}, {"text": "Based on these predictions, competition scores were calculated using four metrics (F1-score, precision, recall, and accuracy).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9994863271713257}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9981753826141357}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9975442290306091}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9995397329330444}]}], "tableCaptions": [{"text": " Table 1: accuracy of feature-classes when tested  against classifiers using the training set for task A.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994439482688904}]}, {"text": " Table 2: The score obtained by the system in  subtasks A and B as evaluated by SemEval.", "labels": [], "entities": []}]}