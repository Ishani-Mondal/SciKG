{"title": [{"text": "A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning", "labels": [], "entities": [{"text": "Multimodal Translation-Based Approach", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.6353311836719513}, {"text": "Knowledge Graph Representation Learning", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.8409301936626434}]}], "abstractContent": [{"text": "Current methods for knowledge graph (KG) representation learning focus solely on the structure of the KG and do not exploit any kind of external information, such as visual and linguistic information corresponding to the KG entities.", "labels": [], "entities": [{"text": "knowledge graph (KG) representation learning", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.6782835892268589}]}, {"text": "In this paper, we propose a multimodal translation-based approach that defines the energy of a KG triple as the sum of sub-energy functions that leverage both mul-timodal (visual and linguistic) and structural KG representations.", "labels": [], "entities": []}, {"text": "Next, a ranking-based loss is minimized using a simple neural network architecture.", "labels": [], "entities": []}, {"text": "Moreover, we introduce anew large-scale dataset for multimodal KG representation learning.", "labels": [], "entities": [{"text": "KG representation learning", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7816334962844849}]}, {"text": "We compared the performance of our approach to other baselines on two standard tasks, namely knowledge graph completion and triple classification, using our as well as the WN9-IMG dataset.", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6908154686292013}, {"text": "triple classification", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.7160135954618454}, {"text": "WN9-IMG dataset", "start_pos": 172, "end_pos": 187, "type": "DATASET", "confidence": 0.9824243485927582}]}, {"text": "1 The results demonstrate that our approach outperforms all baselines on both tasks and datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge Graphs (KGs), e.g., Freebase and, are stores of relational facts, which are crucial for various kinds of tasks, such as question answering and information retrieval.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9569490551948547}, {"text": "question answering", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.874963104724884}, {"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.7803321480751038}]}, {"text": "KGs are structured as triples of head and tail entities along with the relation that holds between them.", "labels": [], "entities": []}, {"text": "Factual knowledge is virtually infinite and is frequently subject to change.", "labels": [], "entities": [{"text": "Factual knowledge", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.863040417432785}]}, {"text": "This raises the question of the incompleteness of the KGs.", "labels": [], "entities": []}, {"text": "To address this problem, several methods have been proposed for automatic KG completion (KGC, fora survey refer to.", "labels": [], "entities": [{"text": "KG completion", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.8578538596630096}]}, {"text": "In recent years, translationbased approaches have witnessed a great success.", "labels": [], "entities": [{"text": "translationbased", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.972424328327179}]}, {"text": "Their main idea is to model the entities and their Code and datasets are released for research purposes: https://github.com/UKPLab/ starsem18-multimodalKB relation as low-dimensional vector representations (embeddings), which in turn can be used to perform different kinds of inferences on the KG.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9660853743553162}, {"text": "KG", "start_pos": 294, "end_pos": 296, "type": "DATASET", "confidence": 0.9164376258850098}]}, {"text": "These include identifying new facts or validating existing ones.", "labels": [], "entities": []}, {"text": "However, translation-based methods rely on the rich structure of the KG and generally ignore any type of external information about the included entities.", "labels": [], "entities": []}, {"text": "In this paper, we propose a translation-based approach for KG representation learning that leverages two different types of external, multimodal representations: linguistic representations created by analyzing the usage patterns of KG entities in text corpora, and visual representations obtained from images corresponding to the KG entities.", "labels": [], "entities": [{"text": "KG representation learning", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.9479886293411255}]}, {"text": "To gain initial insights into the potential benefits of external information for the KGC task, let us consider the embeddings produced by the translationbased TransE method () on the WN9-IMG dataset (.", "labels": [], "entities": [{"text": "KGC task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.8067892789840698}, {"text": "WN9-IMG dataset", "start_pos": 183, "end_pos": 198, "type": "DATASET", "confidence": 0.9786174297332764}]}, {"text": "This dataset contains a subset of WordNet synsets, which are linked according to a predefined set of linguistic relations, e.g. hypernym.", "labels": [], "entities": []}, {"text": "We observed that TransE fails to create suitable representations for entities that appear frequently as the head/tail of one-to-many/many-to-one relations.", "labels": [], "entities": []}, {"text": "For example, the entity person appears frequently in the dataset as a head/tail of the hyponym/hypernym relation; the same holds for entities like animal or tree.", "labels": [], "entities": []}, {"text": "TransE represents such entities as points that are very close to each other in the embedding space (cf. Tab. 1).", "labels": [], "entities": []}, {"text": "Furthermore, the entity embeddings tend to be very similar to the embeddings of relations in which they frequently participate.", "labels": [], "entities": []}, {"text": "Consequently, such a representation suffers from limited discriminativeness and can be considered a main source of error for different KG inference tasks.", "labels": [], "entities": [{"text": "KG inference tasks", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.8103490869204203}]}, {"text": "To understand how multimodal representations may help to overcome this issue, we performed the same analysis by considering two types of external information: linguistic and visual.", "labels": [], "entities": []}, {"text": "The linguistic representations are created using word embedding techniques ( , and the visual ones, called visual embeddings, are obtained from the feature layers of deep networks for image classification (e.g.,) on images that correspond to the entities of the dataset.", "labels": [], "entities": [{"text": "image classification", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.7619619071483612}]}, {"text": "For the same category of entities discussed above, we observed that both the visual and the linguistic embeddings are much more robust than the structurebased embeddings of TransE.", "labels": [], "entities": []}, {"text": "For instance, person is closer to other semantically related concepts, such as Homo erectus in the linguistic embedding space, and to concepts with common visual characteristics (e.g., woman, actor) in the visual embedding space (cf. Tab. 1).", "labels": [], "entities": []}, {"text": "Furthermore, the linguistic and the visual embeddings seem to complement each other and hence are expected to enhance KG representations if they can be leveraged during the representation learning process.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: (1) We propose an approach for KG representation learning that incorporates multimodal (visual and linguistic) information in a translation-based framework and extends the definition of triple energy to consider the new multimodal representations; (2) we investigate different methods for combining multimodal representations and evaluate their performance; (3) we introduce anew large-scale dataset for multimodal KGC based on Freebase; (4) we experimentally demonstrate that our approach outperforms baseline approaches including the state-of-the-art method of on the link prediction and triple classification tasks.", "labels": [], "entities": [{"text": "KG representation learning", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.950482706228892}, {"text": "Freebase", "start_pos": 490, "end_pos": 498, "type": "DATASET", "confidence": 0.9633808732032776}, {"text": "on the link prediction", "start_pos": 625, "end_pos": 647, "type": "TASK", "confidence": 0.6276047751307487}, {"text": "triple classification tasks", "start_pos": 652, "end_pos": 679, "type": "TASK", "confidence": 0.7629926204681396}]}], "datasetContent": [{"text": "We investigated different sets of hyperparameters for training the model.", "labels": [], "entities": []}, {"text": "The best results were obtained using the Adam optimizer) with a fixed learning rate of 0.001 and batch size of 100.", "labels": [], "entities": []}, {"text": "We used the hyperbolic tangent function (tanh) for the activation and one fully-connected layer of 100 hidden units.", "labels": [], "entities": []}, {"text": "We observed that regularization has a minor effect.", "labels": [], "entities": [{"text": "regularization", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9405380487442017}]}, {"text": "In the case of WN9-IMG, we used dropout regularization () with a dropout ratio of 10%; we applied no regularization on the FB-IMG dataset.", "labels": [], "entities": [{"text": "WN9-IMG", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9447999000549316}, {"text": "FB-IMG dataset", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9874539375305176}]}, {"text": "Regarding the margin of the loss function, we experimented with several values for both datasets \u03b3 \u2208 {4, 6, 8, 10, 12}.", "labels": [], "entities": [{"text": "margin", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9850841164588928}]}, {"text": "The best results for both datasets were obtained with \u03b3 = 10.", "labels": [], "entities": []}, {"text": "We investigated different configurations of our approach: (1) Ling considers the linguistic embeddings only, (2) Vis considers the visual embeddings only, (3) multimodal where the visual and the linguistic embeddings are considered according to the presented multimodal combination methods: DeViSE, Imagined, and the Concatenation methods (cf. Sec.", "labels": [], "entities": []}, {"text": "3.2), and (4) only head in which we use the head view only and the concatenation method for combining the multimodal representations.", "labels": [], "entities": []}, {"text": "Here, negative samples are produced by randomly corrupting the head, the tail, or the relation of gold triples.", "labels": [], "entities": []}, {"text": "We compared our approach to other baseline methods including and IKRL ().", "labels": [], "entities": [{"text": "IKRL", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.5774446725845337}]}, {"text": "For TransE, we set the size of the embeddings to 100 dimensions and followed the recommendations of regarding the other hyperparameters.", "labels": [], "entities": []}, {"text": "We also implemented the IKRL approach and the best results were achieved by using margins of 8 and 4 for the WN9-IMG and the FB-IMG datasets, respectively.", "labels": [], "entities": [{"text": "WN9-IMG", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9464010000228882}, {"text": "FB-IMG datasets", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.9718059599399567}]}, {"text": "We tested two configurations of IKRL: (1) IKRL (Vis) uses the visual representation only (as in the original paper) and initializes the structural representations with our learned TransE embeddings, and (2) IKRL (Concat), which uses the concatenation of the linguistic and the visual embeddings.", "labels": [], "entities": []}, {"text": "Please note that we do not apply the attention mechanism for creating image representations as proposed in the IKRL paper (.", "labels": [], "entities": [{"text": "IKRL paper", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.9149305820465088}]}, {"text": "However, we include that model, referred to as IKRL (Paper), in the comparison.", "labels": [], "entities": [{"text": "IKRL", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.606810450553894}]}], "tableCaptions": [{"text": " Table 3: Link prediction results on WN9-IMG.", "labels": [], "entities": [{"text": "WN9-IMG", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9438877701759338}]}, {"text": " Table 4: Link prediction results on FB-IMG.", "labels": [], "entities": [{"text": "FB-IMG", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.8483991026878357}]}, {"text": " Table 5: Triple classification results on WN9-IMG.", "labels": [], "entities": [{"text": "Triple classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7619739174842834}, {"text": "WN9-IMG", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9576587677001953}]}, {"text": " Table 6: Triple classification results on FB-IMG.", "labels": [], "entities": [{"text": "Triple classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6932024508714676}, {"text": "FB-IMG", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9554791450500488}]}]}