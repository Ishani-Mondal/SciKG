{"title": [{"text": "Learning Distributed Event Representations with a Multi-Task Approach", "labels": [], "entities": [{"text": "Learning Distributed Event Representations", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.621976688504219}]}], "abstractContent": [{"text": "Human world knowledge contains information about prototypical events and their participants and locations.", "labels": [], "entities": []}, {"text": "In this paper, we train the first models using multi-task learning that can both predict missing event participants and also perform semantic role classification based on semantic plausibility.", "labels": [], "entities": [{"text": "semantic role classification", "start_pos": 133, "end_pos": 161, "type": "TASK", "confidence": 0.6714134613672892}]}, {"text": "Our best-performing model is an improvement over the previous state-of-the-art on thematic fit modelling tasks.", "labels": [], "entities": []}, {"text": "The event embeddings learned by the model can additionally be used effectively in an event similarity task, also outperforming the state-of-the-art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event representations consist, at minimum, of a predicate, the entities that participate in the event, and the thematic roles of those participants).", "labels": [], "entities": []}, {"text": "The cook cut the cake with the knife expresses an event of cutting in which a cook is the \"agent\", the cake is the \"patient\", and the knife is the \"instrument\" of the action.", "labels": [], "entities": []}, {"text": "Experiments have shown that event knowledge, in terms of the prototypical participants of events and their structured compositions, plays a crucial role inhuman sentence processing, especially from the perspective of thematic fit: the extent to which humans perceive given event participants as \"fitting\" given predicate-role combinations.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7595948576927185}]}, {"text": "Therefore, computational models of language processing should also consist of event representations that reflect thematic fit.", "labels": [], "entities": []}, {"text": "To evaluate this aspect empirically, a popular approach in previous work has been to compare model output to human judgements (.", "labels": [], "entities": []}, {"text": "The best-performing recent work has been the model of, who effectively simulate thematic fit via selectional preferences: generating a probability distribution over the full vocabulary of potential role-fillers.", "labels": [], "entities": []}, {"text": "Given event context as input, including a predicate and a given set of semantic roles and their role-fillers as well as one target role, its training objective is to predict the correct role-filler for the target role.", "labels": [], "entities": []}, {"text": "The objective of predicting upcoming role-fillers is cognitively plausible: there is ample evidence that humans anticipate upcoming input during sentence processing and learn from prediction error ( (even if other details of the implementation like back-propagation may not have much to do with how errors are propagated in humans).", "labels": [], "entities": []}, {"text": "An analysis of role filler predictions by Tilk et al.'s model shows that the model does not make sufficient use of the thematic role input.", "labels": [], "entities": [{"text": "role filler", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8324703574180603}]}, {"text": "For instance, the representation of apple eats boy is similar to the representation of boy eats apple, even though the events are very dissimilar from one another.", "labels": [], "entities": []}, {"text": "Interestingly, humans have been found to make similar errors.", "labels": [], "entities": []}, {"text": "For instance, humans have been shown to frequently misinterpret a sentence with inverse role assignment, when the plausibility of the sentence with swapped role assignment is very high, as in The mother gave the candle the daughter, which is often erroneously interpreted as the daughter receiving the candle, instead of the literal syntax which says that the candle receives the daughter ().", "labels": [], "entities": []}, {"text": "Tilk et al.'s model design makes it more susceptible to this type of error than humans.", "labels": [], "entities": []}, {"text": "The model lacks the ability to process in both directions, i.e., to both comprehend and produce thematic role marking (approximated here as thematic role assignment).", "labels": [], "entities": []}, {"text": "We therefore propose to add a secondary role prediction task to the model, training it to both produce and comprehend language.", "labels": [], "entities": [{"text": "role prediction", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7329954355955124}]}, {"text": "In this paper, we train the first model using multi-task learning) which can ef-fectively predict semantic roles for event participants as well as perform role-filler prediction . Furthermore, we obtain significant improvements and better-performing event embeddings by an adjustment to the architecture (parametric weighted average of role-filler embeddings) which helps to capture role-specific information for participants during the composition process.", "labels": [], "entities": [{"text": "role-filler prediction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7500698268413544}]}, {"text": "The new event embeddings exhibit state-of-the-art performance on a correlation task with human thematic fit judgements and an event similarity task.", "labels": [], "entities": []}, {"text": "Our model is the first joint model for selectional preferences (SPs) prediction and semantic role classification (SRC) to the best of our knowledge.", "labels": [], "entities": [{"text": "selectional preferences (SPs) prediction", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.6926433742046356}, {"text": "semantic role classification (SRC)", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.7350268165270487}]}, {"text": "Previous works used distributional similarity-based ( or LDAbased () SPs for semantic role labelling to leverage lexical sparsity.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.6663905580838522}]}, {"text": "However, when it comes to a situation with domain shift, single task SP models that rely heavily on syntax have high generalisation error.", "labels": [], "entities": []}, {"text": "We show that the multi-task architecture is better suited to generalise in that situation and can be potentially applied to improve current semantic role labelling systems which rely on small annotated corpora.", "labels": [], "entities": []}, {"text": "Our approach is a conceptual improvement on previous models because we address multiple event-representation tasks in a single model: thematic fit evaluation, role-filler prediction/generation, semantic role classification, event participant composition, and structured event similarity evaluation.", "labels": [], "entities": [{"text": "role-filler prediction/generation", "start_pos": 159, "end_pos": 192, "type": "TASK", "confidence": 0.892450362443924}, {"text": "semantic role classification", "start_pos": 194, "end_pos": 222, "type": "TASK", "confidence": 0.6820815801620483}, {"text": "structured event similarity evaluation", "start_pos": 259, "end_pos": 297, "type": "TASK", "confidence": 0.642768569290638}]}, {"text": "proposed a neural network, the non-incremental role-filler (NNRF) model, for role-filler prediction which takes a combination of words and roles as input to predict the filler of a target role.", "labels": [], "entities": [{"text": "role-filler prediction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7967876195907593}]}, {"text": "For example, the model would take \"waiter/ARG0\" and \"serve/PRD\" and target role \"ARG1\" as input and return high probabilities to words like \"breakfast\", \"dish\", and \"drinks\".", "labels": [], "entities": []}], "datasetContent": [{"text": "To learn an event representation from language resources with access to generalised event knowledge, we use the Rollenwechsel-English (RWeng) corpus 3 , a large-scale corpus based on BNC and ukWaC with about 2B tokens, which contains automatically generated PropBank-style semantic role labels for the head words of each argument (.", "labels": [], "entities": [{"text": "Rollenwechsel-English (RWeng) corpus 3", "start_pos": 112, "end_pos": 150, "type": "DATASET", "confidence": 0.8871408402919769}, {"text": "ukWaC", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.7613569498062134}]}, {"text": "We choose the first 99.2% as training data, the next 0.4% as validation data and the last 0.4% as test data, which follows Tilk's setting to make a fair comparison.", "labels": [], "entities": []}, {"text": "From the training data, we extract a word list of the 50K most frequent head words (nouns, verbs, adjectives and adverbs) and add one OOV symbol 4 . For training the model, we distinguish between seven role labels: PRD for predicates, ARG0, ARG1, ARGM-MNR, ARGM-LOC, ARGM-TMP; all other roles are mapped onto a category OTHER.", "labels": [], "entities": [{"text": "OOV", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9403313994407654}]}, {"text": "NNRF is the current state-of-the-art model for event representation; we reimplement this model and use it as the baseline for our evaluation tasks.", "labels": [], "entities": [{"text": "NNRF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9676938652992249}, {"text": "event representation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7078163474798203}]}, {"text": "For a fair comparison, we train the NNRF model and our three multi-task models on the newest version of RW-eng corpus.", "labels": [], "entities": [{"text": "NNRF model", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9317544400691986}, {"text": "RW-eng corpus", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.9338781535625458}]}, {"text": "Each model is trained for 27 iterations (or less if the model converged earlier) . Because we use random parameter initialisation, to observe its effect to our evaluations, we train 10 instances of each model and report average performance (we do not use these 10 models as an ensemble method such as labelling by majority voting).", "labels": [], "entities": []}, {"text": "We begin by testing the new component of the model in terms of how well the model can predict semantic role labels.", "labels": [], "entities": []}, {"text": "Next, we evaluate our multi-task models against human thematic fit ratings in order to assess whether the inclusion of the multi-task architecture leads to improvements on this task, following  The human judgement data consists of verbs, a verbal argument with its role, and an average fit judgement score on a scale from 1 (least common) to 7 (most common), e.g., ask, police/AGENT, 6.5.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 377, "end_pos": 382, "type": "METRIC", "confidence": 0.911853015422821}]}, {"text": "We used: Pado07: the dataset proposed by Pado  The thematic fit judgements from the tasks discussed in section 6 only contain ratings of the fit between the predicate and one role-filler.", "labels": [], "entities": []}, {"text": "However, other event participants contained in a clause can affect human expectations of the upcoming role-fillers.", "labels": [], "entities": []}, {"text": "For instance, mechanics are likely to check tires, while journalists are likely to check spellings.", "labels": [], "entities": []}, {"text": "The B10 dataset ( contains human judgements for 64 pairs of agentverb-patient triples, where one triple in each pair is plausible (e.g., \"journalist check spelling\"), and one is implausible (e.g., \"journalist check type\").", "labels": [], "entities": [{"text": "B10 dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9620569944381714}]}, {"text": "A model is evaluated based on whether it successfully assigns a higher likelihood/rating to the plausible than to the implausible object (also referred to as the Accuracy 1 metric in).", "labels": [], "entities": [{"text": "Accuracy 1 metric", "start_pos": 162, "end_pos": 179, "type": "METRIC", "confidence": 0.9766637682914734}]}, {"text": "The baseline models are NNRF as well as: Random: The naive baseline model consists of choosing the tags uniformly at random.", "labels": [], "entities": [{"text": "NNRF", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8462993502616882}]}, {"text": "Lenci11: Lenci (2011) proposed a composition model for TypeDM.", "labels": [], "entities": []}, {"text": "shows that our new composition method based on parametric weighted average outperforms previous models; the RoFA-MT model achieves the highest accuracy overall and outperforms the baseline (NNRF) significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9981531500816345}]}], "tableCaptions": [{"text": " Table 2: Results of semantic role classification given verb-head pairs. P is precision, R is recall and F 1 is F- measure. F 1 values with a mark are significantly higher than all other values in the same column, where (**)  p < 0.01.", "labels": [], "entities": [{"text": "semantic role classification", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.718305766582489}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9990935325622559}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9968854784965515}, {"text": "F 1", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.948826938867569}, {"text": "F- measure", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9750887354214987}, {"text": "F 1", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9453331232070923}]}, {"text": " Table 3: Results on human thematic fit judgement correlation task (Spearman's \u03c1 \u00d7 100) compared to previous  work. The last column reports the weighted average results by numbers of entries of all five datasets. Values with  a mark are significantly different from the baseline model (NNRF), where (*) p < 0.05, (**) p < 0.01.", "labels": [], "entities": [{"text": "human thematic fit judgement correlation", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.585163128376007}]}, {"text": " Table 4: Results on agent-patient compositionality evaluation comparing to previous models. Values with a mark  are significantly different from the baseline model (NNRF), where (*) p < 0.05.", "labels": [], "entities": [{"text": "agent-patient compositionality evaluation", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.7237925430138906}]}, {"text": " Table 5: Results on event similarity evaluation comparing to previous models. Values with a mark are significantly  different from the baseline model (NNRF), where (*) p < 0.05, (**) p < 0.01.", "labels": [], "entities": [{"text": "event similarity evaluation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7791301210721334}]}, {"text": " Table 6: Ablation study of single task variants. Underlined values indicate the best values with the same com- position method, and bold values indicate the best values on that data set. Values with a mark are significantly  different from the multi-task baseline models (RoFA-MT / ResRoFA-MT), where (*) p < 0.05, (**) p < 0.01,  (***) p < 0.001.", "labels": [], "entities": []}]}