{"title": [{"text": "Texterra at SemEval-2018 Task 7: Exploiting Syntactic Information for Relation Extraction and Classification in Scientific Papers", "labels": [], "entities": [{"text": "Relation Extraction and Classification", "start_pos": 70, "end_pos": 108, "type": "TASK", "confidence": 0.8910833597183228}]}], "abstractContent": [{"text": "In this work we evaluate applicability of entity pair models and neural network architec-tures for relation extraction and classification in scientific papers at SemEval-2018.", "labels": [], "entities": [{"text": "relation extraction and classification", "start_pos": 99, "end_pos": 137, "type": "TASK", "confidence": 0.7459103614091873}, {"text": "SemEval-2018", "start_pos": 162, "end_pos": 174, "type": "DATASET", "confidence": 0.8796435594558716}]}, {"text": "We carryout experiments with representing entity pairs through sentence tokens and through shortest path in dependency tree, comparing approaches based on convolutional and recurrent neural networks.", "labels": [], "entities": []}, {"text": "With convolutional network applied to shortest path in dependency tree we managed to be ranked eighth in subtask 1.1 (\"clean data\"), ninth in 1.2 (\"noisy data\").", "labels": [], "entities": []}, {"text": "Similar model applied to separate parts of the shortest path was mounted to ninth (extraction track) and seventh (classification track) positions in subtask 2 ranking.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction is an important part of natural language processing.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8813941478729248}, {"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6571077803770701}]}, {"text": "During SemEval-2018 an evaluation devoted to extraction and classification of relations in scientific papers was held).", "labels": [], "entities": [{"text": "SemEval-2018", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.721655547618866}, {"text": "extraction and classification of relations in scientific papers", "start_pos": 45, "end_pos": 108, "type": "TASK", "confidence": 0.742499653249979}]}, {"text": "The task is described as follows: given abstracts of scientific articles with detected entities, the goal is to choose correct relations for provided source, target entity pairs (subtask 1 -relation classification) and to determine correct relations among all entity pairs (subtask 2 -relation extraction and classification).", "labels": [], "entities": [{"text": "relation extraction and classification", "start_pos": 285, "end_pos": 323, "type": "TASK", "confidence": 0.7527946978807449}]}, {"text": "The target quality metric in classification is macroaverage of F1-scores of every class; for extraction scenario the target metric is F1-score.", "labels": [], "entities": [{"text": "macroaverage", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9145598411560059}, {"text": "F1-scores", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9297760725021362}, {"text": "F1-score", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9948267340660095}]}, {"text": "Our method is based on multinomial classification of entity pairs and their sentences with neural networks.", "labels": [], "entities": []}, {"text": "We experiment with representing entity pairs through all sentence tokens and through tokens along the shortest path between entities in dependency tree ().", "labels": [], "entities": []}, {"text": "We employ convolutional (CNN) ( and bidirectional Long Short-Term Memory (biL-STM) neural network based approaches to encode sentences and dependency tree paths.", "labels": [], "entities": []}, {"text": "In this work we mainly focus on relation classification, so most of analysis and experiments are carried out for this task.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9676339328289032}]}, {"text": "Slightly modified models which achieve the best results on subtask 1 are adapted for solving relation extraction and classification problem.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8757463991641998}, {"text": "classification", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.9473575949668884}]}, {"text": "The rest of the paper is organized as follows: in section 2 we describe some known approaches for relation extraction and classification.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.9322437047958374}]}, {"text": "In section 3 our approach is presented in details.", "labels": [], "entities": []}, {"text": "Section 4 outlines results of described approach evaluation on official SemEval-2018 task 7 test set.", "labels": [], "entities": [{"text": "SemEval-2018 task 7 test set", "start_pos": 72, "end_pos": 100, "type": "DATASET", "confidence": 0.8024530649185181}]}, {"text": "We wrap up with some final thoughts in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We took part in both relation classification and relation extraction subtasks.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.9251379370689392}, {"text": "relation extraction subtasks", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.8949172496795654}]}, {"text": "All results reported in this section are gained on official SemEval-2018 task 7 test data developed by organizers and released after the evaluation phase.", "labels": [], "entities": [{"text": "SemEval-2018 task 7 test data developed", "start_pos": 60, "end_pos": 99, "type": "DATASET", "confidence": 0.7810730636119843}]}, {"text": "Official scores for corresponding submissions are specified in Tables 2 and 3 after the slash sign.", "labels": [], "entities": []}, {"text": "The difference is explained by minor parameter variations, typically randomness in variables initialization and number of training epochs.", "labels": [], "entities": []}, {"text": "Relation classification (subtask 1) has two datasets -with manually annotated entities (subtask 1.1 -\"clean data\") and with automatically detected entities (subtask 1.2 -\"noisy data\").", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9583449959754944}]}, {"text": "We decided to construct a single model merging both datasets into one in order to increase the amount of training examples and to diminish skew in number of sample relations for different types.", "labels": [], "entities": []}, {"text": "To encode tokens fasttext (skipgram; minimum length of character n-gram is 1, maximum -5) is used.", "labels": [], "entities": []}, {"text": "We build two separate models with different embedding dimensions -100 and 300 -using the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.9207504391670227}]}, {"text": "Evaluation results are presented in.", "labels": [], "entities": []}, {"text": "The target metric is F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9997718930244446}]}, {"text": "The first part of method name specifies whether all sentence tokens or tokens from shortest path in dependency tree are used.", "labels": [], "entities": []}, {"text": "The second part specifies neural network architecture being utilized.", "labels": [], "entities": []}, {"text": "We report results for the following neural network parameter values: attention size A is 400; biLSTM hidden layer size B is 1000; CNN filters F -200 with height 3, 50 with heights 2 and 4, width matches the embedding dimensionality; size of fully-connected layer L is 1000 for biLSTM and 900 for CNN.", "labels": [], "entities": []}, {"text": "Specified values are selected during experiments, which are out of this paper scope.", "labels": [], "entities": []}, {"text": "As for subtask 1.1, we conclude that: context attention tends to be beneficial (the only counterexample is sentence biLSTM with fasttext size 300); larger token embeddings are typically better (the only counterexample is sentence biLSTM); syntactic information is helpful for relation classification with neural networks.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 276, "end_pos": 299, "type": "TASK", "confidence": 0.900373786687851}]}, {"text": "For subtask 1.2 the results are more controversial: smaller embeddings sometimes surpass larger ones; utilizing syntactic information seems still beneficial, but the results are not as convincing as in 1.1; in contrast to subtask 1.1 context attention does not tend to improve quality of the approach.", "labels": [], "entities": []}, {"text": "From our point of view, such strange behaviour on subtask 1.2 dataset requires further investigation.", "labels": [], "entities": [{"text": "subtask 1.2 dataset", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.5918273627758026}]}, {"text": "Quality evaluations for subtask 2 solutions are presented in   tion and evaluation F1.", "labels": [], "entities": []}, {"text": "All experiments are performed with the same input vector size (fasttext dimensionality equals to 300); entity pairs are modelled with shortest path in dependency tree.", "labels": [], "entities": []}, {"text": "The second column specifies negative examples generation strategy: reflection with some portion (0-100%) of in-sentence negative examples is always used.", "labels": [], "entities": [{"text": "negative examples generation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7710625131924947}]}, {"text": "For training purposes both subtask 1.1 and 1.2 data is utilized.", "labels": [], "entities": []}, {"text": "When reflection strategy for negative examples generation is used seven-class approach performs better.", "labels": [], "entities": []}, {"text": "With utilization of both strategies two-step approach breaks forward.", "labels": [], "entities": []}, {"text": "Post-processing improves quality for both approaches, however it is still rather low compared with the results of other participants.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of relations in subtask datasets.", "labels": [], "entities": []}, {"text": " Table 2: Final quality results for subtasks 1.1 and 1.2.", "labels": [], "entities": []}, {"text": " Table 3: Final quality results for subtask 2.", "labels": [], "entities": []}]}