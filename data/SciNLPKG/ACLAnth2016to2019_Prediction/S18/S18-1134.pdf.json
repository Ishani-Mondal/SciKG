{"title": [{"text": "ClaiRE at SemEval-2018 Task 7: Classification of Relations using Embeddings", "labels": [], "entities": [{"text": "ClaiRE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9549303650856018}, {"text": "SemEval-2018 Task 7", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7828907171885172}, {"text": "Classification of Relations", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8652271429697672}]}], "abstractContent": [{"text": "In this paper we describe our system for SemEval-2018 Task 7 on classification of semantic relations in scientific literature for clean (subtask 1.1) and noisy data (subtask 1.2).", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8247360984484354}, {"text": "classification of semantic relations", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.8097085505723953}]}, {"text": "We compare two models for classification , a C-LSTM which utilizes only word em-beddings and an SVM that also takes hand-crafted features into account.", "labels": [], "entities": []}, {"text": "To adapt to the domain of science we train word embeddings on scientific papers collected from arXiv.org.", "labels": [], "entities": []}, {"text": "The hand-crafted features consist of lexical features to model the semantic relations as well as the entities between which the relation holds.", "labels": [], "entities": []}, {"text": "Classification of Relations using Embeddings (ClaiRE) achieved an F1 score of 74.89 % for the first subtask and 78.39 % for the second.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9884867370128632}]}], "introductionContent": [{"text": "The goal of SemEval-2018 Task 7 is to extract and classify semantic relations between entities into six categories that are specific to scientific literature (.", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.850502610206604}]}, {"text": "In this work, we focus on the subtask of classifying relations between entities in manually (subtask 1.1) and automatically annotated and therefore noisy data (subtask 1.2).", "labels": [], "entities": []}, {"text": "Given a pair of related entities, the task is to classify the type of their relation among the following options: Compare, Model-Feature, Part Whole, Result, Topic or Usage.", "labels": [], "entities": []}, {"text": "Relation types are explained in detail in the task description paper).", "labels": [], "entities": []}, {"text": "The following sentence shows an example of a Result relation between the two entities combination methods and system performance: Combination methods are an effective way of improving system performance.", "labels": [], "entities": []}, {"text": "This sentence is a good example for two challenges we face in this task.", "labels": [], "entities": []}, {"text": "First, almost half of all entities consist of noun phrases which has to be considered when constructing features.", "labels": [], "entities": []}, {"text": "Secondly, the vocabulary is domain dependent and therefore background knowledge should be adopted.", "labels": [], "entities": []}, {"text": "Previous approaches for semantic relation classification tasks mainly employed two strategies.", "labels": [], "entities": [{"text": "semantic relation classification tasks", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.8705013245344162}]}, {"text": "Either they made use of a lot of hand-crafted features or they utilized a neural network with as few background knowledge as possible.", "labels": [], "entities": []}, {"text": "The winning system of an earlier SemEval challenge on relation classification () adopted the first approach and achieved an F1 score of 82.2% (.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8389343917369843}, {"text": "relation classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.8511019945144653}, {"text": "F1 score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9875624477863312}]}, {"text": "Later, other works outperformed this approach by using CNNs with and without hand-crafted features () as well as RNNs (.", "labels": [], "entities": []}, {"text": "Approach We present two approaches that use different levels of preliminary information.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9484111070632935}]}, {"text": "Our first approach is inspired by the winning method of the SemEval-2010 challenge.", "labels": [], "entities": [{"text": "SemEval-2010 challenge", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.5766362547874451}]}, {"text": "It models semantic relations by describing the two entities, between which the semantic relation holds, as well as the words between those entities.", "labels": [], "entities": []}, {"text": "We call those in-between words the context of the semantic relation.", "labels": [], "entities": []}, {"text": "We classify relations by using an SVM on lexical features, such as part-of-speech tags.", "labels": [], "entities": []}, {"text": "Additionally we make use of semantic background knowledge and add pre-trained word embeddings to the SVM, as word embeddings have been shown to improve performance in a series of NLP tasks, such as sentiment analysis, question answering ( ) or relation extraction (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.9459876418113708}, {"text": "question answering", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.8701445460319519}, {"text": "relation extraction", "start_pos": 244, "end_pos": 263, "type": "TASK", "confidence": 0.8525819182395935}]}, {"text": "Besides using existing word embeddings generated from a general corpus, we also train embeddings on scientific articles that better reflect scientific vocabulary.", "labels": [], "entities": []}, {"text": "In contrast, our second approach relies on word embeddings only, which are fed into a convolutional long-short term memory (C-LSTM) network, a model that combines convolutional and recurrent neural networks ( . Therefore no hand-crafted features are used.", "labels": [], "entities": []}, {"text": "Because both CNN and RNN models have shown good performance for this task, we assume that a combination of them will positively impact classification performance compared to the individual models.", "labels": [], "entities": []}, {"text": "By combining lexical information and domainadapted scientific word embeddings, our system ClaiRE achieved an F1 score of 74.89% for the first subtask with manually annotated data and 78.39% for the second subtask with automatically annotated data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9880942106246948}]}], "datasetContent": [{"text": "After describing the two models we employ for relation classification, we now portray the data set we use and present results for both SVM and C-LSTM as micro-F1 and macro-F1.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.9235854148864746}]}, {"text": "The latter is the official evaluation score of the SemEval Challenge.", "labels": [], "entities": [{"text": "SemEval Challenge", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8337490260601044}]}, {"text": "We describe the experimental setup for both models and compare different feature sets and pretrained embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: SVM results for subtask 1.1.", "labels": [], "entities": []}, {"text": " Table 4: SVM results for subtask 1.2.", "labels": [], "entities": []}]}