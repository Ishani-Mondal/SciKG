{"title": [{"text": "ZMU at SemEval-2018 Task 11: Machine Comprehension Task using Deep Learning Models", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.5162967542807261}]}], "abstractContent": [{"text": "Machine Comprehension of text is atypical Natural Language Processing task which remains an elusive challenge.", "labels": [], "entities": [{"text": "Machine Comprehension of text", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.852306455373764}]}, {"text": "This paper is to solve the task 11 of SemEval-2018, Machine Comprehension using Commonsense Knowledge task.", "labels": [], "entities": []}, {"text": "We use deep learning model to solve the problem.", "labels": [], "entities": []}, {"text": "We build distributed word embedding of text, question and answering respectively instead of manually extracting features by linguistic tools.", "labels": [], "entities": []}, {"text": "Meanwhile, we use a series of frameworks such as CNN model, LSTM model, LSTM with attention model and biLSTM with attention model for processing word vector.", "labels": [], "entities": []}, {"text": "Experiments demonstrate the superior performance of biLSTM with attention framework compared to other models.", "labels": [], "entities": []}, {"text": "We also delete high frequency words and combine word vector and data augmentation methods, achieved a certain effect.", "labels": [], "entities": []}, {"text": "The approach we proposed rank 6th in official results, with accuracy rate of 0.7437 in test dataset.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9883732497692108}]}], "introductionContent": [{"text": "Machine Comprehension of text is one of the important goals of natural language processing.", "labels": [], "entities": [{"text": "Machine Comprehension of text", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.809675395488739}, {"text": "natural language processing", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6512082815170288}]}, {"text": "The traditional approaches to machine reading and comprehension have been based on either hand engineered grammars), or information extraction methods of detecting predicate argument triples that can later be queried as a relational database ().", "labels": [], "entities": [{"text": "machine reading and comprehension", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8409353941679001}, {"text": "information extraction", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7466219067573547}]}, {"text": "These methods show effectiveness, but they rely on feature extraction and language tools.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7056883871555328}]}, {"text": "Recently, with the advances of neural networks, there have been great interests in building neural architectures for various NLP task, including several pieces of work on machine comprehension (, which have gained significant performance in machine comprehension domain.", "labels": [], "entities": []}, {"text": "We also adopt deep learning models to solve this task.", "labels": [], "entities": []}, {"text": "The goal of Machine Comprehension using Commonsense Knowledge task is to choose a correct answer in two candidates to the question based on the contents of text.", "labels": [], "entities": []}, {"text": "This task relates to how the inclusion of commonsense knowledge in the form of script knowledge would benefit machine comprehension systems, answering the questions requires knowledge beyond the facts mentioned in the text.", "labels": [], "entities": []}, {"text": "We do not employ extra commonsense knowledge resources in the proposed approach, we assume that word vectors have contained some commonsense knowledge information, so we only use the deep learning model to solve this problem.", "labels": [], "entities": []}, {"text": "In the train dataset provided by this task, there are 1432 instances, each instance contains a text and several questions, and each question is associated with a set of two answers which are short and limited to a few words.", "labels": [], "entities": []}, {"text": "The texts used in this task cover more than 100 everyday scenarios, hence include a wide variety of human activities.", "labels": [], "entities": []}, {"text": "Therefore, each example can be summed up as {text, question, answer 0, answer 1, correct option}.", "labels": [], "entities": []}, {"text": "There are 9731, 1411, 2797 examples in train, validation, test datasets, respectively.", "labels": [], "entities": []}, {"text": "Being a binary classification task, we split an example into two triples, which are {text, question, answer 0} and {text, question, answer 1}, the label is true or false.", "labels": [], "entities": []}, {"text": "In validation and test datasets, we employ the same processing mode to determine the matching degree of fit between triples, the highest will be chosen.", "labels": [], "entities": []}, {"text": "We adopt the method of word distributed representation from () and a series of deep learning (DL) models, such as Convolutional Neural Network (CNN) from, Long Short-Term Memory (LSTM) model proposed from (Hochreiter and Schmidhuber, 1997) and improved by, attention mechanism from ().", "labels": [], "entities": [{"text": "word distributed representation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6251426736513773}]}, {"text": "The four main frameworks we applied are as follows: \u2022 CNN framework \u2022 LSTM with attention framework \u2022 biLSTM with attention framework Above the framework, a joint feature vector is constructed, which is used to classify.", "labels": [], "entities": []}, {"text": "In order to increase the accuracy of the model, we also delete high frequency word and combine word vector and data augmentation methods, thus achieve a better effect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993237257003784}]}, {"text": "Experiments demonstrate the superior performance of biLSTM with attention framework compared to other models, and data preprocessing is also important to improve the model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9924052357673645}]}], "datasetContent": [{"text": "Our approach in this task use the accuracy on validation dataset to locate the best parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9990193843841553}]}, {"text": "The final rate of accuracy is expressed in the correct proportion chosen in test dataset.", "labels": [], "entities": [{"text": "final rate of", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8002707958221436}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.7992662787437439}]}, {"text": "All the model parameters were adjusted by preliminary experiment, at the same time, the results are taken three times, and the average value is taken.", "labels": [], "entities": []}, {"text": "In the experiment, we use the loss function of categorical cross entropy and the optimizer of adaptive moment estimation.", "labels": [], "entities": []}, {"text": "The length of text, question and answer tokens sequence all take the maximum length, if the length is not enough, then zero is added.", "labels": [], "entities": []}, {"text": "To prevent over fitting, we employ dropout layers which the parameter is 0.3.", "labels": [], "entities": []}, {"text": "For comparison, we report the performance and analysis of four framework in, which summarizes the results of our system for this task.", "labels": [], "entities": []}, {"text": "All the experiments have deleted the high frequency words.", "labels": [], "entities": []}, {"text": "The word embedding we employed is word2vec in Rows (1) to.", "labels": [], "entities": []}, {"text": "Because in preliminary experiment, the accuracy of model using word2vec is generally better than Glove.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9996891021728516}]}, {"text": "In Row (1) to (2), we list the results on validation dataset and test dataset respectively of CNN framework which employ filter size of 3, and filter number of 64.", "labels": [], "entities": []}, {"text": "The difference is that Row (2) model uses the data augmentation.", "labels": [], "entities": []}, {"text": "Row (3) to (4) correspond to LSTM framework which uses 64 as output dimensionality parameter of LSTM unit.", "labels": [], "entities": []}, {"text": "The framework results in similar result with the CNN framework.", "labels": [], "entities": [{"text": "CNN framework", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8606687188148499}]}, {"text": "In Row (5) to (6), we can observe that the framework for using the attention mechanism has been significantly improved in the accuracy rate.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.9838269650936127}]}, {"text": "In Row to, the improvement from biLSTM with attention compared to LSTM with attention is remarkable, increase more than 2%, illustrating that Bi-directional LSTM can achieve more comprehensive features than unidirectional LSTM.", "labels": [], "entities": []}, {"text": "Row (9) is the approach proposed in this paper, which combines word2vec vector and Glove vector of each tokens.", "labels": [], "entities": [{"text": "Glove vector", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9576475322246552}]}, {"text": "The model gets a significantly result, achieving a precision of 77.75% in validation dataset and 74.37% in test dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9995782971382141}]}, {"text": "Compared to single word2vec, the improvement on the test set is more significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of four main framework", "labels": [], "entities": []}]}