{"title": [{"text": "Wolves at SemEval-2018 Task 10: Semantic Discrimination based on Knowledge and Association", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the system submitted to SemEval 2018 shared task 10 'Capturing Dis-criminative Attributes'.", "labels": [], "entities": [{"text": "SemEval 2018 shared task 10", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.782583212852478}, {"text": "Capturing Dis-criminative Attributes", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.8663380344708761}]}, {"text": "We use a combination of knowledge-based and co-occurrence features to capture the semantic difference between two words in relation to an attribute.", "labels": [], "entities": []}, {"text": "We define scores based on association measures, ngram counts, word similarity, and Concept-Net relations.", "labels": [], "entities": []}, {"text": "The system is ranked 4 th (joint) on the official leaderboard of the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "When it comes to investigating semantic similarities, it is worth noting that similarity between two words can be too general to quantify.", "labels": [], "entities": []}, {"text": "Accordingly, the discriminating power of a model is also important in limiting the scope of similarity between words.", "labels": [], "entities": []}, {"text": "The main idea behind distributional semantics, known as Distributional Hypothesis (DH), states that linguistic items with similar distributions have similar meanings.", "labels": [], "entities": []}, {"text": "Therefore these methods are biased towards finding similarities between concepts.", "labels": [], "entities": []}, {"text": "The SemEval shared task 10 'Capturing Discriminative Attributes' poses the new problem of semantic difference detection, thus putting difference, rather than similarity at the forefront.", "labels": [], "entities": [{"text": "SemEval shared task 10 'Capturing Discriminative Attributes'", "start_pos": 4, "end_pos": 64, "type": "TASK", "confidence": 0.8189463391900063}, {"text": "semantic difference detection", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7917559742927551}]}, {"text": "It is about modeling semantic difference in the case of already related words.", "labels": [], "entities": []}, {"text": "The idea is that while similarity can group words together in a generic way, understanding semantic differences sheds additional light on the meaning of each individual word.", "labels": [], "entities": []}, {"text": "A semantic model can potentially become more robust if it can benefit from sensitivity to differences alongside similarities in meaning.", "labels": [], "entities": []}, {"text": "Considering difference can also help researchers assess semantic representations more rigorously.", "labels": [], "entities": []}, {"text": "The effectiveness of a semantic similarity model can be evaluated further by quantifying its strength in finding differences between words.", "labels": [], "entities": []}, {"text": "In the shared task, semantic difference is operationalised as the relation between two semantically related words and a discriminative feature.", "labels": [], "entities": []}, {"text": "This relation is realised if the feature characterises only the first word.", "labels": [], "entities": []}, {"text": "An example is the triple airplane, helicopter, wings.", "labels": [], "entities": []}, {"text": "In this formulation, semantic difference is an asymmetric relation.", "labels": [], "entities": [{"text": "semantic difference", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.787729024887085}]}, {"text": "In this work, we compute several scores for word pairs and triples with the aim of capturing different semantic relations.", "labels": [], "entities": []}, {"text": "Specifically, we define scores based on a knowledge-based ontology and co-occurrence counts.", "labels": [], "entities": []}, {"text": "For knowledge-based features we rely on ConceptNet semantic network, and our co-occurrence based features are derived from association measures, ngrams and pre-trained embeddings.", "labels": [], "entities": []}, {"text": "We use the scores in both supervised and unsupervised scenarios to identify triples that constitute semantic difference; i.e. the attribute (third) word is discriminative between the first two words.", "labels": [], "entities": []}, {"text": "The code and data used for this system are freely available.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows: Section 2 describes related work.", "labels": [], "entities": []}, {"text": "Section 3 provides a description of the approach including the details of the features we use.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 discuss experiments and results.", "labels": [], "entities": []}, {"text": "Section 6 involves error analysis and some closing remarks and finally the paper concludes with Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the data as provided by the organisers of the shared task.", "labels": [], "entities": []}, {"text": "We train our model on the train set and find the optimised parameters based on the validations set.", "labels": [], "entities": []}, {"text": "Predictions were made on the held-out test data.", "labels": [], "entities": []}, {"text": "The final feature set is the collection of Disc Score measures based on the set of proposed scores.", "labels": [], "entities": [{"text": "Disc Score measures", "start_pos": 43, "end_pos": 62, "type": "METRIC", "confidence": 0.9001987973848978}]}, {"text": "As a result we have 6 associationbased scores, 2 google ngram based scores, 2 embedding based scores, and 1 ConceptNet score.", "labels": [], "entities": []}, {"text": "In total, we have 11 scores as our features.", "labels": [], "entities": []}, {"text": "In ConceptNet, reliability of each relation is given by a weight score.", "labels": [], "entities": [{"text": "reliability", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9962280988693237}]}, {"text": "We decided to ignore this information and opted for raw counts because it didn't help performance.", "labels": [], "entities": []}, {"text": "Furthermore, binarising the scores based on raw counts (with 0 as a threshold) slightly improved the results.", "labels": [], "entities": [{"text": "binarising", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9092095494270325}]}, {"text": "We use the features in both a supervised scenario (using SVM) and an unsupervised scenario (using KMeans).", "labels": [], "entities": []}, {"text": "In both cases all of the 11 features are exploited.", "labels": [], "entities": []}, {"text": "The evaluation in this shared task is in terms of the average of positive and negative F1-scores.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9917085766792297}]}, {"text": "In this paper, we report the precision, recall and F1-score for both positive and negative labels separately, along with the average F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9997201561927795}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9986540079116821}, {"text": "F1-score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9990481734275818}, {"text": "F1-score", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9925082325935364}]}, {"text": "shows the results on the validation set both in the supervised (SVM) and the unsupervised scenario.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Validation and TEST sets.", "labels": [], "entities": [{"text": "Validation", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9528520107269287}, {"text": "TEST", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.5590976476669312}]}]}