{"title": [{"text": "TakeLab at SemEval-2018 Task 7: Combining Sparse and Dense Features for Relation Classification in Scientific Texts", "labels": [], "entities": [{"text": "Relation Classification in Scientific Texts", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.8453533291816712}]}], "abstractContent": [{"text": "We describe two systems for semantic relation classification with which we participated in the SemEval 2018 Task 7, subtask 1 on semantic relation classification: an SVM model and a CNN model.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6968028942743937}, {"text": "SemEval 2018 Task 7", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7648773938417435}, {"text": "semantic relation classification", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.6456879675388336}]}, {"text": "Both models combine dense pretrained word2vec features and hancrafted sparse features.", "labels": [], "entities": []}, {"text": "For training the models, we combine the two datasets provided for the sub-tasks in order to balance the under-represented classes.", "labels": [], "entities": []}, {"text": "The SVM model performed better than CNN, achieving an F1-macro score of 69.98% on subtask 1.1 and 75.69% on subtask 1.2.", "labels": [], "entities": [{"text": "CNN", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9395322203636169}, {"text": "F1-macro score", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.980752170085907}]}, {"text": "The system ranked 7th among 28 submissions on subtask 1.1 and 7th among 20 submissions on subtask 1.2.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ().", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9535433650016785}, {"text": "information extraction task", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7898620963096619}, {"text": "detecting and classifying semantic relations between entities in text", "start_pos": 79, "end_pos": 148, "type": "TASK", "confidence": 0.7126942442523109}]}, {"text": "The task essentially induces structure from unstructured textual information, allowing us to obtain valuable information about the way in which entities interact, thus improving human capacity to analyze (often large) quantities of textual data.", "labels": [], "entities": []}, {"text": "Relation extraction is typically framed as a classification task: pairs of entities from a document are inspected and the type of relation is predicted by means of local linguistic cues ().", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9821431636810303}]}, {"text": "Relation extraction has been extensively studied in the literature; see) fora comprehensive overview.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9671007096767426}]}, {"text": "group the relation extraction approaches into three classes: (1) knowledge-based methods, (2) supervised methods, and (3) self-supervised methods.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8413881659507751}]}, {"text": "Traditional approaches mostly relied on shallow machine learning models with handcrafted features () and specific kernel methods (.", "labels": [], "entities": []}, {"text": "Some systems leverage unlabeled data to improve classification and use semi-supervised or unsupervised learning).", "labels": [], "entities": [{"text": "classification", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9592403769493103}]}, {"text": "The current state of the art is a deep recurrent neural network model by.", "labels": [], "entities": []}, {"text": "Most research on relation extraction has leveraged standard benchmark datasets from the ACE ( and.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9582522511482239}, {"text": "ACE", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.9828631281852722}]}, {"text": "This paper describes the systems with which we participated in the SemEval 2018 task 7 on Semantic relation extraction and classification in scientific papers.", "labels": [], "entities": [{"text": "SemEval 2018 task 7", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8590532392263412}, {"text": "Semantic relation extraction and classification", "start_pos": 90, "end_pos": 137, "type": "TASK", "confidence": 0.7559059262275696}]}, {"text": "We focused on the subtask 1 (relation classification), which featured two scenarios: 1.1 Relation classification on clean (i.e., manually annotated) data and 1.2 Relation classification on noisy (i.e., automatically annotated) data.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7572595477104187}, {"text": "Relation classification", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.9042493402957916}]}, {"text": "Both scenarios start outwith pre-extracted entity pairs, which makes the task simpler than relation extraction proper.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8255332708358765}]}, {"text": "On the other hand, the task remains challenging because of the choice of the domain: scientific publications abound with complex syntactic structures and rely on specialist terminology, which makes it more difficult to predict the correct relation type.", "labels": [], "entities": []}, {"text": "We framed the problem as a supervised classification task and devised two models: a support vector machine (SVM) model, which utilizes a rich set of features combining dense pretrained word2vec features and handcrafted sparse features, and a convolutional neural network (CNN) model.", "labels": [], "entities": []}, {"text": "The best result was achieved with the SVM model, which ranked 7th in both scenarios of subtask 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "The organizers have provided different training datasets for scenarios 1.1 and 1.2 of subtask 1, each consisting of 350 abstracts of scientific papers from the Natural Language Processing (NLP) domain to be used for training the models.", "labels": [], "entities": []}, {"text": "All entities representing domain concepts (e.g., word sense disambiguation and translation) are already annotated and listed in pairs according to one of the relations that holds between them.", "labels": [], "entities": [{"text": "word sense disambiguation and translation)", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.6887170970439911}]}, {"text": "For example, in the sentence \"High quality translation via word sense disambiguation and accurate word order generation of the target language\", the entity word sense disambiguation is used for translation, hence it is annotated as an instance of the USAGE relation type.", "labels": [], "entities": [{"text": "word order generation", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.6267751355965933}, {"text": "USAGE relation type", "start_pos": 251, "end_pos": 270, "type": "DATASET", "confidence": 0.9103078444798788}]}, {"text": "There are six distinct relation types: USAGE, TOPIC, COMPARE, MODEL-FEATURE, RESULT, PART-WHOLE.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8827276825904846}, {"text": "TOPIC", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.7894307971000671}, {"text": "COMPARE", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.6865891218185425}, {"text": "MODEL-FEATURE", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.986781656742096}, {"text": "RESULT", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.993603527545929}, {"text": "PART-WHOLE", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.8162841200828552}]}, {"text": "Except for COMPARE, all relations are asymmetric, which means that the direction of relation matters.", "labels": [], "entities": [{"text": "COMPARE", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.7869570255279541}]}, {"text": "For this reason, every asymmetrical relation instance has additionally been annotated with the direction of the relation using the \"reverse\" flag to indicate that the order of entities should be flipped.", "labels": [], "entities": []}, {"text": "The total number of training instances is 1228 and 1248 for subtask 1.1 and subtask 1.2 datasets, respectively.", "labels": [], "entities": []}, {"text": "Each instance contains exactly two entities and both appear in the same sentence.", "labels": [], "entities": []}, {"text": "The subtask 1.2 dataset uses the same annotation scheme as the first subtask, but the entities are automatically extracted rather than manually annotated, thus introducing noise.", "labels": [], "entities": []}, {"text": "shows the breakdown of relation types for the two datasets.", "labels": [], "entities": []}, {"text": "In general, the class distribution is rather imbalanced, especially the TOPIC relation, which is heavily under-represented in the dataset for subtask 1.1.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9463152885437012}]}, {"text": "As subtask 1.1 and 1.2 are very similar and differ only in how the labeling was carried out (manual or automatic), we decided to combine the training sets of the two tasks in one training set.", "labels": [], "entities": []}, {"text": "This allowed us to increase the number of instances for the TOPIC relation type, which was severely under-represented in subtask 1.1 training set (cf..", "labels": [], "entities": [{"text": "TOPIC relation type", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.5617086887359619}]}, {"text": "The hyperparameters for the SVM model were selected using cross-validated grid search.", "labels": [], "entities": []}, {"text": "The CNN model was trained using early stopping with batch size of 64.", "labels": [], "entities": []}, {"text": "Kernel sizes of the convolution layers are 3, 4, and 5 words, each size having 32 kernels.", "labels": [], "entities": []}, {"text": "Adam algorithm was used for the model optimization.", "labels": [], "entities": [{"text": "model optimization", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8012039661407471}]}, {"text": "A dropout rate of 0.5 was used during the training.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9631955027580261}]}, {"text": "The evaluation was performed on the test sets provided by the task organizers.", "labels": [], "entities": []}, {"text": "Each test set is comprised of 150 scientific paper abstracts with 350 relation instances.", "labels": [], "entities": []}, {"text": "shows macroaveraged precision, recall, and F1 score for the two models on the two test sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9814987778663635}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9997840523719788}, {"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9723232090473175}]}, {"text": "Differently from previous findings, in our case the SVM outperformed the CNN model.", "labels": [], "entities": []}, {"text": "This might be explained by the relatively small size size of the training set.", "labels": [], "entities": []}, {"text": "On subtask 1.1, the CNN performs slightly worse than the SVM model, while it fails completely on the second subtask.", "labels": [], "entities": []}, {"text": "We presume this might be due to an implementation error, but we were unable to identify the problem.", "labels": [], "entities": []}, {"text": "In the official SemEval competition, the SVM model ranked 7th among 28 submissions on subtask 1.1 and 7th among 20 submissions on subtask 1.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Class distribution in subtask 1 training datasets for scenario 1.1 (clean annotation) and scenario 1.2 (noisy  annotation). Last column shows the combined counts of both datasets.", "labels": [], "entities": []}, {"text": " Table 2: Relation classification results", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9601195752620697}]}]}