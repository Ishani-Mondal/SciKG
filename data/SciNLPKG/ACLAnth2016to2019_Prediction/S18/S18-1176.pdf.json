{"title": [{"text": "CSReader at SemEval-2018 Task 11: Multiple Choice Question Answering as Textual Entailment", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7612346808115641}, {"text": "Multiple Choice Question Answering", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7120748460292816}]}], "abstractContent": [{"text": "In this document we present an end-to-end machine reading comprehension system that solves multiple choice questions with a tex-tual entailment perspective.", "labels": [], "entities": []}, {"text": "Since some of the knowledge required is not explicitly mentioned in the text, we try to exploit commonsense knowledge by using pretrained word em-beddings during contextual embeddings and by dynamically generating a weighted representation of related script knowledge.", "labels": [], "entities": []}, {"text": "In the model two kinds of prediction structure are en-sembled, and the final accuracy of our system is 10 percent higher than the naiive baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.99815434217453}]}], "introductionContent": [{"text": "K. M. stated that machine reading comprehension can be defined as a task that deals with the automatic understanding of texts.", "labels": [], "entities": [{"text": "machine reading comprehension", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8473094900449117}]}, {"text": "In their paper, it was also mentioned that machine comprehension can be evaluated by two methods, namely (1) translating the text into formal language representations and evaluating it using structured queries.", "labels": [], "entities": []}, {"text": "(2) evaluating it through natural language questions.", "labels": [], "entities": []}, {"text": "Recently a lot of datasets are available for evaluating machine reading comprehension systems, for example, there are SQuAD() and the MCTest().", "labels": [], "entities": []}, {"text": "On many of these datasets human-like performance has been achieved.", "labels": [], "entities": []}, {"text": "However, one of the biggest challenges in machine comprehension is how to provide commonsense knowledge regarding daily events to machines.", "labels": [], "entities": []}, {"text": "The SemEval2018-Task11() provides a dataset containing questions that can only be answered with the help of commonsense knowledge.To address this problem, we first propose a model to solve normal reading comprehension problems and then try to modify the model to embody commonsense knowledge.", "labels": [], "entities": []}, {"text": "In section two, we give a brief introduction to ideas and models that might be useful to a comprehensive understanding of our work.", "labels": [], "entities": []}, {"text": "Section three carefully describes our model implementation, and why we chose this kind of model structure.", "labels": [], "entities": []}, {"text": "And section four briefly examines the datasets used.", "labels": [], "entities": []}, {"text": "Section five provides a simple evaluation of our result.", "labels": [], "entities": []}, {"text": "Finally, our conclusion can be found in section six.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}