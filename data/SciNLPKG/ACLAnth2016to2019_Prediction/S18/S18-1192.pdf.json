{"title": [{"text": "TakeLab at SemEval-2018 Task12: Argument Reasoning Comprehension with Skip-Thought Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our system for the SemEval-2018 Task 12: Argument Reasoning Comprehension Task.", "labels": [], "entities": [{"text": "SemEval-2018 Task 12: Argument Reasoning Comprehension Task", "start_pos": 40, "end_pos": 99, "type": "TASK", "confidence": 0.8554143756628036}]}, {"text": "We utilize skip-thought vectors, sentence-level distributional vectors inspired by the popular word embeddings and the skip-gram model.", "labels": [], "entities": []}, {"text": "We encode preprocessed sentences from the dataset into vectors, then perform a binary supervised classification of the warrant that justifies the use of the reason as support for the claim.", "labels": [], "entities": []}, {"text": "We explore a few variations of the model, reaching 54.1% accuracy on the test set, which placed us 16th out of 22 teams participating in the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9995662569999695}]}], "introductionContent": [{"text": "Reasoning is the process of thinking in a logical way to form a conclusion.", "labels": [], "entities": []}, {"text": "Inferring conclusions using commonsense reasoning has become a popular topic in NLP.", "labels": [], "entities": [{"text": "Inferring conclusions", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9182350039482117}]}, {"text": "Textual entailment (TE) aims to determine whether a hypothesis can be inferred from a premise ().", "labels": [], "entities": [{"text": "Textual entailment (TE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8302706778049469}]}, {"text": "Approaches to solving TE have ranged from robust approaches based on shallow lexical and semantic features) to formal computational semantics approaches based on translating sentences into logical form (.", "labels": [], "entities": [{"text": "solving TE", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.7584913074970245}]}, {"text": "The current stateof-the-art approaches to TE use deep learning for natural logic inference to capture human deductive reasoning.", "labels": [], "entities": [{"text": "TE", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9928929209709167}]}, {"text": "In online discussions, when arguing for or against a stance, people provide arguments leaving their readers to rely on commonsense and nondeductive reasoning to evaluate the validity of their arguments.", "labels": [], "entities": []}, {"text": "Human annotators can infer the reasons from claims fairly well, even when additional (implicit) premises are required to make reasoning deductive.", "labels": [], "entities": []}, {"text": "emphasize the importance of implicit premises in argumentation by introducing the argument reasoning comprehension task, where one chooses between two mutually exclusive warrants to make a reason warrant the claim.", "labels": [], "entities": []}, {"text": "They demonstrate that human experts can perform this task extremely well (up to 90% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9987900853157043}]}, {"text": "In this paper, we describe a system for solving the argument reasoning comprehension task, with which we participated in the SemEval-2018 Task 12.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.8342656344175339}, {"text": "SemEval-2018 Task 12", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.6777514219284058}]}, {"text": "Given the reason Rand claim C, debate title, debate description, and two warrants, W 1 and W 2 , the task is to choose warrant W that justifies the use of R as support for C.", "labels": [], "entities": []}, {"text": "For all warrant pairs (W 1 , W 2 ), it holds that if warrant W 1 is W , then W 2 is \u00acW , which justifies the use of R as support for \u00acC and vice versa.", "labels": [], "entities": []}, {"text": "Our system frames the problem as supervised classification and utilizes skip-thought vectors to represent sentences.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.733826756477356}]}, {"text": "Our system (TakeLab) ranked 16th out of 22 systems submitted to the SemEval-2018 Task 12, achieving 54.1% accuracy on the test set and 69.0% on the development set.", "labels": [], "entities": [{"text": "SemEval-2018 Task 12", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.6089990735054016}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9992163181304932}]}], "datasetContent": [{"text": "The dataset consists of 1210 training instances, 317 validation instances, and 445 test instances.", "labels": [], "entities": []}, {"text": "Each instance is a tuple (W 1 , W 2 , R, C, debateTitle, debateInfo, y), with y as the label of the correct warrant (0 for W 1 or 1 for W 2 ).", "labels": [], "entities": []}, {"text": "Among the 1210 training instances, there are 111 different debate titles and 169 different claims, indicating the diversity of the training set.", "labels": [], "entities": []}, {"text": "Furthermore, we found that 47.75% of the debate titles had unanimous claims (all for or all against) and 56.69% of the claims were affirmative, but only 21.62% had a balanced number of claims for both sides of the debate (a difference of 10% or less).", "labels": [], "entities": []}, {"text": "The debate title Do We Still Need Libraries? was the most common debate title, and it had unanimously affirmative claims.", "labels": [], "entities": []}, {"text": "Around 35% of the instances contained warrants worded differently, as opposed to being directly negated (by adding not).", "labels": [], "entities": []}, {"text": "All of this presented a challenge in training the system, since the dataset is small, highly variable, and involves multiple domains.", "labels": [], "entities": []}], "tableCaptions": []}