{"title": [{"text": "Solving Feature Sparseness in Text Classification using Core-Periphery Decomposition", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7338115125894547}]}], "abstractContent": [{"text": "Feature sparseness is a problem common to cross-domain and short-text classification tasks.", "labels": [], "entities": [{"text": "cross-domain and short-text classification tasks", "start_pos": 42, "end_pos": 90, "type": "TASK", "confidence": 0.7265173614025116}]}, {"text": "To overcome this feature sparseness problem, we propose a novel method based on graph decomposition to find candidate features for expanding feature vectors.", "labels": [], "entities": []}, {"text": "Specifically , we first create a feature-relatedness graph, which is subsequently decomposed into core-periphery (CP) pairs and use the peripheries as the expansion candidates of the cores.", "labels": [], "entities": []}, {"text": "We expand both training and test instances using the computed related features and use them to train a text classifier.", "labels": [], "entities": []}, {"text": "We observe that prioritising features that are common to both training and test instances as cores during the CP decomposition to further improve the accuracy of text classification.", "labels": [], "entities": [{"text": "CP decomposition", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.8651066720485687}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9984009861946106}, {"text": "text classification", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7826499044895172}]}, {"text": "We evaluate the proposed CP-decomposition-based feature expansion method on benchmark datasets for cross-domain sentiment classification and short-text classification.", "labels": [], "entities": [{"text": "CP-decomposition-based feature expansion", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.6557011206944784}, {"text": "cross-domain sentiment classification", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.7477989594141642}, {"text": "short-text classification", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6915430873632431}]}, {"text": "Our experimental results show that the proposed method consistently outperforms all baselines on short-text classification tasks, and perform competitively with pivot-based cross-domain sentiment classification methods.", "labels": [], "entities": [{"text": "short-text classification tasks", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.774901270866394}, {"text": "cross-domain sentiment classification", "start_pos": 173, "end_pos": 210, "type": "TASK", "confidence": 0.6451554298400879}]}], "introductionContent": [{"text": "Short-texts are abundant on the Web and appear in various different formats such as microblogs (), Question and Answer (QA) forums, review sites, Short Message Service (SMS), email, and chat messages (.", "labels": [], "entities": [{"text": "Question and Answer (QA) forums", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.7312568511281695}]}, {"text": "Unlike lengthy responses that take time to both compose and to read, short responses have gained popularity particularly in social media contexts.", "labels": [], "entities": []}, {"text": "Considering the steady growth of mobile devices that are physically restricted to compact keyboards, which are suboptimal for entering lengthy text inputs, it is safe to predict that the amount of short-texts will continue to grow in the future.", "labels": [], "entities": []}, {"text": "Considering the importance and the quantity of the short-texts in various web-related tasks, such as text classification (kun, and event prediction (, it is important to be able to accurately represent and classify short-texts.", "labels": [], "entities": [{"text": "text classification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7650212347507477}, {"text": "event prediction", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.7381851673126221}]}, {"text": "Compared to performing text mining on longer texts (), for which dense and diverse feature representations can be created relatively easily, handling of shorter texts poses several challenges.", "labels": [], "entities": [{"text": "text mining", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.7358385026454926}]}, {"text": "The number of features that are present in a given short-text will be a small fraction of the set of all features that exist in all of the train instances.", "labels": [], "entities": []}, {"text": "Moreover, frequency of a feature in a short-text will be small, which makes it difficult to reliably estimate the salience of a feature using term frequency-based methods.", "labels": [], "entities": []}, {"text": "This is known as the feature sparseness problem in text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8054952323436737}]}, {"text": "Feature sparseness is not unique to shorttext classification but also encountered in crossdomain text classification, where the training and test data are selected from different domains with small intersection of feature spaces.", "labels": [], "entities": [{"text": "shorttext classification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.839998334646225}, {"text": "crossdomain text classification", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.6978712777296702}]}, {"text": "In the domain adaptation (DA) setting, a classifier trained on one domain (source) might be agnostic to the features that are unique to a different domain (target), which results in a feature mismatch problem similar to the feature-sparseness problem discussed above.", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8346394062042236}]}, {"text": "To address the feature sparseness problem encountered in short-text and cross-domain classification tasks, we propose a novel method that computes related features that can be appended to the feature vectors to reduce the sparsity.", "labels": [], "entities": [{"text": "cross-domain classification tasks", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.7908415595690409}]}, {"text": "Specifically, we decompose a feature-relatedness graph into core-periphery (CP) structures, where a core feature (a vertex) is linked to a set of peripheries (also represented by vertices), indicating the connectivity of the graph.", "labels": [], "entities": []}, {"text": "This graph decomposition problem is commonly known as the CPdecomposition.", "labels": [], "entities": []}, {"text": "Our proposed CP-decomposition algorithm significantly extends existing CP-decomposition methods in three important ways.", "labels": [], "entities": []}, {"text": "\u2022 First, existing CP-decomposition methods consider unweighted graphs, whereas edges in feature-relatedness graphs are weighted (possibly nonnegative) real-valued featurerelatedness scores such as positive pointwise mutual information (PPMI).", "labels": [], "entities": []}, {"text": "Our proposed CP-decomposition method can operate on edge-weighted graphs.", "labels": [], "entities": []}, {"text": "\u2022 Second, considering the fact that in text classification a particular periphery can be related to more than one core, we relax the hard assignment constraints on peripheries and allow a particular periphery attach to multiple cores.", "labels": [], "entities": [{"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7555338740348816}]}, {"text": "\u2022 Third, prior work on pivot-based crossdomain sentiment classification methods have used features that are frequent in training (source) and test (target) data as expansion candidates to overcome the feature mismatch problem.", "labels": [], "entities": [{"text": "crossdomain sentiment classification", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.7325709263483683}]}, {"text": "Inspired by this, we define coreness of a feature as the pointwise mutual information between a feature and the source/target domains.", "labels": [], "entities": []}, {"text": "The CPdecomposition algorithm we propose will then compute the set of cores considering both structural properties of the graph as well as the coreness values computed from the train/test data.", "labels": [], "entities": []}, {"text": "To perform feature vector expansion, we first construct a feature-relatedness graph, where vertices correspond to features and the weight of the undirected edge connecting two features represent the relatedness between those two features.", "labels": [], "entities": [{"text": "feature vector expansion", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7079286972681681}]}, {"text": "Different features and relatedness measures can be flexibly used in the proposed graph construction.", "labels": [], "entities": []}, {"text": "In our experiments, we use the simple (yet popular and effective) setting of n-gram features as vertices and compute their relatedness using PPMI.", "labels": [], "entities": []}, {"text": "We compute the coreness of features as the sum of the two PPMI values between the feature and the source, and the feature and the target domains.", "labels": [], "entities": []}, {"text": "Next, CP-decomposition is performed on this feature-relatedness graph to obtain a set of core-periphery structures.", "labels": [], "entities": []}, {"text": "We then rank the set of peripheries of a particular core by their PPMI values, and select the top-ranked peripheries as the expansion features of the core.", "labels": [], "entities": []}, {"text": "We expand the core features in training and train a logistic regressionbased binary classifier using the expanded feature vectors, and evaluate its performance on the expanded test feature vectors.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of the proposed method using benchmark datasets for two different tasks: short-text classification and crossdomain sentiment classification.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.6823598891496658}, {"text": "crossdomain sentiment classification", "start_pos": 133, "end_pos": 169, "type": "TASK", "confidence": 0.8136714895566305}]}, {"text": "Experimental results on short-text classification show that the proposed method consistently outperforms previously proposed feature expansion-based methods for short-text classification and even some of the sentence embedding learning-based methods.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.8258561491966248}, {"text": "short-text classification", "start_pos": 161, "end_pos": 186, "type": "TASK", "confidence": 0.7693864703178406}]}, {"text": "Moreover, the consideration of coreness during the CP-decomposition improves the text classification accuracy.", "labels": [], "entities": [{"text": "text classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7746917605400085}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9518203735351562}]}, {"text": "In cross-domain sentiment classification experiments, the proposed method outperforms previously proposed pivot-based methods such as the structural correspondence learning (SCL)).", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7394601901372274}, {"text": "structural correspondence learning (SCL))", "start_pos": 138, "end_pos": 179, "type": "TASK", "confidence": 0.6748153666655222}]}], "datasetContent": [{"text": "We evaluate the proposed method on two tasks: short-text classification (a non-DA task) and crossdomain sentiment classification (a DA task).", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6790820211172104}, {"text": "crossdomain sentiment classification", "start_pos": 92, "end_pos": 128, "type": "TASK", "confidence": 0.7954878608385721}]}, {"text": "For short-text classification we use the Stanford sentiment treebank (TR) 3 , customer reviews dataset (CR) (), subjective dataset (SUBJ) () and movie reviews (MR) (Pang and).", "labels": [], "entities": [{"text": "Stanford sentiment treebank (TR) 3", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.9040102873529706}, {"text": "subjective dataset (SUBJ)", "start_pos": 112, "end_pos": 137, "type": "METRIC", "confidence": 0.6656256556510926}]}, {"text": "For DA we use Amazon multi-domain sentiment dataset () containing product reviews from four categories: Books (B), DVDs (D), Electronics (E) and Kitchen Appliances (K).", "labels": [], "entities": [{"text": "DA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9576238989830017}, {"text": "Amazon multi-domain sentiment dataset", "start_pos": 14, "end_pos": 51, "type": "DATASET", "confidence": 0.7862214595079422}]}, {"text": "Each category is regarded as a domain and has 1000 positive and 1000 negative reviews, and a large number of unlabelled reviews.", "labels": [], "entities": []}, {"text": "We train a classifier on 12 domain pairs adapting from source to target (S-T): For the short-text classification datasets, we use the official train/test split.", "labels": [], "entities": []}, {"text": "We represent each instance (document) using a bag-of-features consisting of unigrams.", "labels": [], "entities": []}, {"text": "Stop words are removed using a standard stop words list.", "labels": [], "entities": []}, {"text": "We train an 2 regularised binary logistic regression classifier with each dataset, where the regularisation coefficient is tuned via 5-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the short-text classification task. For each dataset, the best results are shown in bold.", "labels": [], "entities": [{"text": "short-text classification task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7895065248012543}]}, {"text": " Table 3: Proposed vs. feature-based methods for  short-text classification.", "labels": [], "entities": [{"text": "short-text classification", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7168824672698975}]}, {"text": " Table 2: Results for DA tasks. For each S-T pair, the best results are shown in bold. The last row shows  the average of performance over the 12 S-T pairs.", "labels": [], "entities": [{"text": "DA tasks", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.9364343583583832}]}, {"text": " Table 1.  SCL performs poorly on this non-DA task, indicat- ing that it is specifically customised for DA tasks.", "labels": [], "entities": []}]}