{"title": [{"text": "IronyMagnet at SemEval-2018 Task 3: A Siamese Network for Irony Detection in Social Media", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.6397407650947571}]}], "abstractContent": [{"text": "This paper describes our system, entitled", "labels": [], "entities": []}], "introductionContent": [{"text": "Irony is one of the most prominent and pervasive figures of speech inhuman communication, dating back to ancient religious texts to modern microtexts.", "labels": [], "entities": [{"text": "speech inhuman communication", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6998627980550131}]}, {"text": "According to literary scholars, irony has been defined as a trope where the speaker intends to communicate a contradictory situation or the opposite meaning of what is literally said.", "labels": [], "entities": []}, {"text": "It adopts a subtle technique where incongruity is used to suggest a distinction between reality and expectation in order to produce a humorous or emphatic effect on the listener.", "labels": [], "entities": []}, {"text": "Irony poses an important challenge not only from a linguistic perspective but also from a cognitive one.", "labels": [], "entities": []}, {"text": "Even without a solid understanding of irony, one can still recognize and produce 1 https://nlp.stanford.edu/projects/glove/ ironic utterances from as early as childhood.", "labels": [], "entities": []}, {"text": "Such capabilities are often associated with one's ability to correctly infer others' communicative intentions and perspectives towards a given situation.", "labels": [], "entities": []}, {"text": "Psychological theories, such as \"echoic reminder theory\", \"allusion pretense theory\", and \"implicit display theory\", confirm that cues for understanding ironic intent are not restricted to language.", "labels": [], "entities": [{"text": "echoic reminder", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.95573890209198}, {"text": "understanding ironic intent", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.7970088124275208}]}, {"text": "Ironic intent involves several other aspects including, but not limited to, the context of an utterance, the world's perception and familiarity between the listener and the speaker, and psychological dimensions.", "labels": [], "entities": []}, {"text": "Thus, as a purely text classification task, the irony detection task poses a significant challenge for computational linguists.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7899210751056671}, {"text": "irony detection task", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.900688111782074}]}, {"text": "Computational approaches focus on identifying the subtle incongruity between different parts of the text.", "labels": [], "entities": []}, {"text": "Often, an ironic statement starts with an overtly positive attitude (\"Yay I love\") and ends up in an disappointment (\"working on my birthday\") or a negative attitude/statement (\"another outage in less than 8 hours.\") followed by an appreciation (\"Keep up the good work!\") or an incident (\"I asked God to protect me from my enemies\") resulting in a completely unexpected output (\"shortly after I started losing friends\").", "labels": [], "entities": []}, {"text": "Due to the limited scope of expression in social media such as Twitter 2 , authors often end up lacing their statements with ironic cue words ('Yay') or social media specific features such as hashtags ('#irony') to make their ironic intent more obvious for the reader.", "labels": [], "entities": []}, {"text": "Following this intuition, most of the attempts were made using probabilistic classification models which relied on textual cues such as lexical indicators like punctuation symbols (e.g., '?'), interjections (e.g., \"gee\" or \"gosh\"), and intensifiers (; the juxtaposition of positive sentiment and negative situations (; discriminative N-grams like 'yay!' or 'great!' or \"oh really\" or \"yeah right\"; social media markers like hashtags ( ; emoticon usage (); and topics associated with irony (e.g. schools, dentists, church life, public transport, the weather).", "labels": [], "entities": []}, {"text": "exploited text patterns in comments on articles of online newspapers to detect ironic statements, while Van Hee et al.", "labels": [], "entities": []}, {"text": "(2016) developed a irony detection model using support vector machine (SVM) with a combination of lexical, syntactic, sentiment, and semantic (Word2Vec embedding) features.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.9465140998363495}]}, {"text": "In recent times, multiple research attempts, founded on variants of the deep neural network built on top of word embeddings, showed a significant improvement over traditional methods over several natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "A few representative works in this direction for detecting sarcasm, a demeaning variant of irony, especially in the colloquial form, are based on Convolutional Neural Networks(CNN) (, Recurrent Neural Networks (RNN) () and a combination of.", "labels": [], "entities": [{"text": "detecting sarcasm", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8890929222106934}]}, {"text": "Siamese networks, widely used in image classification, have displayed a good performance over sentence similarity or document similarity tasks.", "labels": [], "entities": [{"text": "image classification", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7394708842039108}, {"text": "sentence similarity or document similarity", "start_pos": 94, "end_pos": 136, "type": "TASK", "confidence": 0.6208004772663116}]}, {"text": "A Siamese architecture contains two identical sub-networks, which are trained with two different inputs to distinguish the difference among them.", "labels": [], "entities": []}, {"text": "Since in irony, different parts of a sentence can be incongruous with each other, we adopted the Siamese architecture to detect such incongruity between different sections of a sentence.", "labels": [], "entities": []}, {"text": "In this implementation, each of the subnetworks consists of an embedding layer and a LSTM layer.", "labels": [], "entities": []}, {"text": "We slice each input sentence into two fragments and feed them to the subnetworks.", "labels": [], "entities": []}, {"text": "The output representations of subnetworks are then compared to distinguish if the two fragments are incongruous or not i.e. a sentence is considered as non-ironic if two fragments of a sentence are semantically nonincongruous, otherwise it is ironic.", "labels": [], "entities": []}], "datasetContent": [{"text": "The train and test dataset consists of 3834 tweets and 784 tweets, respectively.", "labels": [], "entities": []}, {"text": "The train dataset for Task 1 is balanced but in task 2, the prominent category was \"irony by polarity contrast\" (table 1).", "labels": [], "entities": []}, {"text": "Since a Siamese network expects two inputs and performs a comparison between the generated weights, each tweet is split into two parts.", "labels": [], "entities": []}, {"text": "A number of training examples are generated by splitting the tweets where each split contains a minimum r number of words.", "labels": [], "entities": []}, {"text": "Consider as an input a tweet containing n words with label y.", "labels": [], "entities": []}, {"text": "The tweet will be split into (n \u2212 2 \u00d7 r + 1) combinations.", "labels": [], "entities": []}, {"text": "In this experiment, the value of r is chosen as 3.", "labels": [], "entities": []}, {"text": "For example, \"Working on Boxing Day is so fun\" will produce the following combinations: 1.", "labels": [], "entities": [{"text": "Boxing Day", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9231119453907013}]}, {"text": "(\"Working on Boxing\", \"Day is so fun\") 2.", "labels": [], "entities": []}, {"text": "(\"Working on Boxing Day\", \"is so fun\") For training purposes, the train dataset is split with a 90%-10% split ratio.", "labels": [], "entities": [{"text": "Boxing Day\"", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9419565399487814}]}, {"text": "Success with a neural network model largely depends on the apt input and optimal hyperparameters settings.", "labels": [], "entities": []}, {"text": "After investigating different combinations of hyper-parameters, the optimal setting is obtained for each layer of the network.", "labels": [], "entities": []}, {"text": "The LSTM has 32 hidden memory units with a sigmoid activation function and recurrent dropout ratio of 0.5.", "labels": [], "entities": [{"text": "recurrent dropout ratio", "start_pos": 75, "end_pos": 98, "type": "METRIC", "confidence": 0.9095498323440552}]}, {"text": "The fully-connected layer consisted of 16 hidden memory units and uses ReLu as the activation function.", "labels": [], "entities": []}, {"text": "Both of the layers are initialized with Xavier normal initializer.", "labels": [], "entities": []}, {"text": "As an optimizer function, Adam optimization is used with a learning rate set to 0.001, while categorical cross-entropy is chosen as a loss function.", "labels": [], "entities": []}, {"text": "The code is developed using the keras 7 library.", "labels": [], "entities": [{"text": "keras 7 library", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.860270361105601}]}], "tableCaptions": []}