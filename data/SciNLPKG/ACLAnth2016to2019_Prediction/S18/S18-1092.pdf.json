{"title": [{"text": "ELiRF-UPV at SemEval-2018 Tasks 1 and 3: Affect and Irony Detection in Tweets", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the participation of ELiRF-UPV team at tasks 1 and 3 of Semeval-2018.", "labels": [], "entities": []}, {"text": "We present a deep learning based system that assembles Convolutional Neural Networks and Long Short-Term Memory neu-ral networks.", "labels": [], "entities": []}, {"text": "This system has been used with slight modifications for the two tasks addressed both for English and Spanish.", "labels": [], "entities": []}, {"text": "Finally, the results obtained in the competition are reported and discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The study of figurative language and affective information expressed in texts is of great interest in sentiment analysis applications because they can change the polarity of a message.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.9515222012996674}]}, {"text": "The objective of tasks 1 and 3 of Semeval 2018 is the study of these phenomena on Twitter.", "labels": [], "entities": []}, {"text": "Task 1 () is related to Affect in Tweets.", "labels": [], "entities": [{"text": "Affect", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9389265179634094}]}, {"text": "Systems have to automatically determine the intensity of emotions and intensity of sentiment or valence of the tweeters from their tweets.", "labels": [], "entities": []}, {"text": "The task is divided in five subtasks: emotion intensity regression (EI-Reg), emotion intensity ordinal classification (EI-Oc), sentiment intensity regression (V-Reg), sentiment analysis ordinal classification (V-Oc) and emotion classification (E-C).", "labels": [], "entities": [{"text": "sentiment analysis ordinal classification", "start_pos": 167, "end_pos": 208, "type": "TASK", "confidence": 0.6858982443809509}, {"text": "emotion classification", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.7117553353309631}]}, {"text": "Task 3 (Van Hee et al., 2018) addresses the problem of Irony detection in English Tweets.", "labels": [], "entities": [{"text": "Irony detection in English Tweets", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7843398809432983}]}, {"text": "It consists of two subtasks.", "labels": [], "entities": []}, {"text": "The first subtask is a two-class (or binary) classification task where the system has to predict whether a tweet is ironic or not.", "labels": [], "entities": []}, {"text": "The second subtask is a multiclass classification task where the system has to predict one out of four labels describing i) verbal irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, iv) non-irony.", "labels": [], "entities": []}, {"text": "This paper describes the main characteristics of the developed system by the ELiRF-UPV team for tasks 1 and 3.", "labels": [], "entities": [{"text": "ELiRF-UPV team", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.8912580907344818}]}, {"text": "We addressed all subtasks of task 1 both for English and Spanish, and all subtasks of task 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a tuning process with the development sets in order to select the best model for each task.", "labels": [], "entities": []}, {"text": "We tested different ways of preprocessing the tweets, we fit the parameters of the models and we evaluated some external lexicons.", "labels": [], "entities": []}, {"text": "Next, we summarize the best results obtained in the tuning process by considering some combinations of the tested models and configurations.", "labels": [], "entities": []}, {"text": "shows the results for 3 of the subtasks in the tuning process for Task 1.", "labels": [], "entities": []}, {"text": "For the two remaining tasks (EI-Oc and V-Oc) we do not learn specific models, in these cases we used the best models obtained for EI-Reg and V-Reg, respectively.", "labels": [], "entities": []}, {"text": "As it can be seen, LSTM achieved the best results for subtasks El-Reg.", "labels": [], "entities": []}, {"text": "The rest of subtasks performed better when we combined CNN and LSTM models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8391987681388855}]}, {"text": "In addition, when we consider the evaluation metric as loss function we improved the results (see the differences between CNN-LSTM + Lexicons (Jaccard) and CNN-LSTM + Lexicons (CCE)).", "labels": [], "entities": [{"text": "CNN-LSTM + Lexicons (Jaccard)", "start_pos": 122, "end_pos": 151, "type": "DATASET", "confidence": 0.8665996491909027}, {"text": "CNN-LSTM + Lexicons (CCE))", "start_pos": 156, "end_pos": 182, "type": "DATASET", "confidence": 0.8398878276348114}]}, {"text": "shows the results for the two subtasks in the tuning process for Task 3.", "labels": [], "entities": []}, {"text": "We can observe the same behavior as Task 1.", "labels": [], "entities": []}, {"text": "The best results are obtained using a combination of CNN and LSTM models and if we consider the evaluation metric as loss function the results are improved.", "labels": [], "entities": []}, {"text": "Once our best system for each subtask with the development set was chosen, we tested it on the official test set and we compare it with the best results obtained by another participant.", "labels": [], "entities": [{"text": "official test set", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.7627623081207275}]}, {"text": "These results are shown in for Task 1, and in for Task 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task 1 development results.", "labels": [], "entities": []}, {"text": " Table 2: Task 3 development results.", "labels": [], "entities": []}, {"text": " Table 3: Task 1 test results.", "labels": [], "entities": []}]}