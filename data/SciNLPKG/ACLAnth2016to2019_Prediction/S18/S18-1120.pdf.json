{"title": [{"text": "Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8117820421854655}]}], "abstractContent": [{"text": "This paper describes our system for SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge (Oster-mann et al., 2018b).", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9140917460123698}]}, {"text": "We use Three-way Attentive Networks (TriAN) to model interactions between the passage, question and answers.", "labels": [], "entities": []}, {"text": "To incorporate commonsense knowledge, we augment the input with relation embedding from the graph of general knowledge ConceptNet (Speer et al., 2017).", "labels": [], "entities": []}, {"text": "As a result, our system achieves state-of-the-art performance with 83.95% accuracy on the official test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995322227478027}]}, {"text": "Code is publicly available at https://github.com/ intfloat/commonsense-rc.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well known that humans have avast amount of commonsense knowledge acquired from everyday life.", "labels": [], "entities": []}, {"text": "For machine reading comprehension, natural language inference and many other NLP tasks, commonsense reasoning is one of the major obstacles to make machines as intelligent as humans.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7936499416828156}]}, {"text": "A large portion of previous work focus on commonsense knowledge acquisition with unsupervised learning or crowdsourcing approach (.", "labels": [], "entities": [{"text": "commonsense knowledge acquisition", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8392176826794943}]}, {"text": "ConceptNet (, WebChild (Tandon et al., 2017) and DeScript ( etc are all publicly available knowledge resources.", "labels": [], "entities": [{"text": "WebChild (Tandon et al., 2017)", "start_pos": 14, "end_pos": 44, "type": "DATASET", "confidence": 0.8600685149431229}]}, {"text": "However, resources based on unsupervised learning tend to be noisy, while crowdsourcing approach has scalability issues.", "labels": [], "entities": []}, {"text": "There is also some research on incorporating knowledge into NLP tasks such as reading comprehension () neural machine translation () and text classification () etc.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.7040629386901855}, {"text": "text classification", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.8033940196037292}]}, {"text": "Though experiments show performance gains over baselines, these gains are often quite marginal over the state-of-the-art system without external knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we present Three-way Attentive Networks(TriAN) for multiple-choice commonsense reading comprehension.", "labels": [], "entities": []}, {"text": "The given task requires modeling interactions between the passage, question and answers.", "labels": [], "entities": []}, {"text": "Different questions need to focus on different parts of the passage, attention mechanism is a natural choice and turns out to be effective for reading comprehension.", "labels": [], "entities": []}, {"text": "Due to the relatively small size of training data, TriAN use word-level attention and consists of only one layer of LSTM.", "labels": [], "entities": []}, {"text": "Deeper models result in serious overfitting and poor generalization empirically.", "labels": [], "entities": []}, {"text": "To explicitly model commonsense knowledge, relation embeddings based on ConceptNet () are used as additional features.", "labels": [], "entities": []}, {"text": "ConceptNet is a large-scale graph of general knowledge from both crowdsourced resources and expert-created resources.", "labels": [], "entities": []}, {"text": "It consists of over 21 million edges and 8 million nodes.", "labels": [], "entities": []}, {"text": "ConceptNet shows state-of-the-art performance on tasks like word analogy and word relatedness.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.7712991237640381}, {"text": "word relatedness", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7258259952068329}]}, {"text": "Besides, we also find that pretraining our network on other datasets helps to improve the overall performance.", "labels": [], "entities": []}, {"text": "There are some existing multiplechoice English reading comprehension datasets contributed by NLP community such as MCTest () and RACE (.", "labels": [], "entities": [{"text": "multiplechoice English reading comprehension datasets", "start_pos": 24, "end_pos": 77, "type": "DATASET", "confidence": 0.4907963514328003}, {"text": "MCTest", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.8400460481643677}]}, {"text": "Although those datasets don't focus specifically on commonsense comprehension, they provide a convenient way for data augmentation.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7935130298137665}]}, {"text": "Augmented data can be used to learn shared regularities of reading comprehension tasks.", "labels": [], "entities": []}, {"text": "Combining all of the aforementioned techniques, our system achieves competitive performance on the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.7654383182525635}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2.  Human performance is shared by task organiz- ers. For TriAN-ensemble, we average the out- put probabilities of 9 models trained with the  same datasets and network architecture but differ- ent random seeds. TriAN-ensemble is the model  that we used for official submission.", "labels": [], "entities": []}, {"text": " Table 2: Main results. TriAN-RACE only use RACE  dataset for training; HFL is the 1st place team for  SemEval-2018 Task 11. The evaluation metric is ac- curacy.", "labels": [], "entities": [{"text": "TriAN-RACE", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.7827101945877075}, {"text": "RACE  dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.7161211669445038}, {"text": "SemEval-2018 Task 11", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.6101870934168497}]}, {"text": " Table 4: Ablation study for input representation.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9763655662536621}]}, {"text": " Table 5: Ablation study for attention. The last one \"w/o  attention\" removes all word-level attentions.", "labels": [], "entities": []}]}