{"title": [{"text": "SemEval-2018 Task 5: Counting Events and Participants in the Long Tail", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper discusses SemEval-2018 Task 5: a referential quantification task of counting events and participants in local, long-tail news documents with high ambiguity.", "labels": [], "entities": [{"text": "SemEval-2018 Task", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8692516088485718}]}, {"text": "The complexity of this task challenges systems to establish the meaning, reference and identity across documents.", "labels": [], "entities": []}, {"text": "The task consists of three subtasks and spans across three domains.", "labels": [], "entities": []}, {"text": "We detail the design of this referential quantifica-tion task, describe the participating systems, and present additional analysis to gain deeper insight into their performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a \"referential quantification\" task that requires systems to establish the meaning, reference and identity of events and participants in news articles.", "labels": [], "entities": [{"text": "referential quantification", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.8402474820613861}, {"text": "establish the meaning, reference and identity of events and participants in news articles", "start_pos": 72, "end_pos": 161, "type": "TASK", "confidence": 0.6106685847043991}]}, {"text": "By \"referential quantification\", we mean questions concerning the number of incidents of an event type (e.g. How many killing incidents happened in 2016 in Columbus, MS?) or participants in roles (e.g. How many people were killed in 2016 in Columbus, MS?), as opposed to factoid questions for specific properties of individual events and entities (e.g. When was 2pac murdered?).", "labels": [], "entities": [{"text": "referential quantification", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.798888623714447}]}, {"text": "The questions are given with certain constraints on the location, time, participants, and event types, which requires understanding of the meaning of words mentioning these properties (e.g. Word Sense Disambiguation), but also adequately establishing the identity (e.g. reference and coreference) across mentions.", "labels": [], "entities": [{"text": "Word Sense Disambiguation)", "start_pos": 190, "end_pos": 216, "type": "TASK", "confidence": 0.6450099870562553}]}, {"text": "The task thus represents both an intrinsic and application-based evaluation, as systems are forced to resolve ambiguity of meaning and reference, as well as variation in reference in order to answer the questions.", "labels": [], "entities": []}, {"text": "shows an overview of our quantification task.", "labels": [], "entities": [{"text": "quantification task", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8848162889480591}]}, {"text": "We provide the participants with a set of questions and their corresponding news documents.", "labels": [], "entities": []}, {"text": "Systems are asked to distill event-and participant-based knowledge from the documents to answer the question.", "labels": [], "entities": []}, {"text": "Systems submit both a numeric answer (3 events in), and the corresponding events with their mentions found in the provided texts (e.g., the leftmost incident in Figure 1 is referred to by the coreferring mentions \"killed\" and \"assault\" found in two separate documents).", "labels": [], "entities": []}, {"text": "Systems are evaluated on both the numeric answers as well as on the sets of coreferring mentions.", "labels": [], "entities": []}, {"text": "Mentions are represented by tokens and offsets provided by the organizers.", "labels": [], "entities": []}, {"text": "The incidents and their corresponding news articles are obtained from structured databases, which greatly reduces the need for annotation and mainly requires validation instead.", "labels": [], "entities": []}, {"text": "Given this data and using a metric-driven strategy, we created a task that further maximizes ambiguity and variation of the data in relation to the questions.", "labels": [], "entities": []}, {"text": "This ambiguity and variation includes a substantial amount of low-frequent, local events and entities, reflecting a large variety of long-tail phenomena.", "labels": [], "entities": []}, {"text": "As such, the task is not only highly ambiguous but can also not be tackled by relying on the most frequent and popular (head) interpretations.", "labels": [], "entities": []}, {"text": "We seethe following contributions of our task: 1.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we propose the first task that is deliberately designed to address large ambiguity of meaning and reference over a high number of infrequent instances.", "labels": [], "entities": []}, {"text": "2. We introduce a methodology for creating large event-based tasks while avoiding a lot of annotation, since we base the task on structured data.", "labels": [], "entities": []}, {"text": "The remaining annotation concerns targeted mentions given the structured data rather than full doc- uments with open-ended interpretations.", "labels": [], "entities": []}, {"text": "3. We made all of our code to create the task available, 3 which may stimulate others to create more tasks and datasets that tackle long-tail phenomena for other aspects of language processing, either within or outside of the SemEval competition.", "labels": [], "entities": []}, {"text": "4. This task provides insights into the strengths and weaknesses of semantic processing systems with respect to various long-tail phenomena.", "labels": [], "entities": []}, {"text": "We expect that systems need to innovate by adjusting (deep) learning techniques to capture the referential complexity and knowledge sparseness, or by explicitly modeling aspects of events and entities to establish identity and reference.", "labels": [], "entities": []}], "datasetContent": [{"text": "This Section describes the evaluation criteria in this task and the baselines we compare against.", "labels": [], "entities": []}, {"text": "The incident-level evaluation assesses whether the system provided the right numeric answer to a question.", "labels": [], "entities": []}, {"text": "The results of this evaluation are given in the, for the subtasks 2 and 3 correspondingly.", "labels": [], "entities": []}, {"text": "On both subtasks, the order of the participating systems is identical, team FEUP having the highest score.", "labels": [], "entities": [{"text": "FEUP", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9475532174110413}]}, {"text": "These tables also show the RMSE values, which measure the proximity between the system and the gold answer, punishing cases where the absolute difference between them is large.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9733678698539734}]}, {"text": "While for subtask 2 the system with the lowest error rate corresponds to the system with the highest accuracy, this is different for subtask 3.", "labels": [], "entities": [{"text": "error rate", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9614091217517853}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9976455569267273}]}, {"text": "NAI-SEA, ranked third in terms of accuracy, has the lowest RMSE.", "labels": [], "entities": [{"text": "NAI-SEA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9641140699386597}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996523857116699}, {"text": "RMSE", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9931071400642395}]}, {"text": "This means that although their answers were not exactly correct, they were on average much closer to the correct answer than those of the other systems.", "labels": [], "entities": []}, {"text": "This is more notable in subtask 3 since here the range of answers is larger than in subtask 2 (the maximum answer in subtask 3 is 171).", "labels": [], "entities": []}, {"text": "We performed additional analysis to compare the performance of systems per subtype and per numeric answer class.", "labels": [], "entities": []}, {"text": "shows that the system FEUP is not only superior in terms of incident-level accuracy overall, but this is also mirrored for most of the event types, especially those corresponding to the gun violence domain.", "labels": [], "entities": [{"text": "FEUP", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9969751834869385}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9737476110458374}]}, {"text": "On the other hand, shows the accuracy distribution of each system per answer class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9993295669555664}]}, {"text": "Notably, for most systems the accuracy is highest for the questions with answer 0 or 1, and gradually declines for higher answers, forming a Zipfian-like distribution.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994605183601379}]}, {"text": "The exception here is the team ID-DE, whose accuracy is almost uniformly spread across the various answer classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9990717172622681}]}, {"text": "The intent behind document-level evaluation is to assess the ability of systems to distinguish between answer and non-answer documents.", "labels": [], "entities": []}, {"text": "The tables 9, 10, and 11 present the F1-scores for the subtasks 1, 2, and 3, respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9992133378982544}]}, {"text": "Curiously, the system ranking is very different and almost opposite compared to the incident-level rankings, with the system NAI-SEA being the one with the highest F1-score.", "labels": [], "entities": [{"text": "NAI-SEA", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.8359453678131104}, {"text": "F1-score", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9973940849304199}]}, {"text": "This can be explained by the multifaceted nature of this task, in which different systems may optimize for different goals.", "labels": [], "entities": []}, {"text": "Next, we investigated the F1-scores of systems per event property pair.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9974677562713623}]}, {"text": "As shown in, the best-performing system consistently has the highest performance overall pairs of event properties.: For subtask 2 (S2) and subtask 3 (S3), we report the incident-level accuracy and the number of questions (#Qs) per event type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9608176946640015}]}, {"text": "The best result per event type fora subtask is marked in bold.", "labels": [], "entities": []}, {"text": "'\u02c6' indicates that the accuracy is normalized for the number of answered questions, in cases where a system answered a subset of all questions.", "labels": [], "entities": [{"text": "\u02c6", "start_pos": 1, "end_pos": 2, "type": "METRIC", "confidence": 0.9675806760787964}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995664954185486}]}, {"text": ".: Document-level F1-score and number of questions (#Qs) for each subtask (S1, S2, and S3) and event property pair as given in the task questions.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6830118298530579}]}, {"text": "The best result per property pair fora subtask is marked in bold.", "labels": [], "entities": []}, {"text": "'\u02c6' indicates that the F1-score is normalized for the number of answered questions, in cases where a system answered a subset of all questions., and MUC (.", "labels": [], "entities": [{"text": "\u02c6", "start_pos": 1, "end_pos": 2, "type": "METRIC", "confidence": 0.9705259203910828}, {"text": "F1-score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993763566017151}, {"text": "MUC", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9947223663330078}]}, {"text": "The individual scores are averaged in a single number (AVG), which is used to rank (R) the systems.", "labels": [], "entities": [{"text": "AVG)", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9479789137840271}]}], "tableCaptions": [{"text": " Table 3: General statistics about trial and test data. For each", "labels": [], "entities": []}, {"text": " Table 4: For subtask 2, we report the normalized", "labels": [], "entities": []}, {"text": " Table 5: For subtask 3, we report the normalized", "labels": [], "entities": []}, {"text": " Table 9: For subtask 1, we report the normalized", "labels": [], "entities": []}, {"text": " Table 10: For subtask 2, we report the normalized", "labels": [], "entities": []}, {"text": " Table 11: For subtask 3, we report the normalized", "labels": [], "entities": []}, {"text": " Table 6: For subtask 2 (S2) and subtask 3 (S3), we report the incident-level accuracy and the number of questions (#Qs) per", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9411396980285645}]}, {"text": " Table 7: Document-level F1-score and number of questions (#Qs) for each subtask (S1, S2, and S3) and event property pair as", "labels": [], "entities": [{"text": "F1-score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9364669919013977}]}, {"text": " Table 8: Results for mention-level evaluation, scored with the customary event coreference metrics: BCUB (Bagga and", "labels": [], "entities": [{"text": "BCUB", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.919344425201416}]}]}