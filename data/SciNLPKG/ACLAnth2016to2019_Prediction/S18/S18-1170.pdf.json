{"title": [], "abstractContent": [{"text": "We describe the University of Mary-land's submission to SemEval-018 Task 10, \"Capturing Discriminative Attributes\": given word triples (w 1 , w 2 , d), the goal is to determine whether dis a discriminating attribute belonging tow 1 but not w 2.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes\"", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.8581501394510269}]}, {"text": "Our study aims to determine whether word embeddings can address this challenging task.", "labels": [], "entities": []}, {"text": "Our submission casts this problem as supervised binary classification using only word embedding features.", "labels": [], "entities": []}, {"text": "Using a gaus-sian SVM model trained only on validation data results in an F-score of 60%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.999605119228363}]}, {"text": "We also show that cosine similarity features are more effective, both in unsupervised systems (F-score of 65%) and supervised systems (F-score of 67%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9949350953102112}, {"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9744540452957153}]}], "introductionContent": [{"text": "SemEval-2018 Task 10 ( offers an opportunity to evaluate word embeddings on a challenging lexical semantics problem.", "labels": [], "entities": []}, {"text": "Much prior work on word embeddings has focused on the well-established task of detecting semantic similarity (.", "labels": [], "entities": [{"text": "detecting semantic similarity", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8358729283014933}]}, {"text": "However, semantic similarity tasks alone cannot fully characterize the differences in meaning between words.", "labels": [], "entities": []}, {"text": "For example, we would expect the word car to have high semantic similarity with truck and with vehicle in distributional vector spaces, while the relation between car and truck differs from the relation between car and vehicle.", "labels": [], "entities": []}, {"text": "In addition, popular datasets for similarity tasks are small, and similarity annotations are subjective with low inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Task 10 focuses instead on determining semantic difference: given a word triple (w 1 , w 2 , d), the task consists in predicting whether dis a discriminating attribute applicable tow 1 , but not tow 2 . For instance, (w 1 =apple, w 2 =banana, d =red) is a positive example as red is atypical attribute of apple, but not of banana.", "labels": [], "entities": []}, {"text": "This work asks to what extent word embeddings can address the challenging task of detecting discriminating attributes.", "labels": [], "entities": []}, {"text": "On the one hand, word embeddings have proven useful fora wide range of NLP tasks, including semantic similarity () and detection of lexical semantic relations, either explicitly by detecting hypernymy, lexical entailment (, or implicitly using analogies).", "labels": [], "entities": [{"text": "detection of lexical semantic relations", "start_pos": 119, "end_pos": 158, "type": "TASK", "confidence": 0.7523333430290222}]}, {"text": "On the other hand, detecting discriminating attributes requires making fine-grained meaning distinctions, and it is unclear to what extent they can be captured with opaque dense representations.", "labels": [], "entities": []}, {"text": "We start our study with unsupervised models.", "labels": [], "entities": []}, {"text": "We propose a straightforward approach where predictions are based on a learned threshold for the cosine similarity difference between (w 1 , d) and (w 2 , d), representing words using Glove embeddings ().", "labels": [], "entities": []}, {"text": "We use this unsupervised approach to evaluate the impact of word embedding dimensions on performance.", "labels": [], "entities": []}, {"text": "We then compare the best unsupervised configuration to supervised models, exploring the impact of different classifiers and training configurations.", "labels": [], "entities": []}, {"text": "Using word embeddings as features, supervised models yield high F-scores on development data, on the final test set they perform worse than the unsupervised models.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.993909478187561}]}, {"text": "Our supervised submission yields an F-score of 60%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9996321201324463}]}, {"text": "In later experiments, we show that using cosine similarity as features is more effective than directly using word embeddings, reaching an F-score of 67%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9993588328361511}]}, {"text": "For development purposes, we are provided with two datasets: a training set and a validation set, whose statistics are summarized in.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset statistics for the training and val- idation set: number of positive examples (Pos),  number of negative examples (Neg), total number  of examples (total), size of vocabulary for discrim- inant words d (d Vocab)", "labels": [], "entities": [{"text": "val- idation", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8328148325284322}]}, {"text": " Table 2: Averaged F-Score across GloVe Dimen- sions between our 2-step unsupervised system and  the baseline from", "labels": [], "entities": [{"text": "Averaged", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9592373371124268}, {"text": "F-Score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.8405582308769226}]}, {"text": " Table 3: F-Score, Precision and Recall computed on truth.txt for the full range of supervised classi- fication models across different embedding dimensions trained on validation.txt. The first 6 row are  supervised systems, the last row shows the performance of the unsupervised 2-step system.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9915084838867188}, {"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9972832202911377}, {"text": "Recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.994168758392334}]}, {"text": " Table 4: F-score for well-performing models of  alternative input variant representations", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9987809062004089}]}, {"text": " Table 5: F-score from well-formed cross- validation sets", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9985824823379517}]}]}