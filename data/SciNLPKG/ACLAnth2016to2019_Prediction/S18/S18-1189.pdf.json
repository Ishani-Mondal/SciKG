{"title": [{"text": "YNU Deep at SemEval-2018 Task 12: A BiLSTM Model with Neural Attention for Argument Reasoning Comprehension", "labels": [], "entities": [{"text": "YNU Deep at SemEval-2018 Task 12", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.7715204258759817}]}], "abstractContent": [{"text": "This paper describes the system submitted to SemEval-2018 Task 12 (The Argument Reasoning Comprehension Task).", "labels": [], "entities": [{"text": "SemEval-2018 Task 12 (The Argument Reasoning Comprehension Task)", "start_pos": 45, "end_pos": 109, "type": "TASK", "confidence": 0.6971281290054321}]}, {"text": "Enabling a computer to understand a text so that it can answer comprehension questions is still a challenging goal of NLP.", "labels": [], "entities": []}, {"text": "We propose a Bidirec-tional LSTM (BiLSTM) model that reads two sentences separated by a delimiter to determine which warrant is correct.", "labels": [], "entities": []}, {"text": "We extend this model with a neural attention mechanism that encourages the model to make reasoning over the given claims and reasons.", "labels": [], "entities": []}, {"text": "Officially released results show that our system ranks 6th among 22 submissions to this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine comprehension of text is an important problem in natural language processing.", "labels": [], "entities": [{"text": "Machine comprehension of text", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8477414697408676}, {"text": "natural language processing", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6476383407910665}]}, {"text": "Traditional approaches to machine comprehension are based on either hand engineered grammars), or information extraction methods.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8045186102390289}]}, {"text": "Recently, recurrent neural networks (RNNs) with long short-term memory (LSTM) cells have been successfully applied to a wide range of NLP tasks, such as machine translation), constituency parsing (, language modeling () and machine comprehension (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.814717173576355}, {"text": "constituency parsing", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.8940283060073853}, {"text": "language modeling", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.7897281050682068}]}, {"text": "A potential issue with the LSTM models is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector ().", "labels": [], "entities": []}, {"text": "This may make it difficult for the neural network to cope with long sentences.", "labels": [], "entities": []}, {"text": "In order to address this issue, attention mechanisms have been successfully extended to the LSTMs.", "labels": [], "entities": []}, {"text": "Attentive Reader () used a tanh layer to compute the attention between document and question embeddings.", "labels": [], "entities": []}, {"text": "This allows a model to focus on the aspects of a document that it believes helpful to answer a question.", "labels": [], "entities": []}, {"text": "The attention-based LSTM models have achieved state-of-the-art results in machine comprehension tasks (.", "labels": [], "entities": []}, {"text": "The argument reasoning comprehension task has been presented by).", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8569393555323283}]}, {"text": "The problem can be described as follows: Given an argument consisting of a claim and a reason, the goal is to select the correct warrant that explains reasoning of this particular argument.", "labels": [], "entities": []}, {"text": "Compared to traditional machine comprehension task, argument reasoning comprehension requires models to possess extra reasoning abilities.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.880347728729248}]}, {"text": "Some models increase the depth of the network, continuously updating the representations of the documents and questions to realize the reasoning process (.", "labels": [], "entities": []}, {"text": "In this paper, we use a BiLSTM model to encode the reason and claim pairs (reason-claim) and warrants.", "labels": [], "entities": []}, {"text": "Then a word-to-sentence neural attention mechanism is implemented to improve the model performance.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 provides the details of the proposed model; Experimental settings and results are discussed in section 3.", "labels": [], "entities": []}, {"text": "Finally, we draw conclusions in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The organizers provided training, development, and test sets, containing 1210, 316, 444 instances, respectively.", "labels": [], "entities": []}, {"text": "We combine the reason and claim to one sentence which can determine if the warrant is corrector not.", "labels": [], "entities": []}, {"text": "The word tokenizer we adopted is T weetT okenizer in Natural Language Toolkit (NLTK 1 ).", "labels": [], "entities": []}, {"text": "We compare two word embedding tools,) and GloVe).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9532166719436646}]}, {"text": "Out-of-vocabulary words in the data sets are randomly initialized by sampling values uniformly from (-0.25, 0.25) and optimized during training.", "labels": [], "entities": []}, {"text": "We set epoch = 10, batchsize = 256 and LST M U nits = 64.", "labels": [], "entities": [{"text": "batchsize", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9496656060218811}, {"text": "LST M U nits", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.8435425907373428}]}, {"text": "Optimization is carried out using Adaptive Moment Estimation (Adam).", "labels": [], "entities": [{"text": "Adaptive Moment Estimation (Adam)", "start_pos": 34, "end_pos": 67, "type": "METRIC", "confidence": 0.7287773291269938}]}, {"text": "All models are attention-based LSTM or BiLSTM architecture.", "labels": [], "entities": []}, {"text": "We additionally try bidirectional LSTMs through experiments.", "labels": [], "entities": []}, {"text": "Given the small scale of the data sets, we run each model 10 times, taking their average as the final result.", "labels": [], "entities": []}, {"text": "We also use data augmentation such as shuffle the sentence order to expand the data set.", "labels": [], "entities": []}, {"text": "Specifically, we randomize the word order of the reason-claims and the warrants to double the data set.", "labels": [], "entities": []}, {"text": "A randomseed is set to ensure our results are reproducible.", "labels": [], "entities": []}, {"text": "The results show that data augmentation like shuffling the sentence order does not have much http://www.nltk.org/ effect on the performance of our models.", "labels": [], "entities": []}, {"text": "So, we use the attention-based BiLSTM model as our final system to the task.", "labels": [], "entities": []}, {"text": "Our final result on the test set is 0.583, which ranks 6th according to the official ranking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between Word2Vec and GloVe.  GloVe performs better on dev data set, but Word2Vec  outperforms GloVe on test data set.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.935430109500885}]}, {"text": " Table 2: Performance on models with or without shuf- fle. Both models are based on attention-based BiLSTM  + GloVe architecture.", "labels": [], "entities": []}]}