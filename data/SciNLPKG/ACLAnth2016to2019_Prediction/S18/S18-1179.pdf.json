{"title": [{"text": "IUCM at SemEval-2018 Task 11: Similar-Topic Texts as a Comprehension Knowledge Source", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the IUCM entry at SemEval-2018 Task 11, on machine comprehension using commonsense knowledge.", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.868378480275472}]}, {"text": "First, clustering and topic modeling are used to divide given texts into topics.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8039085865020752}]}, {"text": "Then, during the answering phase, other texts of the same topic are retrieved and used as commonsense knowledge.", "labels": [], "entities": [{"text": "answering phase", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.8992795646190643}]}, {"text": "Finally, the answer is selected.", "labels": [], "entities": []}, {"text": "While clustering itself shows good results, finding an answer proves to be more challenging.", "labels": [], "entities": []}, {"text": "This paper reports the results of system evaluation and suggests potential improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of is to find away to incorporate commonsense knowledge into a question-answering task (.", "labels": [], "entities": []}, {"text": "In this case, questions are either directly or indirectly related to given English texts; some questions maybe answered using the text while others require background (commonsense) knowledge.", "labels": [], "entities": []}, {"text": "The challenge is to use this knowledge in such away as to enhance the quality of chosen answers.", "labels": [], "entities": []}, {"text": "There are many approaches to question answering including using structured knowledge), knowledge databases (, deep learning methods (Minaee and Liu, 2017) and hybrid methods (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8507127463817596}]}, {"text": "The present task accepts any method or any source of background knowledge.", "labels": [], "entities": []}, {"text": "The training data consists of 1469 texts covering more than 100 topics.", "labels": [], "entities": []}, {"text": "The number of questions per text varies from 1 to 14 and there are two answer options.", "labels": [], "entities": []}, {"text": "The development data has 219 texts and the test data has 430.", "labels": [], "entities": []}, {"text": "() The main idea behind the method proposed in this paper is to use the given texts as potential sources of knowledge.", "labels": [], "entities": []}, {"text": "Texts from training and development data can be divided into topics using existing clustering algorithms (e.g. k-Means, Hierarchical, Grid-based or Density-based).", "labels": [], "entities": []}, {"text": "The hypothesis is that texts which come from the same topic as the current question's text may contain the correct answer.", "labels": [], "entities": []}, {"text": "A matching function with a scoring scheme is used to identify the correct combination of words for the answer.", "labels": [], "entities": []}, {"text": "Another potential source of knowledge is scripts () that have been used to search for an answer.", "labels": [], "entities": []}, {"text": "The DeScript dataset includes descriptions of everyday activities such as baking, getting a haircut, going grocery shopping, and others, corresponding to topics present in the given data.", "labels": [], "entities": [{"text": "DeScript dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.814187616109848}]}, {"text": "Section 2 describes the methods as well as specifics of implementation.", "labels": [], "entities": []}, {"text": "Section 3 provide interpretation and analysis of the results.", "labels": [], "entities": [{"text": "interpretation", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8730581998825073}]}, {"text": "Section 4 concludes the present paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results for the test data are summarized in Table 2.", "labels": [], "entities": []}, {"text": "These are the configurations that resulted in the best performance.", "labels": [], "entities": []}, {"text": "BigARTM's advantage over the baseline solution is not much, but there is an interesting trend that explains why the score is higher.", "labels": [], "entities": [{"text": "BigARTM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7619311809539795}]}, {"text": "Questions with no word-for-word answer in the texts were answered correctly when individual words were found within the same-topic clusters.", "labels": [], "entities": []}, {"text": "This showed that given texts could be a useful knowledge source.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Probability of the topic being the top choice  for the text", "labels": [], "entities": []}, {"text": " Table 3: Doc2Vec/KMeans configurations and accu- racy", "labels": [], "entities": []}]}