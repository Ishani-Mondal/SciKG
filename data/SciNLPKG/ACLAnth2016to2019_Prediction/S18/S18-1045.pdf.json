{"title": [{"text": "DeepMiner at SemEval-2018 Task 1: Emotion Intensity Recognition Using Deep Representation Learning", "labels": [], "entities": [{"text": "Emotion Intensity Recognition", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8834666609764099}]}], "abstractContent": [{"text": "In this paper, we propose a regression system to infer the emotion intensity of a tweet.", "labels": [], "entities": []}, {"text": "We develop a multi-aspect feature learning mechanism to capture the most discriminative semantic features of a tweet as well as the emotion information conveyed by each word in it.", "labels": [], "entities": []}, {"text": "We combine six types of feature groups: (1) a tweet representation learned by an LSTM deep neural network on the training data, (2) a tweet representation learned by an LSTM network on a large corpus of tweets that contain emotion words (a distant supervision corpus), (3) word embeddings trained on the distant supervision corpus and averaged overall words in a tweet, (4) word and character n-grams, (5) features derived from various sentiment and emotion lexicons, and (6) other hand-crafted features.", "labels": [], "entities": []}, {"text": "As part of the word embedding training , we also learn the distributed representations of multi-word expressions (MWEs) and negated forms of words.", "labels": [], "entities": []}, {"text": "An SVR regressor is then trained over the full set of features.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our ensemble feature sets on the SemEval-2018 Task 1 datasets and achieve a Pearson correlation of 72% on the task of tweet emotion intensity prediction.", "labels": [], "entities": [{"text": "SemEval-2018 Task 1 datasets", "start_pos": 66, "end_pos": 94, "type": "DATASET", "confidence": 0.7825145721435547}, {"text": "Pearson correlation", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.9634931981563568}, {"text": "tweet emotion intensity prediction", "start_pos": 151, "end_pos": 185, "type": "TASK", "confidence": 0.7933220863342285}]}], "introductionContent": [{"text": "The widespread use of micro-blogging and social networking websites such as Twitter for conveying information, sharing opinions, and expressing feelings, makes the sentiment analysis of tweets an attractive area of research.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 164, "end_pos": 192, "type": "TASK", "confidence": 0.8753484487533569}]}, {"text": "However, sentiment analysis is challenging because people often convey their emotions indirectly and creatively, rather than explicitly stating how they feel.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9692837297916412}]}, {"text": "Sentiment analysis of tweets is additionally challenging because of the frequent occurrences of nonstandard language and poor grammatical structure.", "labels": [], "entities": [{"text": "Sentiment analysis of tweets", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9495845288038254}]}, {"text": "Tweets also often contain misspellings, abbreviations, hashtags, and emoticons.", "labels": [], "entities": []}, {"text": "Various machine learning approaches have been developed for Twitter sentiment classification.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.8195188442866007}]}, {"text": "Most of these algorithms train a classifier over tweets with manually annotated sentiment intensity labels and learn the most discriminative features.", "labels": [], "entities": []}, {"text": "Hence, designing an effective feature engineering algorithm can improve classification performance, greatly.", "labels": [], "entities": []}, {"text": "used many different sentiment lexicons (manually created and automatically generated), as well as a variety of hand-crafted features to build the topranked system for Twitter sentiment classification tasks in.", "labels": [], "entities": [{"text": "Twitter sentiment classification tasks", "start_pos": 167, "end_pos": 205, "type": "TASK", "confidence": 0.685292586684227}]}, {"text": "Sentiment lexicons, either hand-crafted or algorithmically generated, consist of words and their associated polarity scores.", "labels": [], "entities": []}, {"text": "However, since feature engineering is labour intensive and usually needs domain-specific knowledge, sentiment classification algorithms with less dependency on feature engineering are attracting considerable interest.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8426305055618286}, {"text": "sentiment classification", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.9437664747238159}]}, {"text": "proposed a feature learning algorithm to discover explanatory factors in sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.9364023804664612}]}, {"text": "They consider the representation of a sentence (or document) as a composition of the representations of its constituent words or phrases.", "labels": [], "entities": []}, {"text": "This way, the sentiment classification problem reduces to learning an effective word representation (or word embedding) that not only models the syntactic context of words but also captures sentiment information of the sentence.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.9394956827163696}]}, {"text": "extended the traditional word embedding methods () by encoding sentiment information into the existing continuous representation of words.", "labels": [], "entities": []}, {"text": "They built sentiment-specific word embedding (SSWE) by developing three neural networks wherein the sentiment polarity of the tweet is incorporated in the neural networks' loss functions.", "labels": [], "entities": [{"text": "sentiment-specific word embedding (SSWE)", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.7015364120403925}]}, {"text": "proposed a context-sensitive lexicon-based method using recurrent and simple", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of instances provided in the Tweet  Emotion Intensity dataset (SemEval-2018 Task 1, EI- reg English). The data was divided into train, develop- ment, and test sets.", "labels": [], "entities": [{"text": "Tweet  Emotion Intensity dataset", "start_pos": 46, "end_pos": 78, "type": "DATASET", "confidence": 0.6406965181231499}, {"text": "EI- reg English)", "start_pos": 101, "end_pos": 117, "type": "DATASET", "confidence": 0.762468409538269}]}, {"text": " Table 2: Pearson correlation (r) % obtained on the test sets. The highest score in each emotion is shown in bold.  System I indicates the results of our first overfitted model and System II shows the results of our modified model.  In every experiment on system II, we train SVR regressor with linear kernel to predict emotion intensity of a  tweet while in system I experiments, we use RF regressor and SVM classifier for SemEval-2018 Task 1 and 2,  respectively. The all-features experiment represents the model built on concatenation of all six groups of features  including WE, ngram, TE, polTE, lex, and handcrft.", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9262089729309082}]}]}