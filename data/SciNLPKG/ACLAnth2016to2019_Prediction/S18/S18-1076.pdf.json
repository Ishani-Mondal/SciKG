{"title": [{"text": "SyntNN at SemEval-2018 Task 2: is Syntax Useful for Emoji Prediction? Embedding Syntactic Trees in Multi Layer Perceptrons", "labels": [], "entities": [{"text": "Emoji Prediction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8049124777317047}]}], "abstractContent": [{"text": "In this paper, we present SyntNN as away to include traditional syntactic models in multilayer neural networks used in the task of Semeval Task 2 of emoji prediction (Barbieri et al., 2018).", "labels": [], "entities": [{"text": "Semeval Task 2", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.8475690881411234}, {"text": "emoji prediction", "start_pos": 149, "end_pos": 165, "type": "TASK", "confidence": 0.7737158536911011}]}, {"text": "The model builds on the distributed tree embedder also known as distributed tree kernel (Zanzotto and Dell'Arciprete, 2012).", "labels": [], "entities": []}, {"text": "Initial results are extremely encouraging but additional analysis is needed to overcome the problem of overfitting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic models of language have always played a crucial role in many natural language processing tasks but, in recent years, it has been marginalized by the advent of neural networks and in particular long-short term memory (LSTM).", "labels": [], "entities": []}, {"text": "These latter networks have had a tremendous impact on how linguistic tasks are modeled and, sometimes, solved.", "labels": [], "entities": []}, {"text": "In this paper, we want to explore the use of \"traditional\" syntactic information within a neural network framework in the task of Semeval Task 2 of emoji prediction ().", "labels": [], "entities": [{"text": "Semeval Task 2", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.8394986589749655}, {"text": "emoji prediction", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.786440908908844}]}, {"text": "We propose SyntNN as away to include traditional syntactic models in multilayer neural networks.", "labels": [], "entities": []}, {"text": "The model builds on the distributed tree embedder also known as distributed tree kernel () that is away to transpose syntactic information in small vectors.", "labels": [], "entities": []}, {"text": "Initial results are extremely encouraging: SyntNN outperforms syntax-unaware neural networks on the trial set.", "labels": [], "entities": []}, {"text": "Unfortunately, these promising results are not confirmed on the test set.", "labels": [], "entities": []}, {"text": "Hence, we analyzed these results to try to understand why this has happened.", "labels": [], "entities": []}], "datasetContent": [{"text": "To ensure replicability, this section fully describes the implementation details of SyntN N ( and the values of its metaparameters.", "labels": [], "entities": [{"text": "replicability", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9732849597930908}]}, {"text": "Moreover, it introduces the networks used for comparison and the datasets used on the experiments.", "labels": [], "entities": []}, {"text": "For the Syntactic Subnetwork of SyntNN, we used the Python implementation of the Distributed Tree Encoder 2 . Tweets' parse trees are obtained by using Stanford's CoreNLP 3 probabilistic context free grammar parser.", "labels": [], "entities": [{"text": "CoreNLP 3 probabilistic context free grammar parser", "start_pos": 163, "end_pos": 214, "type": "TASK", "confidence": 0.5450307939733777}]}, {"text": "Distributed trees are represented in a space Rd with d = 4000.", "labels": [], "entities": []}, {"text": "Then, the layer l 1(synt) is composed of 5512 units.", "labels": [], "entities": []}, {"text": "The layer l 2(synt) is a cascade of two dense layers composed, respectively, of 2018 units and 1024 units.", "labels": [], "entities": []}, {"text": "All these tree layers have dropout 0.5 and a ReLU activation function.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9273139238357544}]}, {"text": "For the Semantic Subnetwork of SyntNN, we used pretrained word embeddings as Stanford's Glove and the word embeddings given by the organizers of the task ( . The rest of the semantic subnetwork is the following.", "labels": [], "entities": []}, {"text": "The first layer, the input layer I, is composed by 200/300 neurons.", "labels": [], "entities": []}, {"text": "Each neuron take in input a dimension of the BoW vector.", "labels": [], "entities": [{"text": "BoW", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.5708824992179871}]}, {"text": "The number of input neuron varies according to the word embedding used: 200 if the word embedding used is Glove; 300 if the word embedding used in the other word embedding cited in the word embedding section.", "labels": [], "entities": [{"text": "Glove", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.6770979166030884}]}, {"text": "The second layer l 1(sem) consists of 512 neurons, dropout 0.5 and ReLU activation function.", "labels": [], "entities": [{"text": "ReLU activation function", "start_pos": 67, "end_pos": 91, "type": "METRIC", "confidence": 0.9324561357498169}]}, {"text": "The third layer l 2(sem) consists of 1024 neurons, dropout 0.5 and ReLU activation function.", "labels": [], "entities": [{"text": "ReLU activation function", "start_pos": 67, "end_pos": 91, "type": "METRIC", "confidence": 0.941771388053894}]}, {"text": "To understand whether SyntNN positively uses syntactic information, we compared our system with two neural networks trained in comparable conditions: (1) BOW-MLP and (2) BiLSTM.", "labels": [], "entities": [{"text": "BOW-MLP", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.8657058477401733}]}, {"text": "BOW-MLP is basically the Semantic Subnetwork of SyntNN without the Syntactic Subnetwork.", "labels": [], "entities": [{"text": "BOW-MLP", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8669641613960266}]}, {"text": "BiLSTM is a bidirectional LSTM (, which has been proven effective in many natural language processing tasks.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6503572463989258}]}, {"text": "For the BiL-STM, we used the same embedding layer used in SyntNN, we used a recurrent layer of 100 Bidirectional Long Short Term Memory (LSTM) neurons with activation function tanh, recurrent activation function hard sigmoid, recurrent dropout and dropout probability 0.5 and weight l2 regularizer with \u03bb = 0.01.", "labels": [], "entities": []}, {"text": "The output layer is composed by 20 neurons and activation function softmax.", "labels": [], "entities": []}, {"text": "All models are implemented using Keras library () and run on tensorflow ( back-end on different cuda GPUs.", "labels": [], "entities": []}, {"text": "Models are trained with Adam() gradient descent algorithm with lr = 0.0001, \u03b2 1 = 0.9, \u03b2 2 = 0.999.", "labels": [], "entities": []}, {"text": "The loss function used is the categorical crossentropy function.", "labels": [], "entities": []}, {"text": "The BOW-MLP model and SyntNN model are trained for 140 epochs and batch size = 50, while the BiLSTM model is trained for 18 epochs and batch size = 50.", "labels": [], "entities": [{"text": "BOW-MLP", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.7091585397720337}]}, {"text": "We performed our tests on the emoji prediction dataset and we used the Macro F1 Score evaluator provided by the organizers (.", "labels": [], "entities": [{"text": "emoji prediction dataset", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.7565519710381826}, {"text": "F1 Score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.6613500416278839}]}, {"text": "No additional datasets have been used.", "labels": [], "entities": []}, {"text": "Test Trial en 44.10% 19.81% es 40.67% 7.21% We then tried to analyze where the poor results on the test set came from.", "labels": [], "entities": [{"text": "Test Trial", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8715287744998932}]}, {"text": "The first observation is that unknown words in the test set are larger than for the trial set for both datasets.", "labels": [], "entities": []}, {"text": "The unknown words on the test set is more than double with respect to the unknown words in the trial for the English dataset and more than 4 times for the Spanish dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.8108485639095306}, {"text": "Spanish dataset", "start_pos": 155, "end_pos": 170, "type": "DATASET", "confidence": 0.7249054461717606}]}, {"text": "Test is definitely farer than trial with respect to the training set.", "labels": [], "entities": []}, {"text": "This seems to be the first reason why results are poorer on the test set.", "labels": [], "entities": []}, {"text": "The second observation is on the degree of overfitting.", "labels": [], "entities": []}, {"text": "In fact, this seems to to be the major problem of SyntNN and of the other two models (see.", "labels": [], "entities": [{"text": "SyntNN", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.7877978086471558}]}, {"text": "By looking at the loss function, three models largely overfit with respect to the epochs: the loss functions on the train set and on the trial set diverge.", "labels": [], "entities": []}, {"text": "This can partially explain poor results.", "labels": [], "entities": []}, {"text": "However, to have a more in-dept analysis we need to know what and how these networks are modelling symbols in general and syntactic information in particular.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Macro F1 score on the Trial Set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9430108070373535}, {"text": "Trial Set", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.9378019571304321}]}, {"text": " Table 2: Macro F1 score on the Test Set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9434605538845062}, {"text": "Test Set", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9430452287197113}]}]}