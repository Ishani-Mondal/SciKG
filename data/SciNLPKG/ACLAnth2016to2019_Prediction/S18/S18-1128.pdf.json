{"title": [{"text": "SIRIUS-LTG-UiO at SemEval-2018 Task 7: Convolutional Neural Networks with Shortest Dependency Paths for Semantic Relation Extraction and Classification in Scientific Papers", "labels": [], "entities": [{"text": "Semantic Relation Extraction and Classification", "start_pos": 104, "end_pos": 151, "type": "TASK", "confidence": 0.8124531984329224}]}], "abstractContent": [{"text": "This article presents the SIRIUS-LTG-UiO system for the SemEval 2018 Task 7 on Semantic Relation Extraction and Classification in Scientific Papers.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7 on Semantic Relation Extraction and Classification", "start_pos": 56, "end_pos": 126, "type": "TASK", "confidence": 0.850107616186142}]}, {"text": "First we extract the shortest dependency path (sdp) between two entities , then we introduce a convolutional neural network (CNN) which takes the shortest dependency path embeddings as input and performs relation classification with differing objectives for each subtask of the shared task.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 204, "end_pos": 227, "type": "TASK", "confidence": 0.8178979158401489}]}, {"text": "This approach achieved overall F1 scores of 76.7 and 83.2 for relation classification on clean and noisy data, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9997172951698303}, {"text": "relation classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.9354758560657501}]}, {"text": "Furthermore , for combined relation extraction and classification on clean data, it obtained F1 scores of 37.4 and 33.6 for each phase.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8641701340675354}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.999789297580719}]}, {"text": "Our system ranks 3rd in all three sub-tasks of the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction and classification can be defined as follows: given a sentence where entities are manually annotated, we aim to identify the pairs of entities that are instances of the semantic relations of interest and classify them based on a pre-defined set of relation types.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9749601185321808}]}, {"text": "A range of different approaches have been applied to solve this task in previous work.", "labels": [], "entities": []}, {"text": "Conventional classification approaches have made use of contextual, lexical and syntactic features combined with richer linguistic and background knowledge such as WordNet and FrameNet (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.9557961225509644}]}, {"text": "Recently, the re-emergence of deep neural networks provides away to develop highly automatic features and representations to handle complex interpretation tasks.", "labels": [], "entities": []}, {"text": "These approaches have yielded impressive results for many different NLP tasks.", "labels": [], "entities": []}, {"text": "The use of deep neural networks for relation classification has been investigated in several recent studies;.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.9754804968833923}]}, {"text": "Convolutional neural networks (CNNs) have been effectively applied to extract lexical and sentence level features for relation classification (.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.915571540594101}]}, {"text": "However, these works consider whole sentences or the context between two target entities as input for the CNN.", "labels": [], "entities": []}, {"text": "Such representations suffer from irrelevant sub-sequences or clauses when target entities occur far from each other or there are other target entities in the same sentence.", "labels": [], "entities": []}, {"text": "To avoid negative effects from irrelevant chunks or clauses and capture the relation between two entities,; and employ a CNN to learn more robust and effective relation representations from the shortest dependency path (sdp) between two entities.", "labels": [], "entities": []}, {"text": "The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (.", "labels": [], "entities": []}, {"text": "In this work, we continue this line of work and present a system based on a CNN architecture over shortest dependency paths combined with domain-specific word embeddings to extract and classify semantic relations in scientific papers.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset For each sub-task, the training data includes abstracts of papers from the ACL Anthology corpus with pre-annotated entities.", "labels": [], "entities": [{"text": "ACL Anthology corpus", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.9467904965082804}]}, {"text": "For subtask 1.1 and 2, the training datasets are the same.", "labels": [], "entities": []}, {"text": "It contains entities that are manually annotated and they represent domain concepts specific to Natural Language Processing (NLP).", "labels": [], "entities": []}, {"text": "In sub-task 1.2 the entities are automatically assigned and therefore contain a fair amount of noise (verbs, irrelevant words).", "labels": [], "entities": []}, {"text": "The terms include high-level terms (e.g. \"algorithm\", \"paper\", \"method\") and are not always full.", "labels": [], "entities": []}, {"text": "Since the related entity pairs and the relation types are provided for the full dataset, we extend the dataset for sub-task 1.1 and 2 by extracting the related entities and their corresponding sdp from the sub-task 1.2 dataset.", "labels": [], "entities": []}, {"text": "In order to train a model for sub-task 2, we also augment the dataset by extracting NONE relation instances (see Section 2), extracted from the corresponding dataset.", "labels": [], "entities": []}, {"text": "shows the number of instances for each relation class.", "labels": [], "entities": []}, {"text": "As we can see, the class distribution is clearly unbalanced.", "labels": [], "entities": []}, {"text": "Model settings We keep the value of hyperparameters equal to the ones that are reported in the original work, i.e., 128 filters for each window size, a dropout rate of \u03c1 = 0.5 and l 2 regularization of 3.", "labels": [], "entities": []}, {"text": "To deal with the effects of class imbalance, we weight the cost by the ratio of class instances, thus each observation receives a weight, depending on the class it belongs to.", "labels": [], "entities": []}, {"text": "The effect of the minority class observations is thereby increased simply by a higher weight of these instances and is decreased for majority class observations.", "labels": [], "entities": []}, {"text": "Furthermore, to guarantee that each fold in n-fold cross validation will have the proportion of same classes during training, evaluation and test, we apply the stratification technique proposed by.", "labels": [], "entities": []}, {"text": "We use the validation set to detect when overfitting starts during the training of our model; using early stopping, training is   then stopped before convergence to avoid overfitting.", "labels": [], "entities": []}, {"text": "The official evaluation metric is the macro-averaged F1-score, therefore we implement early-stopping (patience= 20) based on macro-F1 score in the development set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9817765355110168}, {"text": "early-stopping", "start_pos": 86, "end_pos": 100, "type": "METRIC", "confidence": 0.9864170551300049}, {"text": "patience", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9028582572937012}]}, {"text": "Model variants We run experiments with several variants of the model as follows: cnn.rand: A baseline model, where all elements in the embedding layer are randomly initialized and updated in the training process.", "labels": [], "entities": []}, {"text": "cnn.wiki-w2v: The embedding layer is initialized with the pre-trained Wikipeida word embeddings and fine-tuned for the target task.", "labels": [], "entities": []}, {"text": "cnn.acl-w2v: The embedding layer is initialized with the pre-trained ACL Anthology word embeddings and fine-tuned for the target task.", "labels": [], "entities": []}, {"text": "cnn.multi.rand: There are two embedding layers as a 'channel' in the CNN architecture.", "labels": [], "entities": []}, {"text": "Both channels are initialized randomly and only one of them is updated during training while the other remains static.", "labels": [], "entities": []}, {"text": "cnn.multi.wiki-w2v: Same as before, but the channels are initialized with Wikipedia embedding vectors.", "labels": [], "entities": []}, {"text": "cnn.multi.acl-w2v: The two channels are initialized with ACL embedding vectors.", "labels": [], "entities": []}, {"text": "cnn.multi.wiki-w2v.rand: First channel is initialized with Wikipedia embeddings in static mode and the second initialized randomly with a non-static mode.", "labels": [], "entities": []}, {"text": "cnn.multi.acl-w2v.rand: Same as previous setting, but the first channel makes use of ACL embeddings.", "labels": [], "entities": []}, {"text": "Results During development, we investigate the performance of different configurations; different dependency representations (CoNLL08 and Stanford basic) and model variants (see above); by running 5-fold cross validation (i.e. 3 folds for training, 1 fold for evaluation and 1 fold for test).", "labels": [], "entities": [{"text": "CoNLL08", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.8420923948287964}]}, {"text": "The experiments show that, the multi-channel mode performs better only in the classification sub-tasks compared to the single channel setting.", "labels": [], "entities": []}, {"text": "The results suggest that having a significant amount of instances per relation assists the model to classify better.", "labels": [], "entities": []}, {"text": "The use of the pre-trained embeddings helps the model in class assignment.", "labels": [], "entities": []}, {"text": "Particularly, the domain-specific embeddings (i.e. aclw2v) provide higher performance gains when used in the model.", "labels": [], "entities": []}, {"text": "presents the F1-score of the best performing model for each sub-task via 5-fold cross validation on the training data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994055032730103}]}, {"text": "In the evaluation period, we re-run 5-fold cross validation using selected model for each sub-task.", "labels": [], "entities": []}, {"text": "However, in this setting we use 4 folds as training and 1 fold as development set, and we apply the output model to the evaluation dataset.", "labels": [], "entities": []}, {"text": "We select the 1st and 2nd best performing models on the development datasets as well as the majority vote (mv) of 5 models for the final submission.", "labels": [], "entities": []}, {"text": "The final results are shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of instances for each relation in  the final dataset.", "labels": [], "entities": []}, {"text": " Table 2: F1 (macro-average) scores for selected configurations during training.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": " Table 3: Official evaluation results of the submitted  runs on the test set.", "labels": [], "entities": []}]}