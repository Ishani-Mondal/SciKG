{"title": [{"text": "SemEval 2018 Task 6: Parsing Time Normalizations", "labels": [], "entities": [{"text": "SemEval 2018 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8513108491897583}, {"text": "Parsing Time Normalizations", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8933719793955485}]}], "abstractContent": [{"text": "This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018.", "labels": [], "entities": [{"text": "Parsing Time Normalization shared task", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.913156521320343}]}, {"text": "The aim of the task is to parse time expressions into the compo-sitional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches.", "labels": [], "entities": [{"text": "Semantically Compositional Annotation of Time Expressions (SCATE)", "start_pos": 96, "end_pos": 161, "type": "TASK", "confidence": 0.6514668696456485}]}, {"text": "Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.7188763171434402}]}, {"text": "Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).", "labels": [], "entities": [{"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9976654052734375}, {"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9483320713043213}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9937635064125061}]}], "introductionContent": [{"text": "The task of extracting and normalizing time expressions (e.g., finding phrases like two days ago and converting them to a standardized form like 2017-07-17) is a fundamental component of any time-aware language processing system.) included a restricted version of a time normalization task as part of their shared tasks.", "labels": [], "entities": [{"text": "extracting and normalizing time expressions", "start_pos": 12, "end_pos": 55, "type": "TASK", "confidence": 0.7034474968910217}]}, {"text": "However, the annotation scheme used in these tasks) has some significant limitations: it assumes times can be described as a prefix of YYYY-MM-DDTHH:MM:SS (so it can't represent, e.g., the past three summers), it is unable to represent times that are are relative to events (e.g., three weeks postoperative), and it fails to reflect the compositional nature of time expressions (e.g., that following represents a similar temporal operation in the following day and the following year).", "labels": [], "entities": []}, {"text": "This latter issue especially has discouraged machine learning approaches to time normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules.", "labels": [], "entities": [{"text": "time normalization", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6982092410326004}]}, {"text": "The Parsing Time Normalizations shared task is anew approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema, in which times are annotated as compositional time entities.", "labels": [], "entities": [{"text": "Parsing Time Normalizations shared task", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8963944554328919}, {"text": "time normalization", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.712560310959816}]}, {"text": "Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8080707391103109}]}, {"text": "The top of shows an example.", "labels": [], "entities": []}, {"text": "Each annotation in the example corresponds to a formally defined time entity.", "labels": [], "entities": []}, {"text": "For instance, the annotation on top of since corresponds to a BETWEEN entity that identifies an interval starting at the most recent March 6 and ending at the document creation time.", "labels": [], "entities": [{"text": "BETWEEN", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.983653724193573}]}, {"text": "The bottom of shows how those time entities can be composed to identify appropriate intervals on the timeline.", "labels": [], "entities": []}, {"text": "Here, the BETWEEN entity finds the interval on the timeline that is between the intervals of its two arguments: the LAST and the DOC-TIME.", "labels": [], "entities": [{"text": "BETWEEN", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9685039520263672}, {"text": "LAST", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.8543453216552734}]}, {"text": "Formally, this BETWEEN operator is defined as: In the proposed task, systems need only to identify time entities in text and link them correctly to signal how they are to be composed (i.e., systems would only need to produce annotation structures like those at the top of).", "labels": [], "entities": [{"text": "BETWEEN", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9931468367576599}]}, {"text": "The timeline intervals implied by such system output are inferred through a time entity interpreter provided to the participants by the workshop organizers.", "labels": [], "entities": []}, {"text": "88 The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe the task goal and proposed tracks in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 contains the description of the data annotation schema and the statistics of our dataset.", "labels": [], "entities": []}, {"text": "In Section 4, we explain the two evaluation metrics used in the task and in Section 5 the models used as baselines.", "labels": [], "entities": []}, {"text": "We present the participant systems in Section 6 and the results obtained in Section 7.", "labels": [], "entities": []}, {"text": "Finally, we discuss some conclusions learned in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose two types of scoring metrics for this task, one for the evaluation of each track.", "labels": [], "entities": []}, {"text": "The first follows a more traditional information extraction evaluation: measure the precision and recall of finding and linking the various time entities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7667859494686127}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993727803230286}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9968133568763733}]}, {"text": "Specifically, we define: where S is the set of items predicted by the system and H is the set of items produced by the humans.", "labels": [], "entities": []}, {"text": "For these calculations, each item is an annotation, and one annotation is considered as equal to another if it has the same character span (offsets), type, and properties (with the definition applying recursively for properties that point to other annotations).", "labels": [], "entities": []}, {"text": "The second scoring method evaluates the accuracy of systems with respect to the timeline in a more direct way.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989722967147827}]}, {"text": "First, annotations, in either TimeML or SCATE format, are converted into time intervals.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9293263554573059}]}, {"text": "TimeML TIMEX3 (time expression) annotations are translated into intervals following ISO 8601 semantics of their VALUE attribute.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7512986660003662}, {"text": "TIMEX3", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.43298134207725525}, {"text": "VALUE", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.8296871185302734}]}, {"text": "For example, 2010-02-25 is converted to the interval [2010-02-25T00:00:00, 2018-02-26T00:00:00), that is, the 24-hour period starting at the first second of the day on 2010-02-25 and ending just before the first second of the day on 2010-02-26.", "labels": [], "entities": []}, {"text": "SCATE annotations are converted to intervals according to the formal semantics of each entity, using the Scala library provided by.", "labels": [], "entities": []}, {"text": "For example,, SimplePeriod(YEARS, 4)), is converted to [2011-01-01T00:00, 2015-01-01T00:00), i.e., the 4 years following 2010.", "labels": [], "entities": [{"text": "SimplePeriod(YEARS, 4", "start_pos": 14, "end_pos": 35, "type": "DATASET", "confidence": 0.805925440788269}]}, {"text": "Note that there maybe more than one interval associated with a single annotation, as in the every Saturday since March 6 example in.", "labels": [], "entities": []}, {"text": "Once all annotations have been converted into intervals along the timeline, we can calculate the overlap between the intervals of different annotations.", "labels": [], "entities": []}, {"text": "Given two sets of intervals, we define the interval precision, P int , as the total length of the intervals in common between the two sets, divided by the total length of the intervals in the first set.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.5888227224349976}]}, {"text": "Interval recall, R int is defined as the total length of the intervals in common between the two sets, 91 divided by the total length of the intervals in the second set.", "labels": [], "entities": [{"text": "Interval recall", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.7746193110942841}]}, {"text": "Formally:  The official results are presented in and Table 4.", "labels": [], "entities": []}, {"text": "For each track we present the precision (P ), recall (R) and F 1 score obtained by the metrics presented in Section 4.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9512265026569366}, {"text": "recall (R)", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9619567841291428}, {"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9857920209566752}]}, {"text": "The only participant of the task submitted output just for the Newswire domain, thus, we only report the performance of this system in this domain.", "labels": [], "entities": [{"text": "Newswire domain", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9607540965080261}]}, {"text": "The results of the Character-based baseline have been obtained training the model with the training set of the corresponding domain (Newswire or Clinical) and a set of randomly generated dates, as explained in.", "labels": [], "entities": []}, {"text": "In Track 1 () the original version of CHRONO do not reach the Character-based baseline, 0.44 F 1 vs 0.51 F 1 . However, the fixed version of the system (CHRONO*) outperforms the baseline in terms of F 1 (0.55) as well as in terms of P (0.61) and R (0.50).", "labels": [], "entities": [{"text": "F 1", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.9764048159122467}, {"text": "R", "start_pos": 246, "end_pos": 247, "type": "METRIC", "confidence": 0.9382163286209106}]}, {"text": "shows a more detailed comparison between the Character-based baseline and CHRONO*.", "labels": [], "entities": [{"text": "CHRONO", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.6913114190101624}]}, {"text": "These figures represent the performances of both models for each SCATE temporal type.", "labels": [], "entities": [{"text": "SCATE temporal type", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8413108984629313}]}, {"text": "This includes the identification of the time entity, its prop-93) the best system is the Character-based baseline with 0.76 F 1 , followed by HEIDELTIME with 0.74 F 1 . None of the versions of CHRONO performs better than the baselines, although the fixed version (CHRONO*) gets enhanced results, 0.65 F 1 vs 0.70 F 1 , following the improvement obtained in Track 1.", "labels": [], "entities": [{"text": "HEIDELTIME", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.982339084148407}]}, {"text": "It is remarkable that HEIDELTIME and CHRONO*, essentially rule-based systems, obtain better R than the Character-based baseline, that relies strongly on a supervised model.", "labels": [], "entities": [{"text": "HEIDELTIME", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.7563011646270752}, {"text": "R", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.9558725953102112}]}, {"text": "Specifically, HEIDELTIME obtains the best R (0.77), but CHRONO* also outperforms the Character-based baseline in terms of R, 0.75 vs 0.71.", "labels": [], "entities": [{"text": "HEIDELTIME", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.8873845934867859}, {"text": "R", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9896663427352905}, {"text": "CHRONO", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9524166584014893}, {"text": "R", "start_pos": 122, "end_pos": 123, "type": "METRIC", "confidence": 0.9422429203987122}]}, {"text": "However, the Character-based baseline obtains a much higher P (0.83) than the 0.71 of HEIDELTIME and the 0.65 of CHRONO*.", "labels": [], "entities": [{"text": "P", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9971128702163696}, {"text": "HEIDELTIME", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.695177435874939}]}, {"text": "As explained in Section 4, the metric for Track 1 evaluates the individual temporal components extracted by the systems, either time entities or links between time entity pairs.", "labels": [], "entities": []}, {"text": "On the other hand, the intervals scored by the metric for Track 2 are produced by interpreting the whole graph.", "labels": [], "entities": []}, {"text": "Moreover, not all the time expressions yield a finite set of bounded intervals, as can be seen in.", "labels": [], "entities": []}, {"text": "Consequently, better performances in Track 1 do not necessarily yield better results in Track 2.", "labels": [], "entities": []}, {"text": "In particular, although CHRONO* is better than the Character-based baseline in Track 1 it produces an excessive number of time expressions yielding bounded intervals (108), which affects the P in Track 2.", "labels": [], "entities": [{"text": "CHRONO", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9284130334854126}]}, {"text": "In contrast, the Character-based baseline is more conservative and accurate in this respect (85).", "labels": [], "entities": [{"text": "accurate", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9679953455924988}]}, {"text": "Although we didn't receive any submission for the Clinical domain, in order to set a reference for future research, we present in the performances of the baseline systems in this domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents and SCATE annotations for both sections of the corpus following the SCATE  schema.", "labels": [], "entities": []}, {"text": " Table 2: Official results in Track 1 (parsing) for the  Newswire and Clinical domains.", "labels": [], "entities": [{"text": "Newswire and Clinical domains", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.7310551479458809}]}, {"text": " Table 3: Results in Track 1 per SCATE type. Char  stands for Character-based baseline. The number of  gold cases per type is included (#).", "labels": [], "entities": []}, {"text": " Table 4: Official results in Track 2 (intervals) for the  Newswire and Clinical domains.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9711748361587524}]}]}