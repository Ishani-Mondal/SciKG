{"title": [{"text": "ELiRF-UPV at SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the participation of ELiRF-UPV team at task 11, Machine Comprehension using Commonsense Knowledge, of SemEval-2018.", "labels": [], "entities": []}, {"text": "Our approach is based on the use of word embeddings, NumberBatch Embeddings, and a Deep Learning architecture to find the best answer for the multiple-choice questions based on the narrative text.", "labels": [], "entities": []}, {"text": "The results obtained are inline with those obtained by the other participants and they encourage us to continue working on this problem .", "labels": [], "entities": []}], "introductionContent": [{"text": "In the Machine Comprehension using Commonsense Knowledge task, systems must answer multiple-choice questions given narrative texts about everyday activities.", "labels": [], "entities": []}, {"text": "In addition to what is mentioned in the text, a substantial number of questions require inference using script knowledge about different scenarios.", "labels": [], "entities": []}, {"text": "In order to capture some script knowledge we decided to use a word representation based not only on distributional semantics word models but also on a knowledge graph,.", "labels": [], "entities": []}, {"text": "ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges.", "labels": [], "entities": []}, {"text": "It is designed to represent the general knowledge involved in understanding language.", "labels": [], "entities": []}, {"text": "ConceptNet could be used in combination with sources of distributional semantics, particularly the word2vec Google News skip-gram embeddings () and GloVe 1.2 (), to produce new embeddings, NumberBatch embeddings, with state-of-the-art performance across many wordrelatedness evaluations.", "labels": [], "entities": [{"text": "word2vec Google News skip-gram embeddings", "start_pos": 99, "end_pos": 140, "type": "DATASET", "confidence": 0.7951072812080383}]}, {"text": "More specifically, NumberBatch is a list of semantic word vectors which contains a complex meaning of those terms, beyond containing only contextual information like other kinds of embeddings based on distributional semantics e.g. Word2Vec or Glove.", "labels": [], "entities": []}, {"text": "These embeddings are obtained through a combination of Word2Vec and Glove embeddings with knowledge extracted from ConceptNet by means of a technique known as retrofitting).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9609091281890869}]}, {"text": "In this work, we used word representations based on NumberBatch embeddings because these representations encode semantically rich information related to the commonsense.", "labels": [], "entities": []}, {"text": "Moreover, in order to tackle this machine comprehension task, we used a Deep Learning architecture with new attention mechanisms.", "labels": [], "entities": []}, {"text": "The inclusion of these new attention mechanisms allow us to better capture the similarities among the elements of the input.", "labels": [], "entities": []}, {"text": "The attention mechanisms we introduce in this work are suggested in the work (, that obtained very competitive results in Question Answering tasks.", "labels": [], "entities": [{"text": "Question Answering tasks", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.8448440631230673}]}], "datasetContent": [{"text": "The results obtained during the development phase for the different systems above mentioned are shown in the.", "labels": [], "entities": []}, {"text": "It can be observed that Deep Learning systems with simpler attention mechanisms worked better than those with more complex attention mechanisms (system 2 versus systems 1 and 3).", "labels": [], "entities": []}, {"text": "If X2Q was added to compute more explicit relations between x and r, the accuracy slightly improved from 79.21% to 80.08% (system 1 versus system 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.99974125623703}]}, {"text": "Moreover, we tested system 3 with word2vec (Google News skip-gram) instead of NumberBatch embeddings obtaining an accuracy of 78.84%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9993804693222046}]}, {"text": "With these results, we chose the best system in the development phase (system 3 in).", "labels": [], "entities": []}, {"text": "The results obtained with this system on the test set are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the development set.", "labels": [], "entities": []}, {"text": " Table 2: Official results on the test set.", "labels": [], "entities": []}]}