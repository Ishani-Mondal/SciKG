{"title": [{"text": "Element-wise Bilinear Interaction for Sentence Matching", "labels": [], "entities": [{"text": "Sentence Matching", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9255786240100861}]}], "abstractContent": [{"text": "When we build a neural network model predicting the relationship between two sentences , the most general and intuitive approach is to use a Siamese architecture, where the sentence vectors obtained from a shared en-coder is given as input to a classifier.", "labels": [], "entities": []}, {"text": "For the classifier to work effectively, it is important to extract appropriate features from the two vectors and feed them as input.", "labels": [], "entities": []}, {"text": "There exist several previous works that suggest heuristic-based function for matching sentence vectors, however it cannot be said that the heuris-tics tailored fora specific task generalize to other tasks.", "labels": [], "entities": []}, {"text": "In this work, we propose anew matching function, ElBiS, that learns to model element-wise interaction between two vectors.", "labels": [], "entities": []}, {"text": "From experiments, we empirically demonstrate that the proposed ElBiS matching function outperforms the concatenation-based or heuristic-based matching functions on natural language inference and paraphrase identification , while maintaining the fused representation compact.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 195, "end_pos": 220, "type": "TASK", "confidence": 0.8107767701148987}]}], "introductionContent": [{"text": "Identifying the relationship between two sentences is a key component for various natural language processing tasks such as paraphrase identification, semantic relatedness prediction, textual entailment recognition, etc.", "labels": [], "entities": [{"text": "Identifying the relationship between two sentences", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8372978965441386}, {"text": "paraphrase identification", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.8687960207462311}, {"text": "semantic relatedness prediction", "start_pos": 151, "end_pos": 182, "type": "TASK", "confidence": 0.7129294375578562}, {"text": "textual entailment recognition", "start_pos": 184, "end_pos": 214, "type": "TASK", "confidence": 0.7750934958457947}]}, {"text": "The most general and intuitive approach to these problems would be to encode each sentence using a sentence encoder network and feed the encoded vectors to a classifier network.", "labels": [], "entities": []}, {"text": "For a model to predict the relationship correctly, it is important for the input to the classifier to contain appropriate information.", "labels": [], "entities": []}, {"text": "The most na\u00a8\u0131vena\u00a8\u0131ve method is to concatenate the two vectors and delegate the role of extracting features to subsequent network components.", "labels": [], "entities": []}, {"text": "However, despite the theoretical fact that even a single-hidden layer feedforward network can approximate any arbitrary function, the space of network parameters is too large, and it is helpful to narrow down the search space by directly giving information about interaction to the classifier model, as empirically proven in previous works built for various tasks, to name but a few).", "labels": [], "entities": []}, {"text": "In this paper, we propose a matching function which learns from data to fuse two sentence vectors and extract useful features.", "labels": [], "entities": []}, {"text": "Unlike bilinear pooling methods designed for matching vectors from heterogeneous domain (e.g. image and text), our proposed method utilizes element-wise bilinear interaction between vectors rather than interdimensional interaction.", "labels": [], "entities": []}, {"text": "In \u00a73, we will describe the intuition and assumption behind the restriction of interaction.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, we briefly introduce previous work related to our objective.", "labels": [], "entities": []}, {"text": "The detailed explanation of the proposed model is given in \u00a73, and we show its effectiveness in extracting compact yet powerful features in \u00a74.", "labels": [], "entities": []}, {"text": "\u00a75 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evalute our proposed ElBiS model on the natural language inference and paraphrase identification task.", "labels": [], "entities": [{"text": "paraphrase identification task", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.81769198179245}]}, {"text": "Implementation for experiments will be made public.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8961175084114075}]}], "tableCaptions": []}