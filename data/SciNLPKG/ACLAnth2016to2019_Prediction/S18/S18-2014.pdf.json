{"title": [], "abstractContent": [{"text": "Mining electronic health records for patients who satisfy a set of predefined criteria is known in medical informatics as phenotyping.", "labels": [], "entities": []}, {"text": "Phenotyping has numerous applications such as outcome prediction, clinical trial recruitment , and retrospective studies.", "labels": [], "entities": [{"text": "outcome prediction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9510771334171295}, {"text": "clinical trial recruitment", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.6651524504025778}]}, {"text": "Supervised machine learning for phenotyping typically relies on sparse patient representations such as bag-of-words.", "labels": [], "entities": []}, {"text": "We consider an alternative that involves learning patient representations.", "labels": [], "entities": []}, {"text": "We develop a neural network model for learning patient representations and show that the learned representations are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.", "labels": [], "entities": [{"text": "comorbidity detection task", "start_pos": 185, "end_pos": 211, "type": "TASK", "confidence": 0.8014333248138428}]}], "introductionContent": [{"text": "Mining electronic health records for patients who satisfy a set of predefined criteria is known in medical informatics as phenotyping.", "labels": [], "entities": []}, {"text": "Phenotyping has numerous applications such as outcome prediction, clinical trial recruitment, and retrospective studies.", "labels": [], "entities": [{"text": "outcome prediction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9510771334171295}, {"text": "clinical trial recruitment", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.6651524504025778}]}, {"text": "Supervised machine learning is currently the predominant approach to automatic phenotyping and it typically relies on sparse patient representations such as bag-of-words and bag-ofconcepts (.", "labels": [], "entities": [{"text": "Supervised machine learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6235484679539999}]}, {"text": "We consider an alternative that involves learning patient representations.", "labels": [], "entities": []}, {"text": "Our goal is to develop a conceptually simple method for learning lower dimensional dense patient representations that succinctly capture the information about a patient and are suitable for downstream machine learning tasks.", "labels": [], "entities": []}, {"text": "Our method uses cheap supervision in the form of billing codes and thus has representational power of a large dataset.", "labels": [], "entities": []}, {"text": "The learned representations can be used to train phenotyping classifiers with much smaller datasets.", "labels": [], "entities": []}, {"text": "Recent trends in machine learning have used neural networks for representation learning, and these ideas have propagated into the clinical informatics literature, using information from electronic health records to learn dense patient representations ().", "labels": [], "entities": [{"text": "representation learning", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.9591613709926605}]}, {"text": "Most of this work to date has used only codified variables, including ICD (International Classification of Diseases) codes, procedure codes, and medication orders, often reduced to smaller subsets.", "labels": [], "entities": [{"text": "ICD (International Classification of Diseases) codes", "start_pos": 70, "end_pos": 122, "type": "DATASET", "confidence": 0.8189276158809662}]}, {"text": "Recurrent neural networks are commonly used to represent temporality (, and many methods map from code vocabularies to dense embedding input spaces (.", "labels": [], "entities": []}, {"text": "One of the few patient representation learning systems to incorporate electronic medical record (EMR) text is DeepPatient (.", "labels": [], "entities": [{"text": "patient representation learning", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.7770293951034546}]}, {"text": "This system takes as input a variety of features, including coded diagnoses as the above systems, but also uses topic modeling on the text to get topic features, and applies a tool that maps text spans to clinical concepts in standard vocabularies (SNOMED and RxNorm).", "labels": [], "entities": []}, {"text": "To learn the representations they use a model consisting of stacked denoising autoencoders.", "labels": [], "entities": []}, {"text": "In an autoencoder network, the goal of training is to reconstruct the input using hidden layers that compress the size of the input.", "labels": [], "entities": []}, {"text": "The output layer and the input layer therefore have the same size, and the loss function calculates reconstruction error.", "labels": [], "entities": []}, {"text": "The hidden layers thus form the patient representation.", "labels": [], "entities": []}, {"text": "This method is used to predict novel ICD codes (from a reduced set with 78 elements) occurring in the next 30, 60, 90, and 180 days.", "labels": [], "entities": []}, {"text": "Our work extends these methods by building a neural network system for learning patient representations using text variables only.", "labels": [], "entities": []}, {"text": "We train this model to predict billing codes, but solely as a means to learning representations.", "labels": [], "entities": []}, {"text": "We show that the representations learned for this task are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.", "labels": [], "entities": [{"text": "comorbidity detection task", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.7975072065989176}]}, {"text": "Our work can also be viewed as an instance of transfer learning: we store the knowledge gained from a source task (billing code prediction) and apply it to a different but related target task.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9312383234500885}, {"text": "billing code prediction)", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.8087560683488846}]}], "datasetContent": [{"text": "For training patient representations, we utilize the MIMIC III corpus).", "labels": [], "entities": [{"text": "MIMIC III corpus", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8436051408449808}]}, {"text": "MIMIC III contains notes for over 40,000 critical care unit patients admitted to Beth Israel Deaconess Medi-cal Center as well as ICD9 diagnostic, procedure, and Current Procedural Terminology (CPT) codes.", "labels": [], "entities": [{"text": "MIMIC III", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8001197576522827}, {"text": "Beth Israel Deaconess Medi-cal Center", "start_pos": 81, "end_pos": 118, "type": "DATASET", "confidence": 0.7139732420444489}, {"text": "ICD9", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.59340500831604}, {"text": "Current Procedural Terminology (CPT)", "start_pos": 162, "end_pos": 198, "type": "TASK", "confidence": 0.5287465254465739}]}, {"text": "Since our goal is learning patient-level representations, we concatenate all available notes for each patient into a single document.", "labels": [], "entities": []}, {"text": "We also combine all ICD9 and CPT codes fora patient to form the targets for the prediction task.", "labels": [], "entities": [{"text": "ICD9", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.6438146233558655}, {"text": "prediction task", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.891986221075058}]}, {"text": "Finally, we process the patient documents with cTAKES to extract UMLS CUIs.", "labels": [], "entities": []}, {"text": "cTAKES is an open-source system for processing clinical texts which has an efficient dictionary lookup component for identifying CUIs, making it possible to process a large number of patient documents.", "labels": [], "entities": [{"text": "cTAKES", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9465109705924988}]}, {"text": "To decrease training time, we reduce the complexity of the prediction task as follows: we collapse all ICD9 and CPT codes to their more general category (e.g. first three digits for ICD9 diagnostic codes), (2) we drop all CUIs that appear fewer than 100 times, (3) we discard patients that have over 10,000 CUIs, (4) we discard all billing codes that have fewer than 1,000 examples.", "labels": [], "entities": []}, {"text": "This preprocessing results in a dataset consisting of 44,211 patients mapped to multiple codes (174 categories total).", "labels": [], "entities": []}, {"text": "We randomly split the patients into a training set (80%) and a validation set (20%) for tuning hyperparameters.", "labels": [], "entities": []}, {"text": "For evaluating our patient representations, we use a publicly available dataset from the Informatics for Integrating Biology to the Bedside (i2b2) Obesity challenge.", "labels": [], "entities": [{"text": "Bedside (i2b2) Obesity challenge", "start_pos": 132, "end_pos": 164, "type": "DATASET", "confidence": 0.757649838924408}]}, {"text": "Obesity challenge data consisted of 1237 discharge summaries from the Partners HealthCare Research Patient Data Repository annotated with respect to obesity and its fifteen most common comorbidities.", "labels": [], "entities": [{"text": "Partners HealthCare Research Patient Data Repository", "start_pos": 70, "end_pos": 122, "type": "DATASET", "confidence": 0.9605558713277181}]}, {"text": "Each patient was thus labeled for sixteen different categories.", "labels": [], "entities": []}, {"text": "We focus on the more challenging intuitive task, containing three label types (present, absent, questionable), where annotators labeled a diagnosis as present if its presence could be inferred (i.e., even if not explicitly mentioned).", "labels": [], "entities": []}, {"text": "This task involves complicated decision-making and inference.", "labels": [], "entities": []}, {"text": "Importantly, our patient representations are evaluated in sixteen different classification tasks with patient data originating from a healthcare institution different from the one our representations were trained on.", "labels": [], "entities": []}, {"text": "This setup is challenging yet it presents a true test of robustness of the learned representations.", "labels": [], "entities": []}, {"text": "Our first baseline is an SVM classifier trained with bag-of-CUIs features.", "labels": [], "entities": []}, {"text": "Our second baseline involves linear dimensionality reduction performed by running singular value decomposition (SVD) on a patient-CUI matrix derived from the MIMIC corpus, reducing the space by selecting the 1000 largest singular values, and mapping the target instances into the resulting 1000-dimensional space.", "labels": [], "entities": [{"text": "linear dimensionality reduction", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6062367558479309}, {"text": "MIMIC corpus", "start_pos": 158, "end_pos": 170, "type": "DATASET", "confidence": 0.913912832736969}]}, {"text": "Our multi-label billing code classifier is trained to maximize the macro F1 score for billing code prediction on the validation set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9236879348754883}, {"text": "billing code prediction", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.8191524942715963}]}, {"text": "We train the model for 75 epochs with a learning rate of 0.001 and batch size of 50.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9794620871543884}]}, {"text": "These hyperparameters are obtained by tuning the model's macro F1 on the validation set.", "labels": [], "entities": [{"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.7238420248031616}]}, {"text": "Observe that tuning of hyperparameters occurred independently from the target task.", "labels": [], "entities": []}, {"text": "Also note that since our goal is not to obtain the best possible performance on a held outset, we are not allocating separate development and test sets.", "labels": [], "entities": []}, {"text": "Once we determine the best values of these hyperparameters, we combine the training and validation sets and retrain the model.", "labels": [], "entities": []}, {"text": "We train two version of the model: (1) with randomly initialized CUI embeddings, (2) with word2vec-pretrained CUI embeddings.", "labels": [], "entities": []}, {"text": "Pre-trained embeddings are learned using word2vec) by extracting all CUIs from the text of MIMIC III notes and using the CBOW method with windows size of 5 and embedding dimension of 300.", "labels": [], "entities": [{"text": "MIMIC III notes", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8529679576555887}]}, {"text": "We then create a 1000-dimensional vector representation for each patient in the i2b2 obesity challenge data by giving the sparse (CUI-based) representation for each patient as input to the ICD code classifier.", "labels": [], "entities": [{"text": "i2b2 obesity challenge data", "start_pos": 80, "end_pos": 107, "type": "DATASET", "confidence": 0.5693924352526665}, {"text": "ICD code classifier", "start_pos": 189, "end_pos": 208, "type": "DATASET", "confidence": 0.9188647270202637}]}, {"text": "Rather than reading the classifier's predictions, we harvest the hidden layer outputs, forming a 1000-dimensional dense vector.", "labels": [], "entities": []}, {"text": "We then train multi-class SVM classifiers for each disease (using one-vs.-all strategy), building sixteen SVM classifiers.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.8358820676803589}]}, {"text": "Following the i2b2 obesity challenge, the models are evaluated using macro precision, recall, and F1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8820405602455139}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.999442994594574}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9998301267623901}]}, {"text": "We make the code available for use by the research community 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comorbidity challenge results (intuitive task). SVM trained using sparse representations (bag-of-CUIs)  is compared to SVM trained using SVD-based representations and learned dense patient representations.", "labels": [], "entities": [{"text": "Comorbidity", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9513886570930481}]}]}