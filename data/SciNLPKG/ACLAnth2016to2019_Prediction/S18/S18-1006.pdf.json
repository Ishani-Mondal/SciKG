{"title": [{"text": "THU NGN at SemEval-2018 Task 3: Tweet Irony Detection with Densely Connected LSTM and Multi-task Learning", "labels": [], "entities": [{"text": "THU NGN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7170592248439789}, {"text": "Tweet Irony Detection", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6934842964013418}]}], "abstractContent": [{"text": "Detecting irony is an important task to mine fine-grained information from social web messages.", "labels": [], "entities": [{"text": "Detecting irony", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9539181888103485}]}, {"text": "Therefore, the Semeval-2018 task 3 is aimed to detect the ironic tweets (subtask A) and their irony types (subtask B).", "labels": [], "entities": []}, {"text": "In order to address this task, we propose a system based on a densely connected LSTM network with multi-task learning strategy.", "labels": [], "entities": []}, {"text": "In our dense LSTM model, each layer will take all outputs from previous layers as input.", "labels": [], "entities": []}, {"text": "The last LSTM layer will output the hidden representations of texts, and they will be used in three classification task.", "labels": [], "entities": []}, {"text": "In addition, we incorporate several types of features to improve the model performance.", "labels": [], "entities": []}, {"text": "Our model achieved an F-score of 70.54 (ranked 2/43) in the subtask A and 49.47 (ranked 3/29) in the subtask B.", "labels": [], "entities": [{"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9995797276496887}]}, {"text": "The experimental results validate the effectiveness of our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Figurative languages such as irony are widely used in web messages such as tweets to convey different sentiment.", "labels": [], "entities": []}, {"text": "Identifying the ironic texts can help to understand the social web better and has many applications such as sentiment analysis.", "labels": [], "entities": [{"text": "Identifying the ironic texts", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7986030876636505}, {"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9706880748271942}]}, {"text": "Irony detecting techniques are important to improve the performance of sentiment analysis.", "labels": [], "entities": [{"text": "Irony detecting", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7938137352466583}, {"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9489999711513519}]}, {"text": "For example, the tweet \"Monday mornings are my fave:)# not\" is an irony with negative sentiment, but it will be probably classified as a positive one by a standard sentiment analysis model).", "labels": [], "entities": []}, {"text": "Thus, capturing the ironic information in texts is useful to predict sentiment more accurately (Van Hee et al., 2016a).", "labels": [], "entities": [{"text": "capturing the ironic information in texts", "start_pos": 6, "end_pos": 47, "type": "TASK", "confidence": 0.7597194314002991}]}, {"text": "However, determining whether a text is ironic is a challenging task since the the differences between ironic and non-ironic texts are usually subtle.", "labels": [], "entities": []}, {"text": "For example, the tweet \"Love this weather #not\" is ironic, but a similar tweet \"Hate this weather #not happy\" is non-ironic.", "labels": [], "entities": []}, {"text": "Different approaches are proposed to recognize the complex irony in texts.", "labels": [], "entities": []}, {"text": "Existing methods to detect irony are mainly based on rules or machine learning techniques (.", "labels": [], "entities": [{"text": "detect irony", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8585345447063446}]}, {"text": "Rules based methods usually depend on lexicons to identify irony ().", "labels": [], "entities": []}, {"text": "However, these methods cannot utilize the contextual information from texts.", "labels": [], "entities": []}, {"text": "Traditional machine learning based methods such as SVM are also effective in this task, but they usually need manually feature engineering (.", "labels": [], "entities": []}, {"text": "Recently, deep learning techniques are successfully applied to this task.", "labels": [], "entities": []}, {"text": "For example, propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets.", "labels": [], "entities": []}, {"text": "Their method can significantly improve the classification performance without heavy feature engineering.", "labels": [], "entities": [{"text": "classification", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.9609619975090027}]}, {"text": "However, existing methods are aimed to detect irony in tweets with explicit irony related hashtags.", "labels": [], "entities": []}, {"text": "For example, tweets with #irony or #sarcasm hashtags are very likely to be ironic.", "labels": [], "entities": []}, {"text": "Therefore, models may focus on these hashtags rather than the contextual information.", "labels": [], "entities": []}, {"text": "To fill this gap, the SemEval-2018 task 3 1 aims to detect irony of tweets without explicit irony hashtags (Van Hee et al., 2018).", "labels": [], "entities": [{"text": "SemEval-2018 task 3", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8044174114863077}]}, {"text": "The subtask A is aimed to determine whether a tweet is ironic.", "labels": [], "entities": []}, {"text": "the subtask B is aimed to identify the irony types of tweets: Verbal irony by means of a polarity contrast, other verbal irony and situational irony.", "labels": [], "entities": []}, {"text": "Several examples are as follows: \u2022 verbal irony by means of a polarity contrast: I love waking up with migraines #not \u2022 other verbal irony:  \u2022 situational irony: most of us didn't focus in the #ADHD lecture.", "labels": [], "entities": [{"text": "situational irony", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.6882137656211853}]}, {"text": "#irony In order to address this problem, we propose a system 2 based on a densely connected LSTM model () with multitask learning techniques.", "labels": [], "entities": [{"text": "irony", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9597097635269165}]}, {"text": "In our model, each LSTM layer will take all outputs of previous LSTM layers as input.", "labels": [], "entities": []}, {"text": "Then different levels of contextual information can be learned at the same time.", "labels": [], "entities": []}, {"text": "Our model is required to predict in three tasks simultaneously: 1) identifying the missing irony related hashtags; 2) classify ironic or non-ironic; 3) irony type classification.", "labels": [], "entities": [{"text": "irony type classification", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.7603744467099508}]}, {"text": "By using multitask learning strategy, the model can combine the information in the different tasks to improve the performance.", "labels": [], "entities": []}, {"text": "The experimental results in both subtasks validate the effectiveness of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "The detailed statistics of the dataset 6 in this task are shown in.", "labels": [], "entities": []}, {"text": "V-irony, O-irony and S-irony represent the three types respectively: verbal irony by means of a polarity contrast, other types of verbal irony and situational irony (Van Hee et al., 2018).", "labels": [], "entities": []}, {"text": "In subtask A, the performance of systems is evaluated by F-score for the positive class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9938750863075256}]}, {"text": "In subtask B, the macro-averaged F-score overall classes is used as the metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.8986589908599854}]}, {"text": "We combine two pre-trained word embeddings: 1) the embeddings provided by, which are trained on a corpus with 400 million tweets; 2) the embeddings provided by, which are trained on 20 million tweets.", "labels": [], "entities": []}, {"text": "The dimensions of them are 400 and 300 respectively.", "labels": [], "entities": []}, {"text": "They are concatenated together as the embeddings of words.", "labels": [], "entities": []}, {"text": "In our network, the Dense-LSTM model has 4 LSTM layers with 200-dim hidden states.", "labels": [], "entities": []}, {"text": "The hidden dimensions of dense layers are set to 300.", "labels": [], "entities": []}, {"text": "The dropout rate of each layer is set to a random number between 0.2 to 0.4, and it will beset to a fixed value 0.3 in the comparative experiments without ensemble strategy.", "labels": [], "entities": []}, {"text": "In subtask A, the loss weights \u03b1 of the three task are set to 0.5, 1 and 0.5 respectively.", "labels": [], "entities": []}, {"text": "In subtask B, they are 0.5, 0.5 and 1.", "labels": [], "entities": []}, {"text": "We use RMSProp as the optimizer, and the batch size is set to 64.", "labels": [], "entities": []}, {"text": "In addition, we use 10% training data for validation to select the hyperparameters above.", "labels": [], "entities": []}, {"text": "We compare the performance of different methods including: 1) SVM, the benchmark system using SVM and BOW model; 2) CNN, using CNN with a global average pooling layer to obtain the hidden vector h, which is used to predict in the three tasks; 3) LSTM, using one Bi-LSTM layer in the network to get h; 4) 2-layer LSTM, using 2 Bi-LSTM layers; 5) Dense-LSTM, using our Dense-LSTM model; 6) Dense-LSTM+ens, using our Dense-LSTM model and ensemble strategy.", "labels": [], "entities": [{"text": "BOW", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9041417241096497}]}, {"text": "In addition, we apply multi-task learning technique to all models except the benchmark system based on SVM.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The experimental results show that our Dense-LSTM model significantly outperforms the baselines.", "labels": [], "entities": []}, {"text": "Since the layers in our Dense-LSTM can learn from all previous outputs, our model can combine different levels of contextual information to capture the high-level irony clues.", "labels": [], "entities": []}, {"text": "In addition, our model can predict more accurately via ensemble.", "labels": [], "entities": []}, {"text": "Since models with random dropout can extract different information, we can take advantage of all models by voting.", "labels": [], "entities": []}, {"text": "The ensemble strategy can reduce the noise in the dataset and make our system more stable: The performance of different methods.", "labels": [], "entities": []}, {"text": "P, R, F represent precision, recall and F-score respectively.", "labels": [], "entities": [{"text": "F", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9675860404968262}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997503161430359}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996106028556824}, {"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9955171942710876}]}], "tableCaptions": [{"text": " Table 1. V-irony, O-irony and S-irony  represent the three types respectively: verbal irony  by means of a polarity contrast, other types of ver- bal irony and situational irony (Van Hee et al.,  2018). In subtask A, the performance of systems  is evaluated by F-score for the positive class. In  subtask B, the macro-averaged F-score over all  classes is used as the metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 262, "end_pos": 269, "type": "METRIC", "confidence": 0.9586623311042786}, {"text": "F-score", "start_pos": 328, "end_pos": 335, "type": "METRIC", "confidence": 0.8862098455429077}]}, {"text": " Table 1: The detailed statistics of the dataset.", "labels": [], "entities": []}, {"text": " Table  1. The experimental results show that our Dense- LSTM model significantly outperforms the base- lines. Since the layers in our Dense-LSTM can  learn from all previous outputs, our model can  combine different levels of contextual information  to capture the high-level irony clues. In addition,  our model can predict more accurately via ensem- ble. Since models with random dropout can ex- tract different information, we can take advantage  of all models by voting. The ensemble strategy  can reduce the noise in the dataset and make our  system more stable", "labels": [], "entities": [{"text": "ensem- ble", "start_pos": 346, "end_pos": 356, "type": "METRIC", "confidence": 0.7847981651624044}]}, {"text": " Table 2: The performance of different methods. P, R, F  represent precision, recall and F-score respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9996963739395142}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9993334412574768}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.989443838596344}]}, {"text": " Table 3: The performance in two subtasks using differ- ent combinations of training tasks.", "labels": [], "entities": []}, {"text": " Table 4: Influence of pre-trained word embedding. The  emb1 and emb2 denote the embeddings provided by  Godin et al. (2015) and Barbieri et al. (2016) respec- tively.", "labels": [], "entities": []}, {"text": " Table 5: Influence of different features on our model.", "labels": [], "entities": []}]}