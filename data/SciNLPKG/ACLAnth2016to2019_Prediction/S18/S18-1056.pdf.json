{"title": [{"text": "psyML at SemEval-2018 Task 1: Transfer Learning for Sentiment and Emotion Analysis", "labels": [], "entities": [{"text": "Sentiment and Emotion Analysis", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7273019850254059}]}], "abstractContent": [{"text": "In this paper, we describe the first attempt to perform transfer learning from sentiment to emotions.", "labels": [], "entities": [{"text": "transfer learning from sentiment to emotions", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.90013520916303}]}, {"text": "Our system employs Long Short-Term Memory (LSTM) networks, including bidirectional LSTM (biLSTM) and LSTM with attention mechanism.", "labels": [], "entities": []}, {"text": "We perform transfer learning by first pre-training the LSTM networks on sentiment data before concatenating the penultimate layers of these networks into a single vector as input to new dense layers.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9651080667972565}]}, {"text": "For the E-c subtask, we utilize a novel approach to train models for correlated emotion classes.", "labels": [], "entities": []}, {"text": "Our system performs 4/48, 3/39, 8/38, 4/37, 4/35 on all English subtasks EI-reg, EI-oc, V-reg, V-oc, E-c of SemEval 2018 Task 1: Affect in Tweets.", "labels": [], "entities": []}], "introductionContent": [{"text": "SemEval-2018 Task 1: Affect in Tweets) is a shared task expanding on previous SemEval sentiment tasks and the WASSA-2017 Shared Task on Emotion Intensity.", "labels": [], "entities": [{"text": "SemEval sentiment tasks", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.8482996622721354}]}, {"text": "It presents 5 tasks: 1.", "labels": [], "entities": []}, {"text": "Emotion intensity regression (EI-reg): given tweet and emotion (fear, anger, joy or sadness), predict real-valued emotion intensity from 0 to 1. 2. Emotion intensity ordinal classification (EIoc): given tweet and emotion (fear, anger, joy or sadness), predict emotion intensity ordinal class from 0 (no emotion) to 3 (high emotion).", "labels": [], "entities": []}, {"text": "3. Sentiment intensity regression (V-reg): given tweet, predict real-valued sentiment intensity from 0 (no sentiment) to 1 (high sentiment).", "labels": [], "entities": [{"text": "Sentiment intensity regression (V-reg)", "start_pos": 3, "end_pos": 41, "type": "METRIC", "confidence": 0.7649951030810674}]}, {"text": "In this subtask, the directionality of the tweet sentiment is ignored.", "labels": [], "entities": []}, {"text": "A negative tweet will be given the same score as a positive tweet with the same valence.", "labels": [], "entities": []}, {"text": "4. Sentiment intensity ordinal classification (Voc): given tweet, predict sentiment ordinal intensity class from -3 (very negative) to 3 (very positive).", "labels": [], "entities": [{"text": "Sentiment intensity ordinal classification (Voc)", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.6334660138402667}]}, {"text": "5. Emotion classification (E-c): given tweet, predict for each one of 11 emotions (anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust) whether the emotion is neutral (0) or present (1).", "labels": [], "entities": []}, {"text": "The task is particularly challenging since E-c and EI-oc are completely new subtasks.", "labels": [], "entities": []}, {"text": "Thus, no prior data or working models are available for comparison.", "labels": [], "entities": []}, {"text": "The leaderboard is also not public during the competition.", "labels": [], "entities": []}, {"text": "As shown in, taken from , the development sets are particularly small compared to the test sets, and the test sets are comparable in size to the training sets, so the model must generalize.", "labels": [], "entities": []}, {"text": "For EI-oc and EI-reg, the development and test sets are also annotated separately from the training sets.", "labels": [], "entities": []}, {"text": "This impacts performance as our system would have placed 1st with average pearson score 0.755 on the WASSA 2017 task, in which the EI-reg train, development, and test data are annotated in the same format.", "labels": [], "entities": [{"text": "pearson score 0.755", "start_pos": 74, "end_pos": 93, "type": "METRIC", "confidence": 0.9573590755462646}, {"text": "WASSA 2017 task", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.6886295676231384}]}, {"text": "Furthermore, tweets are difficult to analyze due to the unstructuredness of its language (hashtags, emoticons, slang, misspellings, poor grammar).", "labels": [], "entities": []}, {"text": "Previously submitted systems in SemEval sentiment analysis use deep learning models such as CNN, RNN and LSTMs (.", "labels": [], "entities": [{"text": "SemEval sentiment analysis", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.9740697344144186}]}, {"text": "Ina previous run of EI-reg in WASSA-2017 Shared Task on Emotion Intensity, top performing teams use deep learning models ().", "labels": [], "entities": [{"text": "WASSA-2017 Shared Task on Emotion Intensity", "start_pos": 30, "end_pos": 73, "type": "TASK", "confidence": 0.5371066778898239}]}, {"text": "In both tasks, some participants use an ensemble approach (.", "labels": [], "entities": []}, {"text": "To extract linguistic features, some systems employ pre-trained word embeddings ( or a combination of manually created features and/or lexicons.", "labels": [], "entities": []}, {"text": "However, exclusively relying on hand-crafted features for EIreg may result in a model that fails to encompass unforeseen linguistic relationships.", "labels": [], "entities": []}, {"text": "Similarly, relying exclusively on deep learning models without lexicon inputs can lead to simple misclassifications due to the small training data.", "labels": [], "entities": []}, {"text": "To combine the best of both worlds, previous systems collapse high-dimensional word embeddings into a single dimension arithmetically, before combining it with hand-crafted features (usually one-dimensional).", "labels": [], "entities": []}, {"text": "Goel et al. for instance averaged the word embeddings for each word in a tweet in order to concatenate it with a 43-dimensional vector.", "labels": [], "entities": []}, {"text": "Duppada and Hiray simply averaged the two top performing model outputs.", "labels": [], "entities": []}, {"text": "In this paper, we present a deep learning system whose variants competed competitively in all English subtasks in SemEval-2018 Task 1: Affect in Tweets, specifically EI-reg, EI-oc, V-reg, V-oc, and E-c.", "labels": [], "entities": []}, {"text": "We make the following contributions: \u2022 A deep learning system that can take in a combination of one-dimensional handcrafted and multi-dimensional word embedding inputs.", "labels": [], "entities": []}, {"text": "\u2022 A deep learning system that uses transfer learning from sentiment tasks to overcome the lack of training data compared to test data.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first instance of transferring knowledge from sentiment to emotion.", "labels": [], "entities": []}, {"text": "\u2022 Specifically for Task E-c, procedures for training correlated target classes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We build all the models with the Keras library and train them on Google Datalab.", "labels": [], "entities": [{"text": "Google Datalab", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.9639805257320404}]}, {"text": "The dendrogram diagram is built with Plotly.", "labels": [], "entities": [{"text": "Plotly", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9004554748535156}]}, {"text": "SemEval Results Our model ranks 4/48 in EI-reg, 3/39 in EI-oc, 8/38 in V-reg, 4/37 in V-oc, 4/35 in E-c ( . Our performance for V-reg is less than satisfactory because V-reg measures sentiment intensity without regards for directionality, whereas our source task takes into account directionality.", "labels": [], "entities": []}, {"text": "This supports findings by pre-training is less useful in a semantically different transfer.", "labels": [], "entities": []}, {"text": "System To evaluate our system, we assess the performance of each Component and various combinations of them.", "labels": [], "entities": []}, {"text": "shows the development set performance.", "labels": [], "entities": []}, {"text": "In particular, we note that Component A+B performs better than Component A or Component B separately, as with Component C+D.", "labels": [], "entities": []}, {"text": "Furthermore, Component A+B+C+D perform better overall compared to Component A+B and Component C+D.", "labels": [], "entities": []}, {"text": "Cluster training Subtask E-c classes are imbalanced, with 95% of \"Trust\" and \"Surprise\" training examples being negative.", "labels": [], "entities": []}, {"text": "shows the breakdown of negative and positive training examples for each of the E-c classes.", "labels": [], "entities": []}, {"text": "To assess our cluster training procedure, we evaluate the performance of independently training each of the E-c emotion classes (using afresh copy of the pre-trained system for each of the 11 emotions) as well as with various class weighing schemes.", "labels": [], "entities": []}, {"text": "Within independent training experiments, squared inverse weights performed best as measured by accuracy and micro-avg F1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9997546076774597}, {"text": "micro-avg F1", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.7492954134941101}]}, {"text": "Using squared inverse weights, cluster training performs better than independent training, attesting to the utility of cluster training.", "labels": [], "entities": []}, {"text": "For future work, we would like to experiment with other training methods such as multi-task learning and distant supervision, as well as tune the hyper-parameters of our model to augment its performance across all subtasks ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tweets in SemEval-2018: Affect  in Tweets Dataset.", "labels": [], "entities": [{"text": "Affect", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9893732666969299}]}, {"text": " Table 2: Results on development set comparing Component experiments.", "labels": [], "entities": []}, {"text": " Table 3: E-c training set, breakdown of percentage of positive and negative examples.", "labels": [], "entities": []}, {"text": " Table 4: E-c experiment results on development set.", "labels": [], "entities": []}]}