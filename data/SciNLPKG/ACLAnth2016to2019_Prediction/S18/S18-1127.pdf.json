{"title": [{"text": "MIT-MEDG at SemEval-2018 Task 7: Semantic Relation Classification via Convolution Neural Network", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7077743411064148}, {"text": "Semantic Relation Classification", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.8657941420873007}]}], "abstractContent": [{"text": "SemEval 2018 Task 7 tasked participants to build a system to classify two entities within a sentence into one of the 6 possible relation types.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5558270066976547}]}, {"text": "We tested 3 classes of models: Linear classifiers, Long Short-Term Memory (LSTM) models, and Convolutional Neural Network (CNN) models.", "labels": [], "entities": []}, {"text": "Ultimately, the CNN model class proved most performant, so we specialized to this model for our final submissions.", "labels": [], "entities": [{"text": "CNN model class", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9216044743855795}]}, {"text": "We improved performance beyond a vanilla CNN by including a variant of negative sampling , using custom word embeddings learned over a corpus of ACL articles, training over corpora of both tasks 1.1 and 1.2, using reversed feature, using part of context words beyond the entity pairs and using ensemble methods to improve our final predictions.", "labels": [], "entities": []}, {"text": "We also tested attention based pooling, up-sampling, and data augmentation, but none improved performance.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6852604895830154}]}, {"text": "Our model achieved rank 6 out of 28 (macro-averaged F1-score: 72.7) in subtask 1.1, and rank 4 out of 20 (macro F1: 80.6) in subtask 1.2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9592213034629822}]}], "introductionContent": [{"text": "SemEval 2018 Task 7 () focuses on relation classification and extraction on a corpus of 350 scientific paper abstracts consisting of 1228 and 1248 annotated sentences for subtasks 1.1 and 1.2, respectively.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6676571443676949}, {"text": "relation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.9249303638935089}]}, {"text": "There are six possible relations: USAGE, RESULT, MODEL-FEATURE, PART WHOLE, TOPIC, and COMPARE.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7743469476699829}, {"text": "RESULT", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9924395084381104}, {"text": "MODEL-FEATURE", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9841070175170898}, {"text": "PART WHOLE", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.7749195992946625}, {"text": "TOPIC", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8114621639251709}, {"text": "COMPARE", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.7189909219741821}]}, {"text": "Given this data, our task is to take an example sentence, as well as the left and right entities within that sentence, and an indicator as to whether the relation is reversed, and predict the relation type for that sentence.", "labels": [], "entities": []}, {"text": "In subtasks 1.1 and 1.2, all presented sentences have a relation.", "labels": [], "entities": []}, {"text": "We submitted predictions based on a selfensembled convolutional neural network (CNN) model trained with a negative sampling augmented loss using ACL-specific embeddings as input features.", "labels": [], "entities": []}, {"text": "We achieved rank 6 out of 28 (macro-averaged F1-score: 72.7) in subtask 1.1, and rank 4 out of 20 (macro F1 80.6) in subtask 1.2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9700620770454407}]}], "datasetContent": [{"text": "We tested several machine learning methods on these data, including a logistic regression classifier over tf-idf features extracted from words, lemmas, hypernyms, and POS.", "labels": [], "entities": []}, {"text": "Additionally, we tested deep random forests with multi-grain sequence scanning over word embeddings sequences ( presents the architecture of the CNN model.", "labels": [], "entities": []}, {"text": "The model first takes the tokenized sentence, as well as the targeted entities, and transforms it to a sequence of continuous embedding vectors (Subsection 3.3.1).", "labels": [], "entities": []}, {"text": "Next, the model uses a convolution layer to transform the embedded sentence to a fixed-size representation of the whole sentence (Subsection 3.3.2).", "labels": [], "entities": []}, {"text": "Finally, it computes the score for each relation class via a linear transformation (Subsection 3.3.3).", "labels": [], "entities": []}, {"text": "The overall system is trained end-to-end via across entropy loss augmented with a variant of negative sampling (Subsection 3.3.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: CNN Improvements over a series of modifica- tions. Each row includes the modifications of the previ- ous rows. All numbers are macro-F1 scores on test set  after 10 runs in the form of {average}\u00b1{standard de- viation} (the \"Ensemble\" row lacks deviation numbers  as it, being a variance reduction technique, does not  have the same sources of variation as the other models).  We report \u00b120 context words here, which was found to  be optimal in post-submission experimentation, but our  submitted models used \u00b150 context words, which was  preferred under initial cross validation.", "labels": [], "entities": []}, {"text": " Table 4. All the submissions  were based on the ensemble model listed in", "labels": [], "entities": []}, {"text": " Table 4: Final submission performance.", "labels": [], "entities": []}]}