{"title": [], "abstractContent": [{"text": "This paper describes 300-sparsans' participation in SemEval-2018 Task 9: Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings.", "labels": [], "entities": [{"text": "SemEval-2018 Task 9", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8240087032318115}, {"text": "Hypernym Discovery", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7552275061607361}]}, {"text": "Our system took first place in subtasks (1B) Italian (all and entities), (1C) Spanish entities, and (2B) music entities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words.", "labels": [], "entities": []}, {"text": "Turning these dense representations into a much sparser form can help in focusing on most salient parts of word representations.", "labels": [], "entities": []}, {"text": "Sparsity-based techniques often involve the coding of a large number of signals over the same dictionary.", "labels": [], "entities": []}, {"text": "Sparse, overcomplete representations have been motivated in various domains as away to increase separability and interpretability and stability in the presence of noise.", "labels": [], "entities": []}, {"text": "Non-negativity has also been argued to be advantageous for interpretability.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.9604370594024658}]}, {"text": "As illustrates this in the language domain, where sparse features are interpreted as lexical attributes, \"to describe the city of Pittsburgh, one might talk about phenomena typical of the city, like erratic weather and large bridges.", "labels": [], "entities": []}, {"text": "It is redundant and inefficient to list negative properties, like the absence of the Statue of Liberty\".", "labels": [], "entities": []}, {"text": "Berend (2018) utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other.", "labels": [], "entities": [{"text": "word translation", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7654728591442108}]}, {"text": "Here we apply sparse feature pairs to hypernym extraction.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8384210467338562}]}, {"text": "The role of an attribute pair i, j \u2208 \u03c6(q) \u00d7 \u03c6(h) (where q is the query word, h is the hypernym candidate, and \u03c6(w) is the index of a non-zero component in the sparse representations of w) is similar to interaction terms in regression, see section 2 for details.", "labels": [], "entities": []}, {"text": "Sparse representation is related to hypernymy in various natural ways.", "labels": [], "entities": [{"text": "Sparse representation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8077426254749298}]}, {"text": "One of them is through Formal concept Analysis (FCA).", "labels": [], "entities": [{"text": "Formal concept Analysis (FCA)", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7890981833140055}]}, {"text": "The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis (FCA) is relatively new ().", "labels": [], "entities": [{"text": "Formal concept Analysis (FCA)", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.7507948378721873}]}, {"text": "Our submissions experiment with formal concept analysis tool by.", "labels": [], "entities": []}, {"text": "See the next section fora description of formal concept lattices, and how hypernyms can be found in them.", "labels": [], "entities": []}, {"text": "Another natural formulation is related to hierarchical sparse coding (, where trees describe the order in which variables \"enter the model\" (i.e., take non-zero values).", "labels": [], "entities": [{"text": "hierarchical sparse coding", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6326297521591187}]}, {"text": "A node may take a non-zero value only if its ancestors also do: the dimensions that correspond to top level nodes should focus on \"general\" meaning components that are present inmost words.", "labels": [], "entities": []}, {"text": "offer an implementation that is efficient for gigaword corpora.", "labels": [], "entities": []}, {"text": "Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice.", "labels": [], "entities": []}, {"text": "The task) evaluated systems on their ability to extract hypernyms for query words in five subtasks (three languages, English, Italian, and Spanish, and two domains, medical and music).", "labels": [], "entities": []}, {"text": "Queries have been categorized as concepts or entities.", "labels": [], "entities": []}, {"text": "Results were reported for each category separately as well as in combined form, thus resulting in 5 \u00d7 3 combinations.", "labels": [], "entities": []}, {"text": "Our system took first place in subtasks (1B) Italian (all and entities), (1C) Spanish entities, and (2B) music entities.", "labels": [], "entities": []}, {"text": "Detailed results for our system appear in section 3.", "labels": [], "entities": []}, {"text": "Our source code is available online 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "After the evaluation closed, we conducted ablation experiments the results of which are included in.", "labels": [], "entities": []}, {"text": "In these experiments, we investigated the contribution of the features derived from sparse attribute pairs and FCA.", "labels": [], "entities": [{"text": "FCA", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8324177861213684}]}, {"text": "These ablation experiments corroborate the importance of features derived from sparse attribute pairs and reveal that turning off FCA-based features does not hurt performance at all.", "labels": [], "entities": []}, {"text": "For this reason -even though our official shared task submission included FCArelated features -we no longer employed them in our post-evaluation experiments.", "labels": [], "entities": [{"text": "FCArelated", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.8821554780006409}]}, {"text": "Test results of an oracle system which uses candidate filtering. training.", "labels": [], "entities": []}, {"text": "In our post evaluation experiments we investigated the effects of generating more negative samples, i.e. we regarded all the valid hypernyms over the training set -not being a proper hypernym for q -as h upon the creation of the (q, h ) negative training instances.", "labels": [], "entities": []}, {"text": "This latter strategy is referenced as ns = all in.", "labels": [], "entities": []}, {"text": "In our official submission we regarded only those hypernyms as potential candidates to rank during test time which occurred at least once as a correct hypernym in the training data.", "labels": [], "entities": []}, {"text": "We call this strategy as candidate filtering.", "labels": [], "entities": [{"text": "candidate filtering", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7756972014904022}]}, {"text": "Historically, we applied this restriction to speedup the FCA algorithm because this way the size of the concept lattice could be made smaller.", "labels": [], "entities": []}, {"text": "As there are valid hypernyms on the test set which never occurred in the training data, our official submission would not be able to obtain a perfect score even in theory.", "labels": [], "entities": []}, {"text": "Table 7 contains the best possible metrics on the test set that we could achieve when candidate filtering is applied.", "labels": [], "entities": []}, {"text": "In our post evaluation experiments we also investigated the effects of turning this kind of filtering step off.", "labels": [], "entities": []}, {"text": "As illustrates, however, our scores degrade after turning candidate filtering off.", "labels": [], "entities": []}, {"text": "Our post evaluation experiments in  gest that it is advantageous to apply sparse representation of more expressive power (i.e. a higher number of basis vectors).", "labels": [], "entities": []}, {"text": "Generating more negative samples also provides some additional performance boost.", "labels": [], "entities": []}, {"text": "These previous observations hold irrespective whether candidate filtering is employed or not, however, their effects are more pronounced when hypernym candidates are not filtered.", "labels": [], "entities": []}, {"text": "Finally, we report our post-evaluation results for all the subtasks and compare them to the official scores of the best performing systems in.", "labels": [], "entities": []}, {"text": "It can be seen from these enhanced results for category \"all\" (concepts and entities mixed) that we would win (1B) Italian and (1C) Spanish.", "labels": [], "entities": []}, {"text": "Our post-evaluation system -which only differs from our participating system that it fixes the calculation of a features, does not rely on FCA-based features and uses k = 1000 -would also place third in the rest of the subtasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our submissions results: official and those that can be reproduced with the code in the  project repo (with the isFrequentHypernym feature being turned off).", "labels": [], "entities": []}, {"text": " Table 5: Post evaluation results on the 1A dataset investigating the effect of various hyperparameter  choices. k and ns denotes the number of basis vectors and negative samples generated during training  per each positive (q, h) pair. Best results obtained for each metric are marked as bold.", "labels": [], "entities": [{"text": "1A dataset", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.8304260075092316}]}, {"text": " Table 6: Ablation experiments, on the 1A dataset  with k = 200, ns = 50 (and the implementa- tion of isFreqHyp fixed). The first two columns  indicate whether attributePair ij and FCA-derived  features are utilized, respectively.", "labels": [], "entities": [{"text": "1A dataset", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.832901805639267}]}, {"text": " Table 6. In these experiments, we investigated the  contribution of the features derived from sparse  attribute pairs and FCA. These ablation experi- ments corroborate the importance of features de- rived from sparse attribute pairs and reveal that  turning off FCA-based features does not hurt per- formance at all. For this reason -even though  our official shared task submission included FCA- related features -we no longer employed them in  our post-evaluation experiments.", "labels": [], "entities": [{"text": "FCA", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.873788595199585}]}, {"text": " Table 8: Post evaluation results for the different  subtasks using k = 1000, ns = 50 and hypernym  candidate filtering. Upper: our system, lower:  subtask winner.", "labels": [], "entities": []}]}