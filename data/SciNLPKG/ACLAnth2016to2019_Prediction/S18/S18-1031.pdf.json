{"title": [{"text": "Yuan at SemEval-2018 Task 1: Tweets Emotion Intensity Prediction using Ensemble Recurrent Neural Network", "labels": [], "entities": [{"text": "Tweets Emotion Intensity Prediction", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.8084872663021088}]}], "abstractContent": [{"text": "This paper describes the performing system for SemEval-2018 Task 1 subtask 3-Given a tweet, determine the intensity of sentiment or valence (V) that best represents the mental state of the tweeter-a real-valued score between 0 (most negative) and 1 (most positive).", "labels": [], "entities": [{"text": "SemEval-2018 Task 1", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8557600776354471}, {"text": "intensity of sentiment or valence (V)", "start_pos": 106, "end_pos": 143, "type": "METRIC", "confidence": 0.7551556080579758}]}, {"text": "The proposed system gets features in tweets from the existing emotional dictionary and represents the word using word embedding , then utilizes the joint representations as the inputs of the bidire-ctional long short-term memory (BiL-STM) to learn and get the regression result.", "labels": [], "entities": []}, {"text": "To boost performance we ensemble several BiLSTMs together.", "labels": [], "entities": []}, {"text": "We ranked 6th in subtask 3 among all teams.", "labels": [], "entities": []}, {"text": "Our approach achieves the Pearson(All instances) score 0.836 and Pearson(gold in 0.5-1) score 0.667, we outperform the baseline model of this task by 25.1% and 21.8% of Pearson(All instances) and Pearson(gold in 0.5-1) scores respectively.", "labels": [], "entities": [{"text": "Pearson(All instances) score 0.836", "start_pos": 26, "end_pos": 60, "type": "METRIC", "confidence": 0.765298238822392}, {"text": "Pearson(gold in 0.5-1) score 0.667", "start_pos": 65, "end_pos": 99, "type": "METRIC", "confidence": 0.9402865543961525}]}], "introductionContent": [{"text": "Sentiment analysis (SA) is afield of knowledge which deals with the analysis of people's opinions, sentiments, evaluations, appraisals, attitudes and emotions towards particular entities (.) is a shared task hosted by WASSA 2017, aiming to predict the emotion intensity in tweets.) is similar to EmoInt, however the goal of subtask 3 is to detect valence or sentiment intensity, in which scores are floating point values between 0 and 1, representing low and high intensities of the emotion being expressed, respectively.", "labels": [], "entities": [{"text": "Sentiment analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8682981848716735}]}, {"text": "Obviously we don't know in advance whether twitter's emotional intensity is positive or negative, but in EmoInt task we can determine whether twitter emotions are positive or negative based on one of four datasets: anger, fearness, joy, sadness.", "labels": [], "entities": []}, {"text": "This is still a challenging task and remains active areas of research.", "labels": [], "entities": []}, {"text": "These setbacks are: extensive usage of hashtags, slang, abbreviations, and emoticons.", "labels": [], "entities": []}, {"text": "And tweets are usually typed on mobile devices like mobile phone, laptop or iPad which can result in a substantial amount of typos.", "labels": [], "entities": []}, {"text": "Existing methods for modeling emotion intensity rely vastly on manually constructed lexicons, which contain information about intensity weights for each available word.", "labels": [], "entities": [{"text": "modeling emotion intensity", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.7264498869578043}]}, {"text": "The intensity for the whole tweet can be deduced by combining individual scores of words, which is easy and ignores the word order compositionality of the language.", "labels": [], "entities": []}, {"text": "Building such lexicons is a labourintensive procedure.", "labels": [], "entities": []}, {"text": "We can learn from these models the skills of combining feature extraction and classification or regression stages given a sufficient amount of training data.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7452886700630188}]}, {"text": "Some deep learning methods are used to process the same question.", "labels": [], "entities": []}, {"text": "Deep neural architectures for emotion intensity prediction in tweets ( and character-and word-level recurrent neural models for tweet emotion intensity detection ().", "labels": [], "entities": [{"text": "emotion intensity prediction", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7080315550168356}, {"text": "tweet emotion intensity detection", "start_pos": 128, "end_pos": 161, "type": "TASK", "confidence": 0.6786760911345482}]}, {"text": "In our work, we firstly clean tweets, then build lexical features and find optimal combinations of features to produce a final vector representation of a tweet, next train a neural network regression model and finally get the tweet's intensity scores.", "labels": [], "entities": []}, {"text": "In addition, we adjust our models' parameters and through the ensemble models to get the best performing results.", "labels": [], "entities": []}], "datasetContent": [{"text": "All our experiments have been developed using Keras deep learning library with Theano backend, and with CUDA enabled.", "labels": [], "entities": [{"text": "Keras deep learning library", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.7979349941015244}]}, {"text": "And all our experiments are performed on a computer with Intel Core(TM) i3 @3.4GHz 16GB of RAM and GeForce GTX 1060 GPU.", "labels": [], "entities": []}, {"text": "After testing many neural network models, we finally find the best results on LSTM and BiLSTM models.", "labels": [], "entities": []}, {"text": "shows the results of a single layer LSTM changing the loss function and word embedding, we can learn that MAE loss function can get the best result with Glove word embedding, in general the performance on Glove word embedding is better than word2vec embedding.", "labels": [], "entities": [{"text": "MAE", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9516304135322571}]}, {"text": "shows the results of a single BiLSTM changing the loss function and integrating ten models under different loss functions and different word embedding we can learn that MAPE loss function can get the best result with Glove word embedding, in general the performance on Glove word embedding is better than word2vec embedding. is the result of double layers BiLSTM changing the loss function and integrating ten models under different loss functions and different word embedding we can learn that MAPE loss function can get the best result with Glove word embedding, in general the performance on Glove word embedding is better than word2vec embedding.", "labels": [], "entities": []}, {"text": "The system in this subtask are evaluated using the Pearson correlation coefficient, which computes a bivariate linear coefficient, and the secondary evaluation metrics, which is Pearson correlation fora subset of the test set that includes only those tweets with intensity score greater or equal to 0.5.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 51, "end_pos": 82, "type": "METRIC", "confidence": 0.9084617296854655}, {"text": "Pearson correlation", "start_pos": 178, "end_pos": 197, "type": "METRIC", "confidence": 0.9421760141849518}]}, {"text": "We present the results of the system submitted to the competition leaderboard in.", "labels": [], "entities": []}, {"text": "The score of our system is 0.836 (Pearson) and 0.667 (Pearson gold in 0.5-1).", "labels": [], "entities": [{"text": "Pearson)", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8979895710945129}, {"text": "Pearson gold in 0.5-1", "start_pos": 54, "end_pos": 75, "type": "METRIC", "confidence": 0.8813784420490265}]}, {"text": "Note that the model we used on the test set is the best model on the development set, i.e., in: Performance on test dataset.", "labels": [], "entities": []}, {"text": "Final results in about test set on leaderboard and our system ranks 6th overall.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. The score  of our system is 0.836 (Pearson) and 0.667  (Pearson gold in 0.5-1). Note that the model  we used on the test set is the best model on the  development set, i.e., in", "labels": [], "entities": [{"text": "Pearson)", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9194910228252411}, {"text": "Pearson gold in 0.5-1", "start_pos": 66, "end_pos": 87, "type": "METRIC", "confidence": 0.9270214140415192}]}, {"text": " Table 1: Performance on development dataset. Single  layer LSTM under different loss functions and  different word embedding.", "labels": [], "entities": []}, {"text": " Table 2: Performance on development dataset.  Ensemble result of single layer BiLSTM under diff- erent loss functions and different word embedding.", "labels": [], "entities": []}, {"text": " Table 3: Performance on development data-set.  Ensemble result of double layers BiLSTM under diff- erent loss functions and different word embedding.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.6317307353019714}]}, {"text": " Table 4: Performance on test dataset. Final results in about  test set on leaderboard and our system ranks 6th overall.", "labels": [], "entities": []}]}