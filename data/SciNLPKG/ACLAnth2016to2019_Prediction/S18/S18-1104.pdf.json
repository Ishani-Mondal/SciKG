{"title": [{"text": "NLPRL-IITBHU at SemEval-2018 Task 3: Combining Linguistic Features and Emoji Pre-trained CNN for Irony Detection in Tweets", "labels": [], "entities": [{"text": "NLPRL-IITBHU", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8837553262710571}, {"text": "Irony Detection in Tweets", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.7830661609768867}]}], "abstractContent": [{"text": "This paper describes our participation in Se-mEval 2018 Task 3 on Irony Detection in Tweets.", "labels": [], "entities": [{"text": "Irony Detection in Tweets", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7558171674609184}]}, {"text": "We combine linguistic features with pre-trained activations of a neural network.", "labels": [], "entities": []}, {"text": "The CNN is trained on the emoji prediction task.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9461480379104614}, {"text": "emoji prediction task", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8994917472203573}]}, {"text": "We combine the two feature sets and feed them into an XGBoost Classifier for classification.", "labels": [], "entities": []}, {"text": "Subtask-A involves classification of tweets into ironic and non-ironic instances , whereas Subtask-B involves classification of tweets into non-ironic, verbal irony, situational irony or other verbal irony.", "labels": [], "entities": []}, {"text": "It is observed that combining features from these two different feature spaces improves our system results.", "labels": [], "entities": []}, {"text": "We leverage the SMOTE algorithm to handle the problem of class imbalance in Subtask-B. Our final model achieves an F1-score of 0.65 and 0.47 on Subtask-A and Subtask-B respectively.", "labels": [], "entities": [{"text": "SMOTE", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.5886732339859009}, {"text": "F1-score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9994407296180725}]}, {"text": "Our system ranks 4 th on both tasks, respectively, outperforming the baseline by 6% on Subtask-A and 14% on Subtask-B.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to the Merriam-Webster dictionary 1 , one of the meanings of irony is defined as 'the use of words to express something other than and especially the opposite of the literal meaning' (e.g. I love getting spam emails.).", "labels": [], "entities": [{"text": "Merriam-Webster dictionary 1", "start_pos": 17, "end_pos": 45, "type": "DATASET", "confidence": 0.9735592007637024}]}, {"text": "Irony can have different forms, such as verbal, situational, dramatic etc.", "labels": [], "entities": []}, {"text": "Sarcasm is also categorized as a form of verbal irony.", "labels": [], "entities": [{"text": "verbal irony", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.8004368841648102}]}, {"text": "Various attempts have been made in the past for detection of sarcasm (.", "labels": [], "entities": [{"text": "detection of sarcasm", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8966803352038065}]}, {"text": "Sarcastic texts are characterized by the presence of humor and ridicule, which are not always present in the case of ironic texts.", "labels": [], "entities": []}, {"text": "The absence of these characteristics makes automatic irony detection a more difficult problem than sarcasm detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7824567556381226}, {"text": "sarcasm detection", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.9120135009288788}]}, {"text": "Irony detection is a problem that is important for the working of many Natural Language Understanding Systems.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7378882616758347}]}, {"text": "For example, people often use irony to express their opinions on social media like Twitter ().", "labels": [], "entities": []}, {"text": "Detecting irony in social texts can aid in improving opinion analysis.", "labels": [], "entities": [{"text": "Detecting irony in social texts", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9057734131813049}, {"text": "opinion analysis", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.879502147436142}]}, {"text": "The SemEval 2018 task 3 (Van Hee et al., 2018) consists of two subtasks.", "labels": [], "entities": [{"text": "SemEval 2018 task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8881779114405314}]}, {"text": "Subtask-A involves predicting whether a tweet is ironic or not and Subtask-B involves categorizing a tweet into NonIronic, Verbal Irony (by means of a polarity contrast), Situational Irony and Other Forms of Verbal Irony.", "labels": [], "entities": [{"text": "predicting whether a tweet is ironic", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.8414423565069834}]}, {"text": "The task organizers use macro averaged F1, rather than accuracy to force systems to optimize to work well on all the four classes of tweets, as described in Section 3.1.", "labels": [], "entities": [{"text": "macro averaged F1", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.7105040152867635}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9991065859794617}]}, {"text": "Systems builtin the past primarily used handcrafted linguistic features for classification of ironic texts).", "labels": [], "entities": [{"text": "classification of ironic texts", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.8495291173458099}]}, {"text": "In our system, we try to combine them with the pre-trained activations of a neural network.", "labels": [], "entities": []}, {"text": "Our results show that both types of features complement each other, as the results produced by the combination of them surpass the results of using either the linguistic or the pre-trained activation features individually by a large margin.", "labels": [], "entities": []}, {"text": "We use XGBoost Classifier, as it performs at par with neural networks when the provided training data is of small size.", "labels": [], "entities": []}, {"text": "Our results indicate that oversampling techniques like SMOTE () can also be used to oversample the representations generated using neural networks to improve performance on imbalanced datasets.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 gives a detailed description of how our system was built, Section 3 then describes the experimental setup and the results obtained and Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The annotated tweet corpus provided for training consists of 1390 instances of Verbal Irony due to polarity contrast, 205 instances of Other Types of Verbal Irony, 316 Situational Ironic instances, and 1923 Non Ironic instances.", "labels": [], "entities": []}, {"text": "Our system only uses the training data provided by the organizers and no other annotated data is used (Constrained System).", "labels": [], "entities": []}, {"text": "The test dataset for Subtask-A contains 473 non-ironical tweets and 311 ironical tweets.", "labels": [], "entities": []}, {"text": "For Subtask-B, the 311 ironical tweets are further classified into Verbal Irony by means of Polarity Contrast (164), Situational Irony (85) and Other Forms Of Verbal Irony (62).", "labels": [], "entities": []}, {"text": "The evaluation metric used for ranking teams in Sub-task A is the F1 score of the positive (Ironic) class whereas in Subtask-B, the organizers use macro averaged F1 (average of F1 for each class) as an evaluation metric for ranking teams.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9830236732959747}, {"text": "macro averaged F1 (average of F1", "start_pos": 147, "end_pos": 179, "type": "METRIC", "confidence": 0.6323822651590619}]}], "tableCaptions": [{"text": " Table 1: F1 scores in Task A and Macro F1 in Task B on test set.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996705055236816}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.50394606590271}]}]}