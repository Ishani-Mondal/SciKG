{"title": [{"text": "#NonDicevoSulSerio at SemEval-2018 Task 3: Exploiting Emojis and Affective Content for Irony Detection in English Tweets", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the participation of the #NonDicevoSulSerio team at SemEval2018-Task3, which focused on Irony Detection in English Tweets and was articulated in two tasks addressing the identification of irony at different levels of granularity.", "labels": [], "entities": [{"text": "Irony Detection in English Tweets", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.7546983480453491}, {"text": "identification of irony", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.876101016998291}]}, {"text": "We participated in both tasks proposed: Task A is a classical binary classification task to determine whether a tweet is ironic or not, while Task B is a multi-class classification task devoted to distinguish different types of irony, where systems have to predict one out of four labels describing verbal irony by clash, other verbal irony, sit-uational irony, and non-irony.", "labels": [], "entities": []}, {"text": "We addressed both tasks by proposing a model built upon a well-engineered features set involving both syntactic and lexical features, and a wide range of affective-based features, covering different facets of sentiment and emotions.", "labels": [], "entities": []}, {"text": "The use of new features for taking advantage of the affec-tive information conveyed by emojis has been analyzed.", "labels": [], "entities": []}, {"text": "On this line, we also tried to exploit the possible incongruity between sentiment expressed in the text and in the emojis included in a tweet.", "labels": [], "entities": []}, {"text": "We used a Support Vector Machine classifier, and obtained promising results.", "labels": [], "entities": []}, {"text": "We also carried on experiments in an unconstrained setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of creative language and figurative language devices such as irony has been proven to be pervasive in social media (.", "labels": [], "entities": []}, {"text": "The presence of these devices makes the process of mining social media texts challenging, especially because they can influence and twist the sentiment polarity of an utterance in different ways.", "labels": [], "entities": []}, {"text": "Glossing over differences across different theoretical accounts proposed in the context of various disciplines, irony can be defined as an incongruity between the literal meaning of an utterance and its intended meaning (.", "labels": [], "entities": []}, {"text": "The term irony covers mainly two phenomena: verbal and situational irony).", "labels": [], "entities": [{"text": "situational irony", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7209435850381851}]}, {"text": "Situational irony refers to events or situations which fail to meet expectations, such as for instance \"warnings the dangerous effect of smoking on the cigarette advertisement\", while verbal irony occurs when the speaker intend to communicate a different meaning w.r.t what he/she is literally saying.", "labels": [], "entities": [{"text": "Situational irony", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7735258340835571}]}, {"text": "Most of the time it involves the intention of communicating an opposite meaning, and this kind of opposition can be expressed by polarity contrast.", "labels": [], "entities": []}, {"text": "However this is not the only possibility, and social media messages well reflect such variety, including different expressions of verbal irony and descriptions of situational irony).", "labels": [], "entities": []}, {"text": "Automatic irony detection is an important task to improve sentiment analysis ().", "labels": [], "entities": [{"text": "Automatic irony detection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7317915757497152}, {"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.966841846704483}]}, {"text": "However, detecting irony automatically from textual messages is still a challenging task for scholars.", "labels": [], "entities": [{"text": "detecting irony automatically from textual messages", "start_pos": 9, "end_pos": 60, "type": "TASK", "confidence": 0.9161756932735443}]}, {"text": "The linguistic and social factors which impact on the perception of irony contribute to make the task complex.", "labels": [], "entities": []}, {"text": "In this paper, we will describe the irony detection systems we developed for participating in SemEval2018-Task3: Irony Detection in English Tweets (Van Hee et al., 2018).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8213825523853302}, {"text": "SemEval2018-Task3: Irony Detection in English Tweets", "start_pos": 94, "end_pos": 146, "type": "TASK", "confidence": 0.7393792697361538}]}, {"text": "Our systems used a support vector classifier model by exploiting some novel and well-handcrafted features including lexical, syntactical and affective based features.", "labels": [], "entities": []}, {"text": "We participated in 3 different scenarios (Task A constrained, Task A unconstrained, and Task B unconstrained).", "labels": [], "entities": []}, {"text": "The official results show that our system outperformed all systems in the unconstrained setting on both tasks and was able to achieve a reasonable score in Task A constrained, ranking in the top ten out of 44 submissions.", "labels": [], "entities": []}], "datasetContent": [{"text": "SemEval2018-Task3's organizers proposed two subtasks related to the topic of detecting irony in Twitter automatically system should determine whether a tweet is ironic or not ironic.", "labels": [], "entities": []}, {"text": "Meanwhile, SubTask B is defined as a multi-class classification problem, where the aim is to classify each tweet into four different categories including: verbal irony by polarity contrast, other verbal irony, situational irony, and not irony.", "labels": [], "entities": []}, {"text": "In both tasks, organizers allowed submissions in two scenarios: constrained and unconstrained.", "labels": [], "entities": []}, {"text": "In unconstrained settings, participants were allowed to exploit external data from other corpora annotated with irony labels in the training phase.", "labels": [], "entities": []}, {"text": "Standard evaluation metrics were proposed for the task, including, precision, recall, accuracy, and F 1 -score.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.999675989151001}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9990224838256836}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9990457892417908}, {"text": "F 1 -score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9867839217185974}]}, {"text": "Dataset The organizers provided 3,834 training data and 784 test data for both tasks.", "labels": [], "entities": []}, {"text": "Data were collected by using three irony-related hashtags: #irony, #sarcasm, and #not.", "labels": [], "entities": []}, {"text": "Datasets for both tasks were manually labeled by using the fine-grained annotation scheme in (Van Hee et al., 2016b).", "labels": [], "entities": []}, {"text": "A twolayer annotation has been applied on the same tweets, one concerning the presence and absence of irony, the second one identifying different types of irony, when irony is present.", "labels": [], "entities": []}, {"text": "As a consequence, as shows, there is a class imbalance on SubTask B dataset in favor of nonironic class (50%), verbal irony by polarity contrast (25%), other verbal irony (13%) and situational irony (12%).", "labels": [], "entities": [{"text": "SubTask B dataset", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.7021771868069967}]}, {"text": "The irony-related hashtags were removed from the final dataset release.", "labels": [], "entities": []}, {"text": "We built our supervised systems based on available training data.", "labels": [], "entities": []}, {"text": "In this phase performances  were evaluated based on the mean of F 1 -score, by using 10-fold cross validation.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9828898906707764}]}, {"text": "We chose an SVM classifier with radial basis function kernel 3 . Our system implementation is free available for research purpose in GitHub page . Therefore, we lean on feature selection process to improve the system performance.", "labels": [], "entities": []}, {"text": "We carried on an ablation test on our feature sets to get the highest F 1 -score.", "labels": [], "entities": [{"text": "ablation", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9742059707641602}, {"text": "F 1 -score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9921105951070786}]}, {"text": "We decided to participate in three different scenarios: SubTask A constrained, SubTask A unconstrained, and SubTask B unconstrained.", "labels": [], "entities": []}, {"text": "For unconstrained scenario in SubTask A, we used the available corpora from previous work.", "labels": [], "entities": []}, {"text": "We tried to add new data with balance proportion (1500 ironic and 1500 non-ironic).", "labels": [], "entities": [{"text": "balance proportion", "start_pos": 30, "end_pos": 48, "type": "METRIC", "confidence": 0.955778032541275}]}, {"text": "We also added a balance proportion of ironic data based on different hashtag (500 #irony, 500 #sarcasm, and 500 #not) from three different corpora, with the aim of enriching the training data with ironic samples of various provenance and trying to avoid biases.", "labels": [], "entities": []}, {"text": "The distribution and source of our additional data can be seen in.", "labels": [], "entities": []}, {"text": "In SubTask B, we proposed to use a pipeline approach in three-steps classification scenario.", "labels": [], "entities": []}, {"text": "First, we classify the ironic and non-ironic (similar configuration with SubTask A).", "labels": [], "entities": []}, {"text": "Second, we classify the ironic data from step one into two categories, verbal irony by polarity contrast and the rest (other verbal irony+situational irony).", "labels": [], "entities": []}, {"text": "In the second step, we add more training data on the other verbal irony+situational irony class to overcome the imbalance issue.", "labels": [], "entities": [{"text": "verbal irony+situational irony", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6837477922439575}]}, {"text": "We decided to use only additional tweets marked with #irony hashtags, relying on the analysis in ( suggesting that the polarity reversal phenomenon seems to be relevant in messages marked with #sarcasm and #not, but less relevant for messages tagged with #irony.", "labels": [], "entities": []}, {"text": "In the last step, we classify between other verbal irony and situational irony.", "labels": [], "entities": [{"text": "situational irony", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7017079740762711}]}, {"text": "shows selected features on each submitted system based on our ablation test.", "labels": [], "entities": []}, {"text": "shows our experimental results based on four different metrics including accuracy, precision, recall, and F 1 -score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9995562434196472}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9984347224235535}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9989179372787476}, {"text": "F 1 -score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9880277514457703}]}, {"text": "For experiments on the training set we used 10-fold cross validation, and we report the score for each metric.", "labels": [], "entities": []}, {"text": "However, F 1 -score has been used as the criterion to tune the configuration.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.985932782292366}]}, {"text": "Official Codalab results show that our system ranked 10 th out of 44 submissions on SubTask A and 9 th out of 32 on SubTask B.", "labels": [], "entities": [{"text": "Codalab", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.8816076517105103}]}, {"text": "We obtained F 1-score 0.6216 (Best system: 0.7054) on SubTask A and 0.4131 (Best system: 0.5074) on SubTask B.", "labels": [], "entities": [{"text": "F 1-score 0.6216", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.9544216195742289}]}, {"text": "However, our system outperformed all systems in the unconstrained setting on both tasks.", "labels": [], "entities": []}, {"text": "Based on our analysis, several stylistic features were very effective in Task A (both in constrained and unconstrained settings).", "labels": [], "entities": []}, {"text": "Especially, Twitter specific symbols such as hashtags, mentions, and URLs were very useful to discriminate non ironic tweets.", "labels": [], "entities": []}, {"text": "In addition, we found that affective resource were very helpful in the Step 2 and Step 3 of Task B, especially Emolex (Step 2) and EmoSenticNet (Step 3).", "labels": [], "entities": [{"text": "Emolex", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.8996529579162598}]}, {"text": "Another important finding is that additional data on Task A did not improve the classifier performance.", "labels": [], "entities": []}, {"text": "Instead, additional tweets marked with #irony on Task B were very useful to handle the imbalance dataset in Step-2 (verbal irony by polarity contrast vs other verbal irony+situational irony).", "labels": [], "entities": []}, {"text": "Our clas-  sifier was able to achieve a high F 1 score on the training phase in this case.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9914630055427551}]}, {"text": "Furthermore, we also found that our new features for capturing affective information in emojis (e.g. emoji incongruity) were very helpful in classifying between ironic and not ironic data.", "labels": [], "entities": []}, {"text": "shows the confusion matrix of our classification result on SubTask B.", "labels": [], "entities": [{"text": "confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9798175692558289}]}, {"text": "Our system performed quite well in Step 1 (irony vs non-irony) and Step 2 (verbal irony by polarity contrast vs other verbal irony+situational irony).", "labels": [], "entities": []}, {"text": "However, our system was struggling in distinguishing between other verbal irony and situational irony (Step 3).", "labels": [], "entities": [{"text": "situational irony", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7213669568300247}]}, {"text": "Our system got very low precision in detecting situational irony, and this has a huge impact on macro average F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.997231662273407}, {"text": "detecting situational irony", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7803666790326437}, {"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.7870751619338989}]}, {"text": "The difficulties to find an important feature to discriminate other verbal and situational irony was, indeed, for us the main challenge in Task B.", "labels": [], "entities": []}, {"text": "A qualitative error analysis was conducted.", "labels": [], "entities": []}, {"text": "We found a lot of tweets which where difficult to understand without the context), like: (tw1)\"Produce Mobile Apps http://t.co/3OV57ZhqcH http://t.co/wX1DbI8W9M\" (tw2) \"#Consensus of Absolute Hilarious#MichaelMann to lecture on #Professional #Ethics for #Climate #Scientists?", "labels": [], "entities": []}, {"text": "http://t.co/pD0TEMq1Z0\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset Distribution on Both Tasks.", "labels": [], "entities": [{"text": "Dataset Distribution", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6642068326473236}]}, {"text": " Table 4: Results on Training and Test sets.", "labels": [], "entities": []}, {"text": " Table 5: Confusion Matrix SubTask B.", "labels": [], "entities": []}]}