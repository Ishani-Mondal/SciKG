{"title": [{"text": "Zewen at SemEval-2018 Task 1: An Ensemble Model for Affect Prediction in Tweets", "labels": [], "entities": [{"text": "Affect Prediction in Tweets", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7749561220407486}]}], "abstractContent": [{"text": "This paper presents a method for Affect in Tweets, which is the task to automatically determine the intensity of emotions and intensity of sentiment of tweets.", "labels": [], "entities": [{"text": "Affect in Tweets", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8081392248471578}]}, {"text": "The term affect refers to emotion-related categories such as anger, fear, etc.", "labels": [], "entities": []}, {"text": "Intensity of emotions need to be quantified into areal valued score in [0, 1].", "labels": [], "entities": [{"text": "areal valued score", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.8908084233601888}]}, {"text": "We propose an ensemble system including four different deep learning methods which are CNN, Bidirectional LSTM (BLSTM), LSTM-CNN and a CNN-based Attention model (CA).", "labels": [], "entities": [{"text": "Bidirectional LSTM (BLSTM)", "start_pos": 92, "end_pos": 118, "type": "METRIC", "confidence": 0.9110715985298157}]}, {"text": "Our system gets an average Pearson correlation score of 0.682 in the subtask EI-reg and an average Pearson correlation score of 0.784 in subtask V-reg, which ranks 19th among 48 systems in EI-reg and 17th among 38 systems in V-reg.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.9313265681266785}, {"text": "EI-reg", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9254089593887329}, {"text": "Pearson correlation score", "start_pos": 99, "end_pos": 124, "type": "METRIC", "confidence": 0.9626226226488749}]}], "introductionContent": [{"text": "Affect determination is a significant part of nature language processing.", "labels": [], "entities": [{"text": "Affect determination", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8406527936458588}, {"text": "nature language processing", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6710411508878072}]}, {"text": "Especially, affect in tweets becomes a focus in recent years.", "labels": [], "entities": []}, {"text": "Sentiment Analysis in Twitter, which is a task of SemEval, was firstly proposed in 2013 and not replaced until 2018.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8808396607637405}]}, {"text": "In SemEval 2018, the task Affect in Tweets (AIT) () was proposed and the objective is to automatically determine the intensity of emotions (E) and intensity of sentiment (aka valence V) of tweets.", "labels": [], "entities": [{"text": "intensity of sentiment (aka valence V)", "start_pos": 147, "end_pos": 185, "type": "METRIC", "confidence": 0.6566077098250389}]}, {"text": "In this paper, we focus on two subtasks: \u2022 EI-reg (emotion intensity regression) -Given a tweet and an emotion E, determine the intensity of E that best represents the mental state of the tweeter -a real-valued score between 0 (least E) and 1 (most E) \u2022 V-reg (sentiment intensity regression) -Given a tweet, determine the intensity of sentiment or valence (V) that best represents the mental state of the tweeter -a real-valued score between 0 (most negative) and 1 (most positive) Before 2016, most systems use Support Vector Machine (SVM), Naive Bayes, maximum entropy and linear regression (.", "labels": [], "entities": []}, {"text": "In SemEval 2014, deep learning methods started to appear and a team using them won the second place.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.6660383939743042}]}, {"text": "Since 2015, more and more teams who were rank at the top used deep learning methods and now deep learning methods including CNN and LSTM networks become really popular (.", "labels": [], "entities": [{"text": "CNN", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.9048352241516113}]}, {"text": "The system described in this paper is an ensemble of four different DNN methods including CNN, Bidirectional LSTM (Bi-LSTM), LSTM-CNN and a CNN-based Attention model (CA).", "labels": [], "entities": []}, {"text": "In these methods, words in tweets are firstly mapped to word vectors.", "labels": [], "entities": []}, {"text": "After intensity scores are calculated by these models, we use a logistic regression and finally give the scores.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the four various methods and the ensemble method used in our system.", "labels": [], "entities": []}, {"text": "Section 3 and Section 4 give the implementation and training details of our system for subtask EI-reg and V-reg.", "labels": [], "entities": []}, {"text": "Section 5 states the results and discussion in the evaluation period.", "labels": [], "entities": []}, {"text": "Finally, Section 6 makes a conclusion on this work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Fully connected layers hyper-parameters, the  numbers represent the size of outputs of liner layers.", "labels": [], "entities": []}, {"text": " Table 2: Network hyper-parameters for the filters of  CNN and hidden size of LSTM, and p is the dropout  rate. For example, [2, 3, 4], 256 means the filter height  is set to 2, 3 and 4, and the number of filters is set to  256 for different sizes of filters.", "labels": [], "entities": []}, {"text": " Table 1 and Table  2. Also, the four methods use the same word em- beddings, which is a pre-trained 300-dimensional  word vectors with common crawl by GloVe algo- rithm. For different emotions, we train the mod- els for 10 epochs respectively. The network pa- rameters are learned by minimizing the Mean Ab- solute Error (MAE) between the gold labels and  predictions and the four methods used in our sys- tem are trained separately. We optimize the loss  function by back-propagating algorithm via Mini- batch Gradient descent with batch size of 8 for the  4 deep learning models and full batch learning for  the ensemble model, as well as the Adam opti-", "labels": [], "entities": [{"text": "Mean Ab- solute Error (MAE)", "start_pos": 300, "end_pos": 327, "type": "METRIC", "confidence": 0.9321247339248657}]}, {"text": " Table 3: Results on Subtask EI-reg and V-reg.", "labels": [], "entities": [{"text": "V-reg", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.7270166277885437}]}, {"text": " Table 4:  Results of different text preprocessing  method on V-reg task when the other parameters are  kept unchanged.", "labels": [], "entities": []}]}