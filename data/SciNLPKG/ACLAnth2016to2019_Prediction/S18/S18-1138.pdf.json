{"title": [{"text": "NTNU at SemEval-2018 Task 7: Classifier Ensembling for Semantic Relation Identification and Classification in Scientific Papers", "labels": [], "entities": [{"text": "NTNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9157370328903198}, {"text": "Semantic Relation Identification and Classification", "start_pos": 55, "end_pos": 106, "type": "TASK", "confidence": 0.831764030456543}]}], "abstractContent": [{"text": "The paper presents NTNU's contribution to SemEval-2018 Task 7 on relation identification and classification.", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8709417184193929}, {"text": "relation identification and classification", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.8249601274728775}]}, {"text": "The class weights and parameters of five alternative supervised clas-sifiers were optimized through grid search and cross-validation.", "labels": [], "entities": []}, {"text": "The outputs of the classi-fiers were combined through voting for the final prediction.", "labels": [], "entities": []}, {"text": "A wide variety of features were explored, with the most informative identified by feature selection.", "labels": [], "entities": []}, {"text": "The best setting achieved F 1 scores of 47.4% and 66.0% in the relation classification subtasks 1.1 and 1.2.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9584423303604126}]}, {"text": "For relation identification and classification in subtask 2, it achieved F 1 scores of 33.9% and 17.0%,", "labels": [], "entities": [{"text": "relation identification and classification", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.7975804954767227}, {"text": "F 1 scores", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9844746788342794}]}], "introductionContent": [{"text": "Scientific papers are valuable knowledge sources providing authentic insights about certain aspects of the research domains.", "labels": [], "entities": []}, {"text": "With the advancement of scientific research, a massive growth of published articles are observed.", "labels": [], "entities": []}, {"text": "As per the American Journal Experts (AJE) scholarly publishing report 1 , approximately 2.2 million articles were added to the literature in 2016 only.", "labels": [], "entities": [{"text": "American Journal Experts (AJE) scholarly publishing report 1", "start_pos": 11, "end_pos": 71, "type": "DATASET", "confidence": 0.8834014058113098}]}, {"text": "The sheer volume of the ever increasing literature of any scientific discipline makes it hard for human capability and expertise to quickly process and identify information of interest.", "labels": [], "entities": []}, {"text": "Therefore, there is a need to efficiently exploit automatic means of accessing this reliable unstructured knowledge repository.", "labels": [], "entities": []}, {"text": "Semantic relation extraction is one of the main information extraction tasks, and aims to identify a pair of arguments connected by certain predefined relation types based on a target application.", "labels": [], "entities": [{"text": "Semantic relation extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7507782578468323}, {"text": "information extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7628141939640045}]}, {"text": "The relation arguments are of different types such as Named Entities (, nominals (, general keyphrases, quantitative variables) or events ( , and are syntactically represented by noun phrases, clauses or larger complex structures.", "labels": [], "entities": []}, {"text": "A semantic relation maybe either symmetric (undirected) or asymmetric (hierarchical).", "labels": [], "entities": []}, {"text": "Supervised machine learning approaches have been successfully used for identifying semantic relations encoded in texts.", "labels": [], "entities": [{"text": "identifying semantic relations encoded in texts", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.8483289480209351}]}, {"text": "Broadly, three types of supervised approaches to relation extraction have been investigated: feature-based (, kernel-based (, and neural network based (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.9434715509414673}]}, {"text": "In this work, various relation identification and classification subtasks of were addressed using featurebased approaches.", "labels": [], "entities": [{"text": "relation identification and classification subtasks", "start_pos": 22, "end_pos": 73, "type": "TASK", "confidence": 0.8185598015785217}]}, {"text": "A wide variety of features was explored, including lexical (e.g., bag-ofwords, lemmata, n-grams), syntactic (e.g., part-ofspeech, parsing information), semantic (e.g., dependency information, WordNet), and other binary indicators.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 192, "end_pos": 199, "type": "DATASET", "confidence": 0.9316720366477966}]}, {"text": "A \u03c7 2 -based feature selection technique was used to identify informative features.", "labels": [], "entities": []}, {"text": "The class weights and parameters of five different classifiers-Support Vector Machines (SVM), Decision Trees (DT), Random Forests (RF), Multinomial Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (MNB), and k-Nearest Neighbor (kNN)-were optimized for each subtask through grid search and k-fold cross-validation.", "labels": [], "entities": []}, {"text": "These classifiers were chosen as they are effective in identifying and classifying semantic relations in feature-based classification scenario ).", "labels": [], "entities": []}, {"text": "The trained classifiers were ensembled using majority class labels (hard voting) for the final predictions.", "labels": [], "entities": []}, {"text": "All classifier, feature selection and classifier ensembling modules used were implemented in the scikit-learn The tasks and the datasets are described in Section 2, while Section 3 outlines the experimental setup, system architecture and parameter optimisation.", "labels": [], "entities": []}, {"text": "Section 4 discusses the results of the final evaluation of SemEval 2018 Task 7, where the system achieved 47.4% and 66.0% F 1 scores on the relation classification subtasks 1.1 and 1.2.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8423606902360916}, {"text": "F 1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9925119280815125}]}, {"text": "In subtask 2, the system reached 33.9% and 17.0% F 1 scores for relation identification and relation classification, respectively.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9846978386243185}, {"text": "relation identification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.9402418434619904}, {"text": "relation classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.9252375960350037}]}, {"text": "These results are eloborated on in Section 5, before Section 6 concludes and points to future research.", "labels": [], "entities": []}, {"text": "There are six relation types, among which USAGE, RESULT, MODEL-FEATURE, PART_WHOLE, and TOPIC are asymmetric, while COMPARE is the only symmetric relation.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.943450927734375}, {"text": "RESULT", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9826807379722595}, {"text": "MODEL-FEATURE", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.8816086649894714}, {"text": "PART_WHOLE", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.8188250263532003}, {"text": "TOPIC", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.8512771725654602}]}, {"text": "All the relations are intrasentential and there are no referring expressions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training dataset consisted of two subsets: shows the processing pipeline common to both relation identification and classification.", "labels": [], "entities": [{"text": "relation identification and classification", "start_pos": 92, "end_pos": 134, "type": "TASK", "confidence": 0.7472117319703102}]}, {"text": "The processing steps are elaborated on below.", "labels": [], "entities": []}, {"text": "Inputs to brat annotation: The input training and test files are in xml format with the entity mentions marked.", "labels": [], "entities": []}, {"text": "Each entity mention has an ID with two parts, abstract ID and entity number.", "labels": [], "entities": []}, {"text": "For example, the entity ID H91-1045.18 denotes abstract ID H91-1045 and entity number 18.", "labels": [], "entities": []}, {"text": "The relation labels are in a separate file with the format TOPIC(A92-1023.7,A92-1023.8,REVERSE), where the first two arguments of the relation type are entity IDs and the last is the directionality of the relation.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.8829943537712097}, {"text": "A92-1023.7,A92-1023.8,REVERSE", "start_pos": 65, "end_pos": 94, "type": "DATASET", "confidence": 0.8443329334259033}]}, {"text": "The xml and relation label files were converted into 'brat' () format, with the text content of each abstract ID kept in a text file, and entity and relation information kept in an annotation file.", "labels": [], "entities": []}, {"text": "Conversion to brat format helps to visualize and study the annotations of the training and test set output.", "labels": [], "entities": []}, {"text": "Also, the text content (without entity tags) is used for preprocessing.", "labels": [], "entities": []}, {"text": "Text Processing: The text content of each abstract is analyzed with the Stanford CoreNLP toolkit () for sentence boundary detection, tokenization, lemmatization, partof-speech (POS) tagging, and constituent and dependency parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.8692627350489298}, {"text": "sentence boundary detection", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.6594570080439249}, {"text": "tokenization", "start_pos": 133, "end_pos": 145, "type": "TASK", "confidence": 0.9624167084693909}, {"text": "partof-speech (POS) tagging", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.6068763613700867}, {"text": "constituent and dependency parsing", "start_pos": 195, "end_pos": 229, "type": "TASK", "confidence": 0.5915321111679077}]}, {"text": "Character offset-based brat entity annotations are mapped into word level indices using the tokens' character offsets.", "labels": [], "entities": []}, {"text": "Finally, the dependency heads of entity mentions, in between context and the text window representing the relation expression are identified.", "labels": [], "entities": []}, {"text": "Feature Extraction: Given a sentence with more than one entity mention, all possible entity pairs are considered in left to right order.", "labels": [], "entities": []}, {"text": "For each entity pair, the text span containing the entities and their middle context is considered as the representation of the relation instance.", "labels": [], "entities": []}, {"text": "As word features, unigrams and bigrams of the context and entity mentions (excluding articles, adjectives, cardinals, ordinals, pronouns, brackets and punctuations) are considered.", "labels": [], "entities": []}, {"text": "Corresponding to word features, POS, word+POS, and lemma+POS combinations are included, as well as word and POS of entity dependency heads, context dependency heads, and their combinations.", "labels": [], "entities": []}, {"text": "As the shortest dependency path between the entity pair contains major information for relation identification (), dependency path features are added for the distance from left entity head to right entity head, words belonging to the dependency path and their relations to the parent node.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7614772617816925}]}, {"text": "WordNet synonyms and hyponyms of dependency head of entities and contexts are included.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8976380228996277}]}, {"text": "Also, other binary indicators such as adjacent or overlapping entities are included.", "labels": [], "entities": []}, {"text": "Parameters Optimization through CrossValidation (CV): As there was no development data available for model parameter tuning, 20% of the training data was kept as development data, and the remaining training data was used for parameters optimization with 5-fold crossvalidation.", "labels": [], "entities": [{"text": "model parameter tuning", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.6403687198956808}, {"text": "parameters optimization", "start_pos": 225, "end_pos": 248, "type": "TASK", "confidence": 0.7592166066169739}]}, {"text": "For relation labeling in subtask 1.1 and 1.2, the relation type is predicted against 11 classes (five directed and one undirected relation).", "labels": [], "entities": [{"text": "relation labeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.762813925743103}]}, {"text": "Relation instance identification in subtask 2 is a binary classification problem, and the class weights of positive instances are optimized through CV.", "labels": [], "entities": [{"text": "Relation instance identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8515193263689677}]}, {"text": "In the final system, the parameters are optimized on the entire training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relation type statistics in datasets D 1 and D 2", "labels": [], "entities": [{"text": "Relation type", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8708435893058777}]}, {"text": " Table 2: Precision, recall and F-scores of individual  and ensemble classifiers on subtask 1.1. The scores  are micro-averaged over 11 classes. Ensemble-best is  SVM+DT+MNB.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9987081289291382}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9971721768379211}, {"text": "F-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9919493198394775}, {"text": "SVM+DT+MNB", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.4982107639312744}]}, {"text": " Table 3: Result of individual and ensemble classifiers  on subtask 1.2. Scores are micro-averaged over 11  classes. Ensemble-best is SVM+RF+MNB.", "labels": [], "entities": [{"text": "SVM+RF+MNB", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.5099816858768463}]}, {"text": " Table 4: Performance of positive class in relation iden- tification on clean data (subtask 2). Ensemble-best is  SVM+DT+MNB.", "labels": [], "entities": []}]}