{"title": [{"text": "Deep Affix Features Improve Neural Named Entity Recognizers", "labels": [], "entities": [{"text": "Improve Neural Named Entity Recognizers", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.7599112033843994}]}], "abstractContent": [{"text": "We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8139223456382751}]}, {"text": "We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features.", "labels": [], "entities": [{"text": "CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets", "start_pos": 118, "end_pos": 181, "type": "DATASET", "confidence": 0.9320191090757196}]}, {"text": "Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3% from state of the art).", "labels": [], "entities": [{"text": "SemEval 2013 task 9.1 DrugNER", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.5992225527763366}, {"text": "MedLine dataset", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.9699751734733582}]}, {"text": "We also establish anew benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score.", "labels": [], "entities": [{"text": "I2B2 2010 Clinical NER dataset", "start_pos": 40, "end_pos": 70, "type": "DATASET", "confidence": 0.8730818748474121}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9795970320701599}]}], "introductionContent": [{"text": "Named entity recognition (NER), or identifying the specific named entities (eg. person, location, organization etc) in a text, is a precursor to other information extraction tasks such as event extraction.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7917875796556473}, {"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.7412121295928955}, {"text": "event extraction", "start_pos": 188, "end_pos": 204, "type": "TASK", "confidence": 0.6856818646192551}]}, {"text": "The oldest and perhaps most common approach to NER is based on dictionary lookups, and indeed, when the resources are available, this is very useful (e.g.,).", "labels": [], "entities": [{"text": "NER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9703480005264282}]}, {"text": "However, hand-crafting these lexicons is time-consuming and expensive and so these resources are often either unavailable or sparse for many domains and languages.", "labels": [], "entities": []}, {"text": "Neural network (NN) approaches to NER, on the other hand, do not necessitate these resources, and additionally do not require complex feature engineering, which can also be very costly and may not port well from domain to domain and language to language.", "labels": [], "entities": []}, {"text": "Commonly, these NN architectures for NER include a learned representation of individual words as well as an encoding of the word's characters.", "labels": [], "entities": []}, {"text": "However, neither of these representations makes explicit use of the semantics of sub-word units, i.e., morphemes.", "labels": [], "entities": []}, {"text": "Here we propose a simple neural network architecture that learns a custom representation for affixes, allowing fora richer semantic representation of words and allowing the model to better approximate the meaning of words not seen during training . While a full morphological analysis might bring further benefits, to ease re-implementation we take advantage of the Zipfian distribution of language and focus hereon a simple approximation of morphemes as high-frequency prefixes and suffixes.", "labels": [], "entities": []}, {"text": "Our approach thus requires no language-specific affix lexicon or morphological tools.", "labels": [], "entities": []}, {"text": "Our contributions are: 1.", "labels": [], "entities": []}, {"text": "We propose a simple yet robust extension of current neural NER approaches that allows us to learn a representation for prefixes and suffixes of words.", "labels": [], "entities": []}, {"text": "We employ an inexpensive and language-independent method to approximate affixes of a given language using n-gram frequencies.", "labels": [], "entities": []}, {"text": "This extension is able to be applied directly to new languages and domains without any additional resource requirements and it allows fora more compositional, and hence richer, representation of words.", "labels": [], "entities": []}, {"text": "2. We demonstrate the utility of including a dedicated representation for affixes.", "labels": [], "entities": []}, {"text": "Our model shows as much as a 2.3% F1 improvement over an recurrent neural network model with only words and characters, demonstrating that what our model learns about affixes is complementary to a recurrent layer over characters.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9996663331985474}]}, {"text": "We find filtering to high-frequency affixes is essential, as simply using all word-boundary character trigrams degrades performance in some cases.", "labels": [], "entities": []}, {"text": "3. We establish anew state-of-the-art for Spanish, Dutch, and German NER, and MedLine drug NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.6203988194465637}]}, {"text": "Additionally, we achieve near state-of-the-art performance in English NER and DrugBank drug NER, despite using no external dictionaries.", "labels": [], "entities": [{"text": "English NER", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.7914975881576538}, {"text": "DrugBank drug NER", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9219677448272705}]}], "datasetContent": [{"text": "We evaluate our model across multiple languages and domains.", "labels": [], "entities": []}, {"text": "To evaluate on the CoNLL 2002 and 2003 test sets, we trained our model on the combined training + validation data with the general hyper-parameter set from Section 3.2.", "labels": [], "entities": [{"text": "CoNLL 2002 and 2003 test sets", "start_pos": 19, "end_pos": 48, "type": "DATASET", "confidence": 0.9558454751968384}]}, {"text": "Since on the validation data, the majority of our models terminated their training between 100 and 150 epochs, we report two models trained on the combined training + validation data: one after 100 epochs, and one after 150 epochs.", "labels": [], "entities": []}, {"text": "We evaluated our model with all the languages in: DrugNER results with official evaluation script on test dataset consisting of MedLine (ML) (80.10% of the total test data) and DrugBank (DB) test data (19.90 % of the total test data).", "labels": [], "entities": [{"text": "DrugBank (DB) test data", "start_pos": 177, "end_pos": 200, "type": "DATASET", "confidence": 0.8919498920440674}]}, {"text": "We report precision (P), recall (R), and F1-score.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9651360362768173}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9701272696256638}, {"text": "F1-score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9991284012794495}]}], "tableCaptions": [{"text": " Table 1: Performance of our model (with and without affixes), using general set of hyper-parameters and previous  work on four datasets: CoNLL 2002 Spanish (ES), CoNLL 2002 Dutch (NL), CoNLL 2003 English (EN), and  CoNLL 2003 German (DE). Dict indicates whether or not the approach makes use of dictionary lookups.", "labels": [], "entities": [{"text": "CoNLL 2003 German (DE)", "start_pos": 216, "end_pos": 238, "type": "DATASET", "confidence": 0.8524787127971649}, {"text": "Dict", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.9839382767677307}]}, {"text": " Table 2: DrugNER results with official evaluation script on test dataset consisting of MedLine (ML) (80.10% of  the total test data) and DrugBank (DB) test data (19.90 % of the total test data). We report precision (P), recall (R),  and F1-score.", "labels": [], "entities": [{"text": "DrugBank (DB) test data", "start_pos": 138, "end_pos": 161, "type": "DATASET", "confidence": 0.8084863026936849}, {"text": "precision (P)", "start_pos": 206, "end_pos": 219, "type": "METRIC", "confidence": 0.9636878967285156}, {"text": "recall (R)", "start_pos": 221, "end_pos": 231, "type": "METRIC", "confidence": 0.969917044043541}, {"text": "F1-score", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9995617270469666}]}, {"text": " Table 3: DrugNER results on test data using CoNLL evaluation script. ML indicates the results for MedLine test  data and DB indicates results for DrugBank test data. We have reported F1 scores for each entity type in MedLine,  DrugBank and overall dataset (Both). The last column (Both) provides performance on the the combined dataset.", "labels": [], "entities": [{"text": "CoNLL evaluation script", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.8750563859939575}, {"text": "MedLine test  data", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.9399653077125549}, {"text": "DB", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9875834584236145}, {"text": "DrugBank test data", "start_pos": 147, "end_pos": 165, "type": "DATASET", "confidence": 0.9544183015823364}, {"text": "F1", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9941921830177307}, {"text": "DrugBank", "start_pos": 228, "end_pos": 236, "type": "DATASET", "confidence": 0.9485479593276978}]}, {"text": " Table 4: Performance on I2B2 2010 NER (Uzuner et al., 2011) test data 5 using CoNLL evaluation script. We  have reported precision (P), recall (R), and F1-score.", "labels": [], "entities": [{"text": "I2B2 2010 NER (Uzuner et al., 2011) test data", "start_pos": 25, "end_pos": 70, "type": "DATASET", "confidence": 0.8121604671080908}, {"text": "CoNLL evaluation script", "start_pos": 79, "end_pos": 102, "type": "DATASET", "confidence": 0.8302839597066244}, {"text": "precision (P)", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.9614003747701645}, {"text": "recall (R)", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9626750648021698}, {"text": "F1-score", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9997512698173523}]}]}