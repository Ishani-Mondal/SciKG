{"title": [{"text": "WLV at SemEval-2018 Task 3: Dissecting Tweets in Search of Irony", "labels": [], "entities": [{"text": "WLV at SemEval-2018 Task 3", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8994112968444824}]}], "abstractContent": [{"text": "This paper describes the systems submitted to SemEval 2018 Task 3 \"Irony detection in En-glish tweets\" for both subtasks A and B.", "labels": [], "entities": [{"text": "SemEval 2018 Task 3", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8335908800363541}, {"text": "Irony detection", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8379756212234497}]}, {"text": "The first system leveraging a combination of sentiment , distributional semantic, and text surface features is ranked third among 44 teams according to the official leaderboard of the sub-task A.", "labels": [], "entities": []}, {"text": "The second system with slightly different representation of the features ranked ninth in subtask B.", "labels": [], "entities": []}, {"text": "We present a method that entails decomposing tweets into separate parts.", "labels": [], "entities": []}, {"text": "Searching for contrast within the constituents of a tweet is an integral part of our system.", "labels": [], "entities": []}, {"text": "We embrace an extensive definition of contrast which leads to avast coverage in detecting ironic content.", "labels": [], "entities": []}], "introductionContent": [{"text": "In figurative language (also known as trope), there is a departure from literal use of words.", "labels": [], "entities": []}, {"text": "In order to decode meaning, therefore, it is not enough to rely solely on the literal sense of individual words.", "labels": [], "entities": []}, {"text": "Irony and sarcasm are two types of such language that exploit this technique in similar ways.", "labels": [], "entities": []}, {"text": "They \"both involve deliberately saying something that is incongruous or the opposite of what the speaker knows to be true\".", "labels": [], "entities": []}, {"text": "This is sometimes formulated as a transgression of the Gricean maxim of quality . Under this assumption it follows that the violation is only permissible thanks to shared knowledge between the speaker and the hearer.", "labels": [], "entities": []}, {"text": "In order to achieve this goal, the speaker frames the message with some form of commentary or metamessage that signals the ironic or sarcastic nature of the message.", "labels": [], "entities": []}, {"text": "This is usually realised through negation of the original meaning).", "labels": [], "entities": []}, {"text": "Regardless of their similarities, irony and sarcasm are not technically the same as they might be 1 \"Do not say what you believe to be false.\" employed for different purposes.", "labels": [], "entities": []}, {"text": "It is widely accepted that sarcasm involves some degree of verbal aggression and ridicule directed at the hearer, whilst irony can simply be used for humorous or emphatic effect.", "labels": [], "entities": []}, {"text": "It has been shown that computational processing of irony and sarcasm requires some knowledge of the context in which they appear, sometimes including paralinguistic information (.", "labels": [], "entities": [{"text": "computational processing of irony and sarcasm", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.819545199473699}]}, {"text": "Exploring ironicity has practical implications, since performance of sentiment analysis systems is directly affected by knowledge about irony and sarcasm.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8459277451038361}]}, {"text": "As part of the 12th workshop on semantic evaluation (SemEval-2018), Shared Task 3 defines two subtasks with regards to irony detection in English tweets.", "labels": [], "entities": [{"text": "semantic evaluation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7387782633304596}, {"text": "irony detection in English tweets", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.8216182589530945}]}, {"text": "Subtask A involves binary classification.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8507470488548279}, {"text": "binary classification", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7055352032184601}]}, {"text": "The objective is to train a system that can label tweets as ironic or not.", "labels": [], "entities": []}, {"text": "Subtask B is a multi-class classification problem with the objective to label tweets with one of the four specified labels describing the type of irony (verbal irony by means of a polarity contrast, situational irony, other verbal irony, and non-ironic).", "labels": [], "entities": []}, {"text": "To tackle these problems, in this paper we describe two rich feature-based systems addressing each subtask.", "labels": [], "entities": []}, {"text": "Our systems use a combination of sentiment, distributional semantic, and text surface features.", "labels": [], "entities": []}, {"text": "The code and data for this project is freely available 2 . The rest of this paper is organised as follows: Section 2 describes related work.", "labels": [], "entities": []}, {"text": "Section 3 provides a comprehensive description of the overall methodology including pre-processing, feature representation, and system architecture.", "labels": [], "entities": [{"text": "feature representation", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7031477093696594}]}, {"text": "Sections 4 and 5 discuss experiments and results, Section 6 involves error analysis and finally Section 7 concludes the paper with some closing remarks.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8348214328289032}]}], "datasetContent": [{"text": "We use the data (text including emojis) as provided by the organisers of the shared task.", "labels": [], "entities": []}, {"text": "We train our models on the training set using 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "Predictions were made on the held-out test data.", "labels": [], "entities": []}, {"text": "The most informative features are selected using recursive feature elimination (RFE) ().", "labels": [], "entities": [{"text": "recursive feature elimination (RFE)", "start_pos": 49, "end_pos": 84, "type": "METRIC", "confidence": 0.6270493219296137}]}, {"text": "As a result, the algorithm uses 13 features for subtask A as listed in figure 1.", "labels": [], "entities": []}, {"text": "They are concatenated with the vectors that were derived by separately averaging the words and emoji vectors of the left and right parts of tweets.", "labels": [], "entities": []}, {"text": "The best features derived from RFE for subtask B did not improve the performance of the model.", "labels": [], "entities": []}, {"text": "Therefore we use all of the 87 features which are consequently augmented with the concatenation of the word and and emoji vectors of tweets.", "labels": [], "entities": []}, {"text": "The baseline system provided by task organisers is an SVM classifier which uses tf-idf feature vectors.", "labels": [], "entities": []}, {"text": "We consider this as the benchmark and report the results for 2 different settings of our system as follows: \u2022 setting 1: average of word and emoji vectors of bi-sectioned tweets \u2022 setting 2: concatenation of word and emoji vectors In both settings we combine vectors with best features and feed them to the classifiers.", "labels": [], "entities": []}, {"text": "To achieve the best system for subtask A (best system A) we apply a voting classifier with soft voting between LR and SVM whose model components are based on setting 1 plus the 13 best features that were selected using RFE for subtask A.", "labels": [], "entities": []}, {"text": "The best system for subtask B is a voting classifier between 3 LRs with 3 different class weights as shown in.", "labels": [], "entities": []}, {"text": "The components of the models are based on setting 2 plus all features for subtask B. details the results for subtask A, and the results for subtask B are presented in.", "labels": [], "entities": []}, {"text": "After cross-validation on the TRAIN set, the best system which is an ensemble voting classifier trained on models based on setting 1 + best features of subtask A achieves the highest record in F1-score and recall, but is outperformed inaccuracy by its own component model.", "labels": [], "entities": [{"text": "TRAIN set", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.7229205518960953}, {"text": "F1-score", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9982869029045105}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9960202574729919}]}, {"text": "In terms of precision it also scores lower than the system based on setting 2 + all features.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9996610879898071}]}], "tableCaptions": [{"text": " Table 2: Statistics of the data for subtask B", "labels": [], "entities": []}, {"text": " Table 4: Results for subtask A", "labels": [], "entities": []}, {"text": " Table 5: Results for subtask B", "labels": [], "entities": []}, {"text": " Table 6: Per-class F1-scores for the best system in subtask B", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.5412498116493225}]}]}