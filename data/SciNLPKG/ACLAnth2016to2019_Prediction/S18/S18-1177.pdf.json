{"title": [{"text": "YNU-HPCC at SemEval-2018 Task 11: Using an Attention-based CNN-LSTM for Machine Comprehension Using Commonsense Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "This shared task is atypical question answering (QA) task.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8286098192135493}]}, {"text": "Specially, this task must give the answer to the question based on the text provided.", "labels": [], "entities": []}, {"text": "The essence of the problem is actually reading comprehension.", "labels": [], "entities": []}, {"text": "For each question , there are two candidate answers, and only one of them is correct.", "labels": [], "entities": []}, {"text": "Existing method for this task is to use convolutional neural network (CNN) and recurrent neural network (RNN) or their improved models, such as long short-term memory (LSTM).", "labels": [], "entities": []}, {"text": "In this paper, an attention-based CNN-LSTM model is proposed for this task.", "labels": [], "entities": []}, {"text": "By adding an attention mechanism and combining the two models, the experimental results have been significantly improved.", "labels": [], "entities": []}, {"text": "The accuracy of our final submission is 0.7143.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997842907905579}]}], "introductionContent": [{"text": "Question answering has long been an important research topic in the field of natural language processing.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9353692829608917}, {"text": "natural language processing", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6566948393980662}]}, {"text": "Prior to this, there have been many similar tasks, and many scholars have made very significant contributions to the research in this field.", "labels": [], "entities": []}, {"text": "Such as the Allen AI Science Challenge on the Kaggle () and the IJCNLP-2017 shared task 5: Multi-choice Question Answering in Exams (.", "labels": [], "entities": [{"text": "Allen AI Science Challenge on the Kaggle", "start_pos": 12, "end_pos": 52, "type": "DATASET", "confidence": 0.695884610925402}, {"text": "IJCNLP-2017 shared task 5", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.7803793549537659}, {"text": "Multi-choice Question Answering in Exams", "start_pos": 91, "end_pos": 131, "type": "TASK", "confidence": 0.688902348279953}]}, {"text": "Machine comprehension using commonsense knowledge is required to answer multiple-choice questions based on narrative texts about daily activities of human beings.", "labels": [], "entities": []}, {"text": "The answer to many questions does not appear directly in the text, but requires simple reasoning to achieve.", "labels": [], "entities": []}, {"text": "In terms of the nature of the problem, this task can be considered as a binary classification.", "labels": [], "entities": []}, {"text": "That is, for each question, the candidate answers are divided into two categories: the correct answers and the wrong answers.", "labels": [], "entities": []}, {"text": "In recent years, many achievements have been made in machine comprehension-based question answering.", "labels": [], "entities": [{"text": "machine comprehension-based question answering", "start_pos": 53, "end_pos": 99, "type": "TASK", "confidence": 0.6103394702076912}]}, {"text": "Among the existing methods, the main differences are in the data processing and the application of the model.", "labels": [], "entities": []}, {"text": "A dataset for multi-choice question answering was released by.", "labels": [], "entities": [{"text": "multi-choice question answering", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.634754995505015}]}, {"text": "Clark (2015) described how to obtain more information from the background knowledge base by introducing the use of background knowledge to build the best scene.", "labels": [], "entities": []}, {"text": "A large cloze-style dataset using CNN and Daily Mail news articles was created by.", "labels": [], "entities": [{"text": "CNN and Daily Mail news articles", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.8071257521708807}]}, {"text": "Unlike previous datasets, released a machine comprehension-based dataset (SQuAD dataset).", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.7182344794273376}]}, {"text": "It contains over 1M text-question-answer triples crawled from 536 Wikipedia articles, and the questions and answers are structured primarily through crowdsourcing.", "labels": [], "entities": []}, {"text": "It also requires people to submit up to five article-based questions and provide the correct answer that has appeared in the original text.", "labels": [], "entities": []}, {"text": "For the open-domain QA dataset, it is even more challenging to get answers because it requires simple word matching and some simple reasoning.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.7014784961938858}, {"text": "word matching", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.6978709995746613}]}, {"text": "In SearchQA (, the question-answer pairs are crawled from the Jeopardy archives and are augmented with text snippets retrieved from Google search.", "labels": [], "entities": [{"text": "Jeopardy archives", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9392431974411011}]}, {"text": "proposed an end-to-end, problem-based, multi-factor attention network that addresses the task of answering document-based questions.", "labels": [], "entities": []}, {"text": "This model can collect scattered evidence from multiple sentences for the generation of answers.", "labels": [], "entities": []}, {"text": "In this paper, we mainly propose to use an attention-based CNN-LSTM model for this task.", "labels": [], "entities": []}, {"text": "The word-embedding model we choose is Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9605898261070251}]}, {"text": "Then, the word vectors are fed into the convolutional neural network (CNN) layer.", "labels": [], "entities": []}, {"text": "After that, the results of the CNN layer are fed into the long short-term memory (LSTM) layer.", "labels": [], "entities": []}, {"text": "Finally, an attention mechanism is added into the neu-ral networks, and the prediction results are output via the softmax activation.", "labels": [], "entities": []}, {"text": "All the data is processed into (text-question-answer) form.", "labels": [], "entities": []}, {"text": "For each candidate answer, the system will give a correct probability (probability of a correct answer) and a wrong probability (probability of a wrong answer), and the sum of these probabilities is 1.", "labels": [], "entities": []}, {"text": "The answer with the larger correct probability of those two candidate answers will be selected by the system as the correct answer.", "labels": [], "entities": []}, {"text": "Furthermore, in order to exclude the experimental error caused by chance, nine such models are assembled together for training.", "labels": [], "entities": []}, {"text": "The answers are obtained by hard voting.", "labels": [], "entities": []}, {"text": "At the same time, we also selected a number of other models (such as the Bi-LSTM, the attentionbased Bi-LSTM and the attention-based LSTM) for comparative experiments.", "labels": [], "entities": []}, {"text": "The experimental results show that attention-based CNN-LSTM can achieve better results when using Word2Vec as the word embedding technique.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9465776681900024}]}, {"text": "The rest of our paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the CNN, LSTM and attention-based CNN-LSTM.", "labels": [], "entities": [{"text": "CNN", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8791106343269348}]}, {"text": "Experiments and evaluation will be described in Section 3.", "labels": [], "entities": []}, {"text": "The conclusions are drawn in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset provided by the organizer mainly include three parts: texts, questions, and answers.", "labels": [], "entities": []}, {"text": "In the data pre-processing phase, texts and questions-answers pairs are divided into two separate files.", "labels": [], "entities": []}, {"text": "The content of each piece of text data mainly includes the text id and the text content.", "labels": [], "entities": []}, {"text": "Each question-answer pair data mainly includes the text id, the question id, and the question-answer pair content.", "labels": [], "entities": []}, {"text": "In the final experiment, we added validation data to the training set to expand the training data.", "labels": [], "entities": []}, {"text": "We also tried sorting the training data randomly to expand the data set, but the result was not satisfactory.", "labels": [], "entities": []}, {"text": "All input data is converted into word vectors through the word-embedding layer, and the word-embedding model is Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9704992175102234}]}, {"text": "Here, all the punctuation is ignored, and all non-English characters are treated as unknown words.", "labels": [], "entities": []}, {"text": "In the word vectors, unknown word vectors are randomly generated from a uniform distribution U (-0.25, 0.25).", "labels": [], "entities": []}, {"text": "Two different methods of word-embedding are used in this experiment: Word2Vec and GloVe ().", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9553646445274353}, {"text": "GloVe", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.7873321771621704}]}, {"text": "They are used to initialize the weights of the embedding layer in building 300-dimension word vectors for all the texts and question-answer pairs.", "labels": [], "entities": []}, {"text": "Word2Vec achieved better performance than GloVe in every model we used.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.929478645324707}]}, {"text": "Through the list of unknown words, we know that the use of Word2Vec results in fewer unknown words than GloVe.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9588737487792969}]}, {"text": "All the code involved in this experiment was written in Python 3.5.2.", "labels": [], "entities": []}, {"text": "Keras 2.0.4 is used as the framework for the program.", "labels": [], "entities": []}, {"text": "The backend used in this experiment is Ten-, it can achieve an accuracy of 0.638 and 0.629 when respectively using Word2Vec and GloVe as the word-embedding layer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.999139666557312}, {"text": "Word2Vec", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9727315306663513}]}, {"text": "Due to the impact of jagged sentences, the poor result obtained by the CNN model is predictable.", "labels": [], "entities": []}, {"text": "After that, a standard LSTM model is used to complete this task.", "labels": [], "entities": []}, {"text": "It can achieve an accuracy of 0.651 and 0.642 when respectively using Word2Vec and GloVe as the word-embedding layer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9993399977684021}, {"text": "Word2Vec", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9775208830833435}]}, {"text": "However, the results obtained by the LSTM model have been somewhat improved over the CNN model.", "labels": [], "entities": []}, {"text": "Next, we also apply the BiLSTM model and the best result is 0.654, but there are still many points that can be improved.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8421705365180969}]}, {"text": "Combining the two models effectively seems to be the perfect choice.", "labels": [], "entities": []}, {"text": "In this way, we achieve an accuracy of 0.687.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997031092643738}]}, {"text": "Finally, after adding the attention mechanism, the result is raised to 0.699.", "labels": [], "entities": []}, {"text": "Under the same experimental conditions, the attention-based CNN-LSTM model obtained a better result than other models we used inmost cases (.", "labels": [], "entities": []}, {"text": "To exclude the experimental error caused by chance, nine such models are assembled together for training.", "labels": [], "entities": []}, {"text": "The final accuracy can be raised to 0.714.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9873031973838806}]}, {"text": "Table 1 presents the results of a comparative experiment for all models we used.", "labels": [], "entities": []}, {"text": "The choice of model parameters has a significant effect on the final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.994831919670105}]}, {"text": "The main parameters of this model are the word-embedding dimension, the batch size, the epoch, the filter size, the kernel size, the dropout and soon.", "labels": [], "entities": []}, {"text": "To get the optimal parameters, the Sklearn grid search function () is used to determine the best combination of the parameters.", "labels": [], "entities": []}, {"text": "lists the parameters of the model when the best result is obtained.", "labels": [], "entities": []}, {"text": "For this experiment, it measures how well a system is capable of correctly answering questions that may involve commonsense knowledge.", "labels": [], "entities": []}, {"text": "This problem is atypical binary classification problem.", "labels": [], "entities": []}, {"text": "Therefore, the system is evaluated by calculating the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9992996454238892}]}, {"text": "According to the final results provided by the organizers, a total of 199 teams enrolled in the competition.", "labels": [], "entities": []}, {"text": "Only 24 teams eventually submitted their results.", "labels": [], "entities": []}, {"text": "Our team ranked 13th overall among all teams.", "labels": [], "entities": []}, {"text": "As shown in, the attentionbased CNN-LSTM model can achieve the highest accuracy when using Word2Vec as the wordembedding layer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9987792372703552}, {"text": "Word2Vec", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9490006566047668}]}, {"text": "This model combines the advantages of the CNN model, the LSTM model and the attention mechanism.", "labels": [], "entities": []}, {"text": "Furthermore, the use of Word2Vec for word-embedding is better than the GloVe word-embedding.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9564557671546936}]}, {"text": "The main difference between the two embeddings is in the training sets.", "labels": [], "entities": []}, {"text": "The training sets of Word2Vec are practically from the news, while the training sets of GloVe are from Twitter.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9796682000160217}, {"text": "GloVe", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9108520746231079}]}, {"text": "Therefore, the Word2Vec data source is better suited to this task.", "labels": [], "entities": [{"text": "Word2Vec data source", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.9818185766537985}]}], "tableCaptions": [{"text": " Table 1: Comparative experiment results", "labels": [], "entities": []}]}