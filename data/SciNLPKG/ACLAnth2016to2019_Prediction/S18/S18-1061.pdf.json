{"title": [{"text": "UMDuluth-CS8761 at SemEval-2018 Task 2: Emojis: Too many Choices?", "labels": [], "entities": [{"text": "UMDuluth-CS8761", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7815511226654053}]}], "abstractContent": [{"text": "In this paper, we present our system for assigning an emoji to a tweet based on the text.", "labels": [], "entities": []}, {"text": "Each tweet was originally posted with an emoji which the task providers removed.", "labels": [], "entities": []}, {"text": "Our task was to decide out of 20 emojis, which originally came with the tweet.", "labels": [], "entities": []}, {"text": "Two datasets were provided-one in English and the other in Spanish.", "labels": [], "entities": []}, {"text": "We treated the task as a standard classification task with the emojis as our classes and the tweets as our documents.", "labels": [], "entities": []}, {"text": "Our best performing system used a Bag of Words model with a Linear Support Vector Machine as its' classifier.", "labels": [], "entities": []}, {"text": "We achieved a macro F1 score of 32.73% for the English data and 17.98% for the Spanish data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9697679281234741}, {"text": "English data", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9120317995548248}, {"text": "Spanish data", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.7041272819042206}]}], "introductionContent": [{"text": "An AI system that can associate text with appropriate emojis could be useful for generating content that is sparkled with emojis among other uses (.", "labels": [], "entities": []}, {"text": "Given only the text from a tweet in English or Spanish, the SemEval () task was to determine the emoji that was in the original tweet.", "labels": [], "entities": []}, {"text": "To learn how users associate emojis with text, a dataset comprised of 489,609 tweets in English and 98,289 tweets in Spanish was provided.", "labels": [], "entities": []}, {"text": "Each tweet had a corresponding label representing the emoji that was in the tweet.", "labels": [], "entities": []}, {"text": "The labels were assigned based on the frequency of the emoji, 0 being assigned to the most frequent emoji.", "labels": [], "entities": []}, {"text": "The total number of labels was 20 for the English data and 19 for the Spanish data.", "labels": [], "entities": [{"text": "English data", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8576532900333405}]}, {"text": "We classified the tweets, our documents, by their emojis, our classes.", "labels": [], "entities": []}, {"text": "We viewed the emojis as approximations of the sentiment expressed in the text.", "labels": [], "entities": []}, {"text": "For our baseline, we implemented a Bag of Words model using a Bernoulli Naive Bayes classifier.", "labels": [], "entities": []}, {"text": "We analyzed the results and used the insights to implement our final system, which used a Linear Support Vector machine for classification and also used a Bag of Words model to represent each document.", "labels": [], "entities": []}, {"text": "This system performed better than our baseline by ~3.5 percentage points for our English data and ~1.5 percentage points for our Spanish data.", "labels": [], "entities": []}, {"text": "It also performed better than several neural network models we experimented with.", "labels": [], "entities": []}, {"text": "Our macro F1 score were 32.73% and 17.98% for our English data and Spanish data respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.943563848733902}, {"text": "English data and Spanish data", "start_pos": 50, "end_pos": 79, "type": "DATASET", "confidence": 0.7043344497680664}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline results for English", "labels": [], "entities": []}, {"text": " Table 2: Baseline results for Spanish", "labels": [], "entities": []}, {"text": " Table 3: BOW Confusion Matrix for English. We have  choose to leave some numbers out to prevent distrac- tion from the patterns we are trying to show. All left  out numbers are negligible.", "labels": [], "entities": [{"text": "BOW", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.953032374382019}]}, {"text": " Table 4: Some English Emojis", "labels": [], "entities": []}, {"text": " Table 5: BOW Spanish results. %C refers to the per- cent correctly labeled and %emoji refers to the percent  mislabeled as emoji.", "labels": [], "entities": [{"text": "BOW", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6375684142112732}]}, {"text": " Table 7: Macro F1 scores. Base is baseline, L is LSTM,  C is CNN, (f) means each class was equally repre- sented.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8728912472724915}, {"text": "Base", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9634041786193848}, {"text": "LSTM", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8730678558349609}, {"text": "CNN", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9717563390731812}]}]}