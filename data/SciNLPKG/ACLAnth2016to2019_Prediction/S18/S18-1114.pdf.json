{"title": [{"text": "DM NLP at SemEval-2018 Task 8: Neural Sequence Labeling with Linguistic Features", "labels": [], "entities": [{"text": "DM NLP", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.512641429901123}, {"text": "SemEval-2018 Task 8", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6013599435488383}, {"text": "Neural Sequence Labeling", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7451337178548177}]}], "abstractContent": [{"text": "This paper describes our submissions for SemEval-2018 Task 8: Semantic Extraction from CybersecUrity REports using NLP.", "labels": [], "entities": [{"text": "SemEval-2018 Task 8", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9053890506426493}, {"text": "Semantic Extraction from CybersecUrity REports", "start_pos": 62, "end_pos": 108, "type": "TASK", "confidence": 0.7275647163391114}]}, {"text": "The DM NLP participated in two subtasks: Sub-Task 1 classifies if a sentence is useful for inferring malware actions and capabilities, and SubTask 2 predicts token labels (\"Action\", \"Entity\", \"Modifier\" and \"Others\") fora given malware-related sentence.", "labels": [], "entities": [{"text": "DM NLP", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8294131755828857}]}, {"text": "Since we leverage results of Subtask 2 directly to infer the result of Subtask 1, the paper focus on the system solving Subtask 2.", "labels": [], "entities": []}, {"text": "By taking Subtask 2 as a sequence labeling task, our system relies on a recurrent neural network named BiLSTM-CNN-CRF with rich linguistic features, such as POS tags, dependency parsing labels, chunking labels , NER labels, Brown clustering.", "labels": [], "entities": [{"text": "BiLSTM-CNN-CRF", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.8292330503463745}, {"text": "dependency parsing labels", "start_pos": 167, "end_pos": 192, "type": "TASK", "confidence": 0.7707133491834005}]}, {"text": "Our system achieved the highest F1 score in both token level and phrase level.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9863775670528412}]}], "introductionContent": [{"text": "As a growing number of mobile devices and facilities are getting connected and digitized, malware attacks become increasingly rampant and dangerous.", "labels": [], "entities": []}, {"text": "CybersecUrity attracts more public attention but few NLP research and efforts.", "labels": [], "entities": []}, {"text": "A large number of malware-related texts is available online, such as malware reports and relevant blogs.", "labels": [], "entities": []}, {"text": "However due to the sheer volume and diversity of these texts, NLP researchers encounter problems to obtain valuable information, such as the specific actions taken by a certain malware and the capabilities described.", "labels": [], "entities": []}, {"text": "Therefore, automatic screening malware-related contents and labeling every token of the contents become potential applications of NLP and have drawn growing research interests.", "labels": [], "entities": []}, {"text": "In order to create a database in CybersecUrity domain which helps researchers to parse malwarerelated texts, the organizers of) (proposed the follow tasks: 1.", "labels": [], "entities": []}, {"text": "SubTask1: Classify if a sentence is relevant for inferring malware actions and capabilities.", "labels": [], "entities": []}, {"text": "2. SubTask2: Predict token labels fora given malware-related text.", "labels": [], "entities": []}, {"text": "3. SubTask3: Predict relation labels fora given malware-related text.", "labels": [], "entities": []}, {"text": "4. SubTask4: Predict attribute labels fora given malware-related text.", "labels": [], "entities": []}, {"text": "However, due to lack of time, we decided to address only SubTask 1 and SubTask 2.", "labels": [], "entities": []}, {"text": "In this paper, we describe the system that we submitted for the SemEval 2018 shared task.", "labels": [], "entities": [{"text": "SemEval 2018 shared task", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.86599101126194}]}, {"text": "Our system is based on RNN network and ranked first in both token level and phrase level.", "labels": [], "entities": []}, {"text": "Most existing high performance sequence labeling methods are linear statistical models, such as HMM (Hidden Markov Models) and CRF (Conditional Random Fields) ().", "labels": [], "entities": []}, {"text": "In the past few years, neural networks have been widely used to solve NLP problems.", "labels": [], "entities": []}, {"text": "Specially, several RNN-based neural networks have been proposed to handle sequence labeling tasks including Chinese word segmentation, POS tagging (, NER (Chiu and Nichols, 2015) (, which achieved outstanding performance against traditional methods.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.5953377981980642}, {"text": "POS tagging", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.7571937441825867}]}, {"text": "In this paper, we simple derive the result of SubTask1 from SubTask2 and regard SubTask 2 as the preorder.", "labels": [], "entities": []}, {"text": "Namely, our system firstly outputs sequence labels of a given sentence, and then checks whether some target labels turnout, such as Action, Entity, Modifier.", "labels": [], "entities": []}, {"text": "Sentences which have those target labels will be classify as malwarerelated one.", "labels": [], "entities": []}, {"text": "Focusing on SubTask1, We propose a neural network architecture using a hybrid bidirectional LSTM and CNN architecture which takes character-level and word-level representations combined with rich linguistic features as input.", "labels": [], "entities": []}, {"text": "Instead of decoding each label independently, we feed the output vectors of BiLSTM to a CRF layer.", "labels": [], "entities": []}, {"text": "Experiments show the significant improvement of our system compared with baselines.", "labels": [], "entities": []}, {"text": "The remainder of this paper includes a detail description of our system in Section 2.", "labels": [], "entities": []}, {"text": "Experiments and analysis of results are presented in Section 3.", "labels": [], "entities": []}, {"text": "Finally, Section 4 draws a conclusion and Section 5 describes our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We found that in the original dataset provided by organizers, the average percentage of positive samples dropped from 23% in training set to less than 6.6% in development set, which suggests that a model maybe strongly biased if trained on the training set and fine-tuned on the development set without randomization.", "labels": [], "entities": []}, {"text": "Therefore we combined the training data and development data after spelling correction and removal of unreadable characters.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7879466414451599}]}, {"text": "In order to avoid over-fitting problem, we adopted 5-fold cross validation by randomly splitting the combined data into five folds.", "labels": [], "entities": []}, {"text": "Each time we took four folds as training data and the rest as development data.", "labels": [], "entities": []}, {"text": "We downloaded GloVe () data as the source of pre-trained word embeddings.", "labels": [], "entities": [{"text": "GloVe () data", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.7827430367469788}]}, {"text": "For char and feature embeddings, we randomly initialized them with values drawn from the standard normal distribution.", "labels": [], "entities": []}, {"text": "The evaluation metrics were calculated by the CoNLL2000 Perl script at token level and phrase level.", "labels": [], "entities": [{"text": "CoNLL2000 Perl script", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.8772214849789938}]}, {"text": "For each level, precision, recall and F1-score were calculated.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9998581409454346}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9998015761375427}, {"text": "F1-score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999652624130249}]}, {"text": "Based on the highest F1-score we selected the best hyper-parameters (CNN output size, LSTM State size, learning rate, dropout, etc.) for single model in 5-fold cross validation.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9987473487854004}, {"text": "LSTM State size", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.8696445226669312}]}, {"text": "Besides, for the submission generated by the BiLSTM-CNN-CRF, we adopted model average strategy by averaging values of variables of 5 checkpoint files from 5 independent experiments sharing the same experiment settings.", "labels": [], "entities": [{"text": "BiLSTM-CNN-CRF", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.7450001239776611}]}, {"text": "Our experiments focus on improving the performance in phrase level because of two motivations: phrase level is more meaningful than token level https://github.com/tensorflow/tensor2tensor and the model superior in phrase level also outperforms the others in token level.", "labels": [], "entities": []}, {"text": "The comparison of models in shows that neural networks models significantly outperform the traditional model based on CRF.", "labels": [], "entities": []}, {"text": "Meanwhile models including character-level features in pretrained word embeddings show better result.", "labels": [], "entities": []}, {"text": "Last but not least, models with additional linguistic features improve the performance in both phrase level and token level significantly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment results in phrase level and token level.", "labels": [], "entities": []}]}