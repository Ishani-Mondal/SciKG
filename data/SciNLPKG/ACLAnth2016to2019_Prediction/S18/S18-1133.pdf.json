{"title": [{"text": "GU IRLAB at SemEval-2018 Task 7: Tree-LSTMs for Scientific Relation Classification", "labels": [], "entities": [{"text": "IRLAB", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.770136296749115}, {"text": "SemEval-2018 Task 7", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7615167697270712}, {"text": "Scientific Relation Classification", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.858064611752828}]}], "abstractContent": [{"text": "SemEval 2018 Task 7 focuses on relation extraction and classification in scientific literature.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8076940774917603}, {"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8831134140491486}]}, {"text": "In this work, we present our tree-based LSTM network for this shared task.", "labels": [], "entities": []}, {"text": "Our approach placed 9th (of 28) for subtask 1.1 (rela-tion classification), and 5th (of 20) for subtask 1.2 (relation classification with noisy entities).", "labels": [], "entities": [{"text": "relation classification with noisy entities", "start_pos": 109, "end_pos": 152, "type": "TASK", "confidence": 0.8192001819610596}]}, {"text": "We also provide an ablation study of features included as input to the network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Extraction (IE) has applications in a variety of domains, including in scientific literature.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8616519391536712}]}, {"text": "Extracted entities and relations from scientific articles could be used fora variety of tasks, including abstractive summarization, identification of articles that make similar or contrastive claims, and filtering based on article topics.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6470998525619507}, {"text": "identification of articles that make similar or contrastive claims", "start_pos": 132, "end_pos": 198, "type": "TASK", "confidence": 0.7429186635547214}]}, {"text": "While ontological resources can be leveraged for entity extraction (, relation extraction and classification still remains a challenging task.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7970084249973297}, {"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8491332530975342}]}, {"text": "Relations are particularly valuable because (unlike simple entity occurrences) relations between entities capture lexical semantics.", "labels": [], "entities": []}, {"text": "SemEval 2018 Task 7 (Semantic Relation Extraction and Classification in Scientific Papers) encourages research in relation extraction in scientific literature by providing common training and evaluation datasets.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6284348666667938}, {"text": "Semantic Relation Extraction and Classification in Scientific Papers)", "start_pos": 21, "end_pos": 90, "type": "TASK", "confidence": 0.8525331020355225}, {"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8188601732254028}]}, {"text": "In this work, we describe our approach using a tree-structured recursive neural network, and provide an analysis of its performance.", "labels": [], "entities": []}, {"text": "There has been considerable previous work with scientific literature due to its availability and interest to the research community.", "labels": [], "entities": []}, {"text": "A previous shared task (SemEval 2017 Task 10) investigated the extraction of both keyphrases (entities) and relations in scientific literature (.", "labels": [], "entities": [{"text": "SemEval 2017 Task 10", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.711969181895256}]}, {"text": "However, the relation set for this shared task was limited to just synonym and hypernym relationships.", "labels": [], "entities": []}, {"text": "The top three approaches used for relationonly extraction included convolutional neural networks (), bi-directional recurrent neural networks with Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) cells, and conditional random fields (.", "labels": [], "entities": [{"text": "relationonly extraction", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8119361996650696}]}, {"text": "There are several challenges related to scientific relation extraction.", "labels": [], "entities": [{"text": "scientific relation extraction", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6752434273560842}]}, {"text": "One is the extraction of the entities themselves.", "labels": [], "entities": []}, {"text": "produce the best published results on the 2017 ScienceIE shared task for entity extraction using a semisupervised approach with a bidirectional LSTM and a CRF tagger.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7805862128734589}]}, {"text": "provide an unsupervised technique for entity linking scientific entities in the biomedical domain to an ontology.", "labels": [], "entities": [{"text": "entity linking scientific entities", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.8145116567611694}]}, {"text": "Our approach employs a treebased LSTM network using a variety of syntactic features to perform relation label classification.", "labels": [], "entities": [{"text": "relation label classification", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.7796975374221802}]}, {"text": "We rank 9th (of 28) when manual entities are used for training, and 5th (of 20) when noisy entities are used for training.", "labels": [], "entities": []}, {"text": "Furthermore, we provide an ablation analysis of the features used by our model.", "labels": [], "entities": []}, {"text": "Code for our model and experiments is available.", "labels": [], "entities": []}], "datasetContent": [{"text": "SemEval 2018 Task 7 focuses on relation extraction, assuming a gold set of entities.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8041248023509979}, {"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8832264840602875}]}, {"text": "This allows participants to focus on specific issues related to relation extraction with a rich set of semantic relations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8775016665458679}]}, {"text": "Subtask 1.1 uses manual entity labels, and subtask 1.2 uses automatic entity labels (which maybe noisy).", "labels": [], "entities": []}, {"text": "our system for subtasks 1.1 and 1.2.", "labels": [], "entities": []}, {"text": "In both of these subtasks, participants are given scientific abstracts with entities and candidate relation pairs, and are asked to determine the relation label of each pair.", "labels": [], "entities": []}, {"text": "For subtask 1.1, both the entities and relations are manually annotated.", "labels": [], "entities": []}, {"text": "For subtask 1.2, the entities are automatically generated using the procedure described in.", "labels": [], "entities": []}, {"text": "This procedure introduces noise, but represents a more realistic evaluation environment than subtask 1.1.", "labels": [], "entities": []}, {"text": "In both cases, relations and gold labels are produced by human annotators.", "labels": [], "entities": []}, {"text": "All abstracts are from the ACL Anthology Reference Corpus ().", "labels": [], "entities": [{"text": "ACL Anthology Reference Corpus", "start_pos": 27, "end_pos": 57, "type": "DATASET", "confidence": 0.9260935336351395}]}, {"text": "We randomly select 50 texts from the training datasets for validation of our system.", "labels": [], "entities": []}, {"text": "We provide a summary of the datasets for training, validation, and testing in.", "labels": [], "entities": [{"text": "validation", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.9563754200935364}]}, {"text": "Notice how the proportions of each relation label vary considerably among the datasets.", "labels": [], "entities": []}, {"text": "We experiment with two sets of word embeddings: Wiki News and arXiv.", "labels": [], "entities": [{"text": "Wiki News", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9723948836326599}, {"text": "arXiv", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.5411794185638428}]}, {"text": "The Wiki News embeddings benefit from the large amount of general language, and the arXiv embeddings capture specialized domain language.", "labels": [], "entities": [{"text": "Wiki News embeddings", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8979614973068237}]}, {"text": "The Wiki News embeddings are pretrained using fastText with a dimension of 300 (.", "labels": [], "entities": [{"text": "Wiki News embeddings", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9029680093129476}]}, {"text": "The arXiv embeddings are trained on a corpus of text from the cs section of arXiv.org 2 using a window of 8 (to capture adequate term context) and a dimension of 100 (   the two embedding sources, the available embeddings are concatenated with a vector of appropriate size sampled from N (0, 10 \u22128 ).", "labels": [], "entities": []}, {"text": "For our official SemEval submission, we train our model using the concatenated embeddings and one-hot encoded dependency label features.", "labels": [], "entities": [{"text": "SemEval submission", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8466343879699707}]}, {"text": "We use a hidden layer of 200 nodes, a 0.2 dropout rate, and a training batch size of 16.", "labels": [], "entities": []}, {"text": "Syntactic trees were extracted using SpaCy , and the neural model was implemented using MxNet . The official evaluation metric is the macroaveraged F1 score of all relation labels.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9701321423053741}]}, {"text": "For additional analysis, we use the macro precision and recall, and the F1 score for each relation label.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.886981189250946}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9995146989822388}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9873437583446503}]}], "tableCaptions": [{"text": " Table 2: Frequency of relation labels in train, valida- tion, and test sets. See", "labels": [], "entities": []}, {"text": " Table 2. Notice how the pro- portions of each relation label vary considerably  among the datasets.  We experiment with two sets of word embed- dings: Wiki News and arXiv. The Wiki News em- beddings benefit from the large amount of general  language, and the arXiv embeddings capture spe- cialized domain language. The Wiki News em- beddings are pretrained using fastText with a di- mension of 300 (", "labels": [], "entities": []}, {"text": " Table 3: Performance result comparison to other task  participants for subtasks 1.1 and 1.2.", "labels": [], "entities": []}, {"text": " Table 4: Feature ablation results for subtasks 1.1 and 1.2. DEP are dependency labels, POS are part of speech  labels, EntLen is is the length of the input entities, and Height is the height of the entities in the dependency tree.  In both subtasks 1.1 and 1.2, the combination of dependency labels, parts of speech, and entity lengths yield the  best performance in terms of overall F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 385, "end_pos": 393, "type": "METRIC", "confidence": 0.9846898019313812}]}, {"text": " Table 5: Performance comparison for subtasks 1.1 and  1.2 when using Wiki News and arXiv embeddings.  The concatenated embeddings outperform the individ- ual methods.", "labels": [], "entities": [{"text": "Wiki News", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9221291244029999}]}]}