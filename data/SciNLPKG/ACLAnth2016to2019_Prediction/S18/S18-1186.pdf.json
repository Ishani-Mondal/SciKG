{"title": [{"text": "BLCU NLP at SemEval-2018 Task 12: An Ensemble Model for Argument Reasoning Based on Hierarchical Attention", "labels": [], "entities": [{"text": "BLCU NLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7773109376430511}, {"text": "SemEval-2018 Task 12", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.5313600301742554}]}], "abstractContent": [{"text": "The argument comprehension reasoning task aims to reconstruct and analyze the argument reasoning.", "labels": [], "entities": [{"text": "argument comprehension reasoning task", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7165839746594429}]}, {"text": "To comprehend an argument and fill the gap between claims and reasons, it is vital to find the implicit supporting warrants behind.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hierarchical attention model to identify the right warrant which explains why the reason stands for the claim.", "labels": [], "entities": []}, {"text": "Our model focuses not only on the similar part between warrants and other information but also on the contradictory part between two opposing warrants.", "labels": [], "entities": []}, {"text": "In addition, we use the ensemble method for different models.", "labels": [], "entities": []}, {"text": "Our model achieves an accuracy of 61%, ranking second in this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.999714195728302}]}, {"text": "Experimental results demonstrate that our model is effective to make correct choices.", "labels": [], "entities": []}], "introductionContent": [{"text": "The argument reasoning comprehension is a crucial part for natural language understanding and inference, since argument comprehension requires reconstruction and analysis for its reasoning.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6528945565223694}]}, {"text": "Lack of commonsense makes it difficult to infer claims from corresponding reasons directly.", "labels": [], "entities": []}, {"text": "To fill the gap between claims and reasons, several explorations have been performed on argumentative structure of a debate.", "labels": [], "entities": []}, {"text": "Argument reasoning comprehension, anew task in SemEval 2018, sheds some light on the core of reasoning in natural language argumentation: implicit warrants, which are seen as abridge between claims and reasons.", "labels": [], "entities": []}, {"text": "Given a reason R, a claim C and two alternative warrants W0 and W1, the goal of this task is to identify the right warrant which can justify the use of R as support for C.", "labels": [], "entities": []}, {"text": "The difficulty of the task is the warrants are plausible and lexically close * *The corresponding author but lead to contradicting claims (.", "labels": [], "entities": []}, {"text": "To be more specific, the reason Rand the claim C are propositions extracted from a natural language argument.", "labels": [], "entities": []}, {"text": "And warrant Wis the relation between Rand C which is characterized by a rule of inference.", "labels": [], "entities": []}, {"text": "Walton proposed that an argument refers to a claim based on reasons given in the premises.", "labels": [], "entities": []}, {"text": "The most central part of this task is how to find the warrant for the given Rand C. In the argument reasoning comprehension task, the organizer extracts the instances from Room for Debate section of the New York Times.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.8344436486562093}, {"text": "Debate section of the New York Times", "start_pos": 181, "end_pos": 217, "type": "DATASET", "confidence": 0.7491981557437352}]}, {"text": "After a complex crowd-sourcing process, 1970 valid instances are provided for the task.", "labels": [], "entities": []}, {"text": "Two alternative warrants are provided as candidates, where one can justify the use of R as support for C and the other one can justify R as support for the opposite side of C.", "labels": [], "entities": []}, {"text": "We need to reconstruct the reasoning and select the right warrant which stands for claim in this task.", "labels": [], "entities": []}, {"text": "The average score for human is 79.8%, while for those with extensive formal training is 90.9% (.", "labels": [], "entities": []}, {"text": "In this paper, we not only pay attention to the similar part between each warrant and other information but also pay close attention to the contradictory part between two warrants.", "labels": [], "entities": []}, {"text": "We propose a model for this task which consists of four components: sentence representation layer, attended warrant layer, enhanced attention layer and prediction layer.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7512199282646179}]}, {"text": "All the sentences are represented with word embeddings in the sentence representation layer.", "labels": [], "entities": []}, {"text": "And to better understand the meaning of warrant, we incorporate the additional information to re-represent the two warrants.", "labels": [], "entities": []}, {"text": "Then we apply an enhanced attention layer to emphasize the similar and contradictory part between the two alternative warrants W0 and W1.", "labels": [], "entities": []}, {"text": "At last, we make prediction through a feedforward neural network layer).", "labels": [], "entities": []}, {"text": "In addition to the primary model, we propose an ensemble method to achieve a stable and credible accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9723426103591919}]}, {"text": "This method is well established for obtaining highly accurate classifiers by combining less accurate ones).", "labels": [], "entities": []}, {"text": "Our model improves the accuracy by 4% compared to the baseline model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997768998146057}]}], "datasetContent": [{"text": "The argument reasoning comprehension task chooses the Room for Debate section of the New York Times as source data.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.9086701273918152}, {"text": "Debate section of the New York Times", "start_pos": 63, "end_pos": 99, "type": "DATASET", "confidence": 0.7669128051825932}]}, {"text": "The given reason sentences come from stance-taking comments, and the false warrant is generated by annotators.", "labels": [], "entities": []}, {"text": "It is validated manually that the created false warrant can prove the reason which will lead to the opposite claim.", "labels": [], "entities": []}, {"text": "And the right warrant is written by minimal modifications to the false one, which can ensure that this warrant can be inferred from the reason and stands for the claim.", "labels": [], "entities": []}, {"text": "And also, to evaluate the performance of the models, each instance in the dataset is represented as a tuple (R;C;W0;W1) along with a label (0 or 1).", "labels": [], "entities": []}, {"text": "If the label is 0, W0 is the correct warrant, otherwise W1.", "labels": [], "entities": []}, {"text": "More details about dataset can be found in the work of All the data of the argument reasoning comprehension task is provided in the GitHub by task organizers 2 . For the availability and validity, after complex manual processing, only 1955 instances are selected from 11k comments.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7693990394473076}, {"text": "validity", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9895941019058228}]}, {"text": "We use the development set to select models for testing.", "labels": [], "entities": []}, {"text": "Training details are as follows.", "labels": [], "entities": []}, {"text": "We use ADAM optimizer for training, setting the first hyperparameter to be 2 the data can be found in github https://github.com/habernal/semeval2018-task12/tree/master/data 0.9 and the second 0.999.", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.001 and the batch size is 32.", "labels": [], "entities": []}, {"text": "We use word2vec) to pre-train the word embedding of 300 dimention and keep them from updating while training.", "labels": [], "entities": []}, {"text": "The numbers of hidden units and layers of biLSTM networks are 64 and 1 respectively.", "labels": [], "entities": []}, {"text": "And the dropout rate is set to be 0.1, and is applied to all biLSTM networks.", "labels": [], "entities": []}, {"text": "For the prediction layer, we choose standard FNN with 1 layer and set the hidden cells number to 64.", "labels": [], "entities": [{"text": "FNN", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7855843305587769}]}, {"text": "We use accuracy to evaluate the performance of the models, that is, computing the ratio of the right predicted labels of all instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9994315505027771}]}, {"text": "A scorer and detail information are described in the task introduction website .The scorer can give us the expected accuracy of the model.", "labels": [], "entities": [{"text": "detail", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9937397241592407}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993789196014404}]}, {"text": "shows the accuracy of different models on development dataset and test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999495267868042}]}, {"text": "The first row is a baseline model which uses intra-warrant attention between two warrants (.", "labels": [], "entities": []}, {"text": "In this model, all the representations of input sentences except warrant0 are concatenated as an attention vector for warrant0 , and all sentences except warrant1 are concatenated as an attention vector for warrant1.", "labels": [], "entities": []}, {"text": "And the attentive representations is used for prediction .", "labels": [], "entities": [{"text": "prediction", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9687027335166931}]}], "tableCaptions": [{"text": " Table 1: results of different models", "labels": [], "entities": []}]}