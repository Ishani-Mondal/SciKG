{"title": [{"text": "The Limitations of Cross-language Word Embeddings Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "The aim of this work is to explore the possible limitations of existing methods of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods.", "labels": [], "entities": [{"text": "cross-language word embeddings evaluation", "start_pos": 83, "end_pos": 124, "type": "TASK", "confidence": 0.719365194439888}]}, {"text": "To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them.", "labels": [], "entities": []}, {"text": "The results say that the scores even on different intrinsic benchmarks do not correlate to each other.", "labels": [], "entities": []}, {"text": "We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do native speakers process semantics in their cognition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Real-valued word representations called word embeddings are an ubiquitous and effective technique of semantic modeling.", "labels": [], "entities": [{"text": "semantic modeling", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.8342126905918121}]}, {"text": "So it is not surprising that cross-language extensions of such models (crosslanguage word embeddings) rapidly gained popularity in the NLP community, proving their effectiveness in certain crosslanguage NLP tasks (.", "labels": [], "entities": []}, {"text": "However, the problem of proper evaluation of any type of word embeddings still remains open.", "labels": [], "entities": []}, {"text": "In recent years there was a critique to mainstream methods of intrinsic evaluation: some researchers addressed subjectivity of human assessments, obscurity of instructions for certain tasks and terminology confusions.", "labels": [], "entities": []}, {"text": "Despite all these limitations, some of the criticized methods (like the word similarity task) has been started to be actively applied yet for cross-language word embeddings evaluation).", "labels": [], "entities": [{"text": "cross-language word embeddings evaluation", "start_pos": 142, "end_pos": 183, "type": "TASK", "confidence": 0.6912120655179024}]}, {"text": "We argue that if certain tasks are considered as not proper enough for mono-lingual evaluation, then it should be even more inappropriate to use them for cross-language evaluation since new problems would appear due to the new features of cross-linguality wherein the old limitations still remain.", "labels": [], "entities": [{"text": "cross-language evaluation", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.7634207904338837}]}, {"text": "Moreover, it is still unknown for the field of cross-language word embeddings, are we able to make relevant predictions on performance of the model on one method, using another.", "labels": [], "entities": [{"text": "cross-language word embeddings", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6410670578479767}]}, {"text": "We do not know whether can we use the relative ordering of different embeddings obtained by evaluation on an intrinsic task to decide which model will be better on a certain extrinsic task.", "labels": [], "entities": []}, {"text": "So, the aim of this work is to highlight the limitations of cross-language intrinsic benchmarks, studying the connection of outcomes from different cross-language word embeddings evaluation schemes (intrinsic evaluation and extrinsic evaluation), and explain this connection by addressing certain issues of intrinsic benchmarks that hamper us to have a correlation between two evaluation schemes.", "labels": [], "entities": []}, {"text": "In this study as an extrinsic task we consider the cross-language paraphrase detection task.", "labels": [], "entities": [{"text": "cross-language paraphrase detection task", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.8253341466188431}]}, {"text": "This is because we think that the model's features that word similarity and paraphrase detection evaluate are very close: both of them test the quality of semantic modeling (i.e. not the ability of the model to identify POS tags, or the ability to cluster words in groups, or something else) in terms of properness of distances in words pairs with certain types of semantic relations (particularly, semantic similarity).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.945856899023056}]}, {"text": "Therefore, we could not say that a strong difference in performances of word embeddings on these two tasks could be highly expected.", "labels": [], "entities": []}, {"text": "In this paper we propose a comparison of 5 cross-language models on extrinsic and intrinsic datasets for English-Russian language pair constructed specially for this study.", "labels": [], "entities": []}, {"text": "We consider Russian because we are native speakers of this lan-guage (hence, we are able to adequately construct novel datasets according the limitations that we address).", "labels": [], "entities": []}, {"text": "Our work is a step towards exploration of the limitations of cross-language evaluation of word embeddings, and it has three primary contributions: 1.", "labels": [], "entities": [{"text": "cross-language evaluation of word embeddings", "start_pos": 61, "end_pos": 105, "type": "TASK", "confidence": 0.7802984833717346}]}, {"text": "We propose an overview of limitations of current intrinsic cross-language word embeddings evaluation techniques; 2.", "labels": [], "entities": []}, {"text": "We construct 12 cross-language datasets for evaluation on the word similarity task; 3.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7801556587219238}]}, {"text": "We propose a novel task for cross-language extrinsic evaluation that was never addressed before from the benchmarking perspective, and we create a human-assessed dataset for this task.", "labels": [], "entities": [{"text": "cross-language extrinsic evaluation", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.7754578193028768}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 puts our work in the context of previous studies.", "labels": [], "entities": []}, {"text": "Section 3 describes the problems of intrinsic crosslanguage evaluation.", "labels": [], "entities": [{"text": "crosslanguage evaluation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7489148378372192}]}, {"text": "Section 4 is about the experimental setup.", "labels": [], "entities": []}, {"text": "The results of the comparison are reported in Section 5, while Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In an analogy with a monolingual paraphrase detection task (also called sentence similarity identification) (Androutsopoulos and Malakasiotis, 2010), the task is to identify whether sentence a in language A and sentence bin language B are paraphrases or not.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7818270524342855}, {"text": "sentence similarity identification", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.7019830048084259}]}, {"text": "This task is highly scalable, and usually figures as a sub-task of bigger tasks like crosslanguage plagiarism detection.", "labels": [], "entities": [{"text": "crosslanguage plagiarism detection", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.7476517160733541}]}, {"text": "We are not aware of any dataset for this task, so we designed a benchmark ourselves for EnglishRussian language pair.", "labels": [], "entities": []}, {"text": "The dataset was constructed on the base of Wikipedia articles covering wide range of topics from technology to sports.", "labels": [], "entities": []}, {"text": "It contains 8 334 sentences with a balanced class distribution.", "labels": [], "entities": []}, {"text": "The assessments and translations were done by 3 bilingual assessors.", "labels": [], "entities": [{"text": "translations", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9623957276344299}]}, {"text": "The negative results were obtained by automatically randomly sampling another sentence in the same domain from the datasets.", "labels": [], "entities": []}, {"text": "Translations were produced manually by a pool of human translators.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9687566161155701}]}, {"text": "Translators could paraphrase the translations using different techniques (according to our guidelines), and the assessors had to verify paraphrase technique labels and annotate similarity of English-Russian sentences in binary labels.", "labels": [], "entities": []}, {"text": "We invited 3 assessors to estimate inter-annotator agreement.", "labels": [], "entities": []}, {"text": "To obtain the evaluation scores, we conducted 3-fold cross validation and trained Logistic Regression with only one feature: cosine similarity of two sentence vectors.", "labels": [], "entities": []}, {"text": "Sentence representations were built by averaging their word vectors.", "labels": [], "entities": [{"text": "Sentence representations", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8844140470027924}]}, {"text": "In order to validate the correctness of results on our dataset, we automatically constructed a paraphrase set from a corpus of 1 million English- Russian parallel sentences from WMT'16 * , generating for each sentence pair a semantic negative sample, searching for nearest sentence with a monolingual FastText model.", "labels": [], "entities": []}], "tableCaptions": []}