{"title": [{"text": "HashCount at SemEval-2018 Task 3: Concatenative Featurization of Tweet and Hashtags for Irony Detection", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.6636185348033905}]}], "abstractContent": [{"text": "This paper proposes a novel feature extraction process for SemEval task 3: Irony detection in English tweets.", "labels": [], "entities": [{"text": "SemEval task 3", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.9216964840888977}, {"text": "Irony detection", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8015766441822052}]}, {"text": "The proposed system incorporates a concatenative featurization of tweet and hashtags, which helps distinguishing between the irony-related and the other components.", "labels": [], "entities": []}, {"text": "The system embeds tweets into a vector sequence with widely used pretrained word vectors, partially using a character embedding for the words that are out of vocabulary.", "labels": [], "entities": []}, {"text": "Identification was performed with BiLSTM and CNN classifiers, achieving F1 score of 0.5939 (23/42) and 0.3925 (10/28) each for the binary and the multi-class case, respectively.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.7895472645759583}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.986674964427948}]}, {"text": "The reliability of the proposed scheme was verified by analyzing the Gold test data, which demonstrates how hashtags can betaken into account when identifying various types of irony.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9618043899536133}, {"text": "Gold test data", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9267228841781616}]}], "introductionContent": [{"text": "Nowadays, opinion mining from social media has become an important issue in natural language processing (NLP).", "labels": [], "entities": [{"text": "opinion mining from social media", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8348323822021484}, {"text": "natural language processing (NLP)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.8150487939516703}]}, {"text": "Since tweets are globally used social media text that can influence the worldwide readers with just a short arrangement of words, the analysis on tweets has been widely studied in the semantic aspects such as sentiment classification (, hate speech detection (, and irony detection (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 209, "end_pos": 233, "type": "TASK", "confidence": 0.9271164536476135}, {"text": "hate speech detection", "start_pos": 237, "end_pos": 258, "type": "TASK", "confidence": 0.6778533756732941}, {"text": "irony detection", "start_pos": 266, "end_pos": 281, "type": "TASK", "confidence": 0.7898813784122467}]}, {"text": "Especially, the automatic detection of ironic tweets can help the readers who are having difficulty recognizing sarcasm to notice such figurative instances from excessive amount of text data.", "labels": [], "entities": [{"text": "automatic detection of ironic tweets", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7117691457271575}]}, {"text": "Despite the potential usage of the tasks, irony and sarcasm are difficult to grasp simply by analyzing word distribution.", "labels": [], "entities": []}, {"text": "They require understandings on the language and social context, which are dependent on time and space; this implies that the study should accompany constant updates on the database used for the analysis.", "labels": [], "entities": []}, {"text": "Also, it is important to construct a concrete criteria set to distinguish between ironic and non-ironic tweets.", "labels": [], "entities": []}, {"text": "In this paper, we incorporate a classification on manually labeled irony tweet corpus.", "labels": [], "entities": []}, {"text": "The corpus contains 2,396 English tweets for ironic/nonironic each (4,792 total), which were annotated under the scheme suggested in Van Hee et al..", "labels": [], "entities": []}, {"text": "For the competition, the corpus was split into a training (80%, or 3,834 instances) and test (20%, or 958 instances) set.", "labels": [], "entities": []}, {"text": "A training procedure for the system was all performed with the former (constrained).", "labels": [], "entities": []}], "datasetContent": [{"text": "The preprocessing was performed based on the example code 2 , where the corpus was tokenized with NLTK ().", "labels": [], "entities": []}, {"text": "The 100dim GloVe () pretrained with 27B token Twitter data 3 was employed as a dictionary for word embedding.", "labels": [], "entities": [{"text": "27B token Twitter data 3", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.7126301050186157}]}, {"text": "All the training and validation procedure were carried outwith Keras (TensorFlow backend) ().", "labels": [], "entities": [{"text": "validation", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9530484676361084}, {"text": "Keras (TensorFlow backend)", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.8468636870384216}]}, {"text": "The code is provided online 4 . In this part, two main results are investigated.", "labels": [], "entities": []}, {"text": "One deals with the comparison of performance according to classifiers and features.", "labels": [], "entities": []}, {"text": "This is based on 10-fold cross validation using training data.", "labels": [], "entities": []}, {"text": "In other words, the systems are trained using 3,450 instances and validated on 384 instances.", "labels": [], "entities": []}, {"text": "In the other result, the competition score acquired using the submitted systems is inspected.", "labels": [], "entities": []}, {"text": "Afterwards, we analyze the Gold test data, finding supportive evidences for the validity of the proposed approach regarding hashtags.", "labels": [], "entities": [{"text": "Gold test data", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.855233629544576}]}], "tableCaptions": [{"text": " Table 1: 10-fold cross validation on the constrained training dataset. In task B, F1 score was obtained by macro  averaging used in the scoring of competition. Bolded cases denote the best performing system for each labeled  corpus. Underlined ones denote the systems that were originally submitted to the competition for each task. The  score was updated according to an additional experiment.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9883512854576111}]}, {"text": " Table 2: The submitted systems evaluated with the  measurements, compared with the best scoring ones.", "labels": [], "entities": []}, {"text": " Table 3: Analysis with the Gold test data of Task A. @ and # each denotes ID tag and hashtag.", "labels": [], "entities": [{"text": "Gold test data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.738769551118215}]}, {"text": " Table 4: Analysis with the Gold test data of Task B.", "labels": [], "entities": [{"text": "Gold test data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8057962457338969}]}, {"text": " Table 5: The number of effective instances for each  irony types (correct/total) regarding the number of  hashtags. The irony hashtags were not counted.", "labels": [], "entities": []}]}