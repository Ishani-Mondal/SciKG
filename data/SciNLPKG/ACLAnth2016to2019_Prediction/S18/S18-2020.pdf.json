{"title": [{"text": "Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment", "labels": [], "entities": [{"text": "Lexical Entailment", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8959042429924011}]}], "abstractContent": [{"text": "Supervised distributional methods are applied successfully in lexical entailment, but recent work questioned whether these methods actually learn a relation between two words.", "labels": [], "entities": [{"text": "lexical entailment", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7395808100700378}]}, {"text": "Specifically, Levy et al.", "labels": [], "entities": []}, {"text": "(2015) claimed that linear classifiers learn only separate properties of each word.", "labels": [], "entities": []}, {"text": "We suggest a cheap and easy way to boost the performance of these methods by integrating multiplicative features into commonly used representations.", "labels": [], "entities": []}, {"text": "We provide an extensive evaluation with different classi-fiers and evaluation setups, and suggest a suitable evaluation setup for the task, eliminating biases existing in previous ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical entailment is concerned with identifying the semantic relation, if any, holding between two words, as in (pigeon, hyponym, animal).", "labels": [], "entities": [{"text": "Lexical entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.886136919260025}]}, {"text": "The popularity of the task stems from its potential relevance to various NLP applications, such as question answering and recognizing textual entailment () that often rely on lexical semantic resources with limited coverage like Wordnet.", "labels": [], "entities": [{"text": "question answering and recognizing textual entailment", "start_pos": 99, "end_pos": 152, "type": "TASK", "confidence": 0.7124231110016505}, {"text": "Wordnet", "start_pos": 229, "end_pos": 236, "type": "DATASET", "confidence": 0.9557929635047913}]}, {"text": "Relation classifiers can be used either within applications or as an intermediate step in the construction of lexical resources which is often expensive and time-consuming.", "labels": [], "entities": [{"text": "Relation classifiers", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8523143827915192}]}, {"text": "Most methods for lexical entailment are distributional, i.e., the semantic relation holding between x and y is recognized based on their distributional vector representations.", "labels": [], "entities": [{"text": "lexical entailment", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8125320076942444}]}, {"text": "While the first methods were unsupervised and used highdimensional sparse vectors (, in recent years, supervised methods became popular ().", "labels": [], "entities": []}, {"text": "These methods are mostly based on word embeddings () utilizing various vector combinations that are designed to capture relational information between two words.", "labels": [], "entities": []}, {"text": "While most previous work reported success using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (, making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concat v x \u2295 v y ( and), using various classifiers.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Mult v xv y , and the squared difference Sqdiff( . These features, similar to the cosine similarity and the Euclidean distance, might capture a different notion of interaction information about the relationship holding between two words.", "labels": [], "entities": [{"text": "Mult", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.8212920427322388}, {"text": "Sqdiff", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.5740165114402771}]}, {"text": "We directly integrate them into some commonly used representations.", "labels": [], "entities": []}, {"text": "For instance, we consider the concatenation that might capture both the typicality of each word in the relation (e.g., if y is atypical hypernym) and the similarity between the words.", "labels": [], "entities": []}, {"text": "We experiment with multiple supervised distributional methods and analyze which representations perform well in various evaluation setups.", "labels": [], "entities": []}, {"text": "Our analysis confirms that integrating multiplicative features into standard representations can substantially boost the performance of linear classifiers.", "labels": [], "entities": []}, {"text": "While the contribution over non-linear classifiers is sometimes marginal, they are expensive to train, and linear classifiers can achieve the same effect \"cheaply\" by integrating multiplicative fea-tures.", "labels": [], "entities": []}, {"text": "The contribution of multiplicative features is mostly prominent in strict evaluation settings, i.e., lexical split ( and out-ofdomain evaluation that disable the models' ability to achieve good performance by memorizing words seen during training.", "labels": [], "entities": []}, {"text": "We find that Concat \u2295 Mult performs consistently well, and suggest it as a strong baseline for future research.", "labels": [], "entities": [{"text": "Concat \u2295 Mult", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.556734025478363}]}], "datasetContent": [{"text": "We evaluated the methods on four common semantic relation datasets: BLESS  The Only-y representation indicates how well a model can perform without considering the relation between x and y (.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9984951019287109}]}, {"text": "Indeed, in RAND, this method performs similarly to the others, except on ROOT09, which by design disables lexical memorization.", "labels": [], "entities": [{"text": "RAND", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.6387808322906494}, {"text": "ROOT09", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.8107883930206299}]}, {"text": "As expected, a general decrease in performance is observed in LEX and OOD, stemming from the methods' inability to benefit from lexical memorization.", "labels": [], "entities": [{"text": "OOD", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.687615692615509}]}, {"text": "In these setups, there is a more significant gain from using multiplicative features when non-linear classifiers are used.", "labels": [], "entities": []}, {"text": "Word Pair Representations Among the base representations Concat often performed best, while Mult seemed to be the preferred multiplicative addition.", "labels": [], "entities": [{"text": "Word Pair Representations", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6503384013970693}, {"text": "Mult", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.7168216109275818}]}, {"text": "Concat \u2295 Mult performed consistently well, intuitively because Concat captures the typicality of each word in the relation (e.g., if y is atypical hypernym) and Mult captures the similarity between the words (where Concat alone may suggest that animal is a hypernym of apple).", "labels": [], "entities": []}, {"text": "To take a closer look at the gain from adding Mult, shows the performance of the various base representations and combinations with Mult using different classifiers on BLESS.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.6301287412643433}]}, {"text": "Classifiers Multiplicative features substantially boost the performance of linear classifiers.", "labels": [], "entities": []}, {"text": "However, the gain from adding multiplicative features   is smaller when non-linear classifiers are used, since they partially capture such notion of interaction (.", "labels": [], "entities": []}, {"text": "Within the same representation, there is a clear preference to non-linear classifiers over linear classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Metadata on the datasets. Relations are mapped to corresponding WordNet relations, if available.", "labels": [], "entities": []}, {"text": " Table 3: Best test performance (F 1 ) across different datasets and evaluation setups, using Glove. The number in  brackets indicates the performance gap between the best performing combination and base representation setups.", "labels": [], "entities": [{"text": "Best test performance (F 1 )", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7254451172692435}, {"text": "Glove", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.7120828032493591}]}, {"text": " Table 4: Test performance (F 1 ) on BLESS in the RAND and OOD setups, using Glove and Word2vec.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9680200020472208}, {"text": "BLESS", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9651153683662415}, {"text": "RAND", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.85573810338974}, {"text": "Word2vec", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9505885243415833}]}, {"text": " Table 6: Mean pairwise cosine similarity in BLESS.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9757043123245239}]}]}