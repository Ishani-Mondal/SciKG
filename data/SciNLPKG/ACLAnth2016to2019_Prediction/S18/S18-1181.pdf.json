{"title": [{"text": "MITRE at SemEval-2018 Task 11: Commonsense Reasoning without Commonsense Knowledge", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.5625214179356893}]}], "abstractContent": [{"text": "This paper describes MITRE's participation in SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge.", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8961551785469055}]}, {"text": "The techniques explored range from simple bag-of-ngrams classifiers to neural architectures with varied attention and alignment mechanisms.", "labels": [], "entities": []}, {"text": "Logistic regression ties the systems together into an ensemble submitted for evaluation.", "labels": [], "entities": []}, {"text": "The resulting system answers reading comprehension questions with 82.27% accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9936005473136902}]}], "introductionContent": [{"text": "Reading comprehension tasks measure the ability to answer questions that require inference from a free text story.", "labels": [], "entities": []}, {"text": "This SemEval task, like many standardized tests (e.g., the SAT), provides multiplechoice answers.", "labels": [], "entities": []}, {"text": "Reading comprehension may rely on information explicitly contained in the text, such as which actors are present, as well as elements of world knowledge, like understanding common scripts.", "labels": [], "entities": [{"text": "Reading comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8998106122016907}]}, {"text": "Early attempts at statistical reading comprehension include fill-in-the-blank questions and combine rich question categorization, information retrieval techniques, and entity recognition with type-specific, hand-crafted tactics).", "labels": [], "entities": [{"text": "statistical reading comprehension", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.7712876598040262}, {"text": "information retrieval", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7326739430427551}, {"text": "entity recognition", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.774893045425415}]}, {"text": "More recent neural work uses continuous, distributed semantic space rather than n-gram overlap to find answers similar to the story.", "labels": [], "entities": []}, {"text": "compute aversion of word overlap by finding cosine similarity of word representations between sections.", "labels": [], "entities": []}, {"text": "restate the question and answer and incorporate a question-type classifier.", "labels": [], "entities": []}, {"text": "In this effort we explored neural distributed representations and lexicon-based machine learning approaches, especially attention and word overlap.", "labels": [], "entities": [{"text": "attention and word overlap", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.5627483278512955}]}], "datasetContent": [{"text": "Machine Comprehension using Commonsense Knowledge was a shared task organized within).", "labels": [], "entities": []}, {"text": "The task organizers released a dataset of 1,689 stories and 10,872 questions (up to 14 questions per story), split into training and development sets.", "labels": [], "entities": []}, {"text": "The stories were first-person, English language narratives written by Mechanical Turk workers in response to prompts asking them to describe a scenario, like going on a date.", "labels": [], "entities": []}, {"text": "Stories were up to 860 words long, with 90% under 273 words and a median length of 183 words.", "labels": [], "entities": []}, {"text": "Questions are up to 22 words long, with a median length of seven.", "labels": [], "entities": []}, {"text": "Each question has two possible answers.", "labels": [], "entities": []}, {"text": "Answers vary in length from a single word, including yes or no, to 30 words, with a median length of three.", "labels": [], "entities": []}, {"text": "Many questions are repeated between multiple stories.", "labels": [], "entities": []}, {"text": "A sample story and questions are shown in.", "labels": [], "entities": []}, {"text": "Dataset construction is detailed in.", "labels": [], "entities": [{"text": "Dataset construction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7439354062080383}]}, {"text": "The evaluation metric for the task is simple accuracy: the portion of correct answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9539222121238708}]}, {"text": "quantifies the question types in the development set.", "labels": [], "entities": []}, {"text": "We created this taxonomy to better understand the dataset.", "labels": [], "entities": []}, {"text": "It also allowed us to direct our development toward model weaknesses.", "labels": [], "entities": []}, {"text": "We applied several approaches to the problem that did not generalize as well to the development data and were not included in the final ensemble.", "labels": [], "entities": []}, {"text": "Baselines We trained two baseline classifiers with incomplete information to gauge the difficulty of the task and to measure the relative importance of the story, question, and answers separately.", "labels": [], "entities": []}, {"text": "Each baseline was a recurrent neural network with a layer of pretrained word embeddings and a stack of two 128-dimensional GRU layers.", "labels": [], "entities": []}, {"text": "The QA-only baseline received as input only a concatenation of a question and its candidate answers, separated by special tokens.", "labels": [], "entities": []}, {"text": "The A-only baseline received only a concatenation of the two candidate answers.", "labels": [], "entities": []}, {"text": "A-only scored at 71.9% accuracy on the dev set, while QA-only was slightly higher at 73.2% accuracy.", "labels": [], "entities": [{"text": "A-only", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9348022937774658}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.998350977897644}, {"text": "QA-only", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.940153181552887}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9879177212715149}]}, {"text": "Other attempts to augment these models with attention over a lengthy story sequence frequently failed to eclipse the QA-only baseline, leading us to investigate hierarchical attention models and explicit overlap features.", "labels": [], "entities": []}, {"text": "Negative sampling We explored negative sampling to augment the training data, to improve our models' ability to exclude wrong answers.", "labels": [], "entities": []}, {"text": "We selected the 10 nearest neighbors for each question and supplemented the original positive and negative answer with the other questions' answers.", "labels": [], "entities": []}, {"text": "These were deduplicated after minimal preprocessing (normalizing case and punctuation), increasing the number of answers per question to between four and 20.", "labels": [], "entities": []}, {"text": "Our nearest neighbors calculation is based on the average of word vectors for the in-vocabulary words in the questions.", "labels": [], "entities": []}, {"text": "Negative sampling did not improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991239905357361}]}, {"text": "We tested conditions where 1) the original negative answer was sampled with equal probability or 2) always kept, and considered different values of N, where N is the total number of answers the model considered.", "labels": [], "entities": []}, {"text": "We found no accuracy gain from using negative sampling beyond normal variance when the original negative was always included.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993677735328674}]}, {"text": "When N was small, not necessarily in-: Factored and ablated system components evaluated on our dev set and the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.7346711158752441}]}, {"text": "cluding the original negative seems to hurt accuracy, suggesting that the randomly drawn negatives were not as plausible as the original negatives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9988718628883362}]}, {"text": "For larger values of N, both conditions hurt performance.", "labels": [], "entities": []}, {"text": "We experimented with condition 1 and the LR model.", "labels": [], "entities": [{"text": "LR", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.5100968480110168}]}, {"text": "The best value of N was 5 for this model, but accuracy was still below the model trained with the original dataset.", "labels": [], "entities": [{"text": "N", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.9679571390151978}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9997920393943787}]}, {"text": "The systems included in our model were trained only on the data released for this task, aside from word vector pretraining.", "labels": [], "entities": [{"text": "word vector pretraining", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7878420551617941}]}, {"text": "The dev set was used to select hyperparameters for individual components and final ensemble.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Length and count features in the logistic re- gression model, ranked by influence (\u03c3|w|).", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9960732460021973}, {"text": "count", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.4967239201068878}]}, {"text": " Table 3: Stemmed word factors in the logistic regres- sion model ranked by influence (\u03c3|w|).", "labels": [], "entities": []}, {"text": " Table 4: Factored and ablated system components eval- uated on our dev set and the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.7510125935077667}]}]}