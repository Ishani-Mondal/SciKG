{"title": [{"text": "T\u00fcbingen-Oslo at SemEval-2018 Task 2: SVMs perform better than RNNs at Emoji Prediction", "labels": [], "entities": [{"text": "Emoji Prediction", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.8078180849552155}]}], "abstractContent": [{"text": "This paper describes our participation in the SemEval-2018 task Multilingual Emoji Prediction.", "labels": [], "entities": [{"text": "SemEval-2018 task Multilingual Emoji Prediction", "start_pos": 46, "end_pos": 93, "type": "TASK", "confidence": 0.8837611317634583}]}, {"text": "We participated in both English and Spanish subtasks, experimenting with support vector machines (SVMs) and recurrent neural networks.", "labels": [], "entities": []}, {"text": "Our SVM classifier obtained the top rank in both subtasks with macro-averaged F1-measures of 35.99 % for English and 22.36 % for Spanish data sets.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9588010311126709}]}, {"text": "Similar to a few earlier attempts, the results with neural networks were not on par with linear SVMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emojis are graphical symbols that represent an idea or emotion.", "labels": [], "entities": []}, {"text": "The use of emojis has become popular over the last decade, particularly in informal communication in the social media.", "labels": [], "entities": []}, {"text": "Their popularity kindled a recent interest in investigating many aspects of emojis, including their interaction with natural language (e.g.,.", "labels": [], "entities": []}, {"text": "Although the emojis are presumably languageindependent, their use typically goes together with linguistic text.", "labels": [], "entities": []}, {"text": "In this context, the SemEval 2018 task 2, Multilingual Emoji Prediction (, aims predicting the emoji from the surrounding micro-blogging (Twitter) text for English and Spanish.", "labels": [], "entities": [{"text": "SemEval 2018 task", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8659585118293762}]}, {"text": "The task at hand is to predict a label, an emoji, from a short text that it accompanies.", "labels": [], "entities": []}, {"text": "This is essentially a text/document classification problem, and shares many aspects of other text classification problems such as topic classification, sentiment analysis, language identification and authorship attribution -just to name a few.", "labels": [], "entities": [{"text": "text/document classification", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6278996020555496}, {"text": "text classification", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7742605805397034}, {"text": "topic classification", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.8055238723754883}, {"text": "sentiment analysis", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.9453413784503937}, {"text": "language identification", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7761034071445465}, {"text": "authorship attribution", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.6986543536186218}]}, {"text": "Although each of these problems have some task-specific aspects, the same models can be used for all of them.", "labels": [], "entities": []}, {"text": "In this study, we experiment with and compare two well-known methods: support vector machines (SVMs) with bag of word/character n-gram features and recurrent neural networks (RNNs) with word and character sequences as input.", "labels": [], "entities": []}, {"text": "The methods and implementations are similar to our earlier attempts in other text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.8398060401280721}]}, {"text": "1 In the remainder of this paper, we describe our methods and experiments, present and discuss our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We participated in both subtasks using the same architectures.", "labels": [], "entities": []}, {"text": "However, we trained and tuned the model parameters on each data set separately.", "labels": [], "entities": []}, {"text": "The training set for the competition consisted of 500 000 tweets for English and 100 000 tweets for Spanish subtask.", "labels": [], "entities": []}, {"text": "The data sets contained most frequent 20 emojis for English and 19 emojis for Spanish.", "labels": [], "entities": []}, {"text": "Joining late to the party, our training set consisted of 485 151 English tweets, and 97 765 Spanish tweets, since about 3 % of the tweets were not available by the time we crawled them.", "labels": [], "entities": []}, {"text": "As presented in, the label distribution is similar and quite skewed for both languages.", "labels": [], "entities": []}, {"text": "We included pre-processing steps of case normalization and discarding low-frequency features as part of our hyperparameter optimization.", "labels": [], "entities": [{"text": "case normalization", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8126059472560883}]}, {"text": "In all our experiments, we use only the data supplied by the organizers.", "labels": [], "entities": []}, {"text": "We did not use any external sources (e.g., pre-trained word embeddings), nor did we perform any further linguistic processing (e.g., POS tagging, or parsing).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.8469545841217041}]}, {"text": "The test size for English and Spanish is 50 000 and 10 000 respectively.", "labels": [], "entities": []}], "tableCaptions": []}