{"title": [{"text": "Luminoso at SemEval-2018 Task 10: Distinguishing Attributes Using Text Corpora and Relational Knowledge", "labels": [], "entities": [{"text": "Distinguishing Attributes Using Text Corpora and Relational Knowledge", "start_pos": 34, "end_pos": 103, "type": "TASK", "confidence": 0.7756262198090553}]}], "abstractContent": [{"text": "Luminoso participated in the SemEval 2018 task on \"Capturing Discriminative Attributes\" with a system based on ConceptNet, an open knowledge graph focused on general knowledge.", "labels": [], "entities": [{"text": "SemEval 2018 task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9103033145268759}, {"text": "Capturing Discriminative Attributes", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.8822439511617025}]}, {"text": "In this paper, we describe how we trained a linear classifier on a small number of semantically-informed features to achieve an F 1 score of 0.7368 on the task, close to the task's high score of 0.75.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9911582668622335}]}], "introductionContent": [{"text": "Word embeddings are most effective when they learn from both unstructured text and a graph of general knowledge).", "labels": [], "entities": []}, {"text": "ConceptNet 5 ( ) is an open-data knowledge graph that is well suited for this purpose.", "labels": [], "entities": [{"text": "ConceptNet 5", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8417119085788727}]}, {"text": "It is accompanied by a pre-built word embedding model known as ConceptNet Numberbatch 1 , which combines skip-gram embeddings learned from unstructured text with the relational knowledge in ConceptNet.", "labels": [], "entities": []}, {"text": "A straightforward application of the ConceptNet Numberbatch embeddings took first place in SemEval 2017 task 2, on semantic word similarity.", "labels": [], "entities": [{"text": "SemEval 2017 task 2", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8661040216684341}, {"text": "semantic word similarity", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.7241103450457255}]}, {"text": "For SemEval 2018, we built a system with these embeddings as a major component fora slightly more complex task.", "labels": [], "entities": []}, {"text": "The Capturing Discriminative Attributes task ( emphasizes the ability of a semantic model to recognize relevant differences between terms, not just their similarities.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes task", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.9037001579999924}]}, {"text": "As the task description states, \"If you can tell that americano is similar to capuccino and espresso but you can't tell the difference between them, you don't know what americano is.\"", "labels": [], "entities": []}, {"text": "The ConceptNet Numberbatch embeddings only measure the similarity of terms, and we hy-pothesized that we would need to represent more specific relationships.", "labels": [], "entities": []}, {"text": "For example, the input triple \"frog, snail, legs\" asks us to determine whether \"legs\" is an attribute that distinguishes \"frog\" from \"snail\".", "labels": [], "entities": []}, {"text": "The answer is yes, because a frog has legs while a snail does not.", "labels": [], "entities": []}, {"text": "The has relationship is one example of a specific relationship that is represented in ConceptNet.", "labels": [], "entities": []}, {"text": "To capture this kind of specific relationship, we built a model that infers relations between ConceptNet nodes, trained on the existing edges in ConceptNet and random negative examples.", "labels": [], "entities": []}, {"text": "There are many models designed for this purpose; the one we decided on is based on Semantic Matching Energy (SME) (.", "labels": [], "entities": [{"text": "Semantic Matching Energy (SME", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.6655006408691406}]}, {"text": "Our features consisted of direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset.", "labels": [], "entities": [{"text": "Google Books dataset", "start_pos": 286, "end_pos": 306, "type": "DATASET", "confidence": 0.8567718068758646}]}, {"text": "We combined these features based on ConceptNet with features extracted from a few other resources in a LinearSVC classifier, using liblinear) via scikit-learn).", "labels": [], "entities": []}, {"text": "The classifier used only 15 features, of which 12 ended up with non-zero weights, from the five sources described.", "labels": [], "entities": []}, {"text": "We aimed to avoid complexity in the classifier in order to prevent overfitting to the validation set; the power of the classifier should be in its features.", "labels": [], "entities": []}, {"text": "The classifier produced by this design (submitted late to the contest leaderboard) successfully avoided overfitting.", "labels": [], "entities": []}, {"text": "It performed better on the test set than on the validation set, with a test F 1 score of 0.7368, whose margin of error overlaps with the evaluation's reported high score of 0.75.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9842668573061625}, {"text": "margin of error", "start_pos": 103, "end_pos": 118, "type": "METRIC", "confidence": 0.8947096268335978}]}, {"text": "At evaluation time, we accidentally submitted our results on the validation data, instead of the test data, to the SemEval leaderboard.", "labels": [], "entities": [{"text": "SemEval leaderboard", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.8575476109981537}]}, {"text": "Our code had truncated the results to the length of the test data, causing us to not notice the mismatch.", "labels": [], "entities": []}, {"text": "This erroneous submission got a very low score, of course.", "labels": [], "entities": []}, {"text": "This paper presents the corrected test results, which we submitted to the post-evaluation CodaLab leaderboard immediately after the results appeared.", "labels": [], "entities": [{"text": "CodaLab leaderboard", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.9131996929645538}]}, {"text": "We did not change the classifier or data; the change was a one-line change to our code for outputting the classifier's predictions on the test set instead on the validation set.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are other features that we tried and later discarded.", "labels": [], "entities": []}, {"text": "We experimented with a feature similar to the Google Books 2-grams feature, based on the AOL query logs dataset ().", "labels": [], "entities": [{"text": "AOL query logs dataset", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.7495701462030411}]}, {"text": "It did not add to the performance, most likely because any information it could provide was also provided by Google Books 2-grams.", "labels": [], "entities": []}, {"text": "Similiarly, we tried extending the Google Books 2-grams data to include the first and third words of a selection of 3-grams, but this, too, appeared redundant with the 2-grams.", "labels": [], "entities": [{"text": "Google Books 2-grams data", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.930180162191391}]}, {"text": "We also experimented with a feature based on bounding box annotations available in the OpenImages dataset ().", "labels": [], "entities": [{"text": "OpenImages dataset", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.9397710263729095}]}, {"text": "We hoped it would help us capture attributes such as colors, materials, and shapes.", "labels": [], "entities": []}, {"text": "While this feature did not improve the classifier's performance on the validation set, it did slightly improve the performance on the test set.", "labels": [], "entities": []}, {"text": "Before deciding on scikit-learn's LinearSVC,   we experimented with a number of other classifiers.", "labels": [], "entities": []}, {"text": "This included random forests, differentiable models made of multiple ReLU and sigmoid layers, and SVM with an RBF kernel or a polynomial kernel.", "labels": [], "entities": []}, {"text": "We also experimented with different parameters to LinearSVC, such as changing the default value of the penalty parameter C of the error term, changing the penalty from L 2 to L 1 , solving the primal optimization problem instead of the dual problem, and changing the loss from squared hinge to hinge.", "labels": [], "entities": []}, {"text": "These changes either led to lower performance or had no significant effect, so in the end we used LinearSVC with the default parameters for scikit-learn version 0.19.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Coefficients of each feature in our linear clas- sifier. x represents a term and a represents the attribute.", "labels": [], "entities": []}, {"text": " Table 2: F 1 scores by dataset. The reported F 1 score is  the arithmetic mean of the F 1 scores for both classes.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9440104762713114}, {"text": "F 1 score", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9487066864967346}, {"text": "F 1 scores", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9267223477363586}]}]}