{"title": [{"text": "TAJJEB at SemEval-2018 Task 2: Traditional Approaches Just Do the Job with Emoji Prediction", "labels": [], "entities": [{"text": "TAJJEB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5699633955955505}]}], "abstractContent": [{"text": "Emojis are widely used on social media; thus understanding their meaning is important for both practical purposes (e.g. opinion mining, sentiment detection) and theoretical purposes (e.g. How do different L1 speakers use them?, Do they have specific syntax?).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.7507732510566711}, {"text": "sentiment detection", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.9243890643119812}]}, {"text": "This paper presents a set of models that predict an emoji given a tweet as apart of the SemEval-2018 Task 2: Multilingual Emoji Prediction.", "labels": [], "entities": [{"text": "Multilingual Emoji Prediction", "start_pos": 109, "end_pos": 138, "type": "TASK", "confidence": 0.5924760401248932}]}, {"text": "We built different models and we found that the test results were very different from the validation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "To some extent, Twitter can be considered a huge corpus and researchers exploit it in many different ways to get a proxy for various types of annotations.", "labels": [], "entities": []}, {"text": "use distant supervision on tweets to build an emotion detection system; show that it is possible to use Twitter data to predict the stock market; build a corpus from tweets for modelling stance.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7266404181718826}, {"text": "modelling stance", "start_pos": 177, "end_pos": 193, "type": "TASK", "confidence": 0.8892114162445068}]}, {"text": "Social media in general are very popular for building corpora for sentiment-related tasks since the users make a wide use of emojis.", "labels": [], "entities": []}, {"text": "show that it is possible to achieve state-of-the-art performance in sentiment classification using only automatically gathered data.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9525959491729736}]}, {"text": "The wide use of emojis on the web calls for systems that can automatically process them.", "labels": [], "entities": []}, {"text": "When performing opinion mining tasks, for instance, it can be the case that all we have is just an emoji.", "labels": [], "entities": [{"text": "opinion mining tasks", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8641835848490397}]}, {"text": "For example, a single emoji could be used as a reply to an advertisement of a certain product; thus, being able to get the meaning of that emoji is important.", "labels": [], "entities": []}, {"text": "shows that given a text, it is possible to successfully automatically suggest the most appropriate emoji that can accompany that text.", "labels": [], "entities": []}, {"text": "In this paper we describe our participation in the SemEval-2018 Task 2: Multilingual Emoji Prediction (), a challenge that originates from the work of ().", "labels": [], "entities": [{"text": "SemEval-2018 Task 2", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8133048812548319}, {"text": "Multilingual Emoji Prediction", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.5979947745800018}]}, {"text": "The task is structured as follows: given a tweet that originally contained one and only one emoji, predict that emoji; the emoji is removed and given as a training label.", "labels": [], "entities": []}, {"text": "We experimented using three different approaches: first we use a shallow feature representation with two different algorithms (Na\u00a8\u0131veNa\u00a8\u0131ve-Bayes and Support Vector Machine); then we experimented with a dense feature representation (i.e. word embeddings) and a deep neural classifier; lastly, we modeled the problem as a translation problem (i.e. treating English and Spanish as the source language and 'Emoji' as the target language) using a state-of-the-art neural translation system to predict the labels as translated sentences.", "labels": [], "entities": []}, {"text": "To summarize, our main contributions presented in this paper are three machine learning models that predict an emoji given some text.", "labels": [], "entities": []}, {"text": "We confirm a fact that is well known in the literature, i.e. that neural models can give good results but hyper-parameter tuning is a hard task and if it is not successful, then a good linear classifier with a bag-of-words representation can easily outperform the neural model.", "labels": [], "entities": []}, {"text": "Our best English system was ranked 8th in SemEval Shared Task and our best Spanish system was ranked 4th 1 . sets of labels -one per language -and these labels are the emojis that were originally in the tweet and later removed.", "labels": [], "entities": []}, {"text": "The English label set consists of 20 emojis, while the Spanish label set consists of 19 emojis.", "labels": [], "entities": [{"text": "English label set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7976024945576986}]}, {"text": "We manually inspected some portions of the datasets and found that the English set contains some Spanish tweets and vice versa: this is due to the fact that the tweets were collected automatically using geographical information associated with the account of the user who wrote the tweet.", "labels": [], "entities": []}, {"text": "We decided to ignore this fact and assume that the actual test set will also contain some noise.", "labels": [], "entities": []}, {"text": "We did not perform any systematic language identification experiment since after the manual inspection it seemed to us that this would not be a problem.", "labels": [], "entities": [{"text": "language identification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7955977320671082}]}, {"text": "The distribution of the labels is highly skewed for both English and Spanish: shows that the most frequent label (i.e. the red heart) is much more frequent than the others.", "labels": [], "entities": []}, {"text": "We decided to conduct experiments with both the original skewed dataset and a balanced version of it.", "labels": [], "entities": []}, {"text": "To balance the dataset, we randomly sample from all classes a number of tweets which is equal to the size of the smallest class.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed two groups of experiments: for one group we used a shallow feature representation (i.e. bag-of-words), and for the other we used a dense feature representation (i.e. word embeddings).", "labels": [], "entities": []}, {"text": "In the following sections we present these two groups of experiments.", "labels": [], "entities": []}, {"text": "In order to facilitate our work, we utilized Python 3.6.3 and setup a Python working environment using pipenv to make our experiments easily reproducible; for the same reason we are releasing several Jupyter notebooks containing all the code that we wrote 2 . To train our models, we utilized the standard machine learning package, scikit-learn (Pedregosa et al., 2011) for the models using a shallow feature representation.", "labels": [], "entities": []}, {"text": "The neural models were built using Keras.", "labels": [], "entities": []}, {"text": "We used OpenNMT ( to model the problem as a translation problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of English Models.", "labels": [], "entities": []}, {"text": " Table 2: Results of Spanish Models.", "labels": [], "entities": [{"text": "Spanish Models", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8511022627353668}]}, {"text": " Table 3: The Result of Neural Network Models.", "labels": [], "entities": []}]}