{"title": [{"text": "GIST at SemEval-2018 Task 12: A network transferring inference knowledge to Argument Reasoning Comprehension task", "labels": [], "entities": [{"text": "SemEval-2018 Task 12", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.516168346007665}, {"text": "Argument Reasoning Comprehension", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.6618294517199198}]}], "abstractContent": [{"text": "This paper describes our GIST team system that participated in SemEval-2018 Argument Reasoning Comprehension task (Task 12).", "labels": [], "entities": [{"text": "SemEval-2018 Argument Reasoning Comprehension task", "start_pos": 63, "end_pos": 113, "type": "TASK", "confidence": 0.891045081615448}]}, {"text": "Here, we address two challenging factors: unstated common senses and two lexically close warrants that lead to contradicting claims.", "labels": [], "entities": []}, {"text": "A key idea for our system is full use of transfer learning from the Natural Language Inference (NLI) task to this task.", "labels": [], "entities": []}, {"text": "We used Enhanced Sequential Inference Model (ESIM) to learn the NLI dataset.", "labels": [], "entities": [{"text": "NLI dataset", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9063476324081421}]}, {"text": "We describe how to use ESIM for transfer learning to choose correct warrant through a proposed system.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9571335911750793}]}, {"text": "We show comparable results through ablation experiments.", "labels": [], "entities": []}, {"text": "Our system ranked 1st among 22 systems, outperforming all the systems more than 10%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argument Reasoning Comprehension is a task that choose correct warrant from two options given a claim and a reason.", "labels": [], "entities": [{"text": "Argument Reasoning Comprehension", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8314330180486044}]}, {"text": "The Argument Reasoning Comprehension is a very important task because \"argument comprehension requires not only language understanding and logic skills, but it also heavily depends on common sense\", as mentioned by.", "labels": [], "entities": [{"text": "Argument Reasoning Comprehension", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8280670642852783}]}, {"text": "There are two challenging factors.", "labels": [], "entities": []}, {"text": "One is a certain part of an argument is left unstated ().", "labels": [], "entities": []}, {"text": "Because of the unstated part, humans or machines need reasoning ability about that part.", "labels": [], "entities": []}, {"text": "Human can reconstruct the unstated part depending on common knowledge.", "labels": [], "entities": []}, {"text": "However, it has still remained difficult to machines.", "labels": [], "entities": []}, {"text": "Another is that \"both options are plausible and lexically very close while leading to contradicting claims\", as mentioned by.", "labels": [], "entities": []}, {"text": "To address these factors, we have two assumptions.", "labels": [], "entities": []}, {"text": "One is that similar and large datasets may help to address the unstated commonsense by learning various cases.", "labels": [], "entities": []}, {"text": "Another is that an inferrence model to distinguish semantic differences between two sentences may help to choose one of two lexically close warrants that lead to contradicting claims.", "labels": [], "entities": []}, {"text": "There are two suitable datasets in the Natural Language Inference (NLI) task, Stanford NLI (SNLI) and Multi NLI (MNLI) ( datasets.", "labels": [], "entities": []}, {"text": "NLI is a task choosing one of relationships (Entailment, Contradiction, Neutral) between two sentences.", "labels": [], "entities": []}, {"text": "Both SNLI and MNLI are very large corpus (each 0.5M sentence pairs).", "labels": [], "entities": [{"text": "MNLI", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9198387861251831}]}, {"text": "In addition, there is a good performance model for the task, Enhanced Sequential Inference Model (ESIM).", "labels": [], "entities": []}, {"text": "To make use of other datasets for our task, we use transfer learning.", "labels": [], "entities": []}, {"text": "About transfer learning, showed a good precedent, using SNLI dataset.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.9514842927455902}, {"text": "SNLI dataset", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9436134994029999}]}, {"text": "By learning the NLI task, the model can obtain inference knowledge.", "labels": [], "entities": []}, {"text": "Therefore, we propose a network transferring inference knowledge to argument reasoning comprehension task.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.7807897130648295}]}, {"text": "We summarize our system with 5 main components.", "labels": [], "entities": []}, {"text": "1. ESIM is trained on SNLI and MNLI datasets.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.7845872640609741}, {"text": "MNLI datasets", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9029525816440582}]}, {"text": "Then, parameters are frozen and used to transfer the inference knowledge.", "labels": [], "entities": []}, {"text": "2. As inputs of the ESIM, we make sentence pairs such as (claim, warrant), (warrant, reason) and (warrant, other warrant).", "labels": [], "entities": []}, {"text": "3. To add flexibility, we added biLSTM module encoding claim, reason and warrant.", "labels": [], "entities": [{"text": "biLSTM module encoding claim", "start_pos": 32, "end_pos": 60, "type": "METRIC", "confidence": 0.5552557408809662}, {"text": "warrant", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9931420683860779}]}, {"text": "4. To make a fixed length vector from variable one, we used average and max pooling.", "labels": [], "entities": [{"text": "average", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9695601463317871}]}, {"text": "5. Finally, all the fixed length vectors from ESIM and biLSTM are concatenated and fed into a fully-connected neural network to determine whether the warrant is corrector not.", "labels": [], "entities": []}, {"text": "The detail process is described in Section 2.", "labels": [], "entities": [{"text": "detail", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9037575721740723}]}], "datasetContent": [{"text": "Because of page limit, we briefly explain this part.", "labels": [], "entities": []}, {"text": "described that ESIM is composed of the following major components: input encoding, local inference modeling, and inference composition.", "labels": [], "entities": []}, {"text": "shows a high-level view of the architecture.", "labels": [], "entities": []}, {"text": "For more details, refer to the paper (.", "labels": [], "entities": []}, {"text": "ESIM generates two sentence vectors after comparing two input sentences with each other.", "labels": [], "entities": []}, {"text": "We notate it as follows.", "labels": [], "entities": []}, {"text": "The sv consists of vectors of l dimension, the number of which correspond to the length of each sentence.", "labels": [], "entities": []}, {"text": "The sv is the output of the inference composition part.", "labels": [], "entities": []}, {"text": "We implemented it as 300 dimensions.", "labels": [], "entities": []}, {"text": "The ESIM was trained on SNLI and MNLI datasets.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8087612390518188}, {"text": "MNLI datasets", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9117482602596283}]}, {"text": "The training was stopped when the average of development set accuracies was maximum.", "labels": [], "entities": [{"text": "development set accuracies", "start_pos": 45, "end_pos": 71, "type": "METRIC", "confidence": 0.8103947838147482}]}, {"text": "Then, the parameters were frozen so as to be not updated.", "labels": [], "entities": []}, {"text": "Pre-training First, to learn the inference knowledge, we implemented ESIM and trained on NLI training dataset.", "labels": [], "entities": [{"text": "NLI training dataset", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.7750059366226196}]}, {"text": "Our implemented ESIM dimension is 300.", "labels": [], "entities": []}, {"text": "Except for the ESIM dimension, we used the same hyperparameter values as those in.", "labels": [], "entities": []}, {"text": "The preprocessing process is implemented in the same way with subsection 2.1.", "labels": [], "entities": []}, {"text": "The word embeddings are not updated during training.", "labels": [], "entities": []}, {"text": "The training was stopped when the average of development set accuracies is maximum.", "labels": [], "entities": [{"text": "development set accuracies", "start_pos": 45, "end_pos": 71, "type": "METRIC", "confidence": 0.7581304709116617}]}, {"text": "We got development accuracy of 86.58%, 74.09%, 74.67% on SNLI, MNLI match, MNLI mismatch datasets, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9877039194107056}, {"text": "SNLI", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7390115857124329}, {"text": "MNLI", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.694889485836029}, {"text": "MNLI mismatch datasets", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.8033605615297953}]}, {"text": "Training We used the ADAM () optimizer for updating weight parameters.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8007317781448364}]}, {"text": "The parameters of ADAM set to be as follow: \u03b21 = 0.9, \u03b22 = 0.999, = 10 \u22128 . The initial learning rate is 0.0002 and is decayed with 0.9 rate per one epoch.", "labels": [], "entities": []}, {"text": "We did not use dropout but added L2 regularization on the first FCNN layer.", "labels": [], "entities": []}, {"text": "The regularization parameter \u03bb was set to be 5 \u00d7 10 \u22124 . The word embeddings were not updated during training.", "labels": [], "entities": []}, {"text": "We randomly shuffled training data during training.", "labels": [], "entities": []}, {"text": "The minibatch size was 25.", "labels": [], "entities": []}, {"text": "We trained 10 epochs and chose our model when the development set reached the max accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9899464249610901}]}, {"text": "We implemented our system by using lasagne) and theano (Theano Development Team, 2016) library.", "labels": [], "entities": [{"text": "Theano Development Team, 2016) library", "start_pos": 56, "end_pos": 94, "type": "DATASET", "confidence": 0.9582691192626953}]}, {"text": "Our code is available at here 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of each approch. The human and  baseline results are taken from Habernal et al. (2018).  Our approach ranked 1st among 22 systems, outper- forming all the systems more than 10%.  \u2020 indicates ap- proaches implemented by Habernal et al. (2018). Read- ers can check all system results at here 2 .", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970903396606445}]}]}