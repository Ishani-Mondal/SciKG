{"title": [{"text": "Duluth UROP at SemEval-2018 Task 2: Multilingual Emoji Prediction with Ensemble Learning and Oversampling", "labels": [], "entities": [{"text": "Multilingual Emoji Prediction", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6558351417382559}]}], "abstractContent": [{"text": "This paper describes the Duluth UROP systems that participated in SemEval-2018 Task 2, Multilingual Emoji Prediction.", "labels": [], "entities": [{"text": "SemEval-2018 Task 2", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8266065120697021}, {"text": "Multilingual Emoji Prediction", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.6191782653331757}]}, {"text": "We relied on a variety of ensembles made up of classi-fiers using Naive Bayes, Logistic Regression, and Random Forests.", "labels": [], "entities": []}, {"text": "We used unigram and bigram features and tried to offset the skew-ness of the data through the use of oversam-pling.", "labels": [], "entities": []}, {"text": "Our task evaluation results place us 19th of 48 systems in the English evaluation, and 5th of 21 in the Spanish.", "labels": [], "entities": []}, {"text": "After the evaluation we realized that some simple changes to pre-processing could significantly improve our results.", "labels": [], "entities": []}, {"text": "After making these changes we attained results that would have placed us sixth in the English evaluation, and second in the Spanish.", "labels": [], "entities": [{"text": "English evaluation", "start_pos": 86, "end_pos": 104, "type": "DATASET", "confidence": 0.8769077062606812}]}], "introductionContent": [{"text": "Emoji prediction of tweets is an emerging problem () which combines the nuances of sentiment analysis with the noisy data characteristic of social media.", "labels": [], "entities": [{"text": "Emoji prediction of tweets", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.898870512843132}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8716324269771576}]}, {"text": "adds to this the challenge of multilingual processing, where both Spanish and English tweets are used.", "labels": [], "entities": []}, {"text": "Given the relatively large amount of training data available for the task (see Section 2) we decided to approach this as a classification task, where we used relatively simple unigram and bigram features in combination with standard machine learning algorithms.", "labels": [], "entities": []}, {"text": "We particularly focused on the use of Multinomial Naive Bayes, Logistic Regression, and Random Forests, all of which were provided to us via the scikit learn package).", "labels": [], "entities": []}, {"text": "Given the challenging nature of this task we decided to employ a variety of ensemble approaches, since no single classifier seemed likely to perform well in all cases.", "labels": [], "entities": []}, {"text": "In the sections that follow we summarize the experimental data used in the task, and then describe the methods we employed for supervised learning, ensemble building, and oversampling.", "labels": [], "entities": [{"text": "ensemble building", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.800076812505722}]}, {"text": "We close by interpreting and discussing our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task organizers created both training and test data made up of Spanish and English tweets (separately).", "labels": [], "entities": []}, {"text": "The training data consists of 100,000 Spanish tweets and 500,000 English tweets.", "labels": [], "entities": []}, {"text": "The test data is made up of 10,000 Spanish tweets and 50,000 English tweets.", "labels": [], "entities": []}, {"text": "Each instance consists of a single tweet, where 19 different emojis were observed in the Spanish data, and 20 emojis were observed in the English.", "labels": [], "entities": []}, {"text": "We collected the training tweets via the Twitter API on November.", "labels": [], "entities": []}, {"text": "By that time some of the tweets selected by the organizers had been deleted or made private, but we were still able to download the vast majority of training tweets.", "labels": [], "entities": []}, {"text": "For English we downloaded 490,272 tweets, so 9,628 were unavailable.", "labels": [], "entities": []}, {"text": "For Spanish we obtained 98,657 tweets, so 1,343 were unavailable.", "labels": [], "entities": []}, {"text": "One of the most striking aspects of this data for both English and Spanish is that the number of classes (emojis) is relatively large (20 and 19 respectively), and that the distribution of these classes is skewed.", "labels": [], "entities": []}, {"text": "In the English training data the most common emoji is which occurs 21.7% of the time.", "labels": [], "entities": [{"text": "English training data", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.7331076065699259}]}, {"text": "The next most frequent emoji is which occurs 10.5% of the time.", "labels": [], "entities": []}, {"text": "The two emojis were also the most frequent in the Spanish data, occurring 19.9% and 13.7% of the time.", "labels": [], "entities": [{"text": "Spanish data", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8220121264457703}]}, {"text": "By contrast 16 of the emojis occurred less than 5% of the time in English, and 14 occurred less than 5% in the Spanish.", "labels": [], "entities": []}, {"text": "The evaluation measure used to rank systems in this task is the F-measure, which rewards systems that are able to predict instances in the low frequency classes correctly.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.994627058506012}]}, {"text": "Given that we decided to employ oversampling in order to try to improve our results on the low frequency classes which had the negative effect of degrading our performance on the more frequent classes.", "labels": [], "entities": []}, {"text": "For individual classes, F 1 score is calculated as: The overall classification performance of the system is measured by macro-averaged F 1 score: where k is total number of classes.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9906873901685079}, {"text": "F 1 score", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9449369510014852}]}, {"text": "For task evaluation, our two submitted systems are: Ensemble1 and Meta.", "labels": [], "entities": [{"text": "task evaluation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7339105606079102}]}, {"text": "The official results are shown for English in.", "labels": [], "entities": []}, {"text": "Our overall F-score was competitive in both the English (19th of 48) and Spanish tasks (5th of 21).", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9969679713249207}]}, {"text": "In post-evaluation experiments, we revised the preprocessing by including most non-ASCII characters and modified the weights assigned for ensembles.", "labels": [], "entities": []}, {"text": "As a result, the system performance improved greatly, which was largely attributed to the changes in preprocessing.", "labels": [], "entities": []}, {"text": "The system was trained on the whole training data, and tested with the official test data.", "labels": [], "entities": []}, {"text": "We show post-evaluation results for English in and for Spanish in.", "labels": [], "entities": []}, {"text": "The confusion matrices of Meta classifier are shown in and.", "labels": [], "entities": []}, {"text": "In the task evaluation we eliminated all non-ASCII characters during preprocessing.", "labels": [], "entities": []}, {"text": "After the evaluation period we realized that this resulted in a significant loss of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9987282156944275}]}, {"text": "We revised our preprocessing for our post-evaluation experiments and only removed the following non-ASCII characters (shown as Unicode value, description): (U+00B7, middle dot), (U+2019, right single quotation mark), (U+2018, left single quotation mark), (U+2022, bullet), (U+2026, horizontal ellipsis), and (U+30FB, katakana middle dot).", "labels": [], "entities": []}, {"text": "Preserving non-ASCIIs is important for both languages.", "labels": [], "entities": []}, {"text": "Spanish using MNB has a F-score of 16.77 without non-ASCII and 19.08 when preserving all, and English has 25.47 without non-ASCII and 30.00 when including.", "labels": [], "entities": [{"text": "MNB", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.7649759650230408}, {"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9993010759353638}]}, {"text": "While their importance for Spanish is apparent as accents are ubiquitous in Latin languages, their function for English is relatively vague.", "labels": [], "entities": []}, {"text": "Nevertheless, they are clearly providing useful information in the tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English Task Evaluation Results", "labels": [], "entities": [{"text": "English Task Evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7114372849464417}]}, {"text": " Table 2: Spanish Task Evaluation Results", "labels": [], "entities": [{"text": "Spanish Task Evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6377392411231995}]}, {"text": " Table 3: English Post-Evaluation Results", "labels": [], "entities": []}, {"text": " Table 4: Spanish Post-Evaluation Results", "labels": [], "entities": [{"text": "Spanish Post-Evaluation", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.804022341966629}]}]}