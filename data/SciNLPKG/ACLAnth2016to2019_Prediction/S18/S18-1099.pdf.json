{"title": [{"text": "KLUEnicorn at SemEval-2018 Task 3: A Na\u00a8\u0131veNa\u00a8\u0131ve Approach to Irony Detection", "labels": [], "entities": [{"text": "A Na\u00a8\u0131veNa\u00a8\u0131ve Approach", "start_pos": 35, "end_pos": 58, "type": "METRIC", "confidence": 0.7268123073237283}, {"text": "Irony Detection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.611799344420433}]}], "abstractContent": [{"text": "This paper describes the KLUEnicorn system submitted to the SemEval-2018 task on \"Irony detection in English tweets\".", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7054485082626343}]}, {"text": "The proposed system uses a na\u00a8\u0131vena\u00a8\u0131ve Bayes classifier to exploit rather simple lexical, pragmatic and semantic features as well as sentiment.", "labels": [], "entities": []}, {"text": "It further takes a closer look at different adverb categories and named entities and factors in word-embedding information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic irony and sarcasm detection has made great advances in recent years, evolving from considering purely lexical information ( to sentiment) and semantics ().", "labels": [], "entities": [{"text": "Automatic irony and sarcasm detection", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7008798599243165}]}, {"text": "With new approaches that are aware of the context a tweet is produced in, promising results of as much as 87% accuracy () have been achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9968030452728271}]}, {"text": "In the following sections, I present a constrained contribution to the SemEval-2018 irony detection task.", "labels": [], "entities": [{"text": "SemEval-2018 irony detection task", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.9056941717863083}]}, {"text": "As useful context for the training data was rather hard to come by, a solely tweet based approach is explored.", "labels": [], "entities": []}, {"text": "In the next section, the dataset provided by the task organizers will be discussed.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 will elaborate on data preprocessing and the types of features that were tested.", "labels": [], "entities": []}, {"text": "Finally, sections 5, 6 and 7 will present experiments on the usefulness of different features to different classifiers, the settings used for the submitted systems and the competition results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to gain insight on the usefulness of the features, a set of experiments 4 was performed, in which the features were assigned specific groups and a selection of classifiers was either trained on the group alone or on all features but those in the group.", "labels": [], "entities": []}, {"text": "10-fold cross-validation was performed on the training set comparing a Gaussian na\u00a8\u0131vena\u00a8\u0131ve Bayes classifier, support vector machines, a decision tree and a random forest classifier.", "labels": [], "entities": []}, {"text": "The groups are reported in table 2.", "labels": [], "entities": []}, {"text": "Results when training on the features without the bag-of-words, displayed in table 3, show thatwith the exception of group 5 -most of the groups do not seem to make a big contribution to the rest of the feature set and their exclusion does not lead to substantial drops in performance.", "labels": [], "entities": []}, {"text": "For group 5, a decrease in performance of as much as 10% can be observed for the random forest classifier compared to the performance on all features recorded in table 4.", "labels": [], "entities": []}, {"text": "Training on selected features from just one of the groups at a time shows that groups 3 and 5 are already very informative and can produce f-scores Note that these experiments only focussed on task A.", "labels": [], "entities": [{"text": "f-scores", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9475497603416443}]}, {"text": "The bag-of-words was restricted to uni-and bigrams with a minimum document frequency of 5., the best score is still obtained when selecting from the entire feature set and training a random forest.", "labels": [], "entities": []}, {"text": "Taking a look at the importance weights assigned by the random forest classifier, it emerges that the embeddings range among the top 220 ranks and carry 91% of the importance weight.", "labels": [], "entities": []}, {"text": "They are thus quite important for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9651524424552917}]}, {"text": "Tweet length in characters is identified as the most important feature followed by positive word sentiment scores, which might indicate that the assumption by, that ironic utterances are more likely to convey negative sentiment through literally positive one, also holds for the observed tweets.", "labels": [], "entities": []}, {"text": "Regarding adverb categories, demonstrative adverbs appear to be most informative.", "labels": [], "entities": []}, {"text": "Generally, it can be noted that every group contributes to the top 250 important features with at least one or two features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Feature group description", "labels": [], "entities": []}, {"text": " Table 3: F1-score when omitting one group at a time  for binary irony detection", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996009469032288}, {"text": "binary irony detection", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7201121747493744}]}, {"text": " Table 4: F1-score when training on one group at a time  in binary irony detection", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999685525894165}]}, {"text": " Table 5: Results on test data -Task A", "labels": [], "entities": []}, {"text": " Table 6: Results on test data -Task B", "labels": [], "entities": []}, {"text": " Table 7: Example predictions KLUEnicorn -Task A", "labels": [], "entities": [{"text": "KLUEnicorn -Task A", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.4383533075451851}]}, {"text": " Table 8: Example predictions KLUEnicorn* -Task B", "labels": [], "entities": []}]}