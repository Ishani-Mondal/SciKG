{"title": [{"text": "Lyb3b at SemEval-2018 Task 12: Ensemble-based Deep Learning Models for Argument Reasoning Comprehension Task", "labels": [], "entities": [{"text": "SemEval-2018 Task 12", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7609963814417521}]}], "abstractContent": [{"text": "Reasoning is a crucial part of natural language argumentation.", "labels": [], "entities": [{"text": "natural language argumentation", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6484595239162445}]}, {"text": "In order to comprehend an argument , we have to reconstruct and analyze its reasoning.", "labels": [], "entities": []}, {"text": "In this task, given a natural language argument with a reason and a claim, the goal is to choose the correct implicit reasoning from two options, in order to form a reasonable structure of (Reason, Warrant, Claim).", "labels": [], "entities": []}, {"text": "Our approach is to build distributed word embedding of reason, warrant and claim respectively , meanwhile, we use a series of frameworks such as CNN model, LSTM model, GRU with attention model and biLSTM with attention model for processing word vector.", "labels": [], "entities": []}, {"text": "Finally, ensemble mechanism is used to integrate the results of each framework to improve the final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9946150183677673}]}, {"text": "Experiments demonstrate superior performance of ensemble mechanism compared to each separate framework.", "labels": [], "entities": []}, {"text": "We are the 11th in official results, the final model can reach a 0.568 accuracy rate on the test dataset.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.9882226288318634}]}], "introductionContent": [{"text": "Argument reasoning comprehension is a crucial part of natural language argumentation, and the realization of argument reasoning requires the understanding of the deep meaning of the text by the computer.", "labels": [], "entities": [{"text": "Argument reasoning comprehension", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8017371694246928}, {"text": "natural language argumentation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.6826101541519165}]}, {"text": "At the same time, argument reasoning is also an important evaluation criterion for the understanding of natural language by computer.", "labels": [], "entities": [{"text": "argument reasoning", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.8593479990959167}]}, {"text": "This paper is based on the argument reasoning comprehension task proposed by, which proposed a complex, yet scalable crowdsourcing process, and created anew freely licensed dataset based on authentic arguments from news comments.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.8085581511259079}]}, {"text": "The dataset consists of three parts: train dataset, validation dataset and test dataset, with the quantity being 1210, 316 and 444 respectively.", "labels": [], "entities": []}, {"text": "The task is formally defined as follow: given an argument consisting of a reason Rand a claim C along with the title and a short description of the debate they occur in, identify the correct warrant W from two candidates, the goal is to select the correct warrant W that explains reasoning of this particular argument.", "labels": [], "entities": []}, {"text": "There are only two options given and only one answer is correct.", "labels": [], "entities": []}, {"text": "The key point of the task is that it is difficult to find the answer through the shallow semantics, and the answer is usually implicit.", "labels": [], "entities": []}, {"text": "Being a binary classification task, through preliminary experiment, our approach is combining debate title and description into reason, and splitting a sample {R (with debate title and description) ; C; W 0; W 1; correct label} into two quadruples, which are {R; C; W 0; label} and {R; C; W 1; label}.", "labels": [], "entities": []}, {"text": "On the validation, we employ the same processing mode, determining the matching degree of fit between a quadruples, the highest will be chosen.", "labels": [], "entities": [{"text": "validation", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9726510047912598}]}, {"text": "The four main deep learning(DL) frameworks we employed are based on Convolutional Neural Network(CNN) model, Long Short-Term Memory(LSTM) model, GRU with attention model and Bidirectional Long Short-Term Memory (biLSTM) with attention model, which are based on utilizing word distributed representation on Rand C and W respectively, on top of models, a dense layer is used to determine the matching degree.", "labels": [], "entities": []}, {"text": "In our paper, an ensemble mechanism is introduced into neural network (NN) models, we integrate the results of each model to improve the final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9953526258468628}]}, {"text": "Experiments demonstrate superior performance of ensemble mechanism compared to each separate model.", "labels": [], "entities": []}, {"text": "For confirming the effect of ensemble method, we use each separate model as a reference.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach in this task is realized by keras, we use the accuracy on validation dataset to locate the best parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9983134269714355}]}, {"text": "All the results are taken three times, and the average value is taken.", "labels": [], "entities": []}, {"text": "In the experiment, we use the loss function of categorical cross entropy and the optimizer of adaptive moment estimation.", "labels": [], "entities": []}, {"text": "The length of reason, claim and warrant tokens sequence all take the maximum length, if the length is not enough, then zero is added.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9649954438209534}]}, {"text": "For comparison, we report the performance and analysis of four frameworks in.", "labels": [], "entities": []}, {"text": "Rows (1) to (2), list accuracy of task originators models on the validation set and the test set respectively to corresponds to our main four frameworks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9860585331916809}]}, {"text": "Row (3) achieve acceptable results on validation set compared to the baseline.", "labels": [], "entities": []}, {"text": "Row (4) has been greatly improved compared with CNN on the validation set and test set, especially on the validation set, reaching a 66.46% accuracy rate, proving that the LSTM model is more advantageous in processing sequence text.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 140, "end_pos": 153, "type": "METRIC", "confidence": 0.9893334209918976}]}, {"text": "Row (5) has a better effect on the validation set and test set, the results have exceeded to the baseline.", "labels": [], "entities": []}, {"text": "In these four major frameworks, the result of Row (6) is the most satisfying, improves over the baseline already, especially on the validation set, 4% higher than the baseline, on the test set, the accuracy rate of 57.21% is also reached.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 198, "end_pos": 211, "type": "METRIC", "confidence": 0.9899249374866486}]}, {"text": "From the, we can perceive that the attention mechanism is beneficial to improve the capability of the model.", "labels": [], "entities": []}, {"text": "We report the performance of ensemble method in.", "labels": [], "entities": []}, {"text": "As can be seen from the result, we can observe that Row (1) uses the soft voting method, which achieves a 68.35% accuracy rate on the validation set, which surpasses all the single frameworks.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.9867804050445557}]}, {"text": "The performance on the test set is also good, reaching 56.85%, more than the baseline model 1%, though it is not as good as the best result of single framework, but it is also a good result.", "labels": [], "entities": []}, {"text": "Row (2) is the hard voting method, which is slightly worse than the soft voting.", "labels": [], "entities": [{"text": "Row", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9270122647285461}]}, {"text": "The weight of Rows (3) to (5) which were found on validation set achieve the highest accuracy on the validation set, more than 69%, this is a huge improvement, but the performance on the test dataset is not the best, the reason maybe the overfitting caused by this method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9988871216773987}]}, {"text": "It needs to be emphasized that soft voting method is adopted in the actual tasks, that is, the Row (1).", "labels": [], "entities": [{"text": "Row", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9137778878211975}]}, {"text": "The other methods listed here are only theoretical discussions on the ensemble method from the perspective of research.", "labels": [], "entities": []}, {"text": "Through these experiments, we can conclude that remarkable results can be achieved through the ensemble method.", "labels": [], "entities": []}, {"text": "At the same time, the soft voting method is better than other methods, although the results of the validation dataset are not the best in three method, but the effect on test dataset is the best.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of originator and the four main frame- works which our paper employed", "labels": [], "entities": []}, {"text": " Table 2: Results of ensemble method", "labels": [], "entities": []}, {"text": " Table 2. As can be seen from the result, we can  observe that Row (1) uses the soft voting method,  which achieves a 68.35% accuracy rate on the val- idation set, which surpasses all the single frame- works. The performance on the test set is also  good, reaching 56.85%, more than the baseline  model 1%, though it is not as good as the best re- sult of single framework, but it is also a good re- sult. Row (2) is the hard voting method, which is  slightly worse than the soft voting. The weight of  Rows (3) to (5) which were found on validation set  achieve the highest accuracy on the validation set,  more than 69%, this is a huge improvement, but  the performance on the test dataset is not the best,  the reason may be the overfitting caused by this", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9982311129570007}, {"text": "accuracy", "start_pos": 575, "end_pos": 583, "type": "METRIC", "confidence": 0.9887062311172485}]}]}