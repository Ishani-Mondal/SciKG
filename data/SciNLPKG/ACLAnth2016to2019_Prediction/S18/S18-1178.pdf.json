{"title": [{"text": "Jiangnan at SemEval-2018 Task 11: Deep Neural Network with Attention Method for Machine Comprehension Task", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our submission for the International Workshop on Semantic Evaluation (SemEval-2018) shared task 11-Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).", "labels": [], "entities": [{"text": "International Workshop on Semantic Evaluation (SemEval-2018) shared task 11-Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b)", "start_pos": 44, "end_pos": 195, "type": "TASK", "confidence": 0.6870145540345799}]}, {"text": "We use a deep neural network model to choose the correct answer from the candidate answers pair when the document and question are given.", "labels": [], "entities": []}, {"text": "The interactions between document, question and answers are modeled by attention mechanism and a variety of manual features are used to improve model performance.", "labels": [], "entities": []}, {"text": "We also use CoVe (McCann et al., 2017) as an external source of knowledge which is not mentioned in the document.", "labels": [], "entities": [{"text": "CoVe (McCann et al., 2017)", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.866540215909481}]}, {"text": "As a result, our system achieves 80.91% accuracy on the test data, which is on the third place of the leaderboard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.998121440410614}]}], "introductionContent": [{"text": "In recent years, machine reading comprehension (MRC) which attempts to enable machines to answer questions when given a set of documents, has attracted great attentions.", "labels": [], "entities": [{"text": "machine reading comprehension (MRC)", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.8469303399324417}]}, {"text": "Several MRC datasets have been released such as the Stanford Question Answering Dataset (SQuAD) () and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO).", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 52, "end_pos": 95, "type": "DATASET", "confidence": 0.8846988422530038}, {"text": "Microsoft MAchine Reading COmprehension Dataset (MS-MARCO)", "start_pos": 107, "end_pos": 165, "type": "DATASET", "confidence": 0.6877715587615967}]}, {"text": "These datasets provide large scale of manually created data, greatly inspired the research in this field.", "labels": [], "entities": []}, {"text": "And a series of neural network model, such as BiDAF (), R-Net (, have achieved promising results on these evaluation tasks.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.7926138639450073}]}, {"text": "However, machine reading comprehension is still a difficult task because without knowledge, machines cannot really understand the question and make a correct answer.", "labels": [], "entities": [{"text": "machine reading comprehension", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.8740458488464355}]}, {"text": "As an effort to discover how machine reading comprehension systems would be benefited from commonsense knowledge, () developed the Machine Comprehension using Commonsense Knowledge task.", "labels": [], "entities": []}, {"text": "In this task, commonsense knowledge is given as the form of script knowledge.", "labels": [], "entities": []}, {"text": "Script knowledge is defined as the knowledge about everyday activities which is mentioned in narrative documents.", "labels": [], "entities": []}, {"text": "For each document, a series questions are asked and each question is associated with a pair of candidate answers.", "labels": [], "entities": []}, {"text": "Machines have to choose which is the correct answer.", "labels": [], "entities": []}, {"text": "To let machines make correct decisions, explicit information which can be found in the document and external commonsense knowledge are both required.", "labels": [], "entities": []}, {"text": "shows an example of the dataset in this task.", "labels": [], "entities": []}, {"text": "In this paper, we make a description about our submission system for the task.", "labels": [], "entities": []}, {"text": "The system is based on a deep neural network model.", "labels": [], "entities": []}, {"text": "The input of the model is a (document, question, answer) triple and the output is the probability that the answer is the correct one for the given document and question.", "labels": [], "entities": []}, {"text": "We also combine the neural network model with a variety of manual features, including word exact match features and token features such as part-of-speech (POS) ,named entity recognition (NER) and term frequency (TF).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 161, "end_pos": 191, "type": "TASK", "confidence": 0.7897762954235077}, {"text": "term frequency (TF)", "start_pos": 196, "end_pos": 215, "type": "METRIC", "confidence": 0.8879821300506592}]}, {"text": "These manual features are helpful in solving the problem that the correct answer can be easily found in the given document.", "labels": [], "entities": []}, {"text": "Furthermore, for more complicated problem that the answer is not explicitly mentioned in the document, we try to model the interactions between document, question and answer by computing the attention score of question to document and question to answer respectively, which is described in ().", "labels": [], "entities": []}, {"text": "These features add soft alignments between similar but non-identical words.", "labels": [], "entities": []}, {"text": "We evaluate our system on the shared task and obtain 80.91% accuracy on the test set, which is on the third place of the leaderboard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9992941617965698}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "The statistics of official training, development and test data are shown in.", "labels": [], "entities": []}, {"text": "Training Dev Test Num of examples 9,731 1,411 2,797 We remove the words occurring less than 2 times and finally get about 12000 words in the vocabulary.", "labels": [], "entities": []}, {"text": "We keep most pre-trained word embeddings fixed during training and only fine-tune the 100 most frequent words.", "labels": [], "entities": []}, {"text": "For manual features, we get POS and NER features by using Stanford CoreNLP 1 toolkits.", "labels": [], "entities": [{"text": "Stanford CoreNLP 1 toolkits", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.9378442317247391}]}, {"text": "We implement our model by using PyTorch 2 . The model is trained in the given training set and we choose the model which performs best on the development set among training epochs.", "labels": [], "entities": []}, {"text": "We train the model with mini batch size 32.", "labels": [], "entities": []}, {"text": "We use two layers BiLSTM with 128 hidden units.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.4 is applied to word embeddings and all hidden units in BiLSTM layers.", "labels": [], "entities": []}, {"text": "We use logistic loss as the loss function optimized by using Adamax optimizer () with learning rate \u03b7 = 0.002.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9256163636843363}]}], "tableCaptions": [{"text": " Table 3: Results of the single and ensemble model on  development data and test data.", "labels": [], "entities": []}, {"text": " Table 4: Ablation analysis of features.", "labels": [], "entities": [{"text": "Ablation analysis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8883553445339203}]}]}