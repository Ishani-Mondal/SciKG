{"title": [{"text": "SemEval-2018 Task 10: Capturing Discriminative Attributes", "labels": [], "entities": [{"text": "SemEval-2018 Task 10", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8661301334698995}, {"text": "Capturing Discriminative Attributes", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.9158338308334351}]}], "abstractContent": [{"text": "This paper describes the SemEval 2018 Task 10 on Capturing Discriminative Attributes.", "labels": [], "entities": [{"text": "SemEval 2018 Task 10", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8882723301649094}, {"text": "Capturing Discriminative Attributes", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.8326196273167928}]}, {"text": "Participants were asked to identify whether an attribute could help discriminate between two concepts.", "labels": [], "entities": []}, {"text": "For example, a successful system should determine that urine is a discriminating feature in the word pair kidney,bone.", "labels": [], "entities": []}, {"text": "The aim of the task is to better evaluate the capabilities of state of the art semantic models, beyond pure semantic similarity.", "labels": [], "entities": []}, {"text": "The task attracted submissions from 21 teams, and the best system achieved a 0.75 F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9861809611320496}]}], "introductionContent": [{"text": "State of the art semantic models do an excellent job at detecting semantic similarity, a traditional semantic task; for example, they can tell us that cappuccino, espresso and americano are similar to each other.", "labels": [], "entities": [{"text": "detecting semantic similarity", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.7610382835070292}]}, {"text": "It is obvious, however, that no model can claim to capture semantic competence if it does not, in addition to similarity, predict semantic differences between words.", "labels": [], "entities": []}, {"text": "If one can tell that americano is similar to cappuccino and espresso but cannot tell the difference between them, one only has a very approximate idea of the meaning of these words.", "labels": [], "entities": []}, {"text": "As a step beyond similarity, one should at the very least recognize that americano is bigger than espresso, and that capuccino contains milk foam.", "labels": [], "entities": []}, {"text": "In this spirit, we present Semeval 2018 Task 10 (Capturing Discriminative Attributes) as anew challenge for lexical semantic models.", "labels": [], "entities": [{"text": "Semeval 2018 Task 10 (Capturing Discriminative Attributes)", "start_pos": 27, "end_pos": 85, "type": "TASK", "confidence": 0.6983644829856025}]}], "datasetContent": [{"text": "The final dataset consists of 22884 items, divided into: 1.", "labels": [], "entities": []}, {"text": "A training set of 17782 examples with 515 distinct concepts and 1292 distinct features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Composition of the manually validated part of  the dataset", "labels": [], "entities": []}, {"text": " Table 4: Total size of the final dataset.", "labels": [], "entities": [{"text": "Total size", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8522448539733887}]}, {"text": " Table 5: Number of correct and incorrect classifications  for the test set using the cosine baseline.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9706503748893738}]}, {"text": " Table 6: Codalab competition results, compared to  baselines and the human-based performance ceiling.", "labels": [], "entities": []}, {"text": " Table 7: Average and best F1 score per system type.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9917336702346802}, {"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9863904416561127}]}, {"text": " Table 8: Average F1 score per resource type (KB =  Knowledge Base, WE = Word Embeddings).", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9580662846565247}, {"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9275480806827545}]}, {"text": " Table 9: Results of human annotation of the Easy,  Hard and Hardest subsets of the test data.", "labels": [], "entities": []}, {"text": " Table 10: Example label and source distribution for  the Easy and Hard subsets of the test data.", "labels": [], "entities": []}]}