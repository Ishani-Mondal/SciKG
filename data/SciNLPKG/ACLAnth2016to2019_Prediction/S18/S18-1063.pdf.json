{"title": [{"text": "THU NGN at SemEval-2018 Task 2: Residual CNN-LSTM Network with Attention for English Emoji Prediction", "labels": [], "entities": [{"text": "THU NGN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6520794630050659}, {"text": "English Emoji Prediction", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.5190994242827097}]}], "abstractContent": [{"text": "Emojis are widely used by social media and social network users when posting their messages.", "labels": [], "entities": []}, {"text": "It is important to study the relationships between messages and emojis.", "labels": [], "entities": []}, {"text": "Thus, in SemEval-2018 Task 2 an interesting and challenging task is proposed, i.e., predicting which emojis are evoked by text-based tweets.", "labels": [], "entities": []}, {"text": "We propose a residual CNN-LSTM with attention (RCLA) model for this task.", "labels": [], "entities": []}, {"text": "Our model combines CNN and LSTM layers to capture both local and long-range contextual information for tweet representation.", "labels": [], "entities": [{"text": "tweet representation", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.8449966311454773}]}, {"text": "In addition, attention mechanism is used to select important components.", "labels": [], "entities": []}, {"text": "Besides, residual connection is applied to CNN layers to facilitate the training of neural networks.", "labels": [], "entities": []}, {"text": "We also incorporated additional features such as POS tags and sentiment features extracted from lexicons.", "labels": [], "entities": []}, {"text": "Our model achieved 30.25% macro-averaged F-score in the first subtask (i.e., emoji prediction in En-glish), ranking 7 th out of 48 participants.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.958259105682373}, {"text": "emoji prediction", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.6521858870983124}]}], "introductionContent": [{"text": "Emojis such as and are widely used in social media and social network messages such as tweets.", "labels": [], "entities": []}, {"text": "They are frequently combined with plain texts to visually complement the meaning of a message and convey various opinions and emotions (.", "labels": [], "entities": []}, {"text": "Social media platforms such as Twitter has accumulated a large number of emoji-incorporated messages.", "labels": [], "entities": []}, {"text": "Analyzing the relationships between the textual message and emojis has many potential applications, such as emoji recommendation, automatic emoji-enriched message generation, and accurate sentiment analysis of social media messages (.", "labels": [], "entities": [{"text": "emoji recommendation", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8712438941001892}, {"text": "emoji-enriched message generation", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.6118147770563761}, {"text": "sentiment analysis of social media messages", "start_pos": 188, "end_pos": 231, "type": "TASK", "confidence": 0.8589194019635519}]}, {"text": "However, the research on the relationships between textual message and emojis is limited.", "labels": [], "entities": []}, {"text": "Existing studies on emojis mainly focus on analyzing the semantics, usage or sentiment of emojis (.", "labels": [], "entities": []}, {"text": "For example, explored the meaning and usage of emojis across different languages.", "labels": [], "entities": []}, {"text": "proposed to utilize the emoji sense definitions to improve the performance of emoji embedding model.", "labels": [], "entities": []}, {"text": "However, these approaches cannot reveal the interplay between plain texts and emojis.", "labels": [], "entities": []}, {"text": "In order to fill this gap, proposed a novel task to predict which emojis are evoked by text-based tweets.", "labels": [], "entities": []}, {"text": "For example, given a tweet message \"Love my coworkers ! @user\", a system is required to predict that emoji is associated with this tweet.", "labels": [], "entities": []}, {"text": "As an extension, the SemEval-2018 Task 2 1 aims to predict emojis for English and Spanish tweets (.", "labels": [], "entities": []}, {"text": "Given a plain tweet message without emoji, systems are required to predict which emoji is evoked by this message.", "labels": [], "entities": []}, {"text": "We proposed a residual CNN-LSTM with attention model (RCLA) for this task.", "labels": [], "entities": []}, {"text": "Our model combines LSTM and multi-level CNN layers to capture both long-range and local information to learn tweet representation.", "labels": [], "entities": [{"text": "tweet representation", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.847243458032608}]}, {"text": "In addition, attention mechanism () is incorporated into our approach to select important components.", "labels": [], "entities": []}, {"text": "Besides, we applied residual connection technique (  to CNN layers in our model to facilitate the training of neural networks.", "labels": [], "entities": []}, {"text": "We also incorporated additional features such as POS tags and sentiment features extracted from sentiment lexicons.", "labels": [], "entities": []}, {"text": "Our model achieved 30.25% macro-averaged F-score on the test data of the first subtask (i.e., emoji prediction in English), and ranked 7 th out of 48 participants.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9725176095962524}, {"text": "emoji prediction in English", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.7713401168584824}]}], "datasetContent": [{"text": "The dataset 6 for this task is collected from Twitter.", "labels": [], "entities": []}, {"text": "There are 20 emojis in total.", "labels": [], "entities": []}, {"text": "489,277 tweets are used for model training.", "labels": [], "entities": [{"text": "model training", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8601692020893097}]}, {"text": "The number of tweets in the trial and test sets are both 50,000.", "labels": [], "entities": []}, {"text": "We used the pre-trained word embeddings provided by.", "labels": [], "entities": []}, {"text": "They were trained on 20 million geo-localized tweets and their dimension is 300.", "labels": [], "entities": [{"text": "dimension", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9657328724861145}]}, {"text": "These word embedding were fine-tuned during model training.", "labels": [], "entities": []}, {"text": "The hyperparameters in our model were selected via cross-validation on the trail set.", "labels": [], "entities": []}, {"text": "More specifically, the dimension of Bi-LSTM hidden states is 300, the window sizes of CNN filters are 2, 3, 4 respectively.", "labels": [], "entities": []}, {"text": "The number of CNN filters is 200 and the number of sentiment features is 45.", "labels": [], "entities": []}, {"text": "The dimension of dense layer is 300, and the dropout rate is 0.2 for each layer.", "labels": [], "entities": []}, {"text": "The batch size is 500, and the maximal training epoch is set to 100.", "labels": [], "entities": [{"text": "maximal training epoch", "start_pos": 31, "end_pos": 53, "type": "METRIC", "confidence": 0.843161424001058}]}, {"text": "We use RMSProp as the optimizer for network training.", "labels": [], "entities": []}, {"text": "The performance is evaluated by macro-averaged F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9719566702842712}]}, {"text": "The performance of our model on the test set is shown in In addition, the performance of our approach on some infrequent emojis is also satisfactory.", "labels": [], "entities": []}, {"text": "For example, the F-score on emoji is high.", "labels": [], "entities": [{"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9992820620536804}]}, {"text": "This is probably because specific words such as \"Christmas\" are frequently associated with this emoji, making it relatively easy to predict.: Precision, Recall, F-score and percentage of occurrences of each emoji in the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9991003274917603}, {"text": "Recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9991446733474731}, {"text": "F-score", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.999097466468811}]}, {"text": "The visualization of the confusion matrix of our model is shown in.", "labels": [], "entities": []}, {"text": "From this figure, we find two pairs of emojis which are difficult for our model to discriminate between them.", "labels": [], "entities": []}, {"text": "The emoji is often wrongly identified as . This is probably because these two emojis are often used to express similar meaning and feelings.", "labels": [], "entities": []}, {"text": "For example, they both can be used in tweets which convey happy emotion.", "labels": [], "entities": []}, {"text": "Another pair of emojis is and . These two emojis look quite similar, and discriminating them is quite difficult.", "labels": [], "entities": []}, {"text": "In order to further validate the effectiveness of our model, we compare the performance of our model with several baseline methods.", "labels": [], "entities": []}, {"text": "The methods to be compared include: 1) LSTM, using Bi-LSTM for tweet presentation; 2) CNN, 3-layer CNN without residual connections; 3) CNN-LSTM (denoted as CL), using the combination of LSTM and CNN; 4) Residual CNN-LSTM (denoted as RCL), CNN-LSTM with residual connections; 5) Residual CNN-LSTM with attention (denoted as RCLA).", "labels": [], "entities": []}, {"text": "The results are shown in Table 2.", "labels": [], "entities": []}, {"text": "According to, the combination of LSTM and CNN (CL) usually outperforms the single CNN and LSTM.", "labels": [], "entities": []}, {"text": "It indicates that combining CNN and LSTM to capture both local and longrange context information is beneficial for tweet emoji prediction.", "labels": [], "entities": [{"text": "CNN", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8977075815200806}, {"text": "tweet emoji prediction", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.8202194372812907}]}, {"text": "In addition, by comparing RCL with CL, we find that the residual connections can improve the performance of CL model.", "labels": [], "entities": []}, {"text": "It shows that the residual connections can facilitate the training of neural networks.", "labels": [], "entities": []}, {"text": "Besides, the attention mechanism can also significantly improve the performance.", "labels": [], "entities": []}, {"text": "It validates that employing attention mechanism to capture the important contexts for emoji prediction is useful.", "labels": [], "entities": [{"text": "emoji prediction", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.9290879368782043}]}], "tableCaptions": [{"text": " Table 1: Precision, Recall, F-score and percentage of  occurrences of each emoji in the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995473027229309}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988681077957153}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9984971284866333}]}, {"text": " Table 2: The F-score of each emoji and the macro-F of  different methods.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9964924454689026}]}, {"text": " Table 3: Influence of POS tags and sentiment features.", "labels": [], "entities": []}]}