{"title": [{"text": "OhioState at SemEval-2018 Task 7: Exploiting Data Augmentation for Relation Classification in Scientific Papers Using Piecewise Convolutional Neural Networks", "labels": [], "entities": [{"text": "OhioState", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9729141592979431}, {"text": "Exploiting Data Augmentation", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.6369976898034414}, {"text": "Relation Classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.9634582996368408}]}], "abstractContent": [{"text": "We describe our system for SemEval-2018 Shared Task on Semantic Relation Extraction and Classification in Scientific Papers where we focus on the Classification task.", "labels": [], "entities": [{"text": "SemEval-2018 Shared Task on Semantic Relation Extraction and Classification", "start_pos": 27, "end_pos": 102, "type": "TASK", "confidence": 0.7764711346891191}, {"text": "Classification task", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.901131272315979}]}, {"text": "Our simple piecewise convolution neural network (PCNN) performs decently in an end to end manner.", "labels": [], "entities": []}, {"text": "A simple inter-task data augmentation significantly boosts the performance of the model.", "labels": [], "entities": []}, {"text": "Our best-performing systems stood 8th out of 20 teams on the classification task on noisy data and 12th out of 28 teams on the classification task on clean data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction (RE) and Classification (RC) is an integral component of information extraction systems which aim to extract all the entity pairs and their relation e 1 , r, e 2 from a given text corpora.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8666429638862609}, {"text": "Classification (RC)", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7922828793525696}, {"text": "information extraction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7376516312360764}]}, {"text": "An alternate formulation of relation extraction task focuses on identifying if a relation exists between a predefined pair of entities, and if yes classify from a given set of class relations.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8530186216036478}]}, {"text": "RE finds applications in a variety of domains, ranging from knowledge base construction to semantic parsing and question answering.", "labels": [], "entities": [{"text": "RE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9471027255058289}, {"text": "knowledge base construction", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6511838932832082}, {"text": "semantic parsing", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7739081084728241}, {"text": "question answering", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.8921951949596405}]}, {"text": "However, the applicability of existing efforts in relation extraction to scientific text calls fora quantitative and qualitative analysis which is the aim of this shared task.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7701933979988098}]}], "datasetContent": [{"text": "The data contains titles and abstracts of papers from ACL Anthology Corpus where entity mentions are either manually annotated (Subtask 1.1 and Subtask 2) or heuristically (Subtask 1.2) determined.", "labels": [], "entities": [{"text": "ACL Anthology Corpus", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9557964205741882}]}, {"text": "However, the relations are manually annotated across all subtasks.", "labels": [], "entities": []}, {"text": "For the classification scenario, we are provided with relevant entities and the directionality of their relation.", "labels": [], "entities": []}, {"text": "There are 6 class labels: USAGE, RE-SULT, MODEL, PART WHOLE, TOPIC, COM-PARISON.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.7167892456054688}, {"text": "RE-SULT", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9956591129302979}, {"text": "MODEL", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9910353422164917}, {"text": "PART WHOLE", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.8215048313140869}, {"text": "TOPIC", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9238999485969543}, {"text": "COM-PARISON", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.7303481101989746}]}, {"text": "The classes are highly imbalanced in nature as shown in 3  For both Task 1.1 and 1.2, given that the classes are imbalanced, macro-f1 score is used as the official evaluation metric and thus the metric we use for hyperparameter tuning.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 213, "end_pos": 234, "type": "TASK", "confidence": 0.7618466913700104}]}, {"text": "For more details, we  would refer the reader to the task description paper (  While the final training and prediction was performed on the entire training dataset, we use the official validation split provided by contest organizers to perform hyper-parameter tuning.", "labels": [], "entities": []}, {"text": "For the data augmentation scenario, however, we also make use of the validation data from the other task.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7554075121879578}]}, {"text": "Given that CNN's are fast to train, we easily use grid search to find the optimal combination of a subset of parameters for each task and each data configuration (with or without augmentation) which are listed in.", "labels": [], "entities": []}, {"text": "For the remaining parameters, we used standard values as recommended by prior literature as follows: convolution filters of width 3,4 and 5; position and directionality embeddings of size 5; windows size for relative positions from entities was set to 30.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of our best performing systems on the official test set with/without data augmentation.", "labels": [], "entities": [{"text": "official test set", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8595953981081644}]}, {"text": " Table  2. Even a simple mixing of the two datasets which  differ significantly in the nature of tagged entities  lead to a significant improvement. Surprisingly  though, adding the noisy data to the clean dataset  also leads to a 36% increase in performance. This  could be attributed to the fact that while heuris- tically annotated entities are high-level concepts  thus sharing a lot of context with similar concepts,  most of the manually annotated entities are full  noun phrases, thus adding to the complexity of the", "labels": [], "entities": []}]}