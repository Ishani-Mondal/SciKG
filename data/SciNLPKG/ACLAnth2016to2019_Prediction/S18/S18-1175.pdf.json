{"title": [{"text": "ECNU at SemEval-2018 Task 11: Using Deep Learning Method to Address Machine Comprehension Task", "labels": [], "entities": [{"text": "ECNU at SemEval-2018 Task 11", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7708562970161438}]}], "abstractContent": [{"text": "This paper describes the system we submitted to the Task 11 in SemEval 2018, i.e., Machine Comprehension using Commonsense Knowledge.", "labels": [], "entities": []}, {"text": "Given a passage and some questions that each have two candidate answers, this task requires the participate system to select out one answer meet the meaning of original text or commonsense knowledge from the candidate answers.", "labels": [], "entities": []}, {"text": "For this task, we use a deep learning method to obtain final predict answer by calculating relevance of choices representations and question-aware document representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the presentation of challenge and large-scale reading comprehension corpora has driven the development of technology for machine reading comprehension, and most of these machine comprehension datasets do not need commonsense knowledge to answer questions.", "labels": [], "entities": []}, {"text": "The purpose of Machine Comprehension using Commonsense Knowledge task in Semeval 2018 is to provide a platform for finding away for the machine to better understand the text and enable the machine answer questions based on the text, and encourage participants to make use any external resources (e.g., DeScript, narrative chains, Wikipedia, etc) to improve the system performance ().", "labels": [], "entities": []}, {"text": "The task 11 is a multiple-choice machine comprehension, which requires a system read a narrative text about everyday activities () and then answer multiple-choice questions based on this text.", "labels": [], "entities": []}, {"text": "Some questions need to be answered according to the original text, and others can be answered by commonsense knowledge.", "labels": [], "entities": []}, {"text": "Each question is associated with a set of two answers.", "labels": [], "entities": []}, {"text": "gives an example of the dataset.", "labels": [], "entities": []}, {"text": "To address this machine comprehension task, we utilized rule-based methods and a deep learndocument: Early this morning I woke up to the sound of my newspaper landing on my driveway.", "labels": [], "entities": []}, {"text": "I sat up and wrapped my pink robe around me.", "labels": [], "entities": []}, {"text": "I slipped my feet into my slippers and looked at the clock.", "labels": [], "entities": []}, {"text": "It was only 7:00 but it was time for me to get my newspaper and drink some coffee.", "labels": [], "entities": []}, {"text": "I looked out the window and noticed it was raining quite a bit.", "labels": [], "entities": []}, {"text": "I saw the newspaper at the end of my driveway, as faraway as it could be.", "labels": [], "entities": []}, {"text": "I grabbed my umbrella out of my coat closet and opened my front door enough to stick the umbrella through and open it outside.", "labels": [], "entities": []}, {"text": "I stepped out the door and quickly covered my head with the umbrella.", "labels": [], "entities": []}, {"text": "Then Iran to the end of my driveway, scooped up the newspaper in its plastic wrapping, and ran back to my front door.", "labels": [], "entities": []}, {"text": "I closed my umbrella, took off my slippers, and dried off.", "labels": [], "entities": []}, {"text": "Then, I unwrapped my newspaper and sat down to read it. question: Do they read the paper daily?", "labels": [], "entities": []}, {"text": "No they usually watch TV in the mornings.", "labels": [], "entities": []}, {"text": "1. Yes answer: 1 ing method.", "labels": [], "entities": [{"text": "Yes answer", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.8828316628932953}]}, {"text": "Our final submission use GatedAttention Reader () to fuse question information into document and acquire a question-awared document representation, the degree of interaction between choices and document are regard as the probabilities of choices being returned as an answer.", "labels": [], "entities": []}, {"text": "The above two methods do not use additional commonsense knowledge, which may lead to the poor preformance of our system.", "labels": [], "entities": []}, {"text": "In future work, we may explore more methods to integrate common knowledge into models.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our systems.", "labels": [], "entities": []}, {"text": "Section 3 describes datasets, experimental setting and analyse results on datasets.", "labels": [], "entities": []}, {"text": "Finally, Section 4 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the system performance, the official evaluation criterion is accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9991999268531799}]}, {"text": "For rule-based baselines, we first converted words into their lowercase and then performed tokenization and stemming using Stanford CoreNLP 2 . For deep learning system, we use 300-D pretrained word vectors provided by GloVe 3 as initial word embedding, which are fine-tuned during training.", "labels": [], "entities": [{"text": "Stanford CoreNLP 2", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.9331915577252706}]}, {"text": "The encoding layer use one layer biGRU with 128-dims hidden size to encoder texts.", "labels": [], "entities": []}, {"text": "Learning rate is 0.3, droprate is 0.5, epoch is 100, and num of multi-hops is 2.", "labels": [], "entities": [{"text": "Learning rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9614223837852478}, {"text": "droprate", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9510737061500549}, {"text": "epoch", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.997061550617218}, {"text": "num", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9662320017814636}]}, {"text": "We use cross entropy and vanilla stochastic gradient descent (SGD) to train our models.", "labels": [], "entities": []}, {"text": "shows the results of Task 11 with different methods on dev dataset, where \"GA(biGRU)\" denotes the final system we submit, \"GA(biLSTM)\" represents the experiment that we replace all bi-GRU units in the system with biLSTM units, \"GA \u2212f match \" represents the system without 5-dim match feature, \"#text\" and \"#commonsens\" represent the accuracy under different question types, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 333, "end_pos": 341, "type": "METRIC", "confidence": 0.9992181062698364}]}, {"text": "Based on above experimental results, we find that the performance of GA system is much better than rule-based approaches, this is because 2 https://stanfordnlp.github.io/CoreNLP/ 3 http://nlp.stanford.edu/data/wordvecs/glove.6B.zip the multi-hop structure merges the information of the question and the document repeatedly which is helpful to select final answer, unlike the rulebased approach that considers only word matching within a window-size distance.", "labels": [], "entities": []}, {"text": "Furthermore, we find that the improved SW + WD algorithm is better than SW + WD algorithm, because the improved SW + WD algorithm considers the degree of word matching at different distances.", "labels": [], "entities": []}, {"text": "From the GA system results, we find the performance of using biGRU units is better than that of biLSTM units and matching features also improves the system performance.", "labels": [], "entities": []}, {"text": "Compare the accuracy of different types of questions under different methods, we find that the rule-based approaches considers only the word-matched features lead to lower accuracy on the commonsense type questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9982483386993408}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9985066056251526}]}, {"text": "GA systems perform better than rule-based systems on both types of questions, because the GA system takes into account the semantic similarity of the question-aware document and choices.", "labels": [], "entities": []}, {"text": "Further, there are some commonsense types questions which the document content does not clearly indicate the correct answer but clearly does not meet the meaning of wrong answer.", "labels": [], "entities": []}, {"text": "This maybe the reason why we did not use external resources but the accuracy of the commonsense type question predicted by GA system is improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993618130683899}]}, {"text": "The final result we submitted is generated by GA system used biGRU units, the specific configuration of which is mentioned in Section 3.2.", "labels": [], "entities": []}, {"text": "Compared with the top ranked systems, there is much room for improvement in our work.", "labels": [], "entities": []}, {"text": "In addition, the use of external knowledge resources by the system also have an impact on system performance because there are about 26% commonsense type questions in the dataset.", "labels": [], "entities": []}, {"text": "This is where our system lacks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The statistics of data sets in training, develop- ment and test data.", "labels": [], "entities": []}, {"text": " Table 3: The results on dev.", "labels": [], "entities": []}, {"text": " Table 4: Our result and the top three results on test sets.", "labels": [], "entities": []}]}