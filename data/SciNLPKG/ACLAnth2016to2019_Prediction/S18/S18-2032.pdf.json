{"title": [{"text": "Multiplicative Tree-Structured Long Short-Term Memory Networks for Semantic Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Tree-structured LSTMs have shown advantages in learning semantic representations by exploiting syntactic information.", "labels": [], "entities": []}, {"text": "Most existing methods model tree structures by bottom-up combinations of constituent nodes using the same shared compositional function and often making use of input word information only.", "labels": [], "entities": []}, {"text": "The inability to capture the richness of com-positionality makes these models lack expressive power.", "labels": [], "entities": []}, {"text": "In this paper, we propose multi-plicative tree-structured LSTMs to tackle this problem.", "labels": [], "entities": []}, {"text": "Our model makes use of not only word information but also relation information between words.", "labels": [], "entities": []}, {"text": "It is more expressive, as different combination functions can be used for each child node.", "labels": [], "entities": []}, {"text": "In addition to syntactic trees, we also investigate the use of Abstract Meaning Representation in tree-structured models, in order to incorporate both syntactic and semantic information from the sentence.", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.5993002752463022}]}, {"text": "Experimental results on common NLP tasks show the proposed models lead to better sentence representation and AMR brings benefits in complex tasks.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7518306970596313}]}], "introductionContent": [{"text": "Learning the distributed representation for long spans of text from its constituents has been a crucial step of various NLP tasks such as text classification (, semantic matching (, and machine translation ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7926745712757111}, {"text": "semantic matching", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7459099292755127}, {"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7790264189243317}]}, {"text": "Seminal work uses recurrent neural networks (RNN), convolutional neural networks, and tree-structured neural networks) for sequence and tree modeling.", "labels": [], "entities": [{"text": "sequence and tree modeling", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.6066266670823097}]}, {"text": "Long Short-Term Memory (LSTM)) networks area type of recurrent neural net- * Work done as an intern at Amazon.", "labels": [], "entities": []}, {"text": "work that are capable of learning long-term dependencies across sequences and have achieved significant improvements in a variety of sequence tasks.", "labels": [], "entities": []}, {"text": "LSTM has been extended to model tree structures (e.g., TreeLSTM) and produced promising results in tasks such as sentiment classification) and relation extraction.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.8927052021026611}, {"text": "relation extraction", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.9324304163455963}]}, {"text": "shows the topologies of the conventional chain-structured LSTM) and the, illustrating the input (x), cell (c) and hidden node (h) at a time step t.", "labels": [], "entities": []}, {"text": "The key difference between (a) and (b) is the branching factor.", "labels": [], "entities": []}, {"text": "While a cell in the sequential LSTM only depends on the single previous hidden node, a cell in the tree-structured LSTM depends on the hidden states of child nodes.", "labels": [], "entities": []}, {"text": "Despite their success, the tree-structured models have a limitation in their inability to fully capture the richness of compositionality (.", "labels": [], "entities": []}, {"text": "The same combination function is used for all kinds of semantic compositions, though the compositions have different characteristics in nature.", "labels": [], "entities": []}, {"text": "For example, the composition of the adjective and the noun differs significantly from the composition of the verb and the noun.", "labels": [], "entities": []}, {"text": "To alleviate this problem, some researchers propose to use multiple compositional functions, which are predefined according to some partition criterion).", "labels": [], "entities": []}, {"text": "defined different compositional functions in terms of syntactic categories, and a suitable compositional function is selected based on the syntactic categories.", "labels": [], "entities": []}, {"text": "introduced multiple compositional functions and a proper one is selected based on the input information.", "labels": [], "entities": []}, {"text": "These models accomplished their objective to a certain extent but they still face critical challenges.", "labels": [], "entities": []}, {"text": "The predefined compositional functions cannot coverall the compositional rules and they add much more learnable parameters, bearing the risk of overfitting.", "labels": [], "entities": []}, {"text": "In this paper, we propose multiplicative TreeL-STM, an extension to the TreeLSTM model, which injects relation information into every node in the tree.", "labels": [], "entities": []}, {"text": "It allows the model to have different semantic composition matrices to combine child nodes.", "labels": [], "entities": []}, {"text": "To reduce the model complexity and keep the number of parameters manageable, we define the composition matrices using the product of two dense matrices shared across relations, with an intermediate diagonal matrix that is relation dependent.", "labels": [], "entities": []}, {"text": "Though the syntactic-based models have shown to be promising for compositional semantics, they do not make full use of the linguistic information.", "labels": [], "entities": [{"text": "compositional semantics", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.847315102815628}]}, {"text": "For example, semantic nodes are often the argument of more than one predicate (e.g., coreference) and it is generally useful to exclude semantically vacuous words like articles or complementizers, i.e., leave nodes unattached that do not add further meaning to the resulting representations.", "labels": [], "entities": []}, {"text": "Recently, introduced Abstract Meaning Representation (AMR), single rooted, directed, acyclic graphs that incorporate semantic roles, correference, negation, and other linguistic phenomena.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8013076583544413}]}, {"text": "In this paper, we investigate a combination of the semantic process provided by TreeLSTM model with the lexical semantic representation of the AMR formalism.", "labels": [], "entities": []}, {"text": "This differs from most of existing work in this area, where syntactic rather than semantic information is incorporated to the tree-structured models.", "labels": [], "entities": []}, {"text": "We seek to answer the question: To what extent can we do better with AMR as opposed to syntactic representations, such as constituent and dependency trees, in tree-structured models?", "labels": [], "entities": []}, {"text": "We evaluate the proposed models on three common tasks: sentiment classification, sentence relatedness, and natural language inference.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.9623042643070221}, {"text": "sentence relatedness", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.724460244178772}]}, {"text": "The results show that the multiplicative TreeLSTM models outperform TreeLSTM models on the same tree structures.", "labels": [], "entities": []}, {"text": "The results further suggest that using AMR as the backbone for tree-structured models is helpful in the complex task such as for longer sentences in natural language inference but not in sentiment classification, where lexical information alone suffices.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 187, "end_pos": 211, "type": "TASK", "confidence": 0.9594628214836121}]}, {"text": "In short, our contribution is twofold: 1.", "labels": [], "entities": []}, {"text": "We propose the new multiplicative TreeL-STM model that effectively learns distributed representation of a given sentence from its constituents, utilizing not only the lexical information of words, but also the relation information between the words.", "labels": [], "entities": []}, {"text": "2. We conduct an extensive investigation on the usefulness of lexical semantic representation induced by AMR formalism in treestructured models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Results on the SNLI dataset. The first  group contains results of some best-performing  tree-structured LSTM models on this data. (*: a  preprint)", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8293344676494598}]}]}