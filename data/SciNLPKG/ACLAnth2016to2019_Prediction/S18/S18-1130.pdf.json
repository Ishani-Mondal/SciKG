{"title": [{"text": "Bf3R at SemEval-2018 Task 7: Evaluating Two Relation Extraction Tools for Finding Semantic Relations in Biomedical Abstracts", "labels": [], "entities": [{"text": "Evaluating Two Relation Extraction Tools", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.7396902799606323}]}], "abstractContent": [{"text": "Automatic extraction of semantic relations from text can support finding relevant information from scientific publications.", "labels": [], "entities": []}, {"text": "We describe our participation in Task 7 of SemEval-2018 for which we experimented with two relations extraction tools-jSRE and TEES-for the extraction and classification of six relation types.", "labels": [], "entities": [{"text": "relations extraction", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7284553050994873}, {"text": "TEES-for", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9722522497177124}]}, {"text": "The results we obtained with TEES were significantly superior than those with jSRE (33.4% vs. 30.09% and 20.3% vs. 16%).", "labels": [], "entities": [{"text": "TEES", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9950788021087646}, {"text": "jSRE", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.7810367941856384}]}, {"text": "Additionally, we utilized the model trained with TEES for extracting semantic relations from biomedical abstracts, for which we present a preliminary evaluation.", "labels": [], "entities": [{"text": "TEES", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8207207322120667}, {"text": "extracting semantic relations from biomedical abstracts", "start_pos": 58, "end_pos": 113, "type": "TASK", "confidence": 0.8117669026056925}]}], "introductionContent": [{"text": "Finding relevant publications fora certain topic is an important task daily carried out by most researchers in various domains, such as computer science or biomedicine.", "labels": [], "entities": []}, {"text": "However, most information retrieval methods usually consider only words and terms (named entities) and do not usually profit from semantic relationships between these entities (Lu, 2011).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7383697181940079}]}, {"text": "Many approaches frequently consider words and entities as bags of words but do not take advantage from intrinsic properties of scientific texts, such as subsections (e.g., introduction, methods, results), common concepts (e.g., task, material) and relations between these concepts (e.g., model-feature, part-whole).", "labels": [], "entities": []}, {"text": "However, extracting semantic relations from scientific text can potentially support finding relevant information fora certain topic by focusing on particular terms which participate in those relations.", "labels": [], "entities": []}, {"text": "In addition, the relation type and the corresponding arguments provide further information regarding the role that a certain entity plays in the text.", "labels": [], "entities": []}, {"text": "We describe the experiments that we carried out during our participation in Subtask 2 of SemEval-2018 Task 7 1 (.", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7160875797271729}]}, {"text": "The task consisted on the extraction of six semantic relations from scientific abstracts, namely: \"USAGE\", \"RESULT\", \"MODEL\", \"PART WHOLE\", \"TOPIC\" and \"COMPARI-SON\".", "labels": [], "entities": [{"text": "USAGE", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.9556220173835754}, {"text": "RESULT", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9795000553131104}, {"text": "MODEL", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9373068809509277}, {"text": "PART WHOLE", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.8786231875419617}]}, {"text": "While the entities were given (and all belong to the general type \"ENTITY\"), participants of subtask 2 were required to identify the relations and classify these into one of the six types.", "labels": [], "entities": [{"text": "ENTITY", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9707145094871521}]}, {"text": "All relations were asymmetrical (regarding their direction), except for \"COMPARISON\", and the identification of the direction of the relations was mandatory.", "labels": [], "entities": [{"text": "COMPARISON", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.7877489924430847}]}, {"text": "The documents came from the ACL Anthology 2 , thus belonged to the domain of computational linguistic, and were derived from a more comprehensive corpus which includes more relations than the ones under evaluation in the challenge.", "labels": [], "entities": [{"text": "ACL Anthology 2", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.92740398645401}]}, {"text": "Our contribution in this work is two-fold: (a) we experimented with two available relation extraction (RE) tools in the context of the Subtask 2 of we evaluated the models trained on the task data for the extraction of the relations mentioned above from biomedical abstracts.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7543472290039063}]}, {"text": "In the next section, we present a short overview of related work, followed by a detailed description of our methods in section 3, the results that we obtained both during the development and official evaluation phases (section 4) and the discussion of our results and the preliminary experiments with biomedical publications.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for each category for the development set (50 documents).", "labels": [], "entities": []}, {"text": " Table 2: Results for Sub-task 2 of SemEval-2018 Task  7, for the extraction and classification tasks, for both  development (D) and official test (T) sets. The highest  F1 in the official test set were 50% and 49.3% for the  extraction and classification tasks, respectively.", "labels": [], "entities": [{"text": "SemEval-2018 Task  7", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7295862833658854}, {"text": "F1", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.9818823933601379}]}]}