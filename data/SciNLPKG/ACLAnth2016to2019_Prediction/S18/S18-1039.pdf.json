{"title": [{"text": "PlusEmo2Vec at SemEval-2018 Task 1: Exploiting emotion knowledge from emoji and #hashtags", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our system that has been submitted to SemEval-2018 Task 1: Affect in Tweets (AIT) to solve five subtasks.", "labels": [], "entities": []}, {"text": "We focus on modeling both sentence and word level representations of emotion inside texts through large distantly labeled corpora with emojis and hashtags.", "labels": [], "entities": []}, {"text": "We transfer the emotional knowledge by exploiting neural network models as feature extractors and use these representations for traditional machine learning models such as support vector regression (SVR) and logistic regression to solve the competition tasks.", "labels": [], "entities": []}, {"text": "Our system is placed among the Top3 for all subtasks we participated.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finding a good representation of texts is very challenging since texts are sequences of words which are represented in a discrete space of the vocabulary.", "labels": [], "entities": []}, {"text": "For this reason, many past works have investigated in finding the mapping of words () or sentences ( to continuous spaces, so that each text can be represented by a fixed-size, realvalued N-dimensional vector.", "labels": [], "entities": []}, {"text": "This vector representation then can be applied to machine learning models to solve problems like classification and regression.", "labels": [], "entities": []}, {"text": "A good representation should contain essential information inside each text and be a useful input for statistical models.", "labels": [], "entities": []}, {"text": "Emotions in texts further deepen the complexity of modeling natural language since they not only depend on the semantics of a language but also are inherently subjective and ambiguous.", "labels": [], "entities": []}, {"text": "Despite the difficulty, accounting for emotion is important in achieving true natural language understanding, especially in areas involving human-computer interactions such as dialogue systems.", "labels": [], "entities": []}, {"text": "Humans can naturally capture and express different emotions in texts, so machines should also be able to infer them.", "labels": [], "entities": []}, {"text": "Many works ( explored modeling sentiment or emotion in texts in various directions.", "labels": [], "entities": []}, {"text": "This work is highly related to these efforts.", "labels": [], "entities": []}, {"text": "Semeval-2018 Task 1: Affect in Tweets (AIT-2018) encourages more efforts in this area with the task of sentiment analysis, which is one of the most practical applications of modeling emotional text representations.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9451324641704559}, {"text": "modeling emotional text representations", "start_pos": 174, "end_pos": 213, "type": "TASK", "confidence": 0.6861023679375648}]}, {"text": "We have participated in five subtasks regarding English tweets: emotion intensity regression, emotion intensity ordinal classification, valence (sentiment) regression, valence ordinal classification, and emotion classification (More details on the tasks in ).", "labels": [], "entities": [{"text": "emotion intensity ordinal classification", "start_pos": 94, "end_pos": 134, "type": "TASK", "confidence": 0.5502331852912903}, {"text": "valence ordinal classification", "start_pos": 168, "end_pos": 198, "type": "TASK", "confidence": 0.6757294734319051}, {"text": "emotion classification", "start_pos": 204, "end_pos": 226, "type": "TASK", "confidence": 0.6973131150007248}]}, {"text": "Although these five tasks take different formats, the most important objective is finding a good representation of the tweets regarding emotions.", "labels": [], "entities": []}, {"text": "However, the given competition training datasets are too small to achieve our goal.", "labels": [], "entities": []}, {"text": "Therefore, we explore utilizing larger datasets that are distantly supervised by emojis and hashtags to learn a robust representation and transfer the knowledge of each dataset to the competition datasets to solve the tasks.", "labels": [], "entities": []}, {"text": "We aim to minimize the use of lexicons and linguistic features by replacing them with continuous vector representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we compare two models using two different emoji dataset to transform the competition data into robust sentence representations.", "labels": [], "entities": []}, {"text": "First model is the pre-trained DeepMoji model, which is trained through emoji predictions on a dataset of 1.2 billion tweets with 64 common emoji labels.", "labels": [], "entities": []}, {"text": "We use the pretrained deep learning network, which consists of Bidirectional Long Short Term Memory (Bi-LSTM) with attention, except the last softmax layer, as a feature extractor of the original competition datasets.", "labels": [], "entities": [{"text": "Bidirectional Long Short Term Memory (Bi-LSTM)", "start_pos": 63, "end_pos": 109, "type": "METRIC", "confidence": 0.7719780467450619}]}, {"text": "As a result, each sample is transformed into a 2304-dimensional vector from the model.", "labels": [], "entities": []}, {"text": "The second model is our proposed emoji cluster model.", "labels": [], "entities": []}, {"text": "We crawled 8.1 million tweets with each of which has 34 different facial and hand emojis, assuming these kinds of emojis are more relevant to emotions.", "labels": [], "entities": []}, {"text": "Since some emojis appear much less frequently than others, we cluster the 34 emojis into 11 clusters) according to the distance on the correlation matrix of the hierarchical clustering from.", "labels": [], "entities": []}, {"text": "Samples with emojis in the same cluster are assigned the same categorical label for prediction.", "labels": [], "entities": []}, {"text": "Samples with multiple emojis are duplicated in the training set, whereas in the dev and test set we only use samples with one emoji to avoid confusion.", "labels": [], "entities": []}, {"text": "We then train a one-layer Bi-LSTM classifier with 512 hidden units to predict the emoji cluster of each sample.", "labels": [], "entities": []}, {"text": "We take part of the dataset to construct a balanced dev set with 15,000 samples per class (total 165,000) for hyperparameter tuning and early stopping.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7492104172706604}]}, {"text": "We use 200 dimension Glove vectors pre-trained on a much larger Twitter corpus to initialize the embedding layer.", "labels": [], "entities": []}, {"text": "The motivation for exploring two different models is that, firstly, we want to replicate the effectiveness of using emoji for representing emotions from the previous work) with a smaller dataset and a simpler model.", "labels": [], "entities": []}, {"text": "Note that the dataset size of the emoji cluster model is less than 1% of that of the first model, whereas DeepMoji uses more than 1 billion training samples.", "labels": [], "entities": []}, {"text": "Moreover, the first model implements a two-layer Bi-LSTM with self-attention, which has much more parameters than the second model's simple onelayer Bi-LSTM does.", "labels": [], "entities": []}, {"text": "Secondly, we want to verify that ensembling both emoji representations trained from different datasets to boost our performance.", "labels": [], "entities": []}, {"text": "We will present the result of the comparisons and the ensembles in Section 5.2.", "labels": [], "entities": []}, {"text": "One thing i dislike is laggers man I hate inconsistency The paper is irritating me As of right now i hate dre im sick of crying im tired of trying why body pain why uuugh i really have nothing to do right now i dont wanna go back to mex looking forward to holiday well today am on lake garda enjoying the life perfect time to read book im feeling great enjoying my holiday  For every sample in the SemEval competition dataset, we extract all emotional word vectors of the words in the sentence and simply average them  into one vector.", "labels": [], "entities": [{"text": "SemEval competition dataset", "start_pos": 398, "end_pos": 425, "type": "DATASET", "confidence": 0.6795242428779602}]}, {"text": "For words out of vocabulary of the hashtag corpus, we add zero vectors with the same dimension.", "labels": [], "entities": []}, {"text": "As a result, every sentence is transformed into a 300-dimension vector to be used as features for the competition tasks.", "labels": [], "entities": []}, {"text": "We expect these emotional word vectors can replace sentiment or emotion lexicons, since they are continuous representations learned from a large corpus, which can be more robust and rich in information about emotions inside words.", "labels": [], "entities": []}, {"text": "To accumulate a large corpus of emotion-labeled texts, we use a distant supervision method by using hashtags of tweets to automatically annotate emotions.", "labels": [], "entities": []}, {"text": "Such method has proven to provide relevant emotion labels by previous works (.", "labels": [], "entities": []}, {"text": "Their source of the emotion words came from emotion words list made from, where the authors organize emotions into a hierarchy in which the first layer contains six basic emotions and each emotion has a list of emotion words.", "labels": [], "entities": []}, {"text": "again expanded the list by including their lexical variants and also introduced some filtering heuristics, such as only using tweets with emotional hashtags at the end of tweets to make the distant supervision more relevant to human annotation.", "labels": [], "entities": []}, {"text": "We combine their dataset, another public dataset 1 , which used the same method, and our own extracted tweets between January and October 2017 using the Twitter Firehose API.", "labels": [], "entities": []}, {"text": "For the emotion labels, we focus on four emotion categories: joy, sadness, anger, and fear, since the competition tasks are only limited to those categories.", "labels": [], "entities": []}, {"text": "In total, our hashtag dataset consists of 1.9 million tweets).", "labels": [], "entities": []}, {"text": "Most of our system experiments were implemented by using PyTorch ( and Scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.7012351751327515}]}, {"text": "SemEval-2018 Affect in Tweets (AIT) is created by human annotators through crowd-sourcing methods . Total three datasets are given: emotion intensity (with four emotion categories; Subtask 1a & 2a), sentiment intensity (subtask 3a & 4a), and multilabel emotion classification (subtask 5a).", "labels": [], "entities": [{"text": "multilabel emotion classification", "start_pos": 242, "end_pos": 275, "type": "TASK", "confidence": 0.670427143573761}]}, {"text": "For emotion and sentiment intensity datasets, each tweet sample has both an ordinal label (coarse; {0,1,2,3} for emotion, {-3,-2,-1,0,1,2,3} for sentiment) and real-value regression label (fine-grained;).", "labels": [], "entities": []}, {"text": "For multi-label emotion classification dataset, each can have none or up to six number of multi-labels.", "labels": [], "entities": [{"text": "multi-label emotion classification", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.671568234761556}]}, {"text": "We used the given development set to tune the hyper-parameters and select models.", "labels": [], "entities": []}, {"text": "For the final submission, we merged the train & development set together to retrain the model with the best hyper-parameter found.", "labels": [], "entities": []}, {"text": "shows the test set results on regression tasks, Subtask 1a&3a.", "labels": [], "entities": []}, {"text": "We experimented with different features that we introduced before to analyze the effectiveness of each representation.", "labels": [], "entities": []}, {"text": "For emoji sentence representations, emoji cluster worked better on sadness and sentiment, whereas DeepMoji outperformed in anger, fear, and joy.", "labels": [], "entities": [{"text": "emoji sentence representations", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7871553897857666}]}, {"text": "We presumed such difference was due to the different emoji types of the two datasets used to train each model.", "labels": [], "entities": []}, {"text": "Emoji cluster only used 11 classes of emojis that were clustered together, but DeepMoji used 64 emoji classes.", "labels": [], "entities": [{"text": "Emoji cluster", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9045220911502838}]}, {"text": "It maybe possible clustering of emoji classes made it easy for regression models to predict the intensities in certain emotion categories, whereas some emotion categories needed more detailed representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics of the competition dataset for  all 5 subtasks", "labels": [], "entities": []}, {"text": " Table 4: Number of multi-labels. Most samples  have from 1-3 labels, but can have no labels or up  to 6 labels. (subtask 5a)", "labels": [], "entities": []}, {"text": " Table 5: Test set results on Subtask 1a & 3a. For 1a, separate regression models were trained for each  emotion category. The number next to the best result(bold & underlined) indicates our ranking of the  competition. Underlined ones show the models that were selected for ensemble according to the dev set.", "labels": [], "entities": []}, {"text": " Table 6: Test set results on Subtask 2a & 4a.  The predictions of the best regression models are  mapped into ordinal predictions. The number next  to the best result(bold & underlined) indicates our  ranking of the competition. (*) indicates better re- sults that we acquired after our final submission", "labels": [], "entities": [{"text": "re- sults", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.9242631594340006}]}, {"text": " Table 7: Test set results on Subtask 5a. The com- petition metric is Jaccard index.", "labels": [], "entities": [{"text": "Jaccard index", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.738228440284729}]}]}