{"title": [{"text": "IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8204098641872406}]}], "abstractContent": [{"text": "In this paper we introduce our system for the task of Irony detection in English tweets, apart of SemEval 2018.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7922322154045105}]}, {"text": "We propose representation learning approach that relies on a multi-layered bidirectional LSTM, without using external features that provide additional semantic information.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9287689924240112}]}, {"text": "Although our model is able to outperform the baseline in the validation set, our results show limited generalization power over the test set.", "labels": [], "entities": []}, {"text": "Given the limited size of the dataset, we think the usage of more pre-training schemes would greatly improve the obtained results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis and emotion recognition, as two closely related subfields of affective computing, play a key role in the advancement of artificial intelligence.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.939734935760498}, {"text": "emotion recognition", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7231251448392868}]}, {"text": "However, the complexity and ambiguity of natural language constitutes a wide range of challenges for computational systems.", "labels": [], "entities": []}, {"text": "In the past years irony and sarcasm detection have received great traction within the machine learning and NLP community (, mainly due to the high frequency of sarcastic and ironic expressions in social media.", "labels": [], "entities": [{"text": "irony and sarcasm detection", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.6814048066735268}]}, {"text": "Their linguistic collocation inclines to flip polarity in the context of sentiment analysis, which makes machinebased irony detection critical for sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.9378184378147125}, {"text": "machinebased irony detection", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.652043471733729}, {"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9550974369049072}]}, {"text": "Irony is a profoundly pragmatic and versatile linguistic phenomenon.", "labels": [], "entities": []}, {"text": "As its foundations usually lay beyond explicit linguistic patterns in reconstructing contextual dependencies and latent meaning, such as shared knowledge or common knowledge (, automatically detecting it remains a challenging task in natural language processing.", "labels": [], "entities": []}, {"text": "In this paper, we introduce our system for the shared task of Irony detection in English tweets, apart of the 2018 SemEval (Van Hee et al., 2018).", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8176701962947845}]}, {"text": "We note that computational approaches to automatically detecting irony often deploy expensive feature-engineered systems which rely on a rich body of linguistic and contextual cues (.", "labels": [], "entities": [{"text": "automatically detecting irony", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6710662345091502}]}, {"text": "The advent of Deep Learning applied to NLP has introduced models that have succeeded in large part because they learn and use their own continuous numeric representations of words (, offering us the dream of forgetting manually-designed features.", "labels": [], "entities": []}, {"text": "To this extent, in this paper we propose a representation learning approach for irony detection, which relies on a bidirectional LSTM and pre-trained word embeddings.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.9511367082595825}]}], "datasetContent": [{"text": "Our model is implemented in PyTorch (, which allowed us to easily deal with the variable tweet length due to the dynamic nature of the platform.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9128072261810303}]}, {"text": "We experimented with different values for the LSTM hidden state size, as well as for the dropout probability, obtaining best results fora dropout probability of 0.1 and 150 units for the the hidden vector.", "labels": [], "entities": []}, {"text": "We trained our models using 80% of the provided data, while the remaining 20% was used for model development.", "labels": [], "entities": [{"text": "model development", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.8404931426048279}]}, {"text": "We used Adam (, with a learning rate of 0.0001 and early stopping when performance did not improve on the development set.", "labels": [], "entities": []}, {"text": "Using embeddings of size 100 provided better results in practice.", "labels": [], "entities": []}, {"text": "Our final best model is an ensemble of four models with the same architecture but different random initialization.", "labels": [], "entities": []}, {"text": "To compare our results, we use the provided baseline, which is a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs.", "labels": [], "entities": []}, {"text": "For pre-processing, in this case we do not preserve casing and delete English stopwords.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of the obtained best results on the  valid/test sets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.905957043170929}]}]}