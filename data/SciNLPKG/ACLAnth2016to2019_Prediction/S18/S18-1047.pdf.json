{"title": [{"text": "Amrita student at SemEval-2018 Task 1: Distributed Representation of Social Media Text for Affects in Tweets", "labels": [], "entities": [{"text": "Distributed Representation of Social Media Text", "start_pos": 39, "end_pos": 86, "type": "TASK", "confidence": 0.8782005508740743}]}], "abstractContent": [{"text": "In this paper we did an analysis of \"Affects in Tweets\" which was one of the task conducted by SemEval 2018.", "labels": [], "entities": []}, {"text": "Task was to build a model which is able to do regression and classification of different emotions from the given tweets data set.", "labels": [], "entities": []}, {"text": "We developed abase model for all the subtasks using distributed representation (Doc2Vec) and applied machine learning techniques for classification and regression.", "labels": [], "entities": []}, {"text": "Distributed representation is an unsu-pervised algorithm which is capable of learning fixed length feature representation from variable length texts.", "labels": [], "entities": [{"text": "Distributed representation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8830346763134003}, {"text": "learning fixed length feature representation from variable length texts", "start_pos": 77, "end_pos": 148, "type": "TASK", "confidence": 0.7297869357797835}]}, {"text": "Machine learning techniques used for regression is 'Linear Regres-sion' while 'Random Forest Tree' is used for classification purpose.", "labels": [], "entities": [{"text": "regression", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9669094085693359}]}, {"text": "Empirical results obtained for all the subtasks by our model are shown in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most basic form of communication between humans is through language.", "labels": [], "entities": []}, {"text": "Thus it can act as a medium of how we are feeling at any particular instance.", "labels": [], "entities": []}, {"text": "For example, if we are angry at someone rather than just hitting him first we would express our feeling through our words.", "labels": [], "entities": []}, {"text": "Thus from a conversion we can make out the different emotions a person is going through at that time.", "labels": [], "entities": []}, {"text": "Apart from this social media texts can be used for determining the class of a person as described in).", "labels": [], "entities": []}, {"text": "In this work we are doing 2 ordinal classification, 1 classification and 2 regression of different emotions that people exhibits through tweets obtained from twitter) for three different languages namely Arabic, English and Spanish.", "labels": [], "entities": []}, {"text": "The data set given has tweets from all the three languages for each subtask.", "labels": [], "entities": []}, {"text": "There is a total of five subtask an emotion intensity regression task, an emotion intensity ordinal classification task, a sentiment intensity regression task, a sentiment analysis ordinal classification task and an emotion classification task.", "labels": [], "entities": [{"text": "emotion intensity ordinal classification", "start_pos": 74, "end_pos": 114, "type": "TASK", "confidence": 0.6225765645503998}, {"text": "sentiment analysis ordinal classification", "start_pos": 162, "end_pos": 203, "type": "TASK", "confidence": 0.7340578138828278}, {"text": "emotion classification", "start_pos": 216, "end_pos": 238, "type": "TASK", "confidence": 0.7157434523105621}]}, {"text": "We used distributed representation () to create feature vector which can be feed as input to machine learning algorithms for classification and regression.", "labels": [], "entities": []}, {"text": "Bag-of-words is one of the most common method used to create fixed length feature vectors but the ordering and semantics of the words are ignored in this method.", "labels": [], "entities": []}, {"text": "By using Doc2Vec, an unsupervised learning algorithm, we can create fixed length features from variable length data.", "labels": [], "entities": []}, {"text": "Thus by using Doc2Vec we can preserve the ordering as well as the semantics of data.", "labels": [], "entities": []}, {"text": "Another method for word representation is distributional representation which is an extension of co-occurrence based representation and have the same disadvantages as cooccurrence based methods.", "labels": [], "entities": [{"text": "word representation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8067792654037476}]}, {"text": "Once the feature vector is created it is pushed into machine learning algorithm for classification and regression.", "labels": [], "entities": []}, {"text": "We have used Random Forest Tree for classification which is an ensemble learning method that creates a number of decision trees during training and gives an output class which appears most often.", "labels": [], "entities": []}, {"text": "For regression we used Linear Regression which tries to fit a line between the actual and predicted values by minimizing the error sum of squares between them.", "labels": [], "entities": [{"text": "regression", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9769043326377869}]}, {"text": "The final model is obtained after doing hyper parameter tuning for Doc2Vec size and n estimator, max depth for Random Forest Tree which are fixed through a grid search method before pushing to machine learning algorithms.", "labels": [], "entities": [{"text": "Random Forest Tree", "start_pos": 111, "end_pos": 129, "type": "DATASET", "confidence": 0.7811196049054464}]}, {"text": "Section 2 of this paper gives a brief introduction to corpus.", "labels": [], "entities": []}, {"text": "Section 3 describes the theory of different methods used.", "labels": [], "entities": []}, {"text": "Section 4 describes the method-ology used.", "labels": [], "entities": []}, {"text": "Section 5 covers result and discussion.", "labels": [], "entities": []}, {"text": "Section 6 talks about our conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus was obtained from SemEval2018 website.", "labels": [], "entities": []}, {"text": "Once the data was obtained the first process was to extract tweets from the data for all the languages.", "labels": [], "entities": []}, {"text": "Once everything was extracted from the document next step was to build a Doc2Vec model from the extracted tweets which will produce feature vectors which can be used as inputs for our machine learning techniques for regression and classification tasks.", "labels": [], "entities": [{"text": "regression and classification tasks", "start_pos": 216, "end_pos": 251, "type": "TASK", "confidence": 0.6806981936097145}]}, {"text": "Gensim library was used to build the Doc2Vec model.", "labels": [], "entities": [{"text": "Gensim library", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8845650553703308}]}, {"text": "Sklearn library was used for Random Forest Tree and Linear Regression.", "labels": [], "entities": [{"text": "Sklearn library", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9735614657402039}, {"text": "Random Forest Tree", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8307976921399435}, {"text": "Linear Regression", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6871922463178635}]}, {"text": "Before fixing the Doc2Vec base model we did hyper parameter tuning for all subtasks in all languages.", "labels": [], "entities": []}, {"text": "The parameters tuned for regression tasks was Doc2Vec size and for classification were Doc2Vec size and n estimator, max depth for Random Forest Tree.", "labels": [], "entities": [{"text": "Random Forest Tree", "start_pos": 131, "end_pos": 149, "type": "DATASET", "confidence": 0.8138564427693685}]}, {"text": "size of Doc2Vec means the dimensionality of the feature vector, i.e., in which dimension each document in a corpus is represented as. n estimator of Random Forest Tree means the number of decision trees used in the forest, i.e., before taking vote of a class how many different algorithms are to be run.", "labels": [], "entities": []}, {"text": "max depth of Random Forest Tree gives the maximum depth of Tasks size n estimator max depth 10 8 the tree in algorithm.", "labels": [], "entities": [{"text": "Random Forest Tree", "start_pos": 13, "end_pos": 31, "type": "DATASET", "confidence": 0.7309322158495585}]}, {"text": "We did a grid search method to find out the optimum parameter values for each subtasks.", "labels": [], "entities": []}, {"text": "For emotion intensity regression task (Task 1) and sentiment intensity regression task (Task 3) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration.", "labels": [], "entities": []}, {"text": "For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.", "labels": [], "entities": [{"text": "emotion intensity ordinal classification task", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.6783236682415008}, {"text": "sentiment analysis ordinal classification task", "start_pos": 60, "end_pos": 106, "type": "TASK", "confidence": 0.9147098183631897}, {"text": "emotion classification task", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.8035175700982412}, {"text": "Random Forest Tree", "start_pos": 255, "end_pos": 273, "type": "DATASET", "confidence": 0.8213825821876526}, {"text": "max depth", "start_pos": 346, "end_pos": 355, "type": "METRIC", "confidence": 0.9453788101673126}, {"text": "Random Forest Tree", "start_pos": 359, "end_pos": 377, "type": "DATASET", "confidence": 0.9043210744857788}]}, {"text": "Variables used to estimate the ideal parameters for regression tasks were mean square error (MSE) and variance of Linear Regression algorithm.", "labels": [], "entities": [{"text": "mean square error (MSE)", "start_pos": 74, "end_pos": 97, "type": "METRIC", "confidence": 0.9488516648610433}, {"text": "variance", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9300844073295593}]}, {"text": "We selected those parameters that gave the least MSE value ans large variance value.", "labels": [], "entities": [{"text": "MSE", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.7737396359443665}]}, {"text": "Variables used to estimate the ideal parameters for classification tasks was accuracy of the Random Forest Tree algorithm.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.9297339618206024}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9996615648269653}, {"text": "Random Forest Tree algorithm", "start_pos": 93, "end_pos": 121, "type": "DATASET", "confidence": 0.8416035324335098}]}, {"text": "Once the parameters were fixed we build the model for each subtask and used it to predict the values for test data.", "labels": [], "entities": []}, {"text": "Development data was used for hyper-parameter tuning while training data was used for building Doc2Vec model.", "labels": [], "entities": [{"text": "hyper-parameter tuning", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7020978331565857}, {"text": "Doc2Vec", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.8970407247543335}]}, {"text": "The ideal parameters obtained after hyperparameter tuning for each subtask for English is consolidated in, Arabic is consolidated in and Spanish is consolidated in.", "labels": [], "entities": []}, {"text": "The control parameter values obtained for the optimum parameters which in turn are used to build the model is consolidated in for task 1 Table 5 for task 3 for task 2 for task 4 for task", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tuned parameters for Arabic.", "labels": [], "entities": []}, {"text": " Table 3: Tuned parameters for Spanish.", "labels": [], "entities": []}, {"text": " Table 4: Control variable value for optimum parame- ters for Task 1.", "labels": [], "entities": []}, {"text": " Table 5: Control variable value for optimum parame- ters for Task 2.", "labels": [], "entities": []}, {"text": " Table 6: Control variable value for optimum parame- ters for Task 3.", "labels": [], "entities": []}, {"text": " Table 7: Control variable value for optimum parame- ters for Task 4.", "labels": [], "entities": []}, {"text": " Table 8: Control variable value for optimum parame- ters for Task 5.", "labels": [], "entities": []}]}