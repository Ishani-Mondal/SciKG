{"title": [{"text": "HHU at SemEval-2018 Task 12: Analyzing an Ensemble-based Deep Learning Approach for the Argument Mining Task of Choosing the Correct Warrant", "labels": [], "entities": [{"text": "HHU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8954818248748779}, {"text": "Argument Mining Task", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7565823197364807}]}], "abstractContent": [{"text": "This paper describes our participation in the SemEval-2018 Task 12 Argument Reasoning Comprehension Task which calls to develop systems that, given a reason and a claim, predict the correct warrant from two opposing options.", "labels": [], "entities": [{"text": "SemEval-2018 Task 12 Argument Reasoning Comprehension Task", "start_pos": 46, "end_pos": 104, "type": "TASK", "confidence": 0.8891627447945731}]}, {"text": "We decided to use a deep learning architecture and combined 623 models with different hyperparameters into an ensemble.", "labels": [], "entities": []}, {"text": "Our extensive analysis of our architecture and ensemble reveals that the decision to use an ensemble was suboptimal.", "labels": [], "entities": []}, {"text": "Additionally, we benchmark a support vector machine as a baseline.", "labels": [], "entities": []}, {"text": "Furthermore , we experimented with an alternative data split and achieved more stable results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argument mining is a trending research domain that focuses on the extraction of arguments and their relations from text.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8628324568271637}]}, {"text": "It has been applied to multiple languages and multiple application domains, including the legal domain (, persuasive essays (), online participation (, and web discourse.", "labels": [], "entities": []}, {"text": "The three most common subtasks are: argument identification, argument classification, and argument linking.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.745290219783783}, {"text": "argument classification", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7723552882671356}, {"text": "argument linking", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7251817435026169}]}, {"text": "These focus on the identification of argumentative text content, the extraction of argument components according to a specific argument model, and the extraction of relations between arguments components, respectively.", "labels": [], "entities": [{"text": "identification of argumentative text content", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.8308609008789063}]}, {"text": "Currently, different argument models comprising different argument components are being used throughout the community, such as the claimpremise family or Toulmin's model.", "labels": [], "entities": [{"text": "claimpremise family", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.8840754926204681}]}, {"text": "In the scope of this paper, claims, premises, and warrants are the most important argument components.", "labels": [], "entities": []}, {"text": "Claims are often defined as controversial statements that are either true or false.", "labels": [], "entities": []}, {"text": "Premises are reasons that support or attack claims.", "labels": [], "entities": []}, {"text": "In Toulmin's model, they are connected with warrants that state why the premise supports the claim.", "labels": [], "entities": []}, {"text": "introduced anew argument mining task called argument reasoning comprehension with the following definition: Given a reason and a claim, identify the correct warrant from two opposing options.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7472454607486725}, {"text": "argument reasoning comprehension", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.831788957118988}]}, {"text": "This paper describes our participation in the SemEval-2018 Task 12 The Argument Reasoning Comprehension Task () that uses the dataset from as a shared task.", "labels": [], "entities": [{"text": "SemEval-2018 Task 12 The Argument Reasoning Comprehension Task", "start_pos": 46, "end_pos": 108, "type": "TASK", "confidence": 0.7371814846992493}]}, {"text": "Besides a description of our machine learning systems, we evaluate additional machine learning models (we were only allowed to submit a single set of predictions for the official ranking) and we further analyze the test set.", "labels": [], "entities": []}, {"text": "The dataset for the challenge consists of annotated news comments from the New York Times user debate section.", "labels": [], "entities": [{"text": "New York Times user debate section", "start_pos": 75, "end_pos": 109, "type": "DATASET", "confidence": 0.6755633552869161}]}, {"text": "With Amazon Mechanical Turk as a crowdsourcing platform, 5000 randomly selected user comments were annotated in a multistep annotation process that included three free text annotation steps (gist summarization, the creation of warrants, and of alternative warants).", "labels": [], "entities": []}, {"text": "After the final filtering, the dataset for the Argument Reasoning Comprehesion Task comprises 1970 instances, where each instance is a tuple of (R, C, W, AW, G, T, I) comprising a reason (R), a claim (C), a warrant (W), an alternative warrant (AW), a gold label (G) indicating which of both warrants is correct, a debate title (T), and additional debate information (I) about the debate.", "labels": [], "entities": [{"text": "Argument Reasoning Comprehesion Task", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.8349133282899857}]}, {"text": "The task organizers split the dataset into three distinct groups: training set (1,210 instances), development set (316 instances), and a test set (444 instances).", "labels": [], "entities": []}, {"text": "shows a training example of the dataset.", "labels": [], "entities": []}, {"text": "The machine learning task of predicting the correct warrant is difficult since both warrants Title: Do We Still Need Libraries?", "labels": [], "entities": []}, {"text": "Debate information: Do We Still Need Libraries?", "labels": [], "entities": []}, {"text": "What are libraries for, and how should they evolve?", "labels": [], "entities": []}, {"text": "Claim: We need libraries Reason: Libraries have lots to offer in addition to books they provide music, dvd's, magazines and more.", "labels": [], "entities": []}, {"text": "Warrant 1: all these things are readily available to everyone online Warrant 2: none of these things are readily available to everyone online are lexically similar, and can differ in just one or two words.", "labels": [], "entities": []}, {"text": "In the trial phase of the challenge, the participants were given access to the training set and the dev test with gold labels.", "labels": [], "entities": []}, {"text": "In the test phase of the challenge, the task participants were given access to the test set (with omitted gold labels) and had to submit predictions for all test instances.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}