{"title": [], "abstractContent": [{"text": "We study the problem of measuring the quality of automatically-generated stories.", "labels": [], "entities": []}, {"text": "We focus on the setting in which a few sentences of a story are provided and the task is to generate the next sentence (\"continuation\") in the story.", "labels": [], "entities": []}, {"text": "We seek to identify what makes a story continuation interesting, relevant, and have high overall quality.", "labels": [], "entities": []}, {"text": "We crowdsource annotations along these three criteria for the outputs of story continuation systems, design features, and train models to predict the annotations.", "labels": [], "entities": [{"text": "story continuation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7199177592992783}]}, {"text": "Our trained scorer can be used as a rich feature function for story generation, a reward function for systems that use reinforcement learning to learn to generate stories, and as a partial evaluation metric for story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.8123776912689209}, {"text": "story generation", "start_pos": 211, "end_pos": 227, "type": "TASK", "confidence": 0.8086004257202148}]}], "introductionContent": [{"text": "We study the problem of automatic story generation in the climate of neural network natural language generation methods.", "labels": [], "entities": [{"text": "automatic story generation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.631334255139033}, {"text": "neural network natural language generation", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.8085768103599549}]}, {"text": "Story generation has along history, beginning with rule-based systems in the 1970s (.", "labels": [], "entities": [{"text": "Story generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8540855646133423}]}, {"text": "Most story generation research has focused on modeling the plot, characters, and primary action of the story, using simplistic methods for producing the actual linguistic form of the stories.", "labels": [], "entities": [{"text": "story generation", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.7975044250488281}]}, {"text": "More recent work learns from data how to generate stories holistically without a clear separation between content selection and surface realization, with a few recent methods based on recurrent neural networks.", "labels": [], "entities": []}, {"text": "We follow the latter style and focus on a setting in which a few sentences of a story are provided (the context) and the task is to generate the next sentence in the story (the continuation).", "labels": [], "entities": []}, {"text": "Our goal is to produce continuations that are both interesting and relevant given the context.", "labels": [], "entities": []}, {"text": "Neural networks are increasingly employed for natural language generation, most often with encoder-decoder architectures based on recurrent neural networks ().", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6867512464523315}]}, {"text": "However, while neural methods are effective for generation of individual sentences conditioned on some context, they struggle with coherence when used to generate longer texts ().", "labels": [], "entities": []}, {"text": "In addition, it is challenging to apply neural models in less constrained generation tasks with many valid solutions, such as open-domain dialogue and story continuation.", "labels": [], "entities": [{"text": "story continuation", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.7758433520793915}]}, {"text": "The story continuation task is difficult to formulate and evaluate because there can be a wide variety of reasonable continuations for typical story contexts.", "labels": [], "entities": [{"text": "story continuation task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8376368482907613}]}, {"text": "This is also the casein open-domain dialogue systems, in which common evaluation metrics like BLEU () are only weakly correlated with human judgments ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9975612163543701}]}, {"text": "Another problem with metrics like BLEU is the dependence on a gold standard.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9932931661605835}]}, {"text": "In story generation and open-domain dialogue, there can be several equally good continuations for any given context which suggests that the quality of a continuation should be computable without reliance on a gold standard.", "labels": [], "entities": [{"text": "story generation", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7842552065849304}]}, {"text": "In this paper, we study the question of identifying the characteristics of a good continuation fora given context.", "labels": [], "entities": []}, {"text": "We begin by building several story generation systems that generate a continuation from a context.", "labels": [], "entities": []}, {"text": "We develop simple systems based on recurrent neural networks and similarity-based retrieval and train them on the ROC story dataset ( . We use crowdsourcing to collect annotations of the quality of the continuations without revealing the gold standard.", "labels": [], "entities": [{"text": "ROC story dataset", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.952946404616038}]}, {"text": "We ask annotators to judge continuations along three distinct criteria: overall quality, relevance, and interestingness.", "labels": [], "entities": []}, {"text": "We collect multiple annotations for 4586 context/continuation pairs.", "labels": [], "entities": []}, {"text": "These annotations permit us to compare methods for story generation and to study the relationships among the criteria.", "labels": [], "entities": [{"text": "story generation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.8293317258358002}]}, {"text": "We analyze our annotated dataset by developing features of the context and continuation and measuring their correlation with each criterion.", "labels": [], "entities": []}, {"text": "We combine these features with neural networks to build models that predict the human scores, thus attempting to automate the process of human quality judgment.", "labels": [], "entities": [{"text": "human quality judgment", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.6430204610029856}]}, {"text": "We find that our predicted scores correlate well with human judgments, especially when using our full feature set.", "labels": [], "entities": []}, {"text": "Our scorer can be used as a rich feature function for story generation or a reward function for systems that use reinforcement learning to learn to generate stories.", "labels": [], "entities": [{"text": "story generation", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7917501330375671}]}, {"text": "It can also be used as a partial evaluation metric for story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.8401464819908142}]}, {"text": "1 Examples of contexts, generated continuations, and quality predictions from our scorer are shown in.", "labels": [], "entities": []}, {"text": "The annotated data and trained scorer are available at the authors' websites.", "labels": [], "entities": [{"text": "scorer", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.8238915801048279}]}], "datasetContent": [{"text": "The average scores for each data source are shown in: Average criteria scores for each system (O = overall, R = relevance, I = interestingness).", "labels": [], "entities": [{"text": "O = overall, R = relevance, I = interestingness", "start_pos": 95, "end_pos": 142, "type": "METRIC", "confidence": 0.7635650824416768}]}, {"text": "Human-written continuations are best under all three criteria.", "labels": [], "entities": [{"text": "Human-written continuations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5334597676992416}]}, {"text": "The HU-MAN relevance average is higher than interestingness.", "labels": [], "entities": [{"text": "HU-MAN relevance average", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.8665458559989929}]}, {"text": "This matches our intuitions about the ROC corpus: the stories were written to capture commonsense knowledge about everyday events rather than to be particularly surprising or interesting stories in their own right.", "labels": [], "entities": [{"text": "ROC corpus", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.7867192924022675}]}, {"text": "Nonetheless, we do find that the HUMAN continuations have higher interestingness scores than all automatic systems.", "labels": [], "entities": [{"text": "HUMAN continuations", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7175784111022949}]}, {"text": "The RETRIEVAL system actually outperforms all SEQ2SEQ systems on all criteria, though the gap is smallest on relevance.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.983124852180481}]}, {"text": "We found that the SEQ2SEQ systems often produced continuations that fit topically within the world suggested by the context, though they were often generic or merely topically relevant without necessarily moving the story forward.", "labels": [], "entities": []}, {"text": "We found S2S-GREEDY produced outputs that were grammatical and relevant but tended to be more mundane whereas S2S-REVERSE tended to produce slightly more interesting outputs that were still grammatical and relevant on average.", "labels": [], "entities": []}, {"text": "The sampling and diverse beam search outputs were frequently ungrammatical and therefore suffer under all criteria.", "labels": [], "entities": []}, {"text": "We show sample outputs from the different systems in.", "labels": [], "entities": []}, {"text": "We also show predicted criteria scores from our final automatic scoring model (see Section 6 for details).", "labels": [], "entities": []}, {"text": "We show predicted rather than annotated scores here because fora given context, we did not obtain annotations for all continuations for that context.", "labels": [], "entities": []}, {"text": "We can see some of the characteristics of the different models and understand how their outputs differ.", "labels": [], "entities": []}, {"text": "The RETRIEVAL outputs are sometimes more interesting than the HUMAN outputs, though they often mention new entities that were not contained in the context, or they maybe merely topically related to the context without necessarily resulting in a coherent story.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8170305490493774}]}, {"text": "This affects interestingness as well, as a continuation must first be relevant in order to be interesting.", "labels": [], "entities": []}, {"text": "shows correlations among the criteria for different sets of outputs.", "labels": [], "entities": []}, {"text": "RETRIEVAL outputs show a lower correlation between overall score and interestingness than HUMAN outputs.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9415584802627563}]}, {"text": "This is likely because the RETRIEVAL outputs with high interestingness scores frequently contained more surprising content such as new character names or new actions/events that were not found in the context.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.807247519493103}]}, {"text": "Therefore, a high interestingness score was not as strongly correlated with overall quality as with HUMAN outputs, for which interesting continuations were less likely to contain erroneous new material.", "labels": [], "entities": []}, {"text": "After averaging the two annotator scores to get our dataset of 4586 context/continuation pairs, we split the data randomly into 600 pairs for validation, 600 for testing, and used the rest (3386) for training.", "labels": [], "entities": []}, {"text": "For our evaluation metric, we use Spearman correlation between the scorer's predictions and the annotated scores.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 34, "end_pos": 54, "type": "METRIC", "confidence": 0.927425354719162}]}, {"text": "shows results as features are either removed from the full set or added to the featureless model, all when using the \"cont\" input schema.", "labels": [], "entities": []}, {"text": "Each row corresponds to one feature ablation or addition, except for the final row which corresponds to adding two feature sets that are efficient to compute: IDF and Length.", "labels": [], "entities": [{"text": "IDF", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.9952606558799744}, {"text": "Length", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9458795189857483}]}, {"text": "The Mention and PMI features are the most useful for relevance, which matches the pattern of correlations in Table 5, while IDF and Length features are most helpful for interestingness.", "labels": [], "entities": [{"text": "Mention", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9557920098304749}, {"text": "PMI", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.49951571226119995}, {"text": "relevance", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.8544836640357971}, {"text": "IDF", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9977510571479797}, {"text": "Length", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9340165257453918}]}, {"text": "All feature sets contribute in predicting overall quality, with the Mention features showing the largest drop in correlation when they are ablated.", "labels": [], "entities": [{"text": "correlation", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9758422374725342}]}, {"text": "shows our final results on the validation and test sets.", "labels": [], "entities": []}, {"text": "The highest correlations on the test set are achieved by using the sim+cont model with all features.", "labels": [], "entities": []}, {"text": "While interestingness can be predicted reasonably well with just IDF and the Length features, the prediction of relevance is improved greatly with the full feature set.", "labels": [], "entities": [{"text": "IDF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9888986945152283}, {"text": "relevance", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.8527905344963074}]}], "tableCaptions": [{"text": " Table 2: Average criteria scores for each system (O =  overall, R = relevance, I = interestingness).", "labels": [], "entities": [{"text": "O =  overall, R = relevance", "start_pos": 51, "end_pos": 78, "type": "METRIC", "confidence": 0.7964529309953962}, {"text": "I = interestingness", "start_pos": 80, "end_pos": 99, "type": "METRIC", "confidence": 0.8237447937329611}]}, {"text": " Table 3: Sample system outputs for different contexts. Final three columns show predicted scores from our trained  scorer (see Section 6 for details).", "labels": [], "entities": []}, {"text": " Table 4: Pearson correlations between criteria for dif- ferent subsets of the annotated data.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8942144215106964}]}, {"text": " Table 5: Spearman correlations between features and  annotations. The final two rows are \"oracle\" binary  features that return 1 for continuations from those sets.", "labels": [], "entities": []}, {"text": " Table 6: Ablation experiments with several feature sets  (Spearman correlations on the validation set).", "labels": [], "entities": []}, {"text": " Table 7: Correlations (Spearman's \u03c1 \u00d7 100) on valida- tion and test sets for best models with three feature sets.", "labels": [], "entities": [{"text": "Correlations", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9529026746749878}]}]}