{"title": [{"text": "THU NGN at SemEval-2018 Task 1: Fine-grained Tweet Sentiment Intensity Analysis with Attention CNN-LSTM", "labels": [], "entities": [{"text": "THU NGN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5698133409023285}, {"text": "Tweet Sentiment Intensity Analysis", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.7747940719127655}]}], "abstractContent": [{"text": "Traditional sentiment analysis approaches mainly focus on classifying the sentiment polarities or emotion categories of texts.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.925547868013382}, {"text": "classifying the sentiment polarities or emotion categories of texts", "start_pos": 58, "end_pos": 125, "type": "TASK", "confidence": 0.7651892768012153}]}, {"text": "However , they can't exploit the sentiment intensity information.", "labels": [], "entities": []}, {"text": "Therefore, the SemEval-2018 Task 1 is aimed to automatically determine the intensity of emotions or sentiment of tweets to mine fine-grained sentiment information.", "labels": [], "entities": []}, {"text": "In order to address this task, we propose a system based on an attention CNN-LSTM model.", "labels": [], "entities": []}, {"text": "In our model, LSTM is used to extract the long-term contextual information from texts.", "labels": [], "entities": []}, {"text": "We apply attention techniques to selecting this information.", "labels": [], "entities": []}, {"text": "A CNN layer with different kernel sizes is used to extract local features.", "labels": [], "entities": []}, {"text": "The dense layers take the pooled CNN feature maps and predict the intensity scores.", "labels": [], "entities": [{"text": "CNN feature maps", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8504208326339722}]}, {"text": "Our system achieves an average Pear-son correlation score of 0.722 (ranked 12/48) in the emotion intensity regression task, and 0.810 in the valence regression task (ranked 15/38).", "labels": [], "entities": [{"text": "Pear-son correlation score", "start_pos": 31, "end_pos": 57, "type": "METRIC", "confidence": 0.8106858928998312}]}, {"text": "It indicates that our system can be further extended.", "labels": [], "entities": []}], "introductionContent": [{"text": "Detecting the intensity of sentiment is an important task for fine-grained sentiment analysis.", "labels": [], "entities": [{"text": "Detecting the intensity of sentiment", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8368754029273987}, {"text": "fine-grained sentiment analysis", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7004553476969401}]}, {"text": "Intensity refers to the degree or amount of an emotion or degree of sentiment.", "labels": [], "entities": [{"text": "Intensity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9409816861152649}]}, {"text": "For example, we can express our emotion by \"very happy\" or \"a little angry\".", "labels": [], "entities": []}, {"text": "The intensity can be analysis in multiple categories (i.e. low, moderate and high) or real-valued.", "labels": [], "entities": []}, {"text": "Identifying the intensity information of sentiment has potential to applications such as electronic business, social computing and public health.", "labels": [], "entities": [{"text": "Identifying the intensity information of sentiment", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8301232755184174}]}, {"text": "Twitter is asocial platform which contains rich textual content.", "labels": [], "entities": []}, {"text": "There have been many approaches to twitter sentiment analysis ().", "labels": [], "entities": [{"text": "twitter sentiment analysis", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8535958528518677}]}, {"text": "However, twitter sentiment analysis is challenging because tweets usually contain nonstandard languages, including emoticons, emojis, creatively spelled words, and hash tags.", "labels": [], "entities": [{"text": "twitter sentiment analysis", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.8421185612678528}]}, {"text": "In order to improve the collective techniques on tweet sentiment intensity analysis, the SemEval-2018 Task 1 is aimed to identify the categorical and real-valued intensity of emotions or sentiment for English,.", "labels": [], "entities": [{"text": "tweet sentiment intensity analysis", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.7872764319181442}]}, {"text": "Existing approaches to analysis the intensity of emotions or sentiment are mainly based on lexicons and supervised learning.", "labels": [], "entities": [{"text": "analysis the intensity of emotions or sentiment", "start_pos": 23, "end_pos": 70, "type": "TASK", "confidence": 0.884140065738133}]}, {"text": "Lexicon-based methods usually rely on lexicons to assign the intensity scores of affective words in texts.", "labels": [], "entities": []}, {"text": "However, these method can't utilize the contextual information from texts.", "labels": [], "entities": []}, {"text": "Supervised methods are mainly based on SVR (, linear regression ( and neural networks (.", "labels": [], "entities": []}, {"text": "Usually neural network-based methods outperform SVR and linear regression-based methods siginificantly.", "labels": [], "entities": []}, {"text": "Motivated by the successful applications of neural models in this task, we propose a system using a CNN-LSTM model with attention mechanism.", "labels": [], "entities": []}, {"text": "Firstly, a tweet will be converted into a sequence of dense vectors by an embedding layer.", "labels": [], "entities": []}, {"text": "Next, we use a Bi-LSTM layer to extract contextual information from them.", "labels": [], "entities": []}, {"text": "The sequential features will be selected by an attention layer.", "labels": [], "entities": []}, {"text": "Then we apply a CNN with different kernel sizes to extracting different local information.", "labels": [], "entities": []}, {"text": "Thus, our model can exploit both local and longterm information by combining CNN and LSTM.", "labels": [], "entities": [{"text": "CNN", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8389174342155457}]}, {"text": "Finally, two dense layers are used to predict the intensity scores.", "labels": [], "entities": []}, {"text": "The system performance quantified by an average Pearson correlation score is 0.722 in the emotion intensity regression task (EIreg) and 0.810 in the valence regression task (V-reg).", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 48, "end_pos": 73, "type": "METRIC", "confidence": 0.9665343165397644}, {"text": "emotion intensity regression task (EIreg)", "start_pos": 90, "end_pos": 131, "type": "METRIC", "confidence": 0.6841227582522801}]}, {"text": "Our model outperforms several baseline neural networks, which proves that our model can identify the intensity of emotions and sentiment effectively.", "labels": [], "entities": [{"text": "identify the intensity of emotions and sentiment", "start_pos": 88, "end_pos": 136, "type": "TASK", "confidence": 0.603804703269686}]}], "datasetContent": [{"text": "The details of English datasets we use is shown in.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.7589806914329529}]}, {"text": "The intensity in both task is annotated between 0 and 1.", "labels": [], "entities": []}, {"text": "In the EI-reg task, the Pearson correlation scores across all four emotions will be averaged as the final score.", "labels": [], "entities": [{"text": "Pearson correlation scores", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.937620202700297}]}, {"text": "In the V-reg task, the correlation score for valence is used as the competition metric.", "labels": [], "entities": [{"text": "correlation score", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.9757993817329407}]}, {"text": "In our network, the dimension of word embeddings is 400 + 300.", "labels": [], "entities": []}, {"text": "The hidden states of Bi-LSTM are 2\u00d7300-dim.", "labels": [], "entities": []}, {"text": "The kernel sizes of CNN are 3, 5, 7 and 9 respectively.", "labels": [], "entities": [{"text": "CNN", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.8717581629753113}]}, {"text": "The number of feature maps are 4 \u00d7 200.", "labels": [], "entities": []}, {"text": "The dimension of the first dense layer is set to 200.", "labels": [], "entities": []}, {"text": "The padding length of tweets is set to 50.", "labels": [], "entities": []}, {"text": "The dropout rate is a random number between 0.1 and 0.3.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8138952553272247}]}, {"text": "The loss function we use is MAE, and the batch size is set to 8.", "labels": [], "entities": [{"text": "MAE", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9970837235450745}]}, {"text": "We combine the training and development sets in our experiment.", "labels": [], "entities": []}, {"text": "We use 90% for training and reserve 10% for cross validation.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6618591248989105}]}, {"text": "In our official submissions, we use the full training and development sets to train models.", "labels": [], "entities": []}, {"text": "We compare the performance of our model and several baselines.", "labels": [], "entities": []}, {"text": "The models to be compared include: 1) CNN, using CNN and dense layers.", "labels": [], "entities": []}, {"text": "2) LSTM, using LSTM and dense layers.", "labels": [], "entities": []}, {"text": "3) CNN+LSTM, combing CNN with LSTM to predict.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.6767553885777792}]}, {"text": "4) CNN+LSTM+att, adding attention mechanism to CNN-LSTM model.", "labels": [], "entities": []}, {"text": "5) CNN+LSTM+att+ensemble, using ensemble strategy in the attention-based CNN-LSTM model.", "labels": [], "entities": []}, {"text": "The results in the EI-reg and V-reg tasks are shown in.", "labels": [], "entities": []}, {"text": "In comparison, we also present the cross validation results.", "labels": [], "entities": []}, {"text": "Our system reaches average Pearson correlation score of 0.722 in the EI-reg task and 0.810 in the V-reg task.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.9060766299565634}]}, {"text": "The results indicate that our CNN-LSTM model outperforms the CNN and LSTM baselines.", "labels": [], "entities": []}, {"text": "It proves that CNN-LSTM model can combine the long-term information and local information in texts.", "labels": [], "entities": []}, {"text": "The attention mechanism can also improve the model performance.", "labels": [], "entities": []}, {"text": "Since the attention layer can select important information, our model can focus on important words in texts (e.g. affective words) to predict the intensity of emotions and sentiment more accurately.", "labels": [], "entities": []}, {"text": "Although our system still needs to be improved compared with the top systems, our model outperforms the common baseline models, which validates the effectiveness of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The intensity in both task is annotated  between 0 and 1. In the EI-reg task, the Pearson  correlation scores across all four emotions will be  averaged as the final score. In the V-reg task, the  correlation score for valence is used as the compe- tition metric.", "labels": [], "entities": [{"text": "Pearson  correlation", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.9590912461280823}]}, {"text": " Table 1: Detailed statistics of the English datasets in  our experiment", "labels": [], "entities": [{"text": "English datasets", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.7935546338558197}]}, {"text": " Table 2. In comparison, we  also present the cross validation results. Our sys- tem reaches average Pearson correlation score of  0.722 in the EI-reg task and 0.810 in the V-reg  task. The results indicate that our CNN-LSTM  model outperforms the CNN and LSTM baselines.  It proves that CNN-LSTM model can combine the long-term information and local information  in texts. The attention mechanism can also im- prove the model performance. Since the attention  layer can select important information, our model  can focus on important words in texts (e.g. af- fective words) to predict the intensity of emotions  and sentiment more accurately. Although our sys- tem still needs to be improved compared with the  top systems, our model outperforms the common  baseline models, which validates the effectiveness  of our model.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 101, "end_pos": 126, "type": "METRIC", "confidence": 0.9775606592496237}]}, {"text": " Table 2: Evaluation and cross validation performance of our model ande baselines.", "labels": [], "entities": []}, {"text": " Table 3: Influence of using different combinations of  pre-trained word embeddings. The emb1 and emb2 de- note the embeddings provided by Godin et al. (2015)  and Barbieri et al. (2016) respectively.", "labels": [], "entities": []}, {"text": " Table 4: Influence of POS tags and lexicon features.", "labels": [], "entities": []}, {"text": " Table 5: The average differences, p-value and statistical  significance of predictions on the mystery set in each  task. We denote them as Avg-D, p and Sig respectively.", "labels": [], "entities": [{"text": "Avg-D", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9882171750068665}]}, {"text": " Table 6. The green  color represents low attention, while red color rep- resents high attention. We can see that the affec-", "labels": [], "entities": []}]}