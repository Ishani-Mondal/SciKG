{"title": [], "abstractContent": [{"text": "In this paper we present three unsupervised models for capturing discriminative attributes based on information from word embed-dings, WordNet, and sentence-level word co-occurrence frequency.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9319508671760559}]}, {"text": "We show that, of these approaches, the simple approach based on word co-occurrence performs best.", "labels": [], "entities": []}, {"text": "We further consider supervised and unsupervised approaches to combining information from these models, but these approaches do not improve on the word co-occurrence model.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the task of capturing discriminative attributes, a system is presented with three words, and must determine whether the third word -the attribute -characterizes the first word, but not the second.", "labels": [], "entities": []}, {"text": "For example, for the triple (chicken,bread,legs), legs is a discriminative attribute because chickens typically have legs, but bread typically does not.", "labels": [], "entities": []}, {"text": "On the other hand, for the triple (mother,woman,female), female is not a discriminative attribute because both mothers and women are typically female.", "labels": [], "entities": []}, {"text": "In the case of the triple (brush,chocolate,chicken), chicken is not a discriminative attribute because there is no clear relationship between chicken and brushes, or between chicken and chocolate.", "labels": [], "entities": []}, {"text": "In this paper we focus primarily on unsupervised approaches to the task of capturing discriminative attributes.", "labels": [], "entities": []}, {"text": "We consider three unsupervised models drawing on information from word embeddings, WordNet, and sentencelevel word co-occurrence frequency.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9531001448631287}]}, {"text": "We then consider three approaches to combining information from these models: one unsupervised majority vote approach, and two supervised approaches.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, we achieve our best F1 score of 0.61 with the remarkably simple approach based on word co-occurrence.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9855852425098419}]}, {"text": "None of the approaches to model combination improve over this.", "labels": [], "entities": [{"text": "model combination", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6798949837684631}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1 score on the validation data for the WordNet method. Each row corresponds to a different configuration  for this model, with information for word1 (w1), word2 (w2), and the attribute (att) taken from the indicated  relations in WordNet. The best F1 is indicated in boldface.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9674265086650848}, {"text": "WordNet", "start_pos": 241, "end_pos": 248, "type": "DATASET", "confidence": 0.9630303978919983}]}]}