{"title": [{"text": "SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge", "labels": [], "entities": [{"text": "SemEval-2018 Task 11", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8714708487192789}]}], "abstractContent": [{"text": "This report summarizes the results of the Se-mEval 2018 task on machine comprehension using commonsense knowledge.", "labels": [], "entities": []}, {"text": "For this machine comprehension task, we created anew corpus, MCScript.", "labels": [], "entities": []}, {"text": "It contains a high number of questions that require commonsense knowledge for finding the correct answer.", "labels": [], "entities": []}, {"text": "11 teams from 4 different countries participated in this shared task, most of them used neu-ral approaches.", "labels": [], "entities": []}, {"text": "The best performing system achieves an accuracy of 83.95%, outperform-ing the baselines by a large margin, but still far from the human upper bound, which was found to beat 98%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997398257255554}]}], "introductionContent": [{"text": "Developing algorithms for understanding natural language is not trivial.", "labels": [], "entities": []}, {"text": "Natural language comes with its own complexity and inherent ambiguities.", "labels": [], "entities": []}, {"text": "Ambiguities can occur, for example, at the level of word meaning, syntactic structure, or semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7483827769756317}]}, {"text": "Traditionally, Natural Language Understanding (NLU) systems have resolved ambiguities using information from the textual context (e.g. neighboring words and sentences), for example via distributional methods.", "labels": [], "entities": []}, {"text": "However, many times context maybe absent or may lack sufficient information to resolve the ambiguity.", "labels": [], "entities": []}, {"text": "In such cases, it would be beneficial to include commonsense knowledge about the world in an NLU system.", "labels": [], "entities": []}, {"text": "For example, consider example (1).", "labels": [], "entities": []}, {"text": "(1) The waitress brought Rachel's order.", "labels": [], "entities": []}, {"text": "She ate the food with great pleasure.", "labels": [], "entities": []}, {"text": "Looking at the example in isolation, the person eating the food could be either Rachel or the waitress.", "labels": [], "entities": []}, {"text": "Using commonsense knowledge, or, more specifically, script knowledge about the RESTAU-RANT scenario, helps to resolve the referent of the pronoun: Rachel ordered the food.", "labels": [], "entities": [{"text": "RESTAU-RANT", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.6999879479408264}]}, {"text": "The person who orders the food is the customer.", "labels": [], "entities": []}, {"text": "So Rachel should eat the food, she thus refers to Rachel.", "labels": [], "entities": []}, {"text": "This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6651625434557596}]}, {"text": "In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts.", "labels": [], "entities": []}, {"text": "Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc.", "labels": [], "entities": []}, {"text": "(. The concept of scripts has its underpinnings in cognitive psychology and has been shown to bean important component of the human cognitive system.", "labels": [], "entities": []}, {"text": "From an application perspective, scripts have been shown to be useful fora variety of tasks, including story understanding, information extraction (, and drawing inferences from texts.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8124407827854156}, {"text": "information extraction", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.8222381472587585}]}, {"text": "Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers.", "labels": [], "entities": [{"text": "Factual knowledge", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8635739982128143}]}, {"text": "On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender.", "labels": [], "entities": []}, {"text": "Because of this implicitness, learning script knowledge from texts is very challenging.", "labels": [], "entities": []}, {"text": "There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge.", "labels": [], "entities": []}, {"text": "An example is the InScript ( , which contains short and simple narratives, that very explicitly mention script events and participants.", "labels": [], "entities": [{"text": "InScript", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.8884944319725037}]}, {"text": "The Dinners from Hell corpus () is a similar dataset centered around the EATING IN A RESTAURANT scenario.", "labels": [], "entities": [{"text": "EATING IN A RESTAURANT", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.5036052092909813}]}, {"text": "In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (), paraphrasing, event prediction (namely, the narrative cloze task) or story completion (e.g. the story cloze task T It was along day at work and I decided to stop at the gym before going home.", "labels": [], "entities": [{"text": "script modeling", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8503356575965881}, {"text": "event ordering", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.6995332688093185}, {"text": "event prediction", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7006559669971466}]}, {"text": "Iran on the treadmill and lifted some weights.", "labels": [], "entities": []}, {"text": "I decided I would also swim a few laps in the pool.", "labels": [], "entities": []}, {"text": "Once I was done working out, I went in the locker room and stripped down and wrapped myself in a towel.", "labels": [], "entities": []}, {"text": "I went into the sauna and turned on the heat.", "labels": [], "entities": []}, {"text": "I let it get nice and steamy.", "labels": [], "entities": []}, {"text": "I sat down and relaxed.", "labels": [], "entities": []}, {"text": "I let my mind think about nothing but peaceful, happy thoughts.", "labels": [], "entities": []}, {"text": "I stayed in therefor only about ten minutes because it was so hot and steamy.", "labels": [], "entities": []}, {"text": "When I got out, I turned the sauna off to save energy and took a cool shower.", "labels": [], "entities": []}, {"text": "I got out of the shower and dried off.", "labels": [], "entities": []}, {"text": "After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home.", "labels": [], "entities": []}, {"text": "Q1 Where did they sit inside the sauna?", "labels": [], "entities": []}, {"text": "a. on the floor b. on a bench Q2 How long did they stay in the sauna?", "labels": [], "entities": []}, {"text": "a. about ten minutes b. over thirty minutes ().", "labels": [], "entities": []}, {"text": "These tasks test a system's ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 156, "end_pos": 192, "type": "TASK", "confidence": 0.7516885250806808}]}, {"text": "Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension.", "labels": [], "entities": []}, {"text": "The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text.", "labels": [], "entities": []}, {"text": "In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text.", "labels": [], "entities": []}, {"text": "In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts.", "labels": [], "entities": []}, {"text": "For example, consider the short narrative in (1).", "labels": [], "entities": []}, {"text": "For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an information that is not given in the text.", "labels": [], "entities": []}, {"text": "The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the dataset used for our shared task.", "labels": [], "entities": []}, {"text": "Section 4.2 gives details about the setup of our task.", "labels": [], "entities": []}, {"text": "In Section 5, information about participating systems is given.", "labels": [], "entities": []}, {"text": "Results are presented and discussed in Sections 6 and 8, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our evaluation, we measured how well a system was capable of correctly answering questions that may involve commonsense knowledge.", "labels": [], "entities": []}, {"text": "As evaluation metric, we used accuracy, calculated as the ratio between correctly answered questions and all questions in our evaluation data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994578957557678}]}, {"text": "We also evaluated systems with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of techniques and resourced used by the participating systems.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of participating systems and the baselines on the six most frequent question types. The best  performance for each column is marked in bold print. Significant differences in results between two adjacent lines  are marked by an asterisk (* p<0.05) in the upper line.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9532085657119751}]}]}