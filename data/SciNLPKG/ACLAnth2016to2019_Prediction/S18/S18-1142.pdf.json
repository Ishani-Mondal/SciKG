{"title": [{"text": "UMBC at SemEval-2018 Task 8: Understanding Text about Malware", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the systems developed by the UMBC team for 2018 SemEval Task 8, Se-cureNLP (Semantic Extraction from Cyberse-cUrity REports using Natural Language Processing).", "labels": [], "entities": [{"text": "UMBC", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8847991228103638}, {"text": "SemEval Task 8", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.6572727759679159}]}, {"text": "We participated in three of the sub-tasks: (1) classifying sentences as being relevant or irrelevant to malware, (2) predicting token labels for sentences, and (4) predicting attribute labels from the Malware Attribute Enu-meration and Characterization vocabulary for defining malware characteristics.", "labels": [], "entities": [{"text": "predicting token labels", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.8775549332300822}, {"text": "predicting attribute labels", "start_pos": 164, "end_pos": 191, "type": "TASK", "confidence": 0.8768843412399292}]}, {"text": "We achieved F1 scores of 50.34/18.0 (dev/test), 22.23", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9990968704223633}]}], "introductionContent": [{"text": "Task 8 for SemEval 2018 asked participants to work on a set of related sub-tasks involving analyzing information from text about malware drawn from the Advanced Persistent Threats Notes collection () using the semantic framework found in the Malware Attribute Enumeration and Characterization language).", "labels": [], "entities": [{"text": "SemEval 2018", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.9031613171100616}, {"text": "Advanced Persistent Threats Notes collection", "start_pos": 152, "end_pos": 196, "type": "DATASET", "confidence": 0.540817254781723}]}, {"text": "The task was composed of four related sub-tasks that could be part of a processing pipeline for an information extraction system for cybersecurity related text (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7189168184995651}]}, {"text": "Subtask 1 required classifying a sentence as being relevant or irrelevant for inferring malware actions and capabilities.", "labels": [], "entities": []}, {"text": "Subtask 2 involved predicting token labels for entities, actions and modifiers in sentences.", "labels": [], "entities": [{"text": "predicting token labels for entities, actions and modifiers in sentences", "start_pos": 19, "end_pos": 91, "type": "TASK", "confidence": 0.7362038872458718}]}, {"text": "Subtask 3, which we did not undertake, expanded on subtask 2 by asking participants to label relevant relations between the entities.", "labels": [], "entities": []}, {"text": "Subtask 4 required predicting more detailed attribute labels, including ActionName, Capability, StrategicObjectives and TacticalObjectives, drawn from the MAEC vocabulary.", "labels": [], "entities": [{"text": "MAEC vocabulary", "start_pos": 155, "end_pos": 170, "type": "DATASET", "confidence": 0.8762576282024384}]}, {"text": "One of our aims is to better understand the differences between cybersecurity text and general, non-cybersecurity text; another is to also better understand differences and variation within cybersecurity texts.", "labels": [], "entities": []}, {"text": "To that end, we focus on learning and extracting better representations of the input reports.", "labels": [], "entities": []}, {"text": "Specifically, for our approaches, we focus on approaches that incorporate additional, domain-specific knowledge into our system, and we use these enhanced representations and features in well-studied classification, representation, and sequence prediction models.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.6580277234315872}]}], "datasetContent": [{"text": "For Subtask 1, we used all 65 files available as part of SemEval Task 8.", "labels": [], "entities": [{"text": "SemEval Task 8", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7586052219072977}]}, {"text": "We tuned and tested our model on development data available as part of SemEval.", "labels": [], "entities": []}, {"text": "For logistic regression we swept the L2 regularization coefficient ({100, 10, 1, 0.1, 0.01, 0.001}) and chose the value that gave best performance on the development dataset.", "labels": [], "entities": []}, {"text": "For neural approaches we used stochastic gradient descent with momentum of 0.4 for LSTM and 0.9 for MLP.", "labels": [], "entities": [{"text": "MLP", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.6880748271942139}]}, {"text": "We tried multiple learning rates and chose one which gave best performance on the development dataset.", "labels": [], "entities": []}, {"text": "We chose starting learning rate of 0.2 for LSTM and 0.1 for MLP.", "labels": [], "entities": [{"text": "MLP", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.6849145293235779}]}, {"text": "We also tried using Adam optimizer () with the same learning rate as MLP but found the resulting model labeled all test instances with the majority class.", "labels": [], "entities": []}, {"text": "For our implementation we used Keras (Chollet,  2015) with a Tensorflow backend to train neural network based models and Gensim to train word embeddings.", "labels": [], "entities": []}, {"text": "We used Scikit-learn (Pedregosa et al., 2011) for Logistic Regression.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8965577781200409}]}, {"text": "For the LSTM, we let the size of the input sequence be the maximum length of all sentences in the batch and padded shorter sentences with zero vectors.", "labels": [], "entities": []}, {"text": "The input was a matrix of dimension l\u00d7d where dis the size of embedding vector and l is the length of the longest sentence.", "labels": [], "entities": []}, {"text": "We used the CRF++ toolkit () to develop our conditional random field (CRF) models.", "labels": [], "entities": []}, {"text": "For the official evaluation, we ran our system on Test set provided by SemEval2018.", "labels": [], "entities": [{"text": "Test set provided by SemEval2018", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.7798686146736145}]}, {"text": "The test set contains 13,080 tokens in total.", "labels": [], "entities": []}, {"text": "The official scoring reported our F1 performance of 22 for strict scoring, and 32 for relaxed scoring.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.99369215965271}]}, {"text": "Our F1-score for subtask 2 are generally on par with the baselines (23 for the strict, and 31 for the relaxed, measures).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9989981055259705}]}, {"text": "Detailed performance analyses are shown in Tables 4 and 5.", "labels": [], "entities": []}, {"text": "demonstrates that our system performance of predicting Entity is lower than Action and Modifier.", "labels": [], "entities": [{"text": "predicting Entity", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8568554818630219}]}, {"text": "We believe this is because malware-related entities are different from other text; in particular, they can be quite long.", "labels": [], "entities": []}, {"text": "For example, the following (gold test) entity is along clause with complex syntactic structure: 'method of leaving the encoded file on disk and only decoding it in memories.'", "labels": [], "entities": []}, {"text": "This entire clause is labeled as an Entity.", "labels": [], "entities": []}, {"text": "Despite the dependency features, our system cannot identify these long spans as an entity.", "labels": [], "entities": []}, {"text": "Another example of this limitation is shown in.", "labels": [], "entities": []}, {"text": "This is a rich area for future improvement.", "labels": [], "entities": []}, {"text": "To train the AWE model we used all 456 APT reports as text corpus.", "labels": [], "entities": [{"text": "AWE", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6177141070365906}]}, {"text": "In addition we used keywords for each attribute label described in MAEC vocabulary) and gold annotation given for 65 APT reports available as part of the SemEval task to create text annotation.", "labels": [], "entities": [{"text": "MAEC vocabulary", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.8147138059139252}, {"text": "SemEval task", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.8767762184143066}]}, {"text": "To create text annotation we collected keywords from attribute label descriptions and extracted the token groups from the gold annotations.", "labels": [], "entities": []}, {"text": "Token groups consist of the subject, action and object linked to each other via relation labels.", "labels": [], "entities": []}, {"text": "We used these token words and keywords to create text annotation; we deleted stop words.", "labels": [], "entities": []}, {"text": "For example, one token group extracted from gold annotation is \"these configuration issued commands to attack following domain and IPs.\"", "labels": [], "entities": []}, {"text": "After deleting stop words this token group we get \"configuration,\" \"issued,\" \"commands,\" \"attack,\" \"domain,\" and \"IPs.\"", "labels": [], "entities": []}, {"text": "In the gold annotation, this token group has label Capability12 in attribute category of Capability.", "labels": [], "entities": []}, {"text": "In MAEC vocabulary) keywords given for this capability label are \"machine access,\" \"control,\" \"execute,\" \"terminate,\" and \"create.\"", "labels": [], "entities": []}, {"text": "All these token words and keywords will have an annotation of Capability12 in our AWE model.", "labels": [], "entities": [{"text": "AWE", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8495932221412659}]}, {"text": "After creating the text annotation we train an AWE model with 100 dimension feature vectors, window size 5 and negative sampling.", "labels": [], "entities": [{"text": "AWE", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6938664317131042}]}, {"text": "After training embeddings we use these embeddings to create features for classifier.", "labels": [], "entities": []}, {"text": "We use average embeddings of all the words in each token group to create classifier instance.", "labels": [], "entities": []}, {"text": "We use SVM as classifier.", "labels": [], "entities": []}, {"text": "On the test dataset we get F-score of 0.19.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.999607503414154}]}], "tableCaptions": [{"text": " Table 1: Performance on Task 1 (dev-data) for each of the model we implemented. We used the best MLP model  for SemEval submission (test-data) and had F1 score of 18. We found LR and MLP to always label majority class  resulting in zero F1 score using Wiki embeddings.", "labels": [], "entities": [{"text": "SemEval submission", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.9240750372409821}, {"text": "F1 score", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9898575842380524}, {"text": "F1 score", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9835277795791626}]}, {"text": " Table 2: LSTM Performance (dev data). We offer these  as supplementary evaluations.", "labels": [], "entities": []}, {"text": " Table 4: Official Task 2 scores on Test set", "labels": [], "entities": [{"text": "Test set", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9661318063735962}]}, {"text": " Table 5: Official Task 2 relaxed/token-level scores on  Test set", "labels": [], "entities": [{"text": "relaxed", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.7776657938957214}, {"text": "Test set", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9604885280132294}]}]}