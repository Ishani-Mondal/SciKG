{"title": [{"text": "ECNU at SemEval-2018 Task 10: Evaluating Simple but Effective Features on Machine Learning Methods for Semantic Difference Detection", "labels": [], "entities": [{"text": "ECNU at SemEval-2018 Task 10", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7303728699684143}, {"text": "Semantic Difference Detection", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.8316595355669657}]}], "abstractContent": [{"text": "This paper describes the system we submitted to Task 10 (Capturing Discriminative Attributes) in SemEval 2018.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes) in SemEval 2018", "start_pos": 57, "end_pos": 109, "type": "TASK", "confidence": 0.824365530695234}]}, {"text": "Given a triple (word 1 , word 2 , attribute), this task is to predict whether it exemplifies a semantic difference or not.", "labels": [], "entities": []}, {"text": "We design and investigate several word embedding features, PMI features and Word-Net features together with supervised machine learning methods to address this task.", "labels": [], "entities": [{"text": "Word-Net", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9198346734046936}]}, {"text": "Officially released results show that our system ranks above average.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Capturing Discriminative Attributes task) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes task", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.881850853562355}, {"text": "semantic difference detection", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.8368298212687174}, {"text": "machine translation", "start_pos": 248, "end_pos": 267, "type": "TASK", "confidence": 0.8189603388309479}]}, {"text": "The goal of this task is to predict whether a word is a discriminative attribute between two concepts.", "labels": [], "entities": []}, {"text": "Specifically, given two concepts and an attribute, the task is to predict whether the first concept has this attribute but the second concept does not.", "labels": [], "entities": []}, {"text": "For example, given the concepts apple and pineapple, participants are required to predict whether the attribute seeds characterizes the first concept but not the other.", "labels": [], "entities": []}, {"text": "In other words, semantic difference detection is a binary classification task: given a triple (apple, pineapple, seeds), the task is to determine whether it exemplifies a semantic difference or not, i.e., positive or negative.", "labels": [], "entities": [{"text": "semantic difference detection", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7886549830436707}]}, {"text": "If word 1 has a specific attribute but word 2 does not, then the correlation of attribute and word 1 should be higher than that of attribute and word 2 . The semantic similarity is in the same way.", "labels": [], "entities": []}, {"text": "In view of the above considerations, to address this task, we explore supervised machine learn- ing methods which use PMI features and WordNet features.", "labels": [], "entities": []}, {"text": "In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features).", "labels": [], "entities": []}, {"text": "Therefore we use word embeddings to obtain the semantic similarity as word embedding features.", "labels": [], "entities": []}, {"text": "Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the system performance, the official evaluation criterion is macro-averaged F1-score,  which is calculated among two classes (positive and negative) as follows:  Firstly, in order to explore the effectiveness of each feature type, we perform a series of experiments.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9207642674446106}]}, {"text": "lists the comparison of different contributions made by different features on development data with Logistic Regression algorithm.", "labels": [], "entities": []}, {"text": "We observe the following findings.", "labels": [], "entities": []}, {"text": "(1) The simple PMI features and word embedding similarity features are effective for semantic difference detection and it shows the effectiveness of semantic similarity and correlation for semantic difference detection.", "labels": [], "entities": [{"text": "semantic difference detection", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.838514486948649}, {"text": "semantic difference detection", "start_pos": 189, "end_pos": 218, "type": "TASK", "confidence": 0.7785747448603312}]}, {"text": "(2) The combination of the first three features not only achieves the best performance for the overall classification but also for each class.", "labels": [], "entities": []}, {"text": "These three types of features make contributions to semantic difference detection task.", "labels": [], "entities": [{"text": "semantic difference detection task", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.8865263909101486}]}, {"text": "Therefore we use these features in following experiments.", "labels": [], "entities": []}, {"text": "(3) The result of merging the WE operation features is not as good as we expected and the possible reason is that the dimensionality of WE operation features is quite huger than the other three features(3, 000 Vs. 16), which dominates the performance of classification rather than other low dimension features.", "labels": [], "entities": []}, {"text": "And the operations of word vectors are too simple to detect the semantic difference.", "labels": [], "entities": []}, {"text": "(4) The WordNet features are not as effective as expected, and the reason maybe that in many cases the attribute words do not appear in the sense definitions of concepts, so we cannot get nonzero features.", "labels": [], "entities": []}, {"text": "Secondly, we also explore the performance of different supervised machine learning algorithms.: Performance of different features on development data in terms of F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9982727766036987}]}, {"text": "\".+\" means to add current features to the previous feature set.", "labels": [], "entities": []}, {"text": "The numbers in the brackets are the performance increments compared with the previous results.", "labels": [], "entities": []}, {"text": "(1) LR and SVM achive better results than the other supervised machine learning algorithms and Logistic Regression algorithm achieves the best performance when considering single classification algorithm.", "labels": [], "entities": []}, {"text": "(2) The ensemble of the top 3 machine learning algorithms (LR + SVM + XGBoost) achives higher performance than any single learning algorithm, i.e., 0.663.", "labels": [], "entities": []}, {"text": "Based on the above results, the system configuration of our final submission is ensemble of LR, SVM and XGBoost algorithms with WordNet, P-MI and WE similarity features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9423580169677734}]}, {"text": "The models are trained on both training and development data sets.", "labels": [], "entities": []}, {"text": "shows the results of our system and the top-ranked systems provided by organizers for this semantic difference detection task.", "labels": [], "entities": [{"text": "semantic difference detection task", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.8012684136629105}]}, {"text": "Compared with the top ranked systems, there is much room for improvement in our work.", "labels": [], "entities": []}, {"text": "There are several possible reasons for this performance lag.", "labels": [], "entities": []}, {"text": "First, the features we used are simple.", "labels": [], "entities": []}, {"text": "We only record some semantic similarity information and correlations between words.", "labels": [], "entities": []}, {"text": "More complex interactions of word vectors could be tried.", "labels": [], "entities": []}, {"text": "Second, we only extract features from three words that need to be classified and have not used some extended resources like the sentences returned from search engines when retrieving these words.: Performance of our system and the top-ranked systems.", "labels": [], "entities": []}, {"text": "The numbers in the brackets are the official rankings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of different features on develop- ment data in terms of F1-score. \".+\" means to add cur- rent features to the previous feature set. The numbers in  the brackets are the performance increments compared  with the previous results.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.998367965221405}]}, {"text": " Table 4: Performance of different learning algorithms  on development data in terms of F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9987218976020813}]}]}