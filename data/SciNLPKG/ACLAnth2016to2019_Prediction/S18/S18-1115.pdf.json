{"title": [{"text": "SemEval-2018 Task 9: Hypernym Discovery", "labels": [], "entities": [{"text": "SemEval-2018 Task 9", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8641748825709025}, {"text": "Hypernym Discovery", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6847687661647797}]}], "abstractContent": [{"text": "This paper describes the SemEval 2018 Shared Task on Hypernym Discovery.", "labels": [], "entities": [{"text": "SemEval 2018 Shared Task on Hypernym Discovery", "start_pos": 25, "end_pos": 71, "type": "TASK", "confidence": 0.7833030223846436}]}, {"text": "We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input.", "labels": [], "entities": []}, {"text": "Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus.", "labels": [], "entities": []}, {"text": "We proposed five different sub-tasks covering three languages (English, Span-ish, and Italian), and two specific domains of knowledge in English (Medical and Music).", "labels": [], "entities": []}, {"text": "Participants were allowed to compete in any or all of the subtasks.", "labels": [], "entities": []}, {"text": "Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks.", "labels": [], "entities": []}, {"text": "Data, results and further information about the task can be found at https://competitions.", "labels": [], "entities": []}, {"text": "codalab.org/competitions/17119.", "labels": [], "entities": [{"text": "codalab.org/competitions/17119", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.6600893795490265}]}], "introductionContent": [{"text": "Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition.", "labels": [], "entities": []}, {"text": "It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (: indeed, successfully identifying this lexical relation substantially improves Question Answering applications, Textual Entailment and Semantic Search systems.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.8600605130195618}, {"text": "Textual Entailment", "start_pos": 242, "end_pos": 260, "type": "TASK", "confidence": 0.8137144148349762}]}, {"text": "In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (, which are in turn useful resources for downstream tasks such as web retrieval, website navigation or records management ().", "labels": [], "entities": [{"text": "web retrieval", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.7452945411205292}, {"text": "website navigation or records management", "start_pos": 189, "end_pos": 229, "type": "TASK", "confidence": 0.6950311303138733}]}, {"text": "Generally, evaluation benchmarks for modeling hypernymy have been designed such that inmost cases they are reduced to binary classification, where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms.", "labels": [], "entities": []}, {"text": "Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (.", "labels": [], "entities": [{"text": "hypernym detection task", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.7879850665728251}]}, {"text": "In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair).", "labels": [], "entities": []}, {"text": "Crucially,  proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain's vocabulary, and given an input term, discover its best (list of) candidate hypernyms.", "labels": [], "entities": []}, {"text": "This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications).", "labels": [], "entities": []}, {"text": "In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains.", "labels": [], "entities": []}, {"text": "The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting.: Some example terms and hypernyms extracted from different sources (see Section 4.1.4), for each of the subtasks and languages considered in the task.", "labels": [], "entities": [{"text": "hypernymy modeling", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8079929649829865}]}], "datasetContent": [{"text": "Parting ways from the classic precision-recall-F 1 metrics used so far in hypernym detection/extraction, we decided to evaluate this shared task as a soft ranking problem.", "labels": [], "entities": [{"text": "precision-recall-F 1 metrics", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.9597354332605997}, {"text": "hypernym detection/extraction", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.8933659642934799}]}, {"text": "Systems were evaluated over the top 15 (at most) hypernyms retrieved for each input term, which let us assess their performance through Information Retrieval metrics.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.7624485790729523}]}, {"text": "Let us briefly introduce each of them.", "labels": [], "entities": []}, {"text": "Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9085453848044077}]}, {"text": "We use MAP as the main evaluation metric of this task.", "labels": [], "entities": [{"text": "MAP", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.6677604913711548}]}, {"text": "Intuitively, this metric should give a fine estimate on the capability of a system to retrieve a sizable number of hypernyms from textual data, as well as considering the precision of each of them.", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.9984474778175354}]}, {"text": "Formally: where Q is a sample of experiment runs, AP(\u00b7) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space.", "labels": [], "entities": [{"text": "AP", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9987345337867737}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.786612868309021}]}, {"text": "Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.9682597716649374}]}, {"text": "MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: where rank i refers to the rank position of the first relevant outcome for the ith run.", "labels": [], "entities": []}, {"text": "While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8485288918018341}, {"text": "collocation recognition", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7397606372833252}]}, {"text": "In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k \u2208 {1, 3, 5, 15}.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of participating systems and baselines, along with their main features (i.e. with or without  supervision, and usage of external resources).", "labels": [], "entities": []}, {"text": " Table 4: Results for the English subtask (1A). Baselines are marked with *, and those system participating only on  Concepts or Entities are shown at the bottom and marked with either 'C' or 'E'.", "labels": [], "entities": [{"text": "English subtask (1A)", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.7451053857803345}]}, {"text": " Table 5: Results for the Italian subtask (1B). Baselines are marked with *.", "labels": [], "entities": [{"text": "Italian subtask (1B)", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.8832169890403747}]}, {"text": " Table 6: Results for the Spanish subtask (1C). Baselines are marked with *.", "labels": [], "entities": []}, {"text": " Table 7: Results for the Music subtask (2B). Baselines are marked with *.", "labels": [], "entities": []}, {"text": " Table 8: Results for the Medical subtask (2A). Base- lines are marked with * and cross evaluation systems  are followed by ' CE'.", "labels": [], "entities": [{"text": "CE", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9805092811584473}]}]}