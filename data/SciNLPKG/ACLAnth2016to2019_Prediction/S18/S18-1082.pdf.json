{"title": [], "abstractContent": [{"text": "This paper describes the ALANIS system that participated in Task 3 of SemEval-2018.", "labels": [], "entities": [{"text": "ALANIS", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.8447648882865906}]}, {"text": "We develop a system for detection of irony, as well as the detection of three types of irony: verbal polar irony, other verbal irony, and situa-tional irony.", "labels": [], "entities": [{"text": "detection of irony", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8497615655263265}]}, {"text": "The system uses a logistic regression model in subtask A and a voted classifier in subtask B, both of which rely on manually developed features to identify ironic tweets.", "labels": [], "entities": []}, {"text": "ALANIS placed 34th of 43 systems in subtask A and 26th of 31 systems in subtask B.", "labels": [], "entities": [{"text": "ALANIS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6028819680213928}]}], "introductionContent": [{"text": "With the invention and growth of various social networking sites, irony and other creative linguistic devices have become increasingly prevalent in online content.", "labels": [], "entities": []}, {"text": "Particularly when considering microblogging platforms like Twitter, which encourage users to share their thoughts and opinions on a wide variety of topics, the use of irony can be extremely common.", "labels": [], "entities": []}, {"text": "This can have strong implications for various problems in natural language processing, which often have difficulty in processing this ironic content (e.g.,), thus motivating the development of an accurate irony detection system.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 205, "end_pos": 220, "type": "TASK", "confidence": 0.8017674088478088}]}, {"text": "While irony has many varying definitions, it is defined by the SemEval task organizers as a trope or figurative language whose actual meaning differs from what is literally enunciated.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.8122915029525757}]}, {"text": "Our system, ALANIS (Automated Location and Naming of Ironic Sentences), uses a manually developed feature set and a logistic regression classifier for subtask A and a voted classifier for subtask B, achieving mean accuracies of .650 and .607 respectively on the training set and .512 and .434 * Authors are ordered alphabetically by their first name.", "labels": [], "entities": [{"text": "ALANIS", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9805123805999756}, {"text": "Automated Location and Naming of Ironic Sentences)", "start_pos": 20, "end_pos": 70, "type": "TASK", "confidence": 0.6838155761361122}]}, {"text": "respectively on the test set.", "labels": [], "entities": []}, {"text": "The F1-scores are .469 and .276 on the test set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994329810142517}]}], "datasetContent": [{"text": "Our final result is that logistic regression is most accurate classifier for subtask A and the voting system is most accurate for subtask B. shows the average cross validation scores on the training set, while shows the scores when our system is trained on the whole training set and evaluated on the test set.", "labels": [], "entities": []}, {"text": "BOW+NB stands for the Bag of Words Naive Bayes baseline, while LR, RF, and Vote represent are based on 5-fold cross validation with the training data.", "labels": [], "entities": [{"text": "BOW", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9668933153152466}, {"text": "Bag of Words Naive Bayes baseline", "start_pos": 22, "end_pos": 55, "type": "DATASET", "confidence": 0.6073834945758184}, {"text": "RF", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9045521020889282}]}, {"text": "As such we expected comparable results when we applied our classifiers to the test data.", "labels": [], "entities": []}, {"text": "However as can be seen in this is not the case.", "labels": [], "entities": []}, {"text": "The results for LR decline from .65 to .51, and for Vote from .607 to .434.", "labels": [], "entities": [{"text": "LR", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.4698045253753662}, {"text": "Vote", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.5861459374427795}]}, {"text": "While some variation is to be expected, this was surprising to us.", "labels": [], "entities": []}, {"text": "We hypothesize one of two possible explanations.", "labels": [], "entities": []}, {"text": "First, our methods may have overfit the training data and so do not generalize well to other data.", "labels": [], "entities": []}, {"text": "However, since we employed cross validation we are not certain how likely this explanation proves to be.", "labels": [], "entities": []}, {"text": "The second explanation maybe that the test data is in someway different from the training data, to the extent that a model learnt on the training data may not farewell on the test data.", "labels": [], "entities": []}, {"text": "We have not yet analyzed the test data closely enough to resolve this question, but consider this to bean important step in understanding our results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of classifiers on training set", "labels": [], "entities": []}, {"text": " Table 2: Performance of classifiers on test set", "labels": [], "entities": []}, {"text": " Table 3: Weights (in absolute) of features", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9904690384864807}]}]}