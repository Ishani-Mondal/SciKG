{"title": [{"text": "Halo: Learning Semantics-Aware Representations for Cross-Lingual Information Extraction", "labels": [], "entities": [{"text": "Cross-Lingual Information Extraction", "start_pos": 51, "end_pos": 87, "type": "TASK", "confidence": 0.6918181777000427}]}], "abstractContent": [{"text": "Cross-lingual information extraction (CLIE) is an important and challenging task, especially in low resource scenarios.", "labels": [], "entities": [{"text": "Cross-lingual information extraction (CLIE)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.832087387641271}]}, {"text": "To tackle this challenge , we propose a training method, called Halo, which enforces the local region of each hidden state of a neural model to only generate target tokens with the same semantic structure tag.", "labels": [], "entities": []}, {"text": "This simple but powerful technique enables a neural model to learn semantics-aware representations that are robust to noise, without introducing any extra parameter, thus yielding better generalization in both high and low resource settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual information extraction (CLIE) is the task of distilling and representing factual information in a target language from the textual input in a source language ().", "labels": [], "entities": [{"text": "Cross-lingual information extraction (CLIE)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8290850818157196}]}, {"text": "For example, illustrates a pair of input Chinese sentence and its English predicateargument information , where predicate and argument are well used semantic structure tags.", "labels": [], "entities": []}, {"text": "It is of great importance to solve the task, as to provide viable solutions to extracting information from the text of languages that suffer from no or little existing information extraction tools.", "labels": [], "entities": [{"text": "extracting information from the text of languages", "start_pos": 79, "end_pos": 128, "type": "TASK", "confidence": 0.8198179432324001}]}, {"text": "Neural models have empirically proven successful in this task (, but still remain unsatisfactory in low resource (i.e. small number of training samples) settings.", "labels": [], "entities": []}, {"text": "These neural models learn to summarize a given source sentence and target prefix into a hidden state, which aims to generate the correct next target token after being * equal contribution The predicate-argument information is usually denoted by relation tuples.", "labels": [], "entities": []}, {"text": "In this work, we adopt the tree-structured representation generated by PredPatt (), which was a lightweight tool available at https://github.com/hltcoe/PredPatt.", "labels": [], "entities": [{"text": "PredPatt", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.7970960736274719}]}, {"text": "Chinese input text (a) and linearized English PredPatt output (b), where ':p' and blue stand for predicate while ':a' and purple denote argument.", "labels": [], "entities": []}, {"text": "passed through an output layer.", "labels": [], "entities": []}, {"text": "As each member in the target vocabulary is essentially either predicate or argument, a random perturbation on the hidden state should still be able to yield a token with the same semantic structure tag.", "labels": [], "entities": []}, {"text": "This inductive bias motivates an extra term in training objective, as shown in, which enforces the surroundings of any learned hidden state to generate tokens with the same semantic structure tag (either predicate or argument) as the centroid.", "labels": [], "entities": []}, {"text": "We call this technique Halo, because the process of each hidden state taking up its surroundings is analogous to how the halo is formed around the sun.", "labels": [], "entities": []}, {"text": "The method is believed to help the model generalize better, by learning more semantics-aware and noise-insensitive hidden states without introducing extra parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on several real-world CLIE datasets measured by BLEU () and F1, as proposed by.", "labels": [], "entities": [{"text": "CLIE datasets", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.7864046692848206}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.998760461807251}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.998110294342041}]}, {"text": "For the generated linearized PredPatt outputs and their references, the former metric 4 measures their n-gram similarity, and the latter measures their token-level overlap.", "labels": [], "entities": []}, {"text": "In fact, F1 is computed separately for predicate and argument, as F1 PRED and F1 ARG respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9982820749282837}, {"text": "F1 PRED", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.8129294514656067}, {"text": "F1 ARG", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.8555947244167328}]}, {"text": "Multiple datasets were used to demonstrate the effectiveness of our proposed method, where one sample in each dataset is a source language sentence paired with its linearized English PredPatt output.", "labels": [], "entities": []}, {"text": "These datasets were first introduced as the DARPA LORELEI Language Packs (, and then used for this task by.", "labels": [], "entities": [{"text": "DARPA LORELEI Language Packs", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.7257527858018875}]}, {"text": "As shown in table 1, the CHINESE dataset has almost one million training samples and a high token/type ratio, while the oth- The MOSES implementation () was used as in all the previous work on this task.", "labels": [], "entities": [{"text": "CHINESE dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8466149568557739}]}, {"text": "ers are low resourced, meaning they have much fewer samples and lower token/type ratios.", "labels": [], "entities": []}, {"text": "In experiments, instead of using the full vocabularies shown in table 1, we set a minimum count threshold for each dataset, to replace the rare words by a special out-of-vocabulary symbol.", "labels": [], "entities": []}, {"text": "These thresholds were tuned on dev sets.", "labels": [], "entities": []}, {"text": "The Beta distribution is very flexible.", "labels": [], "entities": []}, {"text": "In general, its variance is a decreasing function of \u03b1 + \u03b2, and when \u03b1 + \u03b2 is fixed, the mean is an increasing function of \u03b1.", "labels": [], "entities": []}, {"text": "In our experiments, we fixed \u03b1 + \u03b2 = 20 and only lightly tuned \u03b1 on dev sets.", "labels": [], "entities": []}, {"text": "Optimal values of \u03b1 stay close to 1.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.9496055841445923}]}], "tableCaptions": [{"text": " Table 1: Statistics of each dataset.", "labels": [], "entities": []}, {"text": " Table 2: BLEU and F1 scores of different models on all these datasets, where PRED stands for predicate and ARG for argument.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992974996566772}, {"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9989168643951416}, {"text": "PRED", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.922492504119873}, {"text": "ARG", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9897173643112183}]}]}