{"title": [{"text": "Villani at SemEval-2018 Task 8: Semantic Extraction from Cybersecurity Reports using Representation Learning", "labels": [], "entities": [{"text": "SemEval-2018 Task 8", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8428480823834738}, {"text": "Semantic Extraction from Cybersecurity Reports", "start_pos": 32, "end_pos": 78, "type": "TASK", "confidence": 0.8292991042137146}]}], "abstractContent": [{"text": "In this paper, we describe our proposal for the task of Semantic Extraction from Cybersecu-rity Reports.", "labels": [], "entities": [{"text": "Semantic Extraction from Cybersecu-rity Reports", "start_pos": 56, "end_pos": 103, "type": "TASK", "confidence": 0.8176514625549316}]}, {"text": "The goal is to explore if natural language processing methods can provide relevant and actionable knowledge to contribute to better understand malicious behavior.", "labels": [], "entities": []}, {"text": "Our method consists of an attention-based Bi-LSTM which achieved competitive performance of 0.57 for the Subtask 1.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.8443402051925659}]}, {"text": "In the due process we also present ablation studies across multiple embeddings and their level of representation and also report the strategies we used to mitigate the extreme imbalance between classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cybersecurity represents one of the most comprehensive and challenging tasks to tackle from a data-driven perspective.", "labels": [], "entities": []}, {"text": "It is inherently technical, covering field such as networking and programming languages, but at the same time, it considers human aspects, such as intent, trust and strategy among benign and malicious agents.", "labels": [], "entities": []}, {"text": "This rich mixture makes it an ideal playground for machine learning, to extract patterns and characterize the interaction between the different set of actors involved.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7933337092399597}]}, {"text": "Moreover, as we can collect large amounts of data from security related sources, such as trace logs and reports, the level of generalization that machine learning methods could achieve could increase.", "labels": [], "entities": []}, {"text": "Nevertheless, several challenges also emerge such as noise, lack structure, unavailability of annotated sources and a characteristic class imbalance when data is labeled.", "labels": [], "entities": []}, {"text": "Therefore, for machine learning to be considered useful in a cybersecurity context, it must provide robust and reliable results, overcoming the aforementioned issues.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.716286763548851}]}, {"text": "In that sense, how we represent the data plays a key role, as it is known that in any machine learning setting, different feature representations yield to different results, entangling different explanatory factors of variation on the data (.", "labels": [], "entities": []}, {"text": "We are interested in study the tradeoff between the use of hand crafted features the process of automatically learning feature representations.", "labels": [], "entities": []}, {"text": "In that sense, the present SemEval Task 8 () represents a relevant scenario to test several hypotheses in the context of a controlled semantic extraction competition.", "labels": [], "entities": [{"text": "SemEval Task 8", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8656711578369141}, {"text": "semantic extraction competition", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.7856831252574921}]}, {"text": "The dataset, provided by, contains over 6,800 labeled sentences from 39 malware reports.", "labels": [], "entities": []}, {"text": "From the subtasks, we focused on the first one, as it provides compact goal to assess a proof of concept.", "labels": [], "entities": []}, {"text": "Our approach consists on the use of an attention based LSTM-based recurrent architecture which is capable of learning sentence level feature representations at both character and token level.", "labels": [], "entities": []}, {"text": "Additionally, we prepend an embedding layer from which pretrained feature vectors can be associated to the tokens.", "labels": [], "entities": []}, {"text": "Given the natural class imbalance of the data, we tried several techniques to alleviate generalization issues.", "labels": [], "entities": []}, {"text": "Our results show that out approach can outperform the baselines, reaching up to 0.57 in the competition leaderboard for the Subtask 1.", "labels": [], "entities": []}, {"text": "Nevertheless, the actual scores show that the task is far from being solved, illustrating the difficulty of the problem and the need for more powerful methods that allow us to obtain more expressive feature representations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}