{"title": [{"text": "Exploiting Partially Annotated Data for Temporal Relation Extraction", "labels": [], "entities": [{"text": "Temporal Relation Extraction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.9333245754241943}]}], "abstractContent": [{"text": "Annotating temporal relations (TempRel) between events described in natural language is known to belabor intensive, partly because the total number of TempRels is quadratic in the number of events.", "labels": [], "entities": [{"text": "Annotating temporal relations (TempRel) between events described in natural language", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.8355094641447067}]}, {"text": "As a result, only a small number of documents are typically annotated , limiting the coverage of various lexi-cal/semantic phenomena.", "labels": [], "entities": []}, {"text": "In order to improve existing approaches, one possibility is to make use of the readily available, partially annotated data (P as in partial) that cover more documents.", "labels": [], "entities": []}, {"text": "However, missing annotations in P are known to hurt, rather than help, existing systems.", "labels": [], "entities": []}, {"text": "This work is a case study in exploring various usages of P for TempRel extraction.", "labels": [], "entities": [{"text": "TempRel extraction", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8324521780014038}]}, {"text": "Results show that despite missing annotations, P is still a useful supervision signal for this task within a constrained bootstrapping learning framework.", "labels": [], "entities": []}, {"text": "The system described in this system is publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding the temporal information in natural language text is an important NLP task.", "labels": [], "entities": []}, {"text": "A crucial component is temporal relation (TempRel; e.g., before or after) extraction (.", "labels": [], "entities": []}, {"text": "The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels.", "labels": [], "entities": []}, {"text": "Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges -a highly labor intensive task due to two reasons.", "labels": [], "entities": []}, {"text": "One is that many edges require extensive reasoning over multiple sentences and labeling them is time-consuming.", "labels": [], "entities": []}, {"text": "Perhaps more importantly, the other reason is that #edges is quadratic in #nodes.", "labels": [], "entities": []}, {"text": "If labeling an edge takes 30 seconds (already an optimistic estimation), atypical document with 50 nodes would take more than 10 hours to annotate.", "labels": [], "entities": []}, {"text": "Even if existing annotation schemes make a compromise by only annotating edges whose nodes are from a same sentence or adjacent sentences ), it still takes more than 2 hours to fully annotate atypical document.", "labels": [], "entities": []}, {"text": "Consequently, the only fully annotated dataset, TB-Dense ( ), contains only 36 documents, which is rather small compared with datasets for other NLP tasks.", "labels": [], "entities": []}, {"text": "A small number of documents may indicate that the annotated data provide a limited coverage of various lexical and semantic phenomena, since a document is usually \"homogeneous\" within itself.", "labels": [], "entities": []}, {"text": "In contrast to the scarcity of fully annotated datasets (denoted by F as in full), there are actually some partially annotated datasets as well (denoted by P as in partial); for example, TimeBank ( and AQUAINT) cover in total more than 250 documents.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 187, "end_pos": 195, "type": "DATASET", "confidence": 0.9473351240158081}]}, {"text": "Since annotators are not required to label all the edges in these datasets, it is less labor intensive to collect P than to collect F.", "labels": [], "entities": []}, {"text": "However, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both.", "labels": [], "entities": [{"text": "TempRel extraction", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7694714963436127}]}, {"text": "No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful.", "labels": [], "entities": []}, {"text": "This work is a case study in exploring various usages of P in the TempRel extraction task.", "labels": [], "entities": [{"text": "TempRel extraction task", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8762017091115316}]}, {"text": "We empirically show that P is indeed useful within a (constrained) bootstrapping type of learning approach.", "labels": [], "entities": []}, {"text": "This case study is interesting from two perspectives.", "labels": [], "entities": []}, {"text": "In practice, supervision signals may not always be perfect: they maybe noisy, only partial, based on different annotation schemes, or even on different (but relevant) tasks; incidental supervision is a general paradigm that aims at making use of the abundant, naturally occurring data, as supervision signals.", "labels": [], "entities": []}, {"text": "As for the TempRel extraction task, the existence of many partially annotated datasets P is a good fit for this paradigm and the result here can be informative for future investigations involving other incidental supervision signals.", "labels": [], "entities": [{"text": "TempRel extraction task", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8855165640513102}]}, {"text": "Second, TempRel data collection.", "labels": [], "entities": [{"text": "TempRel data collection", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7088039716084799}]}, {"text": "The fact that P is shown to provide useful supervision signals poses some further questions: What is the optimal data collection scheme for TempRel extraction, fully annotated, partially annotated, or a mixture of both?", "labels": [], "entities": [{"text": "TempRel extraction", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.8234043121337891}]}, {"text": "For partially annotated data, what is the optimal ratio of annotated edges to unannotated edges?", "labels": [], "entities": []}, {"text": "The proposed method in this work can be readily extended to study these questions in the future, as we further discuss in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "TimeBank () is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7837095856666565}, {"text": "TempRel dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.736077070236206}]}, {"text": "Annotators in this setup usually focus only on salient relations but overlook some others.", "labels": [], "entities": []}, {"text": "It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9421468377113342}, {"text": "TempRel", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9070309996604919}]}, {"text": "Consequently, we categorize TimeBank as a partially annotated dataset (P).", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8932930827140808}]}, {"text": "The same argument applies to other datasets that adopted this setup, such as AQUAINT), and RED).", "labels": [], "entities": [{"text": "RED", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.5804620385169983}]}, {"text": "Most existing systems make use of P, including but not limited to,; this applies also to the TempEval workshops systems, e.g.,.", "labels": [], "entities": []}, {"text": "To address the missing annotation issue, Cassidy et al. proposed a dense annotation scheme, TB-Dense.", "labels": [], "entities": []}, {"text": "Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist).", "labels": [], "entities": []}, {"text": "As a result, edges in TB-Dense are considered as fully annotated in this paper.", "labels": [], "entities": [{"text": "TB-Dense", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8465887308120728}]}, {"text": "The first system on TBDense was proposed in . Two recent TempRel extraction systems) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.", "labels": [], "entities": [{"text": "TempRel extraction", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6523885279893875}]}, {"text": "However, there are no existing systems that jointly train on both.", "labels": [], "entities": []}, {"text": "Given that the annotation guidelines of F and P are obviously different, it may not be optimal to simply treat P and F uniformly and train on their union.", "labels": [], "entities": []}, {"text": "This situation necessitates further investigation as we do here.", "labels": [], "entities": []}, {"text": "Before introducing our joint learning approach, we have a few remarks about our choice of F and P datasets.", "labels": [], "entities": []}, {"text": "First, we note that TB-Dense is actually not fully annotated in the strict sense because only edges within a sliding, two-sentence window are presented.", "labels": [], "entities": []}, {"text": "That is, distant event pairs are intentionally ignored by the designers of TB-Dense.", "labels": [], "entities": [{"text": "TB-Dense", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9244022965431213}]}, {"text": "However, since such distant pairs are consistently ruled out in the training and inference phase in this paper, it does not change the nature of the problem being investigated here.", "labels": [], "entities": []}, {"text": "At this point, TB-Dense is the only fully annotated dataset that can be adopted in this study, despite the aforementioned limitation.", "labels": [], "entities": [{"text": "TB-Dense", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.7489688992500305}]}, {"text": "Second, the partial annotations in datasets like TimeBank were not selected uniformly at random from all possible edges.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9712585806846619}]}, {"text": "As described earlier, only salient and non-vague TempRels (which may often be those easy ones) are labeled in these datasets.", "labels": [], "entities": []}, {"text": "Using TimeBank as P might potentially create some bias and we will need to keep this in mind when analyzing the results in Sec.", "labels": [], "entities": []}, {"text": "4. Recent advances in TempRel data annotation can be used in the future to collect both F and P more easily.", "labels": [], "entities": [{"text": "F", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.8495625853538513}]}, {"text": "In this work, we consistently used TB-Dense as the fully annotated dataset (F) and TBAQ as the partially annotated dataset (P).", "labels": [], "entities": []}, {"text": "The corpus statistics of these two datasets are provided in.", "labels": [], "entities": []}, {"text": "Note that TBAQ is the union of TimeBank and AQUAINT and it originally contained 256 documents, but 36 out of them completely overlapped with TB-Dense, so we have excluded these when constructing P.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9448339343070984}]}, {"text": "In addition, the number of edges shown in only counts the event-event relations (i.e., do not consider the event-time relations therein), which is the focus of this work.", "labels": [], "entities": []}, {"text": "We also adopted the original split of TB-Dense (22 documents for training, 5 documents for development, and 9 documents for test).", "labels": [], "entities": [{"text": "TB-Dense", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.5073642134666443}]}, {"text": "Learning parameters were tuned to maximize their corresponding F-metric on the development set.", "labels": [], "entities": [{"text": "F-metric", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9950106143951416}]}, {"text": "Using the selected parameters, systems were retrained with development set incorporated and evaluated against the test split of TB-Dense (about 1.4K relations: 0.6K vague, 0.4K before, 0.3K after, and 0.1K for the rest).", "labels": [], "entities": [{"text": "TB-Dense", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.6652096509933472}]}, {"text": "Results are shown in, where all systems were compared in terms of their performances on \"same sentence\" edges (both nodes are from the same sentence), \"nearby sentence\" edges, all edges, and the temporal awareness metric used by the TempEval3 workshop.", "labels": [], "entities": []}, {"text": "The first part of (Systems 1-5) refers to the baseline method proposed at the beginning of Sec.", "labels": [], "entities": []}, {"text": "3, i.e., simply treating P as F and training on their union.", "labels": [], "entities": []}, {"text": "P F ull is a variant of P by filling its missing edges by vague.", "labels": [], "entities": [{"text": "P F ull", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6642370820045471}]}, {"text": "Since it labels too many vague TempRels, System 2 suffered from a low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.998451828956604}]}, {"text": "In contrast, P does not contain any vague training examples, so System 3 would only predict specific TempRels, leading to a low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9976958632469177}]}, {"text": "Given the obvious difference in F and P F ull , System 4 expectedly performed worse than System 1.", "labels": [], "entities": [{"text": "F", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9981834292411804}, {"text": "F ull", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8432011902332306}]}, {"text": "However, when we see that System 5 was still worse than System 1, it is surprising because the annotated edges in P are correct and should have helped.", "labels": [], "entities": []}, {"text": "This unexpected observation suggests that simply adding the annotated edges from P into F is not a proper approach to learn from both.", "labels": [], "entities": []}, {"text": "The second part (Systems 6-7) serves as an ablation study showing the effect of bootstrapping only.", "labels": [], "entities": []}, {"text": "P Empty is another variant of P we get by removing all the annotated edges (that is, only nodes are kept).", "labels": [], "entities": []}, {"text": "Thus, they did not get any information from the annotated edges in P and any improvement came from bootstrapping alone.", "labels": [], "entities": []}, {"text": "Specifically, System 6 is the standard bootstrapping and System 7 is the constrained bootstrapping.", "labels": [], "entities": []}, {"text": "Built on top of Systems 6-7, Systems 8-9 further took advantage of the annotations of P, which resulted in additional improvements.", "labels": [], "entities": []}, {"text": "Compared to System 1 (trained on F only) and System 5 (simply adding P into F), the proposed System 9 achieved much better performance, which is also statistically significant with p<0.005 (McNemar's test).", "labels": [], "entities": []}, {"text": "While System 7 can be regarded as a reproduction of, the original paper of achieved an overall score of P=43.0, R=46.4, F=44.7 and an awareness score of P=42.6, R=44.0, and F=43.3, and the proposed System 9 is also better than on all metrics.", "labels": [], "entities": [{"text": "R", "start_pos": 112, "end_pos": 113, "type": "METRIC", "confidence": 0.9780396819114685}, {"text": "F", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.9928898811340332}, {"text": "awareness score", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.9657669961452484}, {"text": "F", "start_pos": 173, "end_pos": 174, "type": "METRIC", "confidence": 0.9968871474266052}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics of the fully and partially anno-", "labels": [], "entities": []}, {"text": " Table 2: Performance of various usages of the partially annotated data in training. F : Fully annotated data. P: Partially", "labels": [], "entities": []}]}