{"title": [{"text": "Amobee at SemEval-2018 Task 1: GRU Neural Network with a CNN Attention Mechanism for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.9545251131057739}]}], "abstractContent": [{"text": "This paper describes the participation of Amobee in the shared sentiment analysis task at SemEval 2018.", "labels": [], "entities": [{"text": "shared sentiment analysis task at SemEval 2018", "start_pos": 56, "end_pos": 102, "type": "TASK", "confidence": 0.7959539294242859}]}, {"text": "We participated in all the English sub-tasks and the Spanish va-lence tasks.", "labels": [], "entities": []}, {"text": "Our system consists of three parts: training task-specific word embeddings, training a model consisting of gated-recurrent-units (GRU) with a convolution neural network (CNN) attention mechanism and training stacking-based ensembles for each of the sub-tasks.", "labels": [], "entities": []}, {"text": "Our algorithm reached 3rd and 1st places in the valence ordinal classification sub-tasks in English and Spanish, respectively.", "labels": [], "entities": [{"text": "valence ordinal classification", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.5502896706263224}]}], "introductionContent": [{"text": "Sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9281803369522095}]}, {"text": "The main motivation is enabling computers to better understand human language, particularly sentiment carried by the speaker.", "labels": [], "entities": []}, {"text": "Among the popular sources of textual data for NLP is Twitter, asocial network service where users communicate by posting short messages, no longer than 280 characters long-called tweets.", "labels": [], "entities": []}, {"text": "Tweets can carry sentimental information when talking about events, public figures, brands or products.", "labels": [], "entities": []}, {"text": "Unique linguistic features, such as the use of slang, emojis, misspelling and sarcasm, make Twitter a challenging source for NLP research, attracting the interest of both academia and the industry.", "labels": [], "entities": []}, {"text": "Semeval is a yearly event in which international teams of researchers work on tasks in a competition format where they tackle open research questions in the field of semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7930249571800232}]}, {"text": "We participated in Semeval 2018 task 1, which focuses on sentiment and emotions evaluation in tweets.", "labels": [], "entities": [{"text": "Semeval 2018 task 1", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.6946259886026382}, {"text": "sentiment and emotions evaluation in tweets", "start_pos": 57, "end_pos": 100, "type": "TASK", "confidence": 0.78624161084493}]}, {"text": "There were three main problems: identifying the * These authors contributed equally to this presence of a given emotion in a tweet (sub-tasks EI-reg, EI-oc), identifying the general sentiment (valence) in a tweet (sub-tasks V-reg, V-oc) and identifying which emotions are expressed in a tweet (sub-task E-c).", "labels": [], "entities": []}, {"text": "For a complete description of Semeval 2018 task 1, seethe official task description ( . We developed an architecture based on gatedrecurrent-units (GRU,).", "labels": [], "entities": [{"text": "Semeval 2018 task 1", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7014800310134888}]}, {"text": "We used a bi-directional GRU layer, together with a convolutional neural network (CNN) attentionmechanism, where its input is the hidden states of the GRU layer; lastly there were two fully connected layers.", "labels": [], "entities": []}, {"text": "We will refer to this architecture as the Amobee sentiment classifier (ASC).", "labels": [], "entities": [{"text": "Amobee sentiment classifier (ASC)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7136445492506027}]}, {"text": "We used ASC to train word embeddings to incorporate sentiment information and to classify sentiment using annotated tweets.", "labels": [], "entities": []}, {"text": "We participated in all the English sub-tasks and in the valence Spanish subtasks, achieving competitive results.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: section 2 describes our data sources, section 3 describes the data pre-processing pipeline.", "labels": [], "entities": []}, {"text": "A description of the main architecture is in section 4.", "labels": [], "entities": []}, {"text": "Section 5 describes the word embeddings generation; section 6 describes the extraction of features.", "labels": [], "entities": [{"text": "word embeddings generation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.598874052365621}]}, {"text": "In section 7 we describe the performance of our models; finally, in section 8 we review and summarize the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our general workflow for the tasks is as follows: for each sub-task, we started by cleaning the datasets, obtaining two cleaned versions.", "labels": [], "entities": []}, {"text": "We ran a pipeline that produced all the features we designed: the ASC predictions and the features described in section 6.", "labels": [], "entities": [{"text": "ASC", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9325997829437256}]}, {"text": "We removed sparse features (less than 8 samples).", "labels": [], "entities": []}, {"text": "Next, we defined a shallow neural network with a soft-voting ensemble.", "labels": [], "entities": []}, {"text": "We chose the best features and metaparameters-such as learning rate, batch size and number of epochs-based on the dev dataset.", "labels": [], "entities": []}, {"text": "Finally, we generated predictions for the regression tasks.", "labels": [], "entities": []}, {"text": "For the classification tasks, we used a grid search method on the regression predictions  to optimize the loss.", "labels": [], "entities": []}, {"text": "Most model trainings were conducted on a local machine equipped with a Nvidia GTX 1080 Ti GPU.", "labels": [], "entities": [{"text": "Nvidia GTX 1080 Ti GPU", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.9540273189544678}]}, {"text": "Our official results are summarized in table 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7123633623123169}]}, {"text": " Table 4: Relative contribution of features in the valence", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8679260015487671}]}, {"text": " Table 5: Summary of training parameters for the emotion", "labels": [], "entities": []}, {"text": " Table 6: Complete list of features generated from datasets.", "labels": [], "entities": []}]}