{"title": [{"text": "Epita at SemEval-2018 Task 1: Sentiment Analysis Using Transfer Learning Approach", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.963836669921875}]}], "abstractContent": [{"text": "In this paper we present our system for detecting valence task.", "labels": [], "entities": [{"text": "detecting valence task", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8337964216868082}]}, {"text": "The major issue was to apply a state-of-the-art system despite the small dataset provided : the system would quickly overfit.", "labels": [], "entities": []}, {"text": "The main idea of our proposal is to use transfer learning, which allows to avoid learning from scratch.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8885192573070526}]}, {"text": "Indeed, we start to train a first model to predict if a tweet is positive, negative or neutral.", "labels": [], "entities": []}, {"text": "For this we use an external dataset which is larger and similar to the target dataset.", "labels": [], "entities": []}, {"text": "Then, the pre-trained model is re-used as the starting point to train anew model that classifies a tweet into one of the seven various levels of sentiment intensity.", "labels": [], "entities": []}, {"text": "Our system, trained using transfer learning, achieves 0.776 and 0.763 respectively for Pearson correlation coefficient and weighted quadratic kappa metrics on the subtask evaluation dataset.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 87, "end_pos": 118, "type": "METRIC", "confidence": 0.9180327455202738}]}], "introductionContent": [{"text": "The goal of detecting valence task is to classify a given tweet into one of seven classes, corresponding to various levels of positive and negative sentiment intensity, that best represents the mental state of the tweeter.", "labels": [], "entities": []}, {"text": "This can be seen as a multiclass classification problem, in which each tweet must be classified in one of the following classes : very negative (-3), moderately negative (-2), slightly negative (-1), neutral/mixed (0), slightly positive (1), moderately positive (2) and very positive (3) ( . Several companies have been interested in customer opinion fora given product or service.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.7207520008087158}]}, {"text": "Sentiment analysis is one approach to automatically detect their emotions from comments posted in social networks.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9404333829879761}]}, {"text": "With the recent advances in deep learning, the ability to analyse sentiments has considerably improved.", "labels": [], "entities": [{"text": "analyse sentiments", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8893375992774963}]}, {"text": "Indeed, many experiments have used state-of-the-art systems to achieve high performance.", "labels": [], "entities": []}, {"text": "For example, () use Bidirectional Long Short-Term Memory (B-LSTM) with attention mechanisms while ( use Convolutional Neural Networks (CNN).", "labels": [], "entities": []}, {"text": "Both systems obtained the best performance at the the 2016 and 2017 SemEval 4-A task respectively.", "labels": [], "entities": [{"text": "SemEval 4-A task", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.8697566986083984}]}, {"text": "The amount of data is argued to be the main condition to train a reliable deep neural network.", "labels": [], "entities": []}, {"text": "However, the dataset provided to build our system is limited.", "labels": [], "entities": []}, {"text": "To address this issue, two solutions can be considered.", "labels": [], "entities": []}, {"text": "The first solution consists in extending our dataset by either manually labeling new data, which can be very time consuming, or by using over-sampling approaches.", "labels": [], "entities": []}, {"text": "The second solution consists in applying a transfer learning, which allows to avoid learning the model from scratch.", "labels": [], "entities": []}, {"text": "In this paper, we apply a transfer learning approach, from a model trained on a similar task : we propose to pre-train a model to predict if a tweet is positive, negative or neutral.", "labels": [], "entities": []}, {"text": "Precisely, we apply a B-LSTM on an external dataset.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.977179229259491}]}, {"text": "Then, the pre-trained model is re-used to classify a tweet according to the seven-point scale of positive and negative sentiment intensity.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents a brief definition of transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.9607351124286652}]}, {"text": "The description of our proposed system is presented in Section 3.", "labels": [], "entities": []}, {"text": "The experimental setup and results are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, a conclusion is given with a discussion of future works in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "As proposed in (, we used B-LSTM with the following parameters : size of LSTM layers is 150 (300 for B-LSTM), 2 layers of B-LSTM, with a dropout of 0.3 and 0.5 for embedding and LSTM layers respectively.", "labels": [], "entities": []}, {"text": "Other hyperparameters used are : Gaussian noise with \u03c3 of 0.3, and L 2 regularization of 0.0001.", "labels": [], "entities": [{"text": "L 2 regularization", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.757066547870636}]}, {"text": "We trained the B-LSTM over 18 epochs with a learning rate of 0.01 and batch size of 128 sequences.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9677172303199768}]}, {"text": "We trained our model with external data (more details in section 3.3) but for the evaluation we adapted the training and development sets provided for the target task.", "labels": [], "entities": []}, {"text": "The various levels of positive sentiments (i.e slightly, moderately and very positive) were regrouped in the same class.", "labels": [], "entities": []}, {"text": "The same goes for the various levels of negative sentiments.", "labels": [], "entities": []}, {"text": "Our model achieves 69.4% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995037317276001}]}, {"text": "We adapted the pre-trained model described above by removing the last fully-connected layer, and added a dense layer of 150 neurons followed by an output layer of 7 neurons.", "labels": [], "entities": []}, {"text": "As a reminder, the pre-trained layers are frozen.", "labels": [], "entities": []}, {"text": "We used the training and development sets to train our system, and evaluated by predicting the valence on the evaluation set.", "labels": [], "entities": []}, {"text": "We trained our model over 8 epochs with a learning rate of 0.01 and batch size of 50 sequences.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.9779350459575653}]}, {"text": "Our model achieves 0.776 and 0.763 respectively on P and W .  Finally, we conducted a set of experiments to validate our system and approach.", "labels": [], "entities": []}, {"text": "We evaluated more commonly used systems, with and without transfer learning.", "labels": [], "entities": []}, {"text": "These new systems are built by : -using similar number of layers, parameters and hyper-parameters.", "labels": [], "entities": []}, {"text": "-replacing B-LSTM layers by LSTM, CNN and dense layers.", "labels": [], "entities": []}, {"text": "-for the DNN, computing predictions using the mean of each word-vector of tweets, since it cannot use sequences as input.", "labels": [], "entities": [{"text": "DNN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8833560347557068}]}, {"text": "-for the CNN, using multiple convolutional filters of sizes 3, 4 and 5.", "labels": [], "entities": []}, {"text": "-for the combinations of systems, averaging the output probabilities.", "labels": [], "entities": []}, {"text": "The results are presented on.", "labels": [], "entities": []}, {"text": "We can observe that TL approach achieves better scores, and that B-LSTM is leading the score on both approaches as a single system.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9745256900787354}]}, {"text": "Moreover, combining systems enhances greatly the prediction without TL, but decreases the score with TL : the combination of independent systems compensates a small lack of data, but becomes useless with enough training.", "labels": [], "entities": [{"text": "prediction", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9074121713638306}, {"text": "TL", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9844610095024109}, {"text": "TL", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9971175193786621}]}], "tableCaptions": []}