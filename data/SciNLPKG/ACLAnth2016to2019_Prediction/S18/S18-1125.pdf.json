{"title": [{"text": "The UWNLP system at SemEval-2018 Task 7: Neural Relation Extraction Model with Selectively Incorporated Concept Embeddings", "labels": [], "entities": [{"text": "SemEval-2018 Task 7", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8569860458374023}, {"text": "Neural Relation Extraction Model", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.8630705624818802}]}], "abstractContent": [{"text": "This paper describes our submission for the SemEval 2018 Task 7 shared task on semantic relation extraction and classification in scientific papers.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7 shared task", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.9040314157803854}, {"text": "semantic relation extraction and classification in scientific papers", "start_pos": 79, "end_pos": 147, "type": "TASK", "confidence": 0.7723819315433502}]}, {"text": "We extend the end-to-end relation extraction model of (Miwa and Bansal, 2016) with enhancements such as a character-level encoding attention mechanism on selecting pretrained concept candidate embeddings.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7106201350688934}]}, {"text": "Our official submission ranked the second in relation classification task (Subtask 1.1 and Subtask 2 Senerio 2), and the first in the relation extraction task (Subtask 2 Scenario 1).", "labels": [], "entities": [{"text": "relation classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.908423513174057}, {"text": "relation extraction task", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.8851927717526754}]}, {"text": "1 Task Overview The SemEval 2018 Task 7 Shared Task (G\u00e1bor et al., 2018) focuses on the task of recognizing the semantic relation that holds between scientific concepts.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7 Shared Task", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8410592377185822}]}, {"text": "The task involves semantic relation extraction and classification into six categories specific to scientific literature: USAGE, RESULT, MODEL-FEATURE, PART WHOLE, TOPIC, COMPARE.", "labels": [], "entities": [{"text": "semantic relation extraction", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.7465648253758749}, {"text": "USAGE", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.9593556523323059}, {"text": "RESULT", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9708681702613831}, {"text": "MODEL-FEATURE", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9462558031082153}, {"text": "PART WHOLE", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.819146066904068}, {"text": "TOPIC", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.7214460968971252}]}, {"text": "Two types of tasks are proposed: 1) identifying pairs of entities that are instances of any of the six semantic relations (extraction task), and 2) classifying instances into one of the specific relation types (classification task).", "labels": [], "entities": []}, {"text": "Consider the following input sentence: \"[Unsu-pervised training] is first used to train a [phone n-gram model] fora particular domain.\"", "labels": [], "entities": []}, {"text": "Given the concept pair [Unsupervised training] and [phone n-gram model], the relation extraction task is to identify whether there is a relation between the concepts, while the the relation classification task is to identity the relation as USAGE.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8480996787548065}, {"text": "relation classification", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.741143524646759}, {"text": "USAGE", "start_pos": 241, "end_pos": 246, "type": "DATASET", "confidence": 0.9904384016990662}]}, {"text": "Relation direc-tionality is not taken into account for the evaluation of the extraction task.", "labels": [], "entities": []}, {"text": "Directionality is taken into account when relevant for the classification task (5 out of the 6 semantic relations are asymmetrical).", "labels": [], "entities": []}, {"text": "We will use this example throughout the paper to illustrate various parts of our system.", "labels": [], "entities": []}, {"text": "The SemEval 2018 Task 7 dataset contains 350 abstracts from the ACL Anthology for training and validation, and 150 abstracts for testing each sub-task.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7 dataset", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.7071303844451904}, {"text": "ACL Anthology", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9324698746204376}]}, {"text": "Since the scale of the data is small for supervised training of neural systems, we introduce several strategies to leverage a large quantity of un-labeled scientific articles.", "labels": [], "entities": []}, {"text": "In addition to initializing a neural system with pre-trained word embeddings, as in (Luan et al., 2017), we also try to incorporate embeddings of concepts that span multiple words.", "labels": [], "entities": []}, {"text": "In neural models such as (Miwa and Bansal, 2016), phrases are often represented by an average (or weighted average) of the token's sequential LSTM representation.", "labels": [], "entities": []}, {"text": "The intuition behind explicit mod-eling of multi-word concept embeddings is that the concept use maybe different from that of its individual words.", "labels": [], "entities": []}, {"text": "Due to the size of the dataset and the nature of scientific literature, a large number of the scientific terms in the test set have never appeared in the training set, so supervised learning of the phrase embeddings is not feasible.", "labels": [], "entities": []}, {"text": "Therefore, we pre-trained scientific term embeddings on a large scientific corpus and provide a strategy to selectively incorporate the pre-trained embeddings into the relation extraction system.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7724495232105255}]}, {"text": "2 System Description 2.1 Neural Architecture Model Our system is an extension of (Luan et al., 2017) and (Miwa and Bansal, 2016) with LSTM RNNs that represent both word sequences and dependency tree structures, and perform relation extraction between concepts on top of these RNNs.", "labels": [], "entities": [{"text": "relation extraction between concepts", "start_pos": 223, "end_pos": 259, "type": "TASK", "confidence": 0.812085896730423}]}, {"text": "As illustrated in Figure 1, it is composed of a 5 types of layers in a hierarchical neural model to encode context information.", "labels": [], "entities": []}, {"text": "The first two layers (token, token LSTM) use the neural modeling framework in (Luan et al., 2017).", "labels": [], "entities": []}, {"text": "The forward and backward dependency layers and the relation classification 788", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "External Data We use two external resources for pretraining word embeddings: i) the Semantic Scholar Corpus, 2 a collection of over 20 million research papers from which we extract a subset of 110k abstracts of publications in the artificial intelligence area; and ii) the ACL Anothology Reference Corpus, which contains 22k full papers published in the ACL Anothology ().", "labels": [], "entities": [{"text": "Semantic Scholar Corpus", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.5997117161750793}, {"text": "ACL Anothology Reference Corpus", "start_pos": 273, "end_pos": 304, "type": "DATASET", "confidence": 0.8925767987966537}, {"text": "ACL Anothology", "start_pos": 354, "end_pos": 368, "type": "DATASET", "confidence": 0.8835736513137817}]}, {"text": "Baseline We compare our model with a baseline that removes the Concept Selection Layer and replaces it with a weighted sum (using attention) of hidden states (from the Sequential LSTM Layer) for all words in a concept.", "labels": [], "entities": []}, {"text": "Implementation details All parameters are tuned based on dev set performance; the best parameters are selected and used for final evaluation.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9230263829231262}]}, {"text": "For all experiments, we explore tuning with two different evaluation metrics: macro-F1 score and micro-F1 score.", "labels": [], "entities": []}, {"text": "We keep the pre-trained concept embedding fixed as additional input feature.", "labels": [], "entities": []}, {"text": "The word embedding dimension is 250; the LSTM hidden dimension is 100 (for both sequential and dependency layer); the character-level hidden dimension is 25; and the optimization algorithm is SGD with a learning rate of 0.05.", "labels": [], "entities": []}, {"text": "For Subtask 2, since 5 out of 6 relation types have directionality, we add relation label \" REVERSE\" to all the 5 directional relations together with a \"NONE\" type, which result in 12 labels in total.", "labels": [], "entities": [{"text": "REVERSE", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9230661392211914}]}, {"text": "For each epoch, we also randomly filter out some \"NONE\" samples with probability p during training, since the \"NONE\" type relation dominates the training set and would bias the model towards predicting \"NONE\" types.", "labels": [], "entities": []}, {"text": "We tune p according to dev set, and use p = 0.4 for the final evaluation.", "labels": [], "entities": []}, {"text": "provides the results of an ablation study on the dev set showing the impact of removing different components of our system.", "labels": [], "entities": []}, {"text": "Looking at micro F1 scores, dependency path information is very important (performance dropped 11.5% without it), and the Concept Selection Layer is also important as it gives 2.5 absolute improvement.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.8318434357643127}]}, {"text": "The Dependency relation feature and the distance feature also show 1-2 points gain.", "labels": [], "entities": [{"text": "distance", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.998218834400177}]}, {"text": "It is worth noticing that removing the Concept Layer (-Concept) does better than replacing it with the weighted sequential LSTM sum (Baseline).", "labels": [], "entities": []}, {"text": "With the small amount of training data, it is difficult for the baseline system to learn a good transformation from word to phrase.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ablation study showing the impact of neu- ral network configurations on system performance on  the dev set for the relation classification task (Subtask  2, senerio 2). -DepFeat removes the input dependency  relation embeddings from the Backward & Forward De- pendency Layers. -DistFeat and -Concept omit the dis- tance and concept selection features, respectively, from  the final classification layer. -DepLSTM removes the  Backward & Forward Dependency Layers entirely (us- ing the LSTM embeddings in the weighted token aver- age).", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.8071945110956827}]}, {"text": " Table 2: Competition result for the top 3 teams. The of- ficial evaluation metric is macro F1 score. T1.1 means  Subtask 1.1, T2-E means Subtask 2 senerio 1 (extrac- tion task), T2-C means Subtask 2 senerio 2 (classifica- tion task).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9257350265979767}, {"text": "T1.1", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9503384232521057}]}]}