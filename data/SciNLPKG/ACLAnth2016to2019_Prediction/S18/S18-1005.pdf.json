{"title": [{"text": "SemEval-2018 Task 3: Irony Detection in English Tweets", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7329562306404114}]}], "abstractContent": [{"text": "This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.8589576482772827}]}, {"text": "The ironic tweets were collected using irony-related hashtags (i.e. #irony, #sar-casm, #not) and were subsequently manually annotated to minimise the amount of noise in the corpus.", "labels": [], "entities": []}, {"text": "Prior to distributing the data, hash-tags that were used to collect the tweets were removed from the corpus.", "labels": [], "entities": []}, {"text": "For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets.", "labels": [], "entities": []}, {"text": "Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F 1 = 0.71 and F 1 = 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection.", "labels": [], "entities": [{"text": "F 1 = 0.71", "start_pos": 212, "end_pos": 222, "type": "METRIC", "confidence": 0.9482356756925583}, {"text": "irony classification", "start_pos": 272, "end_pos": 292, "type": "TASK", "confidence": 0.7255739122629166}, {"text": "binary irony detection", "start_pos": 323, "end_pos": 345, "type": "TASK", "confidence": 0.6719827254613241}]}], "introductionContent": [{"text": "The development of the social web has stimulated the use of figurative and creative language, including irony, in public (.", "labels": [], "entities": []}, {"text": "From a philosophical/psychological perspective, discerning the mechanisms that underlie ironic speech improves our understanding of human reasoning and communication, and more and more, this interest in understanding irony also emerges in the machine learning community.", "labels": [], "entities": []}, {"text": "Although an unanimous definition of irony is still lacking in the literature, it is often identified as a trope whose actual meaning differs from what is literally enunciated.", "labels": [], "entities": []}, {"text": "Due to its nature, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce human language.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 56, "end_pos": 95, "type": "TASK", "confidence": 0.784304848739079}]}, {"text": "In fact, automatic irony detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis.", "labels": [], "entities": [{"text": "automatic irony detection", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.6956386963526408}, {"text": "text mining", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.7160145789384842}, {"text": "author profiling", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.708533301949501}, {"text": "detecting online harassment", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.8425129055976868}, {"text": "sentiment analysis", "start_pos": 256, "end_pos": 274, "type": "TASK", "confidence": 0.9657899737358093}]}, {"text": "Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.961677759885788}]}, {"text": "However, the SemEval-2014 shared task Sentiment Analysis in Twitter () demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets.", "labels": [], "entities": [{"text": "SemEval-2014 shared task Sentiment Analysis", "start_pos": 13, "end_pos": 56, "type": "TASK", "confidence": 0.6380715131759643}, {"text": "automatic sentiment classification", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.6676599383354187}]}, {"text": "The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.950016975402832}, {"text": "F 1", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9854274690151215}, {"text": "F 1 = 0.29", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.9612420350313187}, {"text": "F 1", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.955752044916153}]}, {"text": "In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g..", "labels": [], "entities": []}, {"text": "Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge.", "labels": [], "entities": []}, {"text": "Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative.", "labels": [], "entities": []}, {"text": "(1) I feel so blessed to get ocular migraines.", "labels": [], "entities": []}, {"text": "(2) Go ahead drop me hate, I'm looking forward to it.", "labels": [], "entities": []}, {"text": "For human readers, it is clear that the author of example 1 does not feel blessed at all, which can be inferred from the contrast between the positive sentiment expression \"I feel so blessed\", and the negative connotation associated with getting ocular migraines.", "labels": [], "entities": []}, {"text": "Although such connotative infor-mation is easily understood by most people, it is difficult to access by machines.", "labels": [], "entities": []}, {"text": "Example 2 illustrates implicit cyberbullying; instances that typically lack explicit profane words and where the offense is often made through irony.", "labels": [], "entities": []}, {"text": "Similarly to example 1, a contrast can be perceived between a positive statement (\"I'm looking forward to\") and a negative situation (i.e. experiencing hate).", "labels": [], "entities": []}, {"text": "To be able to interpret the above examples correctly, machines need, similarly to humans, to be aware that irony is used, and that the intended sentiment is opposite to what is literally enunciated.", "labels": [], "entities": []}, {"text": "The irony detection task 1 we propose is formulated as follows: given a single post (i.e. a tweet), participants are challenged to automatically determine whether irony is used and which type of irony is expressed.", "labels": [], "entities": [{"text": "irony detection task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9005131324132284}]}, {"text": "We thus defined two subtasks: \u2022 Task A describes a binary irony classification task to define, fora given tweet, whether irony is expressed.", "labels": [], "entities": []}, {"text": "\u2022 Task B describes a multiclass irony classification task to define whether it contains a specific type of irony (verbal irony by means of a polarity clash, situational irony, or another type of verbal irony, see further) or is not ironic.", "labels": [], "entities": [{"text": "multiclass irony classification task", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7545073479413986}]}, {"text": "Concretely, participants should define which one out of four categories a tweet contains: ironic by clash, situational irony, other verbal irony or not ironic.", "labels": [], "entities": []}, {"text": "It is important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, timestamp, location).", "labels": [], "entities": []}, {"text": "Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised.", "labels": [], "entities": [{"text": "recognise irony", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7781325578689575}]}], "datasetContent": [{"text": "For both subtasks, participating systems were evaluated using standard evaluation metrics, including accuracy, precision, recall and F 1 score, calculated as follows: While accuracy provides insights into the system performance for all classes, the latter three measures were calculated for the positive class only (Task A) or were macro-averaged over four class labels (Task B).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9990955591201782}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9973509311676025}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9983134269714355}, {"text": "F 1 score", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9921738505363464}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9972628355026245}]}, {"text": "Macro-averaging of the F 1 score implies that all class labels have equal weight in the final score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9513801336288452}]}, {"text": "For both subtasks, two baselines were provided against which to compare the systems' performance.", "labels": [], "entities": []}, {"text": "The first baseline randomly assigns irony labels and the second one is a linear SVM classifier with standard hyperparameter settings exploiting tf-idf word unigram features (implemented with scikit-learn ().", "labels": [], "entities": []}, {"text": "The second baseline system is made available to the task participants via GitHub 3 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement scores (Kappa) in  two annotation rounds.", "labels": [], "entities": [{"text": "Inter-annotator agreement scores (Kappa)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.8081528196732203}]}, {"text": " Table 2: Distribution of the different irony categories  in the corpus", "labels": [], "entities": []}, {"text": " Table 3: Official (CodaLab) results for Task A, ranked  by F 1 score. The highest scores in each column are  shown in bold and the baselines are indicated in purple.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9735590020815531}]}, {"text": " Table 4: Best constrained systems for Task A.", "labels": [], "entities": []}, {"text": " Table 5: Best unconstrained systems for Task A.", "labels": [], "entities": []}, {"text": " Table 6: Official (CodaLab) results for Task B, ranked  by F 1 score. The highest scores in each column are  shown in bold and the baselines are indicated in purple.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9761286377906799}]}, {"text": " Table 7: Best constrained systems for Task B. The  highest scores in each column are shown in bold.", "labels": [], "entities": []}, {"text": " Table 8: Unconstrained systems for Task B. The high- est scores in each column are shown in bold.", "labels": [], "entities": [{"text": "high- est scores", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.7162080556154251}]}, {"text": " Table 9: Results for Task B, reporting the F 1 score for  the class labels. The highest scores in each column are  shown in bold.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9810178279876709}]}]}