{"title": [{"text": "Robust Handling of Polysemy via Sparse Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Words are polysemous and multi-faceted, with many shades of meanings.", "labels": [], "entities": []}, {"text": "We suggest that sparse distributed representations are more suitable than other, commonly used, (dense) representations to express these multiple facets, and present Category Builder, a working system that, as we show, makes use of sparse representations to support multi-faceted lexical representations.", "labels": [], "entities": []}, {"text": "We argue that the set expansion task is well suited to study these meaning distinctions since a word may belong to multiple sets with a different reason for membership in each.", "labels": [], "entities": [{"text": "set expansion", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.7840587496757507}]}, {"text": "We therefore exhibit the performance of Category Builder on this task, while showing that our representation captures at the same time analogy problems such as \"the Ganga of Egypt\" or \"the Voldemort of Tolkien\".", "labels": [], "entities": []}, {"text": "Category Builder is shown to be a more expressive lexical representation and to outperform dense representations such as Word2Vec in some analogy classes despite being shown only two of the three input terms.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9404206871986389}]}], "introductionContent": [{"text": "Word embeddings have received much attention lately because of their ability to represent similar words as nearby points in a vector space, thus supporting better generalization when comparisons of lexical items are needed, and boosting the robustness achieved by some deep-learning systems.", "labels": [], "entities": []}, {"text": "However, a given surface form often has multiple meanings, complicating this simple picture.", "labels": [], "entities": []}, {"text": "showed that the vector corresponding to a polysemous term often is not close to any of that of its individual senses, thereby breaking the similar-items-map-to-nearby-points promise.", "labels": [], "entities": []}, {"text": "The polysemy wrinkle is not merely an irritation but, in the words of, \"one of the most intractable problems for language processing studies\".", "labels": [], "entities": [{"text": "language processing", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8129315674304962}]}, {"text": "Our notion of Polysemy here is quite broad, since words can be similar to one another along a variety of dimensions.", "labels": [], "entities": []}, {"text": "The following three pairs each has two similar items: (a) {ring, necklace}, (b) {ring, gang}, and (c) {ring, beep}.", "labels": [], "entities": []}, {"text": "Note that ring is similar to all words that appear as second words in these pairs, but for different reasons, defined by the second token in the pairs.", "labels": [], "entities": []}, {"text": "While this example used different senses of ring, it is easy to find examples where a single sense has multiple facets: Clint Eastwood, who is both an actor and a director, shares different aspects with directors than with actors, and Google, both a website and a major corporation, is similar to Wikipedia and General Electric along different dimensions.", "labels": [], "entities": []}, {"text": "Similarity has typically been studied pairwise: that is, by asking how similar item A is to item B.", "labels": [], "entities": []}, {"text": "A simple modification sharply brings to fore the issues of facets and polysemy.", "labels": [], "entities": []}, {"text": "This modification is best viewed through the task of set expansion (, which highlights the similarity of an item (a candidate in the expansion) to a set of seeds in the list.", "labels": [], "entities": []}, {"text": "Given a few seeds, what else belongs in the set?", "labels": [], "entities": []}, {"text": "Note how this expansion is quite different from the expansion of {Ford, Chevy}, and the difference is one of Similar How, since whether a word (say, BMW or FDR) belongs in the expansion depends not just on how much commonality it shares with Ford but on what commonality it shares.", "labels": [], "entities": []}, {"text": "Consequently, this task allows the same surface form to belong to multiple sets, by virtue of being similar to items in distinct sets for different reasons.", "labels": [], "entities": []}, {"text": "The facets along which items are similar is implicitly defined by the members in the set.", "labels": [], "entities": []}, {"text": "In this paper, we propose a context sensitive version of similarity based on highlighting shared facets.", "labels": [], "entities": []}, {"text": "We do this by developing a sparse representation of words that simultaneously captures all 265 facets of a given surface form.", "labels": [], "entities": []}, {"text": "This allows us to define a notion of contextual similarity, in which Ford is similar to Chevy (e.g., when Audi or BMW is in the context) but similar to Obama when Bush or Nixon is in the context (i.e., in the seed list).", "labels": [], "entities": []}, {"text": "In fact, it can even support multi-granular similarity since while {Chevy, Chrysler, Ford} represent the facet of AMERICAN CARS, {Chevy, Audi, Ford} define that of CARS.", "labels": [], "entities": [{"text": "AMERICAN CARS", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.7735639810562134}]}, {"text": "Our contextual similarity is better able to mold itself to this variety since it moves away from the one-size-fits-all nature of cosine similarity.", "labels": [], "entities": []}, {"text": "We exhibit the strength of the representation and the contextual similarity metric we develop by comparing its performance on both set expansion and analogy problems with dense representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the analogy examples used by.", "labels": [], "entities": []}, {"text": "Category Builder evaluation were done by expanding using syntactic and sentence-based-cooccurrence contexts as detailed in Section 4.6 and scoring items according to Equation 9.", "labels": [], "entities": []}, {"text": "For evaluating using Word2Vec, the standard vector arithmetic was used.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9701083302497864}]}, {"text": "In both cases, the input terms from the problem were removed from candidate answers (as was done in the original paper).", "labels": [], "entities": []}, {"text": "Linzen (2016) provides analysis and rationales for why this is done.", "labels": [], "entities": []}, {"text": "A few words are in order for the difference between the published scores for Word2Vec analogies elsewhere (e.g.,: Performance on Analogy classes from.", "labels": [], "entities": []}, {"text": "The first two columns are derived from the same corpus, whereas the last column reports numbers on the data we will release.", "labels": [], "entities": []}, {"text": "For category builder, we used \u03c1 = 3, n = 100 of country-based factual coverage as Wikipedia, and almost all non-grammar based analogy problems are of that nature.", "labels": [], "entities": [{"text": "category builder", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.803127259016037}]}, {"text": "A second matter to point out is why grammar based rows are missing from.", "labels": [], "entities": []}, {"text": "Grammar based analogy classes cannot be solved with just two terms.", "labels": [], "entities": []}, {"text": "For boy:boys::king:?, dropping the first term boy takes away information crucial to the solution in away that dropping the first term of US:dollar::India:?", "labels": [], "entities": []}, {"text": "The same is true for the family class of analogies.", "labels": [], "entities": []}, {"text": "provides a sampler of analogies solved using Category Builder.", "labels": [], "entities": []}, {"text": "We produced three evaluation sets, two closed and one open.", "labels": [], "entities": []}, {"text": "For closed sets, following, we use US States and National Football League teams.", "labels": [], "entities": [{"text": "US States and National Football League", "start_pos": 35, "end_pos": 73, "type": "DATASET", "confidence": 0.895409087340037}]}, {"text": "To increase the difficulty, for NFL teams, we do not use as seeds dismabiguated names such as Detroit Lions or Green Bay Packers, instead using the polysemous lions and packers.", "labels": [], "entities": []}, {"text": "The synsets were produced by adding all variant names for the teams.", "labels": [], "entities": []}, {"text": "For example, Atlanta Falcons are also known as falcs, and so this was added to the synset.", "labels": [], "entities": []}, {"text": "For the open set, we use verbs that indicate things breaking or failing in someway.", "labels": [], "entities": []}, {"text": "We chose ten popular instances (e.g., break, chip, shatter) and these act as seeds.", "labels": [], "entities": []}, {"text": "We expanded the set by manual evaluation: any correct item produced by any of the evaluated systems was added to the list.", "labels": [], "entities": []}, {"text": "There is an element of subjectivity here, and we therefore provide the lists used (Appendix A.1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MAP scores on three categories. The first four  rows use various techniques with Word2Vec. The next  four demonstrate Category Builder built on the same  corpus, to show the effect of \u03c1 and association measure  used. For all four Category Builder rows, we used n =  100. Both increasing \u03c1 and switching to APPMI can be  seen to be individually and jointly beneficial.  \u2020 The last  line reports the score on a different corpus, the release  data, with APPMI and \u03c1 = 3, n = 100.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9628753662109375}, {"text": "Word2Vec", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9732980728149414}, {"text": "association measure", "start_pos": 200, "end_pos": 219, "type": "METRIC", "confidence": 0.9433101713657379}, {"text": "APPMI", "start_pos": 461, "end_pos": 466, "type": "METRIC", "confidence": 0.6682575941085815}]}, {"text": " Table 2: Effect of varying n. APPMI with \u03c1 = 3.", "labels": [], "entities": [{"text": "APPMI", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9515132308006287}]}, {"text": " Table 5: Expansion examples with synthetic polysemy  by replacing all instances of cat and denver into the  hypothetical CatDenver (similarly, TigerAndroid). A  single other term is enough to pick out the right sense.", "labels": [], "entities": []}, {"text": " Table 6: Performance on Analogy classes from", "labels": [], "entities": []}]}