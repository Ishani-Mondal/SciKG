{"title": [{"text": "TRANSRW at SemEval-2018 Task 12: Transforming Semantic Representations for Argument Reasoning Comprehension", "labels": [], "entities": [{"text": "TRANSRW", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6250944137573242}, {"text": "SemEval-2018 Task 12", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8225721120834351}, {"text": "Argument Reasoning Comprehension", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.688678632179896}]}], "abstractContent": [{"text": "This paper describes our system in SemEval-2018 task 12: Argument Reasoning Comprehension.", "labels": [], "entities": [{"text": "SemEval-2018 task 12", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8716474572817484}, {"text": "Argument Reasoning Comprehension", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.7103213965892792}]}, {"text": "The task is to select the correct warrant that explains reasoning of a particular argument consisting of a claim and a reason.", "labels": [], "entities": []}, {"text": "The main idea of our methods is based on the assumption that the semantic composition of the reason and the warrant should be close to the semantic representation of the corresponding claim.", "labels": [], "entities": []}, {"text": "We propose two neural network models.", "labels": [], "entities": []}, {"text": "The first one considers two warrant candidates simultaneously, while the second one processes each candidate separately and then chooses the best one.", "labels": [], "entities": []}, {"text": "We also incorporate sentiment polarity by assuming that there are kinds of sentiment associations between the reason, the warrant and the claim.", "labels": [], "entities": []}, {"text": "The experiments show that the first framework is more effective and sentiment polarity is useful.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7377989590167999}]}], "introductionContent": [{"text": "Argument reasoning is a key step in the process of argumentation mining and is a very challenging task in natural language processing and artificial intelligence.", "labels": [], "entities": [{"text": "Argument reasoning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8947056829929352}, {"text": "argumentation mining", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.9077344238758087}]}, {"text": "suggested that the key factor in the study of natural language understanding is the mastery of natural language reasoning.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.650754859050115}, {"text": "natural language reasoning", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.646232416232427}]}, {"text": "When we argue for an argument, it is necessary to reconstruct the implicit reasoning) under the relevant assumption and premise to get a simple and concise explanation of the whole reasoning process.", "labels": [], "entities": []}, {"text": "The Argument Reasoning Comprehension task is defined as following: Given an argument consisting of a claim C and a reason R, the goal is to select the correct warrant that explains reasoning of this particular ar- * *corresponding author gument.", "labels": [], "entities": [{"text": "Argument Reasoning Comprehension task", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.8544793426990509}]}, {"text": "There are only two options W 0 and W 1 given and only one answer is correct.", "labels": [], "entities": []}, {"text": "Our solution is based on the assumption that the semantic composition of the reason and the true warrant should be close to the semantic representation of the claim.", "labels": [], "entities": []}, {"text": "First one is dependent on the task settings that two warrant candidates are considered simultaneously to make a decision.", "labels": [], "entities": []}, {"text": "The second one is more general that the task is simplified as determine whether a warrant candidate can explain argument reasoning.", "labels": [], "entities": [{"text": "explain argument reasoning", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7199488580226898}]}, {"text": "We found that the first one performed better.", "labels": [], "entities": []}, {"text": "In addition, we attempt to incorporate sentiment polarity to capture the sentiment association between the reason, the warrant and the claim.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that adding sentiment polarity can improve the performance.", "labels": [], "entities": []}, {"text": "The final result is produced by an ensemble approach that combines the outputs of multiple single models.", "labels": [], "entities": []}, {"text": "Our system achieves an accuracy of 0.67 on development dataset and 0.57 on test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996678829193115}]}], "datasetContent": [{"text": "We conducted experiments on the official datasets of SemEval-2018 Task 12.", "labels": [], "entities": [{"text": "official datasets of SemEval-2018 Task 12", "start_pos": 32, "end_pos": 73, "type": "DATASET", "confidence": 0.8883755306402842}]}, {"text": "The model parameters are trained using the training dataset and tuned based on the performance of development dataset.", "labels": [], "entities": []}, {"text": "We will report the results on both development dataset and test dataset.", "labels": [], "entities": []}, {"text": "Accuracy is the official evaluation metric.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9934677481651306}]}, {"text": "We also report precision, recall and F 1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9998080134391785}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.999742329120636}, {"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9882786671320597}]}, {"text": "We are interested in two research questions: \u2022 RQ1: Which proposed model is more effective?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.7582007646560669}]}, {"text": "\u2022 RQ2: Whether incorporating sentiment polarity can benefit this task?", "labels": [], "entities": []}, {"text": "shows the results on the development dataset.", "labels": [], "entities": []}, {"text": "The accuracy of the random baseline is 0.503.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9998111128807068}]}, {"text": "The proposed models significantly outperform the baseline.", "labels": [], "entities": []}, {"text": "By adding sentiment polarity representations, Model1 and Model2 both improve a lot.", "labels": [], "entities": []}, {"text": "The accuracy of Model1 increases 3.16%, while the accuracy of Model2 increases 2.43%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997842907905579}, {"text": "Model1", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9106419682502747}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9996627569198608}, {"text": "Model2", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9432645440101624}]}, {"text": "The precision, recall and F 1 score all have the same trend.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9998059868812561}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.999622106552124}, {"text": "F 1 score", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.988703707853953}]}, {"text": "With the sentiment polarity added, the Model1 performs better than Model2.", "labels": [], "entities": []}, {"text": "Without the sentiment polarity, their performance is very close.", "labels": [], "entities": []}, {"text": "shows the results on test dataset.", "labels": [], "entities": []}, {"text": "The random baseline submitted by task organizer is 0.527.", "labels": [], "entities": []}, {"text": "The accuracy of the ensemble model is 0.57, which outperforms the random baseline by 4.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999889612197876}]}, {"text": "After the task organizer released the gold test dataset, we predicted it again using the ensemble model and the accuracy is 0.5811.", "labels": [], "entities": [{"text": "gold test dataset", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.6606234808762869}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9995349645614624}]}, {"text": "Similar to the results on development dataset, with the sentiment polarity added, both Model1 and Model2 achieve a better performance.", "labels": [], "entities": []}, {"text": "We can see that on test dataset, Model1 outperforms Model2 no matter using or removing sentiment polarity representations.", "labels": [], "entities": []}, {"text": "When using sentiment polarity, the performance difference is larger.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7970841526985168}]}], "tableCaptions": []}