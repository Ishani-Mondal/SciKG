{"title": [{"text": "ITNLP-ARC at SemEval-2018 Task 12: Argument Reasoning Comprehension with Attention", "labels": [], "entities": [{"text": "ITNLP-ARC", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9084819555282593}, {"text": "SemEval-2018 Task 12", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.5217889547348022}, {"text": "Argument Reasoning Comprehension", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.7845054467519125}]}], "abstractContent": [{"text": "Reasoning is a very important topic and has many important applications in the field of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6584077080090841}]}, {"text": "Semantic Evaluation (SemEval) 2018 Task 12 \"The Argument Reasoning Comprehension\" committed to research natural language reasoning.", "labels": [], "entities": [{"text": "natural language reasoning", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6877094109853109}]}, {"text": "In this task, we proposed a novel argument reasoning comprehension system, ITNLP-ARC, which use Neural Networks technology to solve this problem.", "labels": [], "entities": [{"text": "ITNLP-ARC", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8578786253929138}]}, {"text": "In our system, the LSTM model is involved to encode both the premise sentences and the warrant sentences.", "labels": [], "entities": []}, {"text": "The attention model is used to merge the two premise sentence vectors.", "labels": [], "entities": []}, {"text": "Through comparing the similarity between the attention vector and each of the two warrant vectors, we choose the one with higher similarity as our system's final answer .", "labels": [], "entities": []}], "introductionContent": [{"text": "Reasoning is a very challenging, but basic part of Natural Language Inference (NLI), and many relevant tasks have been proposed such as Recognizing Textual Entailment (RTE) and soon.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7742240031560262}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 136, "end_pos": 172, "type": "TASK", "confidence": 0.7922897636890411}]}, {"text": "Stanford University provided Stanford Natural Language Inference (SNLI) corpus to support Natural Language Inference task.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 29, "end_pos": 78, "type": "DATASET", "confidence": 0.4900094233453274}]}, {"text": "It contained two kinds of sentences-the premise sentence and the warrant sentence.The mission is to judge whether the two sentences are inference or not.", "labels": [], "entities": []}, {"text": "Semantic Evaluation (SemEval) 2018 Task 12-The Argument Reasoning Comprehensiongive an argument consisting of the claim, the reason and two warrants.", "labels": [], "entities": [{"text": "Semantic Evaluation (SemEval) 2018 Task 12-The Argument Reasoning Comprehensiongive", "start_pos": 0, "end_pos": 83, "type": "TASK", "confidence": 0.6193577213720842}]}, {"text": "The goal is to select the correct warrant that explains reasoning with this particular argument.", "labels": [], "entities": []}, {"text": "There are two options given and only one is correct.", "labels": [], "entities": []}, {"text": "Compare with Stanford Natural Language Inference (SNLI) task, it has more challenges.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI)", "start_pos": 13, "end_pos": 55, "type": "TASK", "confidence": 0.608120722430093}]}, {"text": "Because it has abundant premise information such as the reason, the claim, text information, as well as the option warrants have high semantic textual similarity (.", "labels": [], "entities": []}, {"text": "In this task, we need to find an effective method to extract important information from these premise sentences.", "labels": [], "entities": []}, {"text": "Natural Language Reasoning can be applied to various fields such as question and answering, information retrieval and soon.", "labels": [], "entities": [{"text": "Natural Language Reasoning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6499624152978262}, {"text": "question and answering", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7778982718785604}, {"text": "information retrieval", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.8493829667568207}]}, {"text": "With the development of Neural Networks applied in Natural Language Processing, sentence representation and reasoning have been researched and taken significant step forwards.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7526922821998596}]}, {"text": "In order to deal with the sequence problem, recurrent neural networks (RNN) ( proposes the concept of hidden state, which can extract features from sequence-shaped data and then convert it to output.", "labels": [], "entities": []}, {"text": "It can be used to encode the sentence to fixed-length vector representations.", "labels": [], "entities": []}, {"text": "In most recent years, long short-term memory (LSTM) network (), and gated recurrent unit (GRU) ( are widely used to get sentence representative vector, and achieved better result compared with traditional methods.", "labels": [], "entities": []}, {"text": "Attention model also known as alignment model pays more attention to two sentences interaction (, which is usually applied in information extraction, relation extraction, text summarization and machine translation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7763232588768005}, {"text": "relation extraction", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.8353407680988312}, {"text": "text summarization", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7089875936508179}, {"text": "machine translation", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.7792034447193146}]}, {"text": "In machine translation, the attention model can be focused on one or a few words of input to make the translation more accurate when generating each new word.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.755634605884552}]}, {"text": "() extend a neural word-by-word attention mechanism to encourage reasoning over entailment of pairs of words and phrases.", "labels": [], "entities": [{"text": "reasoning over entailment of pairs of words and phrases", "start_pos": 65, "end_pos": 120, "type": "TASK", "confidence": 0.784440341922972}]}, {"text": "In our system, we use long short-term memory network to encode sentence.", "labels": [], "entities": []}, {"text": "To make full use of the information of the reason and the claim, we use attention model to get the attention sentence vector.", "labels": [], "entities": []}, {"text": "Then, we compare the warrant sentence vector and the attention sentence vector similarity.", "labels": [], "entities": [{"text": "attention sentence vector similarity", "start_pos": 53, "end_pos": 89, "type": "METRIC", "confidence": 0.5812249183654785}]}, {"text": "The warrant with higher similarity is taken as an answer.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9930076599121094}]}, {"text": "In order to make the system more accurate, we use ensemble result as our final answer.", "labels": [], "entities": []}], "datasetContent": [{"text": "We treat this task as a classification problem, and use log-loss as our loss function.", "labels": [], "entities": []}, {"text": "The format is: where y i is the label of i'th instance, and hi is the probability calculated by the system.", "labels": [], "entities": []}, {"text": "We also treat it as a sort problem, and choosing the top 1 of sorting results as the answer.", "labels": [], "entities": []}, {"text": "The loss function format is: where sim(r, wa) is the true similarity of the premise and the warrant, and sim(r, w) is the false similarity of the premise and the warrant.", "labels": [], "entities": []}, {"text": "Systems will be scored using accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994764924049377}]}, {"text": "The format is: accuracy = correct predictions all instances 3 Experiments and Results shows the parameter setting in our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995344877243042}]}, {"text": "Because we use Tensorflow to build our system, the sentence needs to beset to a fixed length.", "labels": [], "entities": []}, {"text": "The sentences with length greater than 30 words are lstm input unit lstm output unit lstm input dropout lstm output dropout epoch 300 200 0.6 0.6 40    truncated from the back, with length less than 30 words are added 0 in the behind.", "labels": [], "entities": []}, {"text": "In our system, we build the argument reasoning comprehension task with neural networks.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.8559806495904922}]}, {"text": "We try to use the LSTM's last output, max pooling or mean pooling to represent the sentence vector, and use two kinds of attention to merge the reason and the claim.", "labels": [], "entities": []}, {"text": "Because of neural networks contains a lot number of randomly initialized parameters, we run our system ten times and average the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9990634322166443}]}, {"text": "shows the accuracy with log-loss function.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996548891067505}]}, {"text": "shows the accuracy with sort loss function.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996298551559448}, {"text": "sort loss", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8348588645458221}]}, {"text": "From, we can get conclusion that mean pooling performed better than last output and max pooling.", "labels": [], "entities": []}, {"text": "shows the accuracy ensemble all neural network model, and this is our system's final result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992409944534302}]}], "tableCaptions": [{"text": " Table 1: parameter setting in ITNLP ARC system.", "labels": [], "entities": [{"text": "ITNLP ARC system", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8999937971433004}]}, {"text": " Table 3: The accuracy with sort loss function om Semeval 2018 data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9994257688522339}, {"text": "Semeval 2018 data sets", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.945190042257309}]}, {"text": " Table 4: The accuracy of ensembling all neural network  model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997089505195618}]}]}