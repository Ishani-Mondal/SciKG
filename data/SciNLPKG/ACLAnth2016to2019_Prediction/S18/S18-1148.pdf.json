{"title": [], "abstractContent": [{"text": "Hypernym discovery aims to discover the hy-pernym word sets given a hyponym word and proper corpus.", "labels": [], "entities": [{"text": "Hypernym discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.879193902015686}]}, {"text": "This paper proposes a simple but effective method for the discovery of hy-pernym sets based on word embedding, which can be used to measure the contextual similarities between words.", "labels": [], "entities": []}, {"text": "Given a test hyponym word, we get its hypernym lists by computing the similarities between the hyponym word and words in the training data, and fill the test word's hypernym lists with the hypernym list in the training set of the nearest similarity distance to the test word.", "labels": [], "entities": []}, {"text": "In SemEval 2018 task9, our results, achieve 1st on Spanish, 2nd on Italian, 6th on English in the metric of MAP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hypernymy relationship plays a critical role in language understanding because it enables generalization, which lies at the core of human cognition ().", "labels": [], "entities": [{"text": "language understanding", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7402051389217377}]}, {"text": "It has been widely used in various NLP applications), from word sense disambiguation () to information retrieval () , question answering () and textual entailment ().", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7190673251946768}, {"text": "information retrieval", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7583187520503998}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8548013865947723}, {"text": "textual entailment", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7011129409074783}]}, {"text": "To date, the hypernymy relation also plays an important role in Knowledge Base Construction task.", "labels": [], "entities": [{"text": "Knowledge Base Construction", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.662571926911672}]}, {"text": "In the past SemEval contest (SemEval-2015 task 17 1 , SemEval-2016 task 13 2 ), the \"Hypernym Detection\" task was treated as a classfication task, i.e., given a (hyponym, hypernym) pair, deciding whether the pair is a true hypernymic relation or not.", "labels": [], "entities": [{"text": "Hypernym Detection\" task", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8150025606155396}]}, {"text": "This has led to criticisms regarding its oversimplification ().", "labels": [], "entities": []}, {"text": "In the SemEval 2018 Task 9, the task has shifted to \"Hypernym Discovery\" , i.e., given the search space of a domain's vocabulary and an input hyponym, discover its best (set of) candidate hypernyms.", "labels": [], "entities": [{"text": "SemEval 2018 Task 9", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8463494181632996}, {"text": "Hypernym Discovery", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6872099786996841}]}, {"text": "In this paper, the content is organized as follows: Section 2 gives an introduction to the related work; Section 3 describes our methods for this task, including word embedding projection learning as the baseline and the nearest-neighbourbased method as the submission result; The experimental results are presented in Section 4.", "labels": [], "entities": [{"text": "word embedding projection learning", "start_pos": 162, "end_pos": 196, "type": "TASK", "confidence": 0.7117742747068405}]}, {"text": "We conclude the paper with Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Word2vec is used to produce the word embeddings.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9690260887145996}]}, {"text": "The skip-gram model (-cbow 0) is used with the embedding dimension set to 300 (-size 300).", "labels": [], "entities": []}, {"text": "The other options are by default.", "labels": [], "entities": []}, {"text": "We use 10-fold cross validation to evaluate both methods on the provided training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross validation results of the two methods on training set(%). PL stands for the projection-learning  based system. NN stands for the nearest-neighbor based method.", "labels": [], "entities": []}, {"text": " Table 2: Results on the test data for our submissions(%).", "labels": [], "entities": []}]}