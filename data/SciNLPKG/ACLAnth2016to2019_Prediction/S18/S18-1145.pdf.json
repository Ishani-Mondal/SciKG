{"title": [], "abstractContent": [{"text": "The four sub-tasks of SecureNLP build towards a capability for quickly highlighting critical information from malware reports, such as the specific actions taken by a mal-ware sample.", "labels": [], "entities": []}, {"text": "Digital Operatives (DO) submitted to sub-tasks 1 and 2, using standard text analysis technology (text classification for sub-task 1, and a CRF for sub-task 2).", "labels": [], "entities": [{"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.6798896640539169}]}, {"text": "Performance is broadly competitive with other submitted systems on sub-task 1 and weak on sub-task 2.", "labels": [], "entities": []}, {"text": "The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task.", "labels": [], "entities": []}, {"text": "The methods DO chose do not attempt to make use of this linkage, which maybe a missed opportunity.", "labels": [], "entities": [{"text": "DO", "start_pos": 12, "end_pos": 14, "type": "DATASET", "confidence": 0.8618513941764832}]}, {"text": "This motivates a post-hoc error analysis.", "labels": [], "entities": []}, {"text": "It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SecureNLP challenge is motivated by) and further described in (, it aims to provide automation for malware analysts who might otherwise be overwhelmed by the task of finding key data in long threat reports.", "labels": [], "entities": []}, {"text": "The annotation guidelines used to create the task ask analysts to include actions carried out by the malware, but exclude actions carried out by the human designers of the malware.", "labels": [], "entities": []}, {"text": "These actions are related to the MAEC cybersecurity ontology.", "labels": [], "entities": [{"text": "MAEC cybersecurity ontology", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6068513790766398}]}, {"text": "The guidelines include one substantial caveat: As a general guide [for a positive annotation], the sentence should imply a particular malware action or capability, with reference to the list of attribute labels.", "labels": [], "entities": []}, {"text": "[i.e. the MAEC labels] Sub-task 1 calls fora determination of relevance or irrelevance to malware activity on a per-sentence basis.", "labels": [], "entities": [{"text": "MAEC labels", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7755737602710724}]}, {"text": "However a number of issues make this difficult.", "labels": [], "entities": []}, {"text": "First, it is not obvious what to do when a sentence describes malware activity but does not fit in with any MAEC category.", "labels": [], "entities": []}, {"text": "Second, the distinction between things done by the malware and things done (or intended) by its designers is not easy to maintain.", "labels": [], "entities": []}, {"text": "We describe the systems that we built for tasks 1 and 2 and use them to conduct ablation studies and error analyses.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1-scores using only lemmas and dependen- cies between lemmas.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9987176656723022}]}, {"text": " Table 2: Malware capability labels. Note that the frequency distribution is highly skewed. Examples are edited to  fit.", "labels": [], "entities": []}, {"text": " Table 4: Performance against DO's (possibly uninten- tionally biased) annotations.", "labels": [], "entities": []}]}