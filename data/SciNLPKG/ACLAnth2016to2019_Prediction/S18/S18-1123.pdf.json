{"title": [{"text": "LIGHTREL at SemEval-2018 Task 7: Lightweight and Fast Relation Classification", "labels": [], "entities": [{"text": "LIGHTREL", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.959053099155426}, {"text": "SemEval-2018 Task", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7912766337394714}]}], "abstractContent": [{"text": "We present LIGHTREL, a lightweight and fast relation classifier.", "labels": [], "entities": [{"text": "LIGHTREL", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9680216908454895}]}, {"text": "Our goal is to develop a high baseline for different relation extraction tasks.", "labels": [], "entities": [{"text": "relation extraction tasks", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8235606749852499}]}, {"text": "By defining only very few data-internal, word-level features and external knowledge sources in the form of word clusters and word embeddings, we train a fast and simple linear classifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main motivation for our participation at) was the ideal opportunity to test and improve our relation extraction system LIGHTREL.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7774686515331268}, {"text": "LIGHTREL", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.8581094145774841}]}, {"text": "The system design and development was inspired by the work described in.", "labels": [], "entities": []}, {"text": "Their goal was to depart from traditional relation extraction approaches with complicated feature engineering by exploring a deep neural network that would minimize its dependence on external toolkits and resources, e.g. external word embeddings.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8799441158771515}]}, {"text": "That allowed them to design a rather lightweight relation extraction approach that would basically only require supervised training data, external word embeddings and a few hyperparameters.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8490341007709503}]}, {"text": "Their \"end-to-end\" relation extraction approach produced competitive results.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.810768336057663}]}, {"text": "Since tuning hyperparameters for neural networks can be a intricate process, we considered whether it would be possible to define an even simpler system and use it as a baseline for our future research.", "labels": [], "entities": []}, {"text": "Thus, we adopted some of the design decisions made by and combined them with a well-known, fast linear classifier, viz..", "labels": [], "entities": []}, {"text": "Following, we represent a relation mention as a sequence of tokens.", "labels": [], "entities": []}, {"text": "The core idea of our approach consists of transforming this sequence into a a vector of fixed length, such that each token (or word) is represented only by: 1) the word itself, 2) its shape (a small, fixed amount of character-based features), 3) the word's cluster id, and 4) the word's embedding of fixed size.", "labels": [], "entities": []}, {"text": "For this competition, we introduce anew relation-level feature, namely the ID of the word directly following and preceding entities one and two, respectively.", "labels": [], "entities": []}, {"text": "Furthermore, we ignore all tokens to the left of the first entity and to the right of second entity.", "labels": [], "entities": []}, {"text": "The size of the whole vector therefore hinges on the maximum number of elements between the two entities found in the training set.", "labels": [], "entities": []}, {"text": "These representations are then used to train a LibLinear model.", "labels": [], "entities": []}, {"text": "Note that this reduces manual feature engineering to defining the shape features, finding an appropriate number of clusters and word embedding dimensions, and hyperparameters for LibLinear.", "labels": [], "entities": []}, {"text": "All other information is automatically computed from the training data.", "labels": [], "entities": []}, {"text": "In this sense, we consider our system lightweight.", "labels": [], "entities": []}, {"text": "We initially developed and tested our approach on the previous and widely-used SemEval-2010 Task 8 data set, and obtained as our best result an F1 measure of 79.78% on the test-data using the standard evaluation script from SemEval-2010 (see also.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 data set", "start_pos": 79, "end_pos": 107, "type": "DATASET", "confidence": 0.8560279965400696}, {"text": "F1 measure", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9818841814994812}]}, {"text": "Although this result is behind the best reported ones (the majority between 83%-85%, and the best 88.0%, cf. (), we think it provides a strong baseline compared to the manuallyengineered, feature-heavy approaches or complex neural architectures.", "labels": [], "entities": []}, {"text": "Thus, when the SemEval-2018 Task 7 challenge was announced, it was a natural decision to use it as an additional testing ground for LIGHTREL.", "labels": [], "entities": [{"text": "SemEval-2018 Task 7 challenge", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7680803835391998}]}], "datasetContent": [{"text": "As mentioned before, our system was an adaptation to the one that produced the optimal result on SemEval-2010 Task 8.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.6179712017377218}]}, {"text": "Through cross-validation, we tuned our system parameters to produce the best results on SemEval-2018 training data, bearing in mind the goal of keeping our system as lightweight as possible.", "labels": [], "entities": [{"text": "SemEval-2018 training data", "start_pos": 88, "end_pos": 114, "type": "DATASET", "confidence": 0.769711991151174}]}, {"text": "It is important to note that hyperparameter tuning was only done according to system performance on task 1.1's training data; we simply used the optimal parameters from this task while participating in task 1.2 for the competition.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6449213474988937}]}, {"text": "Through experimentation, we found a LibLinear classifier that performed better on the current task than the one that we used for the SemEval-2010 task.", "labels": [], "entities": [{"text": "SemEval-2010 task", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7544315457344055}]}, {"text": "We also developed a novel wordcontext feature for the 2018 task, which modestly improved our cross-validation results (anywhere from a 1% to 3% increase in F1 score, depending on the different parameters used).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9901295006275177}]}, {"text": "We will display the parameters that produced optimal results in cross-validation, as well as the results obtained using these same parameters in the competition phase in tabular form below.", "labels": [], "entities": []}, {"text": "For the SemEval-2010 task, we obtained the best results using LibLinear's support vector classifier by Crammer and Singer) with a cost of 0.1 and a stopping tolerance of 0.3.", "labels": [], "entities": [{"text": "SemEval-2010 task", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8722088038921356}, {"text": "stopping tolerance", "start_pos": 148, "end_pos": 166, "type": "METRIC", "confidence": 0.9745541512966156}]}, {"text": "Given the fewer relation types and smaller training vectors in the SemEval-2018 task, we experimented with different classifiers.", "labels": [], "entities": []}, {"text": "In development, we achieved the best performance in using LibLinear's default classifier, which performs dual L2-regularized L2-loss support vector classification, cf. ().", "labels": [], "entities": [{"text": "L2-regularized L2-loss support vector classification", "start_pos": 110, "end_pos": 162, "type": "TASK", "confidence": 0.6253482222557067}]}, {"text": "We set cost and stopping tolerance parameters equal to 0.1, using default settings for all other parameters.", "labels": [], "entities": [{"text": "stopping tolerance", "start_pos": 16, "end_pos": 34, "type": "METRIC", "confidence": 0.9526892304420471}]}, {"text": "The total competition data was made up of 1228 + 355 = 1583 relation instances (#training + #test), corresponding to an approximate train-test split of \u224878%-22%.", "labels": [], "entities": []}, {"text": "Because of this, we developed our system using 5-fold cross-validation on the training set, which entails an even 80%-20% split of data, i.e. 982 + 246 = 1228 instances.", "labels": [], "entities": []}, {"text": "Even though a subset of the training data was provided by the organizers for system development, we opted to split the data in accordance with the proportion of training and test instances, in order to get the best estimate of system performance in the competition phase.", "labels": [], "entities": []}, {"text": "We obtained the best average F1 score for tasks 1.1 and 1.2 when using all features but the shape feature (word, embeddings, clusters, entity one context, entity two context); the second best score was obtained when using all features and the third best by removing the entity two context feature from the entire feature set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9826923906803131}]}, {"text": "These feature sets represent a modest approach to the task at hand compared to more complex systems in-corporating knowledge from sources like part-ofspeech tagging or dependency parsing.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.7274305522441864}, {"text": "dependency parsing", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8022231161594391}]}, {"text": "An exception to these feature sets was cross-validation on task 1.2's training data, where an average F1 score of 61.83% was obtained when using neither context-related feature.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9850503504276276}]}, {"text": "However, since removing a context-related feature (no e2 context) already produced the best results in development, we decided to hedge our bets in the competition with a feature set composed of all features but the shape feature, i.e. of no manually engineered features.", "labels": [], "entities": []}, {"text": "The results from cross-validation on task 1.1 and task 1.2 are shown in below.", "labels": [], "entities": []}, {"text": "Based on these results, we expected our system to perform similarly in the competition.", "labels": [], "entities": []}, {"text": "feature set task The actual results of the competition can be seen below in  The best result was obtained when using all features except the shape feature.", "labels": [], "entities": []}, {"text": "This points to evidence that there is overlap in the information gained from the shape feature and the word feature.", "labels": [], "entities": []}, {"text": "The same token, differing only in punctuation (e.g. the strings 'IR' and 'IR,'), is represented with both different word and different shape IDs in our system.", "labels": [], "entities": []}, {"text": "However, for the shape feature to provide extra information to the model, the word feature would have to remain the same, since the shape feature changed.", "labels": [], "entities": []}, {"text": "The results on the first task were worse than the cross-validation results suggested.", "labels": [], "entities": []}, {"text": "Since we incorporated the words from the test data into our word and context features, there was no information that the model could have missed in the competition phase.", "labels": [], "entities": []}, {"text": "Therefore, we attribute the slight decrease in performance to the fact that more training and less test data were used in development, meaning that our models were overfitting in training.", "labels": [], "entities": []}, {"text": "Surprisingly, our system performed better on noisily annotated data, given no extra development in relation to the task.", "labels": [], "entities": []}, {"text": "It is difficult to say with conviction why these results occurred, as our features do not incorporate the entity markup.", "labels": [], "entities": []}, {"text": "Another difference in this task is the data itself; it could be that more tokens were found in the embedding and cluster features, providing more information to the model.", "labels": [], "entities": []}, {"text": "However, this fact alone hardly explains an almost 30% increase in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9962899684906006}]}, {"text": "Finally, we assessed our system's speed.", "labels": [], "entities": []}, {"text": "The final system which produced the best results for subtask 1.1 needed a total of 35 seconds to run on a 2012 MacBook Pro with 16GB of RAM and a 2.6 GHz quad-core Intel Core i7 processor 3 . The bottleneck occurred in the creating of vectors (80% of total time), which can be attributed to the simple way we stored and accessed the embeddings.", "labels": [], "entities": []}, {"text": "Training lasted 5 seconds, while testing/prediction only required a fraction of a second.", "labels": [], "entities": [{"text": "prediction", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.7606475949287415}]}, {"text": "These results demonstrate our system's agility in relation to complex neural architectures, which typically need hours, or even days, to train.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average F1 scores from 5-fold cross- validation.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9732225835323334}]}, {"text": " Table 2: Competition F1 scores.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9274106323719025}]}]}