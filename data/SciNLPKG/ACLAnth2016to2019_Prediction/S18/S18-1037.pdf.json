{"title": [{"text": "NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning", "labels": [], "entities": [{"text": "NTUA-SLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9115487933158875}, {"text": "Predicting Affective Content in Tweets", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.8769172549247741}]}], "abstractContent": [{"text": "In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: \"Affect in Tweets\".", "labels": [], "entities": [{"text": "SemEval-2018 Task 1 competition", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7080130279064178}]}, {"text": "We participated in all subtasks for English tweets.", "labels": [], "entities": []}, {"text": "We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism.", "labels": [], "entities": []}, {"text": "The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable.", "labels": [], "entities": []}, {"text": "Our model utilizes a set of word2vec word em-beddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features.", "labels": [], "entities": []}, {"text": "Due to the limited amount of task-specific training data, we opted fora transfer learning approach by pretrain-ing the Bi-LSTMs on the dataset of Semeval 2017, Task 4A.", "labels": [], "entities": [{"text": "Bi-LSTMs on the dataset of Semeval 2017, Task 4A", "start_pos": 119, "end_pos": 167, "type": "DATASET", "confidence": 0.8445651650428772}]}], "introductionContent": [{"text": "Social media content has dominated online communication, enriching and changing language with new syntactic and semantic constructs that allow users to express facts, opinions and emotions in short amount of text.", "labels": [], "entities": []}, {"text": "The analysis of such content has received great attention in NLP research due to the wide availability of data and the interesting language novelties.", "labels": [], "entities": []}, {"text": "Specifically the study of affective content in Twitter has resulted in a variety of novel applications, such as tracking product perception), public opinion detection about political tendencies (Pla and, stock market monitoring () etc.", "labels": [], "entities": [{"text": "public opinion detection about political tendencies", "start_pos": 142, "end_pos": 193, "type": "TASK", "confidence": 0.805101086695989}, {"text": "Pla", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.7817679047584534}]}, {"text": "The wide usage of figurative language, such as emojis and special language forms like abbreviations, hashtags, slang and other social media markers, which do not align with the conventional language structure, make natural language processing in Twitter even more challenging.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 215, "end_pos": 242, "type": "TASK", "confidence": 0.716327170530955}]}, {"text": "In the past, sentiment analysis was tackled by extracting hand-crafted features or features from sentiment lexicons) that were fed to classifiers such as Naive Bayes or Support Vector Machines (SVM) ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9644237756729126}]}, {"text": "The downside of such approaches is that they require extensive feature engineering from experts and thus they cannot keep up with rapid language evolution (, especially in social media/micro-blogging context.", "labels": [], "entities": []}, {"text": "However,: High-level overview of our approach recent advances in artificial neural networks for text classification have shown to outperform conventional approaches ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8445443511009216}]}, {"text": "This can be attributed to their ability to learn features directly from data and also utilize hand-crafted features where needed.", "labels": [], "entities": []}, {"text": "Most of aforementioned works focus on sentiment analysis, but similar approaches have been applied to emotion detection) leading to similar conclusions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9626320004463196}, {"text": "emotion detection", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.7255642116069794}]}, {"text": "SemEval 2018 Task 1: \"Affect in Tweets\" () focuses on exploring emotional content of tweets for both classification and regression tasks concerning the four basic emotions (joy, sadness, anger, fear) and the presence of more fine-grained emotions such as disgust or optimism.", "labels": [], "entities": []}, {"text": "In this paper, we present a deep-learning system that competed in SemEval 2018 Task 1: \"Affect in Tweets\".", "labels": [], "entities": [{"text": "SemEval 2018 Task 1", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7670684158802032}]}, {"text": "We explore a transfer learning approach to compensate for limited training data that uses the sentiment analysis dataset of Semeval Task 4A () for pretraining a model and then further fine-tune it on data for each subtask.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.898705780506134}]}, {"text": "Our model operates at the word-level and uses a Bidirectional LSTM equipped with a deep self-attention mechanism (.", "labels": [], "entities": []}, {"text": "Moreover, to help interpret the inner workings of our model, we provide visualizations of tweets with annotations of the salient tokens as predicted by the attention layer.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training We use Adam algorithm) for optimizing our networks, with minibatches of size 32 and we clip the norm of the gradients ( at 1, as an extra safety measure against exploding gradients.", "labels": [], "entities": []}, {"text": "For developing our models we used PyTorch ( and Scikit-learn ().", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.6966186761856079}]}, {"text": "In subtasks EI-oc and V-oc, some classes have more training examples than others, introducing bias in our models.", "labels": [], "entities": []}, {"text": "To deal with this problem, we apply class weights to the loss function, penalizing more the misclassification of under-represented classes.", "labels": [], "entities": []}, {"text": "These weights are computed as the inverse frequencies of the classes in the training set.", "labels": [], "entities": []}, {"text": "In order to tune the hyperparameter of our model, we adopt a Bayesian optimization () approach, performing a more time-efficient search in the high dimensional space of all the possible values, compared to grid or random search.", "labels": [], "entities": []}, {"text": "We set size of the embedding layer to 310 (300 word2vec + 10 affective dimensions), which we regularize by adding Gaussian noise with \u03c3 = 0.2 and dropout of 0.1.", "labels": [], "entities": []}, {"text": "The sentence encoder is composed of 2 BiLSTM layers, each of size 250 (per direction) with a 2-layer self-attention mechanism.", "labels": [], "entities": []}, {"text": "Finally, we apply dropout of 0.3 to the encoded representation.", "labels": [], "entities": [{"text": "dropout", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9791052937507629}]}, {"text": "In, we compare the proposed transfer learning models against 3 strong baselines.", "labels": [], "entities": []}, {"text": "Pearson correlation is the metric used for the first four subtasks, whereas Jaccard index is used for the E-c multi-label classification subtask.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8492240011692047}]}, {"text": "The first baseline is a unigram Bag-of-Words (BOW) model with TF-IDF weighting.", "labels": [], "entities": []}, {"text": "The second baseline is a Neural Bag-of-Words (N-BOW) model, where we retrieve the word2vec embeddings of the words in a tweet and compute the tweet representation as the average (centroid) of the constituent word2vec embeddings.", "labels": [], "entities": []}, {"text": "Finally, the third baseline is similar to the second one, but with the addition of 10-dimensional affective embeddings that model affect-related dimensions (valence, dominance, arousal, etc).", "labels": [], "entities": []}, {"text": "Both BOW and N-BOW features are then fed to a linear SVM classifier, with tuned C = 0.6.", "labels": [], "entities": [{"text": "BOW", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9775027632713318}]}, {"text": "In order to assess the impact of transfer learning, we evaluate the performance of each model in 3 different settings: (1) random weight initialization (LST-M-RD), (2) transfer learning with frozen weights (LSTM-TL-FR), (3) transfer learning with finetuning (LSTM-TL-FT).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9245938062667847}]}, {"text": "The results of our neural models in are computed by averaging the results of 10 runs to account for model variability.", "labels": [], "entities": []}, {"text": "Our first observation is that N-BOW baselines significantly outperform BOW in subtasks EI-reg, EI-oc, V-reg and V-oc, in which we have to predict the intensity of an emotion, or the tweet's valence.", "labels": [], "entities": [{"text": "BOW", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8806564807891846}]}, {"text": "However, BOW achieves slightly better performance in subtask E-c, in which we have to recognize the emotions expressed in each tweet.", "labels": [], "entities": [{"text": "BOW", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8603067994117737}]}, {"text": "This can be attributed to the fact that BOW models perform well in tasks where we the occurrence of certain words is sufficient, to accurately determine the classification result.", "labels": [], "entities": []}, {"text": "This suggests that in subtask E-c, certain words are highly indicative of some emotions.", "labels": [], "entities": []}, {"text": "Word embeddings, though, that encode the correlation of each word with different dimensions, enable NBOW to better predict the intensity of various emotions.", "labels": [], "entities": []}, {"text": "Further-: Results of our experiments across all subtasks on the official evaluation metrics.", "labels": [], "entities": []}, {"text": "For subtasks EI-reg, EI-oc, V-reg, V-oc, the evaluation metric is Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.9324112236499786}]}, {"text": "For subtask E-c, the evaluation metric is multi-label accuracy (Jaccard index).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.7275837659835815}, {"text": "Jaccard index)", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.7997429569562277}]}, {"text": "BOW stands for Bag-of-Words baseline, N-BOW stands for Neural Bag-of-Words baseline and N-BOW+A indicates the inclusion of the affective word features.", "labels": [], "entities": [{"text": "BOW", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9831253886222839}]}, {"text": "As for the neural models, RD stands for random initialization, TL for Transfer Learning, FR for Frozen pretrained layers (without fine-tuning) and FT for Fine-Tuning.", "labels": [], "entities": [{"text": "TL", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9221158623695374}, {"text": "Transfer Learning", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.920983761548996}, {"text": "FR", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9975188970565796}, {"text": "FT", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9967882633209229}]}, {"text": "For our deep-learning models, the results are computed by averaging 10 runs to account for the variability in training performance.", "labels": [], "entities": []}, {"text": "We observe that our neural models achieved better performance than all baselines by a large margin.", "labels": [], "entities": []}, {"text": "Moreover, we can see that our transfer learning model yielded higher performance over the non-transfer model inmost of the Emotion Intensity (EI) subtasks.", "labels": [], "entities": []}, {"text": "In the Emotion multi-label classification subtask (E-c), transfer learning did not outperform the random initialization model.", "labels": [], "entities": [{"text": "Emotion multi-label classification", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.7653955817222595}, {"text": "transfer learning", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.9455326795578003}]}, {"text": "This can be attributed to the fact that our source dataset (SA17) was not diverse enough to boost the model performance when classifying the tweets into none, one or more of a set of 11 emotions.", "labels": [], "entities": []}, {"text": "As for fine-tuning or freezing the pretrained layers, the overall results show that enabling the model to fine-tune always results in significant gains.", "labels": [], "entities": []}, {"text": "This is consistent with our intuition that allowing the weights of the model to adapt to the target dataset, thus encoding task-specific information, results in performance gains.", "labels": [], "entities": []}, {"text": "Regarding the emotion of joy, we observe that in EI-reg and EI-oc subtasks, LSTM-RD matches the performance of LSTM-TL-FR.", "labels": [], "entities": []}, {"text": "We interpret this result as an indication of the semantic similarity between the source and the target task.", "labels": [], "entities": []}, {"text": "The submitted models were also evaluated against a mystery dataset, in order to investigate if there is statistically significant social bias in them.", "labels": [], "entities": []}, {"text": "This is a very important experiment, especially when automated machine learning algorithms are interacting with social media content and users in the wild.", "labels": [], "entities": []}, {"text": "The mystery dataset consists of pairs of sentences that differ only in the social context (e.g. gender or race).", "labels": [], "entities": []}, {"text": "Submitted models are expected to predict the same affective values for both sentences in the pair.", "labels": [], "entities": []}, {"text": "The evaluation metric is the average difference in prediction scores per class, along with the p-value score indicating if the difference is statistically significant.", "labels": [], "entities": []}, {"text": "Results are summarized in. shows a heat-map of the attention weights on top of 8 example tweets (2 tweets per emotion).", "labels": [], "entities": []}, {"text": "The color intensity corresponds to the weight given to each word by the self-attention mechanism and signifies the importance of this word for the final prediction.", "labels": [], "entities": []}, {"text": "We can see that the salient words correspond to the predicted emotion (e.g. \"irritated\" for anger, \"mourn\" for sadness etc.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of our experiments across all subtasks on the official evaluation metrics. For subtasks  EI-reg, EI-oc, V-reg, V-oc, the evaluation metric is Pearson correlation. For subtask E-c, the evaluation  metric is multi-label accuracy (Jaccard index). BOW stands for Bag-of-Words baseline, N-BOW stands  for Neural Bag-of-Words baseline and N-BOW+A indicates the inclusion of the affective word features.  As for the neural models, RD stands for random initialization, TL for Transfer Learning, FR for Frozen  pretrained layers (without fine-tuning) and FT for Fine-Tuning. For our deep-learning models, the results  are computed by averaging 10 runs to account for the variability in training performance.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 160, "end_pos": 179, "type": "METRIC", "confidence": 0.942688912153244}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.6427092552185059}, {"text": "BOW", "start_pos": 262, "end_pos": 265, "type": "METRIC", "confidence": 0.957448422908783}, {"text": "FR", "start_pos": 505, "end_pos": 507, "type": "METRIC", "confidence": 0.994476854801178}, {"text": "FT", "start_pos": 564, "end_pos": 566, "type": "METRIC", "confidence": 0.9970362186431885}]}, {"text": " Table 3: Analysis for inappropriate biases", "labels": [], "entities": []}]}