{"title": [{"text": "Resolving Event Coreference with Supervised Representation Learning and Clustering-Oriented Regularization", "labels": [], "entities": [{"text": "Resolving Event Coreference", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9141703248023987}, {"text": "Regularization", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.5093008279800415}]}], "abstractContent": [{"text": "We present an approach to event coreference resolution by developing a general framework for clustering that uses supervised representation learning.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8697077830632528}]}, {"text": "We propose a neural network architecture with novel Clustering-Oriented Regularization (CORE) terms in the objective function.", "labels": [], "entities": []}, {"text": "These terms encourage the model to create embeddings of event mentions that are amenable to clustering.", "labels": [], "entities": []}, {"text": "We then use agglom-erative clustering on these embeddings to build event coreference chains.", "labels": [], "entities": []}, {"text": "For both within-and cross-document coreference on the ECB+ corpus, our model obtains better results than models that require significantly more pre-annotated information.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9186588327089945}]}, {"text": "This work provides insight and motivating results fora new general approach to solving coreference and clustering problems with representation learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.8693573772907257}]}], "introductionContent": [{"text": "Event coreference resolution is the task of determining which event mentions expressed in language refer to the same real-world event instances.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7999183138211569}]}, {"text": "The ability to resolve event coreference has improved the quality of downstream tasks such as automatic text summarization), questioning-answering), headline generation (, and text-mining in the medical domain.", "labels": [], "entities": [{"text": "resolve event coreference", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.8023475408554077}, {"text": "text summarization", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6908915340900421}, {"text": "headline generation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.9177966117858887}]}, {"text": "Event mentions are comprised of an action component (or, head) and surrounding arguments.", "labels": [], "entities": []}, {"text": "Consider the following passages, drawn from two different documents; the heads of the event mentions are in boldface and the subscripts indicate mention IDs: (1) The president's speech m1 shocked m2 the audience.", "labels": [], "entities": []}, {"text": "He announced m3 several new controversial policies.", "labels": [], "entities": []}, {"text": "(2) The policies proposed m4 by the president will not surprise m5 those who followed m6 his campaign m7 . In this example, m1, m3, and m4 form a chain of coreferent event mentions (underlined), because they refer to the same real-world event in which the president gave a speech.", "labels": [], "entities": []}, {"text": "The other four are singletons, meaning that they all refer to separate events and do not corefer with any other mention.", "labels": [], "entities": []}, {"text": "This work investigates how to learn useful representations of event mentions.", "labels": [], "entities": []}, {"text": "Event mentions are complex objects, and both the event mention heads and the surrounding arguments are important for the event coreference resolution task.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7486602862675985}]}, {"text": "In our example above, the head words of mentions m2, shocked, and m5, surprise, are lexically similar, but the event mentions do not corefer.", "labels": [], "entities": []}, {"text": "This task therefore necessitates a model that can capture the distributional relationships between event mentions and their surrounding contexts.", "labels": [], "entities": []}, {"text": "We hypothesize that prior knowledge about the task itself can be usefully encoded into the representation learning objective.", "labels": [], "entities": []}, {"text": "For our task, this prior means that the embeddings of corefential event mentions should have similar embeddings to each other (a \"natural clustering\", using the terminology of).", "labels": [], "entities": []}, {"text": "With this prior, our model creates embeddings of event mentions that are directly conducive for the clustering task of building event coreference chains.", "labels": [], "entities": []}, {"text": "This is contrary to the indirect methods of previous work that rely on pairwise decision making followed by a separate model that aggregates the sometimes inconsistent decisions into clusters (Section 2).", "labels": [], "entities": []}, {"text": "We demonstrate these points by proposing a method that learns to embed event mentions into a space that is tuned specifically for clustering.", "labels": [], "entities": []}, {"text": "The representation learner is trained to predict which event cluster the event mention belongs to, 1 using an hourglass-shaped neural network.", "labels": [], "entities": []}, {"text": "We propose a mechanism to modulate this training by introducing Clustering-Oriented Regularization (CORE) terms into the objective function of the learner; these terms impel the model to produce similar embeddings for coreferential event mentions, and dissimilar embeddings otherwise.", "labels": [], "entities": []}, {"text": "Our model obtains strong results on withinand cross-document event coreference resolution, matching or outperforming the system of Cybulska and Vossen (2015) on the ECB+ corpus on all six evaluation measures.", "labels": [], "entities": [{"text": "cross-document event coreference resolution", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.6107382625341415}, {"text": "ECB+ corpus", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9272069533665975}]}, {"text": "We achieve these gains despite the fact that our model requires significantly less pre-annotated or pre-detected information in terms of the internal event structure.", "labels": [], "entities": []}, {"text": "Our model's improvements upon the baselines show that our supervised representation learning framework creates new embeddings that capture the abstract distributional relations between samples and their clusters, suggesting that our framework can be generalized to other clustering tasks 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We run our experiments on the ECB+ corpus, the largest corpus that contains both within-and crossdocument event coreference annotations.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9189922213554382}]}, {"text": "We followed the train/test split of, using topics 1-35 as the train set and 36-45 as the test set.", "labels": [], "entities": []}, {"text": "During training, we split off a validation set 6 for hyperparameter tuning.", "labels": [], "entities": []}, {"text": "Following Cybulska and Vossen, we used the portion of the corpus that has been manually reviewed and checked for correctness.", "labels": [], "entities": []}, {"text": "Some previous work) do not appear to have followed this guideline from the corpus creators, as they report different corpus statistics compared to those reported by Cybulska and Vossen.", "labels": [], "entities": []}, {"text": "As a result, those papers may report results on a data set with known annotation errors.", "labels": [], "entities": []}, {"text": "Since there is no consensus in the coreference resolution literature on the best evaluation measure, we present results obtained according to six different measures, as is common in previous work.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9226793646812439}]}, {"text": "We use the scorer presented by . In this task, the term \"coreference chain\" is synonymous with \"cluster\".", "labels": [], "entities": []}, {"text": "MUC (. Link-level measure which counts the minimum number of link changes required to obtain the correct clustering from the predictions; it does not account for correctly predicted singletons.).", "labels": [], "entities": [{"text": "MUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6167941689491272}]}, {"text": "Mention-level measure which computes precision and recall for each individual mention, overcoming the singleton problem of MUC, but can problematically count the same coreference chain multiple times.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9985456466674805}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.997401237487793}, {"text": "MUC", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.6244547963142395}]}, {"text": "Mention-level measure which reflects the percentage of mentions that are in the correct coreference chains.", "labels": [], "entities": []}, {"text": "Note that precision and recall are the same in this measure since we use pre-annotated mentions.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995763897895813}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9994223117828369}]}, {"text": "Entity-level measure computed by aligning predicted with the gold chains, not allowing one chain to have more than one alignment, overcoming the problem of B 3 . BLANC ( ).", "labels": [], "entities": [{"text": "Entity-level measure", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9546880424022675}, {"text": "B", "start_pos": 156, "end_pos": 157, "type": "METRIC", "confidence": 0.9654220342636108}, {"text": "BLANC", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9487163424491882}]}, {"text": "Computes two Fscores in terms of the pairwise quality of coreference decisions and non-coreference decisions, and averages these scores together for the final results.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9947195053100586}]}, {"text": "The mean of MUC, B 3 , and CEAF-E.", "labels": [], "entities": [{"text": "MUC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8251187205314636}, {"text": "B 3", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.6969554126262665}, {"text": "CEAF-E", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.8681508302688599}]}], "tableCaptions": [{"text": " Table 2: Model comparison based on validation  set B 3 accuracy with optimized \u03c4 cluster-similarity  threshold. For CORE+CCE+LEMMA (indicated as  CORE+CCE+L) we tuned to \u03b4 = 0.89; for LEMMA-\u03b4  we tuned to \u03b4 = 0.67.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9661113023757935}]}, {"text": " Table 3: Combined within-and cross-document test set results on ECB+. Measures CM and CE stand for mention- based CEAF and entity-based CEAF, respectively.", "labels": [], "entities": [{"text": "ECB+", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.953000545501709}, {"text": "CE", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.5195063948631287}]}, {"text": " Table 4: Within-document test set results on ECB+. Note that LEMMA is equivalent to LEMMA-\u03b4 in the within- document setting. Cybulska and Vossen (2015) did not report the performance of their model in this setting.", "labels": [], "entities": [{"text": "ECB+", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9327315092086792}, {"text": "LEMMA", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9931455850601196}, {"text": "LEMMA-\u03b4", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9759483337402344}]}]}