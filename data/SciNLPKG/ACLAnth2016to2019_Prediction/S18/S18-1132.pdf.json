{"title": [{"text": "UniMa at SemEval-2018 Task 7: Semantic Relation Extraction and Classification from Scientific Publications", "labels": [], "entities": [{"text": "Semantic Relation Extraction and Classification", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.7859891355037689}]}], "abstractContent": [{"text": "Large repositories of scientific literature call for the development of robust methods to extract information from scholarly papers.", "labels": [], "entities": []}, {"text": "This problem is addressed by the SemEval 2018 Task 7 on extracting and classifying relations found within scientific publications.", "labels": [], "entities": [{"text": "SemEval 2018 Task 7", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8570247143507004}, {"text": "extracting and classifying relations found within scientific publications", "start_pos": 56, "end_pos": 129, "type": "TASK", "confidence": 0.765737559646368}]}, {"text": "In this paper, we present a feature-based and a deep learning-based approach to the task and discuss the results of the system runs that we submitted for evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, the exploding amount of scientific literature makes it evermore problematic for researchers and scholars to get focused access to the state-of-the-art in a certain field of science.", "labels": [], "entities": []}, {"text": "Therefore, it is getting increasingly important to develop effective computational approaches for extracting information from large scholarly corpora.) addresses this problem with a shared task on extracting and classifying semantic relations in scientific papers.", "labels": [], "entities": [{"text": "extracting and classifying semantic relations in scientific papers", "start_pos": 197, "end_pos": 263, "type": "TASK", "confidence": 0.7638280354440212}]}, {"text": "The task is divided into two subtasks: 1.", "labels": [], "entities": []}, {"text": "Given an existing relation between two entities and their context, the task is to predict the label of the relation out of the set of possible classes, namely USAGE, RESULT, MODEL-FEATURE, PART WHOLE, TOPIC, COMPARISON.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.9169129133224487}, {"text": "RESULT", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.986312210559845}, {"text": "MODEL-FEATURE", "start_pos": 174, "end_pos": 187, "type": "METRIC", "confidence": 0.9482991695404053}, {"text": "PART WHOLE", "start_pos": 189, "end_pos": 199, "type": "METRIC", "confidence": 0.8255372643470764}, {"text": "TOPIC", "start_pos": 201, "end_pos": 206, "type": "METRIC", "confidence": 0.7065473794937134}]}, {"text": "The task is decomposed into two different scenarios according to the data used, namely with either manually annotated entities (1.1) or noisy data with automatically extracted entities (1.2).", "labels": [], "entities": []}], "datasetContent": [{"text": "Here, we briefly give an overview of the data provided by the organizers of the shared task as well as our submitted runs.", "labels": [], "entities": []}, {"text": "We also present and discuss the final results achieved.", "labels": [], "entities": []}, {"text": "The training data provided by the task organizers for subtask 1.1 and subtask 2 is composed of 350 abstracts of scientific publications with manually annotated entities and relations.", "labels": [], "entities": []}, {"text": "In total, the number of relation instances amounts to 1,228 samples.", "labels": [], "entities": []}, {"text": "shows the distribution of the labels for the relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.8062525391578674}]}, {"text": "For the relation extraction task, entity pairs for all semantic relations appear in the very same sentence.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8495067954063416}]}, {"text": "Therefore, we generate relation candidates by pairing all entity mentions found within the same sentence boundary.", "labels": [], "entities": []}, {"text": "This way we end up with 8,386 candidate entity pairs among which 1,228 are positive instances.", "labels": [], "entities": []}, {"text": "The test data for task 1.1 and 2 was provided in the same format as training data, containing 150 abstracts and 355 relation instances (for subtask 1.1 only).", "labels": [], "entities": []}, {"text": "We selected two models for the relation classification and three models for the relation extraction task for the final submission according to their scores on the development data using the official scoring script.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.9123190641403198}, {"text": "relation extraction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8562604486942291}]}, {"text": "The model configurations are summarized in, respectively.", "labels": [], "entities": []}, {"text": "The official scores for the submitted models are listed in: Submitted models for task 2 (relation extraction).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.795649528503418}]}, {"text": "In the features listed, x represents the first entity while y represents the second entity of a candidate pair.", "labels": [], "entities": []}, {"text": "In the relation classification task, SVM achieves better performance than kNN by a large margin, while in the relation extraction task, it is the deep learning models that perform best.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.920411745707194}, {"text": "relation extraction task", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.8826164603233337}]}, {"text": "Within the scope of the traditional models, SVM consistently outperforms kNN for both the classification (on every relation type, cf.), as well as the extraction task).", "labels": [], "entities": []}, {"text": "Furthermore, for task 1.1, SVM performed better than kNN, in every relation type.", "labels": [], "entities": []}, {"text": "The reason could be that the vector-like features we used, such as Tf-Idf and the binary POStags, suit better for SVM, while kNN was notable to handle the high-dimensional dataset.", "labels": [], "entities": []}, {"text": "For task 2 the linear SVM model results in a relatively high recall but considerably low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9993705153465271}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.998254120349884}]}, {"text": "This result could be related to the following reasons.", "labels": [], "entities": []}, {"text": "First, since the training data is highly skewed towards negative examples as described in subsection 4, more false positive cases are predicted.", "labels": [], "entities": []}, {"text": "Second, the data is likely to be nonlin-: Results on relation extraction (task 2).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8440223038196564}]}, {"text": "As a result, the linear SVM is notable to perform better even when increasing feature dimensionality.", "labels": [], "entities": []}, {"text": "In addition to a better feature engineering, we explored the use of an Radial Basis Function (RBF) kernel and Gradient Boosting Tree to increase the precision without hurting the recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9990087747573853}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9981385469436646}]}, {"text": "Another interesting point is that the improvement of precision contributed more to the overall F1-score in this official evaluation method.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9996916055679321}, {"text": "F1-score", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9993677735328674}]}, {"text": "This can be inferred from the results listed in, where the CNN has higher F1-score, with a higher precision but much lower recall compared to the SVM.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9996165037155151}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.998599112033844}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9994257688522339}]}], "tableCaptions": [{"text": " Table 1: Example of how we assign relative posi- tion to training and test instances.", "labels": [], "entities": []}, {"text": " Table 2: Class distribution in the training data.", "labels": [], "entities": []}, {"text": " Table 5: Results (F1) on relation classification  (task 1.1).", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9867597222328186}, {"text": "relation classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9426140189170837}]}, {"text": " Table 6: Results on relation extraction (task 2).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9022848606109619}]}]}