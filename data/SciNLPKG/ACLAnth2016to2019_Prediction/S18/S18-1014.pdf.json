{"title": [{"text": "FOI DSS at SemEval-2018 Task 1: Combining LSTM States, Embeddings, and Lexical Features for Affect Analysis", "labels": [], "entities": [{"text": "FOI DSS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8233830332756042}, {"text": "Affect Analysis", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.6820085346698761}]}], "abstractContent": [{"text": "This paper describes the system used and results obtained for team FOI DSS at SemEval-2018 Task 1: Affect In Tweets.", "labels": [], "entities": [{"text": "FOI DSS", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.6097606718540192}, {"text": "SemEval-2018 Task 1", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.5756500164667765}]}, {"text": "The team participated in all English language subtasks, with a method utilizing transfer learning from LSTM nets trained on large sentiment datasets combined with embeddings and lexical features.", "labels": [], "entities": []}, {"text": "For four out of five subtasks, the system performed in the range of 92-95% of the winning systems, in terms of the competition metrics.", "labels": [], "entities": []}, {"text": "Analysis of the results suggests that improved pre-processing and addition of more lexical features may further elevate performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of automatic emotion detection, many contributions consider the issue of detecting presence of emotions (.", "labels": [], "entities": [{"text": "automatic emotion detection", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6130188306172689}, {"text": "detecting presence of emotions", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8483337759971619}]}, {"text": "The task of detecting intensity of emotion in a given text is less studied, but is relevant to many applications in fields such as e.g., brand management, public health, politics, and disaster handling.", "labels": [], "entities": [{"text": "detecting intensity of emotion in a given text", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.8678693622350693}, {"text": "brand management", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.8795042037963867}, {"text": "disaster handling", "start_pos": 184, "end_pos": 201, "type": "TASK", "confidence": 0.8250188529491425}]}, {"text": "When developing prediction systems, access to suitably annotated data is critical.", "labels": [], "entities": []}, {"text": "Most annotated emotion and affect datasets are categorical, but examples of sets annotated with intensity or degree of emotional content include EmoBank, AFINN, the Pietro Facebook post set (Preot\u00b8iucPreot\u00b8iuc-, and the WarrinerKuperman set ().", "labels": [], "entities": [{"text": "EmoBank", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9304260015487671}, {"text": "AFINN", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.7308132648468018}, {"text": "Pietro Facebook post set", "start_pos": 165, "end_pos": 189, "type": "DATASET", "confidence": 0.9213324040174484}, {"text": "WarrinerKuperman set", "start_pos": 220, "end_pos": 240, "type": "DATASET", "confidence": 0.9443783462047577}]}, {"text": "For tweets, the Tweet Emotion Intensity Dataset has recently been published, with more than 7000 tweets annotated with emotion category and intensity.", "labels": [], "entities": [{"text": "Tweet Emotion Intensity Dataset", "start_pos": 16, "end_pos": 47, "type": "DATASET", "confidence": 0.6717949062585831}]}, {"text": "This paper describes methods used and results achieved with the FOI DSS contribution to the five subtasks for English tweets of SemEval 2018 Task 1: Affect in Tweets ( ).", "labels": [], "entities": [{"text": "FOI DSS contribution", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.944337805112203}, {"text": "SemEval 2018 Task 1", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.6925789713859558}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "A description of Task 1 is provided in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 discusses the provided datasets.", "labels": [], "entities": []}, {"text": "Section 4 describes the methods and system used to produce predictions of scores and labels for all subtasks.", "labels": [], "entities": []}, {"text": "In Sections 5 and 6 results are presented and analyzed, and suggestions for improvements are outlined.", "labels": [], "entities": []}, {"text": "Finally, concluding remarks are found in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the quality of the feature vectors described in Section 4.2 we computed the PCC on the V-reg subtask using the validation data.", "labels": [], "entities": [{"text": "PCC", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.984372079372406}]}, {"text": "For each set of features listed in we performed a hyper-parameter search to find the parameters of the final stage model maximizing the PCC (cf. Section 4.3).", "labels": [], "entities": [{"text": "PCC", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.8359405994415283}]}, {"text": "As seen in the features provided by the Weka Affective Tweets package have the strongest individual predictive power.", "labels": [], "entities": [{"text": "Weka Affective Tweets package", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.8744119107723236}]}, {"text": "From the Weka filters, the feature combination chosen to be included in the combined method was W E + W SS + W L , which produced the highest PCC during evaluation.", "labels": [], "entities": [{"text": "PCC", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9960941672325134}]}, {"text": "An analysis for inapproperiate gender and race bias in scoring and classifications was performed by the task organizers for the \"mystery\" dataset (Section 3).", "labels": [], "entities": [{"text": "mystery\" dataset", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.5898719330628713}]}, {"text": "For most teams, the bias was small (below 3%) but statistically significant, in part likely due to biases in the AIT dataset.", "labels": [], "entities": [{"text": "bias", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9913075566291809}, {"text": "AIT dataset", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.9248442947864532}]}, {"text": "For the FOI DSS system, the biases were below average for EIjoy, EI-sadness and valence, and 1% or less for all datasets except gender bias for EI-fear (2.3%).", "labels": [], "entities": [{"text": "FOI DSS system", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9132397174835205}]}, {"text": "Biases in the datasets used to train our LSTM models as well as in the lexicons used to extract lexical features may have contributed to biases in scoring and classification.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tweets in the datasets for different  subtasks. The sets for EI-reg and EI-oc were identical,  as was also the case for V-reg and V-oc.", "labels": [], "entities": []}, {"text": " Table 2: V-reg validation set: PCC of valence inten- sity score predictions with gold scores for the different  feature vector combinations.", "labels": [], "entities": [{"text": "PCC", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.6745882630348206}]}, {"text": " Table 3: PCC/Jaccard similarity score on validation  and test data for the FOI DSS system for all English  subtasks of Task 1. The performance of the organiz- ers' SVM unigrams baseline model on the test data is  provided for comparison.", "labels": [], "entities": [{"text": "PCC/Jaccard similarity score", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7440381526947022}, {"text": "FOI DSS system", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9055971503257751}]}, {"text": " Table 4: Tweets with large prediction errors for the valence validation and test sets.", "labels": [], "entities": []}]}