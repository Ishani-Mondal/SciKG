{"title": [{"text": "SemEval-2018 Task 12: The Argument Reasoning Comprehension Task", "labels": [], "entities": [{"text": "SemEval-2018 Task 12", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.872108538945516}, {"text": "Argument Reasoning Comprehension Task", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7224322780966759}]}], "abstractContent": [{"text": "A natural language argument is composed of a claim as well as reasons given as premises for the claim.", "labels": [], "entities": []}, {"text": "The warrant explaining the reasoning is usually left implicit, as it is clear from the context and commonsense.", "labels": [], "entities": []}, {"text": "This makes a comprehension of arguments easy for humans but hard for machines.", "labels": [], "entities": []}, {"text": "This paper summarizes the first shared task on argument reasoning comprehension.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.8500587542851766}]}, {"text": "Given a premise and a claim along with some topic information, the goal is to automatically identify the correct warrant among two candidates that are plausible and lexically close, but in fact imply opposite claims.", "labels": [], "entities": []}, {"text": "We describe the dataset with 1970 instances that we built for the task, and we outline the 21 computational approaches that participated , most of which used neural networks.", "labels": [], "entities": []}, {"text": "The results reveal the complexity of the task, with many approaches hardly improving over the random accuracy of \u2248 0.5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.8193536996841431}]}, {"text": "Still, the best observed accuracy (0.712) underlines the principle feasibility of identifying warrants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9991104006767273}, {"text": "identifying warrants", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.8785079717636108}]}, {"text": "Our analysis indicates that an inclusion of external knowledge is key to reasoning comprehension.", "labels": [], "entities": []}], "introductionContent": [{"text": "When we argue in natural language, we give reasons as premises for our claims.", "labels": [], "entities": []}, {"text": "A fundamental pragmatic instrument in this regard is to leave those parts of an argument unstated that can be presupposed.", "labels": [], "entities": []}, {"text": "This is particularly common for the reasoning between an argument's premises and its claim, called implicit warrants there.", "labels": [], "entities": []}, {"text": "A warrant takes the role of an inference rule, i.e., the abstract structure of an argument is reason \u2192 (since) warrant \u2192 (therefore) claim.", "labels": [], "entities": []}, {"text": "In principle, this structure applies to deductive arguments, which allows us to validate arguments properly formalized in propositional logic.", "labels": [], "entities": []}, {"text": "However, most natural language arguments are in fact inductive or defeasible.", "labels": [], "entities": []}, {"text": "Now, when we comprehend an argument, we reconstruct its warrant driven by the cognitive principle of relevance ().", "labels": [], "entities": []}, {"text": "What is easy for humans in many cases, however, turns out to be hard for machines, because reasoning usually depends on context and commonsense.", "labels": [], "entities": []}, {"text": "In (, we have thus introduced the argument reasoning comprehension task in order to study the construction and identification of implicit warrants for natural language arguments.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.7807966619729996}, {"text": "construction and identification of implicit warrants for natural language arguments", "start_pos": 94, "end_pos": 177, "type": "TASK", "confidence": 0.7017538368701934}]}, {"text": "It forms the basis of the shared task presented here: Task Given an argument with a reason serving as a premise fora claim, along with the topic and some additional information of the discussion they occur in, identify the correct warrant among two opposing candidates, warrant0 and warrant1.", "labels": [], "entities": []}, {"text": "With opposing, we here mean that the two candidate warrants actually imply contradicting claims, the correct one and its opposite.", "labels": [], "entities": []}, {"text": "An instance of the task is shown in.", "labels": [], "entities": []}, {"text": "Being a binary classification task, the main evaluation measure of argument reasoning comprehension is accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9981416463851929}]}, {"text": "To our knowledge, this is the first shared NLP task directly targeting argumentation; others tasks have only been sketched so far (.", "labels": [], "entities": []}, {"text": "A solution to our task will represent a substantial step towards automatic warrant reconstruction, which in turn is important for the general longterm goal of automatic argument evaluation.", "labels": [], "entities": [{"text": "automatic warrant reconstruction", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.567683349053065}, {"text": "automatic argument evaluation", "start_pos": 159, "end_pos": 188, "type": "TASK", "confidence": 0.6197801232337952}]}, {"text": "So far, most research on computational argumentation focused on mining claims and premises from text and assessing their properties.", "labels": [], "entities": []}, {"text": "In contrast, filling the gap between claims and premises computationally remains an open issue, due to the inherent difficulty of reconstructing the world knowledge and reasoning patterns in arguments.", "labels": [], "entities": []}, {"text": "Previous tasks have dealt with the textual entailment of a hypothesis from a proposition ( or with semantic inference.", "labels": [], "entities": [{"text": "textual entailment of a hypothesis from a proposition", "start_pos": 35, "end_pos": 88, "type": "TASK", "confidence": 0.8517789915204048}]}, {"text": "While understanding semantics is important in the given task, argumentation also reasoning beyond what is understood, i.e., pragmatics.", "labels": [], "entities": []}, {"text": "As a basis for the shared task, we built anew dataset with 1970 instances based on authentic English arguments, whose concept and construction process is detailed in Section 2.", "labels": [], "entities": []}, {"text": "We outline the systems that participated in the task in Section 3.", "labels": [], "entities": []}, {"text": "Most systems implement a computational approach that employs one or more neural networks (often LSTMs, often with attention) based on different pre-trained embedding models.", "labels": [], "entities": []}, {"text": "We then present the results of all systems on the test set of the shared task in Section 4 and analyze specific cases in Section 5, before we finally conclude (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the dataset with all instances used in the shared task.", "labels": [], "entities": []}, {"text": "We summarize the main points from its construction process, which is described in detail in ().", "labels": [], "entities": []}, {"text": "For the shared task, we split the 1,970 instances into three sets based on the year of the debate they were taken from: 2011-2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances).", "labels": [], "entities": []}, {"text": "This follows the paradigm of learning on past data and predicting on new ones.", "labels": [], "entities": []}, {"text": "In addition, it removes much lexical and topical overlap.", "labels": [], "entities": []}, {"text": "The same split has been used by.", "labels": [], "entities": []}, {"text": "The shared task had two phases, trial and test.", "labels": [], "entities": []}, {"text": "In the trial phase, the training and development set were given, both with gold labels stating the correct warrant for all instances.", "labels": [], "entities": []}, {"text": "In the test phase, all three datasets were available.", "labels": [], "entities": []}, {"text": "Naturally, no labels were given for the test set instances.", "labels": [], "entities": []}, {"text": "All provided data is licensed under Creative Commons-family license.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Final results of the competition. For the star- denoted system, no description has been provided.", "labels": [], "entities": []}, {"text": " Table 3: p-values obtained by running the approximate randomization test among all systems. For convenience,  the diagonal (bold values) shows the accuracy of each system as in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9991371035575867}]}]}