{"title": [{"text": "Joker at SemEval-2018 Task 12: The Argument Reasoning Comprehension with Neural Attention 1", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a classification system that participated in the SemEval-2018 Task 12: The Argument Reasoning Comprehension Task.", "labels": [], "entities": [{"text": "SemEval-2018 Task 12", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8468935887018839}, {"text": "Argument Reasoning Comprehension Task", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.7285092920064926}]}, {"text": "Briefly the task can be described as that a natural language \"argument\" is what we have, with reason, claim, and correct and incorrect warrants, and we need to choose the correct warrant.", "labels": [], "entities": []}, {"text": "In order to make fully understand of the semantic information of the sentences, we proposed a neural network architecture with attention mechanism to achieve this goal.", "labels": [], "entities": []}, {"text": "Besides we try to introduce keywords into the model to improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9988681077957153}]}, {"text": "Finally the proposed system achieved 5th place among 22 participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, as an extremely important part of argument mining, argument reasoning has received considerable research.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8813974857330322}, {"text": "argument reasoning", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.913650631904602}]}, {"text": "The argumentation reasoning can be used in many situations such as automatic score, policy decision, stance detection, and many others ().", "labels": [], "entities": [{"text": "policy decision", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.8290971219539642}, {"text": "stance detection", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.9192270040512085}]}, {"text": "The task can be described as follows in detail: Given an argument consisting of a claim and a reason, the goal is to select the correct warrant that explains reasoning of this particular argument.", "labels": [], "entities": []}, {"text": "There are only two warrants given and only one answer is correct.", "labels": [], "entities": []}, {"text": "The correct warrant inferred from the argument has a supported relation with the argument.", "labels": [], "entities": []}, {"text": "However the other warrant either opposes the argument or has no correlation with the argument.", "labels": [], "entities": []}, {"text": "Actually, this task could be treated as a binary classification(A vs. B).", "labels": [], "entities": []}, {"text": "The task could be regarded as an argumentative relation work.", "labels": [], "entities": []}, {"text": "The argumentative relation mining aims at identifying relations of attack and support between natural language arguments in text, by classifying pairs of pieces of text as attack, support or neither attack nor support relations (.", "labels": [], "entities": [{"text": "argumentative relation mining", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7126459876696268}]}, {"text": "The corrected warrant means supporting the argument, while the other means attacking the argument.", "labels": [], "entities": [{"text": "corrected warrant", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9556602537631989}]}, {"text": "So we refer to some literature methods on the relationship between arguments.", "labels": [], "entities": []}, {"text": "use Long Short Term Memory (LSTM) to classify the relations between arguments.", "labels": [], "entities": []}, {"text": "propose neural network with attention mechanism, making neural networks interpretable.", "labels": [], "entities": []}, {"text": "We infer to their methods and construct our model in new manner.", "labels": [], "entities": []}, {"text": "In this paper, we use the LSTM networks with attention mechanism to construct the classification system.", "labels": [], "entities": [{"text": "classification", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.9593080878257751}]}, {"text": "The following sections are arranged as follows.", "labels": [], "entities": []}, {"text": "In section 2, we will give an overview on the task and have an analysis of the datasets.", "labels": [], "entities": []}, {"text": "In section 3 we describe the system used in this paper and introduce some interesting attempt in detail.", "labels": [], "entities": []}, {"text": "Section 4 introduces the experiment and results.", "labels": [], "entities": []}, {"text": "Finally, we get conclusions and have an outlook of the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "While training the model, the input sentences are separately embedded as 100-dimensional GloVe vectors () and the embedding layer is based on the 100-dimensional GloVe vectors.", "labels": [], "entities": []}, {"text": "We use ADAM () for optimization and set the initial learning rate 0.001.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8336659073829651}]}, {"text": "We trained for 13 epochs or until the performance on development set stopped improving so as to avoid overfitting.", "labels": [], "entities": []}, {"text": "Some Hyper-parameters for model: the dropout is 0.9, the embedding size is 100, the size of LSTM is 64 and the batch size is 256.", "labels": [], "entities": []}, {"text": "We conduct experiment on the training, dev and test corpus downloaded from the SemEval-2018 Task 12.", "labels": [], "entities": [{"text": "test corpus downloaded from the SemEval-2018 Task 12", "start_pos": 47, "end_pos": 99, "type": "DATASET", "confidence": 0.8198323547840118}]}, {"text": "There are five models used to make experiments are LSTM model, LSTM model with attention, LSTM model with word-by-word attention, bag-of-word model and hybrid model.", "labels": [], "entities": []}, {"text": "The results of the experiments could be seen in.", "labels": [], "entities": []}, {"text": "We choose the LSTM model without attention as baseline.", "labels": [], "entities": []}, {"text": "According to the experiment, the simple sequence model couldn't complete the semantic understanding task well.", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9147235751152039}]}, {"text": "The bag-of-word model performs better than the LSTM model, which proves that the keywords could express the semantics of the sentences.", "labels": [], "entities": []}, {"text": "As for the keywords can't express all the information of the sentences while the LSTM model with attention cannot only express the whole information but also grab the important part, the bag-of-word performs worse than the attention model.", "labels": [], "entities": []}, {"text": "LSTM model with word-by-word attention makes a great contribution to the best result.", "labels": [], "entities": []}, {"text": "The hybrid model doesn't have an improvement in dev corpus but have a similar results with the word-by-word attention model.", "labels": [], "entities": []}, {"text": "We guess that what causes such a result is the small data corpus and simply mechanically combining the models with each other.", "labels": [], "entities": []}, {"text": "So we don't get a satisfactory result from the hybrid model.", "labels": [], "entities": []}, {"text": "The LSTM model with word-by-word attention gets the accuracy of 0.586 in the final submission, achieving the fifth place in the shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996095299720764}]}, {"text": "In the future, we will consider more reasonable combinations of the sentence model with keywords to enhance the comprehension of the sentences.", "labels": [], "entities": []}, {"text": "Besides, we will introduce the CNN into our model to extract the word character to improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9983534812927246}]}], "tableCaptions": [{"text": " Table 1: The results of the experiments. Attention,  WBW attention, BOW model stand for LSTM model  with attention, LSTM model with word-by-word  attention, bag-of-word model respectively. Training on  the training corpus while testing accuracy is computed  on the dev corpus and test corpus.", "labels": [], "entities": [{"text": "WBW attention", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.7179619073867798}, {"text": "BOW", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9932073950767517}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9963529109954834}]}]}