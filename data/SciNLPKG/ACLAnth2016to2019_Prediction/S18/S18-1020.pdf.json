{"title": [{"text": "INGEOTEC at SemEval-2018 Task 1: EvoMSA and \u00b5TC for Sentiment Analysis", "labels": [], "entities": [{"text": "\u00b5TC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.555551290512085}, {"text": "Sentiment Analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9261162281036377}]}], "abstractContent": [{"text": "This paper describes our participation in Affective Tweets task for emotional intensity and sentiment intensity subtasks for English, Spanish, and Ara-bic languages.", "labels": [], "entities": [{"text": "Affective Tweets task", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7177854180335999}]}, {"text": "We used two approaches, \u00b5TC and EvoMSA.", "labels": [], "entities": []}, {"text": "The first one is a generic text catego-rization and regression system; and the second one is a two-stage architecture for Sentiment Analysis.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9231860637664795}]}, {"text": "Both approaches are multilingual and domain independent .", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment Analysis is a research area where does a computational analysis of people's feelings or beliefs expressed in texts such as emotions, opinions, attitudes, appraisals, etc.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9428147077560425}, {"text": "computational analysis of people's feelings or beliefs expressed in texts such as emotions, opinions, attitudes, appraisals", "start_pos": 51, "end_pos": 174, "type": "TASK", "confidence": 0.631032994389534}]}, {"text": "People communicate not only the emotion or sentiment they are feeling, but also the intensity, that is, the degree of emotion or sentiment.", "labels": [], "entities": []}, {"text": "In this context, SemEval is one of the forums that conducts evaluations on semantics at different levels, for instance, it proposes tasks such as sentiment analysis, the intensity of emotion or sentiment (affective tweets) ( , irony detection, among others.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.923973947763443}, {"text": "irony detection", "start_pos": 227, "end_pos": 242, "type": "TASK", "confidence": 0.8803125619888306}]}, {"text": "In this work, we present the results of our participation in Affective Tweets task for four of the five subtasks in English, Spanish, and Arabic languages and for all emotions available: anger, fear, joy, and sadness.", "labels": [], "entities": []}, {"text": "The subtasks are A) emotion intensity regression (EI-REG): given a tweet and an emotion, determine the intensity of the emotion that best represents the mental state of the tweeter, a real-value score between 0 and 1.", "labels": [], "entities": [{"text": "A) emotion intensity regression (EI-REG)", "start_pos": 17, "end_pos": 57, "type": "METRIC", "confidence": 0.7715318091213703}]}, {"text": "B) Emotion intensity ordinal classification * corresponding author: sabino.miranda@infotec.mx (EI-OC): given a tweet and an emotion E, classify the tweet into one of four ordinal classes of intensity of emotion: anger, fear, joy, and sadness, that best represents the mental state of the tweeter.", "labels": [], "entities": []}, {"text": "C) A sentiment intensity regression task (V-REG): given a tweet, determine the intensity of sentiment, a real-valued score between 0 (most negative) and 1 (most positive).", "labels": [], "entities": [{"text": "sentiment intensity regression", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.5493447085221609}]}, {"text": "D) A sentiment analysis, ordinal classification (V-OC): given a tweet, classify it into one of seven ordinal classes, corresponding to several levels of positive and negative sentiment intensity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8556701838970184}]}, {"text": "In this context, one crucial step is the procedure used to transform the data (i.e., tweets) into the inputs (vectors) of the supervised learning techniques used.", "labels": [], "entities": []}, {"text": "Typically, Natural Language Processing (NLP) approaches for data representation use n-grams of words, linguistic information such as dependency relations, syntactic information, lexical units (e.g., lemmas, stems), affective lexicons, error correction, etc.", "labels": [], "entities": []}, {"text": "However, selecting the best configuration of those characteristics could be a cumbersome task, many times disregarded in favor of some well-known competitive setups.", "labels": [], "entities": []}, {"text": "() studies the dependency between the performance and the proper selection of the text model.", "labels": [], "entities": []}, {"text": "This selection can be seen as a combinatorial optimization problem where the objective is to maximize the performance metric of the classifier being used; this approach is implemented by \u00b5TC, (.", "labels": [], "entities": []}, {"text": "Due to its combinatorial nature, and the kind of parameters that compose the configuration space, the resulting classifiers are multilingual and domain independent.", "labels": [], "entities": []}, {"text": "Therefore, with a tight dependency on the training set, it is mandatory to provide additional information about the particular task to avoid overfitting.", "labels": [], "entities": []}, {"text": "In this sense, the use of multiple knowledge sources is essential, and combining them simply and effectively is the idea be-hind EvoMSA.", "labels": [], "entities": []}, {"text": "EvoMSA ( \u00a72.2) is a stacking system based on genetic programming, and particularly on the use of semantic genetic operators, that focus on sentiment analysis.", "labels": [], "entities": [{"text": "EvoMSA ( \u00a72.2)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8818708777427673}, {"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9331452548503876}]}, {"text": "The core of our contribution is to use both \u00b5TC and EvoMSA to learn from different annotated collections and then use that diverse knowledge to tackle the SemEval 2018 Task 1 challenge.", "labels": [], "entities": [{"text": "\u00b5TC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8455296754837036}, {"text": "SemEval 2018 Task 1 challenge", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.8949353337287903}]}, {"text": "Looking at systems that obtained the best results in previous SemEval editions, it can be concluded that it is necessary to include more datasets, see for instance BB twtr system for Sentiment Analysis in the Twitter task, which uses more datasets besides the one given in the competition.", "labels": [], "entities": [{"text": "BB", "start_pos": 164, "end_pos": 166, "type": "METRIC", "confidence": 0.8106746077537537}, {"text": "Sentiment Analysis", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.918535053730011}]}, {"text": "Here, it was decided to follow a similar approach by including an additional humanannotated dataset publicly available for English, Spanish, and Arabic to build robust models.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we mentioned, to determine the best configuration of parameters for text modeling, \u00b5TC and B4MSA integrate a hyper-parameter optimization phase that ensures the performance of the classifier based on the training data.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.778113842010498}, {"text": "\u00b5TC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8227359056472778}, {"text": "B4MSA", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.681271493434906}]}, {"text": "The text modeling parameters for B4MSA were set for all process as we show in for English and Spanish language for classification and regression tasks.", "labels": [], "entities": [{"text": "B4MSA", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.6865808963775635}]}, {"text": "In the case of the Arabic language, the parameters were calculated by the optimization phase; an example is showed in.", "labels": [], "entities": []}, {"text": "A text transformation feature could be binary (yes/no) or ternary (group/delete/none) option.", "labels": [], "entities": [{"text": "text transformation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7334894686937332}]}, {"text": "Tokenizers denote how texts must be split after applying the process of each text transformation to texts.", "labels": [], "entities": []}, {"text": "Tokenizers generate text chunks in a range of lengths, all tokens generated are part of the text representation.", "labels": [], "entities": []}, {"text": "Both, B4MSA and \u00b5TC, allow selecting tokenizers based on n-words, q\u2212grams, and skip-grams, in any combination.", "labels": [], "entities": [{"text": "B4MSA", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.5410310626029968}, {"text": "\u00b5TC", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.7182206511497498}]}, {"text": "We call n-words to the wellknown word n-grams; in particular, we allow to use any combination of unigrams, bigrams, and trigrams.", "labels": [], "entities": []}, {"text": "Also, the configuration space allows selecting any combination of character q-grams (or just q-grams) for q = 1 to 9.", "labels": [], "entities": []}, {"text": "Finally, we allow to use (2, 1) and (3, 1) skip-grams (two words separated by one word, and three words separated by a gap).", "labels": [], "entities": []}, {"text": "shows the final configurations for English and Spanish and an example for one emotion for Arabic.", "labels": [], "entities": []}, {"text": "For example, numbers are deleted in Arabic, but it is grouped in English and Spanish.", "labels": [], "entities": []}, {"text": "In the case of English, it is split in unigrams, bigrams, character q-grams of sizes 2, 3, and 4.", "labels": [], "entities": []}, {"text": "Tokenizers n-words {1, 2} {1, 2} {1, 2} q-grams {2, 3, 4} {2, 3, 4} {2, 3, 7, 9} skip-grams ---: Example of set of configurations for text modeling  SemEval provides datasets to train systems for each subtask.", "labels": [], "entities": [{"text": "text modeling  SemEval", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.76891028881073}]}, {"text": "For instance, for emotion Anger in English, subtask emotion intensity ordinal classification, OC, the training data is distributed for four classes (class 0 = 445, class 1 = 322, class 2 = 507, class 3 = 427).", "labels": [], "entities": [{"text": "emotion Anger", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.7039311081171036}, {"text": "subtask emotion intensity ordinal classification", "start_pos": 44, "end_pos": 92, "type": "TASK", "confidence": 0.5830525279045105}]}, {"text": "The Arabic datasets for each emotion have around 800 samples each one, for English the sizes are between 1500 and 2200 samples, and for Spanish are between 1000 and 1150 samples, for more details of the data distribution and how the datasets were built we refer the reader to (.", "labels": [], "entities": []}, {"text": "In addition of SemEval data, we use extra datasets annotated by humans around 73 thousand tweets for English, 223 thousand for Spanish (, and two thousand for Arabic.", "labels": [], "entities": []}, {"text": "shows the distribution of classes for datasets.", "labels": [], "entities": []}, {"text": "Those datasets are mainly used for sentiment analysis; however, we use this extra information to improve the final decision in the approach we implemented (EvoMSA).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.960073322057724}]}], "tableCaptions": [{"text": " Table 2: Statistics of Human-Annotated training data. We", "labels": [], "entities": []}, {"text": " Table 3: Results for Emotion Intensity: Ordinal Classifica-", "labels": [], "entities": [{"text": "Emotion Intensity", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9211748540401459}]}, {"text": " Table 4: Results for Valence: Ordinal Classification (OC)", "labels": [], "entities": [{"text": "Valence", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9813597202301025}, {"text": "Ordinal Classification (OC)", "start_pos": 31, "end_pos": 58, "type": "METRIC", "confidence": 0.7432536244392395}]}]}