{"title": [{"text": "SemEval 2018 Task 4: Character Identification on Multiparty Dialogues", "labels": [], "entities": [{"text": "SemEval 2018 Task 4", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8673427999019623}, {"text": "Character Identification on Multiparty Dialogues", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.7404893577098847}]}], "abstractContent": [{"text": "Character identification is a task of entity linking that finds the global entity of each personal mention in multiparty dialogue.", "labels": [], "entities": [{"text": "Character identification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.847685843706131}]}, {"text": "For this task, the first two seasons of the popular TV show Friends are annotated, comprising a total of 448 dialogues, 15,709 mentions, and 401 entities.", "labels": [], "entities": []}, {"text": "The personal mentions are detected from nomi-nals referring to certain characters in the show, and the entities are collected from the list of all characters in those two seasons of the show.", "labels": [], "entities": []}, {"text": "This task is challenging because it requires the identification of characters that are mentioned but may not be active during the conversation.", "labels": [], "entities": []}, {"text": "Among 90+ participants, four of them submitted their system outputs and showed strengths in different aspects about the task.", "labels": [], "entities": []}, {"text": "Thorough analyses of the distributed datasets, system outputs , and comparative studies are also provided.", "labels": [], "entities": []}, {"text": "To facilitate the momentum, we create an open-source project for this task and publicly release a larger and cleaner dataset, hoping to support researchers for more enhanced modeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of the earlier works in natural language processing (NLP) had focused on formal writing such as newswires, whereas many recent works have targeted at colloquial writing such as text messages or social media.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7549488246440887}]}, {"text": "Since the evolution of Web 2.0, the amount of user-generated contents involving colloquial writing has exceeded the one with formal writing.", "labels": [], "entities": []}, {"text": "NLP tasks are relatively well-explored at this point for certain types of colloquial writing i.e., microblogs and reviews.", "labels": [], "entities": []}, {"text": "However, the genre of multiparty dialogue is still under-explored, even though digital contents in dialogue forms keep increasing at a faster rate than any other types of writing.", "labels": [], "entities": [{"text": "multiparty dialogue", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7824335396289825}]}, {"text": "This inspires us to create anew task called character identification that aims to link personal mentions (e.g, she, mom) to their global entities across multiple dialogues, where the entities indicate the specific characters referred by those mentions (e.g., Judy).", "labels": [], "entities": [{"text": "character identification", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7626712024211884}]}, {"text": "Due to the nature of multiparty dialogue where several speakers take turns to complete a context, character identification is a crucial step for adapting higher-end NLP tasks (e.g., summarization, question answering, machine translation) to this genre.", "labels": [], "entities": [{"text": "character identification", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.8407592177391052}, {"text": "summarization", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.9408671855926514}, {"text": "question answering", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.6863891333341599}, {"text": "machine translation)", "start_pos": 217, "end_pos": 237, "type": "TASK", "confidence": 0.8131208419799805}]}, {"text": "It can also bring another level of sophistication to intelligent personal assistants or tutoring systems.", "labels": [], "entities": []}, {"text": "This task is challenging because it needs to process through colloquialism that includes slangs, grammar mistakes, and/or rhetorical questions, as well as to handle cross-document resolution for the identification of entities that are mentioned but may not be actively participating during the conversation.", "labels": [], "entities": [{"text": "cross-document resolution", "start_pos": 165, "end_pos": 190, "type": "TASK", "confidence": 0.7942088544368744}]}, {"text": "Nonetheless, we believe that models produced by this task will remarkably enhance inference on dialogue contexts (e.g., business meetings, doctorpatient conversations) by providing finer-grained information about individual characters.", "labels": [], "entities": []}, {"text": "Section 2 illustrates the task of character identification and explains the key differences between it and other types of entity linking tasks.", "labels": [], "entities": [{"text": "character identification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8804695904254913}]}, {"text": "Section 3 describes the corpus, based on TV show transcripts, used for this task with annotation details.", "labels": [], "entities": []}, {"text": "Section 4 gives brief overviews of the systems participated in this shared task.", "labels": [], "entities": []}, {"text": "Section 5 explains the evaluation metrics and the results produced by those systems.", "labels": [], "entities": []}, {"text": "Finally, Section 6 gives thorough analysis and comparative studies between these systems.", "labels": [], "entities": []}, {"text": "This task was originally conducted at CodaLab.", "labels": [], "entities": []}, {"text": "The latest dataset and the system outputs can be found from our open source project, Emory NLP.", "labels": [], "entities": [{"text": "Emory NLP", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.972791850566864}]}], "datasetContent": [{"text": "Following, the labeling accuracy (Acc) and the macro-average F1 score (F1) are used for the evaluation (C: the total number of characters, F 1 i : the F1-score for the i'th character): Acc = # of corrected identified mentions # of all mentions F 1 i shows the overall scores from all submitted systems.", "labels": [], "entities": [{"text": "accuracy (Acc)", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.8305463045835495}, {"text": "F1 score (F1)", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.890244460105896}, {"text": "F1-score", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9892958998680115}, {"text": "Acc", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.9985731840133667}]}, {"text": "Two types of evaluation are performed for this task.", "labels": [], "entities": []}, {"text": "The first one is based on seven characters where six of them compose the primary characters (Section 3.2) and every other character is grouped as one entity called Others (Main + Others).", "labels": [], "entities": []}, {"text": "The other is based on 78 characters comprising all characters appeared in the dataset, except for the ones appear either in the training or the evaluation set but not both, which is grouped to the Others (ALL).: Overall scores from the submitted systems.", "labels": [], "entities": []}, {"text": "shows the F1 scores for the primary characters and Others, illustrating detailed evaluation for Main + Others.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994989633560181}]}, {"text": "gives detailed evaluation for ALL, showing the F1-scores for the top-12 most frequently appeared secondary characters and Others that appear only in the training or the evaluation set but not both.", "labels": [], "entities": [{"text": "ALL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.969660222530365}, {"text": "F1-scores", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9989696741104126}]}, {"text": "The 18 characters in these two tables comprise about 85% of all mentions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distributions from the subset of the character identification corpus used for this shared task.", "labels": [], "entities": [{"text": "character identification", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8454112410545349}]}, {"text": " Table 2: Distributions of the annotated entity types  used for this shared task.", "labels": [], "entities": []}, {"text": " Table 3: Examples of the entity annotation described in Section 3.2.", "labels": [], "entities": []}, {"text": " Table 4: Distributions of the training and the evaluation sets in Section 3.3.", "labels": [], "entities": []}, {"text": " Table 5: Example of the first utterance in", "labels": [], "entities": []}, {"text": " Table 6: Overall scores from the submitted systems.", "labels": [], "entities": []}, {"text": " Table 7: Detailed evaluation for Main + Others in Table 6. The Evaluation and Training rows show the  percentages of individual characters appeared in the evaluation and the training set, respectively.", "labels": [], "entities": []}, {"text": " Table 8: Detailed evaluation for ALL in", "labels": [], "entities": [{"text": "ALL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9616685509681702}]}, {"text": " Table 6. Be: Ben, Ca: Carol, Ed: Eddie, Pa: Paolo, Ju: Julie: MB:  Mrs. Bing, Ri: Richard, Sc: Scott, Ca: Carl, Fr: Frank, Ja: Janice, OT: Others.", "labels": [], "entities": [{"text": "MB", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9624574780464172}]}]}