{"title": [{"text": "Igevorse at SemEval-2018 Task 10: Exploring an Impact of Word Embeddings Concatenation for Capturing Discriminative Attributes", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes", "start_pos": 91, "end_pos": 126, "type": "TASK", "confidence": 0.8412253856658936}]}], "abstractContent": [{"text": "Semantic differences extraction is a challenging problem in Natural Language Processing and its solution is necessary fora realistic semantic representation as similarity information is not sufficient to capture individual aspects of meaning.", "labels": [], "entities": [{"text": "Semantic differences extraction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7827414671579996}]}, {"text": "This paper presents a comparison of several approaches for capturing discriminative attributes and considers an impact of concatenation of several word embed-dings of different nature on the classification performance.", "labels": [], "entities": []}, {"text": "A similarity-based method is proposed and compared with machine learning approaches.", "labels": [], "entities": []}, {"text": "It is shown that this method outperforms others on all the considered word vector models and there is a performance increase when concatenated datasets are used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Detecting semantic similarity is done well by state-of-the-art models.", "labels": [], "entities": [{"text": "Detecting semantic similarity", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9034204284350077}]}, {"text": "However, if the model is only good at similarity detection, it would have a limited practical usage (, since the task of understanding the semantics of words cannot be done without capturing semantic differences.", "labels": [], "entities": [{"text": "similarity detection", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7045402973890305}]}, {"text": "Semantic difference is a ternary relation between two concepts (apple, banana) and a discriminative feature (red) that characterizes the first concept but not the other.", "labels": [], "entities": []}, {"text": "Semantic difference detection is a binary classification task: given a triple (apple, banana, red), the task is to determine whether it exemplifies a semantic difference or not (.", "labels": [], "entities": [{"text": "Semantic difference detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.844687819480896}]}, {"text": "In this paper two concepts and a discriminative attribute attr are represented as (word 1 , word 2 , attr).", "labels": [], "entities": []}, {"text": "This research was done during the participation in \"Capturing Discriminative Attributes\" task of SemEval 2018 competition.", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes\" task", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.8907263278961182}, {"text": "SemEval 2018 competition", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.7865907549858093}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the methods used.", "labels": [], "entities": []}, {"text": "Section 3 shows the results and analyzes them.", "labels": [], "entities": []}, {"text": "Section 4 mentions future directions.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Models are evaluated on F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9639094869295756}]}, {"text": "shows that the behavior of the similarity-based model is highly dependent on the selected word vectors model.", "labels": [], "entities": []}, {"text": "Thresholds, learned from the train set for Google News, Wikipedia and concatenated word vectors are 0.053030, 0.066667 and 0.060606 correspondingly.", "labels": [], "entities": [{"text": "Thresholds", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9737470746040344}, {"text": "Wikipedia", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8624197840690613}]}, {"text": "Experimental results on the validation set are presented in.", "labels": [], "entities": [{"text": "validation", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9587785005569458}]}, {"text": "K-nearest neighbors showed the worst F 1 score, while neural network is the best among machine learning methods.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9883012374242147}]}, {"text": "The proposed similarity-based method outperforms all other models on all considered word embeddings.", "labels": [], "entities": []}, {"text": "Word embeddings with the highest F 1 score on the validation set were chosen for the final comparison: Google News for SGD classifier, Wikipedia for k-nearest neighbors and neural network, concatenated embeddings for logistic regression and similarity-based model.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9727803667386373}, {"text": "Google News", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.903828889131546}]}, {"text": "The results on the test set are presented in.", "labels": [], "entities": []}, {"text": "K-nearest neighbors has the smallest F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9857109785079956}]}, {"text": "In contrast with the performance on the validation set, the result of neural network is noticeably worse, which means that overfitting took place.", "labels": [], "entities": []}, {"text": "Logistic regression performed better than SGD classifier, while similarity-based method showed the highest score.", "labels": [], "entities": [{"text": "SGD classifier", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7594267129898071}]}], "tableCaptions": [{"text": " Table 2: Experimental results on the validation set (F 1 score).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9831656614939371}]}]}