{"title": [{"text": "NTUA-SLP at SemEval-2018 Task 3: Tracking Ironic Tweets using Ensembles of Word and Character Level Attentive RNNs", "labels": [], "entities": [{"text": "NTUA-SLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9179559350013733}]}], "abstractContent": [{"text": "In this paper we present two deep-learning systems that competed at SemEval-2018 Task 3 \"Irony detection in English tweets\".", "labels": [], "entities": [{"text": "SemEval-2018 Task 3", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7197022636731466}, {"text": "Irony detection", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.6505888551473618}]}, {"text": "We design and ensemble two independent models , based on recurrent neural networks (Bi-LSTM), which operate at the word and character level, in order to capture both the semantic and syntactic information in tweets.", "labels": [], "entities": []}, {"text": "Our models are augmented with a self-attention mechanism , in order to identify the most informative words.", "labels": [], "entities": []}, {"text": "The embedding layer of our word-level model is initialized with word2vec word embeddings, pretrained on a collection of 550 million English tweets.", "labels": [], "entities": []}, {"text": "We did not utilize any handcrafted features, lexicons or external datasets as prior information and our models are trained end-to-end using back propagation on constrained data.", "labels": [], "entities": []}, {"text": "Furthermore, we provide visualizations of tweets with annotations for the salient tokens of the attention layer that can help to interpret the inner workings of the proposed models.", "labels": [], "entities": []}, {"text": "We ranked 2 nd out of 42 teams in Subtask A and 2 nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-of-the-art results ranking 1 st for both subtasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Irony is a form of figurative language, considered as \"saying the opposite of what you mean\", where the opposition of literal and intended meanings is very clear (.", "labels": [], "entities": []}, {"text": "Traditional approaches in NLP ( model irony based on pattern-based features, such as the contrast between high and low frequent words, the punctuation used by the author, the level of ambiguity of yay its fucking monday life is so perfect and magical i love everything Label: ironic by clash be a u ti f u l w a y to st a rt my mo r n in g .", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to deal with the problem of class imbalances in Subtask B, we apply class weights to the loss function of our models, penalizing more the misclassification of underrepresented classes.", "labels": [], "entities": []}, {"text": "We weight each class by its inverse frequency in the training set.", "labels": [], "entities": []}, {"text": "Training We use Adam algorithm) for optimizing our networks, with minibatches of size 32 and we clip the norm of the gradients ( at 1, as an extra safety measure against exploding gradients.", "labels": [], "entities": []}, {"text": "For developing our models we used PyTorch ( and Scikit-learn ().", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.6966186761856079}]}, {"text": "In order to find good hyperparameter values in a relative short time (compared to grid or random search), we adopt the Bayesian optimization () approach, performing a \"smart\" search in the high dimensional space of all the possible values.", "labels": [], "entities": []}, {"text": "Table 2, shows the selected hyper-parameters.: Hyper-parameters of our models.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9602887630462646}]}], "tableCaptions": [{"text": " Table 2: Hyper-parameters of our models.", "labels": [], "entities": []}, {"text": " Table 3: Competition results for Subtask A", "labels": [], "entities": [{"text": "Subtask", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9464336633682251}]}, {"text": " Table 4: Competition results for Subtask B", "labels": [], "entities": [{"text": "Subtask", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9490882158279419}]}, {"text": " Table 5: Results of our models for Subtask A", "labels": [], "entities": [{"text": "Subtask", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9576387405395508}]}, {"text": " Table 6: Results of our models for Subtask B", "labels": [], "entities": [{"text": "Subtask", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9509022831916809}]}]}