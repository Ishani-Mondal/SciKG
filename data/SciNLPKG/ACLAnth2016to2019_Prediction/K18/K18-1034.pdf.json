{"title": [{"text": "Upcycle Your OCR: Reusing OCRs for Post-OCR Text Correction in Romanised Sanskrit", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a post-OCR text correction approach for digitising texts in Romanised San-skrit.", "labels": [], "entities": [{"text": "text correction", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.724227100610733}]}, {"text": "Owing to the lack of resources our approach uses OCR models trained for other languages written in Roman.", "labels": [], "entities": []}, {"text": "Currently, there exists no dataset available for Romanised San-skrit OCR.", "labels": [], "entities": [{"text": "Romanised San-skrit OCR", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.5479715565840403}]}, {"text": "So, we bootstrap a dataset of 430 images, scanned in two different settings and their corresponding ground truth.", "labels": [], "entities": []}, {"text": "For training, we synthetically generate training images for both the settings.", "labels": [], "entities": []}, {"text": "We find that the use of copying mechanism (Gu et al., 2016) yields a percentage increase of 7.69 in Character Recognition Rate (CRR) than the current state of the art model in solving monotone sequence-to-sequence tasks (Schnober et al., 2016).", "labels": [], "entities": [{"text": "Character Recognition Rate (CRR)", "start_pos": 100, "end_pos": 132, "type": "METRIC", "confidence": 0.8219961573680242}]}, {"text": "We find that our system is robust in combating OCR-prone errors, as it obtains a CRR of 87.01% from an OCR output with CRR of 35.76% for one of the dataset settings.", "labels": [], "entities": [{"text": "CRR", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9937998652458191}, {"text": "CRR", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9881205558776855}]}, {"text": "A human judgement survey performed on the models shows that our proposed model results in predictions which are faster to comprehend and faster to improve fora human than the other systems 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Sanskrit used to be the 'lingua franca' for the scientific and philosophical discourse in ancient India with literature that spans more than 3 millennia.", "labels": [], "entities": []}, {"text": "Sanskrit primarily had an oral tradition, and the script used for writing Sanskrit varied widely across the time spans and regions.", "labels": [], "entities": []}, {"text": "With the advent of printing press, Devanagari emerged as the prominent script for representing Sanskrit.", "labels": [], "entities": []}, {"text": "With standardisation of Romanisation using IAST in 1894, printing in Sanskrit was extended to roman scripts as well.", "labels": [], "entities": []}, {"text": "There has been a surge in digitising printed Sanskrit manuscripts written in Roman such as the ones currently digitised by the 'Krishna Path' project 2 . In this work, we propose a model for post-OCR text correction for Sanskrit written in Roman.", "labels": [], "entities": [{"text": "Krishna Path' project", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.7687055319547653}, {"text": "post-OCR text correction", "start_pos": 191, "end_pos": 215, "type": "TASK", "confidence": 0.694110095500946}]}, {"text": "Post-OCR text correction, which can be seen as a special case of spelling correction (, is the task of correcting errors that tend to appear in the output of the OCR in the process of converting an image to text.", "labels": [], "entities": [{"text": "text correction", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.666233479976654}, {"text": "spelling correction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7283025979995728}]}, {"text": "The errors incurred from OCR can be quite high due to numerous factors including typefaces, paper quality, scan quality, etc.", "labels": [], "entities": [{"text": "errors", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9549420475959778}, {"text": "OCR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8319724798202515}]}, {"text": "The text can often be eroded, can contain noises and the paper can be bleached or tainted as well (.", "labels": [], "entities": []}, {"text": "shows the sample images we have collected for the task.", "labels": [], "entities": []}, {"text": "Hence it is beneficial to perform a post-processing on the OCR output to obtain an improved text.", "labels": [], "entities": []}, {"text": "In the case of Indic OCRs, there have been considerable efforts in collection and annotation of data pertaining to Indic Scripts ().", "labels": [], "entities": []}, {"text": "Earlier attempts on Indian scripts were primarily based on handcrafted templates) or features () which extensively used the script and language-specific information ().", "labels": [], "entities": []}, {"text": "Sequential labelling approaches were later proposed that take the word level inputs and make character level predictions (.", "labels": [], "entities": []}, {"text": "The word based sequence labelling approaches were further extended to use neural architectures, especially using RNNs and its variants such as.", "labels": [], "entities": [{"text": "word based sequence labelling", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.5899362117052078}]}, {"text": "But, OCR is putative in exhibiting few long-range dependencies (.", "labels": [], "entities": []}, {"text": "find that extending the neural models to process the text at the sentence level (or a textline) leads to improvement in the performance of the OCR systems.", "labels": [], "entities": []}, {"text": "This was further corroborated by Saluja et al. where the authors found that using words within a context window of 5 fora given input word worked particularly well for the Post-OCR text correction in Sanskrit.", "labels": [], "entities": [{"text": "Post-OCR text correction", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6147606273492178}]}, {"text": "In the case of providing a text line as input, we are essentially providing more context about the input in comparison to the word level models and the RNN (or LSTM) cells are powerful enough to capture the long-term dependencies.", "labels": [], "entities": []}, {"text": "Particularly for Indian languages, this decision is beyond a question of performance.", "labels": [], "entities": []}, {"text": "In Sanskrit, the word boundaries are often obscured due to phonetic transformations at the word boundaries known as Sandhi.", "labels": [], "entities": []}, {"text": "Word segmentation of Sanskrit constructions is a matter of research on its own (.", "labels": [], "entities": [{"text": "Word segmentation of Sanskrit constructions", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8166589319705964}]}, {"text": "However, none of the existing systems are equipped for incorrect spellings and hence these systems maybe brittle when it comes to handling spelling variations in the input.", "labels": [], "entities": []}, {"text": "Hence, in our case, we assume an unsegmented sequence as our input and then we perform our Post-OCR text correction on the text.", "labels": [], "entities": []}, {"text": "We hypothesise that this will improve the segmentation process and other downstream tasks for Sanskrit in atypical NLP pipeline.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9672098159790039}]}, {"text": "Our major contributions are: 1.", "labels": [], "entities": []}, {"text": "Contrary to what is observed in, an encoder-decoder model, when equipped with copying mechanism (, can outperform a traditional sequence labelling model in a monotone sequence labelling task.", "labels": [], "entities": []}, {"text": "Our model outperforms in the Post-OCR text correction for Romanised Sanskrit task by 7.69 % in terms of CRR.", "labels": [], "entities": [{"text": "text correction for Romanised Sanskrit task", "start_pos": 38, "end_pos": 81, "type": "TASK", "confidence": 0.7289838790893555}, {"text": "CRR", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9366974234580994}]}, {"text": "2. By making use of digitised Sanskrit texts, we generate images as synthetic training data for our models.", "labels": [], "entities": []}, {"text": "We systematically incorporate various distortions to those images so as to emulate the settings of the original images.", "labels": [], "entities": []}, {"text": "3. Through a human judgement experiment, we asked the participants to correct the mistakes from a predicted output from the competing systems.", "labels": [], "entities": []}, {"text": "We find that participants were able to correct predictions from our system more frequently and the corrections were done much faster than the CRF model by.", "labels": [], "entities": []}, {"text": "We observe that predictions from our model score high on acceptability () than other methods as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "Sanskrit is a low-resource language.", "labels": [], "entities": []}, {"text": "It is extremely scarce to obtain datasets with scanned images and the corresponding aligned texts for Romanised Sanskrit.", "labels": [], "entities": []}, {"text": "We obtain 430 scanned images as shown in and manually annotate the corresponding text.", "labels": [], "entities": []}, {"text": "We use this as our test dataset, henceforth to be referred to as OCRTest.", "labels": [], "entities": [{"text": "OCRTest", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.883869469165802}]}, {"text": "For training, we synthetically generate images from digitised Sanskrit texts and use them as our training set and development set.", "labels": [], "entities": []}, {"text": "The images for training, OCRTrain, were generated by synthetically adding distortions to those images to match the settings of the real scanned documents.", "labels": [], "entities": [{"text": "OCRTrain", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.7908904552459717}]}, {"text": "OCRTest contains 430 images from 1) scanned copy of Vishnu Saha\u00b4sran\u00afSaha\u00b4sran\u00af ama 8 and 2) scanned copy of Bhagavad G\u00af \u0131t\u00af a, a sample of each is shown in and 1b.", "labels": [], "entities": [{"text": "OCRTest", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9453309774398804}]}, {"text": "140 out of these 430 are from Saha\u00b4sran\u00afSaha\u00b4sran\u00af ama and the remaining are from Bhagavad G\u00af \u0131t\u00af a.", "labels": [], "entities": []}, {"text": "OCRTrain: Similar to Ul-Hasan and Breuel (2013), we synthetically generate the images, which are then fed to the OCR, to obtain our training data.", "labels": [], "entities": [{"text": "OCRTrain", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9268168210983276}]}, {"text": "We use the digitised text from\u00b4Sr\u00affrom\u00b4 from\u00b4Sr\u00af \u0131mad Bh\u00af agavatam 9 for generating the synthetic images.", "labels": [], "entities": []}, {"text": "The text contains about 14,094 verses in total, divided into 50,971 text-lines.", "labels": [], "entities": []}, {"text": "The dataset is divided into 80-20 split as training set and development set, respectively.", "labels": [], "entities": []}, {"text": "The corpus contains a vocabulary of 52,882 word types.", "labels": [], "entities": []}, {"text": "48,249 of the word types in the vocabulary appear less than or equal to 5 times, of which 32,411 appear exactly once.", "labels": [], "entities": []}, {"text": "This is primarily due to the inflectional nature of Sanskrit.", "labels": [], "entities": []}, {"text": "We find similar trends in the vocabulary of R\u00af am\u00af ayan . a 10 and Digital Corpus of Sanskrit (Hellwig, 2010-2016) as well.", "labels": [], "entities": [{"text": "R\u00af am\u00af ayan . a 10", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.8182111606001854}, {"text": "Digital Corpus of Sanskrit (Hellwig, 2010-2016)", "start_pos": 67, "end_pos": 114, "type": "DATASET", "confidence": 0.9404381513595581}]}], "tableCaptions": [{"text": " Table 1: OCR performances for different languages with overall CRR, total Insertion, Deletion and Sub- stitution errors.", "labels": [], "entities": [{"text": "Insertion", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8903458714485168}, {"text": "Sub- stitution errors", "start_pos": 99, "end_pos": 120, "type": "METRIC", "confidence": 0.6900421679019928}]}, {"text": " Table 3: Performance in terms of CRR, WRR and Norm LP (acceptability) for all the competing models", "labels": [], "entities": [{"text": "CRR", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8440801501274109}, {"text": "WRR", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9872491955757141}, {"text": "Norm LP", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9732871651649475}]}, {"text": " Table 4: Performance in terms of  CRR, WRR for Google OCR", "labels": [], "entities": [{"text": "CRR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.777615487575531}, {"text": "WRR", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9905233979225159}]}]}