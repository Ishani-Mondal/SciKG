{"title": [{"text": "ELMoLex: Connecting ELMo and Lexicon features for Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7910174429416656}]}], "abstractContent": [{"text": "In this paper, we present the details of the neural dependency parser and the neu-ral tagger submitted by our team 'ParisNLP' to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies.", "labels": [], "entities": [{"text": "ParisNLP", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.9477494359016418}]}, {"text": "We augment the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features to perform competitively: we utilize an indomain version of ELMo features (Pe-ters et al., 2018) which provide context-dependent word representations; we utilize disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018), which complements the existing feature set.", "labels": [], "entities": []}, {"text": "Henceforth , we call our system 'ELMoLex'.", "labels": [], "entities": []}, {"text": "In addition to incorporating character embed-dings, ELMoLex leverage pre-trained word vectors, ELMo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology.", "labels": [], "entities": []}, {"text": "EL-MoLex 1 ranked 11 th by Labeled Attachment Score metric (70.64%), Morphology-aware LAS metric (55.74%) and ranked 9 th by Bilexical dependency metric (60.70%).", "labels": [], "entities": [{"text": "EL-MoLex 1", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8290720283985138}, {"text": "Labeled Attachment Score metric", "start_pos": 27, "end_pos": 58, "type": "METRIC", "confidence": 0.7824619263410568}]}, {"text": "In an extrinsic evaluation setup, ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7789859771728516}, {"text": "Negation Resolution tasks", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.8109044233957926}, {"text": "Opinion Analysis task", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.8408798575401306}, {"text": "F1 score", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9794519543647766}]}], "introductionContent": [{"text": "The goal of this paper is to describe ELMoLex, the parsing system submitted by our team 'ParisNLP' to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies (.", "labels": [], "entities": [{"text": "ParisNLP", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9424688220024109}, {"text": "parsing from raw text", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.8177940994501114}]}, {"text": "The backbone of ELMoLex is the BiAF parser consisting of a large, well-tuned network that generates word representations, which are then fed to an effective, biaffine classifier to predict the head of each modifier token and the class of the edge connecting these tokens.", "labels": [], "entities": []}, {"text": "In their follow-up work, the authors further enrich the parser by utilizing character embeddings for generating word representations which could help in generalizing to rare and unknown words (also called Out Of Vocabulary (OOV) words).", "labels": [], "entities": []}, {"text": "They also train their own taggers using a similar architecture and use the resulting Part of Speech (PoS) tags for training the parser in an effort to leverage the potential benefits in PoS quality over off-the-shelf taggers.", "labels": [], "entities": []}, {"text": "We identify two potential shortcomings of the BiAF parser.", "labels": [], "entities": []}, {"text": "The first problem is the context independence of the word embedding layer of the parser: the meaning of a word varies across linguistic contexts, which could be hard to infer automatically for smaller treebanks (especially) due to lack of data.", "labels": [], "entities": []}, {"text": "To handle this bottleneck, we propose to use Embeddings from Language Model (ELMo) features which are context dependent (function of the entire input sentence) and obtained from the linear combination of several layers of a pre-trained BiLSTM-LM 2 . The second problem is the linguistic naivety 3 of the character embeddings: they can generalize over relevant sub-parts of each word such as prefixes Due to lack of time, we could train BiLSTM-LMs on the treebank data only (indomain version).", "labels": [], "entities": []}, {"text": "We leave it for future work to train the model on large raw corpora from each language, which we believe could further strengthen our parser.", "labels": [], "entities": []}, {"text": "The term linguistic naivety was introduced by to refer to the fact that character-based embeddings fora sentence must discover that words exist and are delimited by spaces (basic linguistic facts that are builtin to the structure of word-based models).", "labels": [], "entities": []}, {"text": "In our context, we use a different meaning of this term as the term corresponds to the word-level characterbased embeddings.", "labels": [], "entities": []}, {"text": "1 224 BiAF y y x . .", "labels": [], "entities": [{"text": "BiAF", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9811340570449829}]}, {"text": ". or suffixes, which can be problematic for unknown words which do not always follow such generalizations.", "labels": [], "entities": []}, {"text": "We attempt to lift this burden by resorting to external lexicons 4 , which provides information for both word with an irregular morphology and word not present in the training data, without any quantitative distinction between relevant and less relevant information.", "labels": [], "entities": []}, {"text": "To tap the information from the morphological features (such as gender, tense, mood, etc.) for each word present in the lexicon efficiently, we propose to embed the features and disambiguate them contextually with the help of attention (), before combining them for the focal word.", "labels": [], "entities": []}, {"text": "We showcase the potential of ELMoLex in parsing 82 treebanks provided by the shared task.", "labels": [], "entities": []}, {"text": "ELMoLex ranked 11 th by Labeled Attachment Score (LAS) metric (70.64%), Morphology-aware LAS (MLAS) metric (55.74%) and ranked 9 th by BiLEXical dependency (BLEX) metric (60.70%).", "labels": [], "entities": [{"text": "ELMoLex", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9116079211235046}, {"text": "Labeled Attachment Score (LAS) metric", "start_pos": 24, "end_pos": 61, "type": "METRIC", "confidence": 0.7900932899543217}, {"text": "BiLEXical dependency (BLEX) metric", "start_pos": 135, "end_pos": 169, "type": "METRIC", "confidence": 0.519299790263176}]}, {"text": "We perform ablation and training time studies to have a deeper understanding of ELMoLex.", "labels": [], "entities": []}, {"text": "In an extrinsic evaluation setup), ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.782614678144455}, {"text": "Negation Resolution tasks", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.8069984912872314}, {"text": "Opinion Analysis task", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.8359992901484171}, {"text": "F1 score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9795930683612823}]}, {"text": "On an average, ELMoLex ranked 8 th with a F1 score of 55.48%.", "labels": [], "entities": [{"text": "ELMoLex", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.8240415453910828}, {"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9874260127544403}]}], "datasetContent": [{"text": "A \"good\" parser should not only perform well in the intrinsic metrics such as LAS, BLAS and BLEX, but should strengthen areal world NLP system by providing relevant syntactic features.", "labels": [], "entities": [{"text": "LAS", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8955866098403931}, {"text": "BLAS", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9283196330070496}, {"text": "BLEX", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9941272735595703}]}, {"text": "To understand the impact of ELMoLex in a downstream NLP application, we participiated in the shared task on Extrinsic Parser Evaluation (.", "labels": [], "entities": [{"text": "Extrinsic Parser Evaluation", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6168510218461355}]}, {"text": "The goal of this task is to evaluate the parse trees predicted by ELMoLex on three downstream applications: biological event extraction, fine-grained opinion analysis, and negation resolution, for its usefulness.", "labels": [], "entities": [{"text": "biological event extraction", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.7843960523605347}, {"text": "fine-grained opinion analysis", "start_pos": 137, "end_pos": 166, "type": "TASK", "confidence": 0.5401807924111685}, {"text": "negation resolution", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.9909961521625519}]}, {"text": "Since all the tasks are based on English language, we train ELMoLex on en_ewt treebank (which is the largest English treebank provided by the organizers) without changing the hyper-parameters (as disclosed in Appendix A).", "labels": [], "entities": []}, {"text": "We refer the readers to for details about each of the downstream task and the accompanying system (which takes the features dervied from ELMoLex) used to solve the task.", "labels": [], "entities": []}, {"text": "Our extrinsic evaluation results are displayed in Table 5.", "labels": [], "entities": []}, {"text": "ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.", "labels": [], "entities": [{"text": "ELMoLex", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9027495384216309}, {"text": "Event Extraction", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8085039556026459}, {"text": "Negation Resolution tasks", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8309129277865092}, {"text": "Opinion Analysis task", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.864037275314331}, {"text": "F1 score", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9799172878265381}]}, {"text": "On an average, ELMoLex: Results on the downstream tasks for our ELMoLex system trained on the en_ewt treebank with the corresponding difference from the best system enclosed in ellipses.", "labels": [], "entities": []}, {"text": "ranked 8 th with a F1 score of 55.48%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9895316362380981}]}], "tableCaptions": [{"text": " Table 2: UPoS F1 on Dev. datasets (used as test)", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.533148467540741}, {"text": "Dev. datasets", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8722715377807617}]}, {"text": " Table 3: Ablation study of ELMoLex. LAS dev. score along with training time (with 90% of the training data with the rest used for  selecting the best model) in minutes is reported for selected treebanks. For NLM Init. and ELMoLex models, we report the time taken  to train the parser (excluding the time taken to train the underlying BiLSTM-LM). All the reported models uses Chu-Liu-Edmonds  algorithm (Chu and Liu, 1967) for constructing the final parse tree.", "labels": [], "entities": [{"text": "LAS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9487438201904297}]}, {"text": " Table 4: Comparing LAS scores for 2017 and 2018 participation  of ParisNLP. The increase in the absolute LAS points are enclosed  in the ellipses.", "labels": [], "entities": [{"text": "LAS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8838133811950684}, {"text": "ParisNLP", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9160467386245728}, {"text": "absolute LAS points", "start_pos": 97, "end_pos": 116, "type": "METRIC", "confidence": 0.7812226215998331}]}, {"text": " Table 6: Hyper-parameters for all the models used in the experiments.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9948779344558716}]}, {"text": " Table 8: Performance of ELMOLEX trained and tested using tags from the neural tagger (with the corresponding absolute difference  from our submission final results) and average absolute gain of using the neural tagger compared to tags used at the submissions  **indicates datasets for which tags from the neural tagger where used at test time for the submission", "labels": [], "entities": []}]}