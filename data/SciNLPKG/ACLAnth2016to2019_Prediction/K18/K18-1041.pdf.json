{"title": [{"text": "Resources to Examine the Quality of Word Embedding Models Trained on n-Gram Dat\u00e1 Dat\u00e1", "labels": [], "entities": [{"text": "n-Gram Dat\u00e1 Dat\u00e1", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.819474478562673}]}], "abstractContent": [{"text": "Word embeddings are powerful tools that facilitate better analysis of natural language.", "labels": [], "entities": []}, {"text": "However, their quality highly depends on the resource used for training.", "labels": [], "entities": []}, {"text": "There are various approaches relying on n-gram corpora, such as the Google n-gram corpus.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.8645966649055481}]}, {"text": "However, n-gram corpora only offer a small window into the full text-5 words for the Google corpus at best.", "labels": [], "entities": [{"text": "Google corpus", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9000606834888458}]}, {"text": "This gives way to the concern whether the extracted word semantics are of high quality.", "labels": [], "entities": []}, {"text": "In this paper, we address this concern with two contributions.", "labels": [], "entities": []}, {"text": "First, we provide a resource containing 120 word-embedding models-one of the largest collection of embedding models.", "labels": [], "entities": []}, {"text": "Furthermore, the resource contains the n-gramed versions of all used corpora, as well as our scripts used for corpus generation, model generation and evaluation.", "labels": [], "entities": [{"text": "corpus generation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7235284298658371}, {"text": "model generation", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.7232199758291245}]}, {"text": "Second, we define a set of meaningful experiments allowing to evaluate the aforementioned quality differences.", "labels": [], "entities": []}, {"text": "We conduct these experiments using our resource to show its usage and significance.", "labels": [], "entities": []}, {"text": "The evaluation results confirm that one generally can expect high quality for n-grams with n \u2265 3.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding approaches like Word2Vec () or Glove) are powerful tools for the semantic analysis of natural language.", "labels": [], "entities": [{"text": "semantic analysis of natural language", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.8018841862678527}]}, {"text": "One can train them on arbitrary text corpora.", "labels": [], "entities": []}, {"text": "Each word in the corpus is mapped to a d-dimensional vector.", "labels": [], "entities": []}, {"text": "These vectors feature the semantic similarity and analogy properties, as follows.", "labels": [], "entities": []}, {"text": "Semantic similarity means that representations of words used in a similar context tend to be close to each other in the vector space.", "labels": [], "entities": []}, {"text": "The analogy property can be described by the example that \"man\" is to \"woman\" like \"king\" to \"queen\" (.", "labels": [], "entities": []}, {"text": "These properties have been applied in numerous approaches) like sentiment analysis (), irony detection (, out-of-vocabulary word classification (, or semantic shift detection (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9702144265174866}, {"text": "irony detection", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.9277035295963287}, {"text": "word classification", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7291585206985474}, {"text": "semantic shift detection", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.7565612097581228}]}, {"text": "One prerequisite when creating high-quality embedding models is a good training corpus.", "labels": [], "entities": []}, {"text": "To this end, many approaches use the Google n-gram corpus).", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.7893795768419901}]}, {"text": "It also is the largest currently available corpus with historic data and exists for several languages.", "labels": [], "entities": []}, {"text": "It incorporates over 5 million books from the last centuries split into ngrams (.", "labels": [], "entities": []}, {"text": "n-grams are text segments separated into pieces consisting of n words each.", "labels": [], "entities": []}, {"text": "The fragmentation of a corpus is the size of its n-grams.", "labels": [], "entities": []}, {"text": "To illustrate, a corpus of 2-grams is highly fragmented, one of 5-grams is moderately fragmented.", "labels": [], "entities": []}, {"text": "n-gram counts overtime can be published even if the underlying full text is subject to copyright protection.", "labels": [], "entities": []}, {"text": "Next, this format reduces the data volume very much.", "labels": [], "entities": []}, {"text": "So it is important to know how good models built on n-gram corpora are.", "labels": [], "entities": []}, {"text": "While the quality of word embedding models trained on full-text corpora is fairly well known (, an assessment of models built on fragmented corpora is missing ().", "labels": [], "entities": []}, {"text": "The resource advertised in this paper is a set of such models, which should help to shed some light on the issue, together with some experiments.", "labels": [], "entities": []}, {"text": "An obvious benefit of making these models available is the huge runtime necessary to build them.", "labels": [], "entities": []}, {"text": "However, evaluating them is not straightforward, for various reasons.", "labels": [], "entities": []}, {"text": "First, drawing general conclusions on the quality of em-bedding models only based on the performance of specific approaches, i.e., examining the extrinsic suitability of models, is error-prone (.", "labels": [], "entities": []}, {"text": "Consequently, to come to general conclusions one needs to investigate general properties of the embedding models itself, i.e., examine their intrinsic suitability.", "labels": [], "entities": []}, {"text": "Properties of this kind are semantic similarity and analogy.", "labels": [], "entities": []}, {"text": "For both properties, one can use well-known test sets that serve as comprehensive baselines.", "labels": [], "entities": []}, {"text": "Second, there are various parameters which influence how the model looks like.", "labels": [], "entities": []}, {"text": "Using n-grams as training corpus gives way to two new parameters, fragmentation, as just discussed, and minimum count, i.e., the minimum occurrence count of an n-gram in order to be considered when building the model.", "labels": [], "entities": [{"text": "minimum count", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.8926521241664886}]}, {"text": "The latter is often used to filter error-prone n-grams from a corpus, e.g., spelling errors.", "labels": [], "entities": []}, {"text": "While the effect of the other parameters on the models is known (, the one of these new parameters is not.", "labels": [], "entities": []}, {"text": "We have to define meaningful experiments to quantify and compare the effects.", "labels": [], "entities": []}, {"text": "Third, the full text, such as the Google Books corpus, is not openly available as reference in many cases.", "labels": [], "entities": [{"text": "Google Books corpus", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9631783763567606}]}, {"text": "Hence, we need to examine how to compare results from other corpora, where the full text is available, referring e.g., to well-known baselines as the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.9320653975009918}]}, {"text": "The resource provided here is a systematic collection of word embedding models trained on n-gram corpora, accessible at our project website 1 . The collection consists of 120 word embedding models trained on the Wikipedia and 1 Billion words data set.", "labels": [], "entities": [{"text": "1 Billion words data set", "start_pos": 226, "end_pos": 250, "type": "DATASET", "confidence": 0.705957543849945}]}, {"text": "Its training has required more than two months of computing time on a modern machine.", "labels": [], "entities": []}, {"text": "To our knowledge, it currently is one of the most comprehensive collection of its type.", "labels": [], "entities": []}, {"text": "In order to make this resource re-usable and our experiments repeatable, we also provide the n-grammed versions of the Wikipedia and 1-Billion word datasets, which we used for training and the tools to create n-gram corpora from arbitrary text as well.", "labels": [], "entities": [{"text": "Wikipedia and 1-Billion word datasets", "start_pos": 119, "end_pos": 156, "type": "DATASET", "confidence": 0.6237466275691986}]}, {"text": "In addition, we describe some experiments to examine how much model quality changes when the training corpus is not full-text, but n-grams.", "labels": [], "entities": []}, {"text": "The experiments quantify how much fragmentation (i.e., values of n) and minimum count reduce the average quality of the corresponding word em-bedding model, on common word similarity and analogical reasoning test sets.", "labels": [], "entities": []}, {"text": "To show the usefulness and significance of the experiments and to give general recommendations on which n-gram corpus to use as well as creating a baseline for comparison, we conduct the experiments on the full English Wikipedia dump and Chelba et al.'s 1-Billion word dataset.", "labels": [], "entities": [{"text": "English Wikipedia dump and Chelba et al.'s 1-Billion word dataset", "start_pos": 211, "end_pos": 276, "type": "DATASET", "confidence": 0.6324172243475914}]}, {"text": "However, we recommend to conduct this examination for any corpus before using it as training resource, particularly if the corpus size differs from the ones of the baseline corpora by much: 1.", "labels": [], "entities": []}, {"text": "What is the smallest number n for which an n-gram corpus is good training data for word embedding models?", "labels": [], "entities": []}, {"text": "2. How sensitive is the quality of the models to the fragmentation and the minimum count parameter?", "labels": [], "entities": []}, {"text": "3. What is the actual reason for any quality loss of models trained with high fragmentation or a high minimum count parameter?", "labels": [], "entities": []}, {"text": "Our results for the baseline test sets indicate that minimum count values exceeding a corpus-sizedependent threshold drastically reduce the quality of the models.", "labels": [], "entities": []}, {"text": "Fragmentation in turn brings down the quality only if the fragments are very small.", "labels": [], "entities": [{"text": "quality", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9866414070129395}]}, {"text": "Based on this, one can conclude that n-gram corpora such as Google Books are valid training data for word embedding models.", "labels": [], "entities": [{"text": "Google Books", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9315474331378937}]}], "datasetContent": [{"text": "The experiments allow to to give general recommendations on which n-gram corpus to use to train word embedding models.", "labels": [], "entities": []}, {"text": "In order to do this, we justify the selection of our text corpora and of the parameter values used.", "labels": [], "entities": []}, {"text": "Finally, we explain the actual baseline test sets and say why this choice will yield general insights.", "labels": [], "entities": []}, {"text": "In order to make our experimental results more intuitive we attempt to explicitly answer the three following questions.", "labels": [], "entities": []}, {"text": "What is the smallest number n for which an n-gram corpus is good for the training of embedding models?", "labels": [], "entities": []}, {"text": "The size of any ngram corpus highly increases with large n.", "labels": [], "entities": []}, {"text": "Hence, it is important to know the smallest value that is expected to still yield good results.", "labels": [], "entities": []}, {"text": "How does the minimum count parameter affect the quality of the models?", "labels": [], "entities": []}, {"text": "How does this result compare to the effect caused by the fragmentation?", "labels": [], "entities": []}, {"text": "Having answered the first question, we will be able to quantify the effect of the fragmentation.", "labels": [], "entities": []}, {"text": "However, it is necessary to study the effect of the second parameter as well, in order to quantify the applicability of ngrams for embedding comprehensively.", "labels": [], "entities": []}, {"text": "In other words, we want to compare the effects of both parameters; we will be able to give recommendations for both parameters.", "labels": [], "entities": []}, {"text": "How does the quality loss of models trained on fragmented corpora of size nor with high minimum count parameter manifest itself in the embedding models?", "labels": [], "entities": []}, {"text": "By answering Questions 1 and 2, we are able to quantify the effect of both parameters.", "labels": [], "entities": []}, {"text": "We hypothesize that the parameters affect the quality of the models differently, and that we are able to observe this in the word vectors themselves.", "labels": [], "entities": []}, {"text": "The rationale behind our hypothesis is that large minimum count values might eliminate various meaningful words from the vector space.", "labels": [], "entities": []}, {"text": "Fragmentation in isolation however does not have the effect that a word is lost.", "labels": [], "entities": []}, {"text": "Hence, a quality loss must manifest itself differently, which might be observable.", "labels": [], "entities": []}, {"text": "We now turn to the questions just asked.", "labels": [], "entities": []}, {"text": "We dedicate a subsection to each question.", "labels": [], "entities": []}, {"text": "In the first two sections, we give an overview of the results using the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9814395606517792}]}, {"text": "The results for the 1-Billion word corpus are almost identical.", "labels": [], "entities": []}, {"text": "For brevity, we do not show all results for this corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average quality loss due to fragmentation  compared to the full text on the Wikipedia corpus.", "labels": [], "entities": [{"text": "Average quality loss", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7510560353597006}, {"text": "Wikipedia corpus", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.9031272828578949}]}, {"text": " Table 2: Average quality loss due to fragmentation  compared to the full text on the 1-Billion word corpus.", "labels": [], "entities": []}, {"text": " Table 3: Average quality loss caused by the minimum  count parameter parameter on Wikipedia.", "labels": [], "entities": [{"text": "Average quality loss", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9134019811948141}, {"text": "Wikipedia", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.9269086718559265}]}, {"text": " Table 4: Average movement in cosine distance of the  word vectors with one extra iteration.", "labels": [], "entities": []}]}