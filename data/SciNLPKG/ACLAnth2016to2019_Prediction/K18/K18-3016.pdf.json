{"title": [{"text": "BME-HAS System for CoNLL-SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection", "labels": [], "entities": [{"text": "BME-HAS", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.524303674697876}, {"text": "Universal Morphological Reinflection", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.5437136391798655}]}], "abstractContent": [{"text": "This paper presents an encoder-decoder neu-ral network based solution for both subtasks of the CoNLL-SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 Shared Task", "start_pos": 95, "end_pos": 128, "type": "DATASET", "confidence": 0.8075514733791351}, {"text": "Universal Morphological Reinflection", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.46057748794555664}]}, {"text": "All of our models are sequence-to-sequence neural networks with multiple encoders and a single decoder.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological inflection is the task of inflecting a lemma given either a target form or some contextual information.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8813015222549438}]}, {"text": "Morphology has traditionally been solved by finite state transducers (FST) that employ a large number of handcrafted rules.", "labels": [], "entities": [{"text": "Morphology", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9792121648788452}, {"text": "finite state transducers (FST)", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6809961398442587}]}, {"text": "The discrete nature of such processes makes it difficult to directly translate transducers into neural networks and to effectively train them using backpropagation.", "labels": [], "entities": []}, {"text": "There have been various attempts to replace parts of the FST paradigm with neural networks (.", "labels": [], "entities": [{"text": "FST paradigm", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.7666651606559753}]}, {"text": "SIGMORPHON first organized a shared task on morphological inflection in 2016 () which involved both inflection (inflect a word given its lemma) and reinflection (inflect a word given another inflected form of the same lemma).", "labels": [], "entities": []}, {"text": "The winning solution () used a character sequence-to-sequence network with Bahdanau's attention ().", "labels": [], "entities": []}, {"text": "In the second edition of the shared task () most teams used similar settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our experiments for Task2 were significantly slower than the ones for Task1, we were unable to run extensive parameter search.", "labels": [], "entities": []}, {"text": "We did perform a smaller version of the same random search using the parameter ranges listed in.", "labels": [], "entities": []}, {"text": "We chose the French dataset with medium setting, which is about 10 000 tokens.", "labels": [], "entities": [{"text": "French dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9791125953197479}]}, {"text": "The average length of one experiment was 100 minutes and we were able to run 38 experiments.", "labels": [], "entities": []}, {"text": "We ran the best configuration of the 38 on each language and each data size at least once.", "labels": [], "entities": []}, {"text": "Since our parameter search was very limited, we also varied the parameters manually and tried other combinations.", "labels": [], "entities": []}, {"text": "The exact configurations are available on the GitHub repository.", "labels": [], "entities": [{"text": "GitHub repository", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.934934675693512}]}, {"text": "All experiments were run on NVIDIA GTX TITAN X GPUs (12GB), since they did not fit into the memory of the smaller cards (4GB).", "labels": [], "entities": []}, {"text": "Task2 uses a subset of the parameters that Task1 uses, so we were able to train the \"same\" configuration emerged as the best one during the limited hyperparameter search.", "labels": [], "entities": []}, {"text": "We also tried using 2 layers instead of 1 layer in every encoder and decoder.", "labels": [], "entities": []}, {"text": "Unfortunately time constraints did not allow running more experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of the parameter search. The run- ning time is given in minutes.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy statistics of 20 models trained with  the same parameters but different random seed.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9934019446372986}]}, {"text": " Table 5: The mean accuracy of our Task1 submissions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9594652056694031}]}, {"text": " Table 6. We chose  the French dataset with medium setting, which is  about 10 000 tokens. The average length of one  experiment was 100 minutes and we were able to  run 38 experiments. We ran the best configuration  of the 38 on each language and each data size at  least once. Since our parameter search was very  limited, we also varied the parameters manually  and tried other combinations. The exact configu- rations are available on the GitHub repository. All  experiments were run on NVIDIA GTX TITAN X  GPUs (12GB), since they did not fit into the mem- ory of the smaller cards (4GB).", "labels": [], "entities": [{"text": "French dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.932650089263916}, {"text": "NVIDIA GTX TITAN X  GPUs", "start_pos": 491, "end_pos": 515, "type": "DATASET", "confidence": 0.7331165075302124}]}]}