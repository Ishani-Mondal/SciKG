{"title": [{"text": "An improved neural network model for joint POS tagging and dependency parsing", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8653481602668762}, {"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7135805934667587}]}], "abstractContent": [{"text": "We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "joint part-of-speech (POS) tagging", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.5947225540876389}, {"text": "dependency parsing", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7574675977230072}]}, {"text": "Our model extends the well-known BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the parser.", "labels": [], "entities": [{"text": "BIST graph-based dependency parser", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.6609492599964142}]}, {"text": "On the benchmark English Penn treebank, our model obtains strong UAS and LAS scores at 94.51% and 92.87%, respectively, producing 1.5+% absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97%.", "labels": [], "entities": [{"text": "English Penn treebank", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.9507733583450317}, {"text": "UAS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9314806461334229}, {"text": "LAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9628819227218628}, {"text": "BIST", "start_pos": 165, "end_pos": 169, "type": "DATASET", "confidence": 0.6327610611915588}, {"text": "POS tagging", "start_pos": 228, "end_pos": 239, "type": "TASK", "confidence": 0.6075258553028107}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9158158898353577}]}, {"text": "Furthermore, experimental results on parsing 61 \"big\" Universal Dependencies treebanks from raw texts show that our model outperforms the base-line UDPipe (Straka and Strakov\u00e1, 2017) with 0.8% higher average POS tagging score and 3.6% higher average LAS score.", "labels": [], "entities": [{"text": "Universal Dependencies treebanks from raw texts", "start_pos": 54, "end_pos": 101, "type": "DATASET", "confidence": 0.7066127061843872}, {"text": "POS tagging", "start_pos": 208, "end_pos": 219, "type": "TASK", "confidence": 0.6694402992725372}, {"text": "LAS score", "start_pos": 250, "end_pos": 259, "type": "METRIC", "confidence": 0.9569771885871887}]}, {"text": "In addition, with our model, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6958121061325073}, {"text": "opinion analysis", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.6906588673591614}]}, {"text": "Our code is available together with all pre-trained models at: https://github.", "labels": [], "entities": []}, {"text": "com/datquocnguyen/jPTDP.", "labels": [], "entities": [{"text": "jPTDP", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.9375063180923462}]}], "introductionContent": [{"text": "Dependency parsing -a key research topic in natural language processing (NLP) in the last decade) -has also been demonstrated to be extremely useful in many applications such as relation extraction (), semantic parsing ( and machine translation.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8605084121227264}, {"text": "natural language processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7614980836709341}, {"text": "relation extraction", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.8402862250804901}, {"text": "semantic parsing", "start_pos": 202, "end_pos": 218, "type": "TASK", "confidence": 0.8078820109367371}, {"text": "machine translation", "start_pos": 225, "end_pos": 244, "type": "TASK", "confidence": 0.8240246772766113}]}, {"text": "In general, dependency parsing models can be categorized as graph-based () and transition-based.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.844941258430481}]}, {"text": "Most traditional graph-or transition-based models define a set of core and combined features, while recent stateof-the-art models propose neural network architectures to handle feature-engineering (.", "labels": [], "entities": []}, {"text": "Most traditional and neural network-based parsing models use automatically predicted POS tags as essential features.", "labels": [], "entities": []}, {"text": "However, POS taggers are not perfect, resulting in error propagation problems.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.8169224858283997}]}, {"text": "Some work has attempted to avoid using POS tags for dependency parsing, however, to achieve the strongest parsing scores these methods still require automatically assigned POS tags.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8258885443210602}]}, {"text": "Alternatively, joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8047733008861542}, {"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7417709529399872}]}, {"text": "In this paper, we present a novel neural network-based model for jointly learning POS tagging and dependency paring.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.7939565479755402}, {"text": "dependency paring", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6776937246322632}]}, {"text": "Our joint model extends the well-known BIST graph-based dependency parser) with an additional lower-level BiLSTM-based tagging component.", "labels": [], "entities": [{"text": "BIST graph-based dependency parser", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.733765721321106}]}, {"text": "duces a 1.5+% absolute improvement over the BIST graph-based parser with a strong UAS score of 94.51% and LAS score of 92.87%; and also obtaining a state-of-the-art POS tagging accuracy of 97.97%.", "labels": [], "entities": [{"text": "BIST graph-based parser", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.8363468249638876}, {"text": "UAS score", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9659359157085419}, {"text": "LAS score", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9914602041244507}, {"text": "POS tagging", "start_pos": 165, "end_pos": 176, "type": "TASK", "confidence": 0.6377738118171692}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9365198612213135}]}, {"text": "In addition, multilingual parsing experiments from raw texts on 61 \"big\" Universal Dependencies treebanks show that our model outperforms the baseline UDPipe () with 0.8% higher average POS tagging score, 3.1% higher UAS and 3.6% higher LAS.", "labels": [], "entities": [{"text": "Universal Dependencies treebanks", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.7148926357428232}, {"text": "POS tagging", "start_pos": 186, "end_pos": 197, "type": "TASK", "confidence": 0.6902067065238953}, {"text": "UAS", "start_pos": 217, "end_pos": 220, "type": "METRIC", "confidence": 0.9855426549911499}, {"text": "LAS", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.984086811542511}]}, {"text": "Furthermore, experimental results on downstream task applications show that our joint model helps produce state-of-the-art scores for biomedical event extraction and opinion analysis.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6991693675518036}, {"text": "opinion analysis", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.7467051148414612}]}], "datasetContent": [{"text": "Experimental setup: We evaluate our model using the English WSJ Penn treebank).", "labels": [], "entities": [{"text": "English WSJ Penn treebank", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.8672003149986267}]}, {"text": "We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test, employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (.", "labels": [], "entities": []}, {"text": "Word embeddings are initialized by 100-dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword ().", "labels": [], "entities": []}, {"text": "As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256-dimensional LSTM hidden states (in, we present scores obtained on the development set when using 2 BiLSTM layers).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9840857982635498}]}, {"text": "Main results: compares our UAS and LAS scores on the test set with previous published results in terms of the dependency annotations.", "labels": [], "entities": [{"text": "UAS", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.5884850025177002}, {"text": "LAS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8575193881988525}]}, {"text": "3: Results on the development set.", "labels": [], "entities": []}, {"text": "#states and \"Without pun.\" denote the size of LSTM hidden states and the scores computed without punctuations, respectively.", "labels": [], "entities": []}, {"text": "\"POS\" indicates the POS tagging accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.986080527305603}, {"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.520239993929863}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.921062171459198}]}, {"text": "The first 11 rows present scores of dependency parsers in which POS tags were predicted by using an external POS tagger such as the Stanford tagger ().", "labels": [], "entities": []}, {"text": "The last 6 rows present scores for joint models.", "labels": [], "entities": []}, {"text": "Clearly, our model produces very competitive parsing results.", "labels": [], "entities": []}, {"text": "In particular, our model obtains a UAS score at 94.51% and a LAS score at 92.87% which are about 1.4% and 1.9% absolute higher than UAS and LAS scores of the BIST graph-based model, respectively.", "labels": [], "entities": [{"text": "UAS score", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9647420048713684}, {"text": "LAS score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.983079731464386}, {"text": "UAS", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.8575319051742554}, {"text": "BIST", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.8707444071769714}]}, {"text": "Our model also does better than the previous transition-based joint models in , and, while obtaining similar UAS and LAS scores to the joint model JMT proposed by.", "labels": [], "entities": [{"text": "UAS", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9472197890281677}, {"text": "LAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9345517158508301}]}, {"text": "We achieve 0.9% lower parsing scores than the state-of-the-art dependency parser of.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9598931670188904}]}, {"text": "While also a BiLSTM-and graph-based model, it uses a more sophisticated attention mechanism \"biaffine\" for better decoding dependency arcs and relation types.", "labels": [], "entities": []}, {"text": "In future work, we will extend our model with the biaffine attention mechanism to investigate the benefit for our model.", "labels": [], "entities": []}, {"text": "Other differences are that they use a higher dimensional representation than ours, but rely on predicted POS tags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the development set. #states  and \"Without pun.\" denote the size of LSTM hid- den states and the scores computed without punc- tuations, respectively. \"POS\" indicates the POS  tagging accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.98286372423172}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.8691496849060059}]}, {"text": " Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [\u2022]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [\u2022] and [], obtained parsing  scores are just for reference, not for comparison.", "labels": [], "entities": [{"text": "LAS", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9197925329208374}]}, {"text": " Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9332577586174011}, {"text": "UD parsing from raw texts", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.889993953704834}]}, {"text": " Table 4: UPOS, UAS and LAS scores computed on all tokens of our jPTDP v2.0 model regarding gold- standard segmentation on 73 CoNLL-2018 shared task test sets \"Big\", \"PUD\" and \"Small\" -UD v2.", "labels": [], "entities": [{"text": "UPOS", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6401434540748596}, {"text": "UAS", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9092453718185425}, {"text": "LAS", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9835312366485596}, {"text": "CoNLL-2018 shared task test sets", "start_pos": 126, "end_pos": 158, "type": "DATASET", "confidence": 0.8304920077323914}]}, {"text": " Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE", "labels": [], "entities": [{"text": "Downstream task scores Precision (Prec.)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.8400613835879734}, {"text": "Recall (Rec.)", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9204398542642593}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9993379712104797}, {"text": "F1", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9894368648529053}, {"text": "EPE", "start_pos": 197, "end_pos": 200, "type": "DATASET", "confidence": 0.8650901317596436}]}]}