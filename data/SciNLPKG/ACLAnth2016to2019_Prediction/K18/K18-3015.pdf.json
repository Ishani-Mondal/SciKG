{"title": [{"text": "Combining Neural and Non-Neural Methods for Low-Resource Morphological Reinflection", "labels": [], "entities": [{"text": "Low-Resource Morphological Reinflection", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.6528286933898926}]}], "abstractContent": [{"text": "We describe our systems and results in the type-level low-resource setting of the CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection", "start_pos": 82, "end_pos": 155, "type": "TASK", "confidence": 0.7039419412612915}]}, {"text": "We test non-neural transduction models, as well as more recent neural methods.", "labels": [], "entities": []}, {"text": "We also investigate the effect of leveraging unannotated corpora to improve the performance of selected methods.", "labels": [], "entities": []}, {"text": "Our best system obtains the highest accuracy on 34 out of 103 languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9993706345558167}]}], "introductionContent": [{"text": "In this system paper, we discuss our submissions to the CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "Universal Morphological Reinflection", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.5799846649169922}]}, {"text": "We focus on the sub-task of type-level inflection under the low-resource scenario, in which the training data is limited to 100 labelled examples.", "labels": [], "entities": []}, {"text": "Because of the sheer number of tested languages, we attempted no languagespecific modifications.", "labels": [], "entities": []}, {"text": "The results demonstrate that our non-neural transduction models perform better on average than our neural models.", "labels": [], "entities": []}, {"text": "However, combining neural and non-neural models yields the highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9986829161643982}]}, {"text": "In addition to standard submissions, we test novel methods of leveraging additional monolingual corpora, from which we derive target language models and/or word lists.", "labels": [], "entities": []}, {"text": "We show that substantial gains inaccuracy can be obtained in the way.", "labels": [], "entities": []}, {"text": "Again, a combination of neural and nonneural systems produces the best non-standard results.", "labels": [], "entities": []}, {"text": "The paper has the following structure.", "labels": [], "entities": []}, {"text": "In Section 2, we describe four standard systems, as well as our weighted-voting method of combining them.", "labels": [], "entities": []}, {"text": "Our two non-standard systems and their linear combination are introduced in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 discusses the results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The average accuracy across all languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.983311653137207}]}, {"text": " Table 3: The average accuracy of DTLM on the devel- opment sets of 46 languages with additional data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9991703033447266}]}]}