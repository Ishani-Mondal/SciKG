{"title": [], "abstractContent": [{"text": "This paper describes Stanford's system at the CoNLL 2018 UD Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.8617819547653198}]}, {"text": "We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.6704908758401871}, {"text": "POS tagging", "start_pos": 185, "end_pos": 196, "type": "TASK", "confidence": 0.8810356557369232}, {"text": "dependency parsing", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.8151210248470306}]}, {"text": "Our single system submission achieved very competitive performance on big treebanks.", "labels": [], "entities": []}, {"text": "Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have out-performed all submission systems on low-resource treebank categories on all metrics by a large margin.", "labels": [], "entities": [{"text": "corrected", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9622429609298706}, {"text": "MLAS", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.7398828864097595}, {"text": "BLEX", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9941121935844421}]}, {"text": "We further show the effectiveness of different model components through extensive ablation studies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling, relation extraction (, and machine translation (.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8208884298801422}, {"text": "semantic role labeling", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.6704906423886617}, {"text": "relation extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8634935319423676}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7756291329860687}]}, {"text": "However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7762329876422882}]}, {"text": "In reality, however, these upstream systems are still far from perfect.", "labels": [], "entities": []}, {"text": "To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task (Zeman et al., * These authors contributed roughly equally. 2018).", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8018798589706421}]}, {"text": "Harnessing the power of neural systems, this pipeline achieves competitive performance in each of the inter-linked stages: tokenization, sentence and word segmentation, partof-speech (POS)/morphological features (UFeats) tagging, lemmatization, and finally, dependency parsing.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 123, "end_pos": 135, "type": "TASK", "confidence": 0.9621884226799011}, {"text": "sentence and word segmentation", "start_pos": 137, "end_pos": 167, "type": "TASK", "confidence": 0.5866128280758858}, {"text": "partof-speech (POS)/morphological features (UFeats) tagging", "start_pos": 169, "end_pos": 228, "type": "TASK", "confidence": 0.5546614199876785}, {"text": "dependency parsing", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.8523721098899841}]}, {"text": "Our main contributions include: \u2022 New methods for combining symbolic statistical knowledge with flexible, powerful neural systems to improve robustness; \u2022 A biaffine classifier for joint POS/UFeats prediction that improves prediction consistency; \u2022 A lemmatizer enhanced with an edit classifier that improves the robustness of a sequenceto-sequence model on rare sequences; and \u2022 Extensions to our parser from) to model linearization.", "labels": [], "entities": [{"text": "POS/UFeats prediction", "start_pos": 187, "end_pos": 208, "type": "TASK", "confidence": 0.5134237557649612}]}, {"text": "Our system achieves competitive performance on big treebanks.", "labels": [], "entities": []}, {"text": "After fixing an unfortunate bug, the corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on low-resource treebank categories on all metrics by a large margin.", "labels": [], "entities": [{"text": "MLAS", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.711519718170166}, {"text": "BLEX", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.995425283908844}]}, {"text": "We perform extensive ablation studies to demonstrate the effectiveness of our novel methods, and highlight future directions to improve the system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation results (F 1 ) on the test set, on all treebanks and big treebanks only. For each set of  results on all metrics, we compare it against results from reference systems. A reference system is the top  performing system on that metric if we are not top, or the second-best performing system on that metric.  Reference systems are identified by superscripts ( \u2020: HIT-SCIR,  \u2021: Uppsala, : TurkuNLP,  * : UDPipe  Future). Shaded columns in the table indicate the three official evaluation metrics. \"Stanford+\" is our  system after a bugfix evaluated unofficially; for more details please see the main text.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9423587918281555}, {"text": "HIT-SCIR", "start_pos": 380, "end_pos": 388, "type": "METRIC", "confidence": 0.7995793223381042}, {"text": "Uppsala", "start_pos": 394, "end_pos": 401, "type": "DATASET", "confidence": 0.9858847260475159}, {"text": "TurkuNLP", "start_pos": 405, "end_pos": 413, "type": "DATASET", "confidence": 0.8605855107307434}, {"text": "UDPipe  Future", "start_pos": 420, "end_pos": 434, "type": "DATASET", "confidence": 0.8335285782814026}]}, {"text": " Table 2: Evaluation results (F 1 ) on low-resource  treebank test sets. Reference systems are identi- fied by symbol superscripts ( \u2020: HIT-SCIR,  \u2021: ICS  PAS, : CUNI x-ling,  * : Stanford, \u2022: TurkuNLP).", "labels": [], "entities": [{"text": "HIT-SCIR", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8620405793190002}, {"text": "ICS  PAS", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.48847393691539764}, {"text": "TurkuNLP", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.8274006247520447}]}, {"text": " Table 3: Ablation results for the tokenizer. All  metrics in the table are macro-averaged dev F 1 .", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970279335975647}]}, {"text": " Table 4: Ablation results for the tagger. All  metrics are macro-averaged dev F 1 , except PMI,  which is explained in detail in the main text.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.992942214012146}, {"text": "macro-averaged dev F 1", "start_pos": 60, "end_pos": 82, "type": "METRIC", "confidence": 0.6198333129286766}, {"text": "PMI", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.7874789237976074}]}, {"text": " Table 5: Ablation results for the lemmatizer, split  by different groups of treebanks. All metrics in the  table are macro-averaged dev F 1 .", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966044425964355}]}, {"text": " Table 6: Ablation results for the parser. All met- rics in the table are macro-averaged dev F 1 .", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980633854866028}]}]}