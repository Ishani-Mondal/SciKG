{"title": [{"text": "A Morphology-based Representation Model for LSTM-based Dependency Parsing of Agglutinative Languages", "labels": [], "entities": [{"text": "LSTM-based Dependency Parsing of Agglutinative Languages", "start_pos": 44, "end_pos": 100, "type": "TASK", "confidence": 0.8224114974339803}]}], "abstractContent": [{"text": "We propose two word representation models for agglutinative languages that better capture the similarities between words which have similar tasks in sentences.", "labels": [], "entities": []}, {"text": "Our models highlight the morphological features in words and embed morphological information into their dense representations.", "labels": [], "entities": []}, {"text": "We have tested our models on an LSTM-based dependency parser with character-based word embeddings proposed by Ballesteros et al.", "labels": [], "entities": [{"text": "LSTM-based dependency parser", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.7065218885739645}]}, {"text": "We participated in the CoNLL 2018 Shared Task on multilingual parsing from raw text to universal dependencies as the BOUN team.", "labels": [], "entities": [{"text": "BOUN", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.8822514414787292}]}, {"text": "We show that our morphology-based embedding models improve the parsing performance for most of the agglu-tinative languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes our submission to the) on parsing of Universal Dependencies (UD) (.", "labels": [], "entities": [{"text": "parsing of Universal Dependencies (UD)", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.8942765082631793}]}, {"text": "We propose morphologically enhanced character-based word embeddings to improve the parsing performance especially for agglutinative languages.", "labels": [], "entities": []}, {"text": "We apply our approach to a transitionbased dependency parser by  that uses stack Long Short Term Memory structures (LSTMs) to predict the parser state.", "labels": [], "entities": []}, {"text": "This parser uses character-level word representation, which has been shown to perform better for languages with rich morphology ().", "labels": [], "entities": [{"text": "character-level word representation", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.5959692498048147}]}, {"text": "From our experiment results performed on UD version 2.2 data sets ( we observe that including morphological information to a character-based word embedding model yields a better learning of relationships between words and increases the parsing performance for most of the agglutinative languages with rich morphology.", "labels": [], "entities": [{"text": "UD version 2.2 data sets", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.7181842207908631}]}, {"text": "The rest of the paper is organized as follows: Section 2 provides a brief description of the LSTM-based dependency parser used in this study and introduces our embedding models.", "labels": [], "entities": [{"text": "LSTM-based dependency parser", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6362584928671519}]}, {"text": "Section 3 gives the implementation details of our system and describes the training strategies we apply to different languages.", "labels": [], "entities": []}, {"text": "Section 4 discusses our results on the shared task as well as the post-evaluation experiments and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed another set of experiments using our models on the test data of UD version 2.2 data sets.", "labels": [], "entities": [{"text": "UD version 2.2 data sets", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.8693797588348389}]}, {"text": "The purpose of these experiments is to investigate the effect of our embedding models on parsing performance.", "labels": [], "entities": []}, {"text": "Here we used the gold-standard conll-u files instead of the automatically annotated corpora by UDPipe, since our aim in these experiments is to observe the performance difference between our embedding models and the baseline embedding model.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.8895401358604431}]}, {"text": "In, we compare our models with the baseline model proposed in ( . Due to time constraints, we trained all models without pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "From the comparative results shown in, we observe that on the languages that have rich inflectional and derivational processes mostly by adding suffixes to words, our morphological features model outperforms the baseline model in terms of parsing scores.", "labels": [], "entities": []}, {"text": "This is the case for the Bulgarian, Croatian, Czech, Basque, Gothic, Latin, Polish, Russian, Slovak, Slovene, North Sami, and Ukrainian languages.", "labels": [], "entities": []}, {"text": "The morphological features model is not suitable for the grammatical structure of Arabic, which has derivational morphology and it also fails to outperform the baseline in Romanic languages like French, Spanish, Catalan, Galician, and Portuguese.", "labels": [], "entities": []}, {"text": "The possible reason behind this failure might be the analytic structure of the grammar of these languages where every morpheme is: The effect of using pre-trained word embeddings on parsing performance on Turkish-IMST test data set.", "labels": [], "entities": [{"text": "Turkish-IMST test data set", "start_pos": 205, "end_pos": 231, "type": "DATASET", "confidence": 0.9104220867156982}]}, {"text": "English, Hebrew, Hindi and Urdu languages are also categorized as mostly analytic languages which do not use inflections and have a low morpheme-per-word ratio.", "labels": [], "entities": []}, {"text": "Dutch, Norwegian, and Swedish languages have a very simplified inflectional grammar.", "labels": [], "entities": []}, {"text": "So, these languages are not represented well using our morphology-based embedding models.", "labels": [], "entities": []}, {"text": "Besides, our model is not the best choice for the languages that have high ratio of morphophonological modifications to the root word like Old Church Slavonic.", "labels": [], "entities": []}, {"text": "The lemma-suffix embedding model is applied to the Danish, Hungarian, Kazakh, Turkish, and Uyghur languages.", "labels": [], "entities": []}, {"text": "The best performance is reached in the Hungarian language with more than 4% increase in LAS score.", "labels": [], "entities": [{"text": "LAS score", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9785713851451874}]}, {"text": "Our model outperforms the baseline in Turkish too.", "labels": [], "entities": []}, {"text": "These languages are highly agglutinative languages where words may consist of several morphemes and the boundaries between morphemes are clearcut.", "labels": [], "entities": []}, {"text": "In this type of languages, there is a one-to-one formmeaning correspondence and shape of a morpheme is invariant).", "labels": [], "entities": []}, {"text": "An example word-morpheme relationship in Hungarian and Turkish languages is shown in.", "labels": [], "entities": []}, {"text": "As it can be seen from the table, this structure is very suitable to the lemma-suffix embedding model.", "labels": [], "entities": []}, {"text": "However, the lemma-suffix model fails to reach better performance than the baseline system on the Kazakh and Uyghur treebanks.", "labels": [], "entities": []}, {"text": "A possible reason might be that our embedding model increases the complexity of the system unnecessarily for these languages with very little training data.", "labels": [], "entities": []}, {"text": "Although Danish can be considered as an analytic language with a simplified inflectional grammar, the lemmasuffix model outperforms the baseline for this language.", "labels": [], "entities": []}, {"text": "shows the parsing scores of the parser with lemma-suffix embedding model on the test data of Turkish-IMST treebank version 2.2.", "labels": [], "entities": [{"text": "Turkish-IMST treebank version 2.2", "start_pos": 93, "end_pos": 126, "type": "DATASET", "confidence": 0.9479921609163284}]}, {"text": "We compared the parsing performances when the parser does not use pre-trained word embeddings, when it uses pre-trained embeddings from CoNLL-17 UD word embeddings, and when it uses pre-trained embeddings from word vectors trained on Wikipedia by.", "labels": [], "entities": [{"text": "CoNLL-17 UD word embeddings", "start_pos": 136, "end_pos": 163, "type": "DATASET", "confidence": 0.9148741066455841}]}, {"text": "From the results, we observe that the usage of pre-trained word vectors increases the parsing performance by great extent for Turkish.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9617077112197876}]}, {"text": "We also observe that Facebook word vectors outperform the CoNLL-17 UD word vectors, although the number of words in the Facebook vectors data set is much smaller than the number of words in the CoNLL-17 UD word vectors data set.", "labels": [], "entities": [{"text": "CoNLL-17 UD word vectors", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.9280485212802887}, {"text": "Facebook vectors data set", "start_pos": 120, "end_pos": 145, "type": "DATASET", "confidence": 0.9014548659324646}, {"text": "CoNLL-17 UD word vectors data set", "start_pos": 194, "end_pos": 227, "type": "DATASET", "confidence": 0.946804036696752}]}], "tableCaptions": [{"text": " Table 1: Number of occurrences of some example suffixes and the corresponding dependency labels of  verbs with these suffixes in the development data of Turkish-IMST treebank.", "labels": [], "entities": [{"text": "Turkish-IMST treebank", "start_pos": 154, "end_pos": 175, "type": "DATASET", "confidence": 0.919352114200592}]}, {"text": " Table 5: Comparison of our embedding models with the baseline char-based word embedding model  explained in Section 2.1. MF stands for the morphological features embedding model and LS stands for  the lemma-suffix embedding model.", "labels": [], "entities": []}, {"text": " Table 6: Word-morpheme structure on the Hungarian word ember and the Turkish word adam (English  meaning: man).", "labels": [], "entities": []}, {"text": " Table 7: The effect of using pre-trained word embeddings on parsing performance on Turkish-IMST test  data set.", "labels": [], "entities": [{"text": "Turkish-IMST test  data set", "start_pos": 84, "end_pos": 111, "type": "DATASET", "confidence": 0.9472823143005371}]}]}