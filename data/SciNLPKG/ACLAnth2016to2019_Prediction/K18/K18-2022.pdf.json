{"title": [], "abstractContent": [{"text": "We present SParse, our Graph-Based Parsing model submitted for the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies (Ze-man et al., 2018).", "labels": [], "entities": []}, {"text": "Our model extends the state-of-the-art biaffine parser (Dozat and Manning, 2016) with a structural meta-learning module, SMeta, that combines local and global label predictions.", "labels": [], "entities": []}, {"text": "Our parser has been trained and run on Universal Dependencies datasets (Nivre et al., 2016, 2018) and has 87.48% LAS, 78.63% MLAS, 78.69% BLEX and 81.76% CLAS (Nivre and Fang, 2017) score on the Italian-ISDT dataset and has 72.78% LAS, 59.10% MLAS, 61.38% BLEX and 61.72% CLAS score on the Japanese-GSD dataset in our official submission.", "labels": [], "entities": [{"text": "Universal Dependencies datasets", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.8062902092933655}, {"text": "LAS", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9623419642448425}, {"text": "BLEX", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.996439516544342}, {"text": "Italian-ISDT dataset", "start_pos": 195, "end_pos": 215, "type": "DATASET", "confidence": 0.885733962059021}, {"text": "BLEX", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9973888993263245}, {"text": "Japanese-GSD dataset", "start_pos": 290, "end_pos": 310, "type": "DATASET", "confidence": 0.9313624799251556}]}, {"text": "All other corpora are evaluated after the submission deadline, for whom we present our unofficial test results.", "labels": [], "entities": []}], "introductionContent": [{"text": "End-to-end learning with neural networks has proven to be effective in parsing natural language.", "labels": [], "entities": [{"text": "parsing natural language", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.912604570388794}]}, {"text": "Graph-based dependency parsers) represent dependency scores between words as a matrix representing a weighted fully connected graph, from which a spanning tree algorithm extracts the best parse tree.", "labels": [], "entities": []}, {"text": "This setting is very compatible with neural network models that are good at producing matrices of continuous numbers.", "labels": [], "entities": []}, {"text": "Compared to transition-based parsing (, which was the basis of our university's last year entry, graph-based parsers have the disadvantage of producing n 2 entries for parsing an n-word sentence.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.5666258931159973}]}, {"text": "Furthermore, algorithms used to parse these entries can be even more complex than O(n 2 ).", "labels": [], "entities": []}, {"text": "However, graph-based parsers allow easyto-parallelize static architectures rather than sequential decision mechanisms and are able to parse non-projective sentences.", "labels": [], "entities": []}, {"text": "Non-projective graph-based parsing is the core of last year's winning entry.", "labels": [], "entities": []}, {"text": "Neural graph-based parsers can be divided into two components: encoder and decoder.", "labels": [], "entities": []}, {"text": "The encoder is responsible for representing the sentence as a sequence of continuous feature vectors.", "labels": [], "entities": []}, {"text": "The decoder receives this sequence and produces the parse tree, by first creating a graph representation and then extracting the maximum spanning tree (MST).", "labels": [], "entities": []}, {"text": "We use a bidirectional RNN (bi-RNN) to produce a contextual vector for each word in a sentence.", "labels": [], "entities": []}, {"text": "Use of bi-RNNs is the defacto standard in dependency parsing, as it allows representing each word conditioned on the whole sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.859229564666748}]}, {"text": "Our main contribution in the encoder part is to the word embeddings feeding the bi-RNN.", "labels": [], "entities": []}, {"text": "We use word vectors coming from a language model pretrained on very large language corpora, similar to.", "labels": [], "entities": []}, {"text": "We extend word embeddings with learnable embeddings for UPOS tags, XPOS tags and FEATs where applicable.", "labels": [], "entities": [{"text": "FEATs", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9476577639579773}]}, {"text": "Our decoder can be viewed as a more structured version of the state-of-the-art biaffine decoder of, where we attempt to condition the label-seeking units to a parsetree instead of simple local predictions.", "labels": [], "entities": []}, {"text": "We propose a meta-learning module that allows structured and unstructured predictions to be combined as a weighted sum.", "labels": [], "entities": []}, {"text": "This additional computational complexity is paid off by our simple word-level model in the encoder part.", "labels": [], "entities": []}, {"text": "We call it that we call structured meta-biaffine decoder or shortly SMeta.", "labels": [], "entities": []}, {"text": "We implemented our model using Knet deep learning framework in Julia language (.", "labels": [], "entities": []}, {"text": "Our code will be made available publicly.", "labels": [], "entities": []}, {"text": "We could only get official results for two corpora due to an unexpected software bug.", "labels": [], "entities": []}, {"text": "Therefore, we present unofficial results obtained after the submission deadline as well.", "labels": [], "entities": []}, {"text": "use trainable BiLSTMs to represent features of each word, instead of defining the features manually.", "labels": [], "entities": []}, {"text": "They formulated the structured prediction using hinge loss based on the gold parse tree and parsed scores.", "labels": [], "entities": []}, {"text": "propose deep biaffine attention combined with the parsing model of, which simplifies the architecture by allowing implementation with a single layer instead of two linear layers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Our unofficial F1 scores. Tests are done in TIRA (Potthast et al., 2014) machine allocated for  the task.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.980404794216156}, {"text": "TIRA (Potthast et al., 2014) machine allocated", "start_pos": 54, "end_pos": 100, "type": "DATASET", "confidence": 0.7958218932151795}]}]}