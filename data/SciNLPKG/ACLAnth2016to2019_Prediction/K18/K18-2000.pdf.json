{"title": [], "abstractContent": [], "introductionContent": [{"text": "This volume contains papers describing systems submitted to the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, and two overview papers: one summarizing the main task, its features, evaluation methodology for the main and additional metrics, and some interesting observations about the submitted systems and the task as a whole; the other overview paper discusses a complementary task focusing on Extrinsic Parser Evaluation (EPE).", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8246670812368393}]}, {"text": "This Shared Task (http://universaldependencies.org/conll18/) is an extension of the CoNLL 2017 Shared Task, with certain important differences.", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9278083741664886}]}, {"text": "Like in the previous year, the data come from the Universal Dependencies project (http://universaldependencies.org), which provides annotated treebanks fora large number of languages using the same annotation scheme.", "labels": [], "entities": []}, {"text": "The number of treebanks in the task (82) is similar to the previous year (81), but some treebanks from the 2017 task were not included in the present task, and some new treebanks were added instead.", "labels": [], "entities": []}, {"text": "The datasets are samples from 57 different languages (all languages from the previous year, and eight new languages).", "labels": [], "entities": []}, {"text": "In comparison to 2017, there were more low-resource languages with extremely little training data, calling for cross-lingual transfer techniques.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6983785033226013}]}, {"text": "Unlike 2017, none of the low-resource languages were \"surprise languages\".", "labels": [], "entities": []}, {"text": "Participants had to process all the test sets.", "labels": [], "entities": []}, {"text": "The TIRA platform has been used for evaluation, as was the case already for the that participants had to provide their code on a designated virtual machine to be run by the organizers to produce official results.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.5996972918510437}]}, {"text": "However, test data have been published after the official evaluation period, and participants could run their systems at home to produce additional results they were allowed to include in the system description papers.", "labels": [], "entities": []}, {"text": "The systems were ranked according to three main evaluation metrics -LAS (Labeled Attachment Score), MLAS (Morphology-aware Labeled Attachment Score), and BLEX (BiLEXical Dependency Score).", "labels": [], "entities": [{"text": "LAS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9635508060455322}, {"text": "MLAS", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.4685041010379791}, {"text": "BLEX (BiLEXical Dependency Score)", "start_pos": 154, "end_pos": 187, "type": "METRIC", "confidence": 0.8337826033433279}]}, {"text": "Like last year, participating systems minimally had to find labeled syntactic dependencies between words.", "labels": [], "entities": []}, {"text": "In addition, this year's task featured new metrics that also scored a system's capacity to predict a morphological analysis of each word, including a part-of-speech tag, morphological features, and a lemma.", "labels": [], "entities": []}, {"text": "Regardless of metric, the assumption was that the input should be raw text, with no goldstandard word or sentence segmentation, and no gold-standard morphological annotation.", "labels": [], "entities": [{"text": "word or sentence segmentation", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.6379740312695503}]}, {"text": "However, for teams who wanted to concentrate on one or more subtasks, segmentation and morphology predicted by a baseline system was made available.", "labels": [], "entities": []}, {"text": "The complementary EPE task seeks to provide better estimates of the relative utility of different parsers fora variety of downstream applications that depend centrally on the analysis of grammatical structure, viz.", "labels": [], "entities": []}, {"text": "biomedical event extraction, negation resolution, and fine-grained opinion analysis.", "labels": [], "entities": [{"text": "biomedical event extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.686907023191452}, {"text": "negation resolution", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.995029091835022}, {"text": "fine-grained opinion analysis", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.5948202709356943}]}, {"text": "EPE 2018 was organized as an optional add-on exercise to the core task: the submitted systems were applied to extra texts, about 1.1 million tokens of English.", "labels": [], "entities": [{"text": "EPE 2018", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.933977335691452}]}, {"text": "It is interesting to see to what degree different intrinsic evaluation metrics from the core task correlate with end-to-end EPE results, and comparison to earlier EPE campaigns-with other types of dependency representations and additional sources of training data-further helps to put the core task into perspective.", "labels": [], "entities": []}, {"text": "A total of 25 systems ran successfully and have been ranked (http://universaldependencies.", "labels": [], "entities": []}, {"text": "org/conll18/results.html); 17 teams submitted parser outputs for the EPE test set.", "labels": [], "entities": [{"text": "EPE test set", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9293257395426432}]}, {"text": "While there are clear overall winners in each of the evaluation metrics, we would like to thank all participants for working hard on their submissions and adapting their systems not only to the datasets available, but also to the evaluation platform.", "labels": [], "entities": []}, {"text": "We would like to thank all of them for their effort, since it is the participants who are the core of any shared task's success.", "labels": [], "entities": []}, {"text": "iii We would like to thank the CoNLL organizers for their support and the reviewers for helping to improve the submitted system papers.", "labels": [], "entities": [{"text": "CoNLL organizers", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8672746419906616}]}, {"text": "Special thanks go to Martin Potthast of the TIRA platform for handling such a large number of systems, running often for several hours each, and for being very responsive and helpful to us and all system participants, round the clock during the evaluation phase and beyond.", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8406290113925934}]}, {"text": "We also thank to the 200+ people working on the Universal Dependencies project during the past four years, without whom there would be no data.", "labels": [], "entities": [{"text": "Universal Dependencies project", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.780648946762085}]}], "datasetContent": [], "tableCaptions": []}