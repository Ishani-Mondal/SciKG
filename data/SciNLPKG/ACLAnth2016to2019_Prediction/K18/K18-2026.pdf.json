{"title": [{"text": "A Simple yet Effective Joint Training Method for Cross-Lingual Universal Dependency Parsing", "labels": [], "entities": [{"text": "Cross-Lingual Universal Dependency Parsing", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.6715002730488777}]}], "abstractContent": [{"text": "This paper describes Fudan's submission to CoNLL 2018's shared task Universal Dependency Parsing.", "labels": [], "entities": [{"text": "CoNLL 2018's shared task Universal Dependency Parsing", "start_pos": 43, "end_pos": 96, "type": "TASK", "confidence": 0.7627913430333138}]}, {"text": "We jointly train models when two languages are similar according to linguistic typology and then do an ensemble of the models using a simple re-parse algorithm.", "labels": [], "entities": []}, {"text": "Our system outperforms the baseline method by 4.4% and 2.1% on the development and test set of CoNLL 2018 UD Shared Task, separately.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 95, "end_pos": 120, "type": "DATASET", "confidence": 0.8503887295722962}]}, {"text": "1. Our code is available on https://github.com/ taineleau/FudanParser.", "labels": [], "entities": [{"text": "FudanParser", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9337868690490723}]}], "introductionContent": [{"text": "Dependency Parsing has been a fundamental task in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8855301439762115}, {"text": "Natural Language Processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7046171724796295}]}, {"text": "Recently, universal dependency parsing ( has unified the annotations of different languages and thus made transfer learning among languages possible.", "labels": [], "entities": [{"text": "universal dependency parsing", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6371100743611654}]}, {"text": "Several works using cross-lingual embedding) have successfully increased the accuracy of cross-lingual parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992411136627197}, {"text": "cross-lingual parsing", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.5913882255554199}]}, {"text": "Beyond embedding-based methods, a natural question is whether we can use a simple way to utilize the universal information.", "labels": [], "entities": []}, {"text": "Some previous research either regarded the universal information as extra training signals (e.g., delexicalized embedding), or implicitly trained a network with all features (e.g., adversarial training for parsing in).", "labels": [], "entities": []}, {"text": "In our system, we manually and explicitly share the universal annotations via a shared LSTM component.", "labels": [], "entities": []}, {"text": "Similar to, different languages are first grouped based on typology, as shown in table 1.", "labels": [], "entities": []}, {"text": "Then, we train a shared model for each pair of languages within the same group, and apply a simple ensemble method overall trained models.", "labels": [], "entities": []}, {"text": "Note that our method is orthogonal to other cross-lingual approaches for universal parsing such as cross-lingual embedding.", "labels": [], "entities": []}, {"text": "In the following parts, we first describe the baseline method (Section 2) and our system (Section 3).", "labels": [], "entities": []}, {"text": "We show the result on both development set and test set in Section 4 and provide some analysis of the model in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Slavic languages joint training result.  Acc.(%) bg  hr  cs  pl  ru  sk  sl  max improvement  bg 92.6 92.5 92.8  92.7 92.7 92.3 92.4 0.2  hr 85.7 86  86.1  85.2 85.5 85.8 85.5 0.1  cs 91.2 91.2 91.2  91.1 91.3 91.3 91.2 0.1  pl 90.4 89.8 90.2  90.1 90.2 90.4 90.8 0.3  ru 84.4 84.7 85.2  84.4 83.8 84.1 84.6 1.4  sk 86.4 86.2 87.8  85.9 86.4 86.7 86.1 1.1  sl 91.4 91.8 91.7  91.4 91.4 91  91.2 0.6  Avg.  0.54  # samples 8908 7690 68496 6101 3851 8484 6479", "labels": [], "entities": [{"text": "Acc.", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9990196228027344}, {"text": "Avg.  0.54  # samples 8908 7690 68496 6101 3851 8484 6479", "start_pos": 410, "end_pos": 467, "type": "DATASET", "confidence": 0.8279688308636347}]}, {"text": " Table 3: Romance languages joint training result.  Acc. (%) ca  fr  gl  it  pt  ro  es  max improvement  ca  92.6  92.4  92.6 92.7  92.6 92.4 92.5  0.1  fr  93.1  92.9  93  93.4  93.2 93.1 93.2  0.5  gl  86.9  86.3  86.1 86.7  86.4 86.3 86.4  0.8  it  93  92.6  92.7 92.3  93  92.8 93.1  0.8  pt  93  92.8  92.7 92.8  92.6 92.9 92.8  0.4  ro  88.8  88.9  88.9 89  88.7 88.4 88.7  0.6  es  90.9  90.8  90.5 91.1  91  90.6 90.7  0.4  Avg.  0.56  # samples 13124 14554 2277 12839 8332 8044 14188", "labels": [], "entities": [{"text": "Acc", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9991956353187561}, {"text": "Avg.  0.56  # samples 13124 14554 2277 12839 8332 8044 14188", "start_pos": 433, "end_pos": 493, "type": "DATASET", "confidence": 0.8296224524577459}]}, {"text": " Table 4: Germanic languages joint training result.  Acc. (%) da  nl  en  de  sv  max improvement  da  85  85.1  85  85.2  85.6 0.6  nl  89  88.8  88.7  89.3  88.6 0.5  en  89.4 89.1  88.9  88.9  89.1 0.5  de  88.5 88.9  88.7  88.6  88.5 0.3  sv  85.7 85  86.5  85.9  85  1.5  Avg.  0.68  # samples 4384 12331 12544 14119 4303", "labels": [], "entities": [{"text": "Acc", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9990931749343872}, {"text": "Avg.  0.68  # samples 4384 12331 12544 14119 4303", "start_pos": 277, "end_pos": 326, "type": "DATASET", "confidence": 0.8409614950418473}]}, {"text": " Table 5: All results on test set.", "labels": [], "entities": []}, {"text": " Table 6: All results on development set.", "labels": [], "entities": []}]}