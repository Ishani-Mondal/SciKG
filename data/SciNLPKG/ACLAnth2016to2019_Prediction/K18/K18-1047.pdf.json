{"title": [{"text": "Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications.", "labels": [], "entities": []}, {"text": "We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples.", "labels": [], "entities": []}, {"text": "This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs.", "labels": [], "entities": []}, {"text": "Moreover, training on all strategies combined achieves further improvements, achieving anew state-of-the-art performance on the original task (also verified via human evaluation).", "labels": [], "entities": []}, {"text": "In addition to adversarial training, we also address the ro-bustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive , requires only 1/4 of the original vocabulary size, and is robust to one of the adver-sarial strategies (to which the original model is vulnerable) even without adversarial training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adversarial evaluation aims at filling in the gap between potential train/test distribution mismatch and revealing how models will perform under realworld inputs containing natural or malicious noise.", "labels": [], "entities": []}, {"text": "Recently, there has been substantial work on adversarial attacks in computer vision and NLP.", "labels": [], "entities": []}, {"text": "Unlike vision, where one can simply add in imperceptible perturbations without changing an image's meaning, carrying out such subtle changes in text is harder since text is discrete in nature (Jia and We publicly release all our code and data at https: //github.com/WolfNiu/AdversarialDialogue ).", "labels": [], "entities": []}, {"text": "Thus, some previous works have either avoided modifying original source inputs and only resorted to inserting distractive sentences, or have restricted themselves to introducing spelling errors and adding non-functioning tokens.", "labels": [], "entities": []}, {"text": "Furthermore, there has been limited adversarial work on generative NLP tasks, e.g., dialogue generation (, which is especially important because it is a crucial component of real-world virtual assistants such as Alexa, Siri, and Google Home.", "labels": [], "entities": [{"text": "generative NLP tasks", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.9180418848991394}, {"text": "dialogue generation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8137163519859314}]}, {"text": "It is also a challenging and worthwhile task to keep the output quality of a dialogue system stable, because a conversation usually involves multiple turns, and a small mistake in an early turn could cascade into bigger misunderstanding later on.", "labels": [], "entities": []}, {"text": "Motivated by this, we present a comprehensive adversarial study on dialogue models -we not only simulate imperfect inputs in the real world, but also launch intentionally malicious attacks on the model in order to assess them on both oversensitivity and over-stability.", "labels": [], "entities": []}, {"text": "Unlike most previous works that exclusively focus on Should-NotChange adversarial strategies (i.e., non-semanticschanging perturbations to the source sequence that should not change the response), we demonstrate that it is equally valuable to consider ShouldChange strategies (i.e., semantics-changing, intentional perturbations to the source sequence that should change the response).", "labels": [], "entities": []}, {"text": "We investigate three state-of-the-art models on two task-oriented dialogue datasets.", "labels": [], "entities": []}, {"text": "Concretely, we propose and evaluate five naturally motivated and increasingly complex Should-NotChange and five Should-Change adversarial strategies on the VHRED (Variational Hierarchical Encoder-Decoder) model) and the RL (Reinforcement Learning) model () with the Ubuntu Dialogue Cor-pus (, and Dynamic Knowledge Graph Network with the Collaborative Communicating Agents (CoCoA) dataset.", "labels": [], "entities": []}, {"text": "On the Should-Not-Change side for the Ubuntu task, we introduce adversarial strategies of increasing linguistic-unit complexity -from shallow word-level errors, to phrase-level paraphrastic changes, and finally to syntactic perturbations.", "labels": [], "entities": []}, {"text": "We first propose two rule-based perturbations to the source dialogue context, namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout (randomly removing stopwords).", "labels": [], "entities": []}, {"text": "Next, we propose two data-level strategies that leverage existing parallel datasets in order to simulate more realistic, diverse noises: namely, Data-Level Paraphrasing (replacing words with their paraphrases) and Grammar Errors (e.g., changing a verb to the wrong tense).", "labels": [], "entities": []}, {"text": "Finally, we employ GenerativeLevel Paraphrasing, where we adopt a neural model to automatically generate paraphrases of the source inputs.", "labels": [], "entities": []}, {"text": "On the Should-Change side for the Ubuntu task, we propose the Add Negation strategy, which negates the root verb of the source input, and the Antonym strategy, which changes verbs, adjectives, or adverbs to their antonyms.", "labels": [], "entities": []}, {"text": "As will be shown in Section 6, the above strategies are effective on the Ubuntu task, but not on the collaborative-style, database-dependent CoCoA task.", "labels": [], "entities": []}, {"text": "Thus for the latter, we investigate additional Should-Change strategies including Random Inputs (changing each word in the utterance to random ones), Random Inputs with Entities (like Random Inputs but leaving mentioned entities untouched), and Normal Inputs with Confusing Entities (replacing entities in an agent's utterance with distractive ones) to analyze where the model's robustness stems from.", "labels": [], "entities": []}, {"text": "To evaluate these strategies, we first show that (1) both VHRED and the RL model are vulnerable to most Should-Not-Change and all ShouldChange strategies, and (2) DynoNet's robustness to Should-Change inputs shows that it does not pay any attention to natural language inputs other than the entities contained in them.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8959543108940125}]}, {"text": "Next, observing how our adversarial strategies 'successfully' fool the target models, we try to expose these models to such perturbation patterns early on during training itself, where we feed adversarial input context and ground-truth target pairs as training data.", "labels": [], "entities": []}, {"text": "Importantly, we realize this adversarial training via a maximum-likelihood loss for Should-Not-Change strategies, and via a maxmargin loss for Should-Change strategies.", "labels": [], "entities": []}, {"text": "We show that this adversarial training cannot only make both VHRED and RL more robust to the adversarial data, but also improve their performances when evaluated on the original test set (verified via human evaluation).", "labels": [], "entities": [{"text": "VHRED", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.7275860905647278}, {"text": "RL", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.8280876874923706}]}, {"text": "In addition, when we train VHRED on all of the perturbed data from each adversarial strategy together, the performance on the original task improves even further, achieving the state-of-the-art result by a significant margin (also verified via human evaluation).", "labels": [], "entities": []}, {"text": "Finally, we attempt to resolve the robustness issue directly at the model-level (instead of adversarial-level) by feeding subword units derived from the Byte Pair Encoding (BPE) algorithm) to the VHRED model.", "labels": [], "entities": [{"text": "VHRED model", "start_pos": 196, "end_pos": 207, "type": "DATASET", "confidence": 0.9365411102771759}]}, {"text": "We show that the resulting model not only reduces the vocabulary size by around 75% (thus trains much faster) and obtains results comparable to the original VHRED, but is also naturally (i.e., without requiring adversarial training) robust to the Grammar Errors adversarial strategy.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 157, "end_pos": 162, "type": "DATASET", "confidence": 0.9009236097335815}]}], "datasetContent": [{"text": "In addition to datasets, tasks, models and evaluation methods introduced in Section 2, we present training details in this section (see Appendix fora comprehensive version  we also add a heuristic where an inflected verb is replaced with its respective infinitive form, and a plural noun with its singular form.", "labels": [], "entities": []}, {"text": "Note that for all strategies we only keep an adversarial token if it is within the original vocabulary set.", "labels": [], "entities": []}, {"text": "Should-Change Strategies on Ubuntu: For Add Negation, we negate the first verb in each utterance.", "labels": [], "entities": []}, {"text": "For Antonym, we modify the first verb, adjective or adverb that has an antonym.", "labels": [], "entities": []}, {"text": "Human Evaluation: We also conducted human studies on MTurk to evaluate adversarial training (pairwise comparison for dialogue quality) and generative paraphrasing (five-point Likert scale).", "labels": [], "entities": [{"text": "generative paraphrasing", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.9596060216426849}]}, {"text": "The utterances were randomly shuffled to anonymize model identity, and we used MTurk with US-located human evaluators with approval rate > 98%, and at least 10, 000 approved HITs.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.7345316410064697}]}, {"text": "Results are presented in Section 6.1.", "labels": [], "entities": []}, {"text": "Note that the human studies and automatic evaluation are complementary to each other: while MTurk annotators are good at judging how natural and coherent a response is, they are usually not experts in the Ubuntu operating system's technical details.", "labels": [], "entities": []}, {"text": "On the other hand, automatic evaluation focuses more on the technical side (i.e., whether key activities or entities are present in the response).", "labels": [], "entities": [{"text": "automatic evaluation", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7174599766731262}]}, {"text": "Model on CoCoA: We adopted the publicly available code from, and used their already trained DynoNet model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 results of previous works as compared to  our models. LSTM, HRED and VHRED are results re- ported in Serban et al. (2017a). VHRED (w/ attn.) and  Reranking-RL are our results. Top results are bolded.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9986180067062378}, {"text": "LSTM", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9681291580200195}, {"text": "HRED", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9951234459877014}, {"text": "VHRED", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.7394789457321167}, {"text": "VHRED", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.8812052607536316}, {"text": "Reranking-RL", "start_pos": 159, "end_pos": 171, "type": "METRIC", "confidence": 0.794619619846344}]}, {"text": " Table 2: Activity and Entity F1 results of adversarial strategies on the VHRED model. Numbers marked with * are  stat. significantly higher/lower than their counterparts obtained with Normal Input (upper-right corner of table).", "labels": [], "entities": [{"text": "VHRED model", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9577733278274536}]}, {"text": " Table 3: Activity and Entity F1 results of adversarial strategies on the Reranking-RL model. Numbers marked  with * are stat. significantly higher/lower than their counterparts obtained with Normal Input (upper-right corner).", "labels": [], "entities": [{"text": "Entity F1", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.6798174679279327}, {"text": "Reranking-RL model", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.8369762301445007}, {"text": "Normal Input", "start_pos": 192, "end_pos": 204, "type": "METRIC", "confidence": 0.8753732740879059}]}, {"text": " Table 4: Textual similarity of adversarial strategies on  the VHRED and Reranking-RL models. \"Cont.\" stands  for \"Context\", and \"Resp.\" stands for \"Response\".", "labels": [], "entities": [{"text": "VHRED", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.974987804889679}]}, {"text": " Table 7: VHRED output example before and after adversarial training on the Random Swap strategy.", "labels": [], "entities": []}, {"text": " Table 8: Adversarial Results on DynoNet.", "labels": [], "entities": []}, {"text": " Table 9: Activity, Entity F1 results of VHRED model  vs. BPE-VHRED model tested on normal inputs.", "labels": [], "entities": [{"text": "Entity F1", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.902039647102356}, {"text": "VHRED", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.7037944197654724}, {"text": "BPE-VHRED", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8598378300666809}]}]}