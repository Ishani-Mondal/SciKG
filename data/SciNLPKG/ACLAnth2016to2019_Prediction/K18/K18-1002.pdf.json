{"title": [{"text": "Continuous Word Embedding Fusion via Spectral Decomposition", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have become a mainstream tool in statistical natural language processing.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.7908933609724045}]}, {"text": "Practitioners often use pre-trained word vectors , which were trained on large generic text corpora, and which are readily available on the web.", "labels": [], "entities": []}, {"text": "However, pre-trained word vectors often-times lack important words from specific domains.", "labels": [], "entities": []}, {"text": "It is therefore often desirable to extend the vocabulary and embed new words into a set of pre-trained word vectors.", "labels": [], "entities": []}, {"text": "In this paper, we present an efficient method for including new words from a specialized corpus, containing new words, into pre-trained generic word embeddings.", "labels": [], "entities": []}, {"text": "We build on the established view of word embeddings as matrix factorizations to present a spectral algorithm for this task.", "labels": [], "entities": []}, {"text": "Experiments on several domain-specific corpora with specialized vocabularies demonstrate that our method is able to embed the new words efficiently into the original embedding space.", "labels": [], "entities": []}, {"text": "Compared to competing methods, our method is faster, parameter-free, and deterministic.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a recent surge of neural word embedding models ().", "labels": [], "entities": []}, {"text": "These models have been shown to perform well in a variety of NLP problems, such as word similarity and relational analogy tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7641059458255768}, {"text": "relational analogy tasks", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.8327803413073221}]}, {"text": "Word embeddings play a crucial role in diverse fields such as computer vision), news classification (, machine translation (, and have been extended in various ways (.", "labels": [], "entities": [{"text": "news classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8205904960632324}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.813684493303299}]}, {"text": "Instead of training word embeddings from scratch, practitioners often resort to high-quality, pre-trained word embeddings which can be downloaded from the web.", "labels": [], "entities": []}, {"text": "These embeddings were trained on massive corpora, such as online news.", "labels": [], "entities": []}, {"text": "The downside is that their vocabulary is often restricted, and by nature, is very generic.", "labels": [], "entities": []}, {"text": "An open question remains of how to optimally include new words from highly specialized text corpora into an existing word embedding fit.", "labels": [], "entities": []}, {"text": "Such transfer learning has several advantages.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.9415150284767151}]}, {"text": "First, one saves the computational burden of learning high-quality word vectors from scratch.", "labels": [], "entities": []}, {"text": "Second, one can already rely on the fact that the majority of word embedding vectors are semantically meaningful.", "labels": [], "entities": []}, {"text": "Third, as we show in this paper, there are deterministic and parameter-free approaches that fulfill this goal, making the scheme robust and reproducible.", "labels": [], "entities": []}, {"text": "For a practical application, imagine that we are given a small corpus, such as a collection of scientific articles, and our goal is to include the associated vocabulary into a pre-trained set of word vectors which were learned on Google Books or Wikipedia.", "labels": [], "entities": []}, {"text": "The scientific corpus contains both common words (e.g., \"propose\", \"experiment\") and domain-specific words, such as \"submodular\" and \"sublinear\".", "labels": [], "entities": []}, {"text": "However, this specialized corpus can be safely assumed to be too small to train a word embedding model from scratch.", "labels": [], "entities": []}, {"text": "Alternatively, we could merge the domain-specific corpus with a large generic corpus and train the entire word-embedding from scratch, but this would be computationally demanding and non-reproducible due to the non-convexity of the underlying optimization problem.", "labels": [], "entities": []}, {"text": "In this paper, we show how to include the specialized vocabulary into the generic set of word vectors without having to re-train the model on the large vocabulary, simply relying on linear algebra.", "labels": [], "entities": []}, {"text": "A naive baseline method is to fix the pre-trained word vectors and only update the new ones.", "labels": [], "entities": []}, {"text": "We found that this approach suffers from local optima and sensitive to hyper-parameters; therefore it is not reliable in practice.", "labels": [], "entities": []}, {"text": "In contrast, our approach is not based on gradient descent and therefore more robust, deterministic, and parameter-free.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Spectral Online Word Embedding (SOWE) algorithm for integrating new words into a pre-trained word embedding fit.", "labels": [], "entities": []}, {"text": "Our approach is based on online matrix factorization.", "labels": [], "entities": []}, {"text": "In more detail, our main contributions are as follows: \u2022 We propose a Spectral Online Word Embedding (SOWE) method to include new words into a pre-trained set of word vectors.", "labels": [], "entities": [{"text": "Spectral Online Word Embedding (SOWE)", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.7241708891732352}]}, {"text": "This approach does naturally not suffer from optimization problems, such as initialization, parameter tuning, and local optima.", "labels": [], "entities": [{"text": "initialization", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.9589782953262329}, {"text": "parameter tuning", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.6650541722774506}]}, {"text": "\u2022 Our approach approximately reduces to an online matrix factorization problem.", "labels": [], "entities": []}, {"text": "We provide abound on the approximation error and show that this error does not scale with the vocabulary size (Theorem 1).", "labels": [], "entities": [{"text": "approximation error", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.9669712781906128}]}, {"text": "\u2022 The complexity of proposed method scales linearly with the size of vocabulary and quadratically with embedding dimension, making the approach feasible in large-scale applications.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our method on two domain specific corpora.", "labels": [], "entities": []}, {"text": "Experimental results show that our method is able to embed new vocabulary faster than the baseline method while obtaining more meaningful embeddings.", "labels": [], "entities": []}, {"text": "It is also parameter-free and deterministic, making the approach easily reproducible.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show empirical results where we compare our proposed SOWE with other continuous word embedding algorithms.We first present some generic settings of our experiments, followed by quantitative and qualitative baseline comparisons.", "labels": [], "entities": []}, {"text": "Compared to the baselines, we find that our method is more efficient and finds more semantically meaningful embeddings.", "labels": [], "entities": []}, {"text": "Our method takes less than one minute to insert about 1,000 domain-specific words (e.g., machine learning related words from NIPS abstracts) into a pre-trained embedding model with more than 180,000 words.", "labels": [], "entities": []}, {"text": "Experimental Setup Our approach departs from pre-trained word vectors from a generic training corpus.", "labels": [], "entities": []}, {"text": "We downloaded publicly available pretrained word embeddings and two small text corpora from specific domains.", "labels": [], "entities": []}, {"text": "Our goal is to insert the domain-specific words that do not already appear in the original vocabulary efficiently into the embeddings.", "labels": [], "entities": []}, {"text": "First, we report on basic settings for our experiments.", "labels": [], "entities": []}, {"text": "The pre-trained embedding model based on English Wikipedia is available online 1 . It contains 183,870 words.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9474366903305054}]}, {"text": "The embedding dimension is 300.", "labels": [], "entities": []}, {"text": "The small, domain-specific corpora that we considered were the following ones: (1) \"NIPS Abstracts\": this data set contains abstracts of all the NIPS papers from 2014 to 2016.", "labels": [], "entities": [{"text": "NIPS Abstracts\"", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8705610632896423}, {"text": "NIPS papers from 2014", "start_pos": 145, "end_pos": 166, "type": "DATASET", "confidence": 0.8914342969655991}]}, {"text": "The data set contains 981 new words.", "labels": [], "entities": []}, {"text": "(2) \"Economy News\": This data set contains news articles, containing 868 new words.", "labels": [], "entities": [{"text": "Economy News\"", "start_pos": 5, "end_pos": 18, "type": "DATASET", "confidence": 0.8260587255160013}]}, {"text": "These two corpora are much smaller than base corpus.", "labels": [], "entities": []}, {"text": "For both the base corpus and the new corpus, the text was pre-processed using a window of 5 tokens on each side of the target word, where stop-words and non-textual elements were removed, and sentence splitting and tokenization were applied.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7574486434459686}]}, {"text": "Baselines: FOUN and FOUN+annealing A natural idea for inserting new words into a preestisting word embedding fit is to fix the old word/context vectors, and only to update the new ones.", "labels": [], "entities": [{"text": "FOUN", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9963437914848328}, {"text": "FOUN", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9903389811515808}]}, {"text": "This is our baseline method, referred in the following as \"Fix Old, Update New\" (FOUN).", "labels": [], "entities": [{"text": "FOUN", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.6417264342308044}]}, {"text": "The approach uses the word2vec objective and employs stochastic gradient descent for training.", "labels": [], "entities": []}, {"text": "We employ Robbins-Monro learning schedules, setting the stepsize at the t-th step as t = a(t + \u03b3) \u22120.51 . We used grid-search to find optimal parameters on all considered data sets, and found k = 5, \u03b3 = 1e4, a = 1/10 (for different tasks) to be optimal.", "labels": [], "entities": []}, {"text": "Due to the involved randomness in the baseline approach, we conducted 10 independent trials using different random seeds for each results and reported the average results.", "labels": [], "entities": []}, {"text": "For \"FOUN+annealing\" we used the same settings as in FOUN, but added random zero-mean Gaussian noise to the gradient.", "labels": [], "entities": [{"text": "FOUN", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.8530537486076355}, {"text": "FOUN", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5094577670097351}]}, {"text": "To this end, we employed aversion of Stochastic Gradient Langevin Dynamics (), where we scaled down the noise by a factor of 0.01.", "labels": [], "entities": []}, {"text": "Loss minimization and runtime We considered the word2vec loss on the extended vocabulary and evaluated the value of the loss function on the embedding vectors obtained form the different methods under consideration.", "labels": [], "entities": [{"text": "Loss minimization", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7015319019556046}]}, {"text": "The associated loss values and runtimes are reported in for the \"NIPS Abstracts\", and for \"Economy News\".", "labels": [], "entities": [{"text": "NIPS Abstracts\"", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.959713876247406}, {"text": "Economy News\"", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.896349827448527}]}, {"text": "We found both approaches yield similar values of the loss function.", "labels": [], "entities": []}, {"text": "(In our experiments below, however, we will show that our obtained word vectors seem to reflect the semantics of the original corpus better.)", "labels": [], "entities": []}, {"text": "As a clear improvement, we found that our method: Performance on word analogy task using text8 dataset.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7882416645685831}]}, {"text": "\"ideal\" means learning the word embedding with the full extended vocabulary from scratch.", "labels": [], "entities": []}, {"text": "Here, the matrix factorization method proposed in: Performance on word analogy task using existing embedding results and text8 dataset with different folds of splits.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8055233558019003}]}, {"text": "In the row \"Ideal\", we use the well-trained word embedding results downloaded from Internet.", "labels": [], "entities": []}, {"text": "FOUN and FOUN+annealing are the baseline that we are comparing with. is faster than the baseline, yielding a factor of 8 times speedup.", "labels": [], "entities": [{"text": "FOUN", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9880411624908447}, {"text": "FOUN+annealing", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9041404724121094}]}, {"text": "We consider the baseline method to be converged when the loss value of the current epoch is close (smaller than a threshold) to that of the previous epoch, where the threshold is 1e2 for NIPS abstract and Economics News.", "labels": [], "entities": [{"text": "NIPS abstract", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.9261052906513214}, {"text": "Economics News", "start_pos": 205, "end_pos": 219, "type": "DATASET", "confidence": 0.7803216278553009}]}, {"text": "Qualitative Nearest Neighbor Test To test whether the learned embedding vectors are semantically meaningful, we chose some words from the new vocabulary and reported their nearest neighbors in the extended vocabulary.", "labels": [], "entities": []}, {"text": "We expect the nearest neighbors to have a close semantic meanings.", "labels": [], "entities": []}, {"text": "We chose cosine similarity as a means to measure distance between words.", "labels": [], "entities": []}, {"text": "We chose the words \"eurodollars\" as a query for \"Economic News\" and \"submodular\" for \"NIPS Abstracts\".", "labels": [], "entities": [{"text": "eurodollars", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9415449500083923}, {"text": "Economic News\"", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.7929041981697083}, {"text": "NIPS Abstracts\"", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9448607563972473}]}, {"text": "The results are reported in and 3.", "labels": [], "entities": []}, {"text": "In the case of economics, we see that our algorithm recovered meaningful words such as \"midcap\" and \"ultralow\".", "labels": [], "entities": []}, {"text": "The baseline methods failed to return meaningful results with respect to the query.", "labels": [], "entities": []}, {"text": "One possible reason is that the baseline's underlying optimization algorithm got trapped in a poor local optimum.", "labels": [], "entities": []}, {"text": "In the case of NIPS abstracts, our SOWE method results in words such as \"nonsmooth\" and \"coreset\" which are highly related to the query \"submodular\", while the FOUN-based methods fail.", "labels": [], "entities": [{"text": "FOUN-based", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.8185214400291443}]}, {"text": "Our approach thus outperforms the baseline in providing meaningful relationships between the pre-trained word vectors and the newly embedded ones.", "labels": [], "entities": []}, {"text": "More examples are provided in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.5758060216903687}]}, {"text": "Evaluations on NLP Tasks Additionally, we evaluated the proposed method on some downstream NLP tasks, such as pairwise word similarity.", "labels": [], "entities": [{"text": "pairwise word similarity", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.5832235415776571}]}, {"text": "To this end, we used datasets that contain word pairs associated with human-assigned similarity scores 2 . The word vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation (Spearmans \u03c1) with the human ratings.", "labels": [], "entities": [{"text": "Spearmans \u03c1)", "start_pos": 230, "end_pos": 242, "type": "METRIC", "confidence": 0.9827369650204977}]}, {"text": "We excluded the word pairs of the similarity test from the original vocabulary and trained word2vec with the associated reduced vocabulary on several corpora.", "labels": [], "entities": []}, {"text": "We then added the test words using the three competing methods (SOWE, FOUN, and FOUN+anneal).", "labels": [], "entities": [{"text": "SOWE", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9045804738998413}, {"text": "FOUN", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9928699135780334}, {"text": "FOUN", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9942395687103271}]}, {"text": "The fourth algorithm \"Ideal\" amounts to evaluating the test on the generic pre-trained word embeddings from the web.", "labels": [], "entities": []}, {"text": "The results on the word similarity task are shown in.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8025960922241211}]}, {"text": "Our method obtains the best performance on four out of five word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7560825745264689}]}, {"text": "Word analogy tests consists of questions of the form \"a is to a* as b is to b*\", where b* must be completed ().", "labels": [], "entities": [{"text": "Word analogy", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7270255535840988}]}, {"text": "We performed such word analogy tests; our results are reported in.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7514387965202332}]}, {"text": "We observe that SOWE outperform FOUN-based methods in two out of four cases.", "labels": [], "entities": [{"text": "SOWE", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.7623549699783325}, {"text": "FOUN-based", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9365100264549255}]}], "tableCaptions": [{"text": " Table 1: Performance on NIPS Abstract and Economic News. The unit of running time is second. We  report the average value of 10 independent runs for FOUN and standard deviation in brackets only for  \"loss for new\". For FOUN, \"loss for all words\" is the sum of \"loss for new words\" and \"loss that are only  related to old words\"(which is a constant). So standard deviation of \"loss for all\" is equal to \"loss for all\".", "labels": [], "entities": [{"text": "NIPS Abstract and Economic News", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.8820006251335144}, {"text": "FOUN", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9423678517341614}, {"text": "FOUN", "start_pos": 220, "end_pos": 224, "type": "DATASET", "confidence": 0.8413550853729248}]}, {"text": " Table 3: Nearest Neighbors of \"submodular\" in extended vocabulary. We measure the distance using  cosine distance.", "labels": [], "entities": []}, {"text": " Table 4: Performance on word similarity task using text8 dataset. \"ideal\" means the matrix factorization  method proposed in Levy and Goldberg (2014). FOUN and FOUN+annealing are the baseline that we  are comparing with.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7898495395978292}, {"text": "FOUN", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9928826093673706}, {"text": "FOUN", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9893524646759033}]}, {"text": " Table 5: Performance on word analogy task using text8 dataset. \"ideal\" means learning the word embedding  with the full extended vocabulary from scratch. Here, the matrix factorization method proposed in", "labels": [], "entities": [{"text": "word analogy", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.773319810628891}]}, {"text": " Table 6: Performance on word analogy task using existing embedding results and text8 dataset with  different folds of splits. In the row \"Ideal\", we use the well-trained word embedding results downloaded  from Internet. FOUN and FOUN+annealing are the baseline that we are comparing with.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8075525561968485}, {"text": "FOUN", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9908223152160645}, {"text": "FOUN", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9801259636878967}]}]}