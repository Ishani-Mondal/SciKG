{"title": [{"text": "The Lifted Matrix-Space Model for Semantic Composition", "labels": [], "entities": [{"text": "Semantic Composition", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7124581038951874}]}], "abstractContent": [{"text": "Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen informal linguistics, and have shown empirical improvements over comparable sequence models by doing so.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7362255156040192}]}, {"text": "Moreover, adding multiplicative interaction terms to the composition functions in these models can yield significant further improvements.", "labels": [], "entities": []}, {"text": "However, existing compositional approaches that adopt such a powerful composition function scale poorly, with parameter counts exploding as model dimension or vocabulary size grows.", "labels": [], "entities": []}, {"text": "We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication.", "labels": [], "entities": []}, {"text": "Its composition function effectively transmits a larger number of activations across layers with relatively few model parameters.", "labels": [], "entities": []}, {"text": "We evaluate our model on the Stanford NLI corpus, the MultiGenre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms, the previous best known composition function for treestructured models.", "labels": [], "entities": [{"text": "Stanford NLI corpus", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9158577919006348}, {"text": "MultiGenre NLI corpus", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.870177686214447}, {"text": "Stanford Sentiment Treebank", "start_pos": 85, "end_pos": 112, "type": "DATASET", "confidence": 0.902784506479899}]}], "introductionContent": [{"text": "Contemporary theoretical accounts of natural language syntax and semantics consistently hold that sentences are tree-structured, and that the meaning of each node in each tree is calculated from the meaning of its child nodes using a relatively simple semantic composition process which is applied recursively bottom-up.", "labels": [], "entities": []}, {"text": "In tree-structured recursive neural networks (, a similar procedure is used to build representations for sentences for use in natural language understanding tasks, with distributed representations for words repeatedly fed through a neural network composition function according to a binary tree structure supplied by a parser.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.7885807752609253}]}, {"text": "The success of a tree-structured model largely depends on the design of its composition function.", "labels": [], "entities": []}, {"text": "It has been repeatedly shown that a composition function that captures multiplicative interactions between the two items being composed yields better results) than do otherwise-equivalent functions based on simple linear interactions.", "labels": [], "entities": []}, {"text": "This paper presents a novel model which advances this line of research, the Lifted Matrix-Space model.", "labels": [], "entities": []}, {"text": "We utilize a tensor-parameterized LIFT layer that learns to produce matrix representations of words that are dependent on the content of pre-trained word embedding vectors.", "labels": [], "entities": []}, {"text": "Composition of two matrix representations is carried out by a composition layer, into which the two matrices are sequentially fed.", "labels": [], "entities": []}, {"text": "Our model was inspired by Continuation Semantics (, where each symbolic representation of words is converted to a higher-order function.", "labels": [], "entities": []}, {"text": "There is a consensus in linguistic semantics that a subset of natural language expressions correspond to higher-order functions.", "labels": [], "entities": []}, {"text": "Inspired by the works in programming language theory, Continuation Semantics takes a step further and claims that all expressions have to be converted into a higher-order function before they participate in semantic composition.", "labels": [], "entities": []}, {"text": "The theory bridges a gap between linguistic semantics and programming language theory, and reinterprets various linguistic phenomena from the view of computation.", "labels": [], "entities": [{"text": "programming language theory", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6541423598925272}]}, {"text": "While we do not directly implement Continuation Semantics, we follow its rough contours: We convert low-level representations (vectors) to higher-order functions (matrices), and composition only takes place between the higher-order functions.", "labels": [], "entities": []}, {"text": "A number of models have been developed to capture the multiplicative interactions between distributed representations.", "labels": [], "entities": []}, {"text": "While having a similar objective, the proposed model requires fewer model parameters than the predecessors because it does not necessarily learn each word matrix representation separately, and the number of parameters for the composition function is not proportional to the cube of the hidden state dimension.", "labels": [], "entities": []}, {"text": "Because of this, it can be trained with larger vocabularies and more hidden state activations than was possible with its predecessors.", "labels": [], "entities": []}, {"text": "We evaluate our model primarily on the task of natural language inference (NLI; MacCartney, 2009).", "labels": [], "entities": [{"text": "NLI; MacCartney, 2009)", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.803387055794398}]}, {"text": "The task consists in determining the inferential relation between a given pair of sentences.", "labels": [], "entities": []}, {"text": "It is a principled and widely-used evaluation task for natural language understanding, and knowing the inferential relations is closely related to understanding the meaning of an expression.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6452365616957346}]}, {"text": "While other tasks such as question answering or machine translation require a model to learn task-specific behavior that goes beyond understanding sentence meaning, NLI results highlight sentence understanding performance in isolation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8389781713485718}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7144486606121063}]}, {"text": "We also include an evaluation on sentiment classification for comparison with some earlier work.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.9659075140953064}]}, {"text": "We find that our model outperforms existing approaches to tree-structured modeling on all three tasks, though it does not set the state of the art on any of them, falling behind other types of complex model.", "labels": [], "entities": []}, {"text": "We nonetheless expect that this method will be a valuable ingredient in future models for sentence understanding and a valuable platform for research of compositionality in learned representations.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7850494384765625}]}], "datasetContent": [{"text": "We first train and test our models on the Stanford Natural Language Inference corpus (SNLI; We test our models on the Multi-Genre Natural Language Inference corpus (MultiNLI;.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference corpus (SNLI", "start_pos": 42, "end_pos": 90, "type": "DATASET", "confidence": 0.8433980686323983}]}, {"text": "The corpus consists of 433k pairs of examples, and each pair is labeled for entailment, contradiction, and neutral.", "labels": [], "entities": []}, {"text": "MultiNLI has the same format as SNLI, so it is possible to train on both datasets at the same time (as we do when testing on MultiNLI).", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9148725271224976}]}, {"text": "Two notable features distinguish MultiNLI from SNLI: (i) It is collected from ten distinct genres of spoken and written English.", "labels": [], "entities": []}, {"text": "This makes the dataset more representative of human language use.", "labels": [], "entities": []}, {"text": "(ii) The examples in MultiNLI are considerably longer than the ones in SNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9176767468452454}]}, {"text": "These two features make MultiNLI classification fairly more difficult than SNLI.", "labels": [], "entities": [{"text": "MultiNLI classification", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7588200271129608}, {"text": "SNLI", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7791850566864014}]}, {"text": "The pair of sentences in is an illustrative example.", "labels": [], "entities": []}, {"text": "The sen-tences are from the section of the corpus that is transcribed verbatim from telephone speech.", "labels": [], "entities": []}, {"text": "The MultiNLI training set consists of five different genres of spoken and written English, the matched test set contains sentence pairs from only those five genres, and the mismatched test set contains sentence pairs from additional genres.", "labels": [], "entities": [{"text": "MultiNLI training set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8284148772557577}]}, {"text": "We also experiment on the Stanford Sentiment Treebank (SST; , which is constructed by extracting movie review excerpts written in English from rottentomatoes.com, and labeling them with Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SST;", "start_pos": 26, "end_pos": 59, "type": "DATASET", "confidence": 0.8588137924671173}]}, {"text": "Each example in SST is paired with a parse tree, and each node of the tree is tagged with a finegrained sentiment label (5 classes).", "labels": [], "entities": [{"text": "SST", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.95396488904953}]}, {"text": "summarizes the results on SNLI and MultiNLI classification.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.7667160630226135}, {"text": "MultiNLI classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.729157418012619}]}, {"text": "We use the same preprocessing steps for all results we report, including loading the parse trees supplied with the datasets.", "labels": [], "entities": []}, {"text": "Dropout rate, size of activations, number and size of MLP layers, and L2 regularization term are tuned using repeated random search.", "labels": [], "entities": []}, {"text": "MV-RNN and RNTN introduced in the earlier sections are extremely expensive in terms of computational resources, and training the models with comparative hyperparameter settings quickly runs out of memory on a high-end GPU.", "labels": [], "entities": [{"text": "RNTN", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7423303723335266}]}, {"text": "We do not include them in the comparison for this reason.", "labels": [], "entities": []}, {"text": "TreeLSTM performs the best with one MLP layer, while LMS-LSTM displays the best performance with two MLP layers.", "labels": [], "entities": [{"text": "TreeLSTM", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8162403106689453}]}, {"text": "The difference in parameter count is largely affected by this choice, and in principle one model does not demand notably more computational resources than the other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on NLI classification with sentence-to-vector encoders. Params. is the approximate number of  model parameters, and the numbers in parentheses indicate the parameters contributed by word embeddings. S tr.,  and S te. are the class accuracies (%) on SNLI train set and test set, respectively. M tr., M te. mat., and M te.  mism. are the class accuracies (%) on MultiNLI train set, matched test set, and mismatched test set, respectively.  Underlining marks the best result among tree-structured models.", "labels": [], "entities": [{"text": "NLI classification", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8593290746212006}, {"text": "SNLI train set", "start_pos": 267, "end_pos": 281, "type": "DATASET", "confidence": 0.8232913215955099}, {"text": "MultiNLI train set", "start_pos": 378, "end_pos": 396, "type": "DATASET", "confidence": 0.9367846647898356}]}, {"text": " Table 3: MultiNLI development set classification ac- curacies (%), classified using the tags introduced in  Williams et al. (2017).", "labels": [], "entities": [{"text": "MultiNLI development set classification ac- curacies", "start_pos": 10, "end_pos": 62, "type": "DATASET", "confidence": 0.8293055721691677}]}, {"text": " Table 4: Five-way test set classification accuracies (%)  on the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9456576108932495}]}, {"text": " Table 5: Syntactic category distribution of SNLI de- velopment set, classified using the tags introduced in  Bowman et al. (2015).", "labels": [], "entities": [{"text": "SNLI de- velopment", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7894705235958099}]}, {"text": " Table 6: Syntactic category classification accuracies  (%) on SNLI development set, classified using the tags  introduced in Bowman et al. (2015).", "labels": [], "entities": [{"text": "Syntactic category classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.726343552271525}, {"text": "SNLI development set", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.8458889524141947}]}]}