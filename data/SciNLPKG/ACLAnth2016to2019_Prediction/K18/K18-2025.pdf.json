{"title": [{"text": "AntNLP at CoNLL 2018 Shared Task: A Graph-based Parser for Universal Dependency Parsing", "labels": [], "entities": [{"text": "AntNLP", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9670708179473877}, {"text": "CoNLL 2018 Shared Task", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.8485576808452606}, {"text": "Universal Dependency Parsing", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.5401483774185181}]}], "abstractContent": [{"text": "We describe the graph-based dependency parser in our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.7764451265335083}]}, {"text": "We use bidirectional lstm to get the word representation , then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree.", "labels": [], "entities": []}, {"text": "From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26).", "labels": [], "entities": [{"text": "LAS F1 score", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.8658776879310608}, {"text": "MLAS", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.6701130867004395}, {"text": "BLEX", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9991662502288818}]}], "introductionContent": [{"text": "The focus of the CoNLL 2018 UD Shared Task is learning syntactic dependency parsers that can work over many typologically different languages, even low-resource languages for which there is little or no training data.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.7172301650047302}, {"text": "learning syntactic dependency parsers", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.6645473539829254}]}, {"text": "The Universal Dependencies ( treebank collection has 82 treebanks over 57 kinds of languages.", "labels": [], "entities": [{"text": "Universal Dependencies ( treebank collection", "start_pos": 4, "end_pos": 48, "type": "DATASET", "confidence": 0.842716121673584}]}, {"text": "In this paper we describe our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task.", "labels": [], "entities": [{"text": "AntNLP", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9420103430747986}, {"text": "CoNLL 2018 UD Shared Task", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.8294813871383667}]}, {"text": "Our system is based on the deep biaffine neural dependency parser.", "labels": [], "entities": [{"text": "deep biaffine neural dependency parser", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.5820269465446473}]}, {"text": "The system contains a BiLSTM feature extractor forgetting context-aware word representation and two biaffine classifiers to predict the head token of each word and the label between ahead and its dependent.", "labels": [], "entities": []}, {"text": "There are three main metrics for this task, LAS (labeled attachment score), MLAS (morphologyaware labeled attachment score) and BLEX (bilexical dependency score).", "labels": [], "entities": [{"text": "LAS (labeled attachment score)", "start_pos": 44, "end_pos": 74, "type": "METRIC", "confidence": 0.7502937912940979}, {"text": "MLAS (morphologyaware labeled attachment score)", "start_pos": 76, "end_pos": 123, "type": "METRIC", "confidence": 0.6324752441474369}, {"text": "BLEX", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9964274764060974}]}, {"text": "From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX.", "labels": [], "entities": [{"text": "LAS F1 score", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.8625911871592203}, {"text": "MLAS", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.6558858156204224}, {"text": "BLEX", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9984954595565796}]}, {"text": "Ina word, Our system is ranked top 10 according to the three metrics described above.", "labels": [], "entities": []}, {"text": "Additionally, in the categories of small treebanks, our system obtains the sixth place with a MLAS score of 63.73.", "labels": [], "entities": [{"text": "MLAS score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.7144569456577301}]}, {"text": "Besides that, our system ranked tenth in the EPE 2018 campaign with a 55.71 F1 score.", "labels": [], "entities": [{"text": "EPE 2018 campaign", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.8381228446960449}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9781080186367035}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief description of our overall system, including the system framework and parser architecture.", "labels": [], "entities": []}, {"text": "In Section 3, 4 we describe our monolingual model and multilingual model.", "labels": [], "entities": []}, {"text": "In Section 5, we briefly list our experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our system based on a Nvidia GeForce GTX Titan X. We used the official TIRA () to evaluate the system.", "labels": [], "entities": [{"text": "Nvidia GeForce GTX Titan X.", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8365066409111023}, {"text": "TIRA", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.968689501285553}]}, {"text": "We used Dynet neural network library to build our system ().", "labels": [], "entities": []}, {"text": "The hyperparameters of the final system used for all the reported experiments are detailed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus listed above are languages that don't have development set and the training set size is too  small to be fine-tuned. \"#Train\" means the number of sentences. \"Cross Language\" means the language  with the highest LAS score for corresponding origin language in our delexicalized cross-language model.", "labels": [], "entities": [{"text": "LAS score", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9745264053344727}]}, {"text": " Table 2: Corpus listed above are languages that don't have development set. Because the training set  size is much bigger, we decide to divide the training set into two parts, one for training set and the other  for development set.", "labels": [], "entities": []}, {"text": " Table 3: language families and genera for origin language and cross language (IE = Indo-European). 2", "labels": [], "entities": []}, {"text": " Table 4: Official experiment results with rank. (number): number of corpora. FLAS means F1 score of  LAS.", "labels": [], "entities": [{"text": "FLAS", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9840317666530609}, {"text": "LAS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8702706098556519}]}, {"text": " Table 5: Hyper-parameter values used in shared  task.", "labels": [], "entities": []}]}