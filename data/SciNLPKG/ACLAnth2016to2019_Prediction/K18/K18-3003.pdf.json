{"title": [{"text": "IPS-WASEDA system at CoNLL-SIGMORPHON 2018 Shared Task on morphological inflection", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8770689964294434}, {"text": "morphological inflection", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7387892305850983}]}], "abstractContent": [{"text": "This paper presents the system submitted by IPS-WASEDA University for CoNLL-SIGMORPHON 2018 Shared Task 1: Type level inflection.", "labels": [], "entities": [{"text": "IPS-WASEDA University", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.8954890072345734}]}, {"text": "We develop a system based on a holistic approach which considers whole-word form as a unit, instead of breaking them into smaller pieces (e,g. morphemes) like the baseline systems does.", "labels": [], "entities": []}, {"text": "We also implement an encoder-decoder model which has recently become the new standard in many natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "The results show that the neural approach outperforms the baseline and our holistic approach on bigger resources settings.", "labels": [], "entities": []}, {"text": "The use of data augmentation helps to improve the performance of the model in lower resources settings, although it still cannot beat the other systems.", "labels": [], "entities": []}, {"text": "In the end, for the low resources setting, our holistic approach performs best in comparison to the baseline and the neural approach (even with data augmentation).", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemma: illustrate Target MSDs: V;V.PTCP;PRS Target form: illustrating given the lemma illustrate, we are asked to generate the present participle form illustrating.", "labels": [], "entities": []}, {"text": "We address the problem of inflection task: given a lemma (e.g. the dictionary form of a word) and the target form's morphosyntactic descriptions (MSD), generate a target inflected form.", "labels": [], "entities": []}, {"text": "shows an example of inflection task in English.", "labels": [], "entities": []}, {"text": "Many NLP tasks, like machine translation, require analysis and generation of morphological word forms, even previously unseen ones.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7943819165229797}]}, {"text": "Different languages exhibit different richness of morphology.", "labels": [], "entities": []}, {"text": "This makes the task an interesting problem.", "labels": [], "entities": []}, {"text": "show that data sparsity is a common issue for language with rich morphology which usually leads to poor generalisations in machine learning.", "labels": [], "entities": []}, {"text": "There are three main approaches at the problem: \u2022 The hand-engineered rule-based approach offers a high accuracy but costs time during construction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9983824491500854}]}, {"text": "It usually faces the world coverage problem and is language-dependent.", "labels": [], "entities": [{"text": "world coverage", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.6554618179798126}]}, {"text": "\u2022 The supervised approach automatically induces the rules from a given training data and applies the best rules to generate the target forms by using some classification techniques (.", "labels": [], "entities": []}, {"text": "It is practically language independent and relatively easier to build.", "labels": [], "entities": []}, {"text": "However, the data sparsity is an issue.", "labels": [], "entities": []}, {"text": "\u2022 The neural approach is the model which triumphed in the task recently, especially the RNN encoder-decoder model (.", "labels": [], "entities": []}, {"text": "Some drawbacks of this approach are very long training times and the need fora large amount of training data.", "labels": [], "entities": []}, {"text": "This paper describes the systems we developed for the CoNLL-SIGMORPHON 2018 Shared Task 1 (.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 Shared Task 1", "start_pos": 54, "end_pos": 89, "type": "DATASET", "confidence": 0.8020130872726441}]}, {"text": "The recent success of neural approach encouraged us to implement a sequence-to-sequence (seq2seq) model to solve the task.", "labels": [], "entities": []}, {"text": "Knowing that the neural approach tends to need a large amount of training data, we also consider another approach as a back-off, which is a holistic approach.", "labels": [], "entities": []}, {"text": "We treat the task of generating target forms as the task of solving analogical equations between words.", "labels": [], "entities": []}, {"text": "The languages vary from Germanic, Celtic and Slavic languages, which are mainly used in Europe, to Indo-Aryan, Iranian, etc.", "labels": [], "entities": []}, {"text": "The dataset consists of lines of triplet.", "labels": [], "entities": []}, {"text": "A triplet consists of a lemma, a target form, and a target MSD pattern separated by tabulation characters.", "labels": [], "entities": []}, {"text": "The MSDs are morphological tags presented in Unimorph Schema (.", "labels": [], "entities": []}, {"text": "The provided resources are categorized into:", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the systems using average on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9994120597839355}]}, {"text": "Accuracy is the ratio of correctly predicted target forms by the total number of questions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9949479103088379}]}, {"text": "Please refer to Formula 3 for the exact definition . We carry experiments using training dataset and measure the accuracies on dev dataset for all the languages for all training dataset sizes.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9891353845596313}]}, {"text": "The system which has the highest score will be picked as our representative system in the test phase for that particular language and dataset size.", "labels": [], "entities": []}, {"text": "shows the average accuracy in all languages for each of the systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.999526858329773}]}, {"text": "Our holistic approach is able to perform as good as the baseline system, even slightly better under all of the three dataset sizes.", "labels": [], "entities": []}, {"text": "This is the same observation found in) on previous year dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the dataset given. Number of rules and unseen rules are based on rule extraction method  explained in Section 5.3.1.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.6816667318344116}]}, {"text": " Table 1. Overall, we can observe  a non-decreasing phenomenon from low to high  for all of the number of pieces of information (fea- tures) found in the training dataset. On the oppo- site, we found a non-increasing pattern for the un- seen information contained in the dev dataset rel- atively to training dataset. This shows that bigger  resources gradually cover the unseen data encoun- tered in the smaller ones.", "labels": [], "entities": [{"text": "training dataset", "start_pos": 299, "end_pos": 315, "type": "DATASET", "confidence": 0.7543516457080841}]}, {"text": " Table 2: Average accuracy scores on dev dataset.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9330692291259766}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9043532609939575}, {"text": "dev dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.6604336351156235}]}, {"text": " Table 3: Accuracy scores on development set (dev) in each language for baseline system (B), holistic approach(H),  our seq2seq model without data augmentation (S) and with data augmentation (S+Aug).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990257024765015}]}]}