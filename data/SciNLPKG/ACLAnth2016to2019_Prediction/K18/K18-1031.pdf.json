{"title": [{"text": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "labels": [], "entities": [{"text": "Sentence-Level Fluency Evaluation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7738291223843893}]}], "abstractContent": [{"text": "Motivated by recent findings on the proba-bilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level.", "labels": [], "entities": [{"text": "syntactic log-odds ratio (SLOR)", "start_pos": 99, "end_pos": 130, "type": "METRIC", "confidence": 0.8026774078607559}]}, {"text": "We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact language model.", "labels": [], "entities": [{"text": "WPSLOR", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.5665843486785889}, {"text": "WordPiece-based", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9159303903579712}]}, {"text": "Even though word-overlap metrics like ROUGE are computed with the help of handwritten references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9648310542106628}]}, {"text": "Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references.", "labels": [], "entities": [{"text": "ROUGE-LM", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9941242337226868}, {"text": "WPSLOR", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.6518737077713013}]}, {"text": "We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.", "labels": [], "entities": [{"text": "ROUGE-LM", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9909728765487671}, {"text": "WPSLOR", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.8191779255867004}]}], "introductionContent": [{"text": "Producing sentences which are perceived as natural by a human addressee-a property which we will denote as fluency 1 throughout this paper -is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 165, "end_pos": 198, "type": "TASK", "confidence": 0.8340880970160166}]}, {"text": "Thus, fluency evaluation is important, e.g., during system development, or * *This research was carried out while the first author was interning at Google.", "labels": [], "entities": [{"text": "fluency evaluation", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7337217330932617}]}, {"text": "Alternative names include naturalness, grammaticality or readability.", "labels": [], "entities": []}, {"text": "Note that the exact definitions of all those terms vary slightly throughout the literature.", "labels": [], "entities": []}, {"text": "If access to a synonym dictionary is likely to be of use, then this package may 3 be of service.", "labels": [], "entities": []}, {"text": "Participants are invited to submit a set pair do domain name that is already 1.6 taken along with alternative.", "labels": [], "entities": []}, {"text": "Even $15 was The HSUS.", "labels": [], "entities": [{"text": "The HSUS", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.7100411951541901}]}, {"text": "1: Example compressions from our dataset with their fluency scores; scores in, higher is better.", "labels": [], "entities": []}, {"text": "for filtering unacceptable generations at application time.", "labels": [], "entities": []}, {"text": "However, fluency evaluation of NLG systems constitutes a hard challenge: systems are often not limited to reusing words from the input, but can generate in an abstractive way.", "labels": [], "entities": []}, {"text": "Hence, it is not guaranteed that a correct output will match any of a finite number of given references.", "labels": [], "entities": []}, {"text": "This results in difficulties for current reference-based evaluation, especially of fluency, causing wordoverlap metrics like ROUGE () to correlate only weakly with human judgments (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9909051656723022}]}, {"text": "As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming.", "labels": [], "entities": [{"text": "fluency evaluation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8318768441677094}]}, {"text": "Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science.", "labels": [], "entities": []}, {"text": "In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature.", "labels": [], "entities": []}, {"text": "Within this context, have recently shown that neural lan-guage models (LMs) can be used for modeling human ratings of acceptability.", "labels": [], "entities": []}, {"text": "Namely, they found SLOR ()-sentence logprobability which is normalized by unigram logprobability and sentence length-to correlate well with acceptability judgments at the sentence level.", "labels": [], "entities": [{"text": "SLOR", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9663646817207336}]}, {"text": "However, to the best of our knowledge, these insights have so far gone disregarded by the natural language processing (NLP) community.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the practical implications of's findings for fluency evaluation of NLG, using the task of automatic compression) as an example (cf.).", "labels": [], "entities": [{"text": "fluency evaluation", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8089773058891296}]}, {"text": "Specifically, we test our hypothesis that SLOR should be a suitable metric for evaluation of compression fluency which (i) does not rely on references; (ii) can naturally be applied at the sentence level (in contrast to the system level); and (iii) does not need human fluency annotations of any kind.", "labels": [], "entities": []}, {"text": "In particular the first aspect, i.e., SLOR not needing references, makes it a promising candidate for automatic evaluation.", "labels": [], "entities": [{"text": "SLOR", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7813737392425537}]}, {"text": "Getting rid of human references has practical importance in a variety of settings, e.g., if references are unavailable due to alack of resources for annotation, or if obtaining references is impracticable.", "labels": [], "entities": []}, {"text": "The latter would be the case, for instance, when filtering system outputs at application time.", "labels": [], "entities": []}, {"text": "We further introduce WPSLOR, a novel, WordPiece ()-based version of SLOR, which drastically reduces model size and training time.", "labels": [], "entities": [{"text": "WordPiece", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9107788801193237}]}, {"text": "Our experiments show that both approaches correlate better with human judgments than traditional word-overlap metrics, even though the latter do rely on reference compressions.", "labels": [], "entities": []}, {"text": "Finally, investigating the case of available references and how to incorporate them, we combine WPSLOR and ROUGE to ROUGE-LM, a novel reference-based metric, and increase the correlation with human fluency ratings even further.", "labels": [], "entities": [{"text": "WPSLOR", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.889001727104187}, {"text": "ROUGE", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9902626276016235}, {"text": "ROUGE-LM", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9550691246986389}]}, {"text": "To summarize, we make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We empirically show that SLOR is a good referenceless metric for the evaluation of NLG fluency at the sentence level.", "labels": [], "entities": [{"text": "SLOR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8596510291099548}]}, {"text": "2. We introduce WPSLOR, a WordPiece-based version of SLOR, which disposes of a more compact LM without a significant loss of performance.", "labels": [], "entities": []}, {"text": "3. We propose ROUGE-LM, a reference-based metric, which achieves a significantly higher correlation with human fluency judgments than all other metrics in our experiments.", "labels": [], "entities": [{"text": "ROUGE-LM", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9907763600349426}]}], "datasetContent": [{"text": "Now, we present our main experiment, in which we assess the performances of WordSLOR and WPSLOR as fluency evaluation metrics.", "labels": [], "entities": [{"text": "WordSLOR", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9563701748847961}, {"text": "WPSLOR", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.8224098086357117}]}, {"text": "We experiment on the compression dataset by.", "labels": [], "entities": []}, {"text": "It contains single sentences and two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction.", "labels": [], "entities": [{"text": "Open American National Corpus (OANC)", "start_pos": 66, "end_pos": 102, "type": "DATASET", "confidence": 0.8055037345205035}]}, {"text": "Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. for details) for the test data are provided.", "labels": [], "entities": [{"text": "NAMAS", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.6893124580383301}]}, {"text": "Each example has 3 to 5 independent human ratings for content and fluency.", "labels": [], "entities": []}, {"text": "We are interested in the latter, which is rated on an ordinal scale from 1 (disfluent) through 3 (fluent).", "labels": [], "entities": []}, {"text": "We experiment on the 2955 system outputs for the test split.", "labels": [], "entities": []}, {"text": "Average fluency scores per system are shown in.", "labels": [], "entities": []}, {"text": "As can be seen, ILP produces the best output.", "labels": [], "entities": []}, {"text": "In contrast, NAMAS is the worst system for fluency.", "labels": [], "entities": [{"text": "NAMAS", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6563001275062561}]}, {"text": "In order to be able to judge the reliability of the human annotations, we follow the procedure suggested by and used by, and compute the quadratic weighted \u03ba for the human fluency scores of the system-generated compressions as 0.337.", "labels": [], "entities": []}, {"text": "Following earlier work (, we evaluate our metrics using Pearson correlation with human judgments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.8149869740009308}]}, {"text": "It is defined as the covariance divided by the product of the standard deviations: Mean squared error.", "labels": [], "entities": [{"text": "Mean squared error", "start_pos": 83, "end_pos": 101, "type": "METRIC", "confidence": 0.9842068155606588}]}, {"text": "Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model.", "labels": [], "entities": []}, {"text": "Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size: with f being a linear function.", "labels": [], "entities": [{"text": "mean squared error (MSE)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.9621695578098297}]}, {"text": "Note that, since MSE is invariant to linear transformations of X but not of Y , it is a non-symmetric quasi-metric.", "labels": [], "entities": []}, {"text": "We apply it with Y being the human ratings.", "labels": [], "entities": []}, {"text": "An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating.: Pearson correlation (higher is better) and MSE (lower is better) for all metrics; best results in bold; refs=number of references used to compute the metric.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 164, "end_pos": 183, "type": "METRIC", "confidence": 0.9445190727710724}, {"text": "MSE", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.9932612776756287}]}, {"text": "The results per compression system (cf.) look different from the correlations in: Pearson and MSE are both lower.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9089341759681702}, {"text": "MSE", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.7860124707221985}]}, {"text": "This is due to the outputs of each given system being of comparable quality.", "labels": [], "entities": []}, {"text": "Therefore, the datapoints are similar and, thus, easier to fit for the linear function used for MSE.", "labels": [], "entities": [{"text": "MSE", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.6237608194351196}]}, {"text": "Pearson, in contrast, is lower due to its invariance to linear transformations of both variables.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8533309102058411}]}, {"text": "Note that this effect is smallest for ILP, which has uniformly distributed targets (Var(Y ) = 0.35 vs. Var(Y ) = 0.17 for SEQ2SEQ).", "labels": [], "entities": [{"text": "Var(Y )", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9527654647827148}, {"text": "Var(Y )", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.912929430603981}]}, {"text": "Comparing the metrics, the two SLOR approaches perform best for SEQ2SEQ and T3.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.6637597680091858}]}, {"text": "In particular, they outperform the best word-overlap metric baseline by 0.244 and 0.097 Pearson correlation as well as 0.012 and 0.012 MSE, respectively.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 88, "end_pos": 107, "type": "METRIC", "confidence": 0.9659917056560516}, {"text": "MSE", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9728749990463257}]}, {"text": "Since T3 is an abstractive system, we can conclude that WordSLOR and WPSLOR are applicable even for systems that are not limited to make use of a fixed repertoire of words.", "labels": [], "entities": [{"text": "WordSLOR", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.920982301235199}, {"text": "WPSLOR", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.6964317560195923}]}, {"text": "For ILP and NAMAS, word-overlap metrics obtain best results.", "labels": [], "entities": [{"text": "NAMAS", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.8105828762054443}]}, {"text": "The differences in performance, however, are with a maximum difference of 0.072 for Pearson and ILP much smaller than for SEQ2SEQ.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.6522907614707947}, {"text": "SEQ2SEQ", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.8503089547157288}]}, {"text": "Thus, while the differences are significant, word-overlap metrics do not outperform our SLOR approaches by a wide margin.", "labels": [], "entities": []}, {"text": "Recall, additionally, that word-overlap metrics rely on references being available, while our proposed approaches do not require this.", "labels": [], "entities": []}, {"text": "Looking next at the correlations for all models but different domains (cf.: Pearson correlation (higher is better) and MSE (lower is better), reported by compression system; best results in bold; refs=number of references used to compute the metric.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.967961996793747}, {"text": "MSE", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9595707654953003}]}, {"text": "Next, we focus on an important question: How much does the performance of our SLOR-based metrics depend on the domain, given that the respective LMs are trained on Gigaword, which consists of news data?", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 164, "end_pos": 172, "type": "DATASET", "confidence": 0.9325860738754272}]}, {"text": "Comparing the evaluation performance for individual metrics, we observe that, except for letters, WordSLOR and WPSLOR perform best across all domains: they outperform the best word-overlap metric by at least 0.019 and at most 0.051 Pearson correlation, and at least 0.004 and at most 0.014 MSE.", "labels": [], "entities": [{"text": "WordSLOR", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.8829641342163086}, {"text": "WPSLOR", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.7390311360359192}, {"text": "Pearson correlation", "start_pos": 232, "end_pos": 251, "type": "METRIC", "confidence": 0.976539820432663}, {"text": "MSE", "start_pos": 290, "end_pos": 293, "type": "METRIC", "confidence": 0.9093036651611328}]}, {"text": "The biggest difference in correlation is achieved for the journal domain.", "labels": [], "entities": [{"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9917058944702148}]}, {"text": "Thus, clearly even LMs which have been trained on out-ofdomain data obtain competitive performance for fluency evaluation.", "labels": [], "entities": [{"text": "fluency evaluation", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7760904431343079}]}, {"text": "However, a domain-specific LM might additionally improve the metrics' correlation with human judgments.", "labels": [], "entities": []}, {"text": "We leave a more detailed analysis of the importance of the training data's domain for future work.", "labels": [], "entities": []}, {"text": "First, we assume a setting in which we have the following available: (i) system outputs whose fluency is to be evaluated, (ii) reference generations for evaluating system outputs, (iii) a small set of system outputs with references, which has been annotated for fluency by human raters, and (iv) a large unlabeled corpus for training a LM.", "labels": [], "entities": []}, {"text": "Note that available fluency annotations are often uncommon in real-world scenarios; the reason we use them is that they allow fora proof of concept.", "labels": [], "entities": []}, {"text": "In this setting, we train scikit's (Pedregosa et al., 2011) support vector regression model (SVR) with the default parameters on predicting fluency, given WP-SLOR and ROUGE-L-mult.", "labels": [], "entities": [{"text": "ROUGE-L-mult", "start_pos": 167, "end_pos": 179, "type": "METRIC", "confidence": 0.9582534432411194}]}, {"text": "We use 500 of our total 2955 examples for each of training and development, and the remaining 1955 for testing.", "labels": [], "entities": []}, {"text": "Second, we simulate a setting in which we have only access to (i) system outputs which should be evaluated on fluency, (ii) reference compressions, and (iii) large amounts of unlabeled text.", "labels": [], "entities": []}, {"text": "In particular, we assume to not have fluency ratings for system outputs, which makes training a regression model impossible.", "labels": [], "entities": []}, {"text": "Note that this is the standard setting in which word-overlap metrics are applied.", "labels": [], "entities": []}, {"text": "Under these conditions, we propose to normalize both given scores by mean and variance, and to simply add them up.", "labels": [], "entities": []}, {"text": "We call this new reference-: Sentences for which raters were unsure if they were perceived as problematic due to fluency or content issues, together with the model which generated them.", "labels": [], "entities": []}, {"text": "In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples.", "labels": [], "entities": []}, {"text": "Fluency evaluation is related to grammatical error detection; Liu and Liu, 2017) and grammatical error correction (.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.670917292435964}, {"text": "grammatical error correction", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6618752578894297}]}, {"text": "However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans.", "labels": [], "entities": []}, {"text": "Work on automatic fluency evaluation in NLP has been rare.", "labels": [], "entities": [{"text": "automatic fluency evaluation", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.5342180132865906}]}, {"text": "predicted the fluency (which they called grammaticality) of sentences written by English language learners.", "labels": [], "entities": []}, {"text": "In contrast to ours, their approach is supervised. and found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively.", "labels": [], "entities": []}, {"text": "Explicit fluency evaluation of NLG, including compression and the related task of summarization, has mostly been performed manually.", "labels": [], "entities": [{"text": "summarization", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9723557233810425}]}, {"text": "used LMs for the evaluation of summarization fluency, but their models were based on partof-speech tags, which we do not require, and they were non-neural.", "labels": [], "entities": [{"text": "summarization fluency", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.9482982158660889}]}, {"text": "Further, they evaluated longer texts, not single sentences like we do. compared 80 word-overlap metrics for evaluating the content and fluency of compressions, finding only low correlation with the latter.", "labels": [], "entities": []}, {"text": "However, they did not propose an alternative evaluation.", "labels": [], "entities": []}, {"text": "We aim at closing this gap.", "labels": [], "entities": []}, {"text": "Automatic compression evaluation has mostly had a strong focus on content.", "labels": [], "entities": [{"text": "Automatic compression evaluation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8530263900756836}]}, {"text": "Hence, word-overlap metrics like ROUGE () have been widely used for compression evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9850121736526489}, {"text": "compression evaluation", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.8562198281288147}]}, {"text": "However, they have certain shortcomings, e.g., they correlate best for extractive compression, while we, in contrast, are interested in an approach which generalizes to abstractive systems.", "labels": [], "entities": [{"text": "extractive compression", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.6941717863082886}]}, {"text": "Alternatives include success rate), simple accuracy (), which is based on the edit distance between the generation and the reference, or word accuracy (), the equivalent for multiple references.", "labels": [], "entities": [{"text": "simple", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9452366828918457}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.6935167908668518}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.5021269917488098}]}], "tableCaptions": [{"text": " Table 4: Pearson correlation (higher is better) and MSE (lower is better), reported by compression system; best  results in bold; refs=number of references used to compute the metric.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9346697926521301}, {"text": "MSE", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9905010461807251}]}, {"text": " Table 5: Pearson correlation (higher is better) and MSE (lower is better), reported by domain of the original  sentence or paragraph; best results in bold; refs=number of references used to compute the metric.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9337964355945587}, {"text": "MSE", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9833973050117493}]}, {"text": " Table 6: Sentences for which raters were unsure if they were perceived as problematic due to fluency or content  issues, together with the model which generated them.", "labels": [], "entities": []}, {"text": " Table 7: Combinations; all differences except for 3 and  4 are statistically significant; refs=number of references  used to compute the metric; ROUGE=ROUGE-L-mult;  best results in bold.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9966320395469666}, {"text": "ROUGE-L-mult", "start_pos": 152, "end_pos": 164, "type": "METRIC", "confidence": 0.9530733823776245}]}, {"text": " Table 7. First, we can see  that using SVR (line 1) to combine ROUGE-L- mult and WPSLOR outperforms both individual  scores (lines 3-4) by a large margin. This serves  as a proof of concept: the information contained  in the two approaches is indeed complementary.  Next, we consider the setting where only refer- ences and no annotated examples are available. In", "labels": [], "entities": [{"text": "ROUGE-L- mult", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.941811998685201}, {"text": "WPSLOR", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9300816059112549}]}]}