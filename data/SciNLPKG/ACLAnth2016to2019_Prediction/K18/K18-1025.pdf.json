{"title": [{"text": "Multi-modal Sequence Fusion via Recursive Attention for Emotion Recognition", "labels": [], "entities": [{"text": "Multi-modal Sequence Fusion", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6754932403564453}, {"text": "Emotion Recognition", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7813923954963684}]}], "abstractContent": [{"text": "Natural human communication is nuanced and inherently multi-modal.", "labels": [], "entities": []}, {"text": "Humans possess spe-cialised sensoria for processing vocal, visual, and linguistic, and para-linguistic information, but form an intricately fused percept of the multi-modal data stream to provide a holistic representation.", "labels": [], "entities": []}, {"text": "Analysis of emotional content in face-to-face communication is a cognitive task to which humans are particularly attuned, given its sociological importance, and poses a difficult challenge for machine emulation due to the subtlety and expressive variability of cross-modal cues.", "labels": [], "entities": [{"text": "Analysis of emotional content in face-to-face communication", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8601415838514056}]}, {"text": "Inspired by the empirical success of recent so-called End-To-End Memory Networks (Sukhbaatar et al., 2015), we propose an approach based on recursive multi-attention with a shared external memory updated over multiple gated iterations of analysis.", "labels": [], "entities": []}, {"text": "We evaluate our model across several large multi-modal datasets and show that global contextu-alised memory with gated memory update can effectively achieve emotion recognition.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.8760259449481964}]}], "introductionContent": [{"text": "Multi-modal sequential data pose interesting challenges for learning machines that seek to derive representations.", "labels": [], "entities": []}, {"text": "This constitutes an increasingly relevant sub-field of multi-view learning.", "labels": [], "entities": []}, {"text": "Examples of such modalities include visual, audio and textual data.", "labels": [], "entities": []}, {"text": "Uni-modal observations are typically complementary to each other and hence they can reveal a fuller and more context-rich picture with better generalisation ability when used together.", "labels": [], "entities": []}, {"text": "Through its complementary perspective, each view can unburden sub-modules specific to another modality of some of its modelling onus, which might otherwise learn implicit hidden * Equal contribution.", "labels": [], "entities": []}, {"text": "causes that are over-fitted to training data idiosyncrasies in order to explain the training labels.", "labels": [], "entities": []}, {"text": "On the other hand, multi-modal data introduces many difficulties to model designing and training due to the distinct inherent dynamics of each modality.", "labels": [], "entities": []}, {"text": "For instance, combining modalities with different temporal resolution is an open problem.", "labels": [], "entities": []}, {"text": "Other challenges include deciding where and how modalities are combined, leveraging the weak discriminative power of training label and the presence of variability and noise or dealing with complex situations such as modelling the emotion of sarcasm, where cues among modalities contradict.", "labels": [], "entities": [{"text": "modelling the emotion of sarcasm", "start_pos": 217, "end_pos": 249, "type": "TASK", "confidence": 0.6717178583145141}]}, {"text": "In this paper, we address multi-modal sequence fusion for automatic emotion recognition.", "labels": [], "entities": [{"text": "multi-modal sequence fusion", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6373835901419321}, {"text": "automatic emotion recognition", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.6786099572976431}]}, {"text": "We believe, that a strong model should enable: (i) Specialisation of modality-specific submodules exploiting the inherent properties of its data stream, tapping into the mode-specific dynamics and characteristic patterns.", "labels": [], "entities": []}, {"text": "(ii) Weak (soft) data alignment dividing heterogeneous sequences into segments with co-occuring events across modalities without alignment to a common time axis.", "labels": [], "entities": []}, {"text": "This overcomes limitations of hard alignments which often introduce spurious modelling assumptions and data inefficiencies (e.g. re-sampling) which must be performed again from scratch if views are added or removed.", "labels": [], "entities": []}, {"text": "(iii) Information exchange for both view-specific information and statistical strength for learning shared representations.", "labels": [], "entities": [{"text": "Information exchange", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.7887336909770966}]}, {"text": "(iv) Scalability of the approach to many modalities using (a) parallelisable computation over modalities, and (b) a parameter set size growing (at most) linearly with the number of modalities.", "labels": [], "entities": []}, {"text": "In the present work, we detail a recursively attentive modelling approach.", "labels": [], "entities": []}, {"text": "Our model fulfills the desiderata above and performs multiple sweeps of globally-contextualised analysis so that one modality-specific representation cues the at-tention of the next and vice-versa.", "labels": [], "entities": []}, {"text": "We evaluate our approach on three large-scale multi-modal datasets to verify its suitability.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach on CREMA-D (), RAVDESS (Livingstone and Russo, 2012) and CMU-MOSEI () datasets for multimodal emotion analysis.", "labels": [], "entities": [{"text": "RAVDESS", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.6365034580230713}, {"text": "CMU-MOSEI () datasets", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.7894912958145142}, {"text": "multimodal emotion analysis", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6451627016067505}]}, {"text": "The first two datasets provide audio and visual modalities while CMU-MOSEI adds also text transcriptions.", "labels": [], "entities": []}, {"text": "The CREMA-D dataset contains \u223c7400 clips of 91 actors covering 6 emotions.", "labels": [], "entities": [{"text": "CREMA-D dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.841987669467926}]}, {"text": "The RAVDESS is a speech and song database comprising of \u223c7300 files of 24 actors covering 8 emotional classes (including two canonical classes for \"neutral\" and \"calm\").", "labels": [], "entities": []}, {"text": "The CMU-MOSEI dataset consists of \u223c3300 long clips segmented into \u223c23000 short clips.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9580923914909363}]}, {"text": "In addition to audio and visual data, it contains also text transcriptions allowing evaluation of tri-modal models.", "labels": [], "entities": []}, {"text": "These datasets are annotated by a continuousvalued vector corresponding to multi-class emotion labels.", "labels": [], "entities": []}, {"text": "The ground-truth labels were generated by multiple human transcribers with score normalisation and agreement analysis.", "labels": [], "entities": []}, {"text": "For further details, refer to respective references.", "labels": [], "entities": []}, {"text": "Since each dataset consists of different emotion classification schema, we trained and evaluated all models separately for each of them.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7555843889713287}]}, {"text": "The training was performed in an end-to-end manner with L2 loss defined over multi-class emotion labels.", "labels": [], "entities": []}, {"text": "To establish a baseline, we evaluated a naive classifier predicting the test-set empirical mean intensities (with MSE loss function) for each output regression dimension.", "labels": [], "entities": [{"text": "MSE loss function", "start_pos": 114, "end_pos": 131, "type": "METRIC", "confidence": 0.885144829750061}]}, {"text": "Similar baselines were obtained for other loss functions by training a model with just one parameter per output dimension on that loss, where the model has an access to the training labels but not the training inputs.", "labels": [], "entities": []}, {"text": "For CREMA-D and RAVDESS, we report the accuracy scores as these datasets contain labels for multiclass classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9993211030960083}, {"text": "multiclass classification task", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.7300546665986379}]}, {"text": "For CMU-MOSEI, we report the result of the 6-way emotion recognition.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9149138331413269}, {"text": "emotion recognition", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6867684125900269}]}, {"text": "Recursive models as described in Sec.", "labels": [], "entities": []}, {"text": "3 predicted the 6-dimensional emotion vectors.", "labels": [], "entities": []}, {"text": "Their values represent the emotion intensity of the six emotion classes and are continuous-valued.", "labels": [], "entities": []}, {"text": "Following, these predictions were evaluated against the reference emotions using the criteria of mean square error (MSE) and mean absolute error (MAE), summing across 6 classes.", "labels": [], "entities": [{"text": "mean square error (MSE)", "start_pos": 97, "end_pos": 120, "type": "METRIC", "confidence": 0.932105282942454}, {"text": "mean absolute error (MAE)", "start_pos": 125, "end_pos": 150, "type": "METRIC", "confidence": 0.9442576269308726}]}, {"text": "In addition, an acceptance threshold 0.1 was set for each dimension/emotion, and weighted accuracy () was computed.", "labels": [], "entities": [{"text": "acceptance threshold 0.1", "start_pos": 16, "end_pos": 40, "type": "METRIC", "confidence": 0.9356531500816345}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8855161666870117}]}, {"text": "All experiments in this paper use independent recurrent encoding (Sec. 3.1).", "labels": [], "entities": []}, {"text": "The encoding scheme differs for every modality.", "labels": [], "entities": []}, {"text": "COVAREP) was used for the audio modality.", "labels": [], "entities": [{"text": "COVAREP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7598684430122375}]}, {"text": "OpenFace ( and FACET (iMotion, 2017) were used for visual one and Glove () was used for encoding the text features.", "labels": [], "entities": [{"text": "OpenFace", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9302219152450562}, {"text": "FACET (iMotion, 2017)", "start_pos": 15, "end_pos": 36, "type": "DATASET", "confidence": 0.7913434902826945}]}, {"text": "Independent recurrent encoding used bidirectional view-specific encoders with 2\u00d7128 dimensional outputs on CREMA-D and RAVDESS and 2 \u00d7 512 on CMU-MOSEI.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 142, "end_pos": 151, "type": "DATASET", "confidence": 0.9352689981460571}]}, {"text": "The complementary effects of multiple views from different modalities would be illustrated by controlling the available input views to different systems.", "labels": [], "entities": []}, {"text": "Global contextualised attention (GCA) was implemented for the emotion recognition systems.", "labels": [], "entities": [{"text": "Global contextualised attention (GCA)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6494400501251221}, {"text": "emotion recognition", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.792214423418045}]}, {"text": "Global and view-specific memory were projected to the alignment space (Eq.).", "labels": [], "entities": [{"text": "Eq.", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8113052248954773}]}, {"text": "The attention weights were computed (Eq. (6)-Eq.) and the contextual view representation was derived).", "labels": [], "entities": []}, {"text": "For more details, refer to Sec.", "labels": [], "entities": []}, {"text": "The encoder-decoder used a 128 dimensional (or 512 for CMU-MOSEI) fully-connected layer.", "labels": [], "entities": []}, {"text": "A final linear layer mapped the decoder output to multi-class targets.", "labels": [], "entities": []}, {"text": "GCA was compared to standard \"early\" and \"late\" fusion strategies.", "labels": [], "entities": [{"text": "GCA", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.48483210802078247}]}, {"text": "In early fusion, encoders: Visualisation of view-specific attention across time.", "labels": [], "entities": []}, {"text": "Attention in the text modality focuses on the words \"very\" and \"delicate\" as cues for emotion recogntion.", "labels": [], "entities": [{"text": "emotion recogntion", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7213537842035294}]}, {"text": "Also, the difference in oscillation rates between the audio and visual modalities is noted.", "labels": [], "entities": []}, {"text": "outputs across all views are resampled to their highest temporal resolution (i.e. audio, at 100Hz), and resulting (aligned) outputs are concatenated across views.", "labels": [], "entities": []}, {"text": "We used similar encoder-decoder structure to one described in Sec 3.2 (, except that the three parallel blocks for modalities were reduced to one.", "labels": [], "entities": []}, {"text": "In late fusion, the final-step encoder outputs from all modalities were independently processed by 1-layer feed-forward networks (Sec 3.4) and view-specific multi-class targets were combined using linear weighting.", "labels": [], "entities": []}, {"text": "Memory updates and ablation study.", "labels": [], "entities": [{"text": "Memory updates", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6650910973548889}, {"text": "ablation", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.973478376865387}]}, {"text": "GCA was enhanced with the extra gating functions (cf. Eq.-, Sec.", "labels": [], "entities": [{"text": "GCA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7050921320915222}]}, {"text": "3.3) . The extended system was compared with the GCA system on CMU-MOSEI data.", "labels": [], "entities": [{"text": "CMU-MOSEI data", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9699263572692871}]}, {"text": "To this end, we perform an ablation study using the test data corrupted by additive Gaussian white noise added to the visual modality. and 2 show the results of emotion recognition on the CREMA-D and RAVDESS dataset respectively.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7123843729496002}, {"text": "RAVDESS dataset", "start_pos": 200, "end_pos": 215, "type": "DATASET", "confidence": 0.7834877669811249}]}, {"text": "Audio, visual and the joint use of bi-modal information were compared using identification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8848713040351868}]}, {"text": "Models trained on the visual modality consistently outperformed models that use solely audio data.", "labels": [], "entities": []}, {"text": "Highest accuracy was achieved when the audio and visual modality were jointly modelled, giving 65% and 58.33% on the two datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9996408224105835}]}, {"text": "Interestingly, the joint bimodal system outperformed human performance on CREMA-D () by 1.4%.", "labels": [], "entities": []}, {"text": "On CMU-MOSEI, the errors between the reference and hypothesis six-dimensional emotion vectors were computed and the results were shown in.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.9298019409179688}]}], "tableCaptions": [{"text": " Table 1: Results on the CREMA-D dataset across 8 emotions", "labels": [], "entities": [{"text": "CREMA-D dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.737687960267067}]}, {"text": " Table 2: Results on the RAVDESS dataset across 8 emotions for normal speech mode", "labels": [], "entities": [{"text": "RAVDESS dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8133069574832916}]}, {"text": " Table 3: Results on CMU-MOSEI dataset", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9508933424949646}]}]}