{"title": [{"text": "Morphological Reinflection in Context: CU Boulder's Submission to CoNLL-SIGMORPHON 2018 Shared Task", "labels": [], "entities": [{"text": "Morphological Reinflection in Context", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8899647444486618}, {"text": "CU Boulder's Submission to CoNLL-SIGMORPHON 2018 Shared Task", "start_pos": 39, "end_pos": 99, "type": "DATASET", "confidence": 0.8504023485713534}]}], "abstractContent": [{"text": "This paper describes two systems for the second subtask of CoNLL-SIGMORPHON 2018 shared task on universal morphological re-inflection submitted by the University of Col-orado Boulder team.", "labels": [], "entities": []}, {"text": "Both systems are implementations of RNN encoder-decoder models with soft attention.", "labels": [], "entities": []}, {"text": "The first system is similar to the baseline system with minor differences in architecture and parameters, and is implemented using PyTorch.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9400181174278259}]}, {"text": "It works for both track 1 and track 2 of the subtask and generally outperforms the baseline at low data settings in both tracks.", "labels": [], "entities": []}, {"text": "The second system predicts the morphosyntactic description (MSD) of the lemma to be inflected using an MSD prediction model.", "labels": [], "entities": []}, {"text": "The data for subtask 2 is processed and reformatted to subtask 1 data format to train an inflection model.", "labels": [], "entities": []}, {"text": "Then the inflection model predicts the inflected form for the target lemma given the predicted MSD.", "labels": [], "entities": []}, {"text": "This system achieves higher accuracies than the first system when the training data is the most limited , though it does not perform better when the training data is abundant.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.995262622833252}]}], "introductionContent": [{"text": "Several natural language processing tasks can benefit from representational power in a computational model at the level of morphology.", "labels": [], "entities": []}, {"text": "The task of morphological inflection has been explored recently in great depth (, resulting in several effective models for that task.", "labels": [], "entities": [{"text": "morphological inflection", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8651614785194397}]}, {"text": "Of particular note is an architecture proposed by, which is modeled after an encoder-decoder model that found success in machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7735966444015503}]}, {"text": "A related, but relatively unexplored task is that of morphological inflection in context.", "labels": [], "entities": [{"text": "morphological inflection in context", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.8874967992305756}]}, {"text": "This paper documents the University of Colorado Boulder's system for that task (subtask2) in the CoNLL-SIGMORPHON 2018 shared task.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 shared task", "start_pos": 97, "end_pos": 130, "type": "DATASET", "confidence": 0.7551142424345016}]}, {"text": "We experimented with a model very similar to the provided baseline, which computes the context fora given inflection as the concatenation of word and MSD embeddings to the left and right of the word that is to be inflected.", "labels": [], "entities": []}, {"text": "During encoding of an input sequence, the context vector is concatenated with the character embedding at each time step.", "labels": [], "entities": []}, {"text": "We also experimented with an encoder comprising of three separate LSTMs whose output states are concatenated and used to predict the MSD for the lemma to be inflected.", "labels": [], "entities": []}, {"text": "We then use a second encoder-decoder network to perform inflection over the given lemma according to that MSD, thus formulating the second portion into the problem in task1 where the training data are pairs of lemma, inflected word form, and the MSD for the inflection and the task is to predict the inflected word form given the lemma and MSD.", "labels": [], "entities": []}, {"text": "We find that the first system described here outperforms the second one when there is ample training data, whereas the latter performs better when the training data is scarce.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the first exploration of the task and an evaluation of task complexity, we experimented by copying the lemma directly.", "labels": [], "entities": []}, {"text": "In other words, we simply guess that the inflected form of a lemma in the context is the lemma itself.", "labels": [], "entities": []}, {"text": "This experiment will be referred to as the copy system going further.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Track 1 accuracies for original form of the copy system, system 1, system2, and baseline", "labels": [], "entities": []}, {"text": " Table 2: Track 2 accuracies for original form of the copy system, system 1, system2, and baseline", "labels": [], "entities": []}, {"text": " Table 1. In general, our first system  (SYS 1) outperforms the second system (SYS 2)  in both high and medium data settings, though nei- ther of them get a higher averaged accuracy than  the baseline system when the training data size is  high and the first system is only marginally bet- ter than the baseline when the training data is of  medium size. For the four systems summarized in  the table, our first system performs the best only  with English at high data setting, and it achieves  the highest accuracies with Spanish and Swedish  at medium data setting. The second system out- performs the other three systems with English at  medium data setting. However, when the train- ing data is the most limited, i.e. at the low data  setting, the second system outperforms both the  first system and the baseline as to average accu-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9637086987495422}, {"text": "accuracies", "start_pos": 507, "end_pos": 517, "type": "METRIC", "confidence": 0.9750967621803284}, {"text": "accu", "start_pos": 832, "end_pos": 836, "type": "METRIC", "confidence": 0.8711118102073669}]}]}