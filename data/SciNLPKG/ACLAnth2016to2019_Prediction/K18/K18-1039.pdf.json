{"title": [{"text": "Lessons learned in multilingual grounded language learning", "labels": [], "entities": [{"text": "multilingual grounded language learning", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.6192500665783882}]}], "abstractContent": [{"text": "Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language.", "labels": [], "entities": []}, {"text": "Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model.", "labels": [], "entities": []}, {"text": "We show that multilingual training improves over bilingual training , and that low-resource languages benefit from training with higher-resource languages.", "labels": [], "entities": []}, {"text": "We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal representation learning is largely motivated by evidence of perceptual grounding inhuman concept acquisition and representation.", "labels": [], "entities": [{"text": "Multimodal representation learning", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8654215335845947}]}, {"text": "It has been shown that visually grounded word and sentence-representations () improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.912825882434845}, {"text": "semantic entailment", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.689066469669342}, {"text": "multimodal machine translation", "start_pos": 177, "end_pos": 207, "type": "TASK", "confidence": 0.6160959800084432}]}, {"text": "Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation () and multilingual dependency parsing (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7527996599674225}, {"text": "multilingual dependency parsing", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6866874396800995}]}, {"text": "Recently, proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German.", "labels": [], "entities": []}, {"text": "Their results show that bilingual training improves imagesentence ranking performance over a monolingual * Work carried out at the University of baseline, and it improves performance on semantic textual similarity benchmarks (.", "labels": [], "entities": [{"text": "imagesentence ranking", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8969278037548065}, {"text": "University of baseline", "start_pos": 131, "end_pos": 153, "type": "DATASET", "confidence": 0.8831967910130819}]}, {"text": "These findings suggest that it maybe beneficial to consider another language as another modality in a monolingual grounded language learning model.", "labels": [], "entities": []}, {"text": "In the grounded learning scenario, descriptions of an image in multiple languages can be considered as multiple views of the same or closely related data.", "labels": [], "entities": []}, {"text": "These additional views can help overcome the problems of data sparsity, and have practical implications for efficiently collecting imagetext datasets in different languages.", "labels": [], "entities": []}, {"text": "In real-life applications, many tasks and domains can involve code switching (, which is easier to deal with using a multilingual model.", "labels": [], "entities": [{"text": "code switching", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7247725576162338}]}, {"text": "Furthermore, it is more convenient to maintain a single multilingual system than one system for each considered language.", "labels": [], "entities": []}, {"text": "However, there is a need fora systematic exploration of the conditions under which it is useful to add additional views of the data.", "labels": [], "entities": []}, {"text": "We investigate the impact of the following conditions on the performance of a multilingual grounded language learning model in sentence and image retrieval tasks: Additional languages.", "labels": [], "entities": [{"text": "sentence and image retrieval tasks", "start_pos": 127, "end_pos": 161, "type": "TASK", "confidence": 0.7235017418861389}]}, {"text": "Multilingual models have not been explored yet in a multimodal setting.", "labels": [], "entities": []}, {"text": "We investigate the contribution of adding more than one language by performing bilingual experiments on English and German (Section 5) as well as adding French and Czech captioned images (Section 6).", "labels": [], "entities": []}, {"text": "High-to-low resource transfer: In Section 6.2 we investigate whether low-resource languages benefit from jointly training on larger data sets from higher-resource languages.", "labels": [], "entities": []}, {"text": "This type of transfer has previously been shown to be effective in machine translation (e.g.,).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7616479694843292}]}, {"text": "Training objective: In addition to learning to map images to sentences, we study the effect of also learning relationships between captions of the same image in different languages.", "labels": [], "entities": []}, {"text": "We assess the contribution of such a caption-caption ranking objective throughout our experiments.", "labels": [], "entities": []}, {"text": "Our results show that multilingual joint training improves upon bilingual joint training, and that grounded sentence representations fora lowresource language can be substantially improved with data from different high-resource languages.", "labels": [], "entities": []}, {"text": "Our results suggest that independently-collected captions are more useful than translated captions, for the task of learning multilingual multimodal sentence embeddings.", "labels": [], "entities": []}, {"text": "Finally, we recommend to collect captions for the same set of images in multiple languages, due to the benefits of the additional caption-caption ranking objective function.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train and evaluate our models on the translation and comparable portions of the Multi30K dataset ().", "labels": [], "entities": [{"text": "Multi30K dataset", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9640295505523682}]}, {"text": "The translation portion (a low-resource dataset) contains 29K images, each described in one English caption with German, French, and Czech translations.", "labels": [], "entities": []}, {"text": "The comparable portion (a higher-resource dataset) contains the same 29K images paired with five English and five German descriptions collected independently.", "labels": [], "entities": []}, {"text": "presents an example of the translation and comparable portions of the data.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9782940149307251}]}, {"text": "We used the preprocessed version of the dataset, in which the text is lowercased, punctuation is normalized, and the text is tokenized 3 . To reduce the vocabulary size of the joint models, we replace all words occurring fewer than four times with a spe-cial \"UNK\" symbol.", "labels": [], "entities": []}, {"text": "shows the overlap between the vocabularies of the translation portion of the Multi30K dataset.", "labels": [], "entities": [{"text": "Multi30K dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9657966196537018}]}, {"text": "The total number of tokens across all four languages is 17,571, and taking the union of the tokens in these four languages results in vocabulary of 16,553 tokens -a 6% reduction in vocabulary size.", "labels": [], "entities": []}, {"text": "On the comparable portion of the dataset, the total vocabulary between English and German contains 18,337 tokens, with a union of 17,667, which is a 4% reduction in vocabulary size.", "labels": [], "entities": []}, {"text": "We evaluate our models on the 1K images of the 2016 test set of Multi30K either using the 5K captions from the comparable data or the 1K translation pairs.", "labels": [], "entities": [{"text": "1K images of the 2016 test set of Multi30K", "start_pos": 30, "end_pos": 72, "type": "DATASET", "confidence": 0.718462112877104}]}, {"text": "We evaluate on image-to-text (I\u2192 T) and text-to-image (T\u2192 I) retrieval tasks.", "labels": [], "entities": []}, {"text": "For most experiments we report Recall at 1 (R@1), 5 (R@5) and 10 (R@10) scores averaged over 10 randomly initialised models.", "labels": [], "entities": [{"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9975388050079346}]}, {"text": "However, in Section 6 we only report R@10 due to space limitations and because it has less variance than R@1 or R@5.", "labels": [], "entities": [{"text": "R@10", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9501977761586508}]}, {"text": "We now turnout attention to multilingual learning using the English, German, French and Czech annotations in the translation portion of Multi30K.", "labels": [], "entities": []}, {"text": "We only report the text-to-image (T\u2192I) R@10 results due to space limitations.", "labels": [], "entities": []}, {"text": "We did not repeat the overlapping vs. nonoverlapping experiments from Section 5.3 in a multilingual setting because this would introduce too much data sparsity.", "labels": [], "entities": []}, {"text": "In order to conduct this experiment, we would have to downsample the already low-resource French and Czech captions by 50%, or even further for multi-way experiments.", "labels": [], "entities": [{"text": "French and Czech captions", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.7146700620651245}]}, {"text": "shows the results of repeating the translations vs. comparable captions experiment from Section 5.2 with data in four languages.", "labels": [], "entities": []}, {"text": "The Multi-translation models are trained on 29K images paired with a single caption in each language.", "labels": [], "entities": []}, {"text": "These models perform better than their Monolingual counterparts, and the German, French, and Czech models are further improved with the c2c objective.", "labels": [], "entities": []}, {"text": "The Multi-comparable models are trained by randomly sampling one English and one German caption from the comparable dataset, alongside the French and Czech translation pairs.", "labels": [], "entities": []}, {"text": "These models perform as well as the Multi-translation models, and the c2c objective brings further improvements for all languages in this setting.", "labels": [], "entities": []}, {"text": "These results clearly demonstrate the advantage of jointly training on more than two languages.", "labels": [], "entities": []}, {"text": "Text-to-image retrieval performance increases by more than 11 R@10 points for each of the four languages in our experiment.", "labels": [], "entities": [{"text": "Text-to-image retrieval", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6837744116783142}, {"text": "R", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9442296624183655}]}], "tableCaptions": [{"text": " Table 2: English Image-to-text (I\u2192T) and text-to-image (T\u2192I) retrieval results on the comparable part  of Multi30K, measured by Recall at 1, 5 at 10. Typewriter font shows performance of two sets of  symmetric and asymmetric models from Gella et al. (2017).", "labels": [], "entities": [{"text": "Multi30K", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.8913280963897705}, {"text": "Recall", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.6761462092399597}]}, {"text": " Table 4: R@10 retrieval results on the comparable  part of Multi30K. Bi-translation is trained on 29K  translation pair data; bi-comparable is trained by  downsampling the comparable data to 29K.", "labels": [], "entities": [{"text": "R", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9465303421020508}, {"text": "Multi30K", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9217609167098999}]}, {"text": " Table 5: R@10 retrieval results on the compara- ble part of Multi30K. Full model trained on the  29K images of the comparable part, Half model on  14.5K images using random downsampling. For  Bi-overlap, both English and German captions are  used for 14.5K images. For Bi-disjoint, 14.5K im- ages are used for English and the remaining 14.5K  images for German.", "labels": [], "entities": [{"text": "R", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9706253409385681}, {"text": "Multi30K", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9272070527076721}]}, {"text": " Table 6: The Monolingual and joint Multi- translation models trained on translation pairs, and  the Multi-comparable trained on the downsampled  comparable set with one caption per image.", "labels": [], "entities": []}]}