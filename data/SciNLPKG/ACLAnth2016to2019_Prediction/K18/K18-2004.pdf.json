{"title": [{"text": "Semi-Supervised Neural System for Tagging, Parsing and Lemmatization", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the ICS PAS system which took part in CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "ICS PAS", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.7024036049842834}, {"text": "CoNLL 2018 shared task", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.7746980488300323}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.7869632482528687}]}, {"text": "The system consists of jointly trained tagger, lem-matizer, and dependency parser which are based on features extracted by a biL-STM network.", "labels": [], "entities": []}, {"text": "The system uses both fully connected and dilated convolutional neu-ral architectures.", "labels": [], "entities": []}, {"text": "The novelty of our approach is the use of an additional loss function, which reduces the number of cycles in the predicted dependency graphs, and the use of self-training to increase the system performance.", "labels": [], "entities": []}, {"text": "The proposed system, i.e. ICS PAS (Warszawa), ranked 3th/4th in the official evaluation 1 obtaining the following overall results: 73.02 (LAS), 60.25 (MLAS) and 64.44 (BLEX).", "labels": [], "entities": [{"text": "ICS PAS (Warszawa)", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.6290692448616028}, {"text": "LAS)", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9781762063503265}, {"text": "MLAS)", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.6183917820453644}, {"text": "BLEX)", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.9771042764186859}]}], "introductionContent": [{"text": "Most of contemporary NLP systems for machine translation, question answering, sentiment analysis, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8147464394569397}, {"text": "question answering", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9165688157081604}, {"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9523458182811737}]}, {"text": "operate on preprocessed texts, i.e. texts with tokenised, part-of-speech tagged, and possibly syntactically parsed sentences.", "labels": [], "entities": []}, {"text": "Therefore, the development of high-quality pipelines of NLP tools or entire systems for language preprocessing is still an important issue.", "labels": [], "entities": []}, {"text": "The vast majority of language preprocessing frameworks take advantage of the statistical methods, especially the supervised or semi-supervised statistical methods.", "labels": [], "entities": []}, {"text": "Based on training data, language preprocessing tools learn to analyse sentences and to predict morphosyntactic annotations of these sentences.", "labels": [], "entities": []}, {"text": "The supervised methods require gold-standard training data whose creation is a time-consuming and expensive process.", "labels": [], "entities": []}, {"text": "Nevertheless, the morphosyntactically annotated data sets are publicly available for many languages, in particular within Universal Dependencies initiative (UD,.", "labels": [], "entities": []}, {"text": "The initiators of UD aim at developing a cross-linguistically consistent annotation schema and at building a large multilingual collection of sentences annotated according to this schema with the universal part-of-speech tags and the universal dependency trees.", "labels": [], "entities": []}, {"text": "UD treebanks are nowadays used for multilingual system development (.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8744166493415833}]}, {"text": "The history of developing multilingual systems dates back to 2006 and 2007, when two shared tasks on multilingual dependency parsing were organised at the Conference on Computational Natural Language Learning.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.6273859441280365}]}, {"text": "After 10 years, the shared task was organised again in 2017, and currently there is its fourth edition (.", "labels": [], "entities": []}, {"text": "In this paper we describe our solution submitted to the CoNLL 2018 Universal Dependency shared task.", "labels": [], "entities": [{"text": "CoNLL 2018 Universal Dependency shared task", "start_pos": 56, "end_pos": 99, "type": "DATASET", "confidence": 0.8522659639517466}]}, {"text": "The system and the trained models for participating treebanks are publicly available.", "labels": [], "entities": []}, {"text": "Our system takes a tokenised sentence as input.", "labels": [], "entities": []}, {"text": "The sentence tokenisation is predicted by the baseline model.", "labels": [], "entities": [{"text": "sentence tokenisation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6919536292552948}]}, {"text": "Each word is represented both as an external word embedding and as a character-based word embedding estimated by a dilated convolutional neural network encoder (CNN,.", "labels": [], "entities": []}, {"text": "The concatenation of these embeddings is fed to a bidirectional long short-term memory network (biLSTM,) which ex-tracts the final features (see Section 2.1).", "labels": [], "entities": []}, {"text": "The tagger takes extracted features and predicts universal part-of-speech tags, language-specific tags and morphological features using three separate fully connected neural networks with one hidden layer (see Section 2.2).", "labels": [], "entities": []}, {"text": "The lemmatizer uses a dilated CNN to predict lemmas based on characters of corresponding words and features previously extracted by a biLSTM encoder (see Section 2.3).", "labels": [], "entities": []}, {"text": "As a scoring function, the graph-based dependency parser uses simple dot product of the vector representations of a dependent and its governor.", "labels": [], "entities": []}, {"text": "These representations are output by two single fully connected layers which take feature vectors extracted by a biLSTM encoder as input.", "labels": [], "entities": []}, {"text": "A novel loss function penalizes cycles, in order to reduce their number in the predicted dependency graphs (see Section 2.4.2).", "labels": [], "entities": []}, {"text": "Chu-Liu-Edmonds algorithm ( constructs the final dependency tree.", "labels": [], "entities": []}, {"text": "The dependency labels are predicted with a fully connected neural network based on the dependent and its governor embeddings as well (see Section 2.4 for more details on the parser's architecture).", "labels": [], "entities": []}, {"text": "The system architecture is schematised in.", "labels": [], "entities": []}, {"text": "The whole system is end-to-end trained, separately for each treebank provided for the purposes of the shared task.", "labels": [], "entities": []}, {"text": "The technical details of the implemented system are given in Section 3.", "labels": [], "entities": []}, {"text": "Additionally, for 20 selected treebanks self-training is used to increase the performance of the models (see Section 3.4).", "labels": [], "entities": []}, {"text": "The proposed technique of self-training has an impact on the quality of tagging, lemmatisation and parsing (see Section 4.3).", "labels": [], "entities": [{"text": "tagging", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9657853841781616}, {"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9610282778739929}]}, {"text": "The article ends with the presentation of the results achieved by our system (see Section 4) and some conclusions (see Section 5).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official results of our system in CoNLL  shared task. State-of-the-art results are in bold.", "labels": [], "entities": [{"text": "CoNLL  shared task", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.742231527964274}]}, {"text": " Table 2: Comparison of the models trained with  and without the additional loss function.", "labels": [], "entities": []}]}