{"title": [{"text": "Predefined Sparseness in Recurrent Sequence Models", "labels": [], "entities": [{"text": "Sparseness", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8964941501617432}]}], "abstractContent": [{"text": "Inducing sparseness while training neural networks has been shown to yield models with a lower memory footprint but similar effectiveness to dense models.", "labels": [], "entities": []}, {"text": "However, sparseness is typically induced starting from a dense model, and thus this advantage does not hold during training.", "labels": [], "entities": []}, {"text": "We propose techniques to enforce sparseness upfront in recurrent sequence models for NLP applications, to also benefit training.", "labels": [], "entities": []}, {"text": "First, in language modeling, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters , leading to more expressive models.", "labels": [], "entities": []}, {"text": "Second , for sequence labeling, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6734225451946259}]}], "introductionContent": [{"text": "Many supervised learning problems today are solved with deep neural networks exploiting largescale labeled data.", "labels": [], "entities": []}, {"text": "The computational and memory demands associated with the large amount of parameters of deep models can be alleviated by using sparse models.", "labels": [], "entities": []}, {"text": "Applying sparseness can be seen as a form of regularization, as it leads to a reduced amount of model parameters 1 , forgiven layer widths or representation sizes.", "labels": [], "entities": []}, {"text": "Current successful approaches gradually induce sparseness during training, starting from densely initialized networks, as detailed in Section 2.", "labels": [], "entities": []}, {"text": "However, we propose that models can also be built with predefined sparseness, i.e., such models are already sparse by design and do not require sparseness inducing training schemes.", "labels": [], "entities": []}, {"text": "The main benefit of such an approach is memory efficiency, even at the start of training.", "labels": [], "entities": []}, {"text": "Especially in the area of natural language processing, inline with the hypothesis by  that natural language is \"high-rank\", it maybe useful to train larger sparse representations, even when facing memory restrictions.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6559723019599915}]}, {"text": "For example, in order to train word representations fora large vocabulary using limited computational resources, predefined sparseness would allow training larger embeddings more effectively compared to strategies inducing sparseness from dense models.", "labels": [], "entities": []}, {"text": "The contributions of this paper are (i) a predefined sparseness model for recurrent neural networks, (ii) as well as for word embeddings, and (iii) proof-of-concept experiments on part-of-speech tagging and language modeling, including an analysis of the memorization capacity of dense vs. sparse networks.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.730670839548111}, {"text": "language modeling", "start_pos": 207, "end_pos": 224, "type": "TASK", "confidence": 0.7041466236114502}]}, {"text": "An overview of related work is given in the next Section 2.", "labels": [], "entities": []}, {"text": "We subsequently present predefined sparseness in recurrent layers (Section 3), as well as embedding layers (Section 4), each illustrated by experimental results.", "labels": [], "entities": []}, {"text": "This is followed by an empirical investigation of the memorization capacity of language models with predefined sparseness (Section 5).", "labels": [], "entities": []}, {"text": "Section 6 summarizes the results, and points out potential areas of follow-up research.", "labels": [], "entities": []}, {"text": "The code for running the presented experiments is publically available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now study the impact of sparseness in word embeddings, fora basic POS tagging model, and report results on the PTB Wall Street Journal data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7547841966152191}, {"text": "PTB Wall Street Journal data", "start_pos": 114, "end_pos": 142, "type": "DATASET", "confidence": 0.9440788507461548}]}, {"text": "We embed 43,815 terms in 20-dimensional space, as input fora BiLSTM layer with hidden state size 10 for both forward and backward directions.", "labels": [], "entities": []}, {"text": "The concatenated hidden states go into a fully connected layer with tanh non-linearity (down to dimension 10), followed by a softmax classification layer with 49 outputs (i.e., the number of POS tags).", "labels": [], "entities": []}, {"text": "The total number of parameters is 880k, of which 876k in the embedding layer.", "labels": [], "entities": []}, {"text": "Although character-based models are known to outperform pure word embedding based models (, we wanted to investigate the effect of sparseness in word embeddings, rather than creating more competitive but larger or complex models, risking a smaller resolution in the effect of changing individual building blocks.", "labels": [], "entities": []}, {"text": "To this end we also limited the dimensions, and hence the expressiveness, of the recurrent layer.", "labels": [], "entities": []}, {"text": "Our model is similar to but smaller than the 'word lookup' baseline by. compares the accuracy for variable densities \u03b4 E (for k = 20) vs. different embedding sizes (with \u03b4 E = 1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9994208812713623}]}, {"text": "For easily comparing sparse and dense models with the same number of embedding parameters, we scale \u03b4 E , the x-axis for the sparse case, to the average embedding size of 20 \u03b4 E . With LSTM state sizes of 50, the careful tuning of dropout parameters gave an accuracy of 94.7% when reducing the embedding size to k = 2, a small gap compared to 96.8% for embedding size 50.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.9991782307624817}]}, {"text": "The effect of larger sparse embeddings was therefore much smaller in absolute value than the one visualized in, because of the much more expressive recurrent layer.", "labels": [], "entities": []}, {"text": "Training models with shorter dense embeddings appeared more difficult.", "labels": [], "entities": []}, {"text": "In order to make a fair comparison, we therefore tuned the models over a range of regularization hyperparameters, provided in.", "labels": [], "entities": []}, {"text": "We observe that the sparse embedding layer allows lowering the number of parameters in E down to a fraction of 15% of the original amount, with little impact on the effectiveness, provided E is sparsified rather than reduced in size.", "labels": [], "entities": []}, {"text": "The reason for that is that with sparse 20-dimensional embeddings, the BiLSTM still receives 20-dimensional inputs, from which a significant subset only transmits signals from a small set of frequent terms.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.7544816732406616}]}, {"text": "In the case of smaller dense embeddings, information from all terms is uniformly present over fewer dimensions, and needs to be processed with fewer parameters at the encoder input.", "labels": [], "entities": []}, {"text": "Finally, we verify the validity of our hypothesis from Section 4.1 that frequent terms need to be embedded with more parameters than rare words.", "labels": [], "entities": []}, {"text": "Indeed, one could argue in favor of the opposite strategy.", "labels": [], "entities": []}, {"text": "It would be computationally more efficient if the terms most often encountered had the smallest representation.", "labels": [], "entities": []}, {"text": "Also, stop words are the most frequent ones but are said to carry little information content.", "labels": [], "entities": []}, {"text": "However, confirms our initial hypothesis.", "labels": [], "entities": []}, {"text": "Applying the introduced strategy to sparsify embeddings on randomly ordered words ('no sorting') leads to a significant decrease inaccuracy compared to the proposed sorting strategy ('up').", "labels": [], "entities": []}, {"text": "When the most frequent words are encoded with the shortest embeddings ('down' in the table), the accuracy goes down even further.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9995689988136292}]}, {"text": "The initial model for our learning to recite experiment is the baseline language model used in Section 3.2 (, with the PTB data.", "labels": [], "entities": [{"text": "PTB data", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.9391640424728394}]}, {"text": "We set all regularization parameters to zero, to focus on memorizing the training data.", "labels": [], "entities": []}, {"text": "During training, we measure the ability of the model to correctly predict the next token at every position in the training data, by selecting the token with highest predicted probability.", "labels": [], "entities": []}, {"text": "When the model reaches an accuracy of 100%, it is able to recite the entire training corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993259906768799}]}, {"text": "We propose the following optimization setup (tuned and tested on dense models with different sizes): minibatch SGD (batch size 20, momentum 0.9, and best initial learning rate among 5 or 10).", "labels": [], "entities": []}, {"text": "An exponentially decaying learning rate factor (0.97 every epoch) appeared more suitable for memorization than other learning rate scheduling strategies, and we report the highest accuracy in 150 epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9991783499717712}]}, {"text": "We compare the original model (in terms of network dimensions) with versions that have less parameters, by either reducing the RNN hidden state size h or by sparsifying the RNN, and similarly for the embedding layer.", "labels": [], "entities": []}, {"text": "For making the embedding matrix sparse, M = 10 equal-sized segments are used (as in eq. 1).", "labels": [], "entities": []}, {"text": "The dense model with the original dimensions has 24M parameters to memorize a sequence of in total 'only' 930k tokens, and is able to do so.", "labels": [], "entities": []}, {"text": "When the model's embedding size and intermediate hidden state size are halved, the number of parameters drops to 7M, and the resulting model now makes 67 mistakes out of 10k predictions.", "labels": [], "entities": []}, {"text": "If h is kept, but the recurrent layers are made sparse to yield the same number of parameters, only 5 mistakes are made for every 10k predictions.", "labels": [], "entities": []}, {"text": "Making the embedding layer sparse as well introduces new errors.", "labels": [], "entities": []}, {"text": "If the dimensions are further reduced to a third the original size, the memorization capacity goes down strongly, with less than 4M trainable parameters.", "labels": [], "entities": [{"text": "memorization", "start_pos": 72, "end_pos": 84, "type": "METRIC", "confidence": 0.8656665086746216}]}, {"text": "In this case, sparsifying both the recurrent and embedding layer yields the best result, whereas the dense model works better than   100.0 dense model (see(a)) 200 1 575 7.07M 99.33 sparse RNN (see  the model with sparse RNNs only.", "labels": [], "entities": []}, {"text": "A possible explanation for that is the strong sparseness in the RNNs.", "labels": [], "entities": []}, {"text": "For example, in the middle layer only 1 out of 10 recurrent connections is non-zero.", "labels": [], "entities": []}, {"text": "In this case, increasing the size of the word embeddings (at least, for the frequent terms) could provide an alternative for the model to memorize parts of the data, or maybe it makes the optimization process more robust.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyperparameters for POS tagging model ( \u2020as introduced in (Merity et al., 2017)). A list indicates  tuning over the given values was performed.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7465828955173492}]}, {"text": " Table 3: Impact of vocabulary sorting on POS accuracy with sparse embeddings: up vs. down (most fre- quent words get longest vs. shortest embeddings, resp.) or not sorted, for different embedding densities  \u03b4 E .", "labels": [], "entities": [{"text": "vocabulary sorting", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7315031290054321}, {"text": "POS", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9204353094100952}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9128753542900085}]}, {"text": " Table 4: PTB train set memorization accuracies for dense models vs. models with predefined sparseness  in recurrent and embedding layers with comparable number of parameters.", "labels": [], "entities": [{"text": "PTB train set memorization accuracies", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.656352025270462}]}]}