{"title": [{"text": "T \u00a8 ubingen-Oslo system at SIGMORPHON shared task on morphological inflection. A multi-tasking multilingual sequence to sequence model", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe our three submissions to the inflection track of SIGMORPHON shared task.", "labels": [], "entities": [{"text": "SIGMORPHON shared task", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7862897515296936}]}, {"text": "We experimented with three models: namely, sequence to sequence model (popularly known as seq2seq), seq2seq model with data augmentation, and a multilingual multi-tasking seq2seq model that is multilingual in nature.", "labels": [], "entities": []}, {"text": "Our results with the multilingual model are below the baseline in the case of both high and medium datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological inflection is the task of predicting the target inflected form from a lemma and a bundle of inflectional features.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8578075468540192}, {"text": "predicting the target inflected form", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.8185455083847046}]}, {"text": "For instance, given the Norwegian lemma hus \"house\" and the morphological features N, DEF, PL the task is to predict husene \"houses\".", "labels": [], "entities": [{"text": "DEF", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9504384994506836}]}, {"text": "The SIGMORPHON shared task for 2018) provided three data scenarios consisting of high (10000), medium (1000), and low (100) examples.", "labels": [], "entities": []}, {"text": "This paper described the three systems that we submitted to the inflection track in the SIGMORPHON shared task.", "labels": [], "entities": [{"text": "SIGMORPHON shared task", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7628118197123209}]}, {"text": "All our models are based on encoder-decoder model introduced by for the morphological inflection task.", "labels": [], "entities": []}, {"text": "We trained our models on all the data sizes and tested on the test datasets provided by the organizers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our models at all the three resource settings: high, medium, and low.", "labels": [], "entities": []}, {"text": "In all our experiments, the maximum length of both source and target strings are fixed to 30 and padded with zeroes at the end.", "labels": [], "entities": []}, {"text": "Both the encoder and decoder LSTM units consisted of 256 hidden units.", "labels": [], "entities": []}, {"text": "All the models were trained with Adam ( with minibatches of size 32 or 128 depending on the size of the data; and, used a early-stop with a patience of 5 to prevent overfitting.", "labels": [], "entities": [{"text": "patience", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9748480916023254}]}], "tableCaptions": [{"text": " Table 1: Average accuracies for high and medium  datasets with three different models.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.7570667266845703}]}, {"text": " Table 2: Top-5 and bottom-5 languages at which the three models perform the best and worse for high and medium  datasets.", "labels": [], "entities": []}]}