{"title": [{"text": "Experiments on Morphological Reinflection: CoNLL 2018 Shared Task", "labels": [], "entities": [{"text": "Morphological Reinflection", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8085437119007111}, {"text": "CoNLL 2018 Shared Task", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.9039154648780823}]}], "abstractContent": [{"text": "We present a system for the task of morphological inflection, i.e., finding a target morphological form, given a lemma and a set of target tags.", "labels": [], "entities": []}, {"text": "System is trained on datasets of three sizes: low, medium and high.", "labels": [], "entities": []}, {"text": "The system uses a simple Long Short-Term Memory (LSTM) based encoder-decoder based model.", "labels": [], "entities": []}, {"text": "The performance for low size dataset is poor in general while it improves significantly for medium and high sized training dataset.", "labels": [], "entities": []}, {"text": "The average performance overall languages is poor as compared to baseline for low dataset, it is comparable for medium dataset, and significantly more for high dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "The CoNLL-SIGMOPRHON 2018 shared task consists of two subtasks out of which we participate only in the first subtask, which involves generating a target inflected form from a given lemma with its morphosyntactic descriptions (MSDs) provided as a set of features.", "labels": [], "entities": []}, {"text": "For instance, the word thinking is the present continuous inflected form of the lemma think.", "labels": [], "entities": []}, {"text": "The models were trained on three differently-sized datasets.", "labels": [], "entities": []}, {"text": "The low-sized datasets had around 100 training samples, the medium-sized datasets had around 1000 training samples and the high-sized datasets had around 10000 samples for most languages.", "labels": [], "entities": []}, {"text": "Datasets were provided fora total of 103 languages including surprise data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Top 5 Accuracies for languages for low data", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9987035989761353}]}, {"text": " Table 2: Top 5 Accuracies for languages for medium data", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9978561997413635}]}, {"text": " Table 3: Top 5 Accuracies for languages for high data", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.998608410358429}]}, {"text": " Table 4: Results for all languages for low data.", "labels": [], "entities": []}, {"text": " Table 5: Results for all languages for medium data.", "labels": [], "entities": []}, {"text": " Table 6: Results for all languages for high data.", "labels": [], "entities": []}]}