{"title": [{"text": "IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.6499833464622498}]}], "abstractContent": [{"text": "This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.7608673870563507}]}, {"text": "Our system implements anew joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles tokeniza-tion, part-of-speech tagging, morphological tagging and dependency parsing in one single model.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.6844596415758133}, {"text": "morphological tagging", "start_pos": 176, "end_pos": 197, "type": "TASK", "confidence": 0.6907435953617096}, {"text": "dependency parsing", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.749973475933075}]}, {"text": "By leveraging a combination of character-based modeling of words and recursive composition of partially built linguistic structures we qualified 13th overall and 7th in low resource.", "labels": [], "entities": []}, {"text": "We also present anew sentence segmen-tation neural architecture based on Stack-LSTMs that was the 4th best overall.", "labels": [], "entities": []}], "introductionContent": [{"text": "The CoNLL 2018 Shared Task on Parsing Universal dependencies consists of parsing raw text from different sources and domains into Universal Dependencies () for more than 60 languages and domains.", "labels": [], "entities": []}, {"text": "The task includes extremely low resource languages, like Kurmanji or Buriat, and high-resource languages like English or Spanish.", "labels": [], "entities": []}, {"text": "The competition therefore invites to learn how to make parsers for lowresource language better by exploiting resources available for the high-resource languages.", "labels": [], "entities": []}, {"text": "The task also includes languages from almost all language families, including Creole languages like Nigerian Pidgin 2 and completely different scripts (i.e. Chinese, Latin alphabet, Cyrillic alphabet, or Arabic).", "labels": [], "entities": []}, {"text": "For further description of the task, data, framework and evaluation please refer to.", "labels": [], "entities": []}, {"text": "In this paper we describe the IBM Research AI submission to the Shared Task on Parsing Universal Dependencies.", "labels": [], "entities": [{"text": "Shared Task on Parsing Universal Dependencies", "start_pos": 64, "end_pos": 109, "type": "TASK", "confidence": 0.7533863087495168}]}, {"text": "Our starting point is the Stack-LSTM 3 parser () with character-based word representations ( , which we extend to handle tokenization, POS tagging and morphological tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.8178195655345917}, {"text": "morphological tagging", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.7183330953121185}]}, {"text": "Additionally, we apply the ideas presented by to all low resource languages since they benefited from high-resource languages in the same family.", "labels": [], "entities": []}, {"text": "Finally, we also present two different ensemble algorithms that boosted our results (see Section 2.4).", "labels": [], "entities": []}, {"text": "Participants are requested to obtain parses from raw texts.", "labels": [], "entities": []}, {"text": "This means that, sentence segmentation, tokenization, POS tagging and morphological tagging need to be done besides parsing.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7504185140132904}, {"text": "tokenization", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9627923965454102}, {"text": "POS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.8544049263000488}, {"text": "morphological tagging", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7411178052425385}]}, {"text": "Participants can choose to use the baseline pipeline (UDPipe 1.) for those steps besides parsing, or create their own implementation.", "labels": [], "entities": []}, {"text": "We choose to use our own implementation for most of the languages.", "labels": [], "entities": []}, {"text": "However, in a few treebanks with very hard tokenization, like Chinese and Japanese, we rely on UDPipe 1.2 and a run of our base parser (section 2.1), since this produces better results.", "labels": [], "entities": []}, {"text": "For the rest of languages, we produce parses from raw text that maybe in documents (and thus we need to find the sentence markers within those documents); for some of the treebanks we adapted punctuation prediction system (which is also based in the Stack-LSTM framework) to predict sentence markers.", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.6982271671295166}]}, {"text": "Given that the text to be segmented into sentences can be of a significant length, we implemented a sliding-window extension of the punctuation prediction system where the Stack-LSTM is reinitialized and primed when the window is advanced (see Section 3 for details).", "labels": [], "entities": [{"text": "punctuation prediction", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.6674771457910538}]}, {"text": "Our system ranked 13th overall, 7th for low resource languages and 4th in sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7181490063667297}]}, {"text": "It was also the best qualifying system in low resource language, Kurmanji, evidencing the effectiveness of our adaptation of approach (see Section 2.3).", "labels": [], "entities": [{"text": "Kurmanji", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.8401820063591003}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Final F1 LAS results of our system compared with the baseline. We show our ranking for the particular treebank  between parenthesis next to our score.", "labels": [], "entities": [{"text": "F1 LAS", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.7334981858730316}]}, {"text": " Table 4: Average F1 LAS results (grouped by treebank size  and type) of our system compared with the baseline. We show  our ranking in the same category between parenthesis next to  our score.", "labels": [], "entities": [{"text": "F1 LAS", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.852268397808075}]}]}