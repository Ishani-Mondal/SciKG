{"title": [{"text": "UZH at CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.5168103575706482}, {"text": "Universal Morphological Reinflection", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.5970937013626099}]}], "abstractContent": [{"text": "This paper presents the submissions by the University of Zurich to the CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection", "start_pos": 71, "end_pos": 144, "type": "TASK", "confidence": 0.6665617227554321}]}, {"text": "Our system is based on the prior work on neural transition-based transduction (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2017).", "labels": [], "entities": []}, {"text": "Unlike the prior work, we train the model in a fully end-to-end fashion-without the need for an external character aligner-within the framework of imitation learning.", "labels": [], "entities": []}, {"text": "In the type-level morphological inflection generation challenge (Task I), our five-strong ensemble outperforms all competitors in all three data-size settings.", "labels": [], "entities": [{"text": "type-level morphological inflection generation", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.7379778176546097}]}, {"text": "In the token-level inflection generation challenge (Task II), our single model achieves the best results on three out of four sub-tasks that we have participated in.", "labels": [], "entities": [{"text": "token-level inflection generation challenge", "start_pos": 7, "end_pos": 50, "type": "TASK", "confidence": 0.7955057471990585}]}], "introductionContent": [{"text": "The CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection) focuses on inflection generation at the type level (Task I) and in context (Task II).", "labels": [], "entities": []}, {"text": "Both tasks feature three settings depending on the maximum number of training examples: low, medium, and high.", "labels": [], "entities": []}, {"text": "The team from the University of Zurich has taken part in both tasks with submissions featuring neural transition-based transducers ().", "labels": [], "entities": []}, {"text": "The model transduces a string by a sequence of traditional character edit operations.", "labels": [], "entities": []}, {"text": "The neuralized transducer, which conditions edits on the entire input string and captures unbounded dependencies in the output, has proven very effective in the past editions of the SIGMORPHON shared task (.", "labels": [], "entities": [{"text": "SIGMORPHON shared task", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.806172768274943}]}, {"text": "Typically, this model is trained by maximizing the likelihood of gold action sequences that come from a separate character aligner.", "labels": [], "entities": []}, {"text": "This year, we train with an imitation learning method) that enforces optimal alignment in the loss and additionally supports action-space exploration and the optimization of a task-specific objective.", "labels": [], "entities": [{"text": "action-space exploration", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.7885941565036774}]}, {"text": "Our method entirely eliminates the need fora character aligner and results in substantially stronger models, at the expense of slight increase in training time.", "labels": [], "entities": []}, {"text": "The resulting models evaluate favorably on both CoNLL-SIGMORPHON 2018 tasks.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 tasks", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.7900808850924174}]}, {"text": "On Task I, our five-strong ensemble uzh-02 outperforms the nearest competitor by over 1% absolute accuracy in the high setting (24.4% error reduction) and over 2% absolute in the medium setting and 4% absolute in the low setting (13.9% and 8.6% error reduction, respectively).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8512855768203735}, {"text": "error reduction", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.9444892406463623}, {"text": "error reduction", "start_pos": 245, "end_pos": 260, "type": "METRIC", "confidence": 0.9223873615264893}]}, {"text": "The larger ensemble uzh-01 further improves the result slightly in the high and medium settings (1% and 2% error reduction, respectively).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 107, "end_pos": 122, "type": "METRIC", "confidence": 0.9801500141620636}]}, {"text": "For Task II, we submit a single model to only the low and medium settings.", "labels": [], "entities": []}, {"text": "The single model dominates the low setting, being also the only system that beats the predict-the-lemma baseline.", "labels": [], "entities": []}, {"text": "The model comes second in the track 1 medium setting with almost 4% absolute accuracy behind the winner (8.5% error increase), and is the best in the track 2 medium setting with almost 4% absolute above the runnerup (6.7% error reduction).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.8866135478019714}, {"text": "error increase", "start_pos": 110, "end_pos": 124, "type": "METRIC", "confidence": 0.9792320430278778}, {"text": "error reduction", "start_pos": 222, "end_pos": 237, "type": "METRIC", "confidence": 0.9060794413089752}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Overview of Task I results. H, M, L=high,  medium, low settings; #=number of models that the av- erage is taken over; compet.=nearest competitor.", "labels": [], "entities": []}, {"text": " Table 3: Overview of Task II results.", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9118825197219849}]}]}