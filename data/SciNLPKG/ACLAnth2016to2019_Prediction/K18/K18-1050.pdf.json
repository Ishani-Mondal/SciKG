{"title": [], "abstractContent": [{"text": "Entity Linking (EL) is an essential task for semantic text understanding and information extraction.", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.810780131816864}, {"text": "semantic text understanding", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.7272406816482544}, {"text": "information extraction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7876251041889191}]}, {"text": "Popular methods separately address the Mention Detection (MD) and Entity Dis-ambiguation (ED) stages of EL, without lever-aging their mutual dependency.", "labels": [], "entities": [{"text": "Mention Detection (MD)", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8192309975624085}]}, {"text": "We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document.", "labels": [], "entities": []}, {"text": "The main idea is to consider all possible spans as potential mentions and learn con-textual similarity scores over their entity candidates that are useful for both MD and ED decisions.", "labels": [], "entities": []}, {"text": "Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention-entity map, without demanding other engineered features.", "labels": [], "entities": []}, {"text": "Empirically , we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available.", "labels": [], "entities": [{"text": "Gerbil platform", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.9357866048812866}]}, {"text": "Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 228, "end_pos": 236, "type": "METRIC", "confidence": 0.532123327255249}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2: EL strong matching results on the Gerbil platform. Micro and Macro F1 scores are shown. We  highlight the best and second best models, respectively. Training was done on AIDA-train set.", "labels": [], "entities": [{"text": "EL", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9817348122596741}, {"text": "Gerbil platform", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.936413049697876}, {"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.8806556463241577}, {"text": "AIDA-train set", "start_pos": 180, "end_pos": 194, "type": "DATASET", "confidence": 0.965010792016983}]}, {"text": " Table 3: AIDA A dataset: Gold mentions are split by the position they appear in the p(e|m) dictionary.  In each cell, the upper value is the percentage of the gold mentions that were annotated with the correct  entity (recall), whereas the lower value is the percentage of gold mentions for which our system's highest  scored entity is the ground truth entity, but that might not be annotated in the end because its score is  below the threshold \u03b4.", "labels": [], "entities": [{"text": "recall", "start_pos": 220, "end_pos": 226, "type": "METRIC", "confidence": 0.9660629034042358}]}]}