{"title": [{"text": "Modeling Composite Labels for Neural Morphological Tagging", "labels": [], "entities": [{"text": "Neural Morphological Tagging", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6589741011460623}]}], "abstractContent": [{"text": "Neural morphological tagging has been regarded as an extension to POS tagging task, treating each morphological tag as a mono-lithic label and ignoring its internal structure.", "labels": [], "entities": [{"text": "Neural morphological tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8322488268216451}, {"text": "POS tagging task", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.8108245134353638}]}, {"text": "We propose to view morphological tags as composite labels and explicitly model their internal structure in a neural sequence tag-ger.", "labels": [], "entities": []}, {"text": "For this, we explore three different neural architectures and compare their performance with both CRF and simple neural multiclass baselines.", "labels": [], "entities": []}, {"text": "We evaluate our models on 49 languages and show that the neural architecture that models the morphological labels as sequences of morphological category values performs significantly better than both baselines establishing state-of-the-art results in morphological tagging for most languages.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 251, "end_pos": 272, "type": "TASK", "confidence": 0.7222051620483398}]}], "introductionContent": [{"text": "The common approach to morphological tagging combines the set of word's morphological features into a single monolithic tag and then, similar to POS tagging, employs multiclass sequence classification models such as CRFs ( or recurrent neural networks (.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7094374299049377}, {"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.7381545007228851}]}, {"text": "This approach, however, has a number of limitations.", "labels": [], "entities": []}, {"text": "Firstly, it ignores the intrinsic compositional structure of the labels and treats two labels that differ only in the value of a single morphological category as completely independent; compare for instance labels and that only differ in the value of the NUM category.", "labels": [], "entities": []}, {"text": "Secondly, it introduces a data sparsity issue as the less frequent labels can have only few occurrences in the The source code is available at https://github.com/AleksTk/seq-morph-tagger training data.", "labels": [], "entities": []}, {"text": "Thirdly, it excludes the ability to predict labels not present in the training set which can bean issue for languages such as Turkish where the number of morphological tags is theoretically unlimited).", "labels": [], "entities": []}, {"text": "To address these problems we propose to treat morphological tags as composite labels and explicitly model their internal structure.", "labels": [], "entities": []}, {"text": "We hypothesise that by doing that, we are able to alleviate the sparsity problems, especially for languages with very large tagsets such as Turkish, Czech or Finnish, and at the same time also improve the accuracy over a baseline using monolithic labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9984782338142395}]}, {"text": "We explore three different neural architectures to model the compositionality of morphological labels.", "labels": [], "entities": []}, {"text": "In the first architecture, we model all morphological categories (including POS tag) as independent multiclass classifiers conditioned on the same contextual word representation.", "labels": [], "entities": []}, {"text": "The second architecture organises these multiclass classifiers into a hierarchy-the POS tag is predicted first and the values of morphological categories are predicted conditioned on the value of the predicted POS.", "labels": [], "entities": []}, {"text": "The third architecture models the label as a sequence of morphological category-value pairs.", "labels": [], "entities": []}, {"text": "All our models share the same neural encoder architecture based on bidirectional LSTMs to construct contextual representations for words (.", "labels": [], "entities": []}, {"text": "We evaluate all our models on 49 UD version 2.1 languages.", "labels": [], "entities": [{"text": "UD version 2.1 languages", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.6696595549583435}]}, {"text": "Experimental results show that our sequential model outperforms other neural counterparts establishing state-of-the-art results in morphological tagging for most languages.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.6788971424102783}]}, {"text": "We also confirm that all neural models perform significantly better than a competitive CRF baseline.", "labels": [], "entities": []}, {"text": "In short, our contributions can be summarised as follows: 1) We propose to model the compositional internal structure of complex morphological la-bels for morphological tagging in a neural sequence tagging framework; 2) We explore several neural architectures for modeling the composite morphological labels; 3) We find that tag representation based on the sequence learning model achieves state-of-the art performance on many languages.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.6662907898426056}, {"text": "tag representation", "start_pos": 325, "end_pos": 343, "type": "TASK", "confidence": 0.7216209769248962}]}, {"text": "4) We present state-of-the-art morphological tagging results on 49 languages on the UDv2.1 corpora.", "labels": [], "entities": [{"text": "UDv2.1 corpora", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.966549277305603}]}], "datasetContent": [{"text": "This section details the experimental setup.", "labels": [], "entities": []}, {"text": "We describe the data, then we introduce the baseline models and finally we report the hyperparameters of the models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics for all UDv2.1 datasets. For training sets we report the number of word tokens and types, the", "labels": [], "entities": [{"text": "UDv2.1 datasets", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.8949183821678162}]}, {"text": " Table 2: Hyperparameters for neural models.", "labels": [], "entities": []}, {"text": " Table 3: Morphological tagging accuracies on UDv2.1 test sets for MarMot (MMT) and MC baselines as well as for MCML,", "labels": [], "entities": [{"text": "Morphological tagging", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.867466002702713}, {"text": "accuracies", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.8518434762954712}, {"text": "UDv2.1 test sets", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9158331751823425}, {"text": "MarMot (MMT) and MC baselines", "start_pos": 67, "end_pos": 96, "type": "DATASET", "confidence": 0.8187054565974644}]}, {"text": " Table 4: Performance of SEQ and MC models on indi- vidual features reported as macro-averaged F1-scores.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9678722620010376}]}, {"text": " Table 5: Mean accuracy with standard deviation over  five independent runs for SEQ and MC models.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979524612426758}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8810539245605469}, {"text": "SEQ", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7851639986038208}]}, {"text": " Table 7: Accuracies of the tuned SEQ and MC models  compared to the mean accuracies in Table 5.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9994286894798279}]}, {"text": " Table 8: Accuracies for the SEQ model, Dozat et al.  (2017) and Heigold et al. (2017).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9935269355773926}, {"text": "SEQ", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8840596675872803}]}]}