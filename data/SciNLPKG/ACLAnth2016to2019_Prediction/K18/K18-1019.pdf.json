{"title": [{"text": "Bidirectional Generative Adversarial Networks for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6777970890204111}]}], "abstractContent": [{"text": "Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 94, "end_pos": 126, "type": "TASK", "confidence": 0.8381991386413574}]}, {"text": "However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discrimina-tor training.", "labels": [], "entities": []}, {"text": "To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adver-sarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated.", "labels": [], "entities": [{"text": "GAN", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9650844931602478}, {"text": "Neural Machine Translation (BGAN-NMT)", "start_pos": 138, "end_pos": 175, "type": "TASK", "confidence": 0.7493674357732137}]}, {"text": "To satisfy this property, generator and discrimina-tor are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model.", "labels": [], "entities": []}, {"text": "To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively.", "labels": [], "entities": []}, {"text": "Two GANs are alternately trained to update the parameters.", "labels": [], "entities": [{"text": "GANs", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.734832763671875}]}, {"text": "Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.7753907640775045}, {"text": "GAN training", "start_pos": 123, "end_pos": 135, "type": "TASK", "confidence": 0.8843633830547333}]}], "introductionContent": [{"text": "The past several years have witnessed the rapid development of Neural Machine Translation (NMT) * This work was done when the first author was the intern at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7760881533225378}, {"text": "Microsoft Research Asia", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.8265032370885214}]}, {"text": "(), from catching up with Statistical Machine Translation (SMT) ( to outperforming it by significant margins on many languages (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.810493270556132}]}, {"text": "The most common approach to training NMT is to maximize the conditional log-probability of the correct translation given the source sentence.", "labels": [], "entities": []}, {"text": "However, as argued in, the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that maybe never observed in the training data.", "labels": [], "entities": [{"text": "Maximum Likelihood Estimation (MLE)", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.5293657531340917}]}, {"text": "To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 178, "end_pos": 189, "type": "METRIC", "confidence": 0.9685955941677094}]}, {"text": "Generative Adversarial Network (GAN)) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (.", "labels": [], "entities": []}, {"text": "Formally, GAN consists of two \"adversarial\" models: a generator and a discriminator.", "labels": [], "entities": []}, {"text": "In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence pair is real or generated.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7669847905635834}]}, {"text": "Adversarial training between the two models involves optimizing a min-max objective, in which, the discriminator learns to distinguish whether a given data instance is real or fake, and the generator learns to confuse the discriminator by generating high-quality translation candidates.", "labels": [], "entities": []}, {"text": "Since the generated data is based on discrete symbols (words), we usually adopt policy gradient method ( to update model parameters of the generator.", "labels": [], "entities": []}, {"text": "Specifically, given a bunch of translation candidates sampled from the generator, confidence scores calculated by the discriminator are employed as rewards to update the generator.", "labels": [], "entities": []}, {"text": "However, in this training process, the discriminator typically suffers from inadequate training problem, leading to the instability of GAN training.", "labels": [], "entities": [{"text": "GAN", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9657391309738159}]}, {"text": "In practice, sampling large translation candidates is time-consuming for NMT system, so we only use a few samples to train the discriminator.", "labels": [], "entities": []}, {"text": "For a given source sentence, there is usually only one positive example (real target sentence).", "labels": [], "entities": []}, {"text": "If the sampled negative examples are also few, the discriminator will easily overfit to the data.", "labels": [], "entities": []}, {"text": "This is the inadequate training problem for the discriminator.", "labels": [], "entities": []}, {"text": "In such a case, rewards calculated by the discriminator could be biased, especially for unobserved samples.", "labels": [], "entities": []}, {"text": "These biased rewards will provide a wrong signal to the generator and make it update incorrectly, resulting in performance degradation of the generator.", "labels": [], "entities": []}, {"text": "Since such issue can occur repeatedly throughout the entire training process, GAN training becomes unstable and the performance of generator will drop drastically.", "labels": [], "entities": [{"text": "GAN", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9515948295593262}]}, {"text": "On the other hand, the generator has welldesigned properties that benefit the discriminator, since it models the probability distribution over the entire translation space so that the generator does not overfit to observed samples, while prior knowledge for unobserved samples is naturally considered.", "labels": [], "entities": []}, {"text": "At the same time, the generator also exhibits a certain ability to identify whether a given data instance is good enough.", "labels": [], "entities": []}, {"text": "For example, target-to-source translation model serves as the discriminator to improve source-to-target translation model (.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (aka BGAN-NMT), which employs a generator model to perform the role of the discriminator so as to handle inadequate training problem and stabilize GAN training.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.7548956871032715}, {"text": "GAN training", "start_pos": 260, "end_pos": 272, "type": "TASK", "confidence": 0.8664812445640564}]}, {"text": "To satisfy this property, both generator and discriminator of original GAN are designed to model the joint probability of sentence pairs, with the difference that, the generator model A is decomposed into a source language model and a source-to-target translation model, while the discriminator model B is formulated as a target language model and a target-to-source translation model.", "labels": [], "entities": []}, {"text": "Intuitively, we can also leverage A to act as the discriminator to improve B, and then improved B reversely serves as a better discriminator to guide the training of A.", "labels": [], "entities": []}, {"text": "To make use of this symmetry, we bring in an auxiliary GAN that adopts generator and discriminator models of original one as its own discriminator and generator respectively.", "labels": [], "entities": []}, {"text": "Then we design a joint training algorithm to alternately utilize these two GANs to update the source-to-target and target-tosource translation models.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on GermanEnglish and Chinese-English translation data sets.", "labels": [], "entities": [{"text": "GermanEnglish and Chinese-English translation data sets", "start_pos": 33, "end_pos": 88, "type": "DATASET", "confidence": 0.7524946381648382}]}, {"text": "Experimental results demonstrate that our BGAN-NMT not only achieves the stability of GAN training but also significantly improves translation performance over baseline systems.", "labels": [], "entities": [{"text": "BGAN-NMT", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.784014105796814}]}], "datasetContent": [{"text": "For German-English translation task, following previous work (, we select data from German-English machine translation track of IWSLT2014 evaluation tasks, which consists of sentence-aligned subtitles of TED and TEDx talks.", "labels": [], "entities": [{"text": "German-English translation task", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7440338830153147}, {"text": "IWSLT2014", "start_pos": 128, "end_pos": 137, "type": "DATASET", "confidence": 0.8016673922538757}]}, {"text": "We closely follow the pre-processing as described in.", "labels": [], "entities": []}, {"text": "The training corpus contains 153k sentence pairs with 2.83M English words and 2.68M German words.", "labels": [], "entities": []}, {"text": "The validation set comprises of 6,969 sentence pairs taken from the training data, and the test set is a combination of dev2010, dev2012, tst2010, tst2011 and tst2012 with total number of 6,750 sentence pairs.", "labels": [], "entities": []}, {"text": "For Chinese-English translation task, training data consists of a set of LDC datasets 1 , which has around 2.6M sentence pairs with 65.1M Chinese words and 67.1M English words respectively.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7741141319274902}, {"text": "LDC datasets", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.7390836477279663}]}, {"text": "Any sentence longer than 80 words is removed from training data.", "labels": [], "entities": []}, {"text": "NIST OpenMT 2006 evaluation set is used as the validation set, and datasets as test sets.", "labels": [], "entities": [{"text": "NIST OpenMT 2006 evaluation set", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.9166086435317993}]}, {"text": "We limit the vocabulary to contain up to 50K most frequent words on both source and target sides, and convert remaining words into the <unk> token.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with previous work on  IWSLT2014 German-English translation task.  The \"Baseline\" means the performance of pre- trained model used to warmly start training.", "labels": [], "entities": [{"text": "IWSLT2014 German-English translation task", "start_pos": 44, "end_pos": 85, "type": "TASK", "confidence": 0.7159633785486221}]}, {"text": " Table 2: Case-insensitive BLEU scores (%) on Chinese-English translation. The \"Average\" denotes the  average results of all datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9413725137710571}]}]}