{"title": [{"text": "SEx BiST: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the SEx BiST parser (Seman-tically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies).", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 101, "end_pos": 123, "type": "DATASET", "confidence": 0.8185489624738693}]}, {"text": "The main characteristic of our work is the encoding of three different modes of con-textual information for parsing: (i) Tree-bank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupervised learning from external resources.", "labels": [], "entities": []}, {"text": "Our parser performed well in the official end-to-end evaluation (73.02 LAS-4 th /26 teams, and 78.72 UAS-2 nd /26); remarkably , we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations.", "labels": [], "entities": [{"text": "LAS-4", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9495481848716736}, {"text": "UAS-2", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.7702827453613281}, {"text": "UAS", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.7109599113464355}]}, {"text": "Finally, we were also ranked 1 stat the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign .", "labels": [], "entities": [{"text": "event extraction", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6398987919092178}]}], "introductionContent": [{"text": "Feature representation methods are an essential element for neural dependency parsing.", "labels": [], "entities": [{"text": "Feature representation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7286551892757416}, {"text": "neural dependency parsing", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6534078617890676}]}, {"text": "Methods such as Feed Forward Neural Network (FFN)) or LSTM-based word representations) have been proposed to provide fine-grained token representations, and these methods provide state of the art performance.", "labels": [], "entities": []}, {"text": "However, learning efficient feature representations is still challenging, especially for underresourced languages.", "labels": [], "entities": []}, {"text": "One way to cope with the lack of training data is a multilingual approach, which makes it possible to use different corpora in different languages as training data.", "labels": [], "entities": []}, {"text": "In most cases, for instance in the CoNLL 2017 shared task (, the teams that have adopted this approach used a multilingual delexicalized parser (i.e. a multi-source parser trained without taking into account lexical features).", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8513986766338348}]}, {"text": "However, it is evident that delexicalized parsing cannot capture contextual features that depend on the meaning of words within the sentence.", "labels": [], "entities": []}, {"text": "Following previous proposals promoting a model-transfer approach with lexicalized feature representations (, we have developed the SEx BiST parser (Semantically EXtended Bi-LSTM parser), a multi-source trainable parser using three different contextualized lexical representations: \u2022 Corpus representation: a vector representation of each training corpus.", "labels": [], "entities": []}, {"text": "\u2022 Multilingual word representation: a multilingual word representation obtained by the projection of several pre-trained monolingual embeddings into a unique semantic space (following a linear transformation of each embedding).", "labels": [], "entities": [{"text": "Multilingual word representation", "start_pos": 2, "end_pos": 34, "type": "TASK", "confidence": 0.6407821973164877}]}, {"text": "\u2022 ELMo representation: token-based representation integrating abundant contexts gathered from external resources (.", "labels": [], "entities": [{"text": "ELMo representation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.6836613118648529}]}, {"text": "In this paper, we extend the multilingual graphbased parser proposed by with the three above representations.", "labels": [], "entities": []}, {"text": "Our parser is open source and available at: https://github.com/CoNLL-UD-2018/ LATTICE/.", "labels": [], "entities": [{"text": "LATTICE", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.48573148250579834}]}, {"text": "Our parser performed well in the official end-toend evaluation (73.02 LAS -4 th out of 26 teams, and 78.72 UAS -2 nd out of 26).", "labels": [], "entities": [{"text": "end-toend", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9625996947288513}, {"text": "LAS -4 th", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9628944247961044}, {"text": "UAS -2 nd", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.969653993844986}]}, {"text": "We obtained very good results for French, English and Korean where we were able to extensively exploit the three above features (for these languages, we obtained the best UAS performance on all the treebanks, and among the best LAS performance as well).", "labels": [], "entities": [{"text": "UAS", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.7489637732505798}, {"text": "LAS", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.9418588280677795}]}, {"text": "Unfortunately we were notable to exploit the same strategy for all the languages due to alack of a GPU and, correspondingly, time for training, and also due alack of training data for some languages.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "We first describe the feature extraction and representation methods (Section 2 and 3) and then present our POS tagger and our parser based on multi-task learning (Section 4).", "labels": [], "entities": [{"text": "feature extraction and representation", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.6966750174760818}]}, {"text": "We then give some details on our implementation (Section 5) and we finally provide an analysis of our official results (Section 6).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official experiment results for each corpus, where tr (Treebank), mu (Multilingual) and el  (ELMo) in the column Method denote the feature representation methods used (see Section 2 and 3).", "labels": [], "entities": [{"text": "ELMo", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9180945754051208}]}, {"text": " Table 2: Languages trained with multilingual  word embeddings and their ranking.", "labels": [], "entities": []}, {"text": " Table 3: Relative contribution of the different rep- resentation methods on the overall results.", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9931848645210266}]}, {"text": " Table 4: Official evaluation results on three EPE  task (see https://goo.gl/3Fmjke).", "labels": [], "entities": []}]}