{"title": [{"text": "Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks", "labels": [], "entities": [{"text": "Evolutionary Data Measures", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5987917482852936}, {"text": "Text Classification Tasks", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7671758234500885}]}], "abstractContent": [{"text": "Classification tasks are usually analysed and improved through new model architectures or hyperparameter optimisation but the underlying properties of datasets are discovered on an ad-hoc basis as errors occur.", "labels": [], "entities": [{"text": "Classification tasks", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.878259003162384}]}, {"text": "However, understanding the properties of the data is crucial in perfecting models.", "labels": [], "entities": []}, {"text": "In this paper we analyse exactly which characteristics of a dataset best determine how difficult that dataset is for the task of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.796267569065094}]}, {"text": "We then propose an intuitive measure of difficulty for text classification datasets which is simple and fast to calculate.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9303089380264282}, {"text": "text classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7576256096363068}]}, {"text": "We show that this measure gener-alises to unseen data by comparing it to state-of-the-art datasets and results.", "labels": [], "entities": []}, {"text": "This measure can be used to analyse the precise source of errors in a dataset and allows fast estimation of how difficult a dataset is to learn.", "labels": [], "entities": []}, {"text": "We searched for this measure by training 12 classical and neural network based models on 78 real-world datasets, then use a genetic algorithm to discover the best measure of difficulty.", "labels": [], "entities": []}, {"text": "Our difficulty-calculating code 1 and datasets 2 are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "If a machine learning (ML) model is trained on a dataset then the same machine learning model on the same dataset but with more granular labels will frequently have lower performance scores than the original model (see results in;;; ;;).", "labels": [], "entities": []}, {"text": "Adding more granularity to labels makes the dataset harder to learn -it increases the dataset's difficulty.", "labels": [], "entities": []}, {"text": "It is obvious that some datasets are more difficult for learning models than others, but is it possible to quantify this difficulty?", "labels": [], "entities": []}, {"text": "In order to do so, it would be necessary to understand exactly what characteristics of a dataset are good indicators of how well models will perform on it so that these could be combined into a single measure of difficulty.", "labels": [], "entities": []}, {"text": "Such a difficulty measure would be useful as an analysis tool and as a performance estimator.", "labels": [], "entities": []}, {"text": "As an analysis tool, it would highlight precisely what is causing difficulty in a dataset, reducing the time practitioners need spend analysing their data.", "labels": [], "entities": []}, {"text": "As a performance estimator, when practitioners approach new datasets they would be able to use this measure to predict how well models are likely to perform on the dataset.", "labels": [], "entities": []}, {"text": "The complexity of datasets for ML has been previously examined, but these works focused on analysing feature space data \u2208 IR n . These methods do not easily apply to natural language, because they would require the language be embedded into feature space in someway, for example with a word embedding model which introduces a dependency on the model used.", "labels": [], "entities": [{"text": "ML", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9664086699485779}]}, {"text": "We extend previous notions of difficulty to English language text classification, an important component of natural language processing (NLP) applicable to tasks such as sentiment analysis, news categorisation and automatic summarisation).", "labels": [], "entities": [{"text": "English language text classification", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.5701500996947289}, {"text": "sentiment analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.9420618116855621}, {"text": "news categorisation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.743932843208313}, {"text": "summarisation", "start_pos": 224, "end_pos": 237, "type": "TASK", "confidence": 0.6962082982063293}]}, {"text": "All of our recommended calculations depend only on counting the words in a dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "We gathered 27 real-world text classification datasets from public sources, summarised in; full descriptions are in Appendix A. We created 51 more datasets by taking two or more of the original 27 datasets and combining all of the data points from each into one dataset.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.6925407648086548}]}, {"text": "The label for each data item was the name of the dataset which the text originally came from.", "labels": [], "entities": []}, {"text": "We combined similar datasets in this way, for example  two different datasets of tweets, so that the classes would not be trivially distinguishable -there is no dataset to classify text as either a tweet or Shakespeare for example as this would be too easy for models.", "labels": [], "entities": []}, {"text": "The full list of combined datasets is in Appendix A.2.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.857818603515625}]}, {"text": "Our datasets focus on short text classification by limiting each data item to 100 words.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6893879175186157}]}, {"text": "We demonstrate that the difficulty measure we discover with this setup generalises to longer text classification in Section 3.1.", "labels": [], "entities": [{"text": "longer text classification", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.6902441581090292}]}, {"text": "All datasets were lowercase with no punctuation.", "labels": [], "entities": []}, {"text": "For datasets with no validation set, 15% of the training set was randomly sampled as a validation set at runtime.", "labels": [], "entities": []}, {"text": "We calculated 12 distinct statistics with different n-gram sizes to produce 48 statistics of each dataset.", "labels": [], "entities": []}, {"text": "These statistics are designed to increase in value as difficulty increases.", "labels": [], "entities": []}, {"text": "The 12 statistics are described here and a listing of the full 48 is in Appendix B in.", "labels": [], "entities": []}, {"text": "We used n-gram sizes from unigrams up to 5-grams and recorded the average of each statistic overall n-gram sizes.", "labels": [], "entities": []}, {"text": "All probability distributions were count-based -the probability of a particular n-gram / class / character was the count of occurrences of that particular entity divided by the total count of all entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The 27 different publicly available datasets we gathered with references.", "labels": [], "entities": []}, {"text": " Table 5. We used n-gram sizes from  unigrams up to 5-grams and recorded the average  of each statistic over all n-gram sizes. All proba- bility distributions were count-based -the proba- bility of a particular n-gram / class / character was  the count of occurrences of that particular entity", "labels": [], "entities": []}, {"text": " Table 3: Difficulty measure D2 compared to recent results from papers on large-scale text classification. The  correlation column reports the correlation between difficulty measure D2 and the model scores for that row.", "labels": [], "entities": [{"text": "Difficulty measure D2", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8991758227348328}, {"text": "text classification", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.686259001493454}]}, {"text": " Table 4: Means and standard deviations of the con- stituent statistics of difficulty measure D2 across the  78 datasets from this paper and the eight datasets from  Zhang et al. (2015).", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9820907711982727}, {"text": "difficulty measure D2", "start_pos": 75, "end_pos": 96, "type": "METRIC", "confidence": 0.6944374839464823}]}]}