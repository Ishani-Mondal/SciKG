{"title": [{"text": "Multilingual Universal Dependency Parsing from Raw Text with Low-resource Language Enhancement", "labels": [], "entities": [{"text": "Multilingual Universal Dependency Parsing", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6707213073968887}]}], "abstractContent": [{"text": "This paper describes the system of our team Phoenix for participating CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.7833830118179321}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.750151538848877}]}, {"text": "Given the annotated gold standard data in CoNLL-U format, we train the to-kenizer, tagger and parser separately for each treebank based on an open source pipeline tool UDPipe.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 168, "end_pos": 174, "type": "DATASET", "confidence": 0.9229380488395691}]}, {"text": "Our system reads the plain texts for input, performs the pre-processing steps (tokenization, lemmas, morphology) and finally outputs the syntactic dependencies.", "labels": [], "entities": []}, {"text": "For the low-resource languages with no training data, we use cross-lingual techniques to build models with some close languages instead.", "labels": [], "entities": []}, {"text": "In the official evaluation, our system achieves the macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS, MLAS and BLEX respectively.", "labels": [], "entities": [{"text": "LAS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.46251410245895386}, {"text": "MLAS", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.7337808012962341}, {"text": "BLEX", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.977200984954834}]}], "introductionContent": [{"text": "Universal Dependencies (UD) () is a framework that provides cross-linguistically consistent grammatical annotations for various languages, which enables comparative evaluations for some cross-lingual learning tasks.", "labels": [], "entities": []}, {"text": "As a followup of, the goal of CoNLL 2018 UD Shared Task () is to develop multilingual dependency parsers from raw text for many typologically different languages with training data from UD project.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 30, "end_pos": 55, "type": "DATASET", "confidence": 0.8001287937164306}]}, {"text": "The task comprises 82 test sets from 57 languages.", "labels": [], "entities": []}, {"text": "However, there area category of low-resource languages that have little or no training data, which requires cross-lingual techniques with the help of the data from other languages.", "labels": [], "entities": []}, {"text": "In this paper, we present the system of our team Phoenix for multilingual universal dependency parsing from raw text.", "labels": [], "entities": [{"text": "multilingual universal dependency parsing from raw text", "start_pos": 61, "end_pos": 116, "type": "TASK", "confidence": 0.691239731652396}]}, {"text": "The targeted task is a challenging one in terms of deep learning based natural language processing (.", "labels": [], "entities": []}, {"text": "We adopt the trainable open source tool UDPipe 1.2 ( to train the dependency parser for each test set with UD version 2.2 ( ) treebanks as training data.", "labels": [], "entities": [{"text": "UDPipe 1.2", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8753548562526703}]}, {"text": "There are three main components of our model to perform, tokenization, Part-of-Speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9799456596374512}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.5744284510612487}, {"text": "dependency parsing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7939419746398926}]}, {"text": "When evaluated on the web interface of TIRA () platform, the system reads the raw text for input and chooses the corresponding model fora particular test set with a model selector.", "labels": [], "entities": []}, {"text": "After the tokenization and tagging on the raw text, the system finally outputs the syntactic dependencies in the CoNLL-U format.", "labels": [], "entities": []}, {"text": "To deal with the low-resource languages which have no training data, some cross-lingual techniques are applied by training with other related or close languages.", "labels": [], "entities": []}, {"text": "Our official submission obtains macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS, MLAS and BLEX on all treebanks.", "labels": [], "entities": [{"text": "LAS", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.5049954652786255}, {"text": "MLAS", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7878807187080383}, {"text": "BLEX", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.987194836139679}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the architecture overview of our system.", "labels": [], "entities": []}, {"text": "Section 3 gives the implementation details and the specific strategies applied for the low-resource languages.", "labels": [], "entities": []}, {"text": "Finally, we report and an-alyze the official results with the three main evaluation metrics in Section 4.", "labels": [], "entities": []}, {"text": "illustrates the overall architecture of our system for training and predicting.", "labels": [], "entities": [{"text": "predicting", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.8536936640739441}]}, {"text": "In the training procedure, the system takes as input the treebanks (training set) of CoNLL-U format and trains a model for each of them.", "labels": [], "entities": []}, {"text": "Every model has three components, tokenizer, tagger and parser.", "labels": [], "entities": []}, {"text": "As we train the parser, pretrained word embeddings for word forms in the word2vec format are applied.", "labels": [], "entities": []}, {"text": "In the predicting procedure, the system takes as input the raw text (test set) and selects a model according to the language code and treebank code.", "labels": [], "entities": [{"text": "predicting", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9671416878700256}]}, {"text": "With the selected model, the system outputs the syntactic head and the type of the dependency relation for each word.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parameters for training the tokenizers.", "labels": [], "entities": []}, {"text": " Table 4: Parameters for training the parsers.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9429596662521362}]}, {"text": " Table 6: Evaluation scores of our system on dif- ferent types of treebanks.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of our system (Ours) and  baseline system (Base) on PUD treebanks and  low-resource languages.", "labels": [], "entities": []}, {"text": " Table 8: LAS F1 scores and rankings of our sys- tem on PUD treebanks.", "labels": [], "entities": [{"text": "LAS F1 scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8248335917790731}, {"text": "PUD treebanks", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9212659299373627}]}, {"text": " Table 9: LAS F1 scores and rankings of our sys- tem on low-resource languages.", "labels": [], "entities": [{"text": "LAS F1 scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7911083499590555}]}, {"text": " Table 10: Other metrics and rankings of our sys- tem.", "labels": [], "entities": []}]}