{"title": [{"text": "Comparing Models of Associative Meaning: An Empirical Investigation of Reference in Simple Language Games", "labels": [], "entities": []}], "abstractContent": [{"text": "Simple reference games (Wittgenstein, 1953) are of central theoretical and empirical importance in the study of situated language use.", "labels": [], "entities": []}, {"text": "Although language provides rich, com-positional truth-conditional semantics to facilitate reference, speakers and listeners may sometimes lack the overall lexical and cog-nitive resources to guarantee successful reference through these means alone.", "labels": [], "entities": []}, {"text": "However, language also has rich associational structures that can serve as a further resource for achieving successful reference.", "labels": [], "entities": []}, {"text": "Here we investigate this use of associational information in a setting where only associational information is available: a simplified version of the popular game Codenames.", "labels": [], "entities": []}, {"text": "Using optimal experiment design techniques, we compare a range of models varying in the type of associative information deployed and in level of pragmatic sophistication against human behavior.", "labels": [], "entities": []}, {"text": "In this setting we find that listeners' behavior reflects direct bigram collocational associations more strongly than word-embedding or semantic knowledge graph-based associations and that there is little evidence for pragmatically sophisticated behavior by either speakers or listeners of the type that might be predicted by recursive-reasoning models such as the Rational Speech Acts theory.", "labels": [], "entities": []}, {"text": "These results shed light on the nature of the lexical resources that speakers and listeners can bring to bear in achieving reference through associative meaning alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "In his 1953 book Philosophical Investigations, Wittgenstein makes the argument for studying simple reference games to learn about the nature of language.", "labels": [], "entities": []}, {"text": "Various applications of this idea in different fields, including linguistics, cognitive science, artificial intelligence (, and behaviorbased robotics have validated this fundamental insight and demonstrated the theoretical and empirical importance of studying language learning and use in simplified contexts.", "labels": [], "entities": []}, {"text": "Here we describe a novel framework that uses a simple reference game to study the semantic resources speakers and listeners use to facilitate reference.", "labels": [], "entities": []}, {"text": "In particular, placing strong constraints on word choice and modes of interaction allows us to better isolate specific aspects that contribute towards the complexity of natural language semantics.", "labels": [], "entities": []}, {"text": "Language provides a multitude of different resources for its users to cooperatively achieve reference.", "labels": [], "entities": []}, {"text": "In particular, language provides truthconditional semantic structures.", "labels": [], "entities": []}, {"text": "These information structures are characterized in terms of their logical truth conditions and can be precisely stated using formal logic.", "labels": [], "entities": []}, {"text": "Across many cases, however, successful reference cannot be guaranteed through these means alone.", "labels": [], "entities": []}, {"text": "Another possible source of semantic information are associative resources (e.g. the meaning associations of 'nurse' with 'female nurse' rather than 'male nurse').", "labels": [], "entities": []}, {"text": "The question of how to best formally characterize these rich associative structures to adequately account for our linguistic abilities is still largely unresolved.", "labels": [], "entities": []}, {"text": "We compare the performance of different models in accounting for human behavior in a simple reference game, a modified version of the popular board game Codenames.", "labels": [], "entities": []}, {"text": "Crucially, in this setting, only associational information is available.", "labels": [], "entities": []}, {"text": "To allow us to additionally address questions about possible pragmatic effects when playing the game, our models are formulated in the context of the Rational Speech Act (RSA) framework.", "labels": [], "entities": [{"text": "Rational Speech Act (RSA)", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.6282539119323095}]}, {"text": "The candidate models of human semantic reasoning we consider involve different types of associative resources and different degrees of pragmatic sophistication by speaker and listener.", "labels": [], "entities": [{"text": "human semantic reasoning", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6936802367369334}]}, {"text": "The models correspond to qualitatively different sources of information, including collocations, distributional similarity across contexts, topic similarity, or common-sense conceptual relatedness.", "labels": [], "entities": []}, {"text": "In the closest predecessor to our work, used observational data from the television game Password, where the goal is to guess a target word on an associated cue word freely generated, to model whether speaker and listener alignment based on their differential reliance on forward vs backward word associations (estimated using the experimental norms of).", "labels": [], "entities": [{"text": "Password", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.909160852432251}]}, {"text": "They found that similar mixtures of forward and backward associations best explained both speaker and hearer behaviors, suggesting game participants are well calibrated and cooperative with another, but did not investigate the nature of the lexical knowledge accounting for the associations underlying participant behavior.", "labels": [], "entities": []}, {"text": "In this paper, we construct a simplified reference game involving word associations where constrained sets of potential reference clues words and reference target words are provided.", "labels": [], "entities": []}, {"text": "We construct a variety of different semantic association measures and conduct a series of experiments to test which source of information humans use.", "labels": [], "entities": []}, {"text": "Furthermore, we combine these measures with the RSA framework to derive predictions about pragmatic behavior on the task.", "labels": [], "entities": [{"text": "RSA", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8779576420783997}]}], "datasetContent": [{"text": "We create a simplified version of the board game Codenames where the objective is fora speaker to select a clue word that allows a listener to correctly identify a set of target words.", "labels": [], "entities": []}, {"text": "Subjects play one scenario per turn.", "labels": [], "entities": []}, {"text": "A scenario consists of a set of codenames drawn from a list of 50 common nouns, two of which are targets while the remaining nouns are non-targets.", "labels": [], "entities": []}, {"text": "While both listeners and speakers always seethe set of codenames, only the speaker knows which nouns are targets and non-targets (see, the listener views three identical black and white cards).", "labels": [], "entities": []}, {"text": "We refer to any combination of two nouns as a noun pair.", "labels": [], "entities": []}, {"text": "Each scenario also contains a set of clues drawn from 100 descriptive adjectives.", "labels": [], "entities": []}, {"text": "Throughout the paper, we will interchangeably refer to codenames as nouns and to clues as adjectives.", "labels": [], "entities": []}, {"text": "A configuration is a scenario that additionally includes an index, either indicating the target noun pair (speaker configuration) or the adjective that was provided to the listener (listener configuration).", "labels": [], "entities": []}, {"text": "Thus, while scenarios are just lists of adjectives and nouns, there are #codenames 2 possible speaker configurations and #clues possible listener configurations.", "labels": [], "entities": []}, {"text": "Speakers and listeners participated in separate versions of the experiment, but were told that they would be teamed up with another player to increase engagement with the task.", "labels": [], "entities": []}, {"text": "Subjects were either in the speaker role or in the listener role.", "labels": [], "entities": []}, {"text": "On each trial, depending on their role, they were either given a speaker configuration or a listener configuration, that is, a scenario plus corresponding index.", "labels": [], "entities": []}, {"text": "The speaker's task is to select a single adjective to best communicate the target noun pair without including any non-target nouns.", "labels": [], "entities": []}, {"text": "For the listener task, participants are given an adjective and asked to select the two nouns that the adjective most likely refers to.", "labels": [], "entities": []}, {"text": "To quantify the difficulty of a particular configuration for participants, we additionally asked them to rate how confident they were in their answer on a scale from 1 (least confident) to 5 (most confident).", "labels": [], "entities": []}, {"text": "We conducted four experiments for which we recruited a total of 1460 subjects on Amazon's Mechanical Turk platform.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk platform", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.9421316146850586}]}, {"text": "Each subject completed 20 different configurations, lasting approximately 7 \u2212 10 minutes and were paid a fixed amount of $0.60 for their participation.", "labels": [], "entities": []}, {"text": "We make all data and analysis code available 1 .  Despite focusing on a relatively small set of nouns and adjectives, the space of possible experimental configurations is still too large to allow exhaustive search.", "labels": [], "entities": []}, {"text": "Furthermore, the model rank correlations displayed in suggest that naively picking configurations could result in strongly correlated predictions.", "labels": [], "entities": []}, {"text": "To generate experimental configurations that are highly informative with respect to discriminating between different semantic association metrics, we employed Bayesian optimal ex- LDA was excluded in the final two rounds of experiments.", "labels": [], "entities": []}, {"text": "With the current training regime, its success in fitting human responses was substantially smaller than the other three semantic association measures we chose.", "labels": [], "entities": [{"text": "fitting human responses", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8647132317225138}]}, {"text": "perimental design (OED) techniques.", "labels": [], "entities": [{"text": "perimental design (OED)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8097032606601715}]}, {"text": "Assuming that a particular response y is recorded (a choice of noun pair or adjective), the utility of an experimental configuration c, u(y, c), is proportional to the mutual information between the distributions over models M before and after obtaining datum y.", "labels": [], "entities": []}, {"text": "Since response y has not yet been observed, we compute the expectation of u(y, c) with respect toy to obtain the desired (global) utility of the configuration U (c).", "labels": [], "entities": []}, {"text": "Assuming a uniform prior distribution over models M , the equations simplify in the following way.", "labels": [], "entities": []}, {"text": "Optimal designs were computed using Monte Carlo methods for sampling-based stochastic optimization.", "labels": [], "entities": []}, {"text": "This example speaker configuration shows how different clues are preferred by different models: 'empty' most often co-occurs with 'heart' and 'phone' and is thus favored by the Bigram model.", "labels": [], "entities": [{"text": "Bigram", "start_pos": 177, "end_pos": 183, "type": "DATASET", "confidence": 0.9326592683792114}]}, {"text": "ConceptNet assigns a high association score to 'heart' and 'empty' but the adjective 'rough' fits the noun pair better overall when using product aggregation.", "labels": [], "entities": []}, {"text": "Similarly, since 'insane' appears most often in the context windows for both 'heart' and 'phone', it is the top prediction for Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9660131335258484}]}, {"text": "We also show human data for the configuration, which shows a strong preference for the adjective preferred by the Bigram model. that model predictions diverge strongly, with each semantic measure predicting a different response and little distributional overlap.", "labels": [], "entities": []}, {"text": "The accompanying matrices illustrate how the different models arrive at those predictions.", "labels": [], "entities": []}, {"text": "In this experiment, configurations on each trial consisted of five nouns and eight adjectives.", "labels": [], "entities": []}, {"text": "Subjects completed the task either in the speaker or in the listener condition.", "labels": [], "entities": []}, {"text": "OED was not used for this first experiment, instead words were chosen according to heuristic criteria, detailed in the supplementary material.", "labels": [], "entities": [{"text": "OED", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.952282726764679}]}, {"text": "We did not collect confidence scores for this experiment.", "labels": [], "entities": []}, {"text": "Using the literal speaker and listener equations from in combination with different semantic association metrics, we derived probabilistic predictions for each configuration.", "labels": [], "entities": []}, {"text": "Predictions were scored against human responses using the two performance scores outlined above.", "labels": [], "entities": []}, {"text": "We also explored fits of pragmatic versions of the models to the data but found that they were qualitatively similar.", "labels": [], "entities": []}, {"text": "5 shows that, while all models except LDA perform above chance, the Bigram metric performs best on both the listener task and the speaker task.", "labels": [], "entities": []}, {"text": "While the difference on the listener side is large, differences between Bigram and ConceptNet on the speaker side are substantially smaller.", "labels": [], "entities": []}, {"text": "To gain insights into why the results for Bigram and ConceptNet were so similar we directly evaluated the models' predictions against each other, quantifying how often they make the same top prediction (1C), or how rank correlated their predictions are on average (1C).", "labels": [], "entities": []}, {"text": "The bottom left matrix in shows that the measure's similarity on top answer and rank correlation on the speaker task might in part stem from their overlapping predictions.", "labels": [], "entities": []}, {"text": "This highlights a basic design issue: The experimental designs we picked might not allow us to fully distinguish the different models by capitalizing on the differences they make in their predictions.", "labels": [], "entities": []}, {"text": "To remedy this shortcoming and obtain better discriminability on the speaker side, we utilized optimal experiment design techniques (Section 2.4) to overcome the limitations associated with Experiment 1.", "labels": [], "entities": []}, {"text": "The procedure was run for the four designated models (Bigram, Word2Vec, ConceptNet and LDA), separately for the listener and the speaker side, for 100, 000 sampling iterations.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.7925018072128296}]}, {"text": "We reduced the number of nouns and adjectives to three and four, respectively, significantly decreasing search complexity.", "labels": [], "entities": []}, {"text": "Since some high utility configurations differed only by one or two words, and some words generally occurred much more frequently than others, we eliminated configurations that differed from higher utility configurations in less than two words and by limiting the total occurrence of a word across configurations to 20.", "labels": [], "entities": []}, {"text": "This reduced the top 500 configurations for each down to 119 speaker and 137 listener configurations.", "labels": [], "entities": []}, {"text": "Results from prior experiments show that the difficulty of a configuration, which is not explicitly operationalized and incorporated into our search process, may significantly impact response quality.", "labels": [], "entities": []}, {"text": "To ensure that the selected configurations generate meaningful responses from human participants, we ran a preliminary experiment on the filtered configurations and only admitted those configurations to the main experiment whose confidence rating was above mean (58 speaker and 67 listener configurations).", "labels": [], "entities": []}, {"text": "Model fits were again calculated using the literal speaker and listener equations in section 2.1.", "labels": [], "entities": []}, {"text": "summarizes how well the four semantic association measures fit human responses.", "labels": [], "entities": []}, {"text": "For the listener task, the Bigram association metric scores marginally higher than Word2Vec in top answer but strongly outperforms other models in rank correlation.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9235730171203613}]}, {"text": "While ConceptNet (top answer) and Word2Vec (rank correlation) win on the speaker side, surprisingly, Bigram performs considerably worse than in experiment 1.", "labels": [], "entities": [{"text": "Word2Vec (rank correlation)", "start_pos": 34, "end_pos": 61, "type": "METRIC", "confidence": 0.6845078289508819}]}, {"text": "In terms of task diffi-  culty, speakers judged the task to be more difficult than listeners (t = 8.27, p < 0.001).", "labels": [], "entities": []}, {"text": "The surprisingly low performance of the Bigram model could be due to data sparsity that was systematically exploited by OED.", "labels": [], "entities": [{"text": "OED", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.9338130354881287}]}, {"text": "On average, 45% of the Bigram values for the noun-adjective associations used in the experiment, which are used to compute model predictions, were effectively zero (i.e. zero counts are quantile normalized to 1e \u22127 ).", "labels": [], "entities": []}, {"text": "This level of sparsity is much higher than both the total set of Bigram associations (17%) as well as in subsequent speaker configurations (30%).", "labels": [], "entities": [{"text": "sparsity", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9650481939315796}]}, {"text": "In contrast, on the listener side, the percentage of values with near zero probability is similar between this set of configurations and those in later experiments.", "labels": [], "entities": []}, {"text": "To further explore the data sparsity hypothesis, we computed model fits using bigram associations derived from the Twitter corpus, where only 5% of speaker configurations are sparse.", "labels": [], "entities": [{"text": "Twitter corpus", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.8040984272956848}]}, {"text": "This raises the fit of the Bigram model to human data to 0.37, even though the Twitter and Google Bigram features are highly correlated.", "labels": [], "entities": []}, {"text": "Irrespective of how much of the bad performance of the bigram model could be explained away by data sparsity, the basic asymmetry between Bigram's performance across the two experimental conditions seems to hold.", "labels": [], "entities": []}, {"text": "One likely confound in assessing speaker and listener resources is that we searched for high utility configurations independently, and that this difference in material is driving the difference in performance.", "labels": [], "entities": []}, {"text": "This hypothesis was directly addressed in the next experiment.", "labels": [], "entities": []}, {"text": "To further investigate potential asymmetries between the speaker and the listener condition, we modified the design optimization procedure to jointly optimize the geometric mean of all speaker and listener configurations for the same scenario.", "labels": [], "entities": []}, {"text": "Our intention was to collect data for all possible configurations of a scenario so that we could have listeners and speakers engage with the identical words.", "labels": [], "entities": []}, {"text": "We then applied the same filtering procedure to reduce our set to 120 scenarios (760 unique configurations).", "labels": [], "entities": []}, {"text": "Here we restrict ourselves to three adjectives, matching the number of choices on the speaker side and minimizing differences in task difficulty.", "labels": [], "entities": []}, {"text": "Due to its weak performance in the previous experiments, we eliminated LDA from the comparison set for subsequent experiments.", "labels": [], "entities": [{"text": "LDA", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8640905022621155}]}, {"text": "summarizes how well the remaining three semantic association measures fit human responses.", "labels": [], "entities": []}, {"text": "In contrast to Experiment 2, and inline with the results from Experiment 1, we find that Bigram associations perform best in both the listener and speaker condition.", "labels": [], "entities": []}, {"text": "This difference is more pronounced for the Rank correlation measure, where other models perform at chance with the exception of Word2Vec in the listener task.", "labels": [], "entities": [{"text": "Rank correlation measure", "start_pos": 43, "end_pos": 67, "type": "METRIC", "confidence": 0.8407443165779114}, {"text": "Word2Vec", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9202672839164734}]}, {"text": "Based on this result, it appears likely that the difference in Experiment 2 was driven by choice of scenario configurations.", "labels": [], "entities": []}, {"text": "When adding the constraint of finding scenarios that are jointly informative in discriminating between models on the speaker side and on the listener side, Bigram robustly outperforms other semantic association measures.", "labels": [], "entities": []}, {"text": "While reducing the number of adjectives from 4 to 3 did not result in a significant de-  crease in difficulty, as measured by mean confidence, the difference in difficulty between speaker and listener task (t = 9.38, p < 0.0001) still remains significant.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.7720565795898438}]}, {"text": "Since correlation matrices from the stimuli in Experiment 3), which was only optimized to elicit differences between the semantic association metrics, shows that the literal models' predictions are highly correlated with their pragmatic counterparts, we ran another design optimization iteration to find configurations for which literal and pragmatic models strongly disagree.", "labels": [], "entities": []}, {"text": "We restricted ourselves to the Bigram semantic association metric because it was the highest performing model in nine out of twelve cases (across the speaker/listener sides of three experiments, on two performance scores).", "labels": [], "entities": []}, {"text": "Again, we jointly optimized over speaker and listener configurations, using the literal version of the model and the corresponding pragmatic model with \u03b1 = 1 from applying the RSA equations in.1.", "labels": [], "entities": []}, {"text": "After filtering for overlap and limiting word co-occurrence as in the previous experiments, we select the highest 60 utility scenarios later reduced to 40 by highest mean confidence.", "labels": [], "entities": []}, {"text": "In the experiment, we again tested each scenario in all its six configurations.", "labels": [], "entities": []}, {"text": "summarizes the top answer and rank correlation scores for literal and pragmatic models of various degrees of pragmatic behavior (\u03b1 = [0.1, 1.0, 5.0]).", "labels": [], "entities": [{"text": "rank correlation scores", "start_pos": 30, "end_pos": 53, "type": "METRIC", "confidence": 0.809183935324351}]}, {"text": "We do not see strongly scalar inferential behavior of the type predicted by RSA when applied to our setting.", "labels": [], "entities": []}, {"text": "The literal model outperforms all pragmatic models by a large mar- gin across both performance scores and experimental conditions.", "labels": [], "entities": []}, {"text": "As before, speakers judged the task to be more difficult than listeners (t = 6.56, p < 0.0001).", "labels": [], "entities": []}, {"text": "This stark difference between pragmatic and literal models is surprising.", "labels": [], "entities": []}, {"text": "illustrates a common pattern that helps to better interpret this behavior.", "labels": [], "entities": []}, {"text": "The literal model's predictions are more categorical and best reflect the probabilities from the original association values after aggregation.", "labels": [], "entities": []}, {"text": "Through the recursive reasoning from RSA, small differences in raw probabilities, which might be non-obvious to humans, are magnified to sway top pragmatic model prediction.", "labels": [], "entities": [{"text": "RSA", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9410168528556824}]}, {"text": "For example, 'history' and 'performance' are initially the best choice for the given adjective 'dying' (see top row of matrices in), the pair is an even better choice for the adjective 'violent'.", "labels": [], "entities": []}, {"text": "The nonobvious advantage that 'dying' has over 'violent' for the pair 'wedding' and 'performance' is becomes dominant in the S 1 normalization where this pair becomes the best pair for the clue 'dying'.", "labels": [], "entities": [{"text": "S 1 normalization", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.5142057041327158}]}], "tableCaptions": [{"text": " Table 2: Comparison of semantic association mea- sures in matching human responses in Experiment  1 (No OED). Chance performance is 0.1 (listener)  and 0.125 (speaker) for top answer and 0 for rank  correlation.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of semantic association mea- sures to human data from Experiment 2 (separate  speaker and listener OED). Chance performance  is 0.33 (listener) and 0.25 (speaker) for top answer  and 0 for rank correlation.", "labels": [], "entities": [{"text": "OED", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9674922823905945}]}, {"text": " Table 4: Comparison of semantic association mea- sures to human data from Experiment 3 (joint  speaker-listener OED). Chance performance is  0.33 (listener and speaker) for top answer and 0  for rank correlation.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of pragmatic RSA models  in predicting human responses in Experiment 4.  Chance performance is 0.33 (listener and speaker)  for top answer and 0 for rank correlation.", "labels": [], "entities": [{"text": "RSA", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9711361527442932}]}, {"text": " Table 6: Summary of average success on speakers  and listeners in human data.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7246072292327881}]}]}