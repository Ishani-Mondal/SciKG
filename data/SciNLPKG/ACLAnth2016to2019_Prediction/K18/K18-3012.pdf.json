{"title": [{"text": "What can we gain from language models for morphological inflection?", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates the attempts to augment neural-based inflection models with character-based language models.", "labels": [], "entities": []}, {"text": "We found that inmost cases this slightly improves performance, however , the effect is marginal.", "labels": [], "entities": []}, {"text": "We also propose another language-model based approach that can be used as a strong baseline in low-resource setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological inflection is a task of automatic reconstruction of surface word form given its source form, called lemma, and morphological characteristic of required form.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.881391316652298}]}, {"text": "For example, in Spanish the input word contar together with features v;fin;ind;imp;pst;3;pl should be transformed to contaban.", "labels": [], "entities": []}, {"text": "The obvious way to solve such a task is to handcode transformations using finitestate rules.", "labels": [], "entities": []}, {"text": "However, this approach requires an expert knowledge of the language under consideration and can be extremely time-consuming for the languages with complex morphology.", "labels": [], "entities": []}, {"text": "Therefore a machine learning algorithm should be developed to efficiently solve this task for any language.", "labels": [], "entities": []}, {"text": "Such an algorithm must be able to generalize from known lemma-features-word triples to previously unseen ones, mimicking human behaviour when inflecting a neologism in its native language or an unknown word in a foreign one.", "labels": [], "entities": []}, {"text": "In this setting automatic inflection becomes an instance of string transduction problem, which makes conditional random fields a natural baseline model as suggested in.", "labels": [], "entities": []}, {"text": "Another popular approach is to predict transformation pattern, either as a pair of prefix and suffix changes as used in the baseline model for.", "labels": [], "entities": []}, {"text": "For example, consider a Czech adjective kr\u00e1sn\u00b4kr\u00e1sn\u00b4y and its superlative form nejkr\u00e1\u0161nej\u0161\u00ed.", "labels": [], "entities": []}, {"text": "The inflection pattern can be encoded as a pair of prefix rule $ \u2192 $nej and a suffix rul\u00e9 y \u2192 ej\u0161\u00ed.", "labels": [], "entities": []}, {"text": "Such encoding is, however, too weak to deal with infixation and root vowel alterations, required, for example, for Spanish verb volver and its +Pres+Sg+1 form vuelvo.", "labels": [], "entities": []}, {"text": "An abstract paradigm approach ( compresses this transformation to 1+o+2+er#1+ue+2+o, where digits stand for variables (the parts of verb stem), and constant fragments define the paradigm.", "labels": [], "entities": []}, {"text": "In both cases in order to predict the inflected word form one suffices to guess the transformation pattern, thus solving a standard classification problem.", "labels": [], "entities": []}, {"text": "Both mentioned models (CRFs and abstract paradigms) were quite successful in, however, they were clearly outperformed by neural network approaches.", "labels": [], "entities": []}, {"text": "Indeed, string transduction problems are successfully solved using neural methods, for example, in machine translation.", "labels": [], "entities": [{"text": "string transduction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7868720591068268}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7573590874671936}]}, {"text": "The work of) adopts the seq2seq model with soft attention of ().", "labels": [], "entities": []}, {"text": "It defeated not only non-neural systems mentioned earlier, but also other neural approaches.", "labels": [], "entities": []}, {"text": "It shows that attention mechanism is crucial for word inflection.", "labels": [], "entities": [{"text": "word inflection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7683646380901337}]}, {"text": "However, in contrast to machine translation, a symbol of output word is less prone to depend from multiple input symbols, than a translated word from multiple source words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7216586768627167}]}, {"text": "Consequently, the attention weight is usually concentrated on a single source symbol, being more a pointer than a distributed probability mass.", "labels": [], "entities": []}, {"text": "Moreover, this pointer traverses the source word from left to right in order to generate the inflected form.", "labels": [], "entities": []}, {"text": "All that motivated the hard attention model of, which outperformed the soft attention approaches.", "labels": [], "entities": []}, {"text": "The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol.", "labels": [], "entities": []}, {"text": "This model was further improved by, whose system was the winner of Sigmorphon 2017 evaluation campaign (.", "labels": [], "entities": [{"text": "Sigmorphon 2017 evaluation", "start_pos": 67, "end_pos": 93, "type": "DATASET", "confidence": 0.8974256118138632}]}, {"text": "The approach of Makarov et al. was especially successful in low and medium resource setting, while in high resource setting it achieves an impressive accuracy of over 95% . Does it mean that no further research is required and hard attention method equipped with copy mechanism is the final solution for automatic inflection problem?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9991162419319153}]}, {"text": "Actually, not, since the quality of the winning approach was much lower on medium (about 85%) and low (below 50%) datasets.", "labels": [], "entities": []}, {"text": "This lower quality is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not coverall possible paradigms for this particular form.", "labels": [], "entities": []}, {"text": "For example, Russian verbs has several tens of variants to produce the +Pres+Sg+1 form.", "labels": [], "entities": []}, {"text": "Consequently, to improve the inflection accuracy the system should extract more information from the whole language, not only the instances of the given form.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9655150175094604}]}, {"text": "This task is easier for agglutinative languages with regular inflection paradigm: to predict, say, the +Pres+Sg+1 form in Turkish, the system has just to observe several singular verb form (not necessarily of the first person) to extract the singular suffix and several first person form (of any number and tense).", "labels": [], "entities": []}, {"text": "In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible.", "labels": [], "entities": []}, {"text": "However, this decomposition is already realised in model of () since the grammatical features are treated as a list of atomic elements, not as entire label.", "labels": [], "entities": []}, {"text": "A new source of information about the whole language are the laws of its phonetics.", "labels": [], "entities": []}, {"text": "For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns.", "labels": [], "entities": []}, {"text": "A natural way to capture the phonetic patterns are character language models.", "labels": [], "entities": []}, {"text": "They were already applied to the problem of inflection in and produced a strong boost over the baseline system.", "labels": [], "entities": []}, {"text": "The work of Sorokin used simple ngram models, however, neural language models) has shown their superiority over earlier approaches for various tasks.", "labels": [], "entities": []}, {"text": "Summarizing, our approach was to enrich the model of () with the language model component.", "labels": [], "entities": []}, {"text": "We followed the architecture of (, whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer.", "labels": [], "entities": []}, {"text": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of () for most of the languages.", "labels": [], "entities": []}, {"text": "We conclude that the language model job is already executed by the decoder.", "labels": [], "entities": []}, {"text": "However, given the vitality of language model approach in other areas of modern NLP (, we describe our attempts in detail to give other researchers the ideas for future work in this direction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of baseline and LM-equipped models", "labels": [], "entities": []}, {"text": " Table 2: Comparison of baseline and LM-paradigm models", "labels": [], "entities": []}]}