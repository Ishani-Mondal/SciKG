{"title": [{"text": "CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8597466349601746}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8225923776626587}]}], "abstractContent": [{"text": "Every year, the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets.", "labels": [], "entities": [{"text": "Conference on Computational Natural Language Learning (CoNLL)", "start_pos": 16, "end_pos": 77, "type": "TASK", "confidence": 0.6915436850653754}]}, {"text": "In 2018, one of two tasks was devoted to learning dependency parsers fora large number of languages, in a real-world setting without any gold-standard annotation on the input.", "labels": [], "entities": []}, {"text": "All test sets followed the unified annotation scheme of Universal Dependencies (Nivre et al., 2016).", "labels": [], "entities": []}, {"text": "This shared task constitutes a 2 nd edition-the first one took place in 2017 (Zeman et al., 2017); the main metric from 2017 was kept, allowing for easy comparison, and two new main metrics were introduced.", "labels": [], "entities": []}, {"text": "New datasets added to the Universal Dependencies collection between mid-2017 and the spring of 2018 contributed to the increased difficulty of the task this year.", "labels": [], "entities": [{"text": "Universal Dependencies collection", "start_pos": 26, "end_pos": 59, "type": "DATASET", "confidence": 0.8508041898409525}]}, {"text": "In this overview paper, we define the task and the updated evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The 2017 CoNLL shared task on universal dependency parsing) picked up the thread from the influential shared tasks in 2006 and 2007 and evolved it in two ways: (1) the parsing process started from raw text rather than gold standard tokenization and part-of-speech tagging, and (2) the syntactic representations were consistent across languages thanks to the Universal Dependencies framework).", "labels": [], "entities": [{"text": "universal dependency parsing", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.5469308098157247}, {"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9647617340087891}, {"text": "part-of-speech tagging", "start_pos": 249, "end_pos": 271, "type": "TASK", "confidence": 0.7076305747032166}]}, {"text": "The 2018 CoNLL shared task on universal dependency parsing starts from the same premises but adds a focus on morphological analysis as well as data from new languages.", "labels": [], "entities": [{"text": "universal dependency parsing", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6338557004928589}]}, {"text": "Like last year, participating systems minimally had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.", "labels": [], "entities": []}, {"text": "In addition, this year's task featured new metrics that also scored a system's capacity to predict a morphological analysis of each word, including a part-of-speech tag, morphological features, and a lemma.", "labels": [], "entities": []}, {"text": "Regardless of metric, the assumption was that the input should be raw text, with no gold-standard word or sentence segmentation, and no gold-standard morphological annotation.", "labels": [], "entities": [{"text": "word or sentence segmentation", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.6917010471224785}]}, {"text": "However, for teams who wanted to concentrate on one or more subtasks, segmentation and morphology predicted by the baseline UDPipe system () was made available just like last year.", "labels": [], "entities": []}, {"text": "There are eight new languages this year: Afrikaans, Armenian, Breton, Faroese, Naija, Old French, Serbian, and Thai; see Section 2 for more details.", "labels": [], "entities": []}, {"text": "The two new evaluation metrics are described in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are three main evaluation scores, dubbed LAS, MLAS and BLEX.", "labels": [], "entities": [{"text": "LAS", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9872164130210876}, {"text": "MLAS", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.5019176006317139}, {"text": "BLEX", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9946234226226807}]}, {"text": "All three metrics reflect word segmentation and relations between content words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7233240008354187}]}, {"text": "LAS is identical to the main metric of the 2017 task, allowing for easy comparison; the other two metrics include part-of-speech tags, morphological features and lemmas.", "labels": [], "entities": [{"text": "LAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6471276879310608}]}, {"text": "Participants who wanted to decrease task complexity could concentrate on improvements in just one metric; however, all systems were evaluated with all three metrics, and participants were strongly encouraged to output all relevant annotation, even if they just copy values predicted by the baseline model.", "labels": [], "entities": []}, {"text": "When parsers are applied to raw text, the metric must be adjusted to the possibility that the number of nodes in gold-standard annotation and in the system output vary.", "labels": [], "entities": []}, {"text": "Therefore, the evaluation starts with aligning system nodes and gold nodes.", "labels": [], "entities": []}, {"text": "A dependency relation cannot be counted as correct if one of the nodes could not be aligned to a gold node.", "labels": [], "entities": []}, {"text": "See Section 3.4 and onward for more details on alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.8692135810852051}]}, {"text": "The evaluation software is a Python script that computes the three main metrics and a number of additional statistics.", "labels": [], "entities": []}, {"text": "It is freely available for download from the shared task website.", "labels": [], "entities": []}, {"text": "The metrics described above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications.", "labels": [], "entities": []}, {"text": "Nevertheless, such correlations are not automatically granted.", "labels": [], "entities": []}, {"text": "We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution.", "labels": [], "entities": [{"text": "biological event extraction", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.646076907714208}, {"text": "opinion analysis", "start_pos": 165, "end_pos": 181, "type": "TASK", "confidence": 0.8705216944217682}, {"text": "negation scope resolution", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.9405926068623861}]}, {"text": "This optional track involves English only.", "labels": [], "entities": []}, {"text": "It is organized in collaboration with the EPE initiative; 7 for details see.", "labels": [], "entities": [{"text": "EPE initiative", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9354198575019836}]}, {"text": "Traditionally, evaluations in shared tasks are halfblind (the test data are shared with participants while the ground truth is withheld).", "labels": [], "entities": []}, {"text": "TIRA enables fully blind evaluation, where the software is locked in a datalock together with the test data, its output is recorded but all communication channels to the outside are closed or tightly moderated.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4977726340293884}]}, {"text": "The participants do not even seethe input to their software.", "labels": [], "entities": []}, {"text": "This feature of TIRA was not too important in the present task, as UD data is not secret, and the participants were simply trusted that they would not exploit any knowledge of the test data they might have access to.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.615539014339447}]}, {"text": "However, closing down all communication channels also has its downsides, since participants cannot check their running software; before the system run completes, even the task moderator does not see whether the system is really producing output and not just sitting in an endless loop.", "labels": [], "entities": []}, {"text": "In order to alleviate this extra burden, we made two modifications compared to the previous year: 1.", "labels": [], "entities": []}, {"text": "Participants were explicitly advised to invoke shorter runs that process only a subset of the test files.", "labels": [], "entities": []}, {"text": "The organizers would then stitch the partial runs into one set of results.", "labels": [], "entities": []}, {"text": "2. Participants were able to see their scores on the test set rounded to the nearest multiple of 5%.", "labels": [], "entities": []}, {"text": "This way they could spot anomalies possibly caused by illselected models.", "labels": [], "entities": []}, {"text": "The exact scores remained hidden because we did not want the participants to fine-tune their systems against the test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Ranking of the participating systems by  the labeled attachment F 1 -score (LAS), macro- averaged over 82 test sets. Pairs of systems with  significantly (p < 0.05) different LAS are sepa- rated by a line. Citations refer to the correspond- ing system-description papers in this volume.", "labels": [], "entities": [{"text": "labeled attachment F 1 -score (LAS)", "start_pos": 55, "end_pos": 90, "type": "METRIC", "confidence": 0.7998026874330308}]}, {"text": " Table 7: Ranking of the participating systems by  MLAS, macro-averaged over 82 test sets. Pairs  of systems with significantly (p < 0.05) different  MLAS are separated by a line.", "labels": [], "entities": []}, {"text": " Table 8: Ranking of the participating systems by  BLEX, macro-averaged over 82 test sets. Pairs  of systems with significantly (p < 0.05) different  BLEX are separated by a line.", "labels": [], "entities": [{"text": "BLEX", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9396884441375732}, {"text": "BLEX", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9840472340583801}]}, {"text": " Table 9: Tokenization, word segmentation and  sentence segmentation (ordered by word F 1  scores; out-of-order scores in the other two  columns are bold).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7982378005981445}, {"text": "sentence segmentation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7351369559764862}, {"text": "word F 1  scores", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.7711662575602531}]}, {"text": " Table 10: Universal POS tags, features and lem- mas (ordered by UPOS F 1 scores; out-of-order  scores in the other two columns are bold).", "labels": [], "entities": [{"text": "UPOS F 1 scores", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.6557915434241295}]}, {"text": " Table 11: Average LAS on the 61 \"big\" treebanks  (ordered by LAS F 1 scores; out-of-order scores in  the other two columns are bold).", "labels": [], "entities": [{"text": "Average", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9930604696273804}, {"text": "LAS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.6936578154563904}, {"text": "LAS F 1 scores", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9246606975793839}]}, {"text": " Table 12: Average LAS, MLAS and BLEX on the  9 low-resource languages: Armenian (hy), Bre- ton (br), Buryat (bxr), Faroese (fo), Kazakh (kk),  Kurmanji (kmr), Naija (pcm), Thai (th) and Upper  Sorbian (hsb) (ordered by LAS F 1 scores; out-of- order scores in the other two columns are bold).", "labels": [], "entities": [{"text": "MLAS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7178621292114258}, {"text": "BLEX", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9973447918891907}, {"text": "Bre", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9647251963615417}, {"text": "LAS F 1 scores", "start_pos": 220, "end_pos": 234, "type": "METRIC", "confidence": 0.8062964230775833}]}, {"text": " Table 13: Average attachment score on the 7  small treebanks: Galician TreeGal, Irish, Latin  Perseus, North S\u00e1mi, Norwegian Nynorsk LIA,  Russian Taiga and Slovenian SST (ordered by  LAS F 1 scores; out-of-order scores in the other  two columns are bold).", "labels": [], "entities": [{"text": "Average attachment score", "start_pos": 11, "end_pos": 35, "type": "METRIC", "confidence": 0.8562730352083842}, {"text": "Galician TreeGal", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9094385206699371}, {"text": "LAS F 1 scores", "start_pos": 185, "end_pos": 199, "type": "METRIC", "confidence": 0.9140332639217377}]}, {"text": " Table 14: Average attachment score on the 5 addi- tional test sets for high-resource languages: Czech  PUD, English PUD, Finnish PUD, Japanese Mod- ern and Swedish PUD (ordered by LAS F 1 scores;  out-of-order scores in the other two columns are  bold).", "labels": [], "entities": [{"text": "Average attachment score", "start_pos": 11, "end_pos": 35, "type": "METRIC", "confidence": 0.754659632841746}, {"text": "LAS F 1 scores", "start_pos": 181, "end_pos": 195, "type": "METRIC", "confidence": 0.9393647909164429}]}, {"text": " Table 15: Treebank ranking by best parser LAS  (Avg=average LAS over all systems, out-of-order  scores in bold).", "labels": [], "entities": [{"text": "Avg", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9922977089881897}]}, {"text": " Table 16: Treebank ranking by best parser MLAS.", "labels": [], "entities": []}, {"text": " Table 17: Treebank ranking by best parser BLEX.", "labels": [], "entities": [{"text": "BLEX", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9921078085899353}]}, {"text": " Table 18: Treebank ranking by difference between  average parser LAS and MLAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.6986044049263}, {"text": "MLAS", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.8593686819076538}]}, {"text": " Table 19: Treebanks with most difficult word seg- mentation (by average parser F 1 ).", "labels": [], "entities": [{"text": "seg- mentation", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.5653586089611053}]}, {"text": " Table 20: Treebanks with most difficult sentence  segmentation (by average parser F 1 ).", "labels": [], "entities": [{"text": "sentence  segmentation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7413078248500824}]}]}