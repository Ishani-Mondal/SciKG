{"title": [{"text": "Aggregated Semantic Matching for Short Text Entity Linking", "labels": [], "entities": [{"text": "Aggregated Semantic Matching", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6925231913725535}, {"text": "Short Text Entity Linking", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6049950197339058}]}], "abstractContent": [{"text": "The task of entity linking aims to identify concepts mentioned in a text fragments and link them to a reference knowledge base.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7238889336585999}]}, {"text": "Entity linking in long text has been well studied in previous work.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8216749131679535}]}, {"text": "However, short text entity linking is more challenging since the texts are noisy and less coherent.", "labels": [], "entities": [{"text": "short text entity linking", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.6260040029883385}]}, {"text": "To better utilize the local information provided in short texts, we propose a novel neural network framework, Aggregated Semantic Matching (ASM), in which two different aspects of semantic information between the local context and the candidate entity are captured via representation-based and interaction-based neural semantic matching models, and then two matching signals work jointly for disambiguation with a rank aggregation mechanism.", "labels": [], "entities": [{"text": "Aggregated Semantic Matching (ASM)", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.7264132499694824}]}, {"text": "Our evaluation shows that the proposed model outperforms the state-of-the-arts on public tweet datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of entity linking aims to link a mention that appears in apiece of text to an entry (i.e. entity) in a knowledge base.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7424473166465759}]}, {"text": "For example, as shown in, given a mention Trump in a tweet, it should be linked to the entity Donald Trump 1 in Wikipedia.", "labels": [], "entities": []}, {"text": "Recent research has shown that entity linking can help better understand the text of a document) and benefits several tasks, including named entity recognition ( and information retrieval ().", "labels": [], "entities": [{"text": "entity linking", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.731963500380516}, {"text": "named entity recognition", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.6479359666506449}, {"text": "information retrieval", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.7821701467037201}]}, {"text": "The research of entity linking mainly considers two types of documents: long text (e.g. news articles and web documents) and short text (e.g. tweets).", "labels": [], "entities": [{"text": "entity linking", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7306991219520569}]}, {"text": "In this paper, we focus on short text, particularly tweet entity linking.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.716789166132609}]}, {"text": "* Correspondence author is Rong Pan.", "labels": [], "entities": []}, {"text": "This work was done when the first and second author were interns and the third author was an employee at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.8284181952476501}]}, {"text": "1 https://en.wikipedia.org/wiki/Donald Trump", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experimental results on tweet entity linking.", "labels": [], "entities": [{"text": "tweet entity linking", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7135019898414612}]}, {"text": "Particularly, we investigate the difference between two semantic matching models and the effectiveness of jointly combining these two semantic matching signals.", "labels": [], "entities": []}, {"text": "In our experiments, we evaluate our proposed model ASM on the following two datasets.", "labels": [], "entities": [{"text": "ASM", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9830964803695679}]}, {"text": "NEEL In this paper, we use accuracy as the major evaluation metric for entity disambiguation.", "labels": [], "entities": [{"text": "NEEL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8651816248893738}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9992546439170837}, {"text": "entity disambiguation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.713329628109932}]}, {"text": "Formally, we denote N as the number of queries and M as the number of correctly linked mentions given the gold mention (the top-ranked entity is the golden entity), accuracy = MN . Besides, we use precision, recall and F1 measure to evaluate the end-toend system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9996497631072998}, {"text": "MN", "start_pos": 176, "end_pos": 178, "type": "METRIC", "confidence": 0.9236851334571838}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9995455145835876}, {"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.999329686164856}, {"text": "F1 measure", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.9830678701400757}]}, {"text": "Formally, we denote N as the number of mentions identified by a system and M as the correctly linked mentions.", "labels": [], "entities": []}, {"text": "Thus, precision = MN , recall = MN and F 1 = 2 * precision * recall precision+recall .  In our main experiment, we compare our proposed approaches with the following baselines: (a) The officially ranked 1st and 2nd systems in NEEL 2016 challenge.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9994820952415466}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9992021918296814}, {"text": "F", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.9833567142486572}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9924668669700623}, {"text": "recall precision", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9390814006328583}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.7788227796554565}, {"text": "NEEL 2016 challenge", "start_pos": 226, "end_pos": 245, "type": "DATASET", "confidence": 0.8413276871045431}]}, {"text": "We denote these two systems as To fairly compare with the baselines of Cucerzan and M-CNN, we use the same mention detection and candidate generation for them and our approaches.", "labels": [], "entities": []}, {"text": "We train an LSTM-CRF based tagger ( for mention detection by using the NEEL training dataset.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7997928857803345}, {"text": "NEEL training dataset", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.8934337298075358}]}, {"text": "The precision, recall, and F1 of mention detection on NEEL testing dataset are 96.1%, 89.2%, 92.6% respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997085928916931}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9994716048240662}, {"text": "F1 of mention detection", "start_pos": 27, "end_pos": 50, "type": "METRIC", "confidence": 0.9073446840047836}, {"text": "NEEL testing dataset", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9486417373021444}]}, {"text": "The precision, recall, and F1 of mention detection on MSR-TEL dataset are 80.3% 83.8% and 82% respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997065663337708}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9995943903923035}, {"text": "F1 of mention detection", "start_pos": 27, "end_pos": 50, "type": "METRIC", "confidence": 0.9042329490184784}, {"text": "MSR-TEL dataset", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.954628199338913}]}, {"text": "As we described in the previous section, we use the heuristic rules for candidate generation.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7724885046482086}]}, {"text": "The recall of candidate generation on NEEL and MSR-TEL is 88.7% and 92.5%.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.999387264251709}, {"text": "NEEL", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7890563607215881}, {"text": "MSR-TEL", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.7967681884765625}]}, {"text": "When training our model, we use the stochastic gradient descent algorithm and the AdaDelta optimizer.", "labels": [], "entities": []}, {"text": "The gradients are computed via back-propagation.", "labels": [], "entities": []}, {"text": "The dimensionality of the hidden units in convolution neural network is set to 300.", "labels": [], "entities": []}, {"text": "All the parameters are initialized with a uniform distribution U (\u22120.01, 0.01).", "labels": [], "entities": []}, {"text": "Since there is NIL entity in the dataset, we tune a NIL threshold for the prediction of NIL entities according to the validation dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: End-to-end performance of the systems on the two datasets", "labels": [], "entities": []}, {"text": " Table 4: The performance of two semantic match- ing models and their combinations on NEEL  dataset.", "labels": [], "entities": [{"text": "NEEL  dataset", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.962295800447464}]}, {"text": " Table 3. Similarly, we observe that our proposed  ASM outperforms baseline systems.", "labels": [], "entities": [{"text": "ASM", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.989065408706665}]}, {"text": " Table 7: Comparison of rank aggregation and lin- ear combination on two datasets.", "labels": [], "entities": []}]}