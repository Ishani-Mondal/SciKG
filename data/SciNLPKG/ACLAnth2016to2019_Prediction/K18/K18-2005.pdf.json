{"title": [{"text": "Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation", "labels": [], "entities": [{"text": "UD Parsing", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.8427595794200897}]}], "abstractContent": [{"text": "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies", "start_pos": 60, "end_pos": 146, "type": "TASK", "confidence": 0.7189545104136834}]}, {"text": "We base our submission on Stanford's winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and dependency parser; 2) ensem-bling parsers trained with different initial-ization.", "labels": [], "entities": [{"text": "speech tagger", "start_pos": 193, "end_pos": 206, "type": "TASK", "confidence": 0.7005075663328171}]}, {"text": "We also explore different ways of concatenating treebanks for further improvements.", "labels": [], "entities": []}, {"text": "Experimental results on the development data show the effectiveness of our methods.", "labels": [], "entities": []}, {"text": "In the final evaluation, our system was ranked first according to LAS (75.84%) and outperformed the other systems by a large margin.", "labels": [], "entities": [{"text": "LAS", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9900467991828918}]}], "introductionContent": [{"text": "In this paper, we describe our system (HIT-SCIR) submitted to CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies (.", "labels": [], "entities": [{"text": "HIT-SCIR", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.6883171200752258}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.7710931241512299}]}, {"text": "We base our system on Stanford's winning system ( for the CoNLL 2017 shared task (. and its extension () have shown very competitive performance in both the shared task ( and previous parsing works (;.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.890381246805191}]}, {"text": "A natural question that arises is how can we further improve their part of speech (POS) tagger and dependency parser via a simple yet effective technique.", "labels": [], "entities": [{"text": "part of speech (POS) tagger", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6436110011168888}, {"text": "dependency parser", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.6685080379247665}]}, {"text": "In our system, we make two noteworthy extensions to their tagger and parser: \u2022 Incorporating the deep contextualized word embeddings (, ELMo: Embeddings from Language Models) into the word representaton ( \u00a73); \u2022 Ensembling parsers trained with different initialization ( \u00a74).", "labels": [], "entities": []}, {"text": "For some languages in the shared task, multiple treebanks of different domains are provided.", "labels": [], "entities": []}, {"text": "Treebanks which are of the same language families are provided as well.", "labels": [], "entities": []}, {"text": "Letting these treebanks help each other has been shown an effective way to improve parsing performance in both the crosslingual-cross-domain parsing community and last year's shared tasks ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9765769839286804}, {"text": "crosslingual-cross-domain parsing community", "start_pos": 115, "end_pos": 158, "type": "TASK", "confidence": 0.6216784119606018}]}, {"text": "In our system, we apply the simple concatenation to the treebanks that are potentially helpful to each other and explore different ways of concatenation to improve the parser's performance ( \u00a75).", "labels": [], "entities": []}, {"text": "In dealing with the small treebanks and treebanks from low-resource languages ( \u00a76), we adopt the word embedding transfer idea in the crosslingual dependency parsing ( and use the bilingual word vectors transformation technique ( to map fasttext 2 word embeddings ( of the source rich-resource language and target low-resource language into the same space.", "labels": [], "entities": [{"text": "crosslingual dependency parsing", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.6482820709546407}]}, {"text": "The transferred parser trained on the source language is used for the target low-resource language.", "labels": [], "entities": []}, {"text": "We conduct experiments on the development data to study the effects of ELMo, parser ensemble, and treebank concatenation.", "labels": [], "entities": []}, {"text": "Experimental results show that these techniques substantially im-prove the parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9808223843574524}]}, {"text": "Using these techniques, our system achieved an averaged LAS of 75.84 on the official test set and was ranked the first according to).", "labels": [], "entities": [{"text": "LAS", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9991956353187561}]}, {"text": "This result significantly outperforms the others by a large margin.", "labels": [], "entities": []}, {"text": "We release our pre-trained ELMo for many languages at https://github.com/ HIT-SCIR/ELMoForManyLangs.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the development set and cross validation, we choose the best model and data combination and the choices are shown in along with the test evaluation.", "labels": [], "entities": []}, {"text": "From this table, we can see that our system gains more improvements when both ELMo and parser ensemble are used.", "labels": [], "entities": []}, {"text": "For some treebanks, concatenation also contributes to the improvements.", "labels": [], "entities": []}, {"text": "Parsing Japanese, Vietnamese, and Chinese clearly benefits from better word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7334510087966919}]}, {"text": "Since most of the participant teams use single parser for their system, we also remove the parser ensemble and do a post-contest evaluation.", "labels": [], "entities": []}, {"text": "The results are also shown in this table.", "labels": [], "entities": []}, {"text": "Our system without ensemble achieves an macroaveraged LAS of 75.26, which unofficially ranks the first according to LAS in the shared task.", "labels": [], "entities": [{"text": "LAS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9272414445877075}, {"text": "LAS", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9913272261619568}]}, {"text": "We report the time and memory consumption.", "labels": [], "entities": []}, {"text": "A full run over the 82 test sets on the TIRA virtual machine) takes about 40 hours and consumes about 4G RAM memory.", "labels": [], "entities": [{"text": "TIRA virtual machine", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.875123659769694}]}], "tableCaptions": [{"text": " Table 2: The developement performance with cross-domain concatenation for languages which has mul- tiple treebanks. single means training the parser on it own treebank without concatenation. # train shows  the number of training sentences in the treebank measured in thousand.", "labels": [], "entities": []}, {"text": " Table 3: The 5-fold cross validation results for the cross-domain concatenation of treebank which does  not have development set.", "labels": [], "entities": []}, {"text": " Table 4: Cross-lingual concatenation results. The results for ug udt and uk iu are obtained on the devel- opment set. The results for ga idt and sme giella are obtained with udpipe by 5-fold cross validation.", "labels": [], "entities": []}, {"text": " Table 5: The effects of improved preprocessing on  the parsing performance. The first block shows  the effects of sentence segmentation improvement.  \u2206-sent. means the sentence segmentation F-score  difference between Uppsala segmentor and ud- pipe. The second block shows the effects of word  segmentation improvement. \u2206-word means the  word segmentation in F-score difference between  SCIR tokenizer and udpipe.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.6828232258558273}, {"text": "F-score  difference", "start_pos": 191, "end_pos": 210, "type": "METRIC", "confidence": 0.8243741095066071}, {"text": "Uppsala", "start_pos": 219, "end_pos": 226, "type": "DATASET", "confidence": 0.9667919278144836}, {"text": "word  segmentation", "start_pos": 289, "end_pos": 307, "type": "TASK", "confidence": 0.6636484265327454}, {"text": "F-score", "start_pos": 360, "end_pos": 367, "type": "METRIC", "confidence": 0.9893172383308411}]}]}