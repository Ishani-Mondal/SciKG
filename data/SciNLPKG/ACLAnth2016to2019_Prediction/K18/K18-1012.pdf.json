{"title": [{"text": "Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven Question Generation and Neural based Question Answering", "labels": [], "entities": [{"text": "Linguistically Driven Question Generation", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.5976512804627419}, {"text": "Neural based Question Answering", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.5948079898953438}]}], "abstractContent": [{"text": "Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like English.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.855999743938446}, {"text": "comprehension reading (RC)", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.642071133852005}]}, {"text": "In recent times, there has been a rapid growth of multilingual contents on the web, and this has posed several challenges to the existing QA systems.", "labels": [], "entities": []}, {"text": "Code-mixing is one such challenge that makes the task even more complex.", "labels": [], "entities": []}, {"text": "In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neu-ral network based architecture for code-mixed question answering (CMQA).", "labels": [], "entities": [{"text": "code-mixed question generation (CMQG)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.7323299944400787}, {"text": "code-mixed question answering (CMQA)", "start_pos": 150, "end_pos": 186, "type": "TASK", "confidence": 0.7953808804353079}]}, {"text": "For evaluation, we manually create the code-mixed questions for Hindi-English language pair.", "labels": [], "entities": []}, {"text": "In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, viz.", "labels": [], "entities": []}, {"text": "Experiments show that our proposed model achieves encouraging performance on CMQG and CMQA.", "labels": [], "entities": [{"text": "CMQA", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8761019706726074}]}], "introductionContent": [{"text": "The people who are multilingual in nature often switchback and forth between their native languages and the foreign (popular) languages to express themselves on the web.", "labels": [], "entities": []}, {"text": "This is very common nowadays, particularly when people express their opinions (or making any communication) through various social media platforms.", "labels": [], "entities": []}, {"text": "This phenomenon of embedding the morphemes, words, phrases, etc. of one language into another is popularly termed as code-mixing (CM)).", "labels": [], "entities": []}, {"text": "The recent study has uncovered that users frequently use question patterns, namely 'how' (38%), 'why' (24%), 'where' (15%), 'what' (11%), and 'which' (12%) in their queries as opposed to a 'statement query'.", "labels": [], "entities": []}, {"text": "* Work carried out during the internship at IIT Patna Presently, the search engines have become intelligent and are capable enough to provide precise answer to a natural language query/question .Several virtual assistants such as Siri, Cortana, Alexa, Google Assistant, etc are also equipped with these facilities.", "labels": [], "entities": [{"text": "IIT Patna", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.5811386108398438}]}, {"text": "However, these search engines and virtual assistants are efficient only in handling the queries written in English.", "labels": [], "entities": []}, {"text": "Let us consider the following two representations (English and code-mixed) of the same question.", "labels": [], "entities": []}, {"text": "(i) Q: \"Who is the foreign secretary of USA?\"", "labels": [], "entities": []}, {"text": "(ii) Q: \"USA ke foreign secretary koun hai?\"", "labels": [], "entities": []}, {"text": "(Trans:\"Who is the foreign secretary of USA?\")", "labels": [], "entities": []}, {"text": "Search engines are able to provide the exact answer to the first question.", "labels": [], "entities": []}, {"text": "It is to be noted that although both the questions are same, the search engine is unable to return the exact answer for the second question, which is code-mixed in nature.", "labels": [], "entities": []}, {"text": "It rather returns the top-most relevant web pages.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework for codemixed question generation (CMQG) as well as code-mixed question answering (CMQA) involving English and Hindi.", "labels": [], "entities": [{"text": "codemixed question generation", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6437067091464996}, {"text": "code-mixed question answering (CMQA)", "start_pos": 90, "end_pos": 126, "type": "TASK", "confidence": 0.7459114591280619}]}, {"text": "Firstly, we propose a linguistically motivated technique for generating the code-mixed questions.", "labels": [], "entities": []}, {"text": "We followed this approach as we did not have access to any labeled data for code-mixed question generation.", "labels": [], "entities": [{"text": "code-mixed question generation", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.6567252576351166}]}, {"text": "Thereafter, we propose an effective framework based on deep neural network for Code-mixed Question Answering (CMQA).", "labels": [], "entities": [{"text": "Code-mixed Question Answering (CMQA)", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.7270865142345428}]}, {"text": "In our proposed CMQA technique, we use multiple attention based recurrent units to represent the code-mixed questions and the English passages.", "labels": [], "entities": []}, {"text": "Finally, our answer-type focused network (attentive towards the answer-type of the question being asked) extracts the answer fora given code-mixed question.", "labels": [], "entities": []}, {"text": "We summarize our contributions as follows: (i).", "labels": [], "entities": []}, {"text": "We propose a linguistically motivated unsuper-vised algorithm for Code-mixed Question Generation (CMQG).", "labels": [], "entities": [{"text": "Code-mixed Question Generation (CMQG)", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.7077266772588094}]}, {"text": "We propose a bilinear attention and answer-type focused neural framework to deal with CMQA.", "labels": [], "entities": []}, {"text": "We create two CMQA datasets to further explore the research on CMQA.", "labels": [], "entities": [{"text": "CMQA datasets", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.8843773603439331}]}, {"text": "In addition to this, we manually create a code-mixed question dataset, and subsequently a code-mixed question classification dataset.", "labels": [], "entities": []}, {"text": "(iv) We provide a stateof-the-art setup to extract answers from the English passages for the corresponding code-mixed questions.", "labels": [], "entities": []}, {"text": "The source code of our proposed systems and the datasets can be found here 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report the datasets and the experimental setups.", "labels": [], "entities": []}, {"text": "For CMQG task, we require the input question to be in Hindi.", "labels": [], "entities": []}, {"text": "We use the manually created Hindi questions obtained from the Hindi-English question answering dataset ().", "labels": [], "entities": []}, {"text": "We generate the code-mixed questions by our proposed approach (c.f. Section 3).", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of our proposed CMQG algorithm, we also manually formulate 6 the HindiEnglish code-mixed questions.", "labels": [], "entities": [{"text": "HindiEnglish code-mixed questions", "start_pos": 102, "end_pos": 135, "type": "DATASET", "confidence": 0.8816970189412435}]}, {"text": "Details of this dataset are shown in.", "labels": [], "entities": []}, {"text": "We compute the complexity of code-mixing using the metric, Code-mixing Index (CMI) score.", "labels": [], "entities": [{"text": "Code-mixing Index (CMI) score", "start_pos": 59, "end_pos": 88, "type": "METRIC", "confidence": 0.865792786081632}]}, {"text": "We randomly split the dataset into training, development and test set..", "labels": [], "entities": []}, {"text": "The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser  the performance of CMQG in terms of accuracy, BLEU () and ROUGE) score.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9724453091621399}, {"text": "PoS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.6848876774311066}, {"text": "Hindi Shallow Parser", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.8922556042671204}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9995393753051758}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9993317723274231}, {"text": "ROUGE) score", "start_pos": 156, "end_pos": 168, "type": "METRIC", "confidence": 0.9668788909912109}]}, {"text": "CMQA datasets contain the words both in Roman script and English.", "labels": [], "entities": [{"text": "CMQA datasets", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9668509066104889}]}, {"text": "For English, we use the fastText () word embedding of dimension 300.", "labels": [], "entities": []}, {"text": "We use the Hindi sentences from, and then transliterate it into the Roman script.", "labels": [], "entities": []}, {"text": "These sentences are used to train the word embeddings of dimension 300 by the word embedding algorithm ().", "labels": [], "entities": []}, {"text": "Finally, we align monolingual vectors of English and Roman words in an unified vector space using a linear transformation matrix learned by the approach as discussed in.", "labels": [], "entities": []}, {"text": "Other optimal hyper-parameters are set to: character embedding dimension=50, GRU hidden unit size=150, CNN filter size=150, filter size=3, 4, batch size=60, # of epochs=100, initial learning rate=0.001.", "labels": [], "entities": [{"text": "GRU hidden unit size", "start_pos": 77, "end_pos": 97, "type": "METRIC", "confidence": 0.8149741739034653}, {"text": "CNN filter size", "start_pos": 103, "end_pos": 118, "type": "METRIC", "confidence": 0.5412606795628866}, {"text": "initial learning rate", "start_pos": 174, "end_pos": 195, "type": "METRIC", "confidence": 0.8594208359718323}]}, {"text": "Optimal values of the hyperparameters are decided based on the model performance on the development set of CM-SQuAD dataset.", "labels": [], "entities": [{"text": "CM-SQuAD dataset", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.9581198394298553}]}, {"text": "Adam optimizer () is used to optimize the weights during training.", "labels": [], "entities": []}, {"text": "For the evaluation of CMQA, we adopt the exact match (EM) and F1-score ().", "labels": [], "entities": [{"text": "exact match (EM)", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.9306481838226318}, {"text": "F1-score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.998021125793457}]}], "tableCaptions": [{"text": " Table 1: Statistics of manually formulated CM  questions", "labels": [], "entities": []}, {"text": " Table 1. We compute  the complexity of code-mixing using the metric,  Code-mixing Index (CMI) score", "labels": [], "entities": [{"text": "Code-mixing Index (CMI) score", "start_pos": 71, "end_pos": 100, "type": "METRIC", "confidence": 0.8530569076538086}]}, {"text": " Table 3: Performance comparison of the proposed CMQA algorithm with the IR-based and neural-based  baselines. Test (2) refers the test set of CM-MMQA.", "labels": [], "entities": []}, {"text": " Table 4. For evaluation, we employed  three annotators who were instructed to assign the  label (same or different) depending upon whether  the system generated and manually created ques- tions are similar or dissimilar. The agreement  among the annotators was calculated by", "labels": [], "entities": []}, {"text": " Table 5: Effect of the various components of the  CMQA model on the development set of CM- SQuAD and CM-MMQA dataset. (-) X denotes  the model architecture after removal of 'X'.", "labels": [], "entities": [{"text": "CM-MMQA dataset", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.9227765202522278}]}]}