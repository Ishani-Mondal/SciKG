{"title": [{"text": "CEA LIST : Processing low-resource languages for CoNLL 2018 Shared Task", "labels": [], "entities": [{"text": "CEA LIST", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.706628680229187}, {"text": "CoNLL 2018 Shared Task", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8756258338689804}]}], "abstractContent": [{"text": "In this paper, we describe the first CEA LIST participation at the CoNLL 2018 shared task.", "labels": [], "entities": [{"text": "CEA LIST participation at the CoNLL 2018 shared task", "start_pos": 37, "end_pos": 89, "type": "DATASET", "confidence": 0.7041945258776346}]}, {"text": "The submitted system is based on the state of the art parser from CoNLL 2017, that has been improved by the addition of morphological features predictions and the integration of additional resources to provide accurate models for low-resource languages.", "labels": [], "entities": []}, {"text": "Our approach ranked 5 th of 27 participants in MLAS for building morphology aware dependency trees, 2 nd for morphological features only, and 3 rd for tagging (UPOS) and parsing (LAS) low-resource languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The) is dedicated to developing dependency parsers on many languages, including lowresource languages.", "labels": [], "entities": []}, {"text": "Our system uses the Stanford team parser) which was at the state of the art during the CoNLL 2017 shared task.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.7627940773963928}]}, {"text": "We will refer to it as \"stf parser\" in this article.", "labels": [], "entities": []}, {"text": "We also used UDPipe () for tokenization, sentence segmentation, word alignment, lemmas and XPOS tags.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9692785739898682}, {"text": "sentence segmentation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7180444002151489}, {"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7375462651252747}]}, {"text": "Our main purpose is not to propose anew parser system, our approach is mainly focused on the adaptation of existing systems to low-resources languages.", "labels": [], "entities": []}, {"text": "However, we also propose some improvements of the stf parser on multiple levels: (1) the training time is shorter and the models are more accurate for features and XPOS tags predictions (even if we didn't use the predicted XPOS tags in the final submission) we studied the hyper-parameters in order to find the best configuration (3) we im-plemented an optimal tree construction instead of a greedy one, based on Stanford team recommendations in their 2017 paper.", "labels": [], "entities": []}, {"text": "We spent five man-month to provide these results, with two additional man-month dedicated to Breton corpus.", "labels": [], "entities": [{"text": "Breton corpus", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9828970730304718}]}, {"text": "For low-resource languages, we based our approach on the following available data : OPUS corpus, Wikipedia data (Wiktionary) and word embeddings.", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8811701834201813}, {"text": "Wikipedia data (Wiktionary)", "start_pos": 97, "end_pos": 124, "type": "DATASET", "confidence": 0.8540480732917786}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: SVNO models results, compared to stf model trained on monolingual gold data.", "labels": [], "entities": []}]}