{"title": [{"text": "Sequence to Sequence Mixture Model for Diverse Machine Translation", "labels": [], "entities": [{"text": "Diverse Machine Translation", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7788999676704407}]}], "abstractContent": [{"text": "Sequence to sequence (SEQ2SEQ) models often lack diversity in their generated translations.", "labels": [], "entities": []}, {"text": "This can be attributed to the limitation of SEQ2SEQ models in capturing lexical and syntactic variations in a parallel corpus resulting from different styles, genres, topics, or ambiguity of the translation process.", "labels": [], "entities": []}, {"text": "In this paper , we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation diversity and quality by adopting a committee of specialized translation models rather than a single translation model.", "labels": [], "entities": []}, {"text": "Each mixture component selects its own training dataset via optimization of the marginal log-likelihood, which leads to a soft clustering of the parallel corpus.", "labels": [], "entities": []}, {"text": "Experiments on four language pairs demonstrate the superiority of our mixture model compared to a SEQ2SEQ base-line with standard or diversity-boosted beam search.", "labels": [], "entities": []}, {"text": "Our mixture model uses negligible additional parameters and incurs no extra computation cost during decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural sequence to sequence (SEQ2SEQ) models have been remarkably effective machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.7940876483917236}]}, {"text": "They have revolutionized MT by providing a unified end-to-end framework, as opposed to the traditional approaches requiring several submodels and long pipelines.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9914255142211914}]}, {"text": "The neural approach is superior or on-par with statistical MT in terms of translation quality on various MT tasks and domains e.g. (. A well recognized issue with SEQ2SEQ models is the lack of diversity in the generated translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9453192353248596}, {"text": "MT tasks", "start_pos": 105, "end_pos": 113, "type": "TASK", "confidence": 0.8991543352603912}]}, {"text": "This issue is mostly attributed to the decoding algorithm (, and recently to the model ().", "labels": [], "entities": []}, {"text": "The former direction has attempted to design diversity encouraging decoding algorithm, particularly beam search, as it generates translations sharing the majority of their tokens except a few trailing ones.", "labels": [], "entities": [{"text": "beam search", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.8311240971088409}]}, {"text": "The latter direction has investigated modeling enhancements, particularly the introduction of continuous latent variables, in order to capture lexical and syntactic variations in training corpora, resulted from the inherent ambiguity of the human translation process.", "labels": [], "entities": []}, {"text": "However, improving the translation diversity and quality with SEQ2SEQ models is still an open problem, as the results of the aforementioned previous work are not fully satisfactory.", "labels": [], "entities": [{"text": "translation diversity", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8157176077365875}]}, {"text": "In this paper, we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation quality and diversity by adopting a committee of specialized translation models rather than a single translation model.", "labels": [], "entities": []}, {"text": "Each mixture component selects its own training dataset via optimization of the marginal log-likelihood, which leads to a soft clustering of the parallel corpus.", "labels": [], "entities": []}, {"text": "As such, our mixture model introduces a conditioning global discrete latent variable for each sentence, which leads to grouping together and capturing variations in the training corpus.", "labels": [], "entities": []}, {"text": "We design the architecture of S2SMIX such that the mixture components share almost all of their parameters and computation.", "labels": [], "entities": []}, {"text": "We provide experiments on four translation tasks, translating from English to German/French/Vietnamese/Spanish.", "labels": [], "entities": [{"text": "translating from English to German/French/Vietnamese/Spanish", "start_pos": 50, "end_pos": 110, "type": "TASK", "confidence": 0.8378612236543135}]}, {"text": "The experiments show that our S2SMIX model consistently outperforms strong baselines, including SEQ2SEQ model with the standard and diversity encouraged beam search, in terms of both translation diversity and quality.", "labels": [], "entities": []}, {"text": "The benefits of our mixture model comes with negligible additional parameters and no extra computation at inference time, compared to the vanilla SEQ2SEQ model.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the effectiveness of the S2SMIX model, we conduct a set of translation experiments on TEDtalks on four language pairs: English\u2192French (en-fr), English\u2192German (en-de), English\u2192Vietnamese (en-vi), and English\u2192Spanish (en-es).", "labels": [], "entities": []}, {"text": "We use IWSLT14 dataset 2 for en-es, IWSLT15) to handle rare words on en-fr, en-de and en-es, and share the BPE vocabularies between the encoder and decoder for each language pair.", "labels": [], "entities": [{"text": "IWSLT14 dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9287577271461487}, {"text": "IWSLT15", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9492192268371582}]}, {"text": "All of the models use a one-layer bidirectional LSTM encoder and a twolayer LSTM decoder.", "labels": [], "entities": []}, {"text": "Each LSTM layer in the encoder and decoder has a 512 dimensional hidden state.", "labels": [], "entities": []}, {"text": "Each input word embeddings is 512 dimensional as well.", "labels": [], "entities": []}, {"text": "We adopt the Adam optimizer ().", "labels": [], "entities": []}, {"text": "We adopt dropout with a 0.2 dropout rate.", "labels": [], "entities": []}, {"text": "The minibatch size is 64 sentence pairs.", "labels": [], "entities": []}, {"text": "We train each model 15 epochs, and select the best model in terms of the perplexity on the dev set.", "labels": [], "entities": []}, {"text": "Having more diversity in the candidate translations is one of the major advantages of the S2SMIX model.", "labels": [], "entities": []}, {"text": "To quantify diversity within a set {\u02c6y{\u02c6y m } M m=1 of translation candidates, we propose to evaluate average pairwise BLEU between pairs of sentences according to As an alternative metric of diversity within a set {\u02c6y{\u02c6y m } M m=1 of translations, we propose a metric based on the fraction of the n-grams that are where ngram(y) returns the set of unique n-grams in a sequence y.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9790982007980347}]}, {"text": "We report average div_bleu and average div_ngram across the test set for the translation candidates found by beam search.", "labels": [], "entities": []}, {"text": "We measure and report bigram diversity in the paper and report unigram diversity in the supplementary material.", "labels": [], "entities": []}, {"text": "We start by investigating which of the ways of injecting the conditioning signal into the S2SMIX model is most effective.", "labels": [], "entities": []}, {"text": "As seen in Section 3, the mixture components can be built by adding component-specific vectors to the logits (sf), the top LSTM layer (tp) or the bottom LSTM layer (bt) in the decoder, or all of them (all).", "labels": [], "entities": []}, {"text": "shows the BLEU score of these variants on the translation tasks across four different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994956254959106}, {"text": "translation tasks", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8998408019542694}]}, {"text": "We observe that adding a component-specific vector to the recurrent cells in the bottom layer of the decoder is the most effective, and results in BLEU scores superior or onpar with the other variants across the four language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9990691542625427}]}, {"text": "Therefore, we use this model variant in all experiments for the rest of the paper.", "labels": [], "entities": []}, {"text": "Furthermore, shows the number of parameters in each of the variants as well as the base SEQ2SEQ model.", "labels": [], "entities": []}, {"text": "We confirm that the mixture model variants introduce negligible number en-fr en-de en-vi en-es SEQ2SEQ 173.", "labels": [], "entities": [{"text": "SEQ2SEQ 173", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.7495042681694031}]}, {"text": "As an effective regularization strategy, we adopt label smoothing to strengthen generalisation performance (.", "labels": [], "entities": [{"text": "regularization", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9780431985855103}, {"text": "label smoothing", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7168984711170197}]}, {"text": "Unlike conventional cross-entropy loss, where the probability mass for the ground truth wordy is set to 1 and q(y ) = 0 for y = y, we smooth this distribution as: where is a smoothing parameter, and V is the vocabulary size.", "labels": [], "entities": []}, {"text": "In our experiments, is set to 0.1.  with the corresponding row in the bottom part fora fair comparison in terms of the effective beam size.", "labels": [], "entities": []}, {"text": "Firstly, we observe that increasing the beam size deteriorates the BLEU score for the SEQ2SEQ model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9853452742099762}, {"text": "SEQ2SEQ", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.6815308928489685}]}, {"text": "Similar observations have been made in the previous work (.", "labels": [], "entities": []}, {"text": "This behavior is in contrast to our S2SMIX model where increasing the beam size improves the BLEU score, except en-es, which demonstrates a decreasing trend when beam size increases from 2 to 4.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9849050641059875}]}, {"text": "Secondly, our S2SMIX models outperform their SEQ2SEQ counterparts in all settings with the same number of bins.", "labels": [], "entities": []}, {"text": "shows the diversity comparison between the S2SMIX model and the vanilla SEQ2SEQ model where the number of decoding beams is 8.", "labels": [], "entities": []}, {"text": "The diversity metrics are bigram and BLEU diversity as defined earlier in the section.", "labels": [], "entities": [{"text": "BLEU diversity", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9699677228927612}]}, {"text": "Our S2SMIX models significantly dominate the SEQ2SEQ model across language pairs in terms of the diversity metrics, while keeping the translation quality high (c.f. the BLEU scores in).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9989432692527771}]}, {"text": "We further compare against the SEQ2SEQ model endowed with the beam-diverse decoder (.", "labels": [], "entities": []}, {"text": "This decoder penalizes sibling hypotheses generated from the same parent in the search tree, according to their ranks in each decoding step.", "labels": [], "entities": []}, {"text": "Hence, it tends to rank high those hypotheses from different parents, hence encouraging diversity in the beam.", "labels": [], "entities": []}, {"text": "presents the BLEU scores as well as the diversity measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9959233999252319}]}, {"text": "As seen, the mixture model significantly outperforms the SEQ2SEQ endowed with the beam-diverse decoder, in terms of the diversity in the generated translations.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.7966131567955017}]}, {"text": "Furthermore, the mixture model achieves up to 1.7 BLEU score improvements across three language pairs.: BLEU scores using greedy decoding and training time based on the original log-likelihood objective and online EM coupled with gradient estimation based on a single MC sample.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9750233590602875}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9993671774864197}]}, {"text": "The training time is reported by taking the average running time of one minibatch update across a full epoch.", "labels": [], "entities": []}, {"text": "Memory limitations of the GPU may make it difficult to increase the number of mixture components beyond a certain amount.", "labels": [], "entities": [{"text": "Memory", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9357427358627319}, {"text": "GPU", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9519403576850891}]}, {"text": "One approach is to decrease the number of sentence pairs in a minibatch, however, this results in a substantial increase in the training time.", "labels": [], "entities": []}, {"text": "Another approach is to resort to MC gradient estimation as discussed in Section 3.3.", "labels": [], "entities": [{"text": "MC gradient estimation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.5970900555451711}]}, {"text": "The top-part of compares the models trained by online EM vs the original log-likelihood objective, in terms of the BLEU score and the training time.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9757643342018127}]}, {"text": "As seen, the BLEU score of the EMtrained models are on-par with those trained on the log-likelihood objective.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9997820258140564}]}, {"text": "However, online EM leads to up to 35% increase in the training time for S2SMIX-4 across four different language pairs, as we first need to do a forward pass on the minibatch in order to form the lower bound on the loglikelihood training objective.", "labels": [], "entities": []}, {"text": "The bottom-part of shows the effect of online EM coupled with sampling only one mixture component to form a stochastic approximation to the log-likelihood lower bound.", "labels": [], "entities": []}, {"text": "For each minibatch, we first run a forward pass to compute the probability of each mixture component given each sentence pair in the minibatch.", "labels": [], "entities": []}, {"text": "We then sample a mixture component for each sentence-pair to form the approximation of the log-likelihood lower bound for the minibatch, which is then optimized using back-propagation.", "labels": [], "entities": []}, {"text": "As we increase the number of mixture components from 4 to 8, we see about 0.7 BLEU score increase for en-de; while there is no significant change in the BLEU score for en-fr, en-vi and en-es.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9756993353366852}, {"text": "BLEU score", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9824191927909851}]}, {"text": "Increasing the number of mixture components further to 16 does not produce gains on these datasets.", "labels": [], "entities": []}, {"text": "Time-wise, training with online EM coupled with 1-candidate sampling should be significantly faster that the vanilla online EM and the original likelihood objective in principle, as we need to perform the backpropagation only for the selected mixture component (as opposed to all mixture components).", "labels": [], "entities": []}, {"text": "Nonetheless, the additional computation due to increasing the number of mixtures from 4 to 8 is about 26%, which increases to about 55% when going from 8 to 16 mixture components.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of all language pairs for IWSLT  data after preprocessing", "labels": [], "entities": [{"text": "IWSLT  data", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.7162878811359406}]}, {"text": " Table 2: Size of the parameters (MB) for the base  SEQ2SEQ model and the variants of S2SMIX with  four mixtures.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores of different systems over dif- ferent search space.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989964365959167}]}, {"text": " Table 4: S2SMIX with 4 components vs SEQ2SEQ  endowed with the beam-diverse decoder (Li et al.,  2016) with the beam size of 4.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.7904415130615234}]}, {"text": " Table 5: BLEU scores using greedy decoding and training time based on the original log-likelihood  objective and online EM coupled with gradient estimation based on a single MC sample. The training  time is reported by taking the average running time of one minibatch update across a full epoch.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9969972372055054}]}]}