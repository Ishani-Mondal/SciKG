{"title": [{"text": "Dual Latent Variable Model for Low-Resource Natural Language Generation in Dialogue Systems", "labels": [], "entities": [{"text": "Low-Resource Natural Language Generation", "start_pos": 31, "end_pos": 71, "type": "TASK", "confidence": 0.6729560270905495}]}], "abstractContent": [{"text": "Recent deep learning models have shown improving results to natural language generation (NLG) irrespective of providing sufficient annotated data.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.8200436433156332}]}, {"text": "However, a modest training data may harm such models' performance.", "labels": [], "entities": []}, {"text": "Thus, how to build a generator that can utilize as much of knowledge from a low-resource setting data is a crucial issue in NLG.", "labels": [], "entities": []}, {"text": "This paper presents a variational neural-based generation model to tackle the NLG problem of having limited labeled dataset, in which we integrate a variational inference into an encoder-decoder generator and introduce a novel auxiliary auto-encoding with an effective training procedure.", "labels": [], "entities": [{"text": "variational neural-based generation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.6915300687154134}]}, {"text": "Experiments showed that the proposed methods not only outperform the previous models when having sufficient training dataset but also show strong ability to work acceptably well when the training data is scarce.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language generation (NLG) plays an critical role in Spoken dialogue systems (SDSs) with the NLG task is mainly to convert a meaning representation produced by the dialogue manager, i.e., dialogue act (DA), into natural language responses.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7584205269813538}, {"text": "Spoken dialogue systems (SDSs)", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6738337129354477}]}, {"text": "SDSs are typically developed for various specific domains, i.e., flight reservations (), buying a tv or a laptop), searching fora hotel or a restaurant, and so forth.", "labels": [], "entities": [{"text": "flight reservations", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.717822253704071}]}, {"text": "Such systems often require well-defined ontology datasets that are extremely time-consuming and expensive to collect.", "labels": [], "entities": []}, {"text": "There is, thus, a need to build NLG systems that can work acceptably well when the training data is in short supply.", "labels": [], "entities": []}, {"text": "There are two potential solutions for abovementioned problems, which are domain adaptation training and model designing for low-resource training.", "labels": [], "entities": [{"text": "domain adaptation training", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.886965791384379}]}, {"text": "First, domain adaptation training which aims at learning from sufficient source domain a model that can perform acceptably well on a different target domain with a limited labeled target data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.8486810326576233}]}, {"text": "Domain adaptation generally involves two different types of datasets, one from a source domain and the other from a target domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7966914176940918}]}, {"text": "Despite providing promising results for low-resource setting problems, the methods still need an adequate training data at the source domain site.", "labels": [], "entities": []}, {"text": "Second, model designing for low-resource setting has not been well studied in the NLG literature.", "labels": [], "entities": [{"text": "model designing", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6946630477905273}]}, {"text": "The generation models have achieved very good performances irrespective of providing sufficient labeled datasets.", "labels": [], "entities": []}, {"text": "However, small training data easily result in worse generation models in the supervised learning methods.", "labels": [], "entities": []}, {"text": "Thus, this paper presents an explicit way to construct an effective low-resource setting generator.", "labels": [], "entities": []}, {"text": "In summary, we make the following contributions, in which we: (i) propose a variational approach for an NLG problem which benefits the generator to not only outperform the previous methods when there is a sufficient training data but also perform acceptably well regarding lowresource data; (ii) present a variational generator that can also adapt faster to anew, unseen domain using a limited amount of in-domain data; (iii) investigate the effectiveness of the proposed method in different scenarios, including ablation studies, scratch, domain adaptation, and semi-supervised training with varied proportion of dataset.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 540, "end_pos": 557, "type": "TASK", "confidence": 0.7172557413578033}]}, {"text": "cially RNN Encoder-Decoder models integrating with attention mechanism, such as Enc-Dec (, and RALSTM ( . However, such models have proved to work well only when providing a sufficient in-domain data since a modest dataset may harm the models' performance.", "labels": [], "entities": [{"text": "RALSTM", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.7949724197387695}]}, {"text": "In this context, one can think of a potential solution where the domain adaptation learning is utilized.", "labels": [], "entities": []}, {"text": "The source domain, in this scenario, typically contains a sufficient amount of annotated data such that a model can be efficiently built, while there is often little or no labeled data in the target domain.", "labels": [], "entities": []}, {"text": "A phrase-based statistical generator () using graphical models and active learning, and a multi-domain procedure) via data counterfeiting and discriminative training.", "labels": [], "entities": []}, {"text": "However, a question still remains as how to build a generator that can directly work well on a scarce dataset.", "labels": [], "entities": []}, {"text": "Neural variational framework for generative models of text have been studied extensively.", "labels": [], "entities": [{"text": "generative models of text", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.9256128221750259}]}, {"text": "proposed a recurrent latent variable model for sequential data by integrating latent random variables into hidden state of an RNN.", "labels": [], "entities": []}, {"text": "A hierarchical multi scale recurrent neural networks was proposed to learn both hierarchical and temporal representation (, while presented a variational autoencoder for unsupervised generative language model.", "labels": [], "entities": []}, {"text": "proposed a deep conditional generative model for structured output prediction, whereas introduced a variational neural machine translation that incorporated a continuous latent variable to model underlying semantics of sentence pairs.", "labels": [], "entities": [{"text": "structured output prediction", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6628033518791199}, {"text": "variational neural machine translation", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.6117724925279617}]}, {"text": "To solve the exposure-bias problem (", "labels": [], "entities": []}], "datasetContent": [{"text": "We assessed the proposed models on four different original NLG domains: finding a restaurant and hotel), or buying a laptop and television).", "labels": [], "entities": []}, {"text": "The generator performances were evaluated using the two metrics: the BLEU and the slot error rate ERR by adopting code from an NLG toolkit * . We compared the proposed models against strong baselines which have been recently published as NLG benchmarks of those datasets, including  In this work, the CNN Utterance Encoder consists of L = 3 layers, which fora sentence of length T = 73, embedding size d = 100, stride lengths = {2, 2, 2}, number of filters k = {300, 600, 100} with filter sizes h = {5, 5, 16}, results in feature maps V of sizes {35 \u00d7 300, 16 \u00d7 600, 1 \u00d7 100}, in which the last feature map corresponds to latent representation vector h U . The hidden layer size and beam width were set to be 100 and 10, respectively, and the models were trained with a 70% of keep dropout rate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9989748001098633}, {"text": "slot error rate ERR", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.8472910374403}, {"text": "keep dropout rate", "start_pos": 777, "end_pos": 794, "type": "METRIC", "confidence": 0.9680585265159607}]}, {"text": "We performed 5 runs with different random initialization of the network, and the training process is terminated by using early stopping.", "labels": [], "entities": []}, {"text": "For the variational inference, we set the latent variable size to be 300.", "labels": [], "entities": []}, {"text": "We used Adam optimizer with the learning rate is initially set to be 0.001, and after 5 epochs the learning rate is decayed every epoch using an exponential rate of 0.95.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results evaluated on four domains by training models from scratch with 10%, 30%, and 100%  in-domain data, respectively. The results were averaged over 5 randomly initialized networks. The bold  and italic faces denote the best and second best models in each training scenario, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Results evaluated on Target domains by adaptation training SCLSTM model from 100% (de- noted as ) of Source data, and the CrossVAE model from 30% (denoted as ), 100% (denoted as \u03be) of  Source data. The scenario used only 10% amount of the Target domain data. The last two rows show  results by training the CrossVAE model on the scr10 and semi-supervised learning, respectively.", "labels": [], "entities": []}]}