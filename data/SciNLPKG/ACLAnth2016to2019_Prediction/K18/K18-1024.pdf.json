{"title": [{"text": "Chinese Poetry Generation with a Salient-Clue Mechanism", "labels": [], "entities": [{"text": "Chinese Poetry Generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6120759844779968}]}], "abstractContent": [{"text": "As a precious part of the human cultural heritage , Chinese poetry has influenced people for generations.", "labels": [], "entities": []}, {"text": "Automatic poetry composition is a challenge for AI.", "labels": [], "entities": [{"text": "Automatic poetry composition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6741586526234945}]}, {"text": "In recent years, significant progress has been made in this area benefiting from the development of neural networks.", "labels": [], "entities": []}, {"text": "However, the coherence in meaning, theme or even artistic conception fora generated poem as a whole still remains a big problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel Salient-Clue mechanism for Chinese poetry generation.", "labels": [], "entities": [{"text": "Chinese poetry generation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6843357880910238}]}, {"text": "Different from previous work which tried to exploit all the context information, our model selects the most salient characters automatically from each so-far generated line to gradually form a salient clue, which is utilized to guide successive poem generation process so as to eliminate interruptions and improve coherence.", "labels": [], "entities": [{"text": "poem generation", "start_pos": 245, "end_pos": 260, "type": "TASK", "confidence": 0.7267344295978546}]}, {"text": "Besides , our model can be flexibly extended to control the generated poem in different aspects , for example, poetry style, which further enhances the coherence.", "labels": [], "entities": []}, {"text": "Experimental results show that our model is very effective, outper-forming three strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a fascinating literary form starting from the Pre-Qin Period, Chinese poetry has influenced people for generations and thus influenced Chinese culture and history in thousands of years.", "labels": [], "entities": []}, {"text": "Poets often write poems to record interesting events and express their feelings.", "labels": [], "entities": []}, {"text": "In fact, the ability to create high-quality poetry has become an indicator of knowledge, wisdom and elegance of a person in China.", "labels": [], "entities": []}, {"text": "Generally, a Chinese poem should meet two kinds of requirements.", "labels": [], "entities": []}, {"text": "One is from the perspective of form: it must obey some structural and phonological rules strictly.", "labels": [], "entities": []}, {"text": "For example (as shown in), quatrain (Jueju in Chinese), one of the most popular types of Chinese poetry, contains four lines with each consisting of five or seven characters (called Wujue and Qijue respectively); characters with particular tone must be in particular positions to make the poem cadenced and full of rhythmic beauty; and, the last character of the first (optional), second and fourth lines must rhyme.", "labels": [], "entities": []}, {"text": "The other one is from the perspective of content, concerning: (1) if each line of the poem is adequate syntactically and semantically; (2) if the association between two adjacent lines is reasonable; and (3) if the poem as a whole is coherent in meaning, theme or even in artistic conception.", "labels": [], "entities": []}, {"text": "Obviously, the second requirement is much more complicated and difficult than the first one.", "labels": [], "entities": []}, {"text": "In this paper, we investigate on automatic Chinese poetry generation, with emphasis on quatrains.", "labels": [], "entities": [{"text": "Chinese poetry generation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5901080469290415}]}, {"text": "We believe the form requirement is comparatively easy fora computer to deal with by some constraint checking.", "labels": [], "entities": []}, {"text": "For the content requirement, point (1) and (2) can be also handled well owing to the use of powerful sequence-tosequence neural networks, which are capable of producing well-formed tar-get sentence given a source sentence.", "labels": [], "entities": []}, {"text": "A challenging problem which remains unresolved for researchers is the point (3), where inter-lines associations are 'global' throughout a poem, rather than 'local' in point.", "labels": [], "entities": []}, {"text": "The relevant experience tells us this is a major reason for the distinct gap between computer-generated poems and those written by poets.", "labels": [], "entities": []}, {"text": "In fact, most previous models don't tackle this problem well and will produce incoherences and inconsistencies in generated poems.", "labels": [], "entities": []}, {"text": "Inter-lines coherence is the main concern of this paper.", "labels": [], "entities": []}, {"text": "Intuitively, there should be a clear clue to keep the theme of a poem consistent.", "labels": [], "entities": []}, {"text": "However, setting a fixed pattern of the clue in advance, e.g, pre-determining keywords for each line, may lose the flexibility and imagination, which are essential for poetry.", "labels": [], "entities": []}, {"text": "When writing a poem, human poets will focus on some salient parts of the context to ignore distractions and create relevant content.", "labels": [], "entities": []}, {"text": "During this process, poets gradually build a salient clue (or framework) of the poem, allowing not only coherence but also some flexibility.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose a novel SalientClue Mechanism for poetry generation.", "labels": [], "entities": []}, {"text": "Different from previous models which tried to exploit all the context, our model chooses a few salient characters out of each previously generated line, forming a vital clue for generating succeeding lines, so as to maintain the coherence of the whole poem to the maximum extent.", "labels": [], "entities": []}, {"text": "In addition, owing to the flexible structure of our model, extra useful information (e.g., the user intent and poetry style) can be incorporated with the salient clue to control the generation process, further enhancing coherence.", "labels": [], "entities": []}, {"text": "Contributions of this work are as follows: \u2022 To the best of our knowledge, we first propose to utilize the salient partial context to guide the poetry generation process.", "labels": [], "entities": [{"text": "poetry generation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7553101181983948}]}, {"text": "\u2022 We extend our model to combine user intent and control the style of generated poems, which further enhance coherence.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results show that our model outperforms three strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "To demonstrate the effectiveness of our model, we conduct four evaluations: BLEU Evaluation.", "labels": [], "entities": [{"text": "BLEU Evaluation", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.9358178973197937}]}, {"text": "Following (, we use BLEU () to evaluate our model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9988942742347717}]}, {"text": "BLEU isn't a perfect metric for generated poems, but in the scenario of pursuing better coherence, it still makes sense to some extent.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9868545532226562}]}, {"text": "Because higher BLEU scores indicate that the model can generate more n-grams of ground-truth, which certainly have better coherence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.999355137348175}]}, {"text": "Following), we design five criteria: Fluency (are the lines fluent and wellformed?), Coherence (is the theme of the whole quatrain consistent?), Meaningfulness (does the poem convey some certain messages?), Poeticness (does the poem have some poetic features?), Entirety (the reader's general impression on the poem).", "labels": [], "entities": [{"text": "Fluency", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9823085069656372}, {"text": "Entirety", "start_pos": 262, "end_pos": 270, "type": "METRIC", "confidence": 0.9877312779426575}]}, {"text": "Each criterion needs to be scored in a 5-point scale ranging from 1 to 5.", "labels": [], "entities": []}, {"text": "We select 20 typical keywords and generate two quatrains (one Wujue and one Qijue) for each keyword using these models.", "labels": [], "entities": []}, {"text": "For Human, we select quatrains containing the given keywords.", "labels": [], "entities": []}, {"text": "Therefore, we obtain 240 quatrains (20*6*2) in total.: Human evaluation results.", "labels": [], "entities": []}, {"text": "Diacritics * (p < 0.05) and ** (p < 0.01) indicates SC models significantly outperform the three baselines; + (p < 0.05) and ++ (p < 0.01) indicates Human is significantly better than all the five models.", "labels": [], "entities": []}, {"text": "The Intraclass Correlation Coefficient of the four groups of scores is 0.596, which indicates an acceptable inter-annotator agreement.", "labels": [], "entities": [{"text": "Intraclass Correlation Coefficient", "start_pos": 4, "end_pos": 38, "type": "METRIC", "confidence": 0.9609479109446207}]}, {"text": "We invite 12 experts on Chinese poetry to evaluate these quatrains, who are Chinese literature students or members of a poetry association.", "labels": [], "entities": []}, {"text": "Experts are divided into four groups and required to focus on the quality as objectively as possible, even if they recognize the human-authored ones.", "labels": [], "entities": []}, {"text": "Each group completes the evaluation of all poems and we use the average scores.", "labels": [], "entities": []}, {"text": "Style-SC is not suitable for BLEU, because we can't expect LDA to predict a correct style label by a short keyword.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9688131213188171}]}, {"text": "Thus Style-SC is only tested under Human Evaluation.", "labels": [], "entities": []}, {"text": "We label each keyword with an appropriate style manually, which is used to guide the generation.", "labels": [], "entities": []}, {"text": "Poetry style is usually coupled with content.", "labels": [], "entities": []}, {"text": "Not all keywords are compatible with every style.", "labels": [], "entities": []}, {"text": "Therefore we select ten normal keywords without obvious style (e.g., moon and wind).", "labels": [], "entities": []}, {"text": "We use SC to generate one poem and use Style-SC to generate three poems with the three specified styles.", "labels": [], "entities": []}, {"text": "The experts are asked to identify the style of each poem from four options (Unknown, Battlefield, Romantic and Pastoral).", "labels": [], "entities": []}, {"text": "The main idea of our method is to select the salient partial context to guide successive generation.", "labels": [], "entities": []}, {"text": "To evaluate the reasonableness of selected characters, we randomly select 20 Wujues and 20 Qijues from the test set.", "labels": [], "entities": []}, {"text": "Then three experts are asked to select up to K salient characters from each line.", "labels": [], "entities": []}, {"text": "When experts have different opinions, they stop and discuss until reaching an agreement.", "labels": [], "entities": []}, {"text": "Jaccard similarity is used to measure the overlap of human-selected characters and the model-selected ones.", "labels": [], "entities": [{"text": "Jaccard similarity", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.6479660421609879}]}, {"text": "As shown in, our SC outperforms other models under BLEU evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9831768870353699}]}, {"text": "We also compare different strategies of SC.", "labels": [], "entities": [{"text": "SC", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9607506394386292}]}, {"text": "As we expected, tfidf-SC models outperform the naive ones, because tf-idf values lower the weights of uninformative characters.", "labels": [], "entities": []}, {"text": "We also compare our SSal 1 with TopK (just select top K salient characters) and SSal gets better results.", "labels": [], "entities": []}, {"text": "Please note that, from naive-TopK-SDU to tfidf-SSal-SDU, BLEU scores are getting higher without any increase of model size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9992258548736572}]}, {"text": "SSI is better on Qijue, but performs slightly worse than SDU on Wujue.", "labels": [], "entities": []}, {"text": "We use SSI for Human evaluation here, but SDU is more suitable for longer poetry, e.g., Chinese Song Iambics.", "labels": [], "entities": []}, {"text": "Besides, the intent extension makes a little bit of improvement, not as prominent as we expected.", "labels": [], "entities": []}, {"text": "We think the reason may lie in that the keyword selected by tf-idf can't accurately represent the user intent.", "labels": [], "entities": []}, {"text": "Generally, the results show both for packing and concatenation formalisms, the proper utilization of partial salient context (SDU and SSI) can be better than the improper utilization of full context (Packing Full Context and nLto1L).", "labels": [], "entities": []}, {"text": "SC and Style-SC achieve better results than other models and get close to Human, though there is still a gap.", "labels": [], "entities": []}, {"text": "Especially on Coherence, our Style-SC gets the same score as Human for Wujue.", "labels": [], "entities": []}, {"text": "Moreover, Style-SC makes a considerable improvement on Coherence compared to SC (+0.05 for Wujue and +0.15 for Qijue), which demonstrates that consistent style can actually enhance the coherence, though it's not easy to predict an appropriate style automatically.", "labels": [], "entities": []}, {"text": "Interestingly, Style-SC outperforms SC on most criteria, except for Poeticness.", "labels": [], "entities": []}, {"text": "We believe this is mainly because that style control forces the model to always generate some style-related words, which limits the imagination and thus hurts the Poeticness.", "labels": [], "entities": []}, {"text": "Besides, as we can see, seq2seqPG outperforms other two baselines, but at the expense that it is three times the model size of iPoet (Table 4).", "labels": [], "entities": []}, {"text": "Surprisingly, Planning gets the worst re-", "labels": [], "entities": [{"text": "Planning", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.8062394857406616}, {"text": "re", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9949422478675842}]}], "tableCaptions": [{"text": " Table 1: BLEU evaluation results. The scores are cal- culated by the multi-bleu.perl script.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9814853668212891}]}, {"text": " Table 2: Human evaluation results. Diacritics * (p < 0.05) and ** (p < 0.01) indicates SC models significantly  outperform the three baselines; + (p < 0.05) and ++ (p < 0.01) indicates Human is significantly better than all  the five models. The Intraclass Correlation Coefficient of the four groups of scores is 0.596, which indicates an  acceptable inter-annotator agreement.", "labels": [], "entities": [{"text": "Diacritics", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9702117443084717}, {"text": "Intraclass Correlation Coefficient", "start_pos": 247, "end_pos": 281, "type": "METRIC", "confidence": 0.9655767281850179}]}, {"text": " Table 3: Saliency selection results. Random: randomly  select K characters for three times and use the average  Jaccard values. tf-idf: directly select K characters in  terms of tf-idf, without SC.", "labels": [], "entities": [{"text": "Saliency selection", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.968308687210083}]}, {"text": " Table 4: Extra comparisons of different models. In- novation, Param (million parameters) and Speed (sec- onds per poem) are compared. The generation speed is  tested on an Intel CORE i5 4-core CPU.", "labels": [], "entities": [{"text": "In- novation", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9273644089698792}, {"text": "Param", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.7880665063858032}, {"text": "Speed (sec- onds per poem)", "start_pos": 94, "end_pos": 120, "type": "METRIC", "confidence": 0.9199805110692978}]}]}