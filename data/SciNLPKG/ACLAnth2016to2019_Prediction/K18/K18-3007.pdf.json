{"title": [{"text": "Attention-free encoder decoder for morphological processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present RACAI's Entry for the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection.", "labels": [], "entities": [{"text": "RACAI", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.5521493554115295}, {"text": "CoNLL-SIGMORPHON 2018 shared task", "start_pos": 33, "end_pos": 66, "type": "DATASET", "confidence": 0.7818700969219208}, {"text": "universal morphological reinflection", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.6225050886472067}]}, {"text": "The system is based on an attention-free encoder-decoder neural architecture with a bidirectional LSTM for encoding the input sequence and a uni-directional LSTM for decoding and producing the output.", "labels": [], "entities": []}, {"text": "Instead of directly applying a sequence-to-sequence model at character-level we use a dynamic algorithm to align the input and output sequences.", "labels": [], "entities": []}, {"text": "Based on these alignments we produce a series of special symbols which are similar to those of a finite-state-transducer (FST).", "labels": [], "entities": []}], "introductionContent": [{"text": "Languages with rich morphology convey morphological attributes such as gender, case, number, obliqueness through character/grapheme variations applied to the dictionary form of the word (lemma).", "labels": [], "entities": []}, {"text": "It is often the case where these variations are obtained by suffixing the word rather than altering random characters, but this does not hold for all languages or irregular word forms.", "labels": [], "entities": []}, {"text": "Sill, the variations inside the lemma are usually small, requiring the system just to replace an average of 2.3 letters for the irregular word forms.", "labels": [], "entities": []}, {"text": "In our approach we exploit this property and employ an encoder-decoder sequence-to-sequence model that doesn't require an attention mechanism.", "labels": [], "entities": []}, {"text": "This mitigates attention issues such as repeating or skipping character sequences and reduces the need for models with high representational capacity.", "labels": [], "entities": []}, {"text": "We exploit the property that alignments between the input and output character sequences are monotonic: for example wordform men and lemma man share two letters (alignments) in the same order, without inversions.", "labels": [], "entities": []}, {"text": "The standard attention mechanism is well-suited for machine learning tasks; however, when it comes to monotonic alignments it sometimes fails to achieve satisfactory results, inmost cases due to the fact that repeated characters or character sequences in the input sequence confuse the attention mechanism making it generate loops or skip characters.", "labels": [], "entities": []}, {"text": "There are several proposed methods that try to solve this task with attention mechanisms such as guided attention, locationsensitive attention) and other variations.", "labels": [], "entities": []}, {"text": "Still, given the particularities of morphological reinflection, we argue that there is no need for an explicit attention mechanism.", "labels": [], "entities": []}, {"text": "Instead we train the decoder to focus on a single input symbol at each time-step and \"self-attend\" by shifting the input cursor with one position at a time.", "labels": [], "entities": []}, {"text": "This method, though developed independently, closely resembles that of.", "labels": [], "entities": []}, {"text": "In our previous experiments we used this architecture to perform lemmatization (the opposite task of morphological reinflection) and we obtained state-of-the-art results.", "labels": [], "entities": []}, {"text": "In what follows, we will present the attentionfree encoder-decoder architecture (Section 2), we show our experimental results (Section 3) and finally we draw conclusions (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "For our implementation is based on DyNET (Neubig et al., 2017), which is a dynamic computation graph network framework.", "labels": [], "entities": []}, {"text": "That means that we do not require any padding when we prepare minibatches.", "labels": [], "entities": []}, {"text": "We evaluated our approach on the data provided during the SIGMORPHON 2018 Shared Task on morphological reinflection (.", "labels": [], "entities": [{"text": "SIGMORPHON 2018 Shared Task on morphological reinflection", "start_pos": 58, "end_pos": 115, "type": "TASK", "confidence": 0.5234899861471993}]}, {"text": "During the evaluation campaign, each language was provided with 3 datasets of different sizes (high, medium and low).", "labels": [], "entities": []}, {"text": "Because, neural approaches traditionally require more training data to generalize better, we only built models for the \"high\" datasets, which were composed of 10K training examples for each language.", "labels": [], "entities": []}, {"text": "Our model was trained using ADAM optimization (, with the default parameters \u03b1 = 1e \u22123 , \u03b2 1 = 0.9 and \u03b2 2 = 0.999.", "labels": [], "entities": []}, {"text": "We used a mini-batch size of 1K words and we used trained each model until the accuracy on the development set stopped improving for 20 iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995478987693787}]}, {"text": "At the end, we used the best performing model for each languages.", "labels": [], "entities": []}, {"text": "For all languages we used a two-layer encoder with 200 LSTM cells (in each direction -total 400 cells per layer) and a two-layer decoder of 200 unidirectional cells.", "labels": [], "entities": []}, {"text": "Each character in the vocabulary is embedded as a 100-dimensional vector.", "labels": [], "entities": []}, {"text": "We also use a 100-dimensional embedding size for each unique morphological descriptor.", "labels": [], "entities": []}, {"text": "summarizes the testset results for all languages in the SIGMORPHON Challenge 2018.", "labels": [], "entities": []}, {"text": "During the official evaluation campaign, our system was affected by a bug which caused all weights belonging to non-recurrent cells to be constant (not trainable during backprop).", "labels": [], "entities": []}, {"text": "This issue had a strong negative impact on the results.", "labels": [], "entities": []}, {"text": "After this, we retrained our models and we include the unofficial results in the same table, under the \"Acc.*\" column.", "labels": [], "entities": [{"text": "Acc.", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9986718893051147}]}, {"text": "For almost all languages, after correcting the bug, the accuracy strongly increased; for Welsh we observed no increase, and only for 2 languages did we observe a less than 1 point decrease (probably due to weight initialization compounded by small models where the LSTMs overcame the fixed random weights of the dense layers).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.999675989151001}]}, {"text": "Overall, we observed a strong result increase, from an average of 72.49 to 83.77.", "labels": [], "entities": []}, {"text": "For example, for West Frisian where initially the model would not converge (0.00), we now obtain 93.00; similarly, for Armenian, we have gone from 0.00 to 93.9.", "labels": [], "entities": []}, {"text": "This is mainly (a) because our model introduces the COPY operation and reduces the representational load of the encoderdecoder model and (b) and because we keep track of the focus-index externally.", "labels": [], "entities": [{"text": "COPY", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.7172026634216309}]}, {"text": "Also, we reduce the computational complexity of the model by completely removing calculation involved in the soft attention mechanism (n * m matrix multiplications, where n is the size of the input sequence and m the size of the output sequence).", "labels": [], "entities": []}, {"text": "Moreover, the fact that the decoder does not require taking the previous output and embedding it as input for the next step, demonstrates that there is far less representational overhead involved in generating the output sequence.", "labels": [], "entities": []}, {"text": "As aside note, in our previous experiments with lemmatization, we observed that using this model yields a 2-5% absolute increase inaccuracy over the standard soft-attention sequence-to-sequence model.", "labels": [], "entities": []}], "tableCaptions": []}