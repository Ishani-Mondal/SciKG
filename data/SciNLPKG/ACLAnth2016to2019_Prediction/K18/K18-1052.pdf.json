{"title": [{"text": "Model Transfer with Explicit Knowledge of the Relation between Class Definitions", "labels": [], "entities": [{"text": "Model Transfer", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.717743456363678}]}], "abstractContent": [{"text": "This paper investigates learning methods for multi-class classification using labeled data for the target classification scheme and another labeled data fora similar but different classification scheme (support scheme).", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7486676573753357}]}, {"text": "We show that if we have prior knowledge about the relation between support and target classification schemes in the form of a class correspondence table, we can use it to improve the model performance further than the simple multi-task learning approach.", "labels": [], "entities": []}, {"text": "Instead of learning the individual classification layers for the support and target schemes, the proposed method converts the class label of each example on the support scheme into a set of candidate class labels on the target scheme via the class correspondence table, and then uses the candidate labels to learn the classification layer for the target scheme.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method on two tasks in NLP.", "labels": [], "entities": []}, {"text": "The experimental results show that our method effectively learns the target schemes especially for the classes that have a tight connection to certain support classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning based methods have shown high performance in many NLP tasks, which typically are formulated as some kinds of classification problems.", "labels": [], "entities": []}, {"text": "Although there has been a remarkable progress in methods utilizing unlabeled resources, many tasks still require a large amount (at least thousands, in some cases millions or billions) of high quality labeled data to achieve high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.9746357202529907}]}, {"text": "For many tasks, however, classification schemes vary depending on fields of application or other factors, and large and high quality labeled data following a single scheme is insufficient.", "labels": [], "entities": []}, {"text": "Named entity recognition (NER) ( and text classification (TC) are typical examples that allow variable classification schemes.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7951202243566513}, {"text": "text classification (TC)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.836604642868042}]}, {"text": "For example, there are two kinds of NE type definitions for Japanese NER: IREX) with eight entity types and Sekine's extended NE (ENE) hierarchy with 200 entity types ().", "labels": [], "entities": [{"text": "IREX", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9603226184844971}]}, {"text": "These two schemes are relevant but not incomplete correspondence, i.e., a class in ENE is not necessarily a proper subclass of a class in IREX and vice versa.", "labels": [], "entities": [{"text": "ENE", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8480640053749084}, {"text": "IREX", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.8791124820709229}]}, {"text": "For example, entities with LOCATION type in IREX are (a subtype of) LOCATION or FACILITY in ENE, while some entities with FACILITY type in ENE can also be ORGANIZATION in IREX.", "labels": [], "entities": [{"text": "IREX", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.6498902440071106}, {"text": "LOCATION", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9461740851402283}, {"text": "FACILITY", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9867444038391113}]}, {"text": "It is also the case that a classification scheme for an existing model is revised.", "labels": [], "entities": []}, {"text": "In the case of news categolization, for example, anew category such as world cup would be added at a certain point of time; or the articles about eSports would newly categorised the existing sports category.", "labels": [], "entities": []}, {"text": "To obtain labeled data following the desired scheme, it is often required to create them almost from scratch or modify existing annotation because the existing data follow partly different schemes.", "labels": [], "entities": []}, {"text": "However, the annotation processes to create such on-demand labeled data usually take too much cost to obtain enough data.", "labels": [], "entities": []}, {"text": "This paper addresses the methods to utilize the existing large amount of labeled data with a different classification scheme (support scheme) to learn a good model for the target scheme with a small amount of corresponding labeled data.", "labels": [], "entities": []}, {"text": "One possible solution is the multi-task learning approach) in which the model for each classification scheme is learned while sharing the model parameters for the input representation.", "labels": [], "entities": []}, {"text": "A drawback of typical multi-task approaches is that they cannot exploit relation between two schemes directly, even if we know it in advance.", "labels": [], "entities": []}, {"text": "The problem becomes critical when it is required to preserve the classification performance on the classes that are tightly connected to those of the support scheme.", "labels": [], "entities": []}, {"text": "This corresponds to the following practical situation.", "labels": [], "entities": []}, {"text": "We have a model working on some systems, and are required to modify it to adapt to anew classification scheme given only a small amount of examples related to the change of the scheme.", "labels": [], "entities": []}, {"text": "It is also required that the performance of the retrained model is almost unchanged for the input examples that is not related to the change of the scheme.", "labels": [], "entities": []}, {"text": "In the simple multitask learning, the classification layer for the target scheme is learned only from the small labeled data for the target scheme.", "labels": [], "entities": []}, {"text": "Such small data are often insufficient to learn existing classes in spite of the shared input representation.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to exploit the relation between the two classification schemes which is given in the form of a class correspondence table described in Section 3.", "labels": [], "entities": []}, {"text": "Instead of learning the individual classification layers for the support and target schemes, the proposed method converts the class label of each example on the support scheme into a set of candidate class labels on the target scheme via the class correspondence table, and then uses it to learn the classification layer for the target scheme using the learning with multiple labels framework ().", "labels": [], "entities": []}, {"text": "The difference from the typical multitask learning methods is that the large amount of labeled data on the support scheme are directly used to learn the classification layer for the target scheme.", "labels": [], "entities": []}, {"text": "It enables the model to learn the target scheme while preserving the performance on those classes which are tightly connected to the support scheme effectively.", "labels": [], "entities": []}, {"text": "We conduct experiments for two tasks in NLP to verify the effectiveness of our proposed method.", "labels": [], "entities": []}, {"text": "The contribution of this paper is as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a method to utilize the known relation of the two classification schemes by using the relation as an explicit constraint.", "labels": [], "entities": []}, {"text": "\u2022 We evaluated the proposed method on two task with public data and original but reproducible classification schemes.", "labels": [], "entities": []}, {"text": "The proposed method has the following advantages.", "labels": [], "entities": []}, {"text": "\u2022 We can utilize the prior knowledge on the relation between the support and the target classification schemes to effectively constrain the model.", "labels": [], "entities": []}, {"text": "\u2022 The method can learn the classes existing in the support scheme, even when the target labeled data contain few or no examples on these classes.", "labels": [], "entities": []}, {"text": "\u2022 It can also be used for such tasks in which the output is structured and difficult to be separated, e.g., NER.", "labels": [], "entities": []}, {"text": "\u2022 The proposed method can be applied to the most of current neural network based models which output a probability distribution and take loss to update parameters with learning method such as SGDs.", "labels": [], "entities": []}, {"text": "There is no need to violate the original network architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed method on two tasks: named entity recognition (NER) and text classification (TC).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.7322336037953695}, {"text": "text classification (TC)", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7799788117408752}]}, {"text": "To examine the effectiveness of the proposed method, we adopted datasets that are not only large but also accompanied with well-organized ontology.", "labels": [], "entities": []}, {"text": "We defined the target schemes following the exsiting shared tasks, while we defined different support schemes according to their ontology so that the labels correspond to the different level or granularity in the same ontology.", "labels": [], "entities": []}, {"text": "By doing so, we can compare the proposed method with an ideal settings where all data is labeled according to the target scheme, which can be seen as an upper bound.", "labels": [], "entities": []}, {"text": "All of the following results are averaged over five runs.", "labels": [], "entities": []}, {"text": "shows the F1 scores for NER task and accuracy scores for TC task.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994970560073853}, {"text": "NER task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.8202300071716309}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996768236160278}]}, {"text": "For NER, the performance of CS is competitive to MT, but by combining them (MTCS) we had the improved performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.845630943775177}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.6188559532165527}]}, {"text": "On the other hand, for TC task CS outperforms MT in a certain degree.", "labels": [], "entities": [{"text": "TC task CS", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.860663910706838}, {"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.6575857996940613}]}, {"text": "The effect of their combination of (MTCS) is limited on this task.", "labels": [], "entities": []}, {"text": "We also evaluated the effect of the class shift constraint when the amount of labeled data for the target scheme is quite small. and show how the scores improve as the size of the labeled data for the target scheme increases.", "labels": [], "entities": []}, {"text": "We can see that the advantage of CS and MTCS methods are significant especially when the size of the labeled data for the target scheme is very small.", "labels": [], "entities": []}, {"text": "Next, we evaluated how the models preserve the classification performance on the classes that are tightly connected to some classes in the support scheme.", "labels": [], "entities": []}, {"text": "We first trained a model on the support scheme (support model) using the labeled data for the support scheme.", "labels": [], "entities": []}, {"text": "Next we transfer the model for the target scheme with the labeled data  for the target scheme using Finetune, MT, CS, or MTCS.", "labels": [], "entities": [{"text": "Finetune", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8612093329429626}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "DATASET", "confidence": 0.7157207727432251}, {"text": "MTCS", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.8942112326622009}]}, {"text": "Then, test data are labeled by both the support model and the transferred model.", "labels": [], "entities": []}, {"text": "By doing so, we can compare the classification performance of the support model and the transferred models on the unchanged classes for these tasks, namely Protein for NER and Artist for TC.", "labels": [], "entities": []}, {"text": "We first check the performance of the transferred models on these classes with the small size of labeled data for the target schemes. and show the results.", "labels": [], "entities": []}, {"text": "Compared to Finetune and MT, the performance of CS and MTCS is high even when the size of labeled data for the target scheme is very small.", "labels": [], "entities": [{"text": "Finetune", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8997812271118164}]}, {"text": "It suggests that the proposed methods effectively use the knowledge from support models for recognizing these classes.", "labels": [], "entities": []}, {"text": "shows the number of examples that are correctly classified by the transferred models out of those cor-  rectly classified by the support model.", "labels": [], "entities": []}, {"text": "For NER task, the support model extracted 3193 out of 5067 Protein mentions correctly.", "labels": [], "entities": [{"text": "NER task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.8722732365131378}]}, {"text": "For TC task, the support model categorized 4534 out of 5000 text with Artist labels correctly.", "labels": [], "entities": [{"text": "TC task", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.8748286664485931}]}, {"text": "We can see that the proposed methods succeed to prevent performance deterioration more effectively than Finetune and MT.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics of NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.8902398645877838}]}, {"text": " Table 4: Data statistics of TC task. We show only the  average number of sentences per class for target since  the data is highly balanced.", "labels": [], "entities": [{"text": "TC task", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.8527271151542664}]}, {"text": " Table 6: Results for NER and TC task.", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9434710741043091}, {"text": "TC task", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.7657187283039093}]}]}