{"title": [{"text": "CUNI x-ling: Parsing under-resourced languages in CoNLL 2018 UD Shared Task", "labels": [], "entities": [{"text": "Parsing under-resourced languages", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8531631429990133}, {"text": "CoNLL 2018 UD Shared Task", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.9106724619865417}]}], "abstractContent": [{"text": "This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2018 UD Shared Task", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.8123680830001831}]}, {"text": "We focused on parsing under-resourced languages, with no or little training data available.", "labels": [], "entities": [{"text": "parsing under-resourced languages", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.8966875274976095}]}, {"text": "We employed a wide range of approaches, including simple word-based treebank translation , combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages.", "labels": [], "entities": [{"text": "word-based treebank translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.623162974913915}]}, {"text": "In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes our submission to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies (.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.694506898522377}, {"text": "Multilingual Parsing from Raw Text to Universal Dependencies", "start_pos": 69, "end_pos": 129, "type": "TASK", "confidence": 0.7853750362992287}]}, {"text": "Our primary focus was on the 4 languages with no annotated training data (treebanks) available, as we have significant experience with such a setting); in the shared task, these are Naija, Faroese, Thai, and Breton.", "labels": [], "entities": []}, {"text": "Apart from Naija, there are at least some non-treebank resources available for each of the languages, such as parallel data, monolingual data, or morphological dictionaries.", "labels": [], "entities": []}, {"text": "Furthermore, we also employ treebanks for other languages together with several cross-lingual parsing methods; in our work, we will refer to the language being parsed as the target language, and the other languages that we exploit when parsing it as Parallel data actually exist for all of the languages, at least in the form of the New Testament part of the Bible and the Universal Declaration of Human Rights; however, using these datasets was not allowed in the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the submissions to the shared task was performed by the organizers via the TIRA evaluation platform), running the submitted systems on secret test data and reporting their performance in LAS (labeled attachment score), MLAS (morphology-aware labeled attachment score), and BLEX (bi-lexical dependency score).", "labels": [], "entities": [{"text": "LAS (labeled attachment score)", "start_pos": 205, "end_pos": 235, "type": "METRIC", "confidence": 0.8268288572629293}, {"text": "BLEX (bi-lexical dependency score)", "start_pos": 291, "end_pos": 325, "type": "METRIC", "confidence": 0.8146096666653951}]}, {"text": "For a full description of the metrics, see (Zeman et al., 2018) or the shared task website; here, we only note that while LAS only evaluates parsing accuracy, MLAS also includes evaluation of tagging (UPOS and morphological features), while BLEX also includes lemmatization.", "labels": [], "entities": [{"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.9462683200836182}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.7997198700904846}, {"text": "MLAS", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.8214055895805359}, {"text": "BLEX", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.9254158735275269}]}, {"text": "We also list UPOS tagging accuracies.", "labels": [], "entities": [{"text": "UPOS tagging", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.6733702570199966}]}, {"text": "shows the average scores over the 9 low-resource languages.", "labels": [], "entities": []}, {"text": "Our submission achieved the best average result in all the 3 main scoring metrics; for comparison, we also list the submissions that scored second-best in the metrics, and the baseline setup.", "labels": [], "entities": []}, {"text": "reports the results individually for each low-resource language, together with the ranking of our submission among all of the 26 participants.", "labels": [], "entities": []}, {"text": "All scores are adapted from official results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of available training data.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of Naija results with no  translation, only with Wikipedia examples, and the  full setup which also uses information we learned  from other websites.", "labels": [], "entities": []}, {"text": " Table 5: Macro-average LAS, MLAS, BLEX and  UPOS on the 9 low-resource languages. Best re- sult in bold, second-best result underlined.", "labels": [], "entities": [{"text": "MLAS", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.744329035282135}, {"text": "BLEX", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9955247640609741}]}, {"text": " Table 6: LAS, MLAS, BLEX and UPOS of our submission (ours), as well as the best result achieved  among the other participants (comp.). The ranks are also listed.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9861533641815186}, {"text": "MLAS", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.5011487603187561}, {"text": "BLEX", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9982650876045227}, {"text": "UPOS", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9288073182106018}]}, {"text": " Table 7: Macro-average LAS, MLAS, BLEX and  UPOS on all 82 test sets for 57 languages.", "labels": [], "entities": [{"text": "LAS", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5821689367294312}, {"text": "BLEX", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9973478317260742}, {"text": "UPOS", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.6574001908302307}]}]}