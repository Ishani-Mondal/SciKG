{"title": [], "abstractContent": [{"text": "Many name tagging approaches use local con-textual information with much success, but fail when the local context is ambiguous or limited.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 5, "end_pos": 17, "type": "TASK", "confidence": 0.8480468988418579}]}, {"text": "We present anew framework to improve name tagging by utilizing local, document-level, and corpus-level contextual information.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8585130870342255}]}, {"text": "We retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other topically related documents.", "labels": [], "entities": []}, {"text": "We propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via global attentions, which dynamically weight their respective contextual information, and gating mechanisms, which determine the influence of this information.", "labels": [], "entities": []}, {"text": "Extensive experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets.", "labels": [], "entities": [{"text": "CoNLL-2002", "start_pos": 162, "end_pos": 172, "type": "DATASET", "confidence": 0.9426349401473999}, {"text": "CoNLL-2003 datasets", "start_pos": 177, "end_pos": 196, "type": "DATASET", "confidence": 0.811803936958313}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our methods on the: # of tokens in name tagging datasets statistics.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.6915884017944336}]}, {"text": "# of names is given in parentheses.", "labels": [], "entities": []}, {"text": "We select at most four document-level supporting sentences and five corpus-level supporting sentences.", "labels": [], "entities": []}, {"text": "Since the document-level attention method requires input from each individual document, we do not evaluate it on the CoNLL-2002 Spanish dataset which lacks document delimiters.", "labels": [], "entities": [{"text": "CoNLL-2002 Spanish dataset", "start_pos": 117, "end_pos": 143, "type": "DATASET", "confidence": 0.929648220539093}]}, {"text": "We still evaluate the corpus-level attention on the Spanish dataset by randomly splitting the dataset into documents (30 sentences per document).", "labels": [], "entities": [{"text": "Spanish dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.7497208416461945}]}, {"text": "Although randomly splitting the sentences does not yield perfect topic modeling clusters, experiments show the corpus-level attention still outperforms the baseline (Section 3.3).", "labels": [], "entities": []}, {"text": "For word representations, we use 100-dimensional pre-trained word embeddings and 25-dimensional randomly initialized character embeddings.", "labels": [], "entities": [{"text": "word representations", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7592099010944366}]}, {"text": "We train word embeddings using the word2vec package.", "labels": [], "entities": []}, {"text": "English embeddings are trained on the English Giga-word version 4, which is the same corpus used in (.", "labels": [], "entities": []}, {"text": "Dutch, Spanish, and German embeddings are trained on corresponding Wikipedia articles.", "labels": [], "entities": []}, {"text": "Word embeddings are fine-tuned during training.", "labels": [], "entities": []}, {"text": "For each model with an attention, since the Bi-LSTM encoder must encode the local, documentlevel, and/or corpus-level contexts, we pre-train a Bi-LSTM CRF model for 50 epochs, add our document-level attention and/or corpus-level attention, and then fine-tune the augmented model.", "labels": [], "entities": []}, {"text": "Additionally, report that neural models produce different results even with same hyper-parameters due to the variances in parameter initialization.", "labels": [], "entities": []}, {"text": "Therefore, we run each model ten times and report the mean as well as the maximum F1 scores.", "labels": [], "entities": [{"text": "mean", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9894144535064697}, {"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9993458390235901}]}], "tableCaptions": [{"text": " Table 3: Performance of our methods versus the  baseline and state-of-the-art models.", "labels": [], "entities": []}]}