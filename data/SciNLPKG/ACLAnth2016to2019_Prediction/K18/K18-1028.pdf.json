{"title": [{"text": "Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "Following the recent success of word embed-dings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like se-mantics/syntax and similarity/relatedness.", "labels": [], "entities": []}, {"text": "In this paper, we show that each embedding model captures more information than directly apparent.", "labels": [], "entities": []}, {"text": "A linear transformation that adjusts the similarity order of the model without any external resource can tailor it to achieve better results in those aspects, providing anew perspective on how embeddings encode divergent linguistic information.", "labels": [], "entities": []}, {"text": "In addition, we explore the relation between intrinsic and extrin-sic evaluation, as the effect of our transformations in downstream tasks is higher for unsu-pervised systems than for supervised ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings have recently become a central topic in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.653659055630366}]}, {"text": "Several unsupervised methods have been proposed to efficiently train dense vector representations of words () and successfully applied in a variety of tasks like parsing (), topic modeling ( and document classification.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 174, "end_pos": 188, "type": "TASK", "confidence": 0.8735125064849854}, {"text": "document classification", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.7855672836303711}]}, {"text": "While there is still an active research line to better understand these models from a theoretical perspective (, the fundamental idea behind all of them is to assign a similar vector representation to similar words.", "labels": [], "entities": []}, {"text": "For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts.", "labels": [], "entities": []}, {"text": "Nevertheless, the above argument does not formalize what \"similar words\" means, and it is not entirely clear what kind of relationships an embedding model should capture in practice.", "labels": [], "entities": []}, {"text": "For instance, some authors distinguish between genuine similarity (as in car -automobile) and relatedness 2 (as in car -road) (.", "labels": [], "entities": [{"text": "similarity", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.780333399772644}]}, {"text": "From another perspective, word similarity could focus on semantics (as in sing -chant) or syntax (as in sing -singing).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7532973289489746}]}, {"text": "We refer to these two aspects as the two axes of similarity with two ends each: the semantics/syntax axis and the similarity/relatedness axis.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method to tailor any given set of embeddings towards a specific end in these axes.", "labels": [], "entities": []}, {"text": "Our method is inspired by the work on first order and second order cooccurrences, generalized as a continuous parameter of a linear transformation applied to the embeddings that we call similarity order.", "labels": [], "entities": []}, {"text": "While there have been several proposals to learn specialized word embeddings (, previous work explicitly altered the training objective and often relied on external resources like knowledge bases, whereas the proposed method is applied as a post-processing of any pre-trained embedding model and does not require any additional resource.", "labels": [], "entities": []}, {"text": "As such, our work shows that standard embedding models are able to encode divergent linguistic information but have limits on how this information is surfaced, and analyzes the implications that this has in both intrinsic evaluation and downstream tasks.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a linear transformation with a free parameter that adjusts the perfor-mance of word embeddings in the similarity/relatedness and semantics/syntax axes, as measured in word analogy and similarity datasets.", "labels": [], "entities": []}, {"text": "2. We show that the performance of embeddings as used currently is limited by the impossibility of simultaneously surfacing divergent information (e.g. the aforementioned axes).", "labels": [], "entities": []}, {"text": "Our method uncovers the fact that embeddings capture more information than what is immediately obvious.", "labels": [], "entities": []}, {"text": "3. We show that standard intrinsic evaluation offers a static and incomplete picture, and complementing it with the proposed method can offer a better understanding of what information an embedding model truly encodes.", "labels": [], "entities": []}, {"text": "4. We show that the effect of our method also carries out to downstream tasks, but its effect is larger in unsupervised systems directly using embedding similarities than in supervised systems using embeddings as input features, as the latter have enough expressive power to learn the optimal transformation themselves.", "labels": [], "entities": []}, {"text": "All in all, our work sheds light in how word embeddings represent divergent linguistic information, analyzes the role that this plays in intrinsic evaluation and downstream tasks, and opens new opportunities for improvement.", "labels": [], "entities": []}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe our proposed post-processing in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 and 4 then present the results in intrinsic and extrinsic evaluation, respectively.", "labels": [], "entities": []}, {"text": "Section 5 discusses the implications of our work on embedding evaluation and their integration in downstream tasks.", "labels": [], "entities": [{"text": "embedding evaluation", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8325929641723633}]}, {"text": "Section 6 presents the related work, and Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to better understand the effect of the proposed post-processing in the two similarity axes introduced in Section 1, we adopt the widely used word analogy and word similarity tasks, which offer specific benchmarks for semantics/syntax and similarity/relatedness, respectively.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 150, "end_pos": 162, "type": "TASK", "confidence": 0.7470663189888}]}, {"text": "More concretely, word analogy measures the accuracy in answering questions like \"what is the word that is similar to France in the same sense as Berlin is similar to Germany?\"", "labels": [], "entities": [{"text": "word analogy", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8135854005813599}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.99824059009552}]}, {"text": "(semantic analogy) or \"what is the word that is similar to small in the same sense as biggest is similar to big?\"", "labels": [], "entities": []}, {"text": "(syntactic analogy) using simple word vector arithmetic ().", "labels": [], "entities": []}, {"text": "The analogy resolution method is commonly formalized in terms of vector additions and subtractions.", "labels": [], "entities": [{"text": "analogy resolution", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8006632924079895}]}, {"text": "showed that this was equivalent to searching fora word that maximizes a linear combination of three pairwise word similarities, so the proposed post-processing has a direct effect on it.", "labels": [], "entities": []}, {"text": "For these experiments, we use the dataset published as part of word2vec 5 , which consists of 8,869 semantic and 10,675 syntactic questions of this type.", "labels": [], "entities": []}, {"text": "On the other hand, word similarity measures the correlation 6 between the similarity scores produced by a model and a gold standard created by human annotators fora given set of word pairs.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7321864068508148}]}, {"text": "As discussed before, there is not a single definition of what human similarity scores should capture, which has lead to a distinction between genuine similarity datasets and relatedness datasets.", "labels": [], "entities": []}, {"text": "In order to better understand the effect of our postprocessing in each case, we conduct our experiments in SimLex-999 ( , a genuine similarity dataset that consists of 999 word pairs, and MEN (), a relatedness dataset that consists of 3,000 word pairs 7 . So as to make our evaluation more robust, we run the above experiments for three popular embedding methods, using large pre-trained models released by their respective authors as follows:) is the original implementation of the CBOW and skip-gram architectures that popularized neural word embeddings.", "labels": [], "entities": []}, {"text": "We use the pre-trained model published in the project homepage 8 , which was trained on about 100 billion words of the Google News dataset and consists of 300-dimensional vectors for 3 million words and phrases.) is a global logbilinear regression model to train word embeddings designed to explicitly enforce the model properties needed to solve word analogies.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 119, "end_pos": 138, "type": "DATASET", "confidence": 0.8748791019121805}]}, {"text": "We use the largest pre-trained model published by the authors , which was trained on 840 billion words of the Common Crawl corpus and contains 300-dimensional vectors for 2.2 million words.) is an extension of the skip-gram model implemented by word2vec that enriches the embeddings with subword information using bags of character n-grams.", "labels": [], "entities": [{"text": "Common Crawl corpus", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.9072689414024353}]}, {"text": "We use the largest pre-trained model published in the project website , which was trained on 600 billion tokens of the Common Crawl corpus and con- tains 300-dimensional vectors for 2 million words.", "labels": [], "entities": [{"text": "Common Crawl corpus", "start_pos": 119, "end_pos": 138, "type": "DATASET", "confidence": 0.9270677963892618}]}, {"text": "Given that the above models were trained in very large corpora and have an unusually large vocabulary, we decide to restrict its size to the most frequent 200,000 words in each case, leaving the few resulting out-of-vocabularies outside evaluation.", "labels": [], "entities": []}, {"text": "In all the cases, we test the proposed postprocessing for all the values of the parameter \u03b1 in the [\u22121, 1] range in increments of 0.05.", "labels": [], "entities": []}, {"text": "As the goal of this paper is not to set the state-of-the-art but to perform an empirical exploration, we report results across all parameter values on test data.", "labels": [], "entities": []}, {"text": "shows the results of the original embeddings (\u03b1 = 0) and those of the best \u03b1, while shows the relative error reduction with respect to the original embeddings for all \u03b1 values . As it can be seen, the proposed post-processing brings big improvements in word analogy, with a relative error reduction of about 20% in semantic analogies for word2vec and glove and a relative error reduction of about 10% in both semantic and syntactic analogies for fasttext.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 253, "end_pos": 265, "type": "TASK", "confidence": 0.8360962271690369}]}, {"text": "In order to better understand the effect of the proposed post-processing in downstream systems, we adopt the STS Benchmark dataset on semantic textual similarity . This task is akin to word similarity, but instead of assessing the similarity of individual word pairs, it is the similarity of entire sentence pairs as scored by the model that is compared against the gold standard produced by human annotators . This evaluation is attractive for our purposes because, while the state-of-the-art systems are supervised and based on elaborated deep learning or feature engineer-: Results in semantic textual similarity as measured by Pearson correlation for the original embeddings and the best post-processed model with the corresponding value of \u03b1.", "labels": [], "entities": [{"text": "STS Benchmark dataset", "start_pos": 109, "end_pos": 130, "type": "DATASET", "confidence": 0.9132522940635681}, {"text": "word similarity", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.7455278933048248}, {"text": "Pearson correlation", "start_pos": 631, "end_pos": 650, "type": "METRIC", "confidence": 0.9720652997493744}]}, {"text": "The DAM scores are averaged across 10 runs.", "labels": [], "entities": [{"text": "DAM", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9511423707008362}]}, {"text": "ing approaches, simpler embedding-based unsupervised models are also highly competitive, making it easier to analyze the effect of the proposed post-processing when integrating the embeddings in a larger model.", "labels": [], "entities": []}, {"text": "This way, we test two such systems in our experiments: a simple embeddingbased model that computes the cosine similarity between the centroids of each sentence after discarding stopwords, and the Decomposable Attention Model (DAM) proposed by and minimally adapted for the task . The centroid model is thus a simple but very competitive baseline system where the proposed postprocessing has a direct effect, whereas DAM is a prototypical deep learning model that uses fixed pre-trained embeddings as input features, producing results that are almost at par with the state-ofthe-art in the task.", "labels": [], "entities": []}, {"text": "As the results in and show, the centroid method is much more sensitive to the proposed post-processing than DAM.", "labels": [], "entities": []}, {"text": "More concretely, negative values of \u03b1 are beneficial for the https://github.com/lgazpio/DAM_STS centroid method up to certain point, bringing an improvement of nearly 4.5 points for glove, and the results clearly start degrading after that ceiling.", "labels": [], "entities": []}, {"text": "In contrast, DAM is almost unaffected by negative values of \u03b1.", "labels": [], "entities": [{"text": "DAM", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9758833050727844}]}, {"text": "Positive values do have a clear negative effect in both cases, but the centroid method is much more severely affected than DAM.", "labels": [], "entities": [{"text": "DAM", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9289461970329285}]}, {"text": "For instance, for glove, the performance of the centroid method drops 18.19 points when \u03b1 = 0.50, in contrast with only 3.69 points for DAM.", "labels": [], "entities": []}, {"text": "This behavior can be theoretically explained by the fact that the proposed post-processing consists in a linear transformation.", "labels": [], "entities": []}, {"text": "More concretely, DAM also applies a linear transformation to the input embeddings and, given that the product of two linear transformations is just another linear transformation, its global optimum is unaffected by the linear transformation previously applied by our method.", "labels": [], "entities": [{"text": "DAM", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.5577881932258606}]}, {"text": "Note, moreover, that the same rationale applies to the majority of machine learning systems that use pre-trained embeddings as input features, including both linear and deep learning models.", "labels": [], "entities": []}, {"text": "While there are many practical aspects that can interfere with this theoretical reasoning (e.g. regularization, the optional length normalization of embeddings, the resulting difficulty of the optimization problem...), and explain the variations observed in our experiments, this shows that typical downstream systems are able to adjust the similarity order themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results in intrinsic evaluation for the original embeddings and the best post-processed model with the  corresponding value of \u03b1. The evaluation measure is accuracy for word analogy and Spearman correlation for  word similarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9994853734970093}, {"text": "word analogy", "start_pos": 179, "end_pos": 191, "type": "TASK", "confidence": 0.7679367959499359}, {"text": "Spearman correlation", "start_pos": 196, "end_pos": 216, "type": "METRIC", "confidence": 0.5914892256259918}]}, {"text": " Table 2: Results in semantic textual similarity as measured by Pearson correlation for the original embeddings and  the best post-processed model with the corresponding value of \u03b1. The DAM scores are averaged across 10 runs.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 64, "end_pos": 83, "type": "METRIC", "confidence": 0.9663370847702026}, {"text": "DAM", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9174742102622986}]}]}