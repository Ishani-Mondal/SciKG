{"title": [{"text": "A Trio Neural Model for Dynamic Entity Relatedness Ranking", "labels": [], "entities": [{"text": "Dynamic Entity Relatedness", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7564521233240763}]}], "abstractContent": [{"text": "Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications.", "labels": [], "entities": [{"text": "Measuring entity relatedness", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6950588723023733}, {"text": "natural language processing and information retrieval", "start_pos": 60, "end_pos": 113, "type": "TASK", "confidence": 0.6340027203162512}]}, {"text": "Prior work often studies entity relatedness in static settings and an unsupervised manner.", "labels": [], "entities": []}, {"text": "However , entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic overtime.", "labels": [], "entities": []}, {"text": "In this work, we propose a neural network-based approach for dynamic entity relatedness, leveraging the collective attention as supervision.", "labels": [], "entities": []}, {"text": "Our model is capable of learning rich and different entity representations in a joint framework.", "labels": [], "entities": []}, {"text": "Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring semantic relatedness between entities is an inherent component in many text mining applications.", "labels": [], "entities": [{"text": "text mining", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.720789298415184}]}, {"text": "In search and recommendation, the ability to suggest most related entities to the entity-bearing query has become a standard feature of popular Web search engines (.", "labels": [], "entities": []}, {"text": "In natural language processing, entity relatedness is an important factor for various tasks, such as entity linking) or word sense disambiguation).", "labels": [], "entities": [{"text": "entity linking", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.7105832993984222}, {"text": "word sense disambiguation", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.6457772453625997}]}, {"text": "However, prior work on semantic relatedness often neglects the time dimension and consider entities and their relationships as static.", "labels": [], "entities": []}, {"text": "In practice, many entities are highly ephemeral (, and users seeking information related to those entities would like to see fresh information.", "labels": [], "entities": []}, {"text": "For example, users looking up the entity Taylor Lautner during 2008-2012 might want to be recommended with entities such as The Twilight Saga, due to Lautner's well-known performance in the film series; however the same query in August 2016 should be served with entities related to his appearances in more recent films such as \"Scream Queens\", \"Run the Tide\".", "labels": [], "entities": []}, {"text": "In addition, much of previous work resorts to deriving semantic relatedness from co-occurrence -based computations or heuristic functions without direct optimization to the final goal.", "labels": [], "entities": []}, {"text": "We believe that desirable framework should see entity semantic relatedness as not separate but an integral part of the process, for instance in a supervised manner.", "labels": [], "entities": []}, {"text": "In this work, we address the problem of entity relatedness ranking, that is, designing the semantic relatedness models that are optimized for ranking systems such as top-k entity retrieval or recommendation.", "labels": [], "entities": [{"text": "entity relatedness ranking", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7276810805002848}]}, {"text": "In this setting, the goal is not to quantify the semantic relatedness between two entities based on their occurrences in the data, but to optimize the partial order of the related entities in the top positions.", "labels": [], "entities": []}, {"text": "This problem differs from traditional entity ranking () in that the entity rankings are driven by user queries and are optimized to their (ad-hoc) information needs, while entity relatedness ranking also aims to uncover the meanings of the the relatedness from the data.", "labels": [], "entities": []}, {"text": "In other words, while conventional entity semantic relatedness learns from data (editors or content providers' perspectives), and entity ranking learns from the user's perspective, the entity relatedness ranking takes the tradeoff between these views.", "labels": [], "entities": []}, {"text": "Such a hybrid approach can benefit applications such as exploratory entity search (, where users have a specific goal in mind, but at the same time are opened to other related entities.", "labels": [], "entities": [{"text": "exploratory entity search", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.7377002636591593}]}, {"text": "We also tackle the issue of dynamic ranking and design the supervised-learning model that takes into account the temporal contexts of entities, and proposes to leverage collective attention from public sources.", "labels": [], "entities": [{"text": "dynamic ranking", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6932012140750885}]}, {"text": "As an illustration, when one looks into the Wikipedia page of Taylor Lautner, each navi- gation to other Wikipedia pages indicates the user interest in the corresponding target entity given her initial interest in Lautner.", "labels": [], "entities": [{"text": "Wikipedia page of Taylor Lautner", "start_pos": 44, "end_pos": 76, "type": "DATASET", "confidence": 0.9442819833755494}]}, {"text": "Collectively, the navigation traffic observed overtime is a good proxy to the shift of public attention to the entity).", "labels": [], "entities": []}, {"text": "In addition, while previous work mainly focuses on one aspect of the entities such as textual profiles or linking graphs , we propose a trio neural model that learns the low level representations of entities from three different aspects: Content, structures and time aspects.", "labels": [], "entities": []}, {"text": "For the time aspect, we propose a convolutional model to embed and attend to local patterns of the past temporal signals in the Euclidean space.", "labels": [], "entities": []}, {"text": "Experiments show that our trio model outperforms traditional approaches in ranking correlation and recommendation tasks.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We present the first study of dynamic entity relatedness ranking using collective attention.", "labels": [], "entities": [{"text": "dynamic entity relatedness ranking", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.6762859895825386}]}, {"text": "\u2022 We introduce an attention-based convolutional neural networks (CNN) to capture the temporal signals of an entity.", "labels": [], "entities": []}, {"text": "\u2022 We propose a joint framework to incorporate multiple views of the entities, both from content provider and from user's perspectives, for entity relatedness ranking.", "labels": [], "entities": [{"text": "entity relatedness ranking", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.674491802851359}]}], "datasetContent": [{"text": "In this work we use Wikipedia data as the case study for our entity relatedness ranking problem due to its rich knowledge and dynamic nature.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.8647777438163757}, {"text": "entity relatedness ranking", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7021097838878632}]}, {"text": "It is worth noting that despite experimenting on Wikipedia, our framework is universal can be applied to other sources of entity with available temporal signals and entity navigation.", "labels": [], "entities": [{"text": "entity navigation", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.712559700012207}]}, {"text": "We use Wikipedia pages to represent entities and page views as the temporal signals (details in section 6.1).", "labels": [], "entities": []}, {"text": "For entity navigation, we use the clickstream dataset generated from the Wikipedia webserver logs from February until September, 2016.", "labels": [], "entities": [{"text": "entity navigation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8629544973373413}, {"text": "clickstream dataset generated from the Wikipedia webserver logs", "start_pos": 34, "end_pos": 97, "type": "DATASET", "confidence": 0.8482388742268085}]}, {"text": "These datasets contain an accumulation of transitions between two Wikipedia articles with their respective counts on a monthly basis.", "labels": [], "entities": []}, {"text": "We study only actual pages (e.g. excluding disambiguation or redirects).", "labels": [], "entities": []}, {"text": "In the following, we provide the first analysis of the clickstream data to gain insights into the temporal dynamics of the entity collective attention in Wikipedia.", "labels": [], "entities": []}, {"text": "illustrates the distribution of entities by click frequencies, and the correlation of top popular entities (measured by total navigations) across different months is shown in.", "labels": [], "entities": []}, {"text": "In general, we observe that the user navigation activities in the top popular entities are very dynamic,, there are 24.31% of entities in top-10,000 most active entities of September 2006 do not appear in the same list the previous month.", "labels": [], "entities": [{"text": "user navigation", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6707687377929688}]}, {"text": "And 30.61% are new compared with 5 months before.", "labels": [], "entities": []}, {"text": "In addition, there are 71% of entities in top-10,000 having navigations to new entities compared to the previous month, with approx. 18 new entities are navigated to, on average.", "labels": [], "entities": []}, {"text": "Thus, the datasets are naturally very dynamic and sensitive to change.", "labels": [], "entities": []}, {"text": "The substantial amount of missing past click logs on the newly-formed relationships also raises the necessity of an dynamic measuring approach.", "labels": [], "entities": []}, {"text": "shows the overall architecture of our framework, which consists of three major components: time-, graph-and content-based networks.", "labels": [], "entities": []}, {"text": "Each component can be considered as a separate sub-ranking network.", "labels": [], "entities": []}, {"text": "Each network accepts a tuple of three elements/representations as an input in a pair-wise fashion, i.e., the source entity e s , the target entity e t with higher rank (denoted as e ) and the one with lower rank (denoted as e (\u2212) ).", "labels": [], "entities": []}, {"text": "For the content network, each element is a sequence of terms, coming from entity textual representation.", "labels": [], "entities": []}, {"text": "For the graph network, we learn the embed- dings from the entity linking graph.", "labels": [], "entities": []}, {"text": "For the time network, we propose anew convolutional model learning from the entity temporal signals.", "labels": [], "entities": []}, {"text": "More detailed are described as follows.", "labels": [], "entities": []}, {"text": "To recap from Section 4.1, we use the click stream datasets in 2016.", "labels": [], "entities": [{"text": "click stream datasets in 2016", "start_pos": 38, "end_pos": 67, "type": "DATASET", "confidence": 0.7760198950767517}]}, {"text": "We also use the corresponding Wikipedia article dumps, with over 4 million entities represented by actual pages.", "labels": [], "entities": []}, {"text": "Since the length of the content of an Wikipedia article is often long, in this work, we make use of only its abstract section.", "labels": [], "entities": []}, {"text": "To obtain temporal signals of the entity, we use page view statistics of Wikipedia articles and aggregate the counts by month.", "labels": [], "entities": []}, {"text": "We fetch the data from June, 2014 up until the studied time, which results in the length of 27 months.", "labels": [], "entities": []}, {"text": "Seed entities and related candidates.", "labels": [], "entities": []}, {"text": "To extract popular and trending entities, we extract from the clickstream data the top 10,000 entities based on the number of navigations from major search engines (Google and Bing), at the studied time.", "labels": [], "entities": []}, {"text": "Getting the subset of related entity candidatesfor efficiency purposes-has been well-addressed in related work (  candidates which are visited from the seed entities at studied time.", "labels": [], "entities": []}, {"text": "We filtered out entity-candidate pairs with too few navigations (less than 10) and considered the top-100 candidates.", "labels": [], "entities": []}, {"text": "The time granularity is set to months.", "labels": [], "entities": []}, {"text": "The studied time tn of our experiments is September 2016.", "labels": [], "entities": []}, {"text": "From the seed queries, we use 80% for training, 10% for development and 10% for testing, as shown in.", "labels": [], "entities": []}, {"text": "Note that, for the time-aware setting and to avoid leakage and bias as much as possible, the data for training and development (including supervision) are up until time tn \u2212 1.", "labels": [], "entities": []}, {"text": "In specific, for content and graph data, only tn \u2212 1 is used.", "labels": [], "entities": []}, {"text": "We use 2 correlation coefficient methods, Pearson and Spearman, which have been used often throughout literature, cf. (. The Pearson index focuses on the difference between predicted-vscorrect relatedness scores, while Spearman focuses on the ranking order among entity pairs.", "labels": [], "entities": []}, {"text": "Our work studies on the strength of the dynamic relatedness between entities, hence we focus more on Pearson index.", "labels": [], "entities": [{"text": "Pearson index", "start_pos": 101, "end_pos": 114, "type": "METRIC", "confidence": 0.8928687274456024}]}, {"text": "However, traditional correlation metrics do not consider the positions in the ranked list (correlations at the top or bottom are treated equally).", "labels": [], "entities": []}, {"text": "For this reason, we adjust the metric to consider the rankings at specific top-k positions, which consequently can be used to measure the correlation for only top items in the ranking (based to the ground truth).", "labels": [], "entities": []}, {"text": "In addition, we use Normalized Discounted Cumulative Gain (NDCG) measure to evaluate the recommendation tasks.", "labels": [], "entities": [{"text": "Normalized Discounted Cumulative Gain (NDCG) measure", "start_pos": 20, "end_pos": 72, "type": "METRIC", "confidence": 0.6588344573974609}]}, {"text": "All neural models are implemented in TensorFlow.", "labels": [], "entities": []}, {"text": "Initial learning rate is tuned amongst {1.e-2, 1.e-3, 1.e-4, 1.e-5}.", "labels": [], "entities": []}, {"text": "The batch size is tuned amongst {50, 100, 200}.", "labels": [], "entities": []}, {"text": "The weight matrices are initialized with samples from the uniform distribution).", "labels": [], "entities": []}, {"text": "Models are trained for maximum 25 epochs.", "labels": [], "entities": []}, {"text": "The hidden layers for each network are among {2, 3, 4}, while for hidden nodes are {128, 256, 512}.", "labels": [], "entities": []}, {"text": "Dropout rate is set from {0.2, 0.3, 0.5}.", "labels": [], "entities": [{"text": "Dropout rate", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.5907663702964783}]}, {"text": "The pretrained DW is empirically set to 128 dimensions, and 200 for PV.", "labels": [], "entities": []}, {"text": "For CNN, the filter number are in {10, 20, 30}, window size in {4, 5, 6}, convolutional layers in {1, 2, 3} and decay rate \u03b1 in {1.0, 1.5,\u00b7 \u00b7 \u00b7 ,7.5}.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8369299173355103}]}, {"text": "2 conv-layers with window size 5 and 4, number of filters of 20 and 25 respectively are used for decay hyperparameter analysis.", "labels": [], "entities": []}, {"text": "We evaluate our proposed method in two different scenarios: (1) Relatedness ranking and (2) Entity recommendation.", "labels": [], "entities": [{"text": "Relatedness ranking", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.5839326679706573}]}, {"text": "The first task evaluates how well we can mimic the ranking via the entity navigation.", "labels": [], "entities": []}, {"text": "Here we use the raw number of navigations in Wikipedia clickstream.", "labels": [], "entities": [{"text": "Wikipedia clickstream", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.9448119103908539}]}, {"text": "The second task is formulated as: given an entity, suggest the top-k most related entities to it right now.", "labels": [], "entities": []}, {"text": "Since there is no standard ground-truth for this temporal task, we constructed two relevance ground-truths.", "labels": [], "entities": []}, {"text": "The first one is the proxy ground-truth, with relevance grade is automatically assigned from the (top-100) most navigated target entities.", "labels": [], "entities": []}, {"text": "The graded relevance score is then given as the reversed rank order.", "labels": [], "entities": []}, {"text": "For this, all entities in the test set are used.", "labels": [], "entities": []}, {"text": "The second one is based on the human judgments with 5-level graded relevance scale, i.e., from 4 -highly relevant to 0 -not (temporally) relevant.", "labels": [], "entities": [{"text": "5-level graded relevance scale", "start_pos": 52, "end_pos": 82, "type": "METRIC", "confidence": 0.6129431203007698}]}, {"text": "Two human experts evaluate on the subset of 20 entities (randomly sampled from the test set), with 600 entity pairs (approx. 30 per seed, using pooling method).", "labels": [], "entities": []}, {"text": "The ground-truth size is comparable the widely used ground-truth for static relatedness assessment,).", "labels": [], "entities": [{"text": "static relatedness assessment", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.5985641976197561}]}, {"text": "The Cohen's Kappa agreement is 0.72.", "labels": [], "entities": [{"text": "Cohen's Kappa agreement", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.5704712271690369}]}, {"text": "Performance of the best-performed models on this dataset is then tested with paired t-test against the WLM baseline.", "labels": [], "entities": [{"text": "WLM baseline", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.7770646512508392}]}], "tableCaptions": [{"text": " Table 1: Statistics on the dynamic of clickstream,  e s denote source entities, e t related entities.", "labels": [], "entities": []}, {"text": " Table 3: Performance of different models on task (1) Pearson, Spearman's \u03c1 ranking correlation, and  task (2) recommendation (measured by nDCG). Bold and underlined numbers indicate best and second- to-best results. shows statistical significant over WLM (p < 0.05).", "labels": [], "entities": [{"text": "Pearson", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9615026712417603}, {"text": "Spearman's \u03c1 ranking correlation", "start_pos": 63, "end_pos": 95, "type": "METRIC", "confidence": 0.6315897583961487}]}]}