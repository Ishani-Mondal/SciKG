{"title": [{"text": "Learning text representations for 500K classification tasks on Named Entity Disambiguation", "labels": [], "entities": [{"text": "500K classification tasks", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7463226318359375}, {"text": "Named Entity Disambiguation", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6760562062263489}]}], "abstractContent": [{"text": "Named Entity Disambiguation algorithms typically learn a single model for all target entities.", "labels": [], "entities": [{"text": "Named Entity Disambiguation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6403919657071432}]}, {"text": "In this paper we present a word expert model and train separate deep learning models for each target entity string, yielding 500K classification tasks.", "labels": [], "entities": []}, {"text": "This gives us the opportunity to benchmark popular text representation alternatives on this massive dataset.", "labels": [], "entities": []}, {"text": "In order to face scarce training data we propose a simple data-augmentation technique and transfer-learning.", "labels": [], "entities": []}, {"text": "We show that bag-of-word-embeddings are better than LSTMs for tasks with scarce training data, while the situation is reversed when having larger amounts.", "labels": [], "entities": []}, {"text": "Transferring an LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands.", "labels": [], "entities": [{"text": "Transferring an LSTM", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7823767066001892}, {"text": "context representation", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7056925594806671}]}, {"text": "The experiments show that our system trained on out-of-domain Wikipedia data surpasses comparable NED systems which have been trained on in-domain training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Disambiguation (NED), also known as Entity Linking or Entity Resolution, is a task where entity mentions in running text need to be linked to its entity entry in a Knowledge Base (KB), such as Wikidata, Wikipedia or other derived resources like DBpedia ().", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7953505714734396}, {"text": "Entity Linking or Entity Resolution", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.6901840925216675}]}, {"text": "This task is challenging, as some entity mentions like \"London\" can refer to a number of places, people, fictional characters, brands, movies, books or songs.", "labels": [], "entities": [{"text": "London\" can refer to a number of places, people, fictional characters, brands, movies, books or songs", "start_pos": 56, "end_pos": 157, "type": "Description", "confidence": 0.795442979444157}]}, {"text": "Given a mention in context, NED methods) typically rely on three models: (1) a mention model which collects possible entities which can be referred to by the mention string (aliases or surface forms), possibly weighted according to prior probabilities; (2) a context model which measures to which extent the entities fit well in the context of the mention, using textual features; (3) a coherence model which prefers entities that are related to the other entities in the document.", "labels": [], "entities": []}, {"text": "The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately.", "labels": [], "entities": []}, {"text": "The third model is global, in that all mentions are disambiguated simultaneously.", "labels": [], "entities": []}, {"text": "Recent work has shown that local models can be improved adding a global coherence model.", "labels": [], "entities": []}, {"text": "In this work we focus on a local model, and a global model could improve the results further.", "labels": [], "entities": []}, {"text": "All local and global systems mentioned above, as well as the current state-of-the-art systems (, rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia.", "labels": [], "entities": []}, {"text": "While this has the advantage of reusing the parameters across mentions, it also makes the problem unnecessarily complex.", "labels": [], "entities": []}, {"text": "In this paper we propose to break the task of NED into 500K classification tasks, one for each target mention, as opposed to building a single model for all 500K mentions.", "labels": [], "entities": [{"text": "NED", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.864297091960907}]}, {"text": "The advantage of this approach is that each of the 500K classification tasks is simpler, as the classifier needs to focus on learning a good context model fora single mention and a limited set of entities (those returned by the mention model).", "labels": [], "entities": [{"text": "500K classification tasks", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7285361687342325}]}, {"text": "On the negative side, training instances for mentions follow along tail distribution, with some mentions having a huge number of examples, but with the vast majority of mentions having very limited training data, e.g. 10 occurrences linking to an entity in Wikipedia.", "labels": [], "entities": []}, {"text": "Our results will show that data-augmentation and transfer learning allow us to overcome the sparseness problem, yielding the best results among local systems, very close to the best local/global combined systems.", "labels": [], "entities": []}, {"text": "Contrary to systems trained on in-domain data, ours is trained on Wikipedia and tested out-of-domain.", "labels": [], "entities": []}, {"text": "From another perspective, a set of 500K classification problems provides a great experimental framework for testing text representation and classification algorithms.", "labels": [], "entities": [{"text": "500K classification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.5665344148874283}, {"text": "text representation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7174654006958008}]}, {"text": "More specifically, deep learning methods provide end-to-end algorithms to learn both representations and classifiers jointly ( . In fact, learning text representations models has become a center topic in natural language understanding, as it allows to transfer representation models across tasks.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 204, "end_pos": 234, "type": "TASK", "confidence": 0.6436481575171152}]}, {"text": "In this paper, we explore several popular text representation options, as well as dataaugmentation ( and transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.9061647653579712}]}, {"text": "All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe . This paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first present our models.", "labels": [], "entities": []}, {"text": "Section 3 presents the experiments, followed by related work and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We developed and evaluated our model in standard datasets for easier comparison to the state of the art.", "labels": [], "entities": []}, {"text": "use Aidatesta for model selection only (i.e. the parameters were tuned on a subset of Wikipedia, cf. Section 2), and Aidatestb, Tac2010, Tac2011 and Tac2012 for out-of-domain test.", "labels": [], "entities": []}, {"text": "Note that we used Aidatesta only to select the best models, given that all hyperparameters where tuned over Wikipedia itself.", "labels": [], "entities": []}, {"text": "shows the statistics for all datasets.", "labels": [], "entities": []}, {"text": "From all mentions, only a subset of them actually refers to an entity in the KB provided by the dataset authors (\"inKB mentions\" row).", "labels": [], "entities": []}, {"text": "Our dictionary covers most but not all of those KB entities (\"uniq inKB mentions in dict\" row).", "labels": [], "entities": []}, {"text": "Some of the mentions in the datasets are resolved as NIL, for cases where the mention refers to an entity which is not in the KB.", "labels": [], "entities": [{"text": "NIL", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.891444981098175}]}, {"text": "The simplest method to return NIL is to first resolve overall Wikipedia entities, and if the selected entity is not in the KB then to return NIL.", "labels": [], "entities": []}, {"text": "We focus the evaluation in the mentions linked to an entity in the respective KB, and use the so-called inKB accuracy as the evaluation measure, which is defined as the fraction of correctly disambiguated mentions divided by the total number of mentions which are linked to the KB.", "labels": [], "entities": [{"text": "inKB accuracy", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.7449476718902588}]}, {"text": "We perform 3 runs for each reported result, reporting mean accuracy and standard deviation values.", "labels": [], "entities": [{"text": "mean", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9749627709388733}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8502019643783569}]}, {"text": "We also include MFS baselines in the results: given a mention, the baseline is computed assigning the entity in the dictionary with highest prior probabilities.", "labels": [], "entities": []}, {"text": "During testing, given a mention, we search the document and try to find the longest string that a) contains the mention and b) matches an entry in the dictionary.", "labels": [], "entities": []}, {"text": "Next, we replace every mention string with that longer string in the document.", "labels": [], "entities": []}, {"text": "We also apply the 'One entity per Document' hypothesis, averaging the results of the occurrences for the same mention in the same document ().", "labels": [], "entities": []}, {"text": "Mentions that are named as a DBPedia entity classified as location are not expanded.", "labels": [], "entities": []}, {"text": "shows the performance of each of the context representation models and data augmentation options in Aidatesta.", "labels": [], "entities": []}, {"text": "The MFS baseline obtains 71.91, which is a good point of comparison to benchmark our candidate model (the dictionary) with respect to other systems.", "labels": [], "entities": []}, {"text": "All our models improve over the MFS baseline by a large margin.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.7264540940523148}]}, {"text": "As mentioned in Section 2.5, we have three classifiers for each mention.", "labels": [], "entities": []}, {"text": "P (e|c) orig uses the original training set, P (e|c) aug uses the augmented training set, and P (e|c) combines both.", "labels": [], "entities": []}, {"text": "The table shows that the results of the original and augmented classifiers are more or less comparable, while the combination consistently yields the best results for all context representations options.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets (see text for details).", "labels": [], "entities": []}, {"text": " Table 2: Development results (Aidatesta) as inKB accuracy and standard deviation for Sparse BoW, Continuous  BoW, LSTM and transferred LSTMs. Each row corresponds to the original training data, augmented training data,  and combination.", "labels": [], "entities": [{"text": "Aidatesta", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9663768410682678}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992557168006897}, {"text": "Sparse BoW", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.6385261714458466}]}, {"text": " Table 4: Test results on TAC datasets as inKB accuracy.  * for systems trained on in-domain data.  \u2020 for systems  using semi-supervised methods.", "labels": [], "entities": [{"text": "TAC datasets", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.7610215544700623}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9670055508613586}]}]}