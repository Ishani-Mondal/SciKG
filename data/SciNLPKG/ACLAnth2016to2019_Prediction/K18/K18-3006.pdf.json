{"title": [{"text": "The NYU System for the CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection", "labels": [], "entities": [{"text": "NYU System", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9709474146366119}, {"text": "Universal Morphological Reinflection", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.5276852548122406}]}], "abstractContent": [{"text": "This paper describes the NYU submission to the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection.", "labels": [], "entities": [{"text": "NYU", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8940061330795288}, {"text": "CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection", "start_pos": 47, "end_pos": 120, "type": "TASK", "confidence": 0.5187331885099411}]}, {"text": "Our system participates in the low-resource setting of Task 2, track 2, i.e., it predicts morphologically inflected forms in context: given a lemma and a context sentence, it produces a form of the lemma which might be used at an indicated position in the sentence.", "labels": [], "entities": []}, {"text": "It is based on the standard attention-based LSTM encoder-decoder model, but makes use of multiple encoders to process all parts of the context as well as the lemma.", "labels": [], "entities": []}, {"text": "In the official shared task evaluation, our system obtains the second best results out of 5 submissions for the competition it entered and strongly outperforms the official baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The extreme type sparsity in text in a morphologically rich language, i.e., a language which relies strongly on changes in the surface form of words to express properties like gender, tense or number, requires natural language processing (NLP) systems which are able to handle inflected words in a systematic way.", "labels": [], "entities": []}, {"text": "The SIGMOR-PHON and CoNLL-SIGMORPHON shared tasks on morphological reinflection, which have been held since 2016), encourage the development of computational models for inflection in a large number of languages.", "labels": [], "entities": []}, {"text": "This year's edition ( features two different tasks.", "labels": [], "entities": []}, {"text": "The datasets for Task 1 consist of triplets of lemma, morphological tag (also called the \"target tag\") and the corresponding inflected form, which is given for training and should be produced attest time.", "labels": [], "entities": []}, {"text": "This is the standard inflection setup which has also been subject of the shared tasks in the last years.", "labels": [], "entities": []}, {"text": "Task 2, in contrast, is again split into two different subtasks (called \"tracks\").", "labels": [], "entities": []}, {"text": "Both are focused on inflection in context.", "labels": [], "entities": []}, {"text": "Here, a sentence is given, in the context of which the inflected form of which only the lemma is known should be used.", "labels": [], "entities": []}, {"text": "The setup of the first subtask assumes that the lemmas and tags of all surrounding words are available and can be used for predicting.", "labels": [], "entities": [{"text": "predicting", "start_pos": 123, "end_pos": 133, "type": "TASK", "confidence": 0.9765027761459351}]}, {"text": "These might be used as desired, e.g., the tags of the previous and next words are often strong indicators for the tag of the form to be produced, which is unknown.", "labels": [], "entities": []}, {"text": "Track 2, on the other hand, requires systems to produce inflected forms only from their lemma and the inflected context words; no tags or lemmas are given for the context.", "labels": [], "entities": []}, {"text": "Thus, track 2 is both a more realistic and a harder version of track 1.", "labels": [], "entities": []}, {"text": "All tasks and tracks feature 3 different settings: a low-resource setting (LOW), a medium-resource setting (MEDIUM) and a highresource setting.", "labels": [], "entities": [{"text": "MEDIUM", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.6987606287002563}]}, {"text": "In this paper, we describe the New York University (NYU) submission to the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection", "start_pos": 75, "end_pos": 148, "type": "TASK", "confidence": 0.5086763426661491}]}, {"text": "The system we submitted was exclusively designed for Task 2, track 2, LOW.", "labels": [], "entities": [{"text": "LOW", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7205191850662231}]}, {"text": "Thus, we only focus on this particular competition and do not report numbers for other setups (though, in theory, every system which works for track 2 of Task 2 can also produce output for track 1; the same holds true for LOW/MEDIUM/HIGH).", "labels": [], "entities": [{"text": "LOW/MEDIUM/HIGH", "start_pos": 222, "end_pos": 237, "type": "METRIC", "confidence": 0.6144151210784912}]}, {"text": "Overall, our system obtains the second highest test accuracy out of 5 submitted systems and outperforms the official shared task baseline by a wide margin.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9680683016777039}]}], "datasetContent": [{"text": "The data for Task 2, track 2, LOW consists of sentences taken from the Universal Dependencies   (UD) treebanks.", "labels": [], "entities": [{"text": "LOW", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.601315438747406}, {"text": "Universal Dependencies   (UD) treebanks", "start_pos": 71, "end_pos": 110, "type": "DATASET", "confidence": 0.5846775670846304}]}, {"text": "All context forms, as well as the lemma of the target inflected form are given for each sentence.", "labels": [], "entities": []}, {"text": "Training and development sets feature exactly one correct target form, while, for the test set, additional plausible target forms have been manually given by the shared task organizers (.", "labels": [], "entities": []}, {"text": "The languages we experiment on are German, English, Spanish, Finnish, French, Russian and Swedish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracies when considering only the  gold solution; BL = BASELINE; CPH = COPEN- HAGEN; CUB = CUBoulder. Best results per language  in bold; our results in italic.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.5435192584991455}, {"text": "BL", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9977534413337708}, {"text": "BASELINE", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.7975968718528748}, {"text": "CPH = COPEN- HAGEN", "start_pos": 83, "end_pos": 101, "type": "METRIC", "confidence": 0.7948707222938538}]}, {"text": " Table 2: Test accuracies when counting all plausible  forms as correct; BL = BASELINE; CPH = COPEN- HAGEN; CUB = CUBoulder. Best results per language  in bold; our results in italic.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.5281952023506165}, {"text": "BL", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9976885318756104}, {"text": "BASELINE", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7691474556922913}, {"text": "CPH = COPEN- HAGEN", "start_pos": 88, "end_pos": 106, "type": "METRIC", "confidence": 0.791087532043457}]}]}