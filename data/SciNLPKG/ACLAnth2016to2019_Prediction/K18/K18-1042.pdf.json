{"title": [], "abstractContent": [{"text": "In this paper, we propose anew linguistically-based approach to answering non-factoid open-domain questions from unstructured data.", "labels": [], "entities": []}, {"text": "First, we elaborate on an architecture for textual encoding based on which we introduce a deep end-to-end neural model.", "labels": [], "entities": [{"text": "textual encoding", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7639983594417572}]}, {"text": "This architecture benefits from a bilateral attention mechanism which helps the model to focus on a question and the answer sentence at the same time for phrasal answer extraction.", "labels": [], "entities": [{"text": "phrasal answer extraction", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.6539537409941355}]}, {"text": "Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output.", "labels": [], "entities": []}, {"text": "By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively .", "labels": [], "entities": [{"text": "MS-MARCO datasets", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.9274813234806061}]}], "introductionContent": [{"text": "Reading, comprehending and reasoning over texts and answering a question about them (i.e. Question Answering) is a fundamental aspect of computational intelligence.", "labels": [], "entities": [{"text": "Reading, comprehending and reasoning over texts and answering a question about them (i.e. Question Answering)", "start_pos": 0, "end_pos": 109, "type": "TASK", "confidence": 0.65819670425521}]}, {"text": "Question Answering (QA), as a measure of intelligence, has been even suggested to replace Turing test.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.6730559408664704}]}, {"text": "The development of large datasets of QA in recent years ( advanced the field especially for two significant branches of QA namely factoid and non-factoid QA 1.", "labels": [], "entities": []}, {"text": "Non-factoid QA or QA over unstructured data is a somewhat new challenge in open-domain QA.", "labels": [], "entities": []}, {"text": "A non-factoid QA system answers questions by reading and comprehending a context.", "labels": [], "entities": []}, {"text": "The context in which we assume the answer is mentioned may have different granularities from a single sentence or paragraph to larger units of text.", "labels": [], "entities": []}, {"text": "A QA system is supposed to extract a phrase answer from the provided paragraph or sentence depending on its granularity level.", "labels": [], "entities": []}, {"text": "The context for answering questions is usually extracted using an Information Retrieval (IR) technique.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7772608041763306}]}, {"text": "Then, a QA system should extract the best answer sentence.", "labels": [], "entities": []}, {"text": "There are many studies about extracting answer sentences including but not limited to).", "labels": [], "entities": []}, {"text": "Extracting the final or shortest possible answer from a set of candidate answer sentences is addressed in many studies as well ().", "labels": [], "entities": [{"text": "Extracting the final or shortest possible answer from a set of candidate answer sentences", "start_pos": 0, "end_pos": 89, "type": "TASK", "confidence": 0.6455161997250148}]}, {"text": "Instead of reasoning over and making inference on linguistic symbols (i.e., words or characters), almost all of these models use a neural architecture to encode contexts and questions into a vector representation and to reason over them.", "labels": [], "entities": []}, {"text": "A typical pattern inmost of the current models is the use of a variant of uni-or bi-directional attention schemes (question to context and viceversa) to encode the semantic content of questions' words with a focus on their context's words ().", "labels": [], "entities": []}, {"text": "Compared to these models the novelty of our work is in explicitly conducting attention over both the context and the question for each candidate constituent 2 answer (every constituent  in the context).", "labels": [], "entities": []}, {"text": "The fact that this is better than attending only to the question words is investigated and proved according to the results reported in Section 7.", "labels": [], "entities": []}, {"text": "Another observation is that a majority of recent studies are purely based on data science where one can barely see a linguistic intuition towards the problem.", "labels": [], "entities": []}, {"text": "We show that a pure linguistic intuition could help neural reasoning and attention mechanisms to achieve quantitatively and qualitatively better results in QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.8869854211807251}]}, {"text": "By analyzing a human-generated QA dataset called SQuAD (, we realized that people tend to answer questions in units called constituents.", "labels": [], "entities": []}, {"text": "We expect an answer to a question to be a valid constituent otherwise it would probably not be grammatical.", "labels": [], "entities": []}, {"text": "Constituents and Constituency relations are the bases of Phrase Structure Grammar first proposed by Noam Chomsky.", "labels": [], "entities": [{"text": "Phrase Structure Grammar", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.931788702805837}]}, {"text": "Phrase Structure Grammar and many of its variants including Government and Binding theory) or Generalized and Head-driven Phrase Structure Grammar ( define hierarchical binary relations between the constituents of a text, and hence help to realize an exact and natural answer boundary for answer extraction.", "labels": [], "entities": [{"text": "Phrase Structure Grammar", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7669603228569031}, {"text": "Government and Binding theory", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.7711794823408127}, {"text": "Generalized and Head-driven Phrase Structure Grammar", "start_pos": 94, "end_pos": 146, "type": "TASK", "confidence": 0.6112949351469675}, {"text": "answer extraction", "start_pos": 289, "end_pos": 306, "type": "TASK", "confidence": 0.7666531503200531}]}, {"text": "Having these two points in mind and inspired by attentive pooling networks by, we designed an attentive bilateral model and trained it on the constituents of questions and answers.", "labels": [], "entities": []}, {"text": "We attempted to use some information from the parser, so to go beyond a simple wordbased or vector-based representations.", "labels": [], "entities": []}, {"text": "The results obtained by the model are near to human performance on SQuAD dataset and competitive to a state-of-the-art system on MS-MARCO dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9130986332893372}, {"text": "MS-MARCO dataset", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.9785366058349609}]}, {"text": "The contributions of our work are: \u2022 A bilateral linguistically-based attention model for Question Answering \u2022 Integrating linguistic constituents into a DNN architecture for the QA task.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8720122575759888}]}, {"text": "In the next section, we review some of recent QA systems with a focus on unstructured QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9239532947540283}]}, {"text": "In Section 3, we briefly explain the constituency types.", "labels": [], "entities": []}, {"text": "Then in Section 4, we discuss the details of our system architecture.", "labels": [], "entities": []}, {"text": "In Section 5, we talk about the datasets and the way we prepared them for training.", "labels": [], "entities": []}, {"text": "In Sections 6 and 7, we describe the training details and present the results of our experiments.", "labels": [], "entities": []}, {"text": "Finally, we explain some ablation studies and error analysis in Section 8 before we conclude in Section 9.", "labels": [], "entities": [{"text": "ablation", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9834104180335999}]}], "datasetContent": [{"text": "The  To control the vocabulary size we needed to eliminate redundant numeric values, but at the same time, we wanted to parse the contents, and we needed to keep the semantic values of numeric tokens.", "labels": [], "entities": []}, {"text": "Hence to preprocess the questions and sentences in the dataset, we removed all nonalphanumeric characters from all contents and then replaced numeric values with '9'.", "labels": [], "entities": []}, {"text": "Then we used CoreNLP tool () to tokenize and to perform constituency parsing on the contents.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.80596724152565}]}, {"text": "After extracting constituents from the tree of sentences and comparing them with gold answers, we realized that 72% of the answers are constituents.", "labels": [], "entities": []}, {"text": "Other 21% of the answers had slight divergences from a constituent, like lacking or having a determiner or punctuation mark which were eventually going to be disregarded in the official evaluation script.", "labels": [], "entities": []}, {"text": "The remaining 7% was a combination of two smaller constituents or apart of a larger one.", "labels": [], "entities": []}, {"text": "In the training set, to use constituents as answers, we replaced non-matching answers with the smallest and most similar constituents.", "labels": [], "entities": []}, {"text": "Since at the evaluation time, we needed the gold answers and not their replaced constituents, we did not change the answers in the development set.", "labels": [], "entities": []}, {"text": "We extracted a total number of 48 different constituents types including both terminal and nonterminal ones from SQuAD.", "labels": [], "entities": []}, {"text": "The percentage of each constituent type in training and development sets are presented in.", "labels": [], "entities": []}, {"text": "The figures for development set are computed only based on exact match answers.", "labels": [], "entities": []}, {"text": "We used SQuAD's development set for testing the system and reporting the results.", "labels": [], "entities": [{"text": "SQuAD's development set", "start_pos": 8, "end_pos": 31, "type": "DATASET", "confidence": 0.9046490043401718}]}, {"text": "To prepare the dataset for training and evaluating our system we used a state-of-the-art answer sentence selection system (Aghaebrahimian, 2017a) to extract the best answer sentences.", "labels": [], "entities": [{"text": "Aghaebrahimian, 2017a)", "start_pos": 123, "end_pos": 145, "type": "DATASET", "confidence": 0.8033984005451202}]}, {"text": "The system provides us the best sentence with 94.4 % accuracy given each question.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9924178123474121}]}, {"text": "After pre-processing the sentence as explained above, we extracted its constituents and trained the model using the correct constituents as true and other constituents as negative samples.", "labels": [], "entities": []}, {"text": "At test time, we used the same procedure to extract the constituents, but we used gold answers as they are without substituting non-matching answerconstituents.", "labels": [], "entities": []}, {"text": "Then, we added other constituents as negative samples.", "labels": [], "entities": []}, {"text": "For evaluation purpose, we used SQuAD's official evaluation script which computes the exact match and the F1 score.", "labels": [], "entities": [{"text": "SQuAD's official evaluation script", "start_pos": 32, "end_pos": 66, "type": "DATASET", "confidence": 0.914270544052124}, {"text": "exact match", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.8589305281639099}, {"text": "F1 score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9850705564022064}]}, {"text": "The exact match is the percentage of predictions which exactly match the gold answer and the F1 (Macro-averaged) score is the average overlap between the prediction and ground truth answer while treating them both as bags of tokens, and computing their F1.", "labels": [], "entities": [{"text": "exact match", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.973703920841217}, {"text": "F1 (Macro-averaged) score", "start_pos": 93, "end_pos": 118, "type": "METRIC", "confidence": 0.8083595335483551}, {"text": "F1", "start_pos": 253, "end_pos": 255, "type": "METRIC", "confidence": 0.9943163990974426}]}, {"text": "To experiment our model furthermore, we used MS-MARCO dataset.", "labels": [], "entities": [{"text": "MS-MARCO dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9701161682605743}]}, {"text": "As a machine comprehension dataset, MS-MARCO has two fundamental differences with SQuAD.", "labels": [], "entities": []}, {"text": "Every question in MS-MARCO has several passages from which the best answer should be retrieved.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9625145792961121}]}, {"text": "Moreover, the answers in MS-MARCO are not necessarily sub-spans of the provided contexts so that BLEU and ROUGE are used as the metrics in the official tool of MS-MARCO evaluation.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9353511333465576}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9987046718597412}, {"text": "ROUGE", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9943497776985168}]}, {"text": "During training we used the highest BLEU scored constituent as the answer and in the evaluation, we computed the BLEU and ROUGE scores of the constituents selected by the system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9980334639549255}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9993683695793152}, {"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9713259339332581}]}, {"text": "As the results in show, our system obtained competitive results to another state-of-the-art system trained on the same dataset.", "labels": [], "entities": []}, {"text": "To evaluate our new architecture and to see how integrating linguistic constituents affects its performance we setup two settings.", "labels": [], "entities": []}, {"text": "We designed one setting for evaluating the effect of using constituents instead of words (constituent-base vs. word-base) and another to evaluate the effect of using attention mechanism on top of vector training modules (uni-vs. bi-attention).", "labels": [], "entities": []}, {"text": "Therefore we conducted four experiments on both datasets or eight experiments in total.", "labels": [], "entities": []}, {"text": "In the constituent-base setting, we generated the training and test data as described in Section 5.", "labels": [], "entities": []}, {"text": "In word-base however, we replaced constituents with the tokens in answer sentences for both train and test sets and trained our model to compute two scores for initial and final positions of answer chunks.", "labels": [], "entities": []}, {"text": "In the constituent-base setting attest time, we directly used the predicted constituent as the final answer.", "labels": [], "entities": []}, {"text": "In the word-base setting, however, we got the final answer using the highest-scored words for initial and final positions.", "labels": [], "entities": []}, {"text": "We also investigated the effect of bilateral attention on the model performance.", "labels": [], "entities": []}, {"text": "In the bi-attention model, we used the model as described in Section 4.", "labels": [], "entities": []}, {"text": "In the uni-attention model, we eliminated the attention on sentences and only used the module for attention on questions.", "labels": [], "entities": []}, {"text": "For training our model, we used 300-dimensional pre-trained Glove word vectors () to generate the embedding matrix and kept the embedding matrix updated through training.", "labels": [], "entities": []}, {"text": "We used 128-dimensional LSTMs for all recurrent networks and used 'Adam' with parameters learning rate=0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999 for optimization.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.9621649384498596}]}, {"text": "We set batch size to 32 and dropout rate to 0.5 for all LSTMs and embedding layers.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9445321559906006}]}, {"text": "We performed the accuracy check only on the first best answer.", "labels": [], "entities": [{"text": "accuracy check", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9674968421459198}]}], "tableCaptions": [{"text": " Table 2: The performances of different models in the exact match and F1 metrics for SQuAD and BLEU  and ROUGE for the MS-MARCO dataset.", "labels": [], "entities": [{"text": "match", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.5165112018585205}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9976028800010681}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9975506663322449}, {"text": "ROUGE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9962061643600464}, {"text": "MS-MARCO dataset", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.9690739810466766}]}]}