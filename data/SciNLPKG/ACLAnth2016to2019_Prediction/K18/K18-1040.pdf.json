{"title": [{"text": "Unsupervised Sentence Compression using Denoising Auto-Encoders", "labels": [], "entities": []}], "abstractContent": [{"text": "In sentence compression, the task of shortening sentences while retaining the original meaning, models tend to be trained on large corpora containing pairs of verbose and compressed sentences.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7678732872009277}]}, {"text": "To remove the need for paired corpora, we emulate a summarization task and add noise to extend sentences and train a denoising auto-encoder to recover the original, constructing an end-to-end training regime without the need for any examples of compressed sentences.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.900978684425354}]}, {"text": "We conduct a human evaluation of our model on a standard text summarization dataset and show that it performs comparably to a supervised base-line based on grammatical correctness and retention of meaning.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7113798558712006}]}, {"text": "Despite being exposed to no target data, our unsupervised models learn to generate imperfect but reasonably readable sentence summaries.", "labels": [], "entities": []}, {"text": "Although we underperform supervised models based on ROUGE scores, our models are competitive with a supervised baseline based on human evaluation for grammatical correctness and retention of meaning.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9453341364860535}]}], "introductionContent": [{"text": "Sentence compression is the task of condensing a longer sentence into a shorter one that still retains the meaning of the original.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9365028142929077}]}, {"text": "Past models for sentence compression have tended to rely heavily on strong linguistic priors such as syntactic rules or heuristics ().", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7640604376792908}]}, {"text": "More recent work using deep learning involves models trained without strong linguistic priors, instead requiring large corpora consisting of pairs of longer and shorter sentences.", "labels": [], "entities": []}, {"text": "* Denotes equal contribution Sentence compression can also be can be seen as a \"scaled down version of the text summarization problem\" ().", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.9473893344402313}, {"text": "text summarization", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7231593430042267}]}, {"text": "Within text summarization, two broad approaches exist: extractive approaches extract explicit tokens or phrases from the reference text, whereas abstractive approaches involve a compressed paraphrasing of the reference text, similar to the approach humans might take).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.6716057807207108}]}, {"text": "In the related domain of machine translation, a task that also involves learning a mapping from one string of tokens to another, state of the art models using deep learning techniques are trained on large parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.766404926776886}]}, {"text": "Recent promising work on unsupervised neural machine translation ( has shown that with the right training regime, it is possible to train models for machine translation between two languages given only two unpaired monolingual corpora.", "labels": [], "entities": [{"text": "unsupervised neural machine translation", "start_pos": 25, "end_pos": 64, "type": "TASK", "confidence": 0.6493586897850037}, {"text": "machine translation between two languages", "start_pos": 149, "end_pos": 190, "type": "TASK", "confidence": 0.8207972407341003}]}, {"text": "In this paper, we apply neural text summarization techniques to the task of sentence compression, focusing on on extractive summarization.", "labels": [], "entities": [{"text": "neural text summarization", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.6638717651367188}, {"text": "sentence compression", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.759037435054779}, {"text": "extractive summarization", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.5724536329507828}]}, {"text": "However, we depart significantly from prior work by taking a fully unsupervised training approach.", "labels": [], "entities": []}, {"text": "Beyond not using parallel corpora, we train our model using a single corpus.", "labels": [], "entities": []}, {"text": "In contrast to unsupervised neural machine translation, which still uses two corpora, we do not have separate corpora of longer and shorter sentences.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.8127842942873637}]}, {"text": "We show that a simple denoising auto-encoder model, trained on removing and reordering words from a noised input sequence, can learn effective sentence compression, generating shorter sequences of reasonably grammatical text that retain the original meaning.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.724361851811409}]}, {"text": "While the models are still prone to both errors in grammar and meaning, we believe that this is a strong step toward reducing reliance on paired corpora.", "labels": [], "entities": []}, {"text": "We evaluate our model using both a stan-dard text-summarization benchmark as well as human evaluation of compressed sentences based on grammatical correctness and retention of meaning.", "labels": [], "entities": []}, {"text": "Although our models do not capture the written style of the target summaries (headlines), they still produce reasonably readable and accurate compressed sentence summaries, without ever being exposed to any target sentence summaries.", "labels": [], "entities": []}, {"text": "We find that our model underperforms based on ROUGE metrics, especially compared to supervised models, but performs competitively with supervised baselines inhuman evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9034377336502075}]}, {"text": "We further show that providing the model with a sentence embedding of the original sentence leads to better ROUGE scores but worse human evaluation scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9978176355361938}]}, {"text": "However, both unsupervised and supervised methods still fall short based on human evaluation, and effective sentence compression and summarization remains an open problem.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7635790705680847}, {"text": "summarization", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.8877621293067932}]}], "datasetContent": [{"text": "In, we evaluate our models on ROUGE) F1 scores, where a higher score is better.", "labels": [], "entities": [{"text": "ROUGE) F1 scores", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.8276630192995071}]}, {"text": "We provide a comparison with a simple but strong baseline, F8W is simply first 8 words of the input, as is done in and similarly to the Prefix baseline (first 75 bytes) of, as well as the ROUGE of the whole text with the target.", "labels": [], "entities": [{"text": "F8W", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8553216457366943}, {"text": "ROUGE", "start_pos": 188, "end_pos": 193, "type": "METRIC", "confidence": 0.9959734082221985}]}, {"text": "We provide scores of two supervised text-summarization methods on Gigaword.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9467698335647583}]}, {"text": "One is our own baseline, consisting of a sequence-to-sequence attentional encoder-decoder trained on pairs of reference and target summary text, but incorporating the same length countdown mechanism as in our unsupervised models.", "labels": [], "entities": []}, {"text": "The other is the words-lvt2k-1sent model of.", "labels": [], "entities": []}, {"text": "Although not their best model, it is most comparable to ours since it only uses the first sentence and does not extract tf-idf vectors nor named entities tags.", "labels": [], "entities": []}, {"text": "F8W and All text are strong baselines due to the tendency of news articles to contain specific terms that are rarely rephrased.", "labels": [], "entities": [{"text": "F8W", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9443941116333008}, {"text": "All", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.8336372375488281}]}, {"text": "We find that our models perform competitively with these baselines, although they pale in comparison to supervised methods, likely because they do not learn any style transfer and use only the reference's vocabulary and writing style.", "labels": [], "entities": []}, {"text": "While our ROUGE-1 scores are inline with the baselines, our ROUGE-2 scores fall somewhat behind.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9826369881629944}, {"text": "ROUGE-2", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9821761250495911}]}, {"text": "Including InferSent sentence embeddings improves our ROUGE scores across the board.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9916558265686035}]}, {"text": "Our supervised baseline performance is close to that of, with results lower in ROUGE-2 likely due to their use of beam search.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9390730261802673}, {"text": "beam search", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.791871041059494}]}, {"text": "Nevertheless, the supervised baseline is representative of the performance of a standard sequence-to-sequence attentional model on this task.", "labels": [], "entities": []}, {"text": "A direct comparison of ROUGE scores is not completely adequate for evaluating our model.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9720790982246399}]}, {"text": "Because of our training regime, our model primarily learned to generate shortened sentences that often still retain the style of the input sentences.", "labels": [], "entities": []}, {"text": "Unlike other model setups, our model has never been exposed to any examples of summaries, and hence never adapts its output to match the style of the target summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.9724101424217224}]}, {"text": "In the case of Gigaword, the summaries are headlines from news articles, which are written in a particular linguistic style (e.g. dropping articles, having clauses rather than full sentences).", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9171850085258484}]}, {"text": "ROUGE will thus penalize our model, that tends to output longer, full sentences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9073473811149597}]}, {"text": "In addition, ROUGE is an imperfect metric for summarization as word/n-gram overlap does not fully capture summary relevancy and retention of meaning.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9698771238327026}, {"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9893696308135986}]}, {"text": "For this reason, we also conduct a separate human evaluation of our different models against Example 1: I: nearly ### of the released hostages remain in hospital , and more than ### of them are in very serious condition , russian medical authorities said sunday . G: nearly ### people still hospitalized more than ### in critical condition 2-g shuf: more than ### hostages are in serious condition , russian medical authorities said . 2-g shuf + InferSent: nearly ### hostages of the nearly released in serious medical condition , said . Example 2: I: french president jacques chirac arrived here friday at the start of a <unk> during which he is expected to hold talks with romanian leaders on bucharest 's application to join nato . G: chirac arrives in romania 2-g shuf: french president jacques chirac arrived here friday to hold talks with romanian leaders on nato . 2-g shuf + InferSent: french president jacques chirac arrived here friday at the start of talks to join nato . Example 4: I: wall street stocks rallied friday as a weak report onus economic growth boosted hopes for an easier interest rate policy from the federal reserve and investors reacted to upbeat earnings news . G: wall street shrugs off weak gdp pushes higher 2-g shuf: wall street stocks rallied friday as investors reacted to upbeat economic news and interest rate .  outputs from two of our models.", "labels": [], "entities": []}, {"text": "I is input, G (gold) is the true summaries.", "labels": [], "entities": []}, {"text": "Example 1 and 2 show our models summarizing pertinent information from the input.", "labels": [], "entities": [{"text": "summarizing pertinent information", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.9086165030797323}]}, {"text": "Example 3 demonstrates the ability to recover long ordered strings of tokens, even though the models are trained on shuffle data.", "labels": [], "entities": []}, {"text": "Example 4 shows cases where the models output grammatical but semantically incorrect sentences.", "labels": [], "entities": []}, {"text": "a supervised baseline (Section 5.4).", "labels": [], "entities": []}, {"text": "To qualitatively evaluate our model, we take inspiration from the methodology of to design our human evaluation.", "labels": [], "entities": []}, {"text": "We asked 6 native English speakers to evaluate randomly chosen summaries from five models: our best models with and without InferSent sentence embeddings, a summary generated from a trained supervised model, and the ground truth summary.", "labels": [], "entities": []}, {"text": "The sentences are evaluated based on two separate criteria: the grammaticality of the summary and how well it retained the information of the original sentence.", "labels": [], "entities": []}, {"text": "In the former, only the summary is provided, whereas in the latter, the evaluator is shown both the original sentence as well as the summary.", "labels": [], "entities": []}, {"text": "Each of these criteria were graded on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "The examples are from the test set, with 50 examples randomly sampled for each evaluator and criterion.", "labels": [], "entities": []}, {"text": "We report the average evaluation given by our 6 evaluators in.", "labels": [], "entities": []}, {"text": "That the Meaning score for the ground truth is somewhat low (3.87) is not surprising.", "labels": [], "entities": [{"text": "Meaning score", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9851384460926056}]}, {"text": "Within the Gigaword dataset, summaries (headlines) sometimes include information not within the reference (main line of the article).", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9655418694019318}]}, {"text": "We observe that quantitative evaluation does not correlate well with human evaluation.", "labels": [], "entities": []}, {"text": "Methods using InferSent embeddings improved our ROUGE scores but perform worse inhuman evaluation, which is inline with the summaries presented in 2.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9899146556854248}]}, {"text": "Notably, the model trained on shuffled bigrams and InferSent embeddings performed best within our ablation study, but the worst among the three models inhuman evaluation.", "labels": [], "entities": []}, {"text": "Encouragingly, the model without InferSent embeddings performs competitively with the supervised baseline in both grammar and meaning scores, indicating that although it does not capture the style of headlines, it succeeds in generating grammatical sentences that    roughly match the meaning in the reference.", "labels": [], "entities": []}, {"text": "Some evaluators highlighted that it was problematic to rate meaning for ungrammatical sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of Baseline, Unsupervised and Supervised Models. Our unsupervised models pale in comparison to", "labels": [], "entities": []}, {"text": " Table 2: Ablation study. We find that using attention, shuf-", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.98348468542099}]}, {"text": " Table 3: Effect of input sentence length on performance, us-", "labels": [], "entities": []}]}