{"title": [{"text": "Weakly-supervised Neural Semantic Parsing with a Generative Ranker", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7248724897702535}]}], "abstractContent": [{"text": "Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent.", "labels": [], "entities": []}, {"text": "The task is challenging due to the large search space and spuriousness of logical forms.", "labels": [], "entities": []}, {"text": "In this paper we introduce a neural parser-ranker system for weakly-supervised semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.7452344298362732}]}, {"text": "The parser generates candidate tree-structured logical forms from utterances using clues of denotations.", "labels": [], "entities": []}, {"text": "These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics.", "labels": [], "entities": []}, {"text": "We present a scheduled training procedure to balance the contribution of the two objectives.", "labels": [], "entities": []}, {"text": "Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model.", "labels": [], "entities": []}, {"text": "Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range.", "labels": [], "entities": [{"text": "Freebase datasets", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.969508022069931}]}], "introductionContent": [{"text": "Semantic parsing is the task of converting natural language utterances into machine-understandable meaning representations or logical forms.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8279220759868622}]}, {"text": "The task has attracted much attention in the literature due to a wide range of applications ranging from question answering () to relation extraction, goal-oriented dialog , and instruction understanding (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8480121493339539}, {"text": "relation extraction", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8877133727073669}, {"text": "instruction understanding", "start_pos": 178, "end_pos": 203, "type": "TASK", "confidence": 0.7594765722751617}]}, {"text": "Ina typical semantic parsing scenario, a logical form is executed against a knowledge base to produce an outcome (e.g., an answer) known as denotation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7594784200191498}]}, {"text": "Conventional semantic parsers are trained on collections of utterances paired with annotated logical forms (; Wong and Mooney, 2006;.", "labels": [], "entities": []}, {"text": "However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale.", "labels": [], "entities": [{"text": "labeling of logical forms", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.8353127837181091}]}, {"text": "As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7170625329017639}]}, {"text": "One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs, since such data are relatively easy to obtain via crowdsourcing ().", "labels": [], "entities": []}, {"text": "However, the unavailability of logical forms in the weakly-supervised setting, renders model training more difficult.", "labels": [], "entities": []}, {"text": "A fundamental challenge in learning semantic parsers from denotations is finding consistent logical forms, i.e., those which execute to the correct denotation.", "labels": [], "entities": []}, {"text": "This search space can be very large, growing exponentially as compositionality increases.", "labels": [], "entities": []}, {"text": "Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousness -some of them will accidentally execute to the correct denotation without reflecting the meaning of the utterance.", "labels": [], "entities": []}, {"text": "These spurious logical forms are misleading supervision sig-nals for the semantic parser.", "labels": [], "entities": []}, {"text": "In this work we introduce a weakly-supervised neural semantic parsing system which aims to handle both challenges.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.762602965037028}]}, {"text": "Our system, shown in, mainly consists of a sequence-to-tree parser which generates candidate logical forms fora given utterance.", "labels": [], "entities": []}, {"text": "These logical forms are subsequently ranked by two components: a log-linear model scores the likelihood of each logical form executing to the correct denotation, and an inverse neural parser measures the degree to which the logical form represents the meaning of the utterance.", "labels": [], "entities": []}, {"text": "We present a scheduled training scheme which balances the contribution of the two components and objectives.", "labels": [], "entities": []}, {"text": "To further boost performance, we propose to neurally encode a lexicon, as a means of injecting prior domain knowledge to the neural parameters.", "labels": [], "entities": []}, {"text": "We evaluate our system on three Freebase datasets which consist of utterance denotation pairs: WEBQUESTIONS (Berant et al., 2013a), GRAPHQUESTIONS (, and SPADES (.", "labels": [], "entities": [{"text": "Freebase datasets", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9560748040676117}, {"text": "WEBQUESTIONS", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9354525208473206}, {"text": "GRAPHQUESTIONS", "start_pos": 132, "end_pos": 146, "type": "METRIC", "confidence": 0.9968546628952026}, {"text": "SPADES", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.8957321643829346}]}, {"text": "Experimental results across datasets show that our weakly-supervised semantic parser achieves state-of-the-art performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the performance our semantic parser.", "labels": [], "entities": []}, {"text": "We introduce the various datasets used in our experiments, training settings, model variants used for comparison, and finally present and analyze our results.", "labels": [], "entities": []}, {"text": "We evaluated our model on three", "labels": [], "entities": []}], "tableCaptions": []}