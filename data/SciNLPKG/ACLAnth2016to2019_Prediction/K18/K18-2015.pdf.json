{"title": [{"text": "The SLT-Interactions Parsing System at the CoNLL 2018 Shared Task", "labels": [], "entities": [{"text": "SLT-Interactions Parsing", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9637964963912964}, {"text": "CoNLL 2018 Shared Task", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.9360544085502625}]}], "abstractContent": [{"text": "This paper describes our system (SLT-Interactions) for the CoNLL 2018 shared task: Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.8173604160547256}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.8014567017555236}]}, {"text": "Our system performs three main tasks: word segmen-tation (only for few treebanks), POS tagging and parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.8699767589569092}]}, {"text": "While segmentation is learned separately, we use neural stacking for joint learning of POS tagging and parsing tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.7818278670310974}, {"text": "parsing tasks", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.7849343717098236}]}, {"text": "For all the tasks, we employ simple neural network architectures that rely on long short-term memory (LSTM) networks for learning task-dependent features.", "labels": [], "entities": []}, {"text": "At the basis of our parser, we use an arc-standard algorithm with Swap action for general non-projective parsing.", "labels": [], "entities": []}, {"text": "Additionally, we use neural stacking as a knowledge transfer mechanism for cross-domain parsing of low resource domains.", "labels": [], "entities": [{"text": "neural stacking", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.758857250213623}, {"text": "cross-domain parsing of low resource domains", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.8331047197182974}]}, {"text": "Our system shows substantial gains against the UDPipe baseline, with an average improvement of 4.18% in LAS across all languages.", "labels": [], "entities": [{"text": "UDPipe baseline", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.7966455221176147}, {"text": "LAS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.7854459881782532}]}, {"text": "Overall, we are placed at the 12 th position on the official test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our system for the CoNLL 2018 shared task) contains the following modules: word segmentation, part-of-speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8327362090349197}, {"text": "word segmentation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7744009792804718}, {"text": "part-of-speech (POS) tagging", "start_pos": 94, "end_pos": 122, "type": "TASK", "confidence": 0.6291960597038269}, {"text": "dependency parsing", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7833423316478729}]}, {"text": "In some cases, we also use a transliteration module to transcribe data into Roman form for efficient processing.", "labels": [], "entities": []}, {"text": "\u2022 Segmentation We mainly use this module to identify word boundaries in certain languages such as Chinese where space is not used as a boundary marker.", "labels": [], "entities": []}, {"text": "\u2022 POS tagging For all the languages, we only focus on universal POS tags while ignoring language specific POS tags and morphological features.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.7786067426204681}]}, {"text": "\u2022 Dependency parsing We use an arc-standard transition system with an additional Swap action for unrestricted parsing.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7639727294445038}]}, {"text": "We rely on UDPipe 1.2 ( for tokenization for almost all the treebanks except for Chinese and Japanese where we observed that the UDPipe segmentation had an adverse effect on parsing performance as opposed to gold segmentation on the development sets.", "labels": [], "entities": []}, {"text": "Moreover, we also observed that training a separate POS tagger was also beneficial as the UDPipe POS tagger had slightly lower performance in some languages.", "labels": [], "entities": [{"text": "UDPipe POS tagger", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.7904632290204366}]}, {"text": "However, other than tokenization, we ignored other morphological features predicted by UDPipe and didn't explore their effect on parsing.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.8441712260246277}]}, {"text": "Additionally, we use knowledge transfer approaches to enhance the performance of parsers trained on smaller treebanks.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7567009627819061}]}, {"text": "We leverage related treebanks (other treebanks of the same language) using neural stacking for learning better cross-domain parsers.", "labels": [], "entities": []}, {"text": "We also trained a generic character-based parsing system for languages that have neither in-domain nor cross-domain training data.", "labels": [], "entities": []}, {"text": "Upon the official evaluation on 82 test sets, our system (SLT-Interactions) obtained the 12 th position in the parsing task and achieved an average improvement of 4.18% in LAS over the UDPipe baseline.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.8688693344593048}, {"text": "LAS", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9914336204528809}]}], "datasetContent": [{"text": "We train three kinds of parsing models based on the availability of training data: stack-prop models trained for languages having large treebanks, ii) stacking models for languages having smaller in-domain treebanks and large out-domain treebanks, and iii) backoff character models for those languages which have neither in-domain nor outdomain training data.", "labels": [], "entities": []}, {"text": "We will first discuss the details about the experimental setup for all these models and subsequently, we will discuss the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word segmentation results on Chinese  and Japanese development sets. B and I mark the  Begining and Inside of a word.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7397698909044266}, {"text": "B", "start_pos": 79, "end_pos": 80, "type": "METRIC", "confidence": 0.9778945446014404}]}, {"text": " Table 2. As  shown in the Table, we achieved an average im- provement of 3% in LAS over the UDPipe base- line. By using our segmentation models, we have  achieved better ranking for these two languages  than our average ranking in the official evaluation.", "labels": [], "entities": [{"text": "im- provement", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9267645478248596}, {"text": "LAS", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9811559915542603}, {"text": "UDPipe base- line", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.8212761282920837}]}, {"text": " Table 2: Impact of our word segmentation mod- els on Chinese and Japanese development sets.", "labels": [], "entities": [{"text": "word segmentation mod- els", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.8109645426273346}]}, {"text": " Table 3. For En- glish domains, there is an improvement of 1% to  2% using eng ewt as source domain for knowl- edge transfer, while for French improvements are  quite high (2% to 5%) using fr gsd as source do- main. Similar to the impact of word segmentation,  our ranking on treebanks that use neural stacking  is better than our average.", "labels": [], "entities": [{"text": "knowl- edge transfer", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.5922593772411346}, {"text": "word segmentation", "start_pos": 242, "end_pos": 259, "type": "TASK", "confidence": 0.7725300490856171}, {"text": "neural stacking", "start_pos": 296, "end_pos": 311, "type": "TASK", "confidence": 0.7816929817199707}]}, {"text": " Table 3: Impact of neural stacking on multiple  domains of English and French.", "labels": [], "entities": [{"text": "neural stacking", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7535509169101715}]}, {"text": " Table 4: Accuracy of different parsing models on the evaluation set. POS tags  are jointly predicted with parsing. LID = Language tag, TRN = Translitera- tion/normalization.", "labels": [], "entities": [{"text": "LID", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9513247013092041}, {"text": "TRN", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9267069697380066}]}]}