{"title": [{"text": "Learning to Actively Learn Neural Machine Translation", "labels": [], "entities": [{"text": "Learning to Actively Learn Neural Machine Translation", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.591888998235975}]}], "abstractContent": [{"text": "Traditional active learning (AL) methods for machine translation (MT) rely on heuristics.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.6610442340373993}, {"text": "machine translation (MT)", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8635207176208496}]}, {"text": "However, these heuristics are limited when the characteristics of the MT problem change due to e.g. the language pair or the amount of the initial bitext.", "labels": [], "entities": [{"text": "MT problem", "start_pos": 70, "end_pos": 80, "type": "TASK", "confidence": 0.9373141527175903}]}, {"text": "In this paper, we present a framework to learn sentence selection strategies for neural MT.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.706520602107048}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.8239148855209351}]}, {"text": "We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest.", "labels": [], "entities": []}, {"text": "The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget.", "labels": [], "entities": []}, {"text": "Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel training bitext plays a key role in the quality neural machine translation (NMT).", "labels": [], "entities": [{"text": "quality neural machine translation (NMT)", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.7270714853491101}]}, {"text": "Learning high-quality NMT models in bilingually lowresource scenarios is one of the key challenges, as NMT's quality degrades severely in such setting (.", "labels": [], "entities": []}, {"text": "Recently, the importance of learning NMT models in scarce parallel bitext scenarios has gained attention.", "labels": [], "entities": []}, {"text": "Unsupervised approaches try to learn NMT models without the need for parallel bitext).", "labels": [], "entities": []}, {"text": "Dual learning/backtranslation tries to start off from a small amount of bilingual text, and leverage monolingual text in the source and target language.", "labels": [], "entities": []}, {"text": "Zero/few shot approach attempts to transfer NMT learned from rich bilingual settings to low-resource settings.", "labels": [], "entities": []}, {"text": "In this paper, we approach this problem from the active learning (AL) perspective.", "labels": [], "entities": []}, {"text": "Assuming the availability of an annotation budget and a pool of monolingual source text as well as a small training bitext, the goal is to select the most useful source sentences and query their translation from an oracle up to the annotation budget.", "labels": [], "entities": []}, {"text": "The queried sentences need to be selected carefully to get the value for the budget, i.e. get the highest improvements in the translation quality of the retrained model.", "labels": [], "entities": []}, {"text": "The AL approach is orthogonal to the aforementioned approaches to bilingually lowresource NMT, and can be potentially combined with them.", "labels": [], "entities": [{"text": "AL", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9733242988586426}]}, {"text": "We present a framework to learn the sentence selection policy most suitable and effective for the NMT task at hand.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7322998344898224}, {"text": "NMT task", "start_pos": 98, "end_pos": 106, "type": "TASK", "confidence": 0.6988638937473297}]}, {"text": "This is in contrast to the majority of work in AL-MT where hard-coded heuristics are used for query selection).", "labels": [], "entities": [{"text": "query selection", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7425476312637329}]}, {"text": "More concretely, we learn the query policy based on a high-resource language-pair sharing similar characteristics with the low-resource language-pair of interest.", "labels": [], "entities": []}, {"text": "After trained, the policy is applied to the language-pair of interest capitalising on the learned signals for effective query selection.", "labels": [], "entities": []}, {"text": "We make use of imitation learning (IL) to train the query policy.", "labels": [], "entities": []}, {"text": "Previous work has shown that the IL approach leads to more effective policy learning ( ), compared to reinforcement learning (RL) . Our proposed method effectively trains AL policies for batch queries needed for NMT, as opposed to the previous work on single query selection.", "labels": [], "entities": []}, {"text": "We conduct experiments on three language pairs Finnish-English, German-English, and Czech-English.", "labels": [], "entities": []}, {"text": "Simulating low resource scenarios, we consider various settings, including cold-start and warm-start as well as small and extremely small data conditions.", "labels": [], "entities": []}, {"text": "The experiments show the effectiveness and superiority of our policy query compared to strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets Our experiments use the following language pairs in the news domain based on WMT2018: English-Czech (EN-CS), EnglishGerman (EN-DE), English-Finnish (EN-FI).", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9721328020095825}]}, {"text": "For AL evaluation, we randomly sample 500K sentence pairs from the parallel corpora in WMT2018 for each of the three language pairs, and take 100K as the initially available bitext and the rest of 400K as the pool of untranslated sentences, pretending the translation is not available.", "labels": [], "entities": [{"text": "AL evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9086973667144775}, {"text": "WMT2018", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9676016569137573}]}, {"text": "During the AL iterations, the translation is revealed for the queried source sentences in order to retrain the underlying NMT model.", "labels": [], "entities": []}, {"text": "For pre-processing the text, we normalise the punctuations and tokenise using moses 1 scripts.", "labels": [], "entities": []}, {"text": "The trained models are evaluated using BLEU on tokenised and cased sensitive test data from the newstest 2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9983628392219543}, {"text": "cased sensitive test data from the newstest 2017", "start_pos": 61, "end_pos": 109, "type": "DATASET", "confidence": 0.7417611107230186}]}, {"text": "NMT Model Our baseline model consists of a 2-layer bi-directional LSTM encoder with an embeddings size of 512 and a hidden size of 512.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8633852601051331}]}, {"text": "The 1-layer LSTM decoder with 512 hidden units uses an attention network with 128 hidden units.", "labels": [], "entities": []}, {"text": "We use a multiplicative-style attention attention architecture ().", "labels": [], "entities": []}, {"text": "The model is optimized using Adam () with a learning rate of 0.0001, where the dropout rate is set to 0.3.", "labels": [], "entities": []}, {"text": "We set the mini-batch size to 200 and the maximum sentence length to 50.", "labels": [], "entities": []}, {"text": "We train the base NMT models for 5 epochs on the initially available bitext, as the perplexity on the dev set do not improve beyond more training epochs.", "labels": [], "entities": []}, {"text": "After getting new translated text in each AL iteration, we further sample \u00d75 more bilingual sentences from the previously available bitext, and make one pass over this data to re-train the underlying NMT model.", "labels": [], "entities": []}, {"text": "For decoding, we use beam-search with the beam size of 3.", "labels": [], "entities": []}, {"text": "Selection Strategies We compare our policybased sentence selection for NMT-AL with the following heuristics: \u2022 Random We randomly select monolingual sentences up to the AL budget.", "labels": [], "entities": [{"text": "NMT-AL", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8583633303642273}]}, {"text": "\u2022 Length-based We use shortest/longest monolingual sentences up to the AL budget.", "labels": [], "entities": [{"text": "AL budget", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9262692928314209}]}, {"text": "Given a monolingual sentence xx x, we compute the TTE as |\u02c6y|\u02c6y y y| i=1 Entropy[P i (.|\u02c6y|\u02c6y y y <i , xx x, \u03c6 \u03c6 \u03c6)] wher\u00ea y y y is the decoded translation based on the current underlying NMT model \u03c6 \u03c6 \u03c6, and P i (.|\u02c6y|\u02c6y y y <i , xx x, \u03c6 \u03c6 \u03c6) is the distribution over the vocabulary words for the position i of the translation given the source sentence and the previously generated words.", "labels": [], "entities": [{"text": "TTE", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.46179378032684326}]}, {"text": "We also experimented with the normalised version of this measure, i.e. dividing TTE by |\u02c6y|\u02c6y y y|, and found that their difference is negligible.", "labels": [], "entities": [{"text": "TTE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.8189132809638977}]}, {"text": "So we only report TTE results.", "labels": [], "entities": [{"text": "TTE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8279075026512146}]}], "tableCaptions": [{"text": " Table 1: BLEU scores on tests sets with different selection strategies, the budget is at token level with annotation  for 135.45k tokens and 677.25k tokens respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9958147406578064}]}, {"text": " Table 3: BLEU scores on tests sets using different selection strategies. The token level annotation budget is 677K.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9956865310668945}]}, {"text": " Table 4: The table gives an estimation of the resorted  heuristics.", "labels": [], "entities": []}]}