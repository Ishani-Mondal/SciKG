{"title": [{"text": "Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random Permutations (EARP)", "labels": [], "entities": []}], "abstractContent": [{"text": "Word order is clearly a vital part of human language, but it has been used comparatively lightly in distributional vector models.", "labels": [], "entities": []}, {"text": "This paper presents anew method for incorporating word order information into word vector embedding models by combining the benefits of permutation-based order encoding with the more recent method of skip-gram with negative sampling.", "labels": [], "entities": []}, {"text": "The new method introduced here is called Embeddings Augmented by Random Permutations (EARP).", "labels": [], "entities": [{"text": "Embeddings Augmented by Random Permutations (EARP)", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.7038375735282898}]}, {"text": "It operates by applying permutations to the coordinates of context vector representations during the process of training.", "labels": [], "entities": []}, {"text": "Results show an 8% improvement inaccuracy on the challenging Bigger Analogy Test Set, and smaller but consistent improvements on other analogy reference sets.", "labels": [], "entities": [{"text": "Bigger Analogy Test Set", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.7355513498187065}]}, {"text": "These findings demonstrate the importance of order-based information in analogical retrieval tasks, and the utility of random permutations as a means to augment neural embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recognition of the utility of corpus-derived distributed representations of words fora broad range of Natural Language Processing (NLP) tasks) has led to a resurgence of interest in methods of distributional semantics.", "labels": [], "entities": []}, {"text": "In particular, the neural-probabilistic word representations produced by the Skip-gram and Continuous Bag-of-Words (CBOW) architectures () implemented in the word2vec and fastText software packages have been extensively evaluated in recent years.", "labels": [], "entities": []}, {"text": "As was the case with preceding distributional models (see for example), these architectures generate vector representations of words -or word embeddings -such that words with similar proximal neighboring terms within a corpus of text will have similar vector representations.", "labels": [], "entities": []}, {"text": "As the relative position of these neighboring terms is generally not considered, distributional models of this nature are often (and sometimes derisively) referred to as bag-of-words models.", "labels": [], "entities": []}, {"text": "While methods of encoding word order into neural-probabilistic representations have been evaluated, these methods generally require learning additional parameters, either for each position in the sliding window, or for each context word-byposition pair.", "labels": [], "entities": []}, {"text": "In this paper we evaluate an alternative method of encoding the position of words within a sliding window into neural word embeddings using Embeddings Augmented by Random Permutations (EARP).", "labels": [], "entities": []}, {"text": "EARP leverages random permutations of context vector representations (), a technique that has not been applied during the training of neural word embeddings previously.", "labels": [], "entities": [{"text": "EARP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.555659294128418}]}, {"text": "Unlike prior approaches to encoding position into neural word embeddings, it imposes no computational and negligible space requirements.", "labels": [], "entities": []}, {"text": "Results show that the word order information encoded through EARP leads to a nearly 8% improvement (from 29.17% to 37.07%) in the accuracy of analogy predictions in the Bigger Analogy Test Set of , with the improvement being largest (over 20%) in the category of analogies that exhibit derivational morphology (a case where the use of subword information also improves accuracy for all representations with and without order information).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9985814094543457}, {"text": "Bigger Analogy Test Set", "start_pos": 169, "end_pos": 192, "type": "DATASET", "confidence": 0.662332072854042}, {"text": "accuracy", "start_pos": 369, "end_pos": 377, "type": "METRIC", "confidence": 0.9921886920928955}]}, {"text": "Smaller improvements in performance are evident on other analogy sets, and in downstream sequence labeling tasks.", "labels": [], "entities": [{"text": "downstream sequence labeling tasks", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.6395020484924316}]}, {"text": "This makes EARP a strong contender for enriching word embeddings with order-based information, leading to greater accuracy on more challenging semantic processing tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9984109401702881}]}], "datasetContent": [{"text": "To evaluate the nature and utility of the additional information encoded by permutation-based variants, we utilized a set of analogical retrieval reference sets, including the MSR set () consisting of 8,000 proportional analogy questions that are morphological in nature (e.g. young:younger:quick:?) and the Google analogy set () which includes 8,869 semantic (and predominantly geographic, e.g. brussels:belgium:dublin:?) and 10,675 morphologically-oriented \"syntactic\" questions.", "labels": [], "entities": [{"text": "Google analogy set", "start_pos": 308, "end_pos": 326, "type": "DATASET", "confidence": 0.7997147838274637}]}, {"text": "We also included the Bigger Analogy Test Set (BATS) set ( ), a more challenging set of 99,200 proportional analogy questions balanced across 40 linguistic types in four categories: Inflections (e.g. plurals, infinitives), Derivation (e.g. verb+er), Lexicography (e.g. hypernyms, synonyms) and Encylopedia (e.g. country:capital, male:female).", "labels": [], "entities": [{"text": "Bigger Analogy Test Set (BATS) set", "start_pos": 21, "end_pos": 55, "type": "DATASET", "confidence": 0.7177598178386688}]}, {"text": "We obtained these sets from the distribution described in To evaluate the effects of encoding word order on the relative distance between terms, we used a series of widely used reference sets that mediate comparison between human and machine estimates of pairwise similarity and relatedness between term pairs.", "labels": [], "entities": []}, {"text": "Specifically, we used Wordsim-353 (), split into subsets emphasizing similarity and relatedness); MEN () and Simlex-999 (.", "labels": [], "entities": []}, {"text": "For each of these sets, we estimated the Spearman correlation of the cosine similarity between vector representations of the words in a given pair, with the human ratings (averaged across raters) of similarity and/or relat-edness provided in the reference standards.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.6413846164941788}, {"text": "similarity", "start_pos": 199, "end_pos": 209, "type": "METRIC", "confidence": 0.9651009440422058}]}, {"text": "Only those examples in which all relevant terms were represented in our vector spaces were considered.", "labels": [], "entities": []}, {"text": "Consequently, our analogy test sets consisted of 6136; 19,420 and 88,108 examples for the MSR 8 , Google and BATS sets respectively.", "labels": [], "entities": [{"text": "MSR 8", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8712745904922485}, {"text": "BATS sets", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.7604309022426605}]}, {"text": "With pairwise similarity, we retained 998; 335 and all 3,000 of the Simlex, Wordsim and MEN examples respectively.", "labels": [], "entities": []}, {"text": "These numbers were identical across models, including fastText baselines.", "labels": [], "entities": [{"text": "fastText baselines", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9519502818584442}]}, {"text": "In addition we evaluated the effects of incorporating word order with EARP on three standard sequence labeling tasks: part-of-speech tagging of the Wall Street Journal sections of the Penn Treebank (PTB) and the CoNLL'00 sentence chunking and CoNLL'03 named entity recognition) shared tasks.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7294608354568481}, {"text": "Wall Street Journal sections of the Penn Treebank (PTB)", "start_pos": 148, "end_pos": 203, "type": "DATASET", "confidence": 0.8748240145769987}, {"text": "CoNLL'00 sentence chunking and CoNLL'03 named entity recognition", "start_pos": 212, "end_pos": 276, "type": "TASK", "confidence": 0.6224217787384987}]}, {"text": "As was the case with the pairwise similarity and relatedness evaluations, we conducted these evaluations using the repEval2016 package 9 after converting all vectors to the word2vec binary format.", "labels": [], "entities": []}, {"text": "This package provides implementations of the neural NLP architecture developed by Collobert and his colleagues (2011), which uses vectors for words within a five-word window as input, a single hidden layer of 300 units and an output Softmax layer.", "labels": [], "entities": []}, {"text": "The implementation provided in repEval2016 deviates by design from the original implementation by fixing word vectors during training in order to emphasize difference between models for the purpose of comparative evaluation, which tends to reduce performance (for further details, see ().", "labels": [], "entities": []}, {"text": "As spaces constructed with narrower sliding windows generally perform better on these tasks (, we conducted these experiments with models of window radius 2 only.", "labels": [], "entities": []}, {"text": "To facilitate fair comparison, we added random vectors representing tokens available in the fastText-derived spaces only to all spaces, replacing the original vectors where these existed.", "labels": [], "entities": []}, {"text": "This was important in this evaluation as only fastText retains vector representation for punctuation marks (these are eliminated by the Semantic Vectors tokenization procedure), resulting in a relatively large number of out-of-vocabulary terms and predictably reduced performance with the Semantic Vectors implementation of the same algorithm.", "labels": [], "entities": []}, {"text": "With the random vectors added, out-of-vocabulary rates were equivalent across the two SGNS implementations, resulting in similar performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Analogical retrieval results at radius 2 (top) and 5 (bottom). SW: subwords. Gsem/syn: \"semantic\"  and \"syntactic\" components of Google set. Binf/der/lex/enc: inflectional, derivational, lexical and encyclopedia- derived components of the BATS. SGNS: fastText and Semantic Vectors (semVec) implementations of  skipgram-with-negative-sampling. EARP: Embeddings Augmented by Random Permutations. EARPx: EARP  with exact window positions. Best results are in boldface, and rows concerning baseline models are shaded.", "labels": [], "entities": []}, {"text": " Table 2: Performance on Sequence Labeling Tasks. % accuracy shown for PTB, and % F-measure otherwise", "labels": [], "entities": [{"text": "Sequence Labeling Tasks", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.9121520320574442}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992097616195679}, {"text": "PTB", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.5919792652130127}, {"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9990794658660889}]}]}