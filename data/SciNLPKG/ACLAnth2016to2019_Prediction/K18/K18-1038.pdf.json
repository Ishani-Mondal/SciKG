{"title": [{"text": "Vectorial semantic spaces do not encode human judgments of intervention similarity", "labels": [], "entities": [{"text": "Vectorial semantic spaces", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8177956541379293}]}], "abstractContent": [{"text": "Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret.", "labels": [], "entities": []}, {"text": "Ina vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies.", "labels": [], "entities": []}, {"text": "Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent.", "labels": [], "entities": []}, {"text": "This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components.", "labels": [], "entities": []}, {"text": "We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret.", "labels": [], "entities": []}, {"text": "To cast light on what linguistic information is learnt and encoded in these representations, several pieces of work have recently studied core properties of language in syntax (, semantics, morphology.", "labels": [], "entities": []}, {"text": "Ina similar vein, we study another core, defining property of human languages: the property of long-distance dependencies.", "labels": [], "entities": []}, {"text": "Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent.", "labels": [], "entities": []}, {"text": "1 Sentence (1a) is a question about the object of the verb buy, whose canonical position is shown in angle brackets, thus connecting the first and last element in the sentence.", "labels": [], "entities": []}, {"text": "Sentence (2a) is a relative clause where the object of the verb wash is also the semantic object of the verb show, connecting two distant elements.", "labels": [], "entities": []}, {"text": "Sentence (3a) is also a relative clause where the word\u00e9tudiantword\u00b4word\u00e9tudiant (student) is the semantic object of the verb endort (put to sleep).", "labels": [], "entities": []}, {"text": "(1a) What do you wonder John bought <what> ? (2a) Show me the elephant that the lion is washing <the elephant>.", "labels": [], "entities": []}, {"text": "(3a) Jules sourit aux\u00e9tudiantsaux\u00b4aux\u00e9tudiants que l'orateur endort < \u00b4 etudiants> s\u00e9rieusement depuis le d\u00e9but.", "labels": [], "entities": []}, {"text": "'Jules smiles to the students who the speaker is putting seriously to sleep from the beginning.'", "labels": [], "entities": []}, {"text": "Long-distance dependencies are not all equally acceptable.", "labels": [], "entities": []}, {"text": "The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals To clarify the perhaps confusing terminology: the term long-distance dependencies is a technical term that refers to discontinuous constructions where two elements in the string receive the same interpretation.", "labels": [], "entities": []}, {"text": "Long-distance dependency constructions are wh-questions, relative clauses, right-node raising, among others (.", "labels": [], "entities": []}, {"text": "Not all long-distance are actually long, for example subject-oriented relative clauses, and not all long dependencies are long-distance dependencies, for example, long subject-verb agreement as studied in;; is usually not considered a long-distance dependency.", "labels": [], "entities": []}, {"text": "The unpronounced element(s) in the long-distance relation are indicated by < >.", "labels": [], "entities": []}, {"text": "We will adopt an intuitive and simple explanation, called intervention theory, some aspects of which will be explained in more detail below).", "labels": [], "entities": [{"text": "intervention theory", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9096046090126038}]}, {"text": "Ina nutshell, a long-distance dependency between two elements in a sentence is difficult or even impossible if a similar element intervenes.", "labels": [], "entities": []}, {"text": "For example, sentence (1a) is acceptable while (2a) causes trouble for children () and (3a) triggers agreement errors, because in (1a) there is no sufficiently similar intervener (John is animate and is not a question word while what introduces a question and is not animate), while in and there is (lion is animate like elephant and\u00e9tudiantsand\u00b4and\u00e9tudiants (students) is animate like orateur (speaker)).", "labels": [], "entities": []}, {"text": "We present results that show, under precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the notion of intervention similarity involved in longdistance dependencies, but probably only semantic associations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows, we describe the multiple steps necessary to construct the materials of our experiments.", "labels": [], "entities": []}, {"text": "To verify our hypothesis, we need two sets of materials: the experimental measures reflecting the grammaticality of a sentence and the word embeddings to calculate a vector space of similarities.", "labels": [], "entities": []}, {"text": "We describe these in turn.", "labels": [], "entities": []}, {"text": "We refer to the sentences in as examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Regressions (m), correlations (Pear- son r) and p-values.  ss=semantic similarity  (cosine); as=asymmetric similarity (lexical en- tailment); WI=weak island; OR=object relative  clauses; b1/2=bare noun 1/2; whp1/2=wh-phrase  1/2; v=verb.", "labels": [], "entities": [{"text": "Regressions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9767329096794128}]}]}