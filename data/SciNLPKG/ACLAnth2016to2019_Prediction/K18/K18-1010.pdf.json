{"title": [{"text": "Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction", "labels": [], "entities": [{"text": "Sequence-to-Sequence Prediction", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8820720911026001}]}], "abstractContent": [{"text": "Current state-of-the-art machine translation systems are based on encoder-decoder archi-tectures, that first encode the input sequence, and then generate an output sequence based on the input encoding.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7271485775709152}]}, {"text": "Both are interfaced with an attention mechanism that recombines a fixed encoding of the source tokens based on the decoder state.", "labels": [], "entities": []}, {"text": "We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences.", "labels": [], "entities": []}, {"text": "Each layer of our network re-codes source tokens on the basis of the output sequence produced so far.", "labels": [], "entities": []}, {"text": "Attention-like properties are therefore pervasive throughout the network.", "labels": [], "entities": []}, {"text": "Our model yields excellent results , outperforming state-of-the-art encoder-decoder systems, while being conceptually simpler and having fewer parameters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks have made a profound impact on natural language processing technology in general, and machine translation in particular.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6511672039826711}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8128640353679657}]}, {"text": "Machine translation (MT) can be seen as a sequenceto-sequence prediction problem, where the source and target sequences are of different and variable length.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9215529084205627}, {"text": "sequenceto-sequence prediction", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7048558443784714}]}, {"text": "Current state-of-the-art approaches are based on encoder-decoder architectures).", "labels": [], "entities": []}, {"text": "The encoder \"reads\" the variable-length source sequence and maps it into a vector representation.", "labels": [], "entities": []}, {"text": "The decoder takes this vector as input and \"writes\" the target sequence, updating its state each step with the most recent word that it generated.", "labels": [], "entities": []}, {"text": "The basic encoder-decoder model is generally equipped with an attention model (, which repetitively re-accesses the source sequence during the decoding process.", "labels": [], "entities": []}, {"text": "Given the current state of the decoder, a probability distribution over the elements in the source sequence is computed, which is then used to selector aggregate features of these elements into a single \"context\" vector that is used by the decoder.", "labels": [], "entities": []}, {"text": "Rather than relying on the global representation of the source sequence, the attention mechanism allows the decoder to \"look back\" into the source sequence and focus on salient positions.", "labels": [], "entities": []}, {"text": "Besides this inductive bias, the attention mechanism bypasses the problem of vanishing gradients that most recurrent architectures encounter.", "labels": [], "entities": []}, {"text": "However, the current attention mechanisms have limited modeling abilities and are generally a simple weighted sum of the source representations (, where the weights are the result of a shallow matching between source and target elements.", "labels": [], "entities": []}, {"text": "The attention module re-combines the same source token codes and is unable to re-encode or re-interpret the source sequence while decoding.", "labels": [], "entities": []}, {"text": "To address these limitations, we propose an alternative neural MT architecture, based on deep 2D convolutional neural networks (CNNs).", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9054097533226013}]}, {"text": "The product space of the positions in source and target sequences defines the 2D grid over which the network is defined.", "labels": [], "entities": []}, {"text": "The convolutional filters are masked to prohibit accessing information derived from future tokens in the target sequence, obtaining an autoregressive model akin to generative models for images and audio.", "labels": [], "entities": []}, {"text": "This approach allows us to learn deep feature hierarchies based on a stack of 2D convolutional layers, and benefit from parallel computation during training.", "labels": [], "entities": []}, {"text": "Every layer of our network computes features of the the source tokens, based on the target sequence produced so far, and uses these to predict the next output token.", "labels": [], "entities": []}, {"text": "Our model therefore has attention-like capabilities by construction, that are pervasive throughout the layers of the network, Convolutional layers in our model use masked 3\u00d73 filters so that features are only computed from previous output symbols.", "labels": [], "entities": []}, {"text": "Illustration of the receptive fields after one (dark blue) and two layers (light blue), together with the masked part of the field of view of a normal 3\u00d73 filter (gray).", "labels": [], "entities": []}, {"text": "rather than using an \"add-on\" attention model.", "labels": [], "entities": []}, {"text": "We validate our model with experiments on the IWSLT 2014 German-to-English (De-En) and English-to-German(En-De) tasks.", "labels": [], "entities": [{"text": "IWSLT 2014 German-to-English", "start_pos": 46, "end_pos": 74, "type": "DATASET", "confidence": 0.83411173025767}]}, {"text": "We improve on state-of-the-art encoder-decoder models with attention, while being conceptually simpler and having fewer parameters.", "labels": [], "entities": []}, {"text": "In the next section we will discuss related work, before presenting our approach in detail in Section 3.", "labels": [], "entities": []}, {"text": "We present our experimental evaluation results in Section 4, and conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experimental setup, followed by quantitative results, qualitative examples of implicit sentence alignments from our model, and a comparison to the state of the art.", "labels": [], "entities": [{"text": "sentence alignments", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7386972308158875}]}, {"text": "We experiment with the IWSLT 2014 bilingual dataset (), which contains transcripts of TED talks aligned at sentence level, and translate between German (De) and English (En) in both directions.", "labels": [], "entities": [{"text": "IWSLT 2014 bilingual dataset", "start_pos": 23, "end_pos": 51, "type": "DATASET", "confidence": 0.901354044675827}]}, {"text": "Following the setup of (Edunov et al., 2018), sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data.", "labels": [], "entities": []}, {"text": "There are 160+7K training sentence pairs, 7K of which are separated and used for validation/development.", "labels": [], "entities": [{"text": "validation/development", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8301424582799276}]}, {"text": "We report results on a test set of 6,578 pairs obtained by concatenating dev2010 and tst2010-2013.", "labels": [], "entities": []}, {"text": "We tokenized and lowercased all data using the standard scripts from the Moses toolkit ().", "labels": [], "entities": []}, {"text": "For open-vocabulary translation, we segment sequences using joint byte pair encoding) with 14K merge operations on the concatenation of source and target languages.", "labels": [], "entities": [{"text": "open-vocabulary translation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7420834600925446}]}, {"text": "This results in a German and English vocabularies of around 12K and 9K types respectively.", "labels": [], "entities": []}, {"text": "Unless stated otherwise, we use DenseNets with masked convolutional filters of size 5 \u00d7 3, as given by the light blue area in.", "labels": [], "entities": []}, {"text": "To train our models, we use maximum likelihood estimation (MLE) with Adam (\u03b2 1 = 0.9, \u03b2 2 = 0.999, = 1e \u22128 ) starting with a learning rate of 5e \u22124 that we scale by a factor of 0.8 if no improvement (\u03b4 \u2264 0.01) is noticed on the validation loss after three evaluations, we evaluate every 8K updates.", "labels": [], "entities": []}, {"text": "After training all models up to 40 epochs, the best performing model on the validation set is used for decoding the test set.", "labels": [], "entities": []}, {"text": "We use a beam-search of width 5 without any length or coverage penalty and measure translation quality using the BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9725145101547241}]}, {"text": "For comparison with state-of-theart architectures, we implemented a bidirectional LSTM encoder-decoder model with dotproduct attention ( 256 (128 in each direction).", "labels": [], "entities": []}, {"text": "The decoder is a single layer LSTM with similar input size and a hidden size of 256, the target input embeddings are also used in the pre-softmax projection.", "labels": [], "entities": []}, {"text": "For regularization, we apply a dropout of rate 0.2 to the inputs of both encoder and decoder and to the output of the decoder prior to softmax.", "labels": [], "entities": [{"text": "regularization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9745810627937317}]}, {"text": "As in (, we refer to this model as RNNsearch.", "labels": [], "entities": [{"text": "RNNsearch", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.7683712840080261}]}, {"text": "The ConvS2S model we trained has embeddings of dimension 256, a 16-layers encoder and 12-layers decoder.", "labels": [], "entities": []}, {"text": "Each convolution uses 3\u00d71 filters and is followed by a gated linear unit with a total of 2 \u00d7 256 channels.", "labels": [], "entities": []}, {"text": "Residual connections link the input of a convolutional block to its output.", "labels": [], "entities": []}, {"text": "We first trained the default architecture for this dataset as suggested in, which has only 4 layers in the encoder and 3 in the decoder, but achieved better results with the deeper version described above.", "labels": [], "entities": []}, {"text": "The model is trained with MLE using Nesterov accelerated gradient with a momentum of 0.99 and an initial learning rate of 0.25 decaying by a factor of 0.1 every epoch.", "labels": [], "entities": [{"text": "MLE", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.7550395727157593}]}, {"text": "ConvS2S is also regularized with a dropout rate of 0.2.", "labels": [], "entities": []}, {"text": "For the transformer model, use the settings of (.", "labels": [], "entities": []}, {"text": "We use token embeddings of dimension 512, and the encoder and decoder have 6 layers and 8 attention heads.", "labels": [], "entities": []}, {"text": "For the inner layer in the per-position feed-forawrd network we used ff = 2048.", "labels": [], "entities": []}, {"text": "For MLE training we use Adam (\u03b2 1 = 0.9, \u03b2 2 = 0.98, = 1e \u22128 ) (, and a learning rate starting from 1e \u22125 that is increased during 4,000 warm-up steps then used a learning rate of 5e \u22124 that follows an inverse-square-root schedule afterwards (.", "labels": [], "entities": [{"text": "MLE training", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9279016554355621}]}, {"text": "Similar to previous models we set the dropout rate to 0.2.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.8995835781097412}]}, {"text": "In this section we explore the impact of several parameters of our model: the token embedding dimension, depth, growth rate and filter sizes.", "labels": [], "entities": []}, {"text": "We also evaluate different aggregation mechanisms across the source dimension: max-pooling, average-pooling, and attention.", "labels": [], "entities": []}, {"text": "In each chosen setting, we train five models with different initializations and report the mean and standard deviation of the BLEU scores.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 100, "end_pos": 118, "type": "METRIC", "confidence": 0.907053530216217}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9979864358901978}]}, {"text": "We also state the number of parameters of each model and the computational cost of training, estimated in a similar way as, based on the wall clock time of training and the GPU single precision specs.", "labels": [], "entities": []}, {"text": "In we see that using max-pooling instead average-pooling across the source dimension increases the performance with around 2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9991406202316284}]}, {"text": "Scaling the average representation with |s| Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9732217788696289}]}, {"text": "(3) helped improving the performance but it is still largely outperformed by the max-pooling.", "labels": [], "entities": []}, {"text": "Adding gated linear units on top of each convolutional layer does not improve the BLEU scores, but increases the variance due to the additional parameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9987319111824036}]}, {"text": "Stand-alone selfattention i.e. weighted average-pooling is slightly better than uniform average-pooling but it is still outperformed by max-pooling.", "labels": [], "entities": []}, {"text": "Concatenating the max-pooled features (Eq.) with the representation obtained with self-attention (Eq.", "labels": [], "entities": []}, {"text": "(9)) leads to a small but significant increase in performance, from 33.70 to 33.81.", "labels": [], "entities": [{"text": "performance", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9787771701812744}]}, {"text": "In the remainder of our experiments we only use max-pooling for simplicity, unless stated otherwise.", "labels": [], "entities": []}, {"text": "In we consider the effect of the token embedding size, the growth rate of the network, and its depth.", "labels": [], "entities": []}, {"text": "The token embedding size together with the growth rate g control the number of features that are passed though the pooling operator along the source dimension, and that can be used used for token prediction.", "labels": [], "entities": [{"text": "token prediction", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.9443998038768768}]}, {"text": "Using the same embedding size d = d t = d son both source and target, the total number of features for token prediction produced by the network is f L = 2d + gL.", "labels": [], "entities": [{"text": "token prediction", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.9203074872493744}]}, {"text": "In we see that for token embedding sizes between 128 to 256 lead to BLEU scores vary between 33.5 and 34.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9994195699691772}]}, {"text": "Smaller embedding sizes quickly degrade the performance to 32.2 for embeddings of size 64.", "labels": [], "entities": []}, {"text": "The growth rate (g) has an important impact on performance, increasing it from 8 to 32 increases the BLEU scrore by more than 2.5 point.", "labels": [], "entities": [{"text": "BLEU scrore", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9055506587028503}]}, {"text": "Beyond g = 32 performance saturates and we observe only a small improvement.", "labels": [], "entities": []}, {"text": "For a good trade-off between performance and computational cost we choose g = 32 for the remaining experiments.", "labels": [], "entities": []}, {"text": "The depth of the network also has an important impact on performance, increasing the BLEU score by about 2 points when increasing the depth from 8 to 24 layers.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9847605526447296}]}, {"text": "Beyond this point performance drops due to over-fitting, which means we should either increase the dropout rate or add another level of regularization before considering deeper networks.", "labels": [], "entities": []}, {"text": "The receptive field of our model is controlled by its depth and the filter size.", "labels": [], "entities": []}, {"text": "In Table 2, we note that narrower receptive fields are better than larger ones with less layers at equivalent complextities e.g. comparing (k = 3, L = 20) to (k = 5, L = 12), and (k = 5, L = 16) with (k = 7, L = 12).", "labels": [], "entities": []}, {"text": "Comparison to the state of the art.", "labels": [], "entities": []}, {"text": "We compare our results to the state of the art in Ta-: Performance of our model (g = 32, d s = d t = 128) for different filter sizes k and depths Land filter sizes k. ble 3 for both directions German-English (De-En) and English-German (En-De).", "labels": [], "entities": []}, {"text": "We refer to our model as Pervasive Attention . Unless stated otherwise, the parameters of all models are trained using maximum likelihood estimation (MLE).", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE", "start_pos": 119, "end_pos": 153, "type": "METRIC", "confidence": 0.7612401247024536}]}, {"text": "For some models we additionally report results obtained with sequence level estimation (SLE, e.g. using reinforcement learning approaches), typically aiming directly to optimize the BLEU measure rather than the likelihood of correct translation.", "labels": [], "entities": [{"text": "BLEU measure", "start_pos": 182, "end_pos": 194, "type": "METRIC", "confidence": 0.9762319624423981}]}, {"text": "First of all we find that all results obtained using byte-pair encodings (BPE) are superior to wordbased results.", "labels": [], "entities": []}, {"text": "Our model has about the same number of parameters as RNNsearch, yet improves performance by almost 3 BLEU points.", "labels": [], "entities": [{"text": "RNNsearch", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7771835327148438}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9993367791175842}]}, {"text": "It is also better than the recent work of  on recurrent architectures with variational attention.", "labels": [], "entities": []}, {"text": "Our model outperforms both the recent transformer approach of and the convolutional model of in both translation directions, while having about 3 to 8 times fewer parameters.", "labels": [], "entities": []}, {"text": "Our model has an equivalent training cost to the transformer (as implemented in fairseq) while the convs2s implementation is well optimized with fast running 1d-convolutions leading to shorter training times.", "labels": [], "entities": []}, {"text": "In we consider translation quality as a function of sentence length, and compare our model to RNNsearch, ConvS2S and Transformer.", "labels": [], "entities": []}, {"text": "Our model gives the best results across all sentence lengths, except for the longest ones where ConvS2S and Transformer are better.", "labels": [], "entities": []}, {"text": "Overall, our model combines the strong performance of RNNsearch on short sentences with good perfor- Implicit sentence alignments.", "labels": [], "entities": []}, {"text": "Following the method described in Section 3, we illustrate in Figure 6 the implicit sentence alignments the maxpooling operator produces in our model.", "labels": [], "entities": []}, {"text": "For reference we also show the alignment produced by our model using self-attention.", "labels": [], "entities": []}, {"text": "We see that with both max-pooling and attention qualitatively similar implicit sentence alignments emerge.", "labels": [], "entities": []}, {"text": "Notice in the first example how the max-pool model, when writing I've been working, looks at arbeite but also at seit which indicates the past tense of the former.", "labels": [], "entities": []}, {"text": "Also notice some cases of non-monotonic alignment.", "labels": [], "entities": []}, {"text": "In the first example for sometime occurs at the end of the English sentence, but seit einiger zeit appears earlier in the German source.", "labels": [], "entities": []}, {"text": "For the second example there is non-monotonic alignment around the negation at the start of the sentence.", "labels": [], "entities": []}, {"text": "The first example illustrates the ability of the model to translate proper names by breaking them down into BPE units.", "labels": [], "entities": [{"text": "translate proper names", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8512078921000162}]}, {"text": "In the second example the German word Karriereweg is broken into the four BPE units karri,er,e,weg.", "labels": [], "entities": []}, {"text": "The first and the fourth are mainly used to produce the English a carreer, while for the subsequent path the model looks at weg.", "labels": [], "entities": []}, {"text": "Finally, we can observe an interesting pattern in the alignment map for several phrases across the three examples.", "labels": [], "entities": []}, {"text": "A rough lower triangular pattern is observed for the English phrases for sometime, and it's fantastic, and it's not, a little step, and in that direction.", "labels": [], "entities": []}, {"text": "In all these cases the phrase seems to be decoded as a unit, where features are first taken across the entire corresponding source   phrase, and progressively from the part of the source phrase that remains to be decoded.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our model (L = 24, g = 32, d s = d t = 128)  with different pooling operators and using gated  convolutional units.", "labels": [], "entities": []}, {"text": " Table 2: Performance of our model (g = 32, d s =  d t = 128) for different filter sizes k and depths L  and filter sizes k.", "labels": [], "entities": []}, {"text": " Table 3: Comparison to state-of-the art results on IWSLT German-English translation. (*): results ob- tained using our implementation. (**): results obtained using FairSeq (Gehring et al., 2017b).", "labels": [], "entities": [{"text": "IWSLT German-English translation", "start_pos": 52, "end_pos": 84, "type": "DATASET", "confidence": 0.8656225403149923}, {"text": "FairSeq", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9507861137390137}]}]}