{"title": [{"text": "Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge", "labels": [], "entities": [{"text": "Regularising Neural NLI", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.8291351795196533}]}], "abstractContent": [{"text": "Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake.", "labels": [], "entities": []}, {"text": "They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation.", "labels": [], "entities": [{"text": "regularisation", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.9569281339645386}]}, {"text": "In NLP, however, most example generation strategies produce input text by using known, pre-specified semantic transformations , requiring significant manual effort and in-depth understanding of the problem and domain.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI).", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 152, "end_pos": 184, "type": "TASK", "confidence": 0.719212015469869}]}, {"text": "We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguistically-plausible examples.", "labels": [], "entities": []}, {"text": "Furthermore, we propose a method for adversarially regularising neu-ral NLI models for incorporating background knowledge.", "labels": [], "entities": []}, {"text": "Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets-up to a 79.6% relative improvement while drastically reducing the number of background knowledge violations.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8329284191131592}, {"text": "MultiNLI datasets", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.9212775230407715}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9084387421607971}]}, {"text": "Furthermore , we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to ad-versarial examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "An open problem in Artificial Intelligence is quantifying the extent to which algorithms exhibit intelligent behaviour.", "labels": [], "entities": []}, {"text": "In Machine Learning, a standard procedure consists in estimating the generalisation error, i.e. the prediction error over an independent test sample ().", "labels": [], "entities": []}, {"text": "However, machine learning models can succeed simply by recognising patterns that happen to be predictive on instances in the test sample, while ignoring deeper phenomena (.", "labels": [], "entities": []}, {"text": "Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake ().", "labels": [], "entities": []}, {"text": "In Natural Language Processing (NLP) and Machine Reading, generating adversarial examples can be really useful for understanding the shortcomings of NLP models and for regularisation (.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7658736407756805}, {"text": "regularisation", "start_pos": 168, "end_pos": 182, "type": "TASK", "confidence": 0.9638898372650146}]}, {"text": "In this paper, we focus on the problem of generating adversarial examples for Natural Language Inference (NLI) models in order to gain insights about the inner workings of such systems, and regularising them.", "labels": [], "entities": []}, {"text": "NLI, also referred to as Recognising Textual Entailment (, is a central problem in language understanding, and thus it is especially well suited to serve as a benchmark task for research in machine reading.", "labels": [], "entities": [{"text": "Recognising Textual Entailment", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.811659038066864}, {"text": "language understanding", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7537239789962769}, {"text": "machine reading", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.8071032166481018}]}, {"text": "In NLI, a model is presented with two sentences, a premise p and a hypothesis h, and the goal is to determine whether p semantically entails h.", "labels": [], "entities": []}, {"text": "The problem of acquiring large amounts of labelled data for NLI was addressed with the creation of the SNLI () and MultiNLI ( datasets.", "labels": [], "entities": [{"text": "MultiNLI ( datasets", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.7432814240455627}]}, {"text": "In these processes, annotators were presented with a premise p drawn from a corpus, and were required to generate three new sentences (hypotheses) based on p, according to the following criteria: a) Entailment -h is definitely true given p (p entails h); b) Contradiction -h is definitely not true given p (p contradicts h); and c) Neutral -h might be true given p.", "labels": [], "entities": []}, {"text": "Given a premise-hypothesis sentence pair (p, h), a NLI model is asked to classify the relationship between p and h -i.e. either entailment, contradiction, or neutral.", "labels": [], "entities": []}, {"text": "Solving NLI requires to fully capture the sentence meaning by handling complex linguistic phenomena like lexical entailment, quantification, co-reference, tense, belief, modality, and lexical and syntactic ambiguities (.", "labels": [], "entities": []}, {"text": "In this work, we use adversarial examples for: a) identifying cases where models violate existing background knowledge, expressed in the form of logic rules, and b) training models that are robust to such violations.", "labels": [], "entities": []}, {"text": "The underlying idea in our work is that NLI models should adhere to a set of structural constraints that are intrinsic to the human reasoning process.", "labels": [], "entities": []}, {"text": "For instance, contradiction is inherently symmetric: if a sentence p contradicts a sentence h, then h contradicts p as well.", "labels": [], "entities": []}, {"text": "Similarly, entailment is both reflexive and transitive.", "labels": [], "entities": []}, {"text": "It is reflexive since a sentence a is always entailed by (i.e. is true given) a.", "labels": [], "entities": []}, {"text": "It is also transitive, since if a is entailed by b, and b is entailed by c, then a is entailed by c as well.", "labels": [], "entities": []}, {"text": "Example 1 (Inconsistency).", "labels": [], "entities": []}, {"text": "Consider three sentences a, band c each describing a situation, such as: a) \"The girl plays\", b) \"The girl plays with a ball\", and c) \"The girl plays with a red ball\".", "labels": [], "entities": []}, {"text": "Note that if a is entailed by b, and b is entailed by c, then also a is entailed by c.", "labels": [], "entities": []}, {"text": "If a NLI model detects that b entails a, c entails b, but c does not entail a, we know that it is making an error (since its results are inconsistent), even though we may not be aware of the sentences a, b, and c and the true semantic relationships holding between them.", "labels": [], "entities": []}, {"text": "Our adversarial examples are different from those used in other fields such as computer vision, where they typically consist in small, semantically invariant perturbations that result in drastic changes in the model predictions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for generating adversarial examples that cause a model to violate preexisting background knowledge (Section 4), based on reducing the generation problem to a combinatorial optimisation problem.", "labels": [], "entities": []}, {"text": "Furthermore, we outline a method for incorporating such background knowledge into models by means of an adversarial training procedure (Section 5).", "labels": [], "entities": []}, {"text": "Our results show that, even though the proposed adversarial training procedure does not sensibly improve accuracy on SNLI and MultiNLI, it yields significant relative improvement inaccuracy (up to 79.6%) on adversarial datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9990405440330505}, {"text": "SNLI", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.8052085638046265}, {"text": "MultiNLI", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.8988953232765198}]}, {"text": "Furthermore, we show that adversarial examples transfer across models, and that the proposed method allows training significantly more robust NLI models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We crafted a series of datasets for assessing the robustness of the proposed regularisation method to adversarial examples.", "labels": [], "entities": [{"text": "regularisation", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9744787216186523}]}, {"text": "Starting from the SNLI test set, we proceeded as follows.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.9299826820691427}]}, {"text": "We selected the k instances in the SNLI test set that maximise the inconsistency loss in Eq.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8592302203178406}, {"text": "Eq", "start_pos": 89, "end_pos": 91, "type": "DATASET", "confidence": 0.8411418795585632}]}, {"text": "(4) with respect to the rules in R 1 , R 2 , R 3 , and R 4 in.", "labels": [], "entities": []}, {"text": "We refer to the generated datasets as A km , where m identifies the model used for selecting the sentence pairs, and k denotes number of examples in the dataset.", "labels": [], "entities": []}, {"text": "For generating each of the A km datasets, we proceeded as follows.", "labels": [], "entities": [{"text": "A km datasets", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8969953854878744}]}, {"text": "Let D = {(x 1 , y i ), . .", "labels": [], "entities": []}, {"text": ", (x n , y n )} be a NLI dataset (such as SNLI), where each instance xi = (p i , hi ) is a premise-hypothesis sentence pair, and y i denotes the relationship holding between pi and hi . For each instance xi = (p i , hi ), we consider two substitution sets: Si = {X 1 \u2192 pi , X 2 \u2192 hi } and Si = {X 1 \u2192 hi , X 2 \u2192 pi }, each corresponding to a mapping from variables to sentences.", "labels": [], "entities": []}, {"text": "We compute the inconsistency score associated to each instance xi in the dataset D as J I (S i ) + J I (S i ).", "labels": [], "entities": []}, {"text": "Note that the inconsistency score only depends on the premise pi and hypothesis hi in each instance xi , and it does not depend on its label y i . After computing the inconsistency scores for all sentence pairs in D using a model m, we select the k instances with the highest inconsistency score, we create two instances xi = (p i , hi ) and\u02c6xand\u02c6 and\u02c6x i = (h i , pi ), and add both (x i , y i ) and (\u02c6 xi , \u02c6 y i ) to the dataset A km . Note that, while y i is already known from the dataset D, \u02c6 y i is unknown.", "labels": [], "entities": []}, {"text": "For this reason, we find\u02c6yfind\u02c6 find\u02c6y i by manual annotation.", "labels": [], "entities": []}, {"text": "We trained DAM, ESIM and cBiLSTM on the SNLI corpus using the hyperparameters provided in the respective papers.", "labels": [], "entities": [{"text": "DAM", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7668784260749817}, {"text": "SNLI corpus", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.8798794746398926}]}, {"text": "The results provided by such models on the SNLI and MultiNLI validation and tests sets are provided in.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8224833607673645}, {"text": "MultiNLI validation and tests sets", "start_pos": 52, "end_pos": 86, "type": "DATASET", "confidence": 0.8994207739830017}]}, {"text": "In the case of MultiNLI, the validation set was obtained by removing 10,000 instances from the training set (originally composed by 392,702 instances), and the test set consists in the matched validation set.", "labels": [], "entities": []}, {"text": "As a first experiment, we count the how likely our model is to violate rules R 1 , R 2 , R 3 , R 4 in.", "labels": [], "entities": []}, {"text": "In we report the number sentence pairs in the SNLI training set where DAM, ESIM and cBiLSTM violate R 1 , R 2 , R 3 , R 4 . In the |B| column we report the number of times the body  of the rule holds, according to the model.", "labels": [], "entities": [{"text": "SNLI training set", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.7273004055023193}, {"text": "ESIM", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.7818014621734619}]}, {"text": "In the |B \u2227 \u00acH| column we report the number of times where the body of the rule holds, but the head does not -which is clearly a violation of available rules.", "labels": [], "entities": []}, {"text": "We can see that, in the case of rule R 1 (reflexivity of entailment), DAM and ESIM make a relatively low number of violations -namely 0.09 and 1.00 %, respectively.", "labels": [], "entities": [{"text": "DAM", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.956287145614624}, {"text": "ESIM", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9605125188827515}]}, {"text": "However, in the case of cBiLSTM, we can see that, each sentence s \u2208 S in the SNLI training set, with a 23.76 % chance, s does not entail itself -which violates our background knowledge.", "labels": [], "entities": [{"text": "SNLI training set", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.6731993754704794}]}, {"text": "With respect to R 2 (symmetry of contradiction), we see that none of the models is completely consistent with the available background knowledge.", "labels": [], "entities": []}, {"text": "Given a sentence pair s 1 , s 2 \u2208 S from the SNLI training set, if -according to the model -s 1 contradicts s 2 , a significant number of times (between 9.84% and 46.17%) the same model also infers that s 2 does not contradict s 1 . This phenomenon happens 16.70 % of times with DAM, 9.84 % of times with ESIM, and 46.17 % with cBiLSTM: this indicates that all considered models are prone to violating R 2 in their predictions, with ESIM being the more robust.", "labels": [], "entities": [{"text": "SNLI training set", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.704816202322642}]}, {"text": "In Appendix A.2 we report several examples of such violations in the SNLI training set.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.8134162724018097}, {"text": "SNLI training set", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8237674832344055}]}, {"text": "We select those that maximise the inconsistency loss described in Eq.", "labels": [], "entities": []}, {"text": "(4), violating rules R 2 and R 3 . We can notice that the presence of inconsistencies is often correlated with the length of the sentences.", "labels": [], "entities": []}, {"text": "The model tends to detect entailment relationships between longer (i.e., possibly more specific) and shorter (i.e., possibly more general) sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy on the SNLI and MultiNLI  datasets with different neural NLI models before  (left) and after (right) adversarial regularisation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9810469746589661}, {"text": "MultiNLI  datasets", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.9360083341598511}]}, {"text": " Table 4: Violations (%) of rules R 1 , R 2 , R 3 , R 4  from Table 1 on the SNLI training set, yield by  cBiLSTM, DAM, and ESIM.", "labels": [], "entities": [{"text": "SNLI training set", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.7646322051684061}]}, {"text": " Table 5: Accuracy of unregularised and regularised neural NLI models DAM, cBiLSTM, and ESIM, and  their adversarially regularised versions DAM AR , cBiLSTM AR , and ESIM AR , on adversarial datasets  A k  m .", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9599175453186035}]}]}