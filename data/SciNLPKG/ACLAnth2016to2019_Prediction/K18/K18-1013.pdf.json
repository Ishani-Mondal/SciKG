{"title": [{"text": "Learning to Embed Semantic Correspondence for Natural Language Understanding", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6866336266199747}]}], "abstractContent": [{"text": "While learning embedding models has yielded fruitful results in several NLP subfields, most notably Word2Vec, embedding correspondence has relatively not been well explored especially in the context of natural language understanding (NLU), a task that typically extracts structured semantic knowledge from a text.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9410893321037292}, {"text": "natural language understanding (NLU)", "start_pos": 202, "end_pos": 238, "type": "TASK", "confidence": 0.8208073278268179}]}, {"text": "A NLU embedding model can facilitate analyzing and understanding relationships between unstructured texts and their corresponding structured semantic knowledge, essential for both researchers and practitioners of NLU.", "labels": [], "entities": []}, {"text": "Toward this end, we propose a framework that learns to embed semantic correspondence between text and its extracted semantic knowledge , called semantic frame.", "labels": [], "entities": []}, {"text": "One key contributed technique is semantic frame reconstruction used to derive a one-to-one mapping between embedded vectors and their corresponding semantic frames.", "labels": [], "entities": [{"text": "semantic frame reconstruction", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.725387454032898}]}, {"text": "Embedding into semantically meaningful vectors and computing their distances in vector space provide a simple, but effective way to measure semantic similarities.", "labels": [], "entities": []}, {"text": "With the proposed framework, we demonstrate three key areas where the embedding model can be effective: visualization, semantic search and re-ranking.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of NLU is to extract meaning from a natural language and infer the user intention.", "labels": [], "entities": []}, {"text": "NLU typically involves two tasks: identifying user intent and extracting domain-specific entities, the second of which is often referred to as slot-filling (.", "labels": [], "entities": []}, {"text": "Typically, the NLU task can be viewed as an extraction of structured text from a raw text.", "labels": [], "entities": []}, {"text": "In NLU literature, the structured form of intent and filled slots is called a semantic frame.: Semantic vector learning framework and applications.", "labels": [], "entities": []}, {"text": "We assume a pair of corresponding text and semantic frame (t, s), which has semantically the same meaning in a raw text domain (\u03c7 T ), and a semantic frame domain (\u03c7 S ) can be encoded to a vector v in a shared embedding vector space Z.", "labels": [], "entities": []}, {"text": "R T and R S are two reader functions that encode raw and structured text to a semantic vector.", "labels": [], "entities": []}, {"text": "Wis a writing function that decodes a semantic vector to a symbolic semantic frame.", "labels": [], "entities": []}, {"text": "In this study, we aim to learn the meaningful distributed semantic representation, rather than focusing on building the NLU system itself.", "labels": [], "entities": []}, {"text": "Once we obtained a reliable and reasonable semantic representation in a vector form, we can devise many useful and new applications around the NLU.", "labels": [], "entities": []}, {"text": "Because all the instances of text and semantic frame are placed on a single vector space, we can obtain the natural and direct distance measure between them.", "labels": [], "entities": []}, {"text": "Using the distance measure, the similar text or semantic frame instances can be searched directly and interchangeably by the distance comparison.", "labels": [], "entities": []}, {"text": "Moreover, re-ranking of multiple NLU results can be applied without further learning by comparing the distances between the text and the corresponding predicted semantic frame.", "labels": [], "entities": []}, {"text": "Converting symbols to vectors makes it possible to do visualization naturally as well.", "labels": [], "entities": []}, {"text": "In this study, we assumed that the reasonable semantic vector representation satisfies the following properties.", "labels": [], "entities": []}, {"text": "\u2022 Property -embedding correspondence: Distributed representation of text should be the same as the distributed representation of the corresponding semantic frame.", "labels": [], "entities": []}, {"text": "\u2022 Property -reconstruction: Symbolic semantic frame should be recovered from the learned semantic vector.", "labels": [], "entities": [{"text": "Property -reconstruction", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6463218530019125}]}, {"text": "We herein introduce a novel semantic vector learning framework called ESC (Embedding Semantic Correspondence learning), which satisfies the assumed properties.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: Section 2 describes the detailed structure of the framework.", "labels": [], "entities": []}, {"text": "Section 3 introduces semantic vector applications in NLU.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental settings and results.", "labels": [], "entities": []}, {"text": "Section 5 discusses the related work.", "labels": [], "entities": []}, {"text": "Finally, section 6 presents the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training and testing purposes, we used the ATIS2 dataset.", "labels": [], "entities": [{"text": "ATIS2 dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9707716703414917}]}, {"text": "The ATIS2 dataset consists of an annotated intent and slot corpus for an air travel information search task.", "labels": [], "entities": [{"text": "ATIS2 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9479026198387146}, {"text": "air travel information search task", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.6412538409233093}]}, {"text": "ATIS2 data set comes with a commonly used training and test split.", "labels": [], "entities": [{"text": "ATIS2 data set", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9812650283177694}]}, {"text": "For tuning parameters, we further split the training set into 90% training and 10% development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Same sentence pattern (intent and slot- tags should be matched) search performance (Pre- cision @K). SF, I, S and J stand for semantic  frame, intent, slot-tag and joint of intent and slot- tag.", "labels": [], "entities": [{"text": "SF", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.8725586533546448}]}, {"text": " Table 5: NLU performance of multiple NLU sys- tems and re-ranked results. Acc., prec., rec., and  f m stand for accuracy, precision, recall, and f- measure, respectively.", "labels": [], "entities": [{"text": "Acc.", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.998551070690155}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993541836738586}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9974328875541687}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9975559711456299}, {"text": "f- measure", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9516191482543945}]}]}