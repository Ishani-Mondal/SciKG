{"title": [], "abstractContent": [{"text": "In this paper we describe the system submitted by UHH to the CoNLL-SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "UHH", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.923088788986206}, {"text": "CoNLL-SIGMORPHON 2018 Shared Task", "start_pos": 61, "end_pos": 94, "type": "DATASET", "confidence": 0.7746111899614334}]}, {"text": "We propose a neural architecture based on the concepts of UZH (Makarov et al., 2017), adding new ideas and techniques to their key concept and evaluating different combinations of parameters.", "labels": [], "entities": []}, {"text": "The resulting system is a language-agnostic network model that aims to reduce the number of learned edit operations by introducing equivalence classes over graphical features of individual characters.", "labels": [], "entities": []}, {"text": "We try to pinpoint advantages and drawbacks of this approach by comparing different network configurations and evaluating our results over a wide range of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The system described in this paper 1 was submitted for the, part 1 only.", "labels": [], "entities": []}, {"text": "This assignment challenges the participants to design systems that generate inflected forms based on an input lemma and feature set as shown in.", "labels": [], "entities": []}, {"text": "Training data is usually provided in three different volumes (see), all conforming to the UniMorph standard proposed by.", "labels": [], "entities": []}, {"text": "The entire data set comprises 103 languages, although not every training volume is available for every language.", "labels": [], "entities": []}, {"text": "In addition, some languages have significantly less training samples than the maximum depicted in.", "labels": [], "entities": []}, {"text": "With such a high count of diverse languages, our system is not tailored towards specific linguistic * These authors contributed equally Source code available at https://gitlab.com/ nats/sigmorphon18 bungas N;INST;PL \u21d3 bung\u00af am: Maximum training data volumes features of a language, but instead learns transitionbased character actions to transform a lemma into its inflected form.", "labels": [], "entities": [{"text": "INST", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9174216389656067}]}, {"text": "We try to limit the number of output actions that our network has to learn by grouping certain characters into common groups based on graphical features like accents or symbol modifiers.", "labels": [], "entities": []}, {"text": "Lastly, we propose a method to enhance the training data of the low setting without the use of external resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "While we used the same architecture for all languages and training set sizes, we performed individual hyperparameter optimization for each languagesize-pair.", "labels": [], "entities": []}, {"text": "The parameters tested are the hidden size of encoder/decoder, size of the character embeddings, whether to use patches or not and what amount of additional training data to hallucinate with the enhancer (1\u00d7, 5\u00d7).", "labels": [], "entities": []}, {"text": "During the development we noticed that the results are strongly influenced by the random initialization of the network weights.", "labels": [], "entities": []}, {"text": "We therefore tested every parameter combination with five different random seeds to mitigate this issue.", "labels": [], "entities": []}, {"text": "Our final evaluation on the test set used the best parameters we found during the hyperparameter search on the development set for each language-size-pair.", "labels": [], "entities": []}, {"text": "Furthermore, we observed our model sometimes fails to output EOW and instead either tries to copy non-existent lemma characters or endlessly EMITs the same character.", "labels": [], "entities": [{"text": "EOW", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7179197072982788}, {"text": "EMITs", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.7195947170257568}]}, {"text": "The string transducer includes fixes for these issues when the pointer has moved beyond the input lemma.", "labels": [], "entities": []}, {"text": "In this case COPY and PATCH do not modify the output sequence at all and EMIT actions cannot append the previously written character again.", "labels": [], "entities": [{"text": "COPY", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8054865002632141}, {"text": "PATCH", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.963359534740448}, {"text": "EMIT", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.781190812587738}]}, {"text": "However, this results in a few missing characters at the end of inflected forms in some corner cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Maximum training data volumes", "labels": [], "entities": []}, {"text": " Table 5: Excerpt from the language model for  swedish (low volume)", "labels": [], "entities": []}, {"text": " Table 7: Results for our system compared to the  baseline. Languages with the best and worst accu- racies and languages that were the furthest above  and below the baseline, trained on the medium set  and evaluated on the test set.", "labels": [], "entities": []}]}