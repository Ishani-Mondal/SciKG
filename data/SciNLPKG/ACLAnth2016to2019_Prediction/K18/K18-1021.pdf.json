{"title": [{"text": "Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction", "labels": [], "entities": [{"text": "Generalizing Procrustes Analysis", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8688610394795736}, {"text": "Bilingual Dictionary Induction", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.5923271775245667}]}], "abstractContent": [{"text": "Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages.", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6624155441919962}]}, {"text": "We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of ex-pressivity, makes it easier to learn approximate alignments.", "labels": [], "entities": []}, {"text": "Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.", "labels": [], "entities": [{"text": "alignment process", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.8854791522026062}]}], "introductionContent": [{"text": "Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI), the task of identifying translational equivalents across two languages.", "labels": [], "entities": [{"text": "bilingual dictionary induction (BDI)", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.7754110842943192}]}, {"text": "These approaches cast BDI as a problem of aligning monolingual word embeddings.", "labels": [], "entities": [{"text": "BDI", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8952763676643372}]}, {"text": "Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see).", "labels": [], "entities": []}, {"text": "Alternatively, weak supervision can be provided in the form of numerals ( or identically spelled words (.", "labels": [], "entities": []}, {"text": "Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training, present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7708045840263367}, {"text": "Procrustes Analysis", "start_pos": 401, "end_pos": 420, "type": "TASK", "confidence": 0.7570498585700989}]}, {"text": "show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.", "labels": [], "entities": []}, {"text": "We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here.", "labels": [], "entities": [{"text": "Procrustes Analysis (PA)", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.6003387987613678}]}, {"text": "The implementation of PA in yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space.", "labels": [], "entities": [{"text": "PA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8691337704658508}]}, {"text": "Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (.", "labels": [], "entities": [{"text": "supervised alignment of word vector spaces", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.7809167007605234}]}, {"text": "The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, an idea that receives support from cognitive science ().", "labels": [], "entities": []}, {"text": "Word vector spaces are not perfectly isomorphic, however, as shown by, who use a Laplacian graph similarity metric to measure this property.", "labels": [], "entities": []}, {"text": "In this work, we show that projecting both source and target vector spaces into a third space), using a variant of PA known as Generalized Procrustes Analysis, makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in.", "labels": [], "entities": []}, {"text": "Contributions We show that Generalized Procrustes Analysis (GPA), a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset () from a P@1 score of 66.2% to 67.6%.", "labels": [], "entities": [{"text": "Generalized Procrustes Analysis (GPA)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7511237114667892}, {"text": "PA", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9849048852920532}, {"text": "EnglishItalian dataset", "start_pos": 214, "end_pos": 236, "type": "DATASET", "confidence": 0.9469226598739624}, {"text": "P@1 score", "start_pos": 247, "end_pos": 256, "type": "METRIC", "confidence": 0.9474970400333405}]}, {"text": "We compare GPA to PA on aligning English with five languages representing different language families, showing that GPA consistently outperforms PA.", "labels": [], "entities": [{"text": "PA", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9939449429512024}, {"text": "GPA", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9944342970848083}]}, {"text": "GPA also allows for the use of additional support languages, aligning three or more languages at a time, which can boost performance even further.", "labels": [], "entities": [{"text": "GPA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.922471821308136}]}, {"text": "We present experiments with multi-source GPA on an additional five lowresource languages from the same language families (Hebrew, Afrikaans, Occitan, Estonian, and Bosnian), using their bigger counterpart as a support language.", "labels": [], "entities": []}, {"text": "Our code is publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we generally use the same hyper-parameters as used in, unless otherwise stated.", "labels": [], "entities": []}, {"text": "When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000.", "labels": [], "entities": []}, {"text": "We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish.", "labels": [], "entities": []}, {"text": "Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in, i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.", "labels": [], "entities": [{"text": "PA", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9954967498779297}, {"text": "GPA", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9579306244850159}]}, {"text": "Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set (.", "labels": [], "entities": [{"text": "Precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.999275267124176}]}, {"text": "Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by 5 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Bilingual dictionary induction performance, measured in P@k, of PA and GPA across five language pairs.", "labels": [], "entities": [{"text": "Bilingual dictionary induction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7779705127080282}, {"text": "PA", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.996712327003479}]}, {"text": " Table 3: Results on standard benchmarks, measured in P@1. * Results as reported in the original paper. Notes:  Conneau et al. (2018) report 63.7 on Italian with Wikipedia embeddings; results with different embedding sets  are not comparable due to a non-zero out-of-vocabulary rate on the test set for Wikipedia embeddings; Wikipedia  embeddings are trained on corpora with removed numerals, so supervision from numerals cannot be applied.", "labels": [], "entities": []}, {"text": " Table 4: Results for low-resource languages with PA, GPA and two multi-support settings.", "labels": [], "entities": [{"text": "PA", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9847283363342285}]}]}