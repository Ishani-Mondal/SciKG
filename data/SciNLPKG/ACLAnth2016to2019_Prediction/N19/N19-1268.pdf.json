{"title": [{"text": "Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout", "labels": [], "entities": []}], "abstractContent": [{"text": "A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment.", "labels": [], "entities": []}, {"text": "One key challenge here is to learn to navigate in new environments that are unseen during training.", "labels": [], "entities": []}, {"text": "Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones.", "labels": [], "entities": []}, {"text": "In this paper, we present a generalizable nav-igational agent.", "labels": [], "entities": []}, {"text": "Our agent is trained in two stages.", "labels": [], "entities": []}, {"text": "The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization.", "labels": [], "entities": []}, {"text": "The second stage is fine-tuning via newly-introduced 'unseen' triplets (envi-ronment, path, instruction).", "labels": [], "entities": []}, {"text": "To generate these unseen triplets, we propose a simple but effective 'environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability.", "labels": [], "entities": []}, {"text": "Next, we apply semi-supervised learning (via back-translation) on these dropped-out environments to generate new paths and instructions.", "labels": [], "entities": []}, {"text": "Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the important goals in AI is to develop a robot/agent that can understand instructions from humans and perform actions in complex environments.", "labels": [], "entities": []}, {"text": "In order to do so, such a robot is required to perceive the surrounding scene, understand our spoken language, and act in a real-world: Room-to-Room Task.", "labels": [], "entities": []}, {"text": "The agent is given an instruction, then starts its navigation from some staring viewpoint inside the given environment.", "labels": [], "entities": []}, {"text": "At time t, the agent selects one view (highlighted as red dotted bounding boxes) from a set of its surrounding panoramic views to step into, as an action at . house.", "labels": [], "entities": []}, {"text": "Recent years have witnessed various types of embodied action based NLP tasks being proposed (.", "labels": [], "entities": []}, {"text": "In this paper, we address the task of instructionguided navigation, where the agent seeks a route from a start viewpoint to an end viewpoint based on a given natural language instruction in a given environment, as shown in.", "labels": [], "entities": [{"text": "instructionguided navigation", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.7017967849969864}]}, {"text": "The navigation simulator we use is the recent Room-to-Room (R2R) simulator), which uses real images from the Matterport3D ( indoor home environments and collects complex navigable human-spoken instructions inside the environments, hence connecting problems in vision, language, and robotics.", "labels": [], "entities": [{"text": "Matterport3D", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9501554369926453}]}, {"text": "The instruction in is \"Walk past the piano through an archway directly in front.", "labels": [], "entities": []}, {"text": "Go through the hallway when you seethe window door.", "labels": [], "entities": []}, {"text": "Turn right to the hanged pictures...\".", "labels": [], "entities": []}, {"text": "At each position (viewpoint), the agent perceives panoramic views (a set of surrounding images) and selects one of them to step into.", "labels": [], "entities": []}, {"text": "In this challenging task, the agent is required to understand each piece of the instruction and localize key views (\"piano\", \"hallway\", \"door\", etc.) for making actions at each time step.", "labels": [], "entities": []}, {"text": "Another crucial challenge is to generalize the agent's navigation understanding capability to unseen test room environments, considering that the R2R task has substantially different unseen (test) rooms as compared to seen (trained) ones.", "labels": [], "entities": [{"text": "navigation understanding", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.9004216492176056}]}, {"text": "Such generalization ability is important for developing a practical navigational robot that can operate in the wild.", "labels": [], "entities": [{"text": "navigational robot", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9022267162799835}]}, {"text": "Recent works () have shown promising progress on this R2R task, based on speakerfollower, reinforcement learning, imitation learning, cross-modal, and look-ahead models.", "labels": [], "entities": []}, {"text": "However, the primary issue in this task is that most models perform substantially worse in unseen environments than in seen ones, due to the lack of generalizability.", "labels": [], "entities": []}, {"text": "Hence, in our paper, we focus on improving the agent's generalizability in unseen environments.", "labels": [], "entities": []}, {"text": "For this, we propose a twostage training approach.", "labels": [], "entities": []}, {"text": "The first stage is training the agent via mixed imitation learning (IL) and reinforcement learning (RL) which combines off-policy and on-policy optimization; this significantly outperforms using IL or RL alone.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 76, "end_pos": 103, "type": "METRIC", "confidence": 0.8047157406806946}]}, {"text": "The second, more important stage is semisupervised learning with generalization-focused 'environmental dropout'.", "labels": [], "entities": []}, {"text": "Here, the model is finetuned using additional training data generated via back-translation.", "labels": [], "entities": []}, {"text": "This is usually done based on a neural speaker model () that synthesizes new instructions for additional routes in the existing environments.", "labels": [], "entities": []}, {"text": "However, we found that the bottleneck for this semi-supervised learning method is the limited variability of given (seen) environments.", "labels": [], "entities": []}, {"text": "Therefore, to overcome this, we propose to generate novel and diverse environments via a simple but effective 'environmental dropout' method based on view-and viewpointconsistent masking of the visual features.", "labels": [], "entities": []}, {"text": "Next, the new navigational routes are collected from these new environments, and lastly the new instructions are generated by a neural speaker on these routes, and these triplets are employed to fine-tune the model training.", "labels": [], "entities": []}, {"text": "Overall, our fine-tuned model based on backtranslation with environmental dropout substantially outperforms the previous state-of-the-art models, and achieves the most recent rank-1 on the Vision and Language Navigation (VLN) R2R challenge leaderboard's private test data, outperforming all other entries in success rate under all evaluation setups (single run, beam search, and pre-exploration).", "labels": [], "entities": [{"text": "Vision and Language Navigation (VLN) R2R challenge leaderboard's private test data", "start_pos": 189, "end_pos": 271, "type": "TASK", "confidence": 0.7182409933635167}]}, {"text": "We also present detailed ablation and analysis studies to explain the effectiveness of our generalization method.", "labels": [], "entities": [{"text": "generalization", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.971193253993988}]}], "datasetContent": [{"text": "Implementation Details Similar to the traditional dropout method, the environmental dropout mask is computed and applied at each training iteration.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9338028430938721}]}, {"text": "Thus, the amount of unlabeled semisupervised data used is not higher in our dropout method.", "labels": [], "entities": []}, {"text": "We also find that sharing the environmental dropout mask in different environments inside a batch will stabilize the training.", "labels": [], "entities": []}, {"text": "To avoid overfitting, the model is early-stopped according to the success rate on the unseen validation set.", "labels": [], "entities": []}, {"text": "More training details in appendices.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Leaderboard results under different experimental setups. NL, SR, and SPL are Navigation Length, Success  Rate and Success rate weighted by Path Length. The primary metric for each setup is in italics. The best results  are in bold font and the second best results are underlined.", "labels": [], "entities": [{"text": "NL", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.976705014705658}, {"text": "SR", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9045864939689636}, {"text": "Navigation Length", "start_pos": 87, "end_pos": 104, "type": "METRIC", "confidence": 0.7926912903785706}, {"text": "Success  Rate", "start_pos": 106, "end_pos": 119, "type": "METRIC", "confidence": 0.8808713853359222}, {"text": "Success rate", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.957014799118042}]}, {"text": " Table 2: For the ablation study, we show the results of our different methods on validation sets. Our full model  (single run) gets 8.6% improvement in validation unseen success rate above our baseline. And both the supervised  learning (IL+RL) and semi-supervised learning methods (back translation + env drop) have substantial contribu- tions to our final result.", "labels": [], "entities": []}]}