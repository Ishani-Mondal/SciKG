{"title": [{"text": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.", "labels": [], "entities": []}, {"text": "Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence.", "labels": [], "entities": []}, {"text": "At test time the CKY algorithm extracts the highest scoring parse.", "labels": [], "entities": []}, {"text": "DIORA achieves anew state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.", "labels": [], "entities": [{"text": "DIORA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8721491694450378}, {"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9979301691055298}, {"text": "WSJ", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8932233452796936}, {"text": "MultiNLI", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.8864351511001587}]}], "introductionContent": [{"text": "Syntactic parse trees are useful for downstream tasks such as relation extraction (, semantic role labeling, machine translation (, and text classification (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8656194508075714}, {"text": "semantic role labeling", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.6655713220437368}, {"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8256290555000305}, {"text": "text classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7824741899967194}]}, {"text": "Traditionally, supervised parsers trained on datasets such as the Penn) are used to obtain syntactic trees.", "labels": [], "entities": [{"text": "Penn)", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9711379706859589}]}, {"text": "However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain.", "labels": [], "entities": []}, {"text": "Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming.", "labels": [], "entities": []}, {"text": "Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (; Devlin et al., * Equal contribution, randomly ordered.", "labels": [], "entities": [{"text": "BERT", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9855347871780396}]}, {"text": "Under the current circumstances he says their scenario no longer seems unrealistic 2019), we propose anew deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data.", "labels": [], "entities": []}, {"text": "In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks.", "labels": [], "entities": []}, {"text": "Our model builds on existing work developing latent tree chart parsers).", "labels": [], "entities": []}, {"text": "These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting overall possible sub-trees ( \u00a72).", "labels": [], "entities": []}, {"text": "Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference.", "labels": [], "entities": []}, {"text": "To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without requiring any supervised training data.", "labels": [], "entities": []}, {"text": "DIORA incorporates the inside-outside algorithm) into a latent tree chart parser.", "labels": [], "entities": [{"text": "DIORA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8050978779792786}]}, {"text": "The bottom-up inside step calculates a representation for all possible constituents within a binary tree over the input sentence.", "labels": [], "entities": []}, {"text": "This step is equivalent to the forward-pass of previous latent tree chart parsers ().", "labels": [], "entities": []}, {"text": "These inside representations only encode the current subtree, ignor- Figure 2: The illustrated inside and outside pass of DIORA operating over an input of length three, 'the cat drinks'.", "labels": [], "entities": []}, {"text": "a) The inside pass: The inside vector \u00af a(k) for the phrase 'the cat drinks' is a weighted average of the compositions for the two possible segmentations -((the cat), drinks) and (the, (cat drinks)).", "labels": [], "entities": []}, {"text": "The scalar weights come from a learned compatibility function.", "labels": [], "entities": []}, {"text": "b) The outside pass: The outside vector \u00af b(k) for the phrase 'the cat' is a function of the outside vector of its parent 'the cat drinks' and the inside vector of its sibling 'drinks'.", "labels": [], "entities": []}, {"text": "Thus, we perform an additional top-down outside calculation for each node in the tree, providing external context into the subtree representations in each chart cell.", "labels": [], "entities": []}, {"text": "The model is then trained with the objective that the outside representations of the leaf cells should reconstruct the corresponding leaf input word, analogous to masked language model pretraining, except by using dynamic programming we predict every word from a completely unmasked context.", "labels": [], "entities": []}, {"text": "The single most likely tree can be recovered using the CKY algorithm and compatibility scores between constituents.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9603818655014038}]}, {"text": "Previous work either predict trees that are not well aligned with known treebanks (, or has no mechanism for explicitly modeling phrases, requiring a complex procedure to extract syntactic structures.", "labels": [], "entities": []}, {"text": "To probe different properties of our model, we run experiments on unsupervised parsing, segment recall, and phrase representations.", "labels": [], "entities": [{"text": "phrase representations", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7390017211437225}]}, {"text": "DIORA achieves multiple new state-of-the-art results for unsupervised constituency parsing (absolute improvements of 13.7%, 11.5%, and 7.8% on WSJ, WSJ-40, and MultiNLI), has a greater recall on more constituent types than a strong baseline, and produces meaningful phrase representations.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7039005160331726}, {"text": "WSJ", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.9784272909164429}, {"text": "WSJ-40", "start_pos": 148, "end_pos": 154, "type": "DATASET", "confidence": 0.8047530055046082}, {"text": "MultiNLI", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.903069794178009}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9989431500434875}]}], "datasetContent": [{"text": "To evaluate the effectiveness of DIORA, we run experiments on unsupervised parsing, unsuper- Initialize terminal values.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Full WSJ (test set) unsupervised unlabeled bi- nary constituency parsing including punctuation.  \u2020 in- dicates trained to optimize NLI task. Mean and max  are calculated over five random restarts. PRPN F1 was  calculated using the parse trees and results provided by  Htut et al. (2018). The depth (\u03b4) is the average tree  height. +PP refers to post-processing heuristic that at- taches trailing punctuation to the root of the tree. The  top F1 value in each column is bolded.", "labels": [], "entities": [{"text": "bi- nary constituency parsing", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.6473468124866486}, {"text": "Mean and max", "start_pos": 151, "end_pos": 163, "type": "METRIC", "confidence": 0.7526267369588217}, {"text": "PRPN F1", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.4406778961420059}]}, {"text": " Table 2: NLI unsupervised unlabeled binary con- stituency parsing comparing to CoreNLP predicted  parses. PRPN F1 was calculated using the parse trees  and results provided by Htut et al. (2018). F1 median  and max are calculated over five random seeds and the  top F1 value in each column is bolded. Note that we  use median rather than mean in order to compare with  previous work.", "labels": [], "entities": [{"text": "NLI unsupervised unlabeled binary con- stituency parsing", "start_pos": 10, "end_pos": 66, "type": "TASK", "confidence": 0.5401267558336258}, {"text": "PRPN F1", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.4891940802335739}, {"text": "F1 median", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9783273935317993}]}, {"text": " Table 3: WSJ-10 and WSJ-40 unsupervised non-binary  unlabeled constituency parsing with punctuation re- moved.  \u2020 indicates that the model predicts a full, non- binary parse with additional resources.  \u2021 indicates  model was trained on WSJ data and PRPN N LI was  trained on MultiNLI data. CCM uses predicted POS  tags while CCM gold uses gold POS tags. PRPN F1 was  calculated using the parse trees and results provided by  Htut et al. (2018). LB and RB are the left and right- branching baselines. UB is the upper bound attainable  by a model that produces binary trees.", "labels": [], "entities": [{"text": "WSJ-10", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9491199851036072}, {"text": "WSJ-40 unsupervised non-binary  unlabeled constituency parsing", "start_pos": 21, "end_pos": 83, "type": "TASK", "confidence": 0.578257292509079}, {"text": "WSJ data", "start_pos": 237, "end_pos": 245, "type": "DATASET", "confidence": 0.8666153848171234}, {"text": "MultiNLI data", "start_pos": 276, "end_pos": 289, "type": "DATASET", "confidence": 0.9234147667884827}, {"text": "UB", "start_pos": 501, "end_pos": 503, "type": "METRIC", "confidence": 0.9702246785163879}]}, {"text": " Table 4: Segment recall from WSJ separated by phrase  type. The 10 most frequent phrase types are shown  above, and the highest value in each row is bolded. P- UP=PRNP-UP, P-LM=PRPN-LM", "labels": [], "entities": [{"text": "Segment recall", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8446601629257202}, {"text": "PRPN-LM", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.7049161791801453}]}, {"text": " Table 5: P@1, P@10, and P@100 for labeled chunks from CoNLL-2000 and CoNLL 2012 datasets. For all  metrics, higher is better. The top value in each column is bolded. Diora uses the concatenation of the inside and  outside vector at each cell which performed better than either in isolation.", "labels": [], "entities": [{"text": "CoNLL-2000 and CoNLL 2012 datasets", "start_pos": 55, "end_pos": 89, "type": "DATASET", "confidence": 0.7986396789550781}]}, {"text": " Table 6: F1 for different model variants on the binary  WSJ validation set with included punctuation. The bi- nary trees are as-is (\u2205) or modified according to the  post-processing heuristic (+P P ). The mean F1 is  shown across three random seeds.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984732270240784}, {"text": "WSJ validation set", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.896062433719635}, {"text": "F1", "start_pos": 210, "end_pos": 212, "type": "METRIC", "confidence": 0.8090258836746216}]}]}