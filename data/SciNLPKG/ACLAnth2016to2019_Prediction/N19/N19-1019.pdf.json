{"title": [{"text": "How Bad are PoS Taggers in Cross-Corpora Settings? Evaluating Annotation Divergence in the UD Project", "labels": [], "entities": [{"text": "PoS Taggers", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8330647647380829}, {"text": "Cross-Corpora Settings", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.708115965127945}, {"text": "Evaluating Annotation Divergence", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8176580866177877}, {"text": "UD Project", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8363022804260254}]}], "abstractContent": [{"text": "The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8756960928440094}, {"text": "Universal Dependencies project", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.7814737558364868}]}, {"text": "This work points out that these variations may result from divergences between the annotation of train and test sets.", "labels": [], "entities": []}, {"text": "We show how the annotation variation principle, introduced by Dickinson and Meurers (2003) to automatically detect errors in gold standard, can be used to identify inconsistencies between annotations ; we also evaluate their impact on prediction performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data.", "labels": [], "entities": [{"text": "Part-of-Speech (PoS) taggers", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.63799769282341}]}, {"text": "To illustrate this claim, reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project).", "labels": [], "entities": [{"text": "error rate", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9759730696678162}, {"text": "French treebanks", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.8656370341777802}]}, {"text": "1 It shows that depending on the train and test sets considered, the performance can vary by a factor of more than 25.", "labels": [], "entities": []}, {"text": "Many studies) attribute this drop inaccuracy to covariate shift, characterizing the differences between domains by a change in the marginal distribution p(x) of the input (e.g. increase of out-of-vocabulary words, missing capitalization, different usage of punctuation, etc), while assuming that the conditional label distribution remains unaffected.", "labels": [], "entities": []}, {"text": "This work adopts a different point of view : we believe that the variation in tagging performance is due to a dataset shift), i.e. a change in the joint distribution of the features and labels.", "labels": [], "entities": []}, {"text": "We assume that this change mainly results.", "labels": [], "entities": []}, {"text": "See Section 2 for details regarding our experimental setting from incoherences in the annotations between corpora or even within the same corpus.", "labels": [], "entities": []}, {"text": "Indeed, ensuring inter-annotator agreement in PoS tagging is known to be a difficult task as annotation guidelines are not always interpreted in a consistent manner.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.889727771282196}]}, {"text": "For instance, shows that many errors in the WSJ corpus are just mistakes rather than uncertainties or difficulties in the task ; reports some of these annotation divergences that can be found in UD project.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9270535111427307}, {"text": "UD project", "start_pos": 195, "end_pos": 205, "type": "DATASET", "confidence": 0.9165380597114563}]}, {"text": "The situation is naturally worse in cross-corpora settings, in which treebanks are annotated by different laboratories or groups.", "labels": [], "entities": []}, {"text": "The contribution of this paper is threefold : -we show that, as already pointed out by, the variation principle of can be used to flag potential annotation discrepancies in the UD project.", "labels": [], "entities": [{"text": "UD project", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.6787753999233246}]}, {"text": "Building on this principle, we introduce, to evaluate the annotation consistency of a corpus, several methods and metrics that can be used, during the annotation to improve the quality of the corpus.", "labels": [], "entities": []}, {"text": "-we generalize the conclusions of, highlighting how error rates in PoS tagging are stemming from the poor quality of annotations and inconsistencies in the resources ; we also systematically quantify the impact of annotation variation on PoS tagging performance fora large number of languages and corpora.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.8735922574996948}, {"text": "PoS tagging", "start_pos": 238, "end_pos": 249, "type": "TASK", "confidence": 0.8751074969768524}]}, {"text": "-we show that the evaluation of PoS taggers in cross-corpora settings (typically in domain adaptation experiments) is hindered by systematic annotation discrepancies between the corpora and quantify the impact of this divergence on PoS tagger evaluation.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.820962518453598}, {"text": "PoS tagger evaluation", "start_pos": 232, "end_pos": 253, "type": "TASK", "confidence": 0.9235289295514425}]}, {"text": "Our observations stress the fact that comparing in-and out-domain scores as many works do (e.g. to evaluate the quality of a domain adaptation method or the measure the difficulty of the domain adaptation task) can be flawed and that this metrics has to be corrected to take into account the annotation divergences that exists between corpora.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first present the corpora and the tools used in our experiments ( \u00a7 2).", "labels": [], "entities": []}, {"text": "We then describe the annotation variation principle of Dickinson and Meurers (2003) ( \u00a7 3) and its application to the treebanks of the Universal Dependencies project ( \u00a7 4).", "labels": [], "entities": [{"text": "Dickinson and Meurers (2003)", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.935592770576477}, {"text": "Universal Dependencies project", "start_pos": 135, "end_pos": 165, "type": "DATASET", "confidence": 0.8073827028274536}]}, {"text": "We eventually assess the impact of annotation variations on prediction performance ( \u00a7 5 and \u00a7 6).", "labels": [], "entities": []}, {"text": "The code and annotations of all experiments are available on the first author website.", "labels": [], "entities": []}, {"text": "For the sake of clarity, we have only reported our observations for the English treebanks of the UD project and, sometimes, for the French treebanks (because it has seven treebanks).", "labels": [], "entities": [{"text": "English treebanks of the UD project", "start_pos": 72, "end_pos": 107, "type": "DATASET", "confidence": 0.789317270119985}, {"text": "French treebanks", "start_pos": 132, "end_pos": 148, "type": "DATASET", "confidence": 0.9596500396728516}]}, {"text": "Similar results have however been observed for other languages and corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data All experiments presented in this work use the Universal Dependencies (UD) 2.3 dataset () that aims at developing cross-linguistically consistent treebank annotations fora wide array of languages.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) 2.3 dataset", "start_pos": 52, "end_pos": 91, "type": "DATASET", "confidence": 0.5443399165357862}]}, {"text": "This version of the UD project contains 129 treebanks covering 76 languages.", "labels": [], "entities": []}, {"text": "Among those, 97 treebanks define a train set that contains between 19 sentences and 68,495 sentences and a test set that contains between 34 and 10,148 sentences.", "labels": [], "entities": []}, {"text": "For 21 languages, several test sets are available : there are, for instance, 7 test sets for French,.", "labels": [], "entities": []}, {"text": "https://perso.limsi.fr/wisniews/ recherche/#coherence 6 for English, 5 for Czech and 4 for Swedish, Chinese, Japanese, Russian and Italian.", "labels": [], "entities": []}, {"text": "Overall, it is possible to train and test 290 taggers (i.e. there are 290 possible combinations of a train and a test set of the same language), 191 of these conditions (i.e. pairs of a train set and a test set) correspond to a cross-corpus setting and can be considered for domain adaptation experiments.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 275, "end_pos": 292, "type": "TASK", "confidence": 0.7092234045267105}]}, {"text": "Many of these corpora 3 result from an automatic transformation (with, for some of them, manual corrections) from existing dependency or constituent treebanks (.", "labels": [], "entities": []}, {"text": "Because most treebanks have been annotated and/or converted independently by different groups, 4 the risk of inconsistencies and errors in the application of annotation guidelines is increased.", "labels": [], "entities": []}, {"text": "There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation.", "labels": [], "entities": []}, {"text": "Actually, several studies have recently pointed out that treebanks for the same language are not consistently annotated.", "labels": [], "entities": []}, {"text": "Ina closely related context, have also shown that, in spite of common annotation guidelines, one of the main bottleneck in cross-lingual transfer between UD corpora is the difference in the annotation conventions across treebanks and languages.", "labels": [], "entities": []}, {"text": "\u008c With regard to the effect of the programme on the convergence of high level ADJ training for trainers , it was not possible to make an assessment as there was not sufficient information on the link between national strategies and the activities under Pericles . With a view to enabling the assessment of the effect of the programme , among others on the convergence of high level NOUN training for trainers , the evaluator recommends the preparation of a strategy document , to be finalised before the new Pericles enters into effect . \u008d Notice NOUN Regarding Privacy and Confidentiality : PaineWebber reserves the right to monitor and review the content of all e-mail communications sent and or received by its employees . Notice PROPN Regarding Privacy and Confidentiality : PaineWebber reserves the right to monitor and review the content of all e-mail communications sent and or received by its employees . \u008e The above applies to the Work as incorporated in a Collective Work , but this does not require the Collective Work apart from the Work itself to be made subject ADJ to the terms of this License.", "labels": [], "entities": [{"text": "PaineWebber", "start_pos": 592, "end_pos": 603, "type": "DATASET", "confidence": 0.9293740391731262}]}, {"text": "The above applies to the Derivative Work as incorporated in a Collective Work , but this does not require the Collective Work apart from the Derivative Work itself to be made subject NOUN to the terms of this License .: Examples of annotation divergences in the English Web Treebank (EWT) corpus : these sentences share some common words (in bold) that do not have the same annotation.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.944625198841095}, {"text": "English Web Treebank (EWT) corpus", "start_pos": 262, "end_pos": 295, "type": "DATASET", "confidence": 0.8875367556299482}]}, {"text": "Only the labels that differ are represented.", "labels": [], "entities": []}, {"text": "PoS tagger In all our experiments, we use a history-based model) with a LaSO-like training method).", "labels": [], "entities": []}, {"text": "This model reduces PoS tagging to a sequence of multi-class classification problems : the PoS of the words in the sentence are predicted one after the other using an averaged perceptron.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9541338980197906}, {"text": "PoS", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9501964449882507}]}, {"text": "We consider the standard feature set for PoS tagging (Zhang and Nivre, 2011) : current word, two previous and following words, the previous two predicted labels, etc.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.8968139588832855}]}, {"text": "This 'standard' feature set has been designed for English and has not been adapted to the other languages considered in our experiments.", "labels": [], "entities": []}, {"text": "Our PoS tagger achieves an average precision of 91.10% overall UD treebanks, a result comparable to the performance of UDPipe 1.2 (, the baseline of CoNLL'17 Shared Task 'Multilingual Parsing from Raw Text to Universal Dependencies' that achieves an average precision of 91.22%.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9956889748573303}, {"text": "UD treebanks", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.6891874372959137}, {"text": "CoNLL'17 Shared Task 'Multilingual Parsing from Raw Text to Universal Dependencies'", "start_pos": 149, "end_pos": 232, "type": "TASK", "confidence": 0.6413798009355863}, {"text": "precision", "start_pos": 258, "end_pos": 267, "type": "METRIC", "confidence": 0.9767732620239258}]}, {"text": "When not otherwise specified, all PoS tagging scores reported below are averaged over 10 runs (i.e. independent training of a model and evaluation of the test performance).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8447808623313904}]}], "tableCaptions": [{"text": " Table 1: Error rate (%) achieved by a PoS tagger trained and tested on all possible combinations of the French train  and test sets of the UD project. To mitigate the variability of our learning algorithm, all scores are averaged over  10 training sessions.", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9918579161167145}, {"text": "French train  and test sets of the UD project", "start_pos": 105, "end_pos": 150, "type": "DATASET", "confidence": 0.8656984368960062}]}, {"text": " Table 4: Percentage of suspicious repeats between the  EWT and PUD corpora that contain an annotation  inconsistency according to a human annotator either  when the disjoint heuristic is used or when only sus- picious repeats with at least n words are considered.", "labels": [], "entities": [{"text": "EWT and PUD corpora", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.7488501965999603}]}, {"text": " Table 5: Percentage of sentences with a repeat of at  least three words in the English treebanks (% sent.  repeat) and percentage of these repeats that are not  labeled consistently (% var.).", "labels": [], "entities": [{"text": "English treebanks", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9035610258579254}]}, {"text": " Table 6: Percentage of repeats between a train and a test sets that are not annotated consistently. In-domain settings  (i.e. when the train and test sets come from the same treebank) are reported in bold ; for each train set, the most  consistent setting is underlined.", "labels": [], "entities": []}, {"text": " Table 8: Number of repeated sequence of words across  the different combinations of a train set and a test set  ('repeats' column) and number of these sequences that  are annotated differently ('suspicious repeats' column)  when no filtering is applied.", "labels": [], "entities": []}]}