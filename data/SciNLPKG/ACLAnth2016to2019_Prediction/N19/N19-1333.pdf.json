{"title": [{"text": "Corpora Generation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Corpora Generation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7906420230865479}, {"text": "Grammatical Error Correction", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7497463623682658}]}], "abstractContent": [{"text": "Grammatical Error Correction (GEC) has been recently modeled using the sequence-to-sequence framework.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8616176644961039}]}, {"text": "However, unlike sequence transduction problems such as machine translation, GEC suffers from the lack of plentiful parallel data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7504397928714752}, {"text": "GEC", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.6383724808692932}]}, {"text": "We describe two approaches for generating large parallel datasets for GEC using publicly available Wikipedia data.", "labels": [], "entities": []}, {"text": "The first method extracts source-target pairs from Wikipedia edit histories with minimal filtration heuristics, while the second method introduces noise into Wikipedia sentences via round-trip translation through bridge languages.", "labels": [], "entities": []}, {"text": "Both strategies yield similar sized parallel corpora containing around 4B tokens.", "labels": [], "entities": []}, {"text": "We employ an iterative decoding strategy that is tailored to the loosely supervised nature of our constructed corpora.", "labels": [], "entities": []}, {"text": "We demonstrate that neural GEC models trained using either type of corpora give similar performance.", "labels": [], "entities": []}, {"text": "Fine-tuning these models on the Lang-8 corpus and ensembling allows us to surpass the state of the art on both the CoNLL-2014 benchmark and the JFLEG task.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9287861883640289}, {"text": "CoNLL-2014 benchmark", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.9380384087562561}]}, {"text": "We provide systematic analysis that compares the two approaches to data generation and highlights the effectiveness of ensembling.", "labels": [], "entities": [{"text": "data generation", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8018478453159332}]}, {"text": "Jared conducted systematic experiments to determine useful variants of the Wikipedia revisions corpus, pre-training and fine-tuning strategies, and iterative decoding.", "labels": [], "entities": [{"text": "Wikipedia revisions corpus", "start_pos": 75, "end_pos": 101, "type": "DATASET", "confidence": 0.8484997749328613}]}, {"text": "Chris implemented the ensemble and provided background knowledge and resources related to GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.6043497920036316}]}, {"text": "Shankar ran training and decoding experiments using round-trip translated data.", "labels": [], "entities": []}, {"text": "Jared, Chris and Shankar wrote the paper.", "labels": [], "entities": []}, {"text": "Noam identified Wikipedia revisions as a source of training data.", "labels": [], "entities": []}, {"text": "Noam developed the heuristics for using the full Wikipedia revisions at scale and conducted initial experiments to train Transformer models for GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.9817743301391602}]}, {"text": "Noam and Niki provided guidance on training Transformer models using the Tensor2Tensor toolkit.", "labels": [], "entities": []}, {"text": "Simon proposed using round-trip translations as a source for training data, and corrupting them with common errors extracted from Wikipedia revisions.", "labels": [], "entities": []}, {"text": "Simon generated such data for this paper .", "labels": [], "entities": []}], "introductionContent": [{"text": "Much progress in the Grammatical Error Correction (GEC) task can be credited to approaching the problem as a translation task) from an ungrammatical source language to a grammatical target language.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC) task", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.778838745185307}]}, {"text": "This has enabled Neural Machine Translation (NMT) sequence-tosequence (S2S) models and techniques to be applied to the GEC task (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT) sequence-tosequence (S2S)", "start_pos": 17, "end_pos": 75, "type": "TASK", "confidence": 0.8392290592193603}, {"text": "GEC task", "start_pos": 119, "end_pos": 127, "type": "TASK", "confidence": 0.7462497651576996}]}, {"text": "However, the efficacy of NMT techniques is degraded for low-resource tasks.", "labels": [], "entities": []}, {"text": "This poses difficulties for S2S approaches to GEC, as Lang-8, the largest publicly available parallel corpus, contains only \u223c25M words ().", "labels": [], "entities": [{"text": "GEC", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8497495055198669}]}, {"text": "Motivated by this data scarcity, we present two contrasting approaches to generating parallel data for GEC that make use of publicly available English language Wikipedia revision histories . Our first strategy is to mine real-world errors.", "labels": [], "entities": [{"text": "GEC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8639861941337585}]}, {"text": "We attempt to accumulate source-target pairs from grammatical errors and their human-curated corrections gleaned from the Wikipedia revision histories.", "labels": [], "entities": [{"text": "Wikipedia revision histories", "start_pos": 122, "end_pos": 150, "type": "DATASET", "confidence": 0.8564244310061137}]}, {"text": "Unlike previous work), we apply minimal filtering so as to generate a large and noisy corpus of \u223c4B tokens (.", "labels": [], "entities": []}, {"text": "As a consequence of such permissive filtering, the generated corpus contains a large number of real grammatical corrections, but also noise from a variety of sources, including edits with drastic semantic changes, imperfect corrections, ignored errors, and Wikipedia spam.", "labels": [], "entities": []}, {"text": "Our second strategy is to synthesize data by corrupting clean sentences.", "labels": [], "entities": []}, {"text": "We extract target sentences from Wikipedia, and generate corre-sponding source sentences by translating the target into another language and back.", "labels": [], "entities": []}, {"text": "This roundtrip translation introduces relatively clean errors, so the generated corpus is much less noisy than the human-derived Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.8423998951911926}]}, {"text": "However, these synthetic corruptions, unlike human errors, are limited to the domain of errors that the translation models are prone to making.", "labels": [], "entities": []}, {"text": "Both approaches benefit from the broad scope of topics in Wikipedia.", "labels": [], "entities": []}, {"text": "We train the Transformer sequence-to-sequence model () on data generated from the two schemes.", "labels": [], "entities": []}, {"text": "Fine-tuning the models on the Lang-8 corpus gives us additional improvements which allow a single model to surpass the state-of-art on both the CoNLL-2014 and the JF-LEG tasks.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9556941092014313}, {"text": "CoNLL-2014", "start_pos": 144, "end_pos": 154, "type": "DATASET", "confidence": 0.9199169874191284}]}, {"text": "Finally, we explore how to combine the two data sources by comparing a single model trained on all the data to an ensemble of models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results on the CoNLL-2014 test set () and the JFLEG test set ().", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9739290873209635}, {"text": "JFLEG test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9651373624801636}]}, {"text": "Our initial experiments with iterative decoding showed that increasing beam sizes beyond 4 did not yield improvements in performance.", "labels": [], "entities": []}, {"text": "Thus, we report all results using abeam size of 4.", "labels": [], "entities": []}, {"text": "Our ensemble models are obtained by decoding with 4 identical Transformers trained and finetuned separately.", "labels": [], "entities": []}, {"text": "Ensembles of neural translation systems are typically constructed by computing the logits from each individual system and combining them using either an arithmetic average) or a geometric average ().", "labels": [], "entities": [{"text": "neural translation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7744030654430389}]}, {"text": "Similar to, we find that a geometric average outperforms an arithmetic average.", "labels": [], "entities": []}, {"text": "Hence, we report results using only this scheme.", "labels": [], "entities": []}, {"text": "Following (Grundkiewicz and JunczysDowmunt, 2018; Junczys-Dowmunt et al., 2018), we preprocess JFLEG development and test sets with a spell-checking component but do not apply spelling correction to CoNLL sets.", "labels": [], "entities": []}, {"text": "For CoNLL sets, we pick the best iterative decoding threshold and number of iterations on a subset of the CoNLL-2014 training set, sampled to have the same ratio of modified to unmodified sentences as the CoNLL-2014 dev set.", "labels": [], "entities": [{"text": "CoNLL-2014 training set", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.9181541204452515}, {"text": "CoNLL-2014 dev set", "start_pos": 205, "end_pos": 223, "type": "DATASET", "confidence": 0.8322609066963196}]}, {"text": "For JFLEG, we pick the best decoding threshold on the JFLEG dev set.We report performance of our models by measuring    rate of revision downsampling, and maximum edit distance.", "labels": [], "entities": [{"text": "JFLEG dev set.We", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.8706315159797668}]}, {"text": "We generate four data sets using variations of these values: Default setting uses the default values described in Section 2, Max-edit-28 and Max-edit-6 correspond to maximum edit distance of 28 and 6 wordpieces respectively, and Dwnsample-1.35 corresponds to a revision downsampling rate of log 1.35 (n) fora page with a total of n revisions (whereas the default setting uses a rate of log 1.5 (n)).", "labels": [], "entities": [{"text": "Dwnsample-1.35", "start_pos": 229, "end_pos": 243, "type": "METRIC", "confidence": 0.8678354024887085}]}, {"text": "We train a fifth model on the union of the datasets.", "labels": [], "entities": []}, {"text": "shows that varying the data generation parameters led to modest variation in performance, but training on the union of the diverse datasets did not yield any benefit.", "labels": [], "entities": []}, {"text": "Finetuning yields large improvements for all models.", "labels": [], "entities": []}, {"text": "As a sanity check, we also trained a model only on Lang-8 with the same architecture.", "labels": [], "entities": []}, {"text": "All pretrained and fine-tuned models substantially outperform this Lang-8 only model, confirming the usefulness of pre-training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics computed over extant training sets for", "labels": [], "entities": []}, {"text": " Table 3: Comparing iterative decoding to single-shot decod-", "labels": [], "entities": []}, {"text": " Table 5: Statistics for test/dev data.", "labels": [], "entities": []}, {"text": " Table 6: Performance of the models trained on variants of", "labels": [], "entities": []}, {"text": " Table 7: Performance of the models trained on the round-", "labels": [], "entities": []}, {"text": " Table 8: Combining datasets using either a single model", "labels": [], "entities": []}, {"text": " Table 9: Comparison of recent state-of-the-art models (top) and our best single-system and ensemble models (bottom) on the", "labels": [], "entities": []}, {"text": " Table 11: F0.5 across error categories on the CoNLL-2014 test set.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9979776740074158}, {"text": "CoNLL-2014 test set", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9805135925610861}]}]}