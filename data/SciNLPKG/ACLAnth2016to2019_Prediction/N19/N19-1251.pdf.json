{"title": [{"text": "A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors", "labels": [], "entities": [{"text": "Detecting Subject-Verb Agreement Errors", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.8674130439758301}]}], "abstractContent": [{"text": "While rule-based detection of subject-verb agreement (SVA) errors is sensitive to syntactic parsing errors and irregularities and exceptions to the main rules, neural sequential la-belers have a tendency to overfit their training data.", "labels": [], "entities": [{"text": "rule-based detection of subject-verb agreement (SVA) errors", "start_pos": 6, "end_pos": 65, "type": "TASK", "confidence": 0.842929310268826}]}, {"text": "We observe that rule-based error generation is less sensitive to syntactic parsing errors and irregularities than error detection and explore a simple, yet efficient approach to getting the best of both worlds: We train neural sequential labelers on the combination of large volumes of silver standard data, obtained through rule-based error generation, and gold standard data.", "labels": [], "entities": [{"text": "rule-based error generation", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6063216427961985}, {"text": "error detection", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7514175176620483}, {"text": "rule-based error generation", "start_pos": 325, "end_pos": 352, "type": "TASK", "confidence": 0.6873882611592611}]}, {"text": "We show that our simple protocol leads to more robust detection of SVA errors on both in-domain and out-of-domain data, as well as in the context of other errors and long-distance dependencies; and across four standard benchmarks , the induced model on average achieves anew state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical Error Detection (GED,) is the task of detecting grammatical errors in text.", "labels": [], "entities": [{"text": "Grammatical Error Detection (GED", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7703590989112854}]}, {"text": "It is used in various real-world applications, such as writing assistance tools, self-assessment frameworks and language tutoring systems, facilitating incremental and/or exploratory editing of one's writing.", "labels": [], "entities": []}, {"text": "Accurate error detection systems also have potential applications for language generation and machine translation systems, guiding automatically generated output towards grammatically correct sequences.", "labels": [], "entities": [{"text": "Accurate error detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5438476701577505}, {"text": "language generation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7634322047233582}, {"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8005258738994598}]}, {"text": "The problem of detecting subject-verb agreement (SVA) errors is an important subtask of GED.", "labels": [], "entities": [{"text": "detecting subject-verb agreement (SVA) errors", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.8265198724610465}, {"text": "GED", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.8739542365074158}]}, {"text": "In this work, we focus on detecting subjectverb agreement errors in the English as a Second Language (ESL) domain.", "labels": [], "entities": []}, {"text": "Most SVA errors occur at the third-person present tense when determining whether the subject describes a singular or a plural concept.", "labels": [], "entities": []}, {"text": "The following examples demonstrate subject-verb agreement errors (bold): (1) a.", "labels": [], "entities": []}, {"text": "*They all knows where the conference is. b. *The Hotel are very close to Town Hall.", "labels": [], "entities": [{"text": "Town Hall", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9845966100692749}]}, {"text": "The task can be formulated as a sequence labeling problem, with the goal of labeling subjectverb pairs as being in agreement or not.", "labels": [], "entities": []}, {"text": "Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8959795236587524}, {"text": "GED", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8235725164413452}]}, {"text": "However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting.", "labels": [], "entities": [{"text": "GED", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9323159456253052}]}, {"text": "Previous work has also shown that neural language models with a similar architecture have difficulty learning subject-verb agreement patterns in the presence of agreement attractors (.", "labels": [], "entities": []}, {"text": "Rule-based approaches) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees.", "labels": [], "entities": []}, {"text": "The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules.", "labels": [], "entities": []}, {"text": "On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know.", "labels": [], "entities": []}, {"text": "Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that hurt the downstream GED performance.", "labels": [], "entities": [{"text": "rule-based matching", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6799444109201431}]}, {"text": "In this work, we compare the performance of rule-based approaches and end-to-end neural models for the detection of SVA errors.", "labels": [], "entities": []}, {"text": "We show that rule-based systems are vulnerable to errors in the underlying syntactic parsers, while also failing to capture irregularities and exceptions.", "labels": [], "entities": []}, {"text": "In contrast, end-to-end neural architectures are limited by the available labeled examples and sensitive to the variance in these datasets.", "labels": [], "entities": []}, {"text": "We then make the following observation: while rule-based error detection is severely affected by errors and irregularities in syntactic parsing, rulebased error generation is more robust.", "labels": [], "entities": [{"text": "rule-based error detection", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6100630660851797}, {"text": "rulebased error generation", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.6156761348247528}]}, {"text": "SVA errors can be generated without identifying subject dependency relations in advance, and changing the number of a verb almost always leads to an error.", "labels": [], "entities": []}, {"text": "This generated data can be used as a silver standard for optimizing neural sequence labeling models.", "labels": [], "entities": [{"text": "neural sequence labeling", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6349820693333944}]}, {"text": "We demonstrate that a system trained on a combination of available labeled data and large volumes of silver standard data outperforms both neural and rule-based baselines by a margin on three out of four standard benchmarks, and on average achieves anew state-of-the-art on detecting SVA errors.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our neural model trained on both artificially generated errors and ESL data (LSTM ESL+art ) to three baselines: a neural model trained only on ESL data (LSTM ESL ) (i.e., reflecting the performance of current state-of-the-art approaches for GED), a language model based method (BERT-LM) and our rule-based system.", "labels": [], "entities": [{"text": "GED", "start_pos": 252, "end_pos": 255, "type": "TASK", "confidence": 0.981775164604187}, {"text": "BERT-LM", "start_pos": 289, "end_pos": 296, "type": "METRIC", "confidence": 0.9859639406204224}]}, {"text": "In order to measure the real performance of a language model (LM) on the detection of SVA errors, we choose to use the BERT system (Devlin et al., 2018) to assign probabilities to different versions of the test sentences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.997344434261322}]}, {"text": "Specifically, we use the pre-trained uncased BERT-Base model.", "labels": [], "entities": [{"text": "BERT-Base", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.986809253692627}]}, {"text": "We duplicate the sentences each time a corruptible verb occurs (flipping its number).", "labels": [], "entities": []}, {"text": "The LM assigns a probability to both possible versions of the verbs.", "labels": [], "entities": []}, {"text": "We select the version which has the highest probability, if this probability is at least 0.1 6 higher than the probability of the verb in the original sentence.", "labels": [], "entities": []}, {"text": "We tune the model hyperparameters on the FCE development set, according to the F 0.5 score.", "labels": [], "entities": [{"text": "FCE development set", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9849692583084106}, {"text": "F 0.5 score", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9606746633847555}]}, {"text": "Training is stopped when F 0.5 on the FCE development set does not improve over 7 epochs.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9632583856582642}, {"text": "FCE development set", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9880907535552979}]}, {"text": "Word representations have size 300, while character representations have size 100.", "labels": [], "entities": []}, {"text": "The word-level LSTM hidden layers have size 300 for each direction, and the character-level LSTM hidden layers have size 100 for each direction.", "labels": [], "entities": []}, {"text": "Existing approaches are typically optimised for high precision at the cost of recall, as a system's utility depends strongly on the ratio of true to false positives, which has been found to be more important in terms of learning effect.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9971163272857666}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9992234706878662}]}, {"text": "A high number of false positives would mean that the system often flags correct language as incorrect, and may therefore end up doing more harm than good).", "labels": [], "entities": []}, {"text": "Because of this, F 0.5 is preferred to F 1 in the GED domain as it puts more weight on precision than recall.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9674528539180756}, {"text": "F 1", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9746985137462616}, {"text": "GED domain", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.751908540725708}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9987913966178894}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9962286949157715}]}, {"text": "For each experiment, we report the token-level precision (P), the recall (R), and the F 0.5 scores.", "labels": [], "entities": [{"text": "token-level precision (P)", "start_pos": 35, "end_pos": 60, "type": "METRIC", "confidence": 0.8367804884910583}, {"text": "recall (R)", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9792237132787704}, {"text": "F 0.5 scores", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9717718362808228}]}], "tableCaptions": [{"text": " Table 1: Performance of our systems (rule-based and LSTMs) and baselines. BERT-LM is the language model  baseline.", "labels": [], "entities": [{"text": "BERT-LM", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9958689212799072}]}, {"text": " Table 1. Loo- king at the performance of the LSTM ESL+art sys- tem, we see that on 3 out of 4 benchmarks, our  neural model trained on artificially generated er- rors outperforms the LSTM ESL system with res- pect to F 0.5 . On average, over the four bench- marks, its F 0.5 score is 2.43 points higher than the  best performing baseline. Both neural models ob- tain higher F 0.5 scores than the rule-based base- line, on average and across the board, i.e., +10.6  for LSTM ESL and +15.7 for LSTM ESL+Art . The  BERT-LM outperforms the LSTM ESL (mostly  due to its higher recall, i.e., +18.66) but still does  not reach the F 0.5 score of the LSTM ESL+Art sys- tem which gets higher precision and recall overall  (+2.62 and +1.51 respectively).  Furthermore, we observe a trend that the two", "labels": [], "entities": [{"text": "F", "start_pos": 218, "end_pos": 219, "type": "METRIC", "confidence": 0.9359591603279114}, {"text": "F 0.5 score", "start_pos": 270, "end_pos": 281, "type": "METRIC", "confidence": 0.9634701013565063}, {"text": "F", "start_pos": 375, "end_pos": 376, "type": "METRIC", "confidence": 0.9780189990997314}, {"text": "BERT-LM", "start_pos": 513, "end_pos": 520, "type": "METRIC", "confidence": 0.9982115030288696}, {"text": "recall", "start_pos": 573, "end_pos": 579, "type": "METRIC", "confidence": 0.9984024167060852}, {"text": "F 0.5 score", "start_pos": 625, "end_pos": 636, "type": "METRIC", "confidence": 0.9703398744265238}, {"text": "precision", "start_pos": 684, "end_pos": 693, "type": "METRIC", "confidence": 0.9989326596260071}, {"text": "recall", "start_pos": 698, "end_pos": 704, "type": "METRIC", "confidence": 0.9991693496704102}]}, {"text": " Table 2: Performance (F 0.5 scores) of the LSTM mo- dels when trained using an additional set of 'clean' sen- tences (cor) where non-SVA errors have been correc- ted.", "labels": [], "entities": [{"text": "F 0.5 scores)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9622593224048615}]}, {"text": " Table 3: The Stanford PoS Tagger and Dependency  Parser's performance on different treebanks. Subject- verb precision/recall relates to subject-verb relations.  PoS tag accuracy is only for PoS tags of the subjects  and verbs.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9292160868644714}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9669283032417297}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8589478135108948}]}]}