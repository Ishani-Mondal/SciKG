{"title": [], "abstractContent": [{"text": "Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order.", "labels": [], "entities": [{"text": "Recurrent neural network grammars (RNNG)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7667137086391449}]}, {"text": "Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9409278631210327}]}, {"text": "In this work, we experiment with unsupervised learning of RNNGs.", "labels": [], "entities": []}, {"text": "Since directly marginal-izing over the space of latent trees is intractable , we instead apply amortized varia-tional inference.", "labels": [], "entities": []}, {"text": "To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser.", "labels": [], "entities": []}, {"text": "On language modeling, unsu-pervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7079610079526901}]}, {"text": "On constituency grammar induction , they are competitive with recent neu-ral language models that induce tree structures from words through attention mechanisms.", "labels": [], "entities": [{"text": "constituency grammar induction", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.8969354629516602}]}], "introductionContent": [{"text": "Recurrent neural network grammars (RNNGs)) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words.", "labels": [], "entities": [{"text": "Recurrent neural network grammars (RNNGs))", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6970395020076207}]}, {"text": "Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (, better encode syntactic properties of language ( , and correlate with electrophysiological responses in the human brain . However, these all require annotated syntactic trees for training.", "labels": [], "entities": []}, {"text": "In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7603742182254791}, {"text": "grammar induction", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7556139528751373}]}, {"text": "Work done while the first author was an intern at DeepMind.", "labels": [], "entities": []}, {"text": "Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model p \u03b8 (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximize the log marginal likelihood log p \u03b8 (x) = log z p \u03b8 (x, z).", "labels": [], "entities": [{"text": "log marginal likelihood log p \u03b8", "start_pos": 277, "end_pos": 308, "type": "METRIC", "confidence": 0.7982885440190634}]}, {"text": "Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives () or priors.", "labels": [], "entities": []}, {"text": "These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions.", "labels": [], "entities": []}, {"text": "Like RNN language models, RNNGs make no independence assumptions.", "labels": [], "entities": []}, {"text": "Instead they encode structural bias through operations that compose linguistic constituents.", "labels": [], "entities": []}, {"text": "The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, but make unsupervised learning challenging.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.6855118274688721}]}, {"text": "First, marginalization is intractable.", "labels": [], "entities": []}, {"text": "Second, the biases imposed by the RNNG are relatively weak compared to those imposed by models like PCFGs.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.910563588142395}]}, {"text": "There is little pressure for non-trivial tree structure to emerge during unsupervised RNNG (URNNG) learning.", "labels": [], "entities": []}, {"text": "In this work, we explore a technique for handling intractable marginalization while also injecting inductive bias.", "labels": [], "entities": []}, {"text": "Specifically we employ amortized variational inference () with a structured inference network.", "labels": [], "entities": []}, {"text": "Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure.", "labels": [], "entities": []}, {"text": "In particular, a con-ditional random field (CRF) constituency parser (, which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior (.", "labels": [], "entities": []}, {"text": "We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs.", "labels": [], "entities": []}, {"text": "In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7580851912498474}]}, {"text": "Our results, along with other recent work on joint language modeling/structure learning with deep networks), suggest that it is possible learn generative models of language that model the underlying data well (i.e. assign high likelihood to held-out data) and at the same time induce meaningful linguistic structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform a syntactic evaluation of the different models based on the setup from: the model is given two minimally different sentences, one grammatical and one ungrammatical, and must identify the grammatical sentence by assigning it higher probability.", "labels": [], "entities": []}, {"text": "Overall the supervised RNNG significantly outperforms the other models, indicating opportunities for further work in unsupervised modeling.", "labels": [], "entities": []}, {"text": "While the URNNG does slightly outperform an RNNLM, the distribution of errors made from both models are similar, and thus it is not clear whether the outperformance is simply due to better perplexity or learning different structural biases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language modeling perplexity (PPL) and  grammar induction F 1 scores on English (PTB) and  Chinese (CTB) for the different models. Note that our  PTB setup from Dyer et al. (2016) differs consider- ably from the usual language modeling setup (Mikolov  et al., 2010) since we model each sentence indepen- dently and use a much larger vocabulary (see  \u00a73.1).", "labels": [], "entities": [{"text": "grammar induction F 1", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.5863901823759079}]}, {"text": " Table 2: (Top) Comparison of this work as a language  model against prior works on sentence-level PTB with  preprocessing from Dyer et al. (2016). Note that pre- vious versions of RNNG differ from ours in terms of  parameterization and model size. (Bottom) Results on  a subset (1M sentences) of the one billion word corpus.  PRPN  \u2020 is the model from Shen et al. (2018), whose hy- perparameters were tuned by us. RNNG  \u2021 is trained on  predicted parse trees from Kitaev and Klein (2018).", "labels": [], "entities": []}, {"text": " Table 3: (Left) F 1 scores of URNNG against other  trees. \"Self\" refers to another URNNG trained with  a different random seed. (Right) Recall of constituents  by label for URNNG and PRPN. Recall for a particular  label is the fraction of ground truth constituents of that  label that were identified by the model.", "labels": [], "entities": [{"text": "F", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.9919049739837646}, {"text": "Recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9490610361099243}]}, {"text": " Table 4: PTB F 1 scores using the same evaluation  setup as Drozdov et al. (2019), which evaluates against  binarized trees, counts punctuation and trivial spans,  and uses sentence-level F 1 . +PP indicates a post- processing heuristic which directly attaches trailing  punctuation to the root. This does not change URNNG  results since it learns to do so anyway. Results with  \u2021  are copied from Table 1 of Drozdov et al. (2019).", "labels": [], "entities": [{"text": "PTB F 1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7017132639884949}]}, {"text": " Table 5:  Metrics related to the generative  model/inference network for RNNG/URNNG.  For the supervised RNNG we take the \"inference  network\" to be the discriminative parser trained  alongside the generative model (see  \u00a73.3).", "labels": [], "entities": []}, {"text": " Table 6: Syntactic evaluation based on the setup from", "labels": [], "entities": [{"text": "Syntactic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9339840710163116}]}]}