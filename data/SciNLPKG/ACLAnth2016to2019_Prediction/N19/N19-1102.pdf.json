{"title": [{"text": "Strong Baselines for Complex Word Identification across Multiple Languages", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6656809945901235}]}], "abstractContent": [{"text": "Complex Word Identification (CWI) is the task of identifying which words or phrases in a sentence are difficult to understand by a target audience.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7693772912025452}]}, {"text": "The latest CWI Shared Task released data for two settings: monolingual (i.e. train and test in the same language) and cross-lingual (i.e. test in a language not seen during training).", "labels": [], "entities": []}, {"text": "The best monolingual models relied on language-dependent features, which do not generalise in the cross-lingual setting, while the best cross-lingual model used neural networks with multi-task learning.", "labels": [], "entities": []}, {"text": "In this paper , we present monolingual and cross-lingual CWI models that perform as well as (or better than) most models submitted to the latest CWI Shared Task.", "labels": [], "entities": []}, {"text": "We show that carefully selected features and simple learning models can achieve state-of-the-art performance, and result in strong baselines for future development in this area.", "labels": [], "entities": []}, {"text": "Finally, we discuss how inconsistencies in the annotation of the data can explain some of the results obtained.", "labels": [], "entities": []}], "introductionContent": [{"text": "Complex Word Identification (CWI) consists of deciding which words (or phrases) in a text could be difficult to understand by a specific type of reader.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7861154476801554}]}, {"text": "In this work, we follow the CWI Shared Tasks ( and assume that a target word or multi-word expression ) in a sentence is given, and our goal is to determine if it is complex or not (an example is shown in).", "labels": [], "entities": []}, {"text": "Under this setting, CWI is normally treated using supervised learning and feature engineering to build monolingual models.", "labels": [], "entities": []}, {"text": "Unfortunately, this approach is infeasible for languages with scarce resources of annotated  data.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in both monolingual and cross-lingual CWI; in the latter, we build models to make predictions for languages not seen during training.", "labels": [], "entities": []}, {"text": "While monolingual CWI has been studied extensively (see a survey in ), the cross-lingual setup of the task was introduced only recently by, who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains): News, WikiNews and Wikipedia.", "labels": [], "entities": []}, {"text": "For the Second CWI Shared Task (, participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data.", "labels": [], "entities": []}, {"text": "In the monolingual track, the best systems for English) differed significantly in terms of feature set size and the model's complexity, to the best systems for German and Spanish).", "labels": [], "entities": []}, {"text": "The latter used Random Forests with eight features, whilst the former used AdaBoost with 5000 estimators or ensemble voting combining AdaBoost and Random Forest classifiers, with about 20 features.", "labels": [], "entities": [{"text": "AdaBoost", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9215899705886841}]}, {"text": "In the cross-lingual track, only two teams achieved better scores than the baseline: who used length and frequency based features with Random Forests, and who implemented an ensemble of Random Forests and feed-forward neural networks in a multi-task learning architecture.", "labels": [], "entities": []}, {"text": "Our approach to CWI differs from previous work in that we begin by building competitive monolingual models, but using the same set of features and learning algorithm across languages.", "labels": [], "entities": []}, {"text": "This reduces the possibility of getting high scores due to modelling annotation artifacts present in the dataset of one language.", "labels": [], "entities": []}, {"text": "Our monolingual models achieve better scores for Spanish and German than the best systems in the Second CWI Shared Task.", "labels": [], "entities": [{"text": "Second CWI Shared Task", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.7822777330875397}]}, {"text": "After that, we focus on language-independent features, and keep those that achieve good performance in cross-lingual experiments across all possible combinations of languages.", "labels": [], "entities": []}, {"text": "This results in a small set of five language-independent features, which achieve a score as high as the top models in the French test set.", "labels": [], "entities": [{"text": "French test set", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9032610654830933}]}, {"text": "Finally, we analyse the annotation of the datasets and find some inconsistencies that could explain some of our results.", "labels": [], "entities": []}, {"text": "Code for all our models can be found at:", "labels": [], "entities": []}], "datasetContent": [{"text": "To find out which features were best suited for the cross-lingual approach, we performed an iterative ablation analysis (see Appendix B for details).", "labels": [], "entities": []}, {"text": "Using this process, we arrived at our final cross-lingual feature set: number of syllables in the target, number of tokens in the target, number of complex punctuation marks (such as hyphens), sentence length, and unigram probabilities.", "labels": [], "entities": []}, {"text": "Furthermore, we analyse the effect of different language combinations on the performance of the cross-lingual model in order to investigate how the relationship between the languages trained and tested on would influence model performance.", "labels": [], "entities": []}, {"text": "Recall that we only have training data for English, Spanish and German, but not French.", "labels": [], "entities": []}, {"text": "We train models using all possible combinations (each language independently, each pairing, and all three) and evaluate on each of the four languages that have test data (i.e. the former three and French), excluding training combinations that include the test language.", "labels": [], "entities": []}, {"text": "Results are shown in  When testing on French, we achieved the highest performance by training on German only (75.8), followed closely by training on a combination of German and Spanish (75.7) and only Spanish (75.5).", "labels": [], "entities": []}, {"text": "The worst performance was achieved by training only on English (69.2), and the performance also noticeably decreased for all training combinations that included English.", "labels": [], "entities": []}, {"text": "When testing on German, language choice had a weaker effect.", "labels": [], "entities": []}, {"text": "The highest score came from combining, but using only one of those languages gave comparable results (72.6 for Spanish, 73.0 for English).", "labels": [], "entities": []}, {"text": "For Spanish, the best results were achieved when training only on German (72.6).", "labels": [], "entities": []}, {"text": "Adding English to the training languages decreased the: Comparison between the monolingual and cross-lingual state of the art (SotA), and our crosslingual system.", "labels": [], "entities": []}, {"text": "performance (70.8), which was even lower when training only on English (69.1).", "labels": [], "entities": []}, {"text": "It is noteworthy that adding English to the training languages noticeably decreases performance for both Spanish and French, but not for German.", "labels": [], "entities": []}, {"text": "One possible reason for Spanish and French not benefiting from English when German does is that both English and German are Germanic languages, whereas Spanish and French are Romance languages.", "labels": [], "entities": []}, {"text": "Another possible explanation for the decrease of performance caused by training with English is that there are inconsistencies in the way MWEs in the datasets were labelled across languages, which we explore in Sec.", "labels": [], "entities": []}, {"text": "5. We finally compare our cross-lingual models against the state of the art: the best monolingual system for Spanish and German, and the best cross-lingual system for French, where no monolingual systems exist.", "labels": [], "entities": []}, {"text": "As shows, our crosslingual models come close to the best monolingual models for Spanish and especially for German.", "labels": [], "entities": []}, {"text": "This is remarkable given how simple our model and features are, and that the approaches we compare against train complex models for each language.", "labels": [], "entities": []}, {"text": "Furthermore, this points towards the possibility of extending CWI to more languages which lack training data.", "labels": [], "entities": []}, {"text": "Finally, compares the coefficients for models trained on Romance and Germanic languages.", "labels": [], "entities": []}, {"text": "Notably, use of complex punctuation (such as the hyphenation in \"laser-activated\" or \"drug-related\") and the number of tokens are inversely correlated w.r.t. the word or MWE being complex.", "labels": [], "entities": []}, {"text": "More words in the target was correlated with complexity for English and German, and inversely correlated for Spanish.", "labels": [], "entities": [{"text": "complexity", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9606488943099976}]}, {"text": "While examining our models' incorrect predictions, we observed inconsistencies in labelling in the datasets between target MWEs and their subwords/sub-expressions (SWs).: Coefficients for cross-lingual models trained on Germanic and Romance languages.", "labels": [], "entities": []}, {"text": "The First CWI Shared Task (Paetzold and Specia, 2016) used the annotations of a group (i.e. ten annotators on the training data) to predict the annotation of an individual (i.e. one annotator on the test data).", "labels": [], "entities": []}, {"text": "The resulting inconsistencies in labelling may have contributed to the low F-scores of systems in the task (.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9956477284431458}]}, {"text": "Although the Second CWI Shared Task improved on the first by having multiple annotators for all splits of the data, it contains some labelling inconsistencies arising from annotators now being able to label phrases, and words within them, separately.", "labels": [], "entities": []}, {"text": "More concretely, we found that across all datasets, 72% of MWEs contain at least one SW with the opposite label (see).", "labels": [], "entities": []}, {"text": "While this makes sense in some cases, every SW in 25% of MWE instances has the opposite label.", "labels": [], "entities": []}, {"text": "For example, \"numerous falsifications and ballot stuffing\" is not annotated as complex, despite its SWs \"numerous\", \"numerous falsifications\", \"falsifications\", \"ballot\", \"ballot stuffing\" and \"stuffing\" all being complex.", "labels": [], "entities": []}, {"text": "Conversely, \"crise des march\u00e9s du cr\u00e9dit\" is complex, despite \"crise\", \"march\u00e9s\" and \"cr\u00e9dit\" being labelled non-complex.", "labels": [], "entities": []}, {"text": "It is difficult to see how classifiers that extract features for MWEs from their individual SWs could predict the labels of both correctly.", "labels": [], "entities": []}, {"text": "Furthermore, every target MWE in the Spanish, German and French datasets is labelled complex.", "labels": [], "entities": [{"text": "Spanish, German and French datasets", "start_pos": 37, "end_pos": 72, "type": "DATASET", "confidence": 0.5994595388571421}]}, {"text": "This may bias a classifier trained on the Spanish or German datasets towards learning MWEs and long individual words (if length is a feature) are complex.", "labels": [], "entities": []}, {"text": "In particular, this observation may help explain why adding English as a training language decreased the performance of our crosslingual system when testing on French and Spanish (where all MWEs are complex).", "labels": [], "entities": []}, {"text": "An analysis in further found that their cross-lingual French model was effective at predicting long complex words/MWEs but had difficulty predicting long non-complex words.: MWE annotation analysis: numbers of MWEs labelled complex (C) and non-complex (NC), numbers with at least one SW (\u2265 1 Irreg) and all SWs (All Irreg.) having the opposite label.", "labels": [], "entities": [{"text": "predicting long complex words/MWEs", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.7143904368082682}, {"text": "Irreg", "start_pos": 292, "end_pos": 297, "type": "METRIC", "confidence": 0.9286328554153442}]}, {"text": "It is also worth noting that considering a word or MWE as complex is subjective and may differ from person to person, even within the same target audience.", "labels": [], "entities": []}, {"text": "investigated predicting complex words based on the gaze patterns of children with reading difficulties.", "labels": [], "entities": [{"text": "predicting complex words", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.9018872578938802}]}, {"text": "They found a high degree of specificity in misreadings between children, that is, which words they found complex when reading aloud.", "labels": [], "entities": []}, {"text": "This variety of complexity judgements even within one target group points towards the high degree of subjectivity in the task, which may also partly explain the inconsistencies in the dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of annotated samples in each dataset  for each language.", "labels": [], "entities": []}, {"text": " Table 3: Macro-F1 for the baseline (BL), our monolin- gual approach (MA), and the state of the art (SotA) on  the Dev and Test splits of each dataset.", "labels": [], "entities": [{"text": "monolin- gual approach (MA)", "start_pos": 46, "end_pos": 73, "type": "METRIC", "confidence": 0.6712847564901624}]}, {"text": " Table 4: Comparison of Test and Dev results for all  permutations of training languages.", "labels": [], "entities": []}, {"text": " Table 5: Comparison between the monolingual and  cross-lingual state of the art (SotA), and our cross- lingual system.", "labels": [], "entities": []}, {"text": " Table 7: MWE annotation analysis: numbers of MWEs  labelled complex (C) and non-complex (NC), numbers  with at least one SW (\u2265 1 Irreg) and all SWs (All Ir- reg.) having the opposite label.", "labels": [], "entities": [{"text": "MWE annotation analysis", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8962186376253763}, {"text": "Irreg", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9873440265655518}]}]}