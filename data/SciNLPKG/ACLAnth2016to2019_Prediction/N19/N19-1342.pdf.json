{"title": [{"text": "CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition", "labels": [], "entities": [{"text": "Chinese Named Entity Recognition", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6010847166180611}]}], "abstractContent": [{"text": "Named entity recognition (NER) is a common task in Natural Language Processing (NLP), but it remains more challenging in Chinese because of its lack of natural delimiters.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8241377274195353}]}, {"text": "Therefore , Chinese Word Segmentation (CWS) is usually necessary as the first step for Chi-nese NER.", "labels": [], "entities": [{"text": "Chinese Word Segmentation (CWS)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.7610804190238317}, {"text": "NER", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.6827370524406433}]}, {"text": "However, models based on word-level embeddings and lexicon features often suffer from segmentation errors and out-of-vocabulary (OOV) problems.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a Convolutional Attention Network (CAN) for Chinese NER, which consists of a character-based convolutional neural network (CNN) with local-attention layer and a gated recurrent unit (GRU) with global self-attention layer to capture the information from adjacent characters and sentence contexts.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.5397050231695175}]}, {"text": "Moreover, differently from other approaches, CAN-NER does not depend on any external resources like lexicons and employing small-size char em-beddings makes CAN-NER more practical for real systems scenarios.", "labels": [], "entities": []}, {"text": "Extensive experimental results show that our approach outperforms state-of-the-art methods without word embedding and external lexicon resources on different domains datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition (NER) aims at identifying text spans which are associated with a specific semantic entity type such as person (PER), organization (ORG), location (LOC), and geopolitical entity (GPE).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8037245819965998}]}, {"text": "NER has received constant research attention as it is the first step in a wide range of downstream Natural Language Processing (NLP) tasks, e.g., entity linking (, relation extraction (, event extraction (, and coreference resolution.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7479477524757385}, {"text": "entity linking", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7712410688400269}, {"text": "relation extraction", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.789047509431839}, {"text": "event extraction", "start_pos": 187, "end_pos": 203, "type": "TASK", "confidence": 0.7490851581096649}, {"text": "coreference resolution", "start_pos": 211, "end_pos": 233, "type": "TASK", "confidence": 0.9546018540859222}]}, {"text": "The standard approach in existing state-of-the-art models * This work was performed when the first author was an intern at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.7968572775522867}]}, {"text": "for English NER treats the problem as a word-byword sequence labeling task and makes full use of the Recurrent Neural Network (RNN) and Conditional Random Field (CRF) to capture context information at the word level ().", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.6993488073348999}, {"text": "word-byword sequence labeling task", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.6964253410696983}]}, {"text": "These models for English NER pre- dicta tag for each word assuming that words can be separated clearly by explicit word separators, e.g., blank spaces.", "labels": [], "entities": [{"text": "NER pre- dicta tag", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7517206311225891}]}, {"text": "As the Chinese language has no natural delimiters, it would be intuitive to apply Chinese Word Segmentation (CWS) first to get word boundaries and then use a word-level sequence labeling model similar to the English NER models.", "labels": [], "entities": [{"text": "Chinese Word Segmentation (CWS)", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.672380343079567}]}, {"text": "However, word boundaries can be ambiguous in Chinese, which leads to the possibility that entity boundaries do not match word boundaries.", "labels": [], "entities": []}, {"text": "For example, the term \"\u897f\u85cf\u81ea\u6cbb\u533a (Tibet Autonomous Region)\" is a GPE-type entity in NER, but it could be segmented as a single word or as two words \"\u897f \u85cf (Tibet)\" and \"\u81ea \u6cbb \u533a (autonomous region)\" separately, depending on different granularity of segmentation tools.", "labels": [], "entities": [{"text": "NER", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6821864247322083}]}, {"text": "Most of the time, however, it is hard to determine the correct granularity for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7344913929700851}]}, {"text": "Also, as shown in, different segmentation can lead to different sentence meanings in Chinese, which could even result in different named entities.", "labels": [], "entities": []}, {"text": "Obviously, if entity boundaries are mistakenly detected in segmentation, it will negatively affect entity tagging in word-based NER models.", "labels": [], "entities": [{"text": "entity tagging", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6982851624488831}]}, {"text": "Furthermore, most recent neural networkbased Chinese NER models rely heavily on wordlevel embeddings and external lexicon sets (.", "labels": [], "entities": []}, {"text": "The quality of such models strongly relies on the different word embedding representations and lexicon features.", "labels": [], "entities": []}, {"text": "Moreover, word-based models tend to suffer from OOV issues as Chinese words can be very diverse and named entities are an important source of OOV words.", "labels": [], "entities": []}, {"text": "Other potential limitations are as follows: (1) Dependency on word embeddings increases model size and makes the finetuning process more costly during training (while negatively affecting latency in testing/decoding); (2) It is hard to learn word representation correctly without enough labeled utterances for named entities are usually rarer proper nouns.", "labels": [], "entities": []}, {"text": "(3) Large lexicons are very costly for real NER systems as they greatly increase memory usage and latency in feature extraction (matching), which makes models inefficient; (4) It is very costly to remove noise from large lexicons and any update to pre-trained word embeddings or lexicons requires model retraining.", "labels": [], "entities": [{"text": "feature extraction (matching", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.7317920550704002}]}, {"text": "Meanwhile, character-level embedding by itself can only carry limited information due to losing word and word-sequence information.", "labels": [], "entities": []}, {"text": "For instance, the character \"\u62cd\" in words \"\u7403 \u62cd\" (bat) and \"\u62cd\u5356\" (auction) has very different meanings.", "labels": [], "entities": []}, {"text": "How to better integrate segmentationrelated information and exploit local context information is the key feature in a character-based model.", "labels": [], "entities": []}, {"text": "leverage lexicons to add all the embeddings of candidate word segmentation to their last character embeddings as soft features, and construct a convolutional neural network (CNN) to encode characters as word-level information.", "labels": [], "entities": []}, {"text": "propose a multitask architecture to learn NER tagging and Chinese word segmentation together, with each part using a character-based Bi-LSTM.", "labels": [], "entities": [{"text": "NER tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8829891383647919}, {"text": "Chinese word segmentation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6055471698443095}]}, {"text": "In this paper, we propose a convolutional attention layer to capture the implicit relations within adjacent characters, in which the position features from word segmentation are soft hints for character combinations.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.696712464094162}]}, {"text": "With the segmentation vector softly concatenating into character embedding, the convolutional attention layer is able to group implicitly meaning-related characters and help bypass the impact of segmentation errors.", "labels": [], "entities": []}, {"text": "A BiGRU structure with a global self-attention layer on the whole sentence is utilized to capture sentence-level dependencies.", "labels": [], "entities": []}, {"text": "Extensive experimental results show that our approach outperforms state-of-the-art methods without relying on external resources (e.g. word embedding, external lexicon) across different corpora.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: \u2022 We first combine CNNs with the localattention mechanism to enhance the ability of the model to capture implicitly local context relations among character sequences.", "labels": [], "entities": []}, {"text": "Compared with experimental results against a baseline with a regular CNN layer, our Convolutional Attention layer leads to substantial performance improvements.", "labels": [], "entities": []}, {"text": "\u2022 We introduce a character-based Chinese NER model that consists of combined CNN with local attention and BiGRU with global selfattention layers.", "labels": [], "entities": [{"text": "NER", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8024396896362305}, {"text": "BiGRU", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9524187445640564}]}, {"text": "Our model achieves stateof-the-art F1-scores without using any external resources like word embeddings and lexicon resources, which make it very practical for real-world NER systems.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9887520670890808}]}], "datasetContent": [{"text": "To demonstrate the effectiveness of our proposed model, we have run multiple experiments on Chinese NER datasets covering different domains.", "labels": [], "entities": [{"text": "Chinese NER datasets", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.6716468930244446}]}, {"text": "This section describes the details of each dataset, settings, and results in our experiments.", "labels": [], "entities": []}, {"text": "Standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9385461211204529}, {"text": "recall (R)", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9606342166662216}, {"text": "F1-score (F1)", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.939472109079361}]}, {"text": "Data to automatically segment these by using the model described in.", "labels": [], "entities": []}, {"text": "We treat NER as a sequential labeling problem and adopt the BIOES tagging style since it has been shown to produce better results than straight BIO.", "labels": [], "entities": [{"text": "BIOES tagging", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.7258091270923615}]}, {"text": "Hyper-parameter settings For hyperparameter configuration, we adjust them according to the performance on the described development sets for Chinese NER.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 141, "end_pos": 152, "type": "DATASET", "confidence": 0.9187400639057159}]}, {"text": "We set the character embedding size, hidden sizes of CNN and BiGRU to 300 dims.", "labels": [], "entities": [{"text": "CNN", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9489105343818665}, {"text": "BiGRU", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.501940906047821}]}, {"text": "After comparing experimental results with different CNN window sizes, we set the window size as 5.", "labels": [], "entities": []}, {"text": "Adadelta is used for optimization, with an initial learning rate of 0.005.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8014159202575684}]}, {"text": "The character embeddings used in our experiments are from, which is trained by Skip-Gram with Negative Sampling (SGNS) on Baidu Encyclopedia.", "labels": [], "entities": [{"text": "Baidu Encyclopedia", "start_pos": 122, "end_pos": 140, "type": "DATASET", "confidence": 0.9656282067298889}]}, {"text": "In this section, we describe the experimental results of our proposed model and previous state-ofthe-art methods on four datasets: Weibo, Chinese Resume, OntoNotes 4, and MSRA.", "labels": [], "entities": [{"text": "Weibo", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9920205473899841}, {"text": "Chinese Resume", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.8105515241622925}, {"text": "MSRA", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.9564504623413086}]}, {"text": "We propose two baselines for comparison, and show the CAN-NER model results.", "labels": [], "entities": []}, {"text": "In the experiment results table, we use Baseline to represent a pure BiGRU + CRF model; and Baseline + CNN to indicate the base model with a CNN layer.", "labels": [], "entities": []}, {"text": "Here we compare our proposed model with the latest models on the Weibo dataset.", "labels": [], "entities": [{"text": "Weibo dataset", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9922914803028107}]}, {"text": "3 shows the F1-scores for named entities (NE), nominal entities (NM, excluding named entities), and both (Overall).", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9989535808563232}]}, {"text": "We observe that our proposed model achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Existing state-of-the-art systems include Peng and Dredze (2016), He and Sun (2017b), and, which leverage rich external data like cross-domain data, semi-supervised data, and lexicons, or joint-train NER and Chinese Word Segmentation (CWS).", "labels": [], "entities": [{"text": "NER and Chinese Word Segmentation (CWS)", "start_pos": 200, "end_pos": 239, "type": "TASK", "confidence": 0.670871939510107}]}, {"text": "In the first block of, we report the performance of the latest models.", "labels": [], "entities": []}, {"text": "propose a model that jointly trains embeddings with NER and it achieves a F1-score of 56.05% on overall performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.954886257648468}, {"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9997304081916809}]}, {"text": "The model) that jointly trains NER and CWS reaches a F1-score of 58.99%.", "labels": [], "entities": [{"text": "NER", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.5430363416671753}, {"text": "CWS", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.5511721968650818}, {"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9997771382331848}]}, {"text": "propose a unified model to exploit crossdomain and semi-supervised data, which improves the F1-score from 54.82% to 58.23% compared with the model proposed by. use an adversarial transfer learning framework to incorporate task-shared word boundary information from CWS and achieves a F1-score of 58.70%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9997331500053406}, {"text": "F1-score", "start_pos": 284, "end_pos": 292, "type": "METRIC", "confidence": 0.9993308782577515}]}, {"text": "leverage a lattice structure to integrate lexicon information into their model and achieve a F1-score of 58.79%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9997648596763611}]}, {"text": "In the second block of, we give the results of our baselines and proposed models.", "labels": [], "entities": []}, {"text": "While the BiGRU + CRF baseline only achieves a F1-score of 53.80%, adding a normal CNN layer as featurizer improves the score to 55.91%.", "labels": [], "entities": [{"text": "BiGRU + CRF baseline", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.796574592590332}, {"text": "F1-score", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996688365936279}]}, {"text": "Replacing the CNN with our convolutional attention layer greatly improves the F1-score to 59.31%, which outperforms other models.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996638298034668}]}, {"text": "The improvement demonstrates the effectiveness of our proposed model.", "labels": [], "entities": []}, {"text": "Adding our convolutional attention leads a further improvement and achieves state-of-the-art F1-score of 94.94%, which further demonstrates the effectiveness of our proposed model.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9989156723022461}]}, {"text": "shows the results of our baselines and proposed model.", "labels": [], "entities": []}, {"text": "Consistently with observations on the Weibo and Resume datasets, our Convolutional Attention layer leads to a substantial increment on F1-score.", "labels": [], "entities": [{"text": "Weibo", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9804834127426147}, {"text": "Resume datasets", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8233522176742554}, {"text": "Convolutional Attention layer", "start_pos": 69, "end_pos": 98, "type": "METRIC", "confidence": 0.7554824749628702}, {"text": "F1-score", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9972879886627197}]}, {"text": "Our proposed model achieves a competitive F1-score of 73.64% among character-based model without using external data (e.g., Zhang and Yang (2018) \u2021 ).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9995729327201843}]}, {"text": "introduce a lattice structure to incorporate lexicon information into the neural network, which actually includes word embedding information.", "labels": [], "entities": []}, {"text": "Although this model achieves state-of-the-art F1-score at 93.18%, it leverages external lexicon data and thus the result is dependent on the quality of the lexicon.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9984102249145508}]}, {"text": "At the bottom section of the table, we can see that Baseline + CNN already outperforms most previous methods.", "labels": [], "entities": []}, {"text": "Compared with, our char-based method achieves a competitive F1-score of 92.97% without any additional lexicon data and word embedding information.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993981122970581}]}, {"text": "Moreover, CAN-NER model achieves stateof-the-art result among the character-based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Weibo NER results", "labels": [], "entities": [{"text": "Weibo NER", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9182937741279602}]}, {"text": " Table 3: Results on Chinese Resume Dataset. For  models proposed by Zhang and Yang (2018), 1 rep- resents the char-based LSTM model, 2 indicates the  word-based LSTM model and 3 is the Lattice model.", "labels": [], "entities": [{"text": "Chinese Resume Dataset", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7939382394154867}]}, {"text": " Table 4: Results on OntoNotes", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.5148547291755676}]}, {"text": " Table 5: Results on MSRA dataset", "labels": [], "entities": [{"text": "MSRA dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8675223290920258}]}]}