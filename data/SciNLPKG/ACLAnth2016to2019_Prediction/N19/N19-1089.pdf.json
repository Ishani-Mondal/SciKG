{"title": [{"text": "PoMo: Generating Entity-Specific Post-Modifiers in Context", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce entity post-modifier generation as an instance of a collaborative writing task.", "labels": [], "entities": [{"text": "entity post-modifier generation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6487833559513092}]}, {"text": "Given a sentence about a target entity, the task is to automatically generate a post-modifier phrase that provides contextually relevant information about the entity.", "labels": [], "entities": []}, {"text": "For example, for the sentence, \"Barack Obama, , supported the #MeToo movement.\", the phrase \"a father of two girls\" is a contextually relevant post-modifier.", "labels": [], "entities": []}, {"text": "To this end, we build PoMo, a post-modifier dataset created automatically from news articles reflecting a jour-nalistic need for incorporating entity information that is relevant to a particular news event.", "labels": [], "entities": []}, {"text": "PoMo consists of more than 231K sentences with post-modifiers and associated facts extracted from Wikidata for around 57K unique entities.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to show that modeling contextual relevance is necessary for accurate post-modifier generation.", "labels": [], "entities": [{"text": "post-modifier generation", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.7568072378635406}]}, {"text": "We adapt a number of existing generation approaches as baselines for this dataset.", "labels": [], "entities": []}, {"text": "Our results show there is large room for improvement in terms of both identifying relevant facts to include (knowing which claims are relevant gives a > 20% improvement in BLEU score), and generating appropriate post-modifier text for the context (providing relevant claims is not sufficient for accurate generation).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 172, "end_pos": 182, "type": "METRIC", "confidence": 0.9857184290885925}]}, {"text": "We conduct an error analysis that suggests promising directions for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of machine-in-the-loop writing systems is to assist human writers by directly augmenting their text.", "labels": [], "entities": []}, {"text": "Examples include systems that refine human text for grammar, collaborate on story plot generation systems, or modify the content for style (.", "labels": [], "entities": [{"text": "story plot generation", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7032666703065237}]}, {"text": "In this paper, we introduce Professor Melman 's arguments appealed to a wide spectrum, attracting unions like the United Automobile Workers and the Machinists Union ...", "labels": [], "entities": []}, {"text": "Noam Chomsky , , said Dr. Melman helped mobilize what once was weak and scattered resistance to war and other military operations.", "labels": [], "entities": []}, {"text": "\"The country is a lot different than it was 30 to 40 years ago, and he had a big role in that,\" Mr.  post-modifier generation as an instance of such an assistive writing task in the news domain.", "labels": [], "entities": [{"text": "post-modifier generation", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7259058654308319}]}, {"text": "Journalists use post-modifiers to introduce background information about entities discussed in news articles.", "labels": [], "entities": [{"text": "introduce background information about entities discussed in news articles", "start_pos": 34, "end_pos": 108, "type": "TASK", "confidence": 0.6248151593738132}]}, {"text": "To write these post-modifiers journalists often need to lookup relevant facts about entities.", "labels": [], "entities": []}, {"text": "A post-modifier generation system can be seen as a collaborative assistant that automatically finds relevant facts and inserts a small text fragment that augments the text produced by the human writer.", "labels": [], "entities": []}, {"text": "Post-modifier generation is a contextual data-totext generation problem, where the data is the set of known facts about the target entity, and the text to be generated is a post-modifier that is relevant to the rest of the information conveyed in the text.", "labels": [], "entities": [{"text": "Post-modifier generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6806990653276443}, {"text": "contextual data-totext generation", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7045365969340006}]}, {"text": "Given a sentence about the anti-war resistance work of Noam Chomsky, the target entity, and a set of known facts about him, the task is to generate a post-modifier that introduces Chomsky as a professor and mentions his background as an anti-war activist.", "labels": [], "entities": []}, {"text": "An effective post-modifier generation system must: (i) select suitable facts about the entity given the text, and (ii) produce text that covers these facts in away that fits in with the rest of the text.", "labels": [], "entities": []}, {"text": "We introduce PoMo, an automatically generated dataset for developing post-modifier generation systems.", "labels": [], "entities": []}, {"text": "PoMo is a collection of sentences that contain entity post-modifiers, along with a collection of facts about the entities obtained from Wikidata).", "labels": [], "entities": []}, {"text": "We use a small number of dependency patterns to automatically identify and extract post-modifiers of entities in sentences.", "labels": [], "entities": []}, {"text": "We then link the extracted entities with the entries in Wikidata.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9478676915168762}]}, {"text": "The resulting dataset has 231,057 instances covering 57,966 unique entities.", "labels": [], "entities": []}, {"text": "Our analysis show that the post-modifiers often combine multiple facts and are specific to the sentential context.", "labels": [], "entities": []}, {"text": "We conduct two sets of experiments that highlight the challenges in post-modifier generation.", "labels": [], "entities": [{"text": "post-modifier generation", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.750816822052002}]}, {"text": "(i) Claim Selection: Given an input sentence, the first step in generating a post-modifier is to figure out which facts to use.", "labels": [], "entities": []}, {"text": "We formulate this as a distantly-supervised ranking problem, where we train neural models that learn to identify relevant claims fora given sentence.", "labels": [], "entities": []}, {"text": "These claim ranking models perform well when predicting the relevance of coarse-grained facts (e.g. occupation), but fare poorly when predicting finer-grained facts (e.g. place of birth).", "labels": [], "entities": []}, {"text": "(ii) Generation: We adapt recent sequence-to-sequence generation models for this task.", "labels": [], "entities": []}, {"text": "Results show that generation remains a challenge.", "labels": [], "entities": []}, {"text": "Even though our automatic claim ranking does not improve generation, further experiments with oracle selected claims demonstrate that when relevant claims are known, the models can generate post-modifiers which humans deem comparable in quality to ones written by professional journalists.", "labels": [], "entities": []}, {"text": "In summary, the main contributions of this work are: 1) a data-to-text problem that introduces new challenges, 2) an automated dataset creation pipeline and a large resulting dataset, 3) a crowdsourcing study that verifies the contextual relevance of post-modifiers, and 4) a characterization of the difficulty of the task via performance analysis of numerous baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Post-modifier generation can be formulated as a data-to-text generation problem.", "labels": [], "entities": [{"text": "Post-modifier generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7447499334812164}, {"text": "data-to-text generation", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7842689454555511}]}, {"text": "The input is text mentioning a target entity and a set of known facts about the entity.", "labels": [], "entities": []}, {"text": "The output is a phrase that: (i) fits as a post-modifier of the target entity mentioned in the input text, and (ii) conveys a subset of facts relevant to the context of the input text.", "labels": [], "entities": []}, {"text": "shows an example for the target entity Noam Chomsky.", "labels": [], "entities": []}, {"text": "The input includes a sentence mentioning Chomsky's work on mobilizing antiwar groups along with its surrounding context, and a listing of all facts about Chomsky that are available in Wikidata.", "labels": [], "entities": []}, {"text": "Given these inputs, the task is to output a post-modifier phrase that conveys facts about Chomsky that fit within the sentence.", "labels": [], "entities": []}, {"text": "In this example the post-modifier conveys both general background information about Chomsky (his occupation), and specific information relevant to the context of the sentence (being an anti-war activist).", "labels": [], "entities": []}, {"text": "This task can be seen as an instance of collaborative writing, where the journalist writes text about specific news events involving entities, and the generation system assists the journalist by inserting new text that augments the story.", "labels": [], "entities": []}, {"text": "Given a large collection of news articles, we can automatically create training data for such systems by removing the pieces of text that we want the assistant to generate.", "labels": [], "entities": []}, {"text": "This requires reliable ways to identify text to remove and sources of information that can be used to generate the text.", "labels": [], "entities": []}, {"text": "Here we describe a pipeline for generating such a dataset for our task.", "labels": [], "entities": []}, {"text": "We construct the PoMo dataset using three different news corpora: NYTimes (Sandhaus, 2008), CNN and DailyMail (.", "labels": [], "entities": [{"text": "PoMo dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9203281104564667}, {"text": "NYTimes (Sandhaus, 2008)", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.881602942943573}, {"text": "CNN", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9183705449104309}, {"text": "DailyMail", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.9032543301582336}]}, {"text": "We use Wikidata to collect facts about entities.", "labels": [], "entities": []}, {"text": "We experiment with two types of encoder/decoder modules: bidirectional LSTMs, and transform-ers (.", "labels": [], "entities": []}, {"text": "We use a vocabulary of size 50K, truncate the maximum input sequence length to 500, and use a batch size of 32 in all experiments.", "labels": [], "entities": []}, {"text": "To help models distinguish between claims and context we demarcate claim fields with special <claim>, <key>, and <value> tokens.", "labels": [], "entities": []}, {"text": "We train all the models for 150k steps, and evaluate on the validation dataset every 10k steps.", "labels": [], "entities": []}, {"text": "Evaluation is performed using the BLEU () and METEOR () translation metrics, and Precision, Recall and F 1 score of the predicted bag-of-words (omitting stopwords).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9986104965209961}, {"text": "METEOR", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9696369767189026}, {"text": "Precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9972611665725708}, {"text": "Recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9330438375473022}, {"text": "F 1 score", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9818150997161865}]}, {"text": "The model with the highest F 1 score on the validation set is used during test time.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9895050923029581}, {"text": "validation set", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.7399950325489044}]}, {"text": "For the bidirectional LSTM, we use 2 hidden layers with 512 hidden units, 500-dimensional word embeddings, and apply dropout between layers with a keep probability of 0.7.", "labels": [], "entities": [{"text": "keep probability", "start_pos": 147, "end_pos": 163, "type": "METRIC", "confidence": 0.9721605479717255}]}, {"text": "Models are trained using stochastic gradient descent with a learning rate of 1.0.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9649387300014496}]}, {"text": "For the transformer model, we use 4 attention heads, 4 layers of transformer blocks with 64 hidden units for the encoder and the decoder, a penultimate hidden layer with 256 units, and 64-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "Transformer models are trained using Adam () with an initial learning rate of 2.0, and a label smoothing () factor of 0.1 when calculating loss.", "labels": [], "entities": []}, {"text": "We perform a variety of experiments, the results of which are displayed in.", "labels": [], "entities": []}, {"text": "In this table, Transformer and BiLSTM refer to models trained using the default approach to combining context and claims, while Tri-encoder refers to a BiLSTM model trained using the approach described in 4.1 (we do not train a transformer version since its performance is lackluster).", "labels": [], "entities": []}, {"text": "Here are detailed descriptions of the experiments performed in each section: \u2022 All Claims: Results for vanilla seq2seq models.", "labels": [], "entities": [{"text": "All Claims", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9190216362476349}]}, {"text": "\u2022 Oracle: Hard claim selection is performed using the oracle relevant claims.", "labels": [], "entities": [{"text": "Hard claim selection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6562288204828898}]}, {"text": "\u2022 Neural Ranker (n = 10): Hard claim selection is performed using the top-10 claims returned by the neural ranker baseline.", "labels": [], "entities": [{"text": "Hard claim selection", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6263519525527954}]}, {"text": "\u2022 End-to-End Claim Selection: Results for the end-to-end claim selection model.", "labels": [], "entities": []}, {"text": "In order to understand the relative contribution of the different inputs, we also include results for the BiLSTM model trained using either only the claims, or only the context sentences.", "labels": [], "entities": []}, {"text": "In and 6, we show the performances by post-modifier and sentence lengths to examine the impact of the such variables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset distribution by sources.", "labels": [], "entities": []}, {"text": " Table 2. We find that AIDA-light  agrees with our entity linking in 91.2% of the cases.  AIDA-light is able to link 94.3% of the entities we  found from NYTimes, but for CNN and DailyMail,  it links only 87.0% and 86.34% of the entities, re- spectively. This decrease is likely due to the fact  that AIDA-light was last updated in", "labels": [], "entities": [{"text": "NYTimes", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9773838520050049}, {"text": "DailyMail", "start_pos": 179, "end_pos": 188, "type": "DATASET", "confidence": 0.9067555665969849}]}, {"text": " Table 4: Distribution of the inferred occupations of the  target entities. Entities clustered by their occupation.", "labels": [], "entities": []}, {"text": " Table 5: Baseline model performance on the claim se- lection task.", "labels": [], "entities": []}, {"text": " Table 6: F 1 score of neural ranker (n = 2) on top 15  fact types.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9890256524085999}]}, {"text": " Table 7: Post modifier generation model performances  with seq2seq models. Precision, recall and F 1 scores  are computed ignoring stopwords.", "labels": [], "entities": [{"text": "Post modifier generation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5994684298833212}, {"text": "Precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9975677132606506}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.994355320930481}, {"text": "F 1 scores", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9610204497973124}]}, {"text": " Table 9: Evaluation metrics for most-common claim  baseline for different values of n.", "labels": [], "entities": []}, {"text": " Table 10: Evaluation metrics for neural baseline for dif- ferent values of n.", "labels": [], "entities": []}]}