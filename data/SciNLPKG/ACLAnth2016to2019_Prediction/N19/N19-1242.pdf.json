{"title": [{"text": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8347315788269043}, {"text": "Aspect-based Sentiment Analysis", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7135464549064636}]}], "abstractContent": [{"text": "Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.", "labels": [], "entities": [{"text": "machine reading comprehension (MRC)", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.7196018844842911}]}, {"text": "We call this problem Review Reading Comprehension (RRC).", "labels": [], "entities": [{"text": "Review Reading Comprehension (RRC)", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.5876966416835785}]}, {"text": "To the best of our knowledge, no existing work has been done on RRC.", "labels": [], "entities": [{"text": "RRC", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.6708273887634277}]}, {"text": "In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis.", "labels": [], "entities": [{"text": "RRC dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.7410719692707062}, {"text": "ReviewRC", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8234100937843323}, {"text": "aspect-based sentiment analysis", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.6414666374524435}]}, {"text": "Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.", "labels": [], "entities": [{"text": "ReviewRC", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.9401146173477173}, {"text": "aspect-based sentiment analysis", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6669053733348846}, {"text": "BERT", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.8948293328285217}]}, {"text": "To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7893782556056976}, {"text": "aspect sentiment classification", "start_pos": 146, "end_pos": 177, "type": "TASK", "confidence": 0.7100479900836945}, {"text": "aspect-based sentiment analysis", "start_pos": 181, "end_pos": 212, "type": "TASK", "confidence": 0.7140221893787384}]}, {"text": "Experimental results demonstrate that the proposed post-training is highly effective 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "For online commerce, question-answering (QA) serves either as a standalone application of customer service or as a crucial component of a dialogue system that answers user questions.", "labels": [], "entities": [{"text": "question-answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.694938026368618}]}, {"text": "Many intelligent personal assistants (such as Amazon Alexa and Google Assistant) support online shopping by allowing the user to speak directly to the assistants.", "labels": [], "entities": []}, {"text": "One major hindrance for this mode of shopping is that such systems have limited capability to answer user questions about products (or services), which are vital for customer decision making.", "labels": [], "entities": [{"text": "customer decision making", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6353667477766672}]}, {"text": "As such, an intelligent agent that can automatically answer customers' questions is very important for the success of online businesses.", "labels": [], "entities": []}, {"text": "Given the ever-changing environment of products and services, it is very hard, if not impossible, to pre-compile an up-to-date and reliable knowledge base to cover a wide assortment of questions that customers may ask, such as in factoidbased KB-QA (.", "labels": [], "entities": []}, {"text": "As a compromise, many online businesses leverage community question-answering (CQA) to crowdsource answers from existing customers.", "labels": [], "entities": []}, {"text": "However, the problem with this approach is that many questions are not answered, and if they are answered, the answers are delayed, which is not suitable for interactive QA.", "labels": [], "entities": []}, {"text": "In this paper, we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions.", "labels": [], "entities": []}, {"text": "Although there are existing studies that have used information retrieval (IR) techniques to find a whole review as the response to a user question, giving the whole review to the user is undesirable as it is quite time-consuming for the user to read it.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.822869086265564}]}, {"text": "Inspired by the success of Machine Reading Comphrenesions (MRC) (, we propose a novel task called Review Reading Comprehension (RRC) as following.", "labels": [], "entities": [{"text": "Machine Reading Comphrenesions (MRC)", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.7237865726153055}, {"text": "Review Reading Comprehension (RRC)", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.7000385224819183}]}, {"text": "Problem Definition: Given a question q = (q 1 , . .", "labels": [], "entities": []}, {"text": ", q m ) from a customer (or user) about a product and a review d = (d 1 , . .", "labels": [], "entities": []}, {"text": ", d n ) for that product containing the information to answer q, find a sequence of tokens (a text span) a = (d s , . .", "labels": [], "entities": []}, {"text": ", d e ) ind that answers q correctly, where 1 \u2264 s \u2264 n, 1 \u2264 e \u2264 n, and s \u2264 e.", "labels": [], "entities": []}, {"text": "A sample laptop review is shown in.", "labels": [], "entities": []}, {"text": "RRC poses some domain challenges compared to the traditional MRC on Wikipedia, such as the need for rich product knowledge, informal text, and fine-grained opinions (there is almost no subjective content in Wikipedia articles).", "labels": [], "entities": [{"text": "RRC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7738292813301086}]}, {"text": "Research also shows that yes/no questions are very frequent for products with complicated specifications.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no existing work has been done in RRC.", "labels": [], "entities": [{"text": "RRC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.5758552551269531}]}, {"text": "This work first builds an RRC dataset called ReviewRC, using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect-based sentiment analysis (ABSA) () in the domains of laptop and restaurant.", "labels": [], "entities": [{"text": "ReviewRC", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.762347400188446}, {"text": "SemEval 2016 Task 5 2", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.6512519478797912}, {"text": "aspect-based sentiment analysis (ABSA)", "start_pos": 129, "end_pos": 167, "type": "TASK", "confidence": 0.7928815086682638}]}, {"text": "We detail ReviewRC in Sec.", "labels": [], "entities": [{"text": "ReviewRC in Sec.", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8058166950941086}]}, {"text": "5. Given the wide spectrum of domains (types of products or services) in online businesses and the prohibitive cost of annotation, ReviewRC can only be considered to have a limited number of annotated examples for supervised training, which still leaves the domain challenges partially unresolved.", "labels": [], "entities": [{"text": "ReviewRC", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.8955222964286804}]}, {"text": "This work adopts BERT () as the base model as it achieves the state-ofthe-art performance on MRC (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.996605634689331}, {"text": "MRC", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9332672953605652}]}, {"text": "Although BERT aims to learn contextualized representations across a wide range of NLP tasks (to be task-agnostic), leveraging BERT alone still leaves the domain challenges un-resolved (as BERT is trained on Wikipedia articles and has almost no understanding of opinion text), and it also introduces another challenge of task-awareness (the RRC task), called the task challenge.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.8828974962234497}, {"text": "BERT", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9165909290313721}, {"text": "BERT", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.8885886073112488}]}, {"text": "This challenge arises when the taskagnostic BERT meets the limited number of finetuning examples in ReviewRC (see Sec.", "labels": [], "entities": [{"text": "BERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9814113974571228}, {"text": "ReviewRC", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9492309093475342}]}, {"text": "5) for RRC, which is insufficient to fine-tune BERT to ensure full task-awareness of the system 3 . To address all the above challenges, we propose a novel joint post-training technique that takes BERT's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task (MRC) knowledge before fine-tuning using the domain end task annotated data for the domain RRC.", "labels": [], "entities": [{"text": "RRC", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8417115211486816}, {"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9264035224914551}, {"text": "basic language understanding", "start_pos": 252, "end_pos": 280, "type": "TASK", "confidence": 0.6027753750483195}]}, {"text": "This technique leverages knowledge from two sources: unsupervised domain reviews and supervised (yet out-of-domain) MRC data , where the former enhances domain-awareness and the latter strengthens MRC task-awareness.", "labels": [], "entities": [{"text": "MRC task-awareness", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.8247551918029785}]}, {"text": "As a general-purpose approach, we show that the proposed method can also benefit ABSA tasks such as aspect extraction (AE) and aspect sentiment classification (ASC).", "labels": [], "entities": [{"text": "ABSA", "start_pos": 81, "end_pos": 85, "type": "TASK", "confidence": 0.9518524408340454}, {"text": "aspect extraction (AE)", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.6417745471000671}, {"text": "aspect sentiment classification (ASC)", "start_pos": 127, "end_pos": 164, "type": "TASK", "confidence": 0.7921546002229055}]}, {"text": "The main contributions of this paper are as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "We aim to answer the following research questions (RQs) in the experiment: RQ1: what is the performance gain of posttraining for each review-based task, with respect to the state-of-the-art performance?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.886511504650116}]}, {"text": "RQ2: what is the performance of BERT's pretrained weights on three review-based tasks without any domain and task adaptation?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7572425007820129}, {"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9245976209640503}]}, {"text": "RQ3: upon ablation studies of separate domain knowledge post-training and task-awareness posttraining, what is their respective contribution to the whole post-training performance gain?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9024099707603455}]}, {"text": "As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis, we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC.", "labels": [], "entities": [{"text": "RRC", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8854817748069763}, {"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9360395967960358}, {"text": "laptop and restaurant reviews of SemEval 2016 Task 5", "start_pos": 122, "end_pos": 174, "type": "DATASET", "confidence": 0.727636393573549}, {"text": "RRC", "start_pos": 212, "end_pos": 215, "type": "TASK", "confidence": 0.9077951312065125}]}, {"text": "We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review(document)-level XML tags to recover whole reviews from review sentences.", "labels": [], "entities": [{"text": "SemEval 2015 Task 12", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.4503629207611084}]}, {"text": "We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets ().", "labels": [], "entities": [{"text": "SemEval 2016 Task 5 datasets", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.7988949894905091}, {"text": "SQuAD 1.1 datasets", "start_pos": 166, "end_pos": 184, "type": "DATASET", "confidence": 0.7739381392796835}]}, {"text": "To make sure our questions are close to realworld questions, 2 annotators are first exposed to 400 QAs from CQA (under the laptop category in Amazon.com or popular restaurants in Yelp.com) to get familiar with real questions.", "labels": [], "entities": [{"text": "Yelp.com", "start_pos": 179, "end_pos": 187, "type": "DATASET", "confidence": 0.9489073157310486}]}, {"text": "Then they are asked to read reviews and independently label textual spans and ask corresponding questions when they feel the textual spans contain valuable information that customers may care about.", "labels": [], "entities": []}, {"text": "The textual spans are labeled to be as concise as possible but still human-readable.", "labels": [], "entities": []}, {"text": "Note that the annotations for sentiment analysis tasks are not exposed to annotators to avoid biased annotation on RRC.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.9479141434033712}, {"text": "RRC", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.7551687955856323}]}, {"text": "Since it is unlikely that the two annotators can label the same QAs (the same questions with the same answer spans), they further mutually check each other's annotations and disagreements are discussed until agreements are reached.", "labels": [], "entities": []}, {"text": "Annotators are encouraged to label as many questions as possible from testing reviews to get more test examples.", "labels": [], "entities": []}, {"text": "A training review is encouraged to have 2 questions (training examples) on average to have good coverage of reviews.", "labels": [], "entities": []}, {"text": "The annotated data is in the format of SQuAD 1.1 () to ensure compatibility with existing implementations of MRC models.", "labels": [], "entities": []}, {"text": "The statistics of the RRC dataset (ReviewRC) are shown in.", "labels": [], "entities": [{"text": "RRC dataset (ReviewRC)", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.8244521260261536}]}, {"text": "Since SemEval datasets do not come with a validation set, we further split 20% of reviews from the training set for validation.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 6, "end_pos": 22, "type": "DATASET", "confidence": 0.7846054136753082}]}, {"text": "Statistics of datasets for AE and ASC are given in.", "labels": [], "entities": [{"text": "AE", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9045336246490479}, {"text": "ASC", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.5701999068260193}]}, {"text": "For AE, we choose SemEval 2014 Task 4 for laptop and SemEval-2016 Task 5 for restaurant to be consistent with () and other previous works.", "labels": [], "entities": [{"text": "AE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.4933212995529175}]}, {"text": "For ASC, we use SemEval 2014 Task 4 for both laptop and restaurant as existing research frequently uses this version.", "labels": [], "entities": [{"text": "ASC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9331796765327454}, {"text": "SemEval 2014 Task 4", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.649996817111969}]}, {"text": "We use 150 examples from the training set of all these datasets for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.9687687158584595}]}, {"text": "For domain knowledge post-training, we use Amazon laptop reviews and Yelp Dataset Challenge reviews 8 . For laptop, we filtered out reviewed products that have appeared in the validation/test reviews to avoid training bias for test data (Yelp reviews do not have this issue as the source reviews of SemEval are not from Yelp).", "labels": [], "entities": [{"text": "Yelp Dataset Challenge", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.9633225798606873}, {"text": "Yelp", "start_pos": 320, "end_pos": 324, "type": "DATASET", "confidence": 0.9511632323265076}]}, {"text": "Since the number of reviews is small, we choose a duplicate factor of 5 (each review generates about 5 training examples) during BERT data pre-processing.", "labels": [], "entities": [{"text": "BERT", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.5773385763168335}]}, {"text": "This gives us 1,151,863 posttraining examples for laptop domain knowledge.", "labels": [], "entities": []}, {"text": "For the restaurant domain, we use Yelp reviews from restaurant categories that the SemEval reviews also belong to ().", "labels": [], "entities": []}, {"text": "We choose 700K reviews to ensure it is large enough to generate training examples (with a duplicate factor of 1) to coverall post-training steps that we can afford (discussed in Section 5.3) . This gives us 2,677,025 post-training examples for restaurant domain knowledge learning.", "labels": [], "entities": [{"text": "restaurant domain knowledge learning", "start_pos": 244, "end_pos": 280, "type": "TASK", "confidence": 0.6824182197451591}]}, {"text": "For MRC task-awareness post-training, we leverage SQuAD 1.1 () that come with 87,599 training examples from 442 Wikipedia articles.", "labels": [], "entities": [{"text": "MRC task-awareness post-training", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8358583052953085}]}, {"text": "To be consistent with existing research on MRC, we use the same evaluation script from SQuAD 1.1 ( for RRC, which reports Exact Match (EM) and F1 scores.", "labels": [], "entities": [{"text": "MRC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.951899528503418}, {"text": "SQuAD 1.1", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.836468517780304}, {"text": "RRC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.6525939702987671}, {"text": "Exact Match (EM)", "start_pos": 122, "end_pos": 138, "type": "METRIC", "confidence": 0.9229207873344422}, {"text": "F1 scores", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9421159029006958}]}, {"text": "EM requires the answers to have exact string match with human annotated answer spans.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5663383603096008}]}, {"text": "F1 score is the averaged F1 scores of individual answers, which is typically higher than EM and is the major metric.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9844114482402802}, {"text": "F1 scores", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.963692307472229}, {"text": "EM", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9819962978363037}]}, {"text": "Each individual F1 score is the harmonic mean of individual precision and recall computed based on the number of overlapped words between the predicted answer and human annotated answers.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.985141783952713}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9760518074035645}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9988998174667358}]}, {"text": "For AE, we use the standard evaluation scripts come with the SemEval datasets and report the F1 score.", "labels": [], "entities": [{"text": "AE", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.4760683476924896}, {"text": "SemEval datasets", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8390623927116394}, {"text": "F1 score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.982518196105957}]}, {"text": "For ASC, we compute both accuracy and Macro-F1 over 3 classes of polarities, where Macro-F1 is the major metric as the imbalanced classes introduce biases on accuracy.", "labels": [], "entities": [{"text": "ASC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9725128412246704}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993664622306824}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9956448078155518}]}, {"text": "To be consistent with existing research (, examples belonging to the conflict polarity are dropped due to a very small number of examples.", "labels": [], "entities": []}, {"text": "We set the maximum number of epochs to 4 for BERT variants, though most runs converge just within 2 epochs.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.4136095941066742}]}, {"text": "Results are reported as averages of 9 runs (9 different random seeds for random batch generation).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the ReviewRC Dataset. Reviews  with no questions are ignored.", "labels": [], "entities": [{"text": "ReviewRC Dataset", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.9749183356761932}]}, {"text": " Table 4: RRC in EM (Exact Match) and F1.", "labels": [], "entities": [{"text": "RRC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8280805945396423}, {"text": "EM", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9117546677589417}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.99677973985672}]}, {"text": " Table 5: AE in F1.", "labels": [], "entities": [{"text": "AE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991282820701599}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.4734126627445221}]}, {"text": " Table 6: ASC in Accuracy and Macro-F1(MF1).", "labels": [], "entities": [{"text": "ASC", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9013833403587341}, {"text": "Accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.971839189529419}]}]}