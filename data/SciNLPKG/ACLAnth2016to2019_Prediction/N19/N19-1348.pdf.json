{"title": [{"text": "DISCOFUSE: A Large-Scale Dataset for Discourse-Based Sentence Fusion", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentence fusion is the task of joining several independent sentences into a single coherent text.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9256978631019592}]}, {"text": "Current datasets for sentence fusion are small and insufficient for training modern neu-ral models.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8010156154632568}]}, {"text": "In this paper, we propose a method for automatically-generating fusion examples from raw text and present DISCOFUSE, a large scale dataset for discourse-based sentence fusion.", "labels": [], "entities": [{"text": "discourse-based sentence fusion", "start_pos": 143, "end_pos": 174, "type": "TASK", "confidence": 0.6077605783939362}]}, {"text": "We author a set of rules for identifying a diverse set of discourse phenomena in raw text, and decomposing the text into two independent sentences.", "labels": [], "entities": []}, {"text": "We apply our approach on two document collections: Wikipedia and Sports articles, yielding 60 million fusion examples annotated with discourse information required to reconstruct the fused text.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.9512752890586853}]}, {"text": "We develop a sequence-to-sequence model on DIS-COFUSE and thoroughly analyze its strengths and weaknesses with respect to the various discourse phenomena, using both automatic as well as human evaluation.", "labels": [], "entities": []}, {"text": "Finally, we conduct transfer learning experiments with WEB-SPLIT, a recent dataset for text simplification.", "labels": [], "entities": [{"text": "WEB-SPLIT", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.8269094824790955}, {"text": "text simplification", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8352243900299072}]}, {"text": "We show that pretraining on DISCOFUSE substantially improves performance on WEB-SPLIT when viewed as a sentence fusion task.", "labels": [], "entities": [{"text": "WEB-SPLIT", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.6545364856719971}, {"text": "sentence fusion task", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.8001770377159119}]}], "introductionContent": [{"text": "Sentence fusion is the task of combining several independent sentences into a single coherent text ().", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9293677508831024}]}, {"text": "Sentence fusion is important in many NLP applications, including retrieval-based dialogue (), text summarization () and question answering ().", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9328218996524811}, {"text": "text summarization", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7725490033626556}, {"text": "question answering", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.8833864331245422}]}, {"text": "Such systems retrieve multiple sentences from different sources, documents or paragraphs, and use them to construct a coherent text.", "labels": [], "entities": []}, {"text": "* Work done during internship at Google AI.", "labels": [], "entities": []}, {"text": "\u2020 Work done at Google AI.", "labels": [], "entities": []}, {"text": "Sentence fusion is challenging because it requires understanding the discourse semantics between the input sentences.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9414105713367462}]}, {"text": "Consider the example in: a coherent fusion of the sentences requires understanding that the second sentence contrasts the first one, in order to insert the discourse connective \"However\".", "labels": [], "entities": []}, {"text": "In addition, the gender and syntactic role of the entity \"Zeitler\" needs to be inferred to insert the pronoun \"he\".", "labels": [], "entities": []}, {"text": "Prior work on sentence fusion () utilized very small amounts of labeled data, which are insufficient to train modern neural models.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.794137716293335}]}, {"text": "In this work, we propose a method for automatically generating sentence fusion examples at scale from raw text corpora.", "labels": [], "entities": []}, {"text": "To this end, we go over sentences and contiguous pairs of sentences in a corpus, and apply a set of manually-constructed rules, which identify the occurrence of prevalent fusion operations.", "labels": [], "entities": []}, {"text": "The rules specify how to modify the sentences such that they are \"unfused\" into two independent sentences.", "labels": [], "entities": []}, {"text": "E.g., in one rule will delete the discourse connective \"However\", and another will replace the pronoun \"he\" with the named entity \"Zeitler\".", "labels": [], "entities": []}, {"text": "In the generated examples, the original fused text becomes the target, and the unfused sentences (generated by rules) are the input.", "labels": [], "entities": []}, {"text": "Importantly, sentence fusion models trained on our data cannot simply learn to invert rule application, because information is lost and can be recovered only by understanding the text semantics . As mentioned, learning to insert \"However\" in requires inferring that the sentences contrast.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7232706397771835}]}, {"text": "We cover a wide range of fusion phenomena such as inserting discourse connectives in various positions of the sentences, anaphora and cataphora identification, and sentence merging through coordination, relative clauses and apposition.", "labels": [], "entities": [{"text": "anaphora and cataphora identification", "start_pos": 121, "end_pos": 158, "type": "TASK", "confidence": 0.7042218446731567}, {"text": "sentence merging", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.7202006578445435}]}, {"text": "We applied our method on two large document collections, Wikipedia and sports articles from the Web, resulting in two datasets of 16 million and 44 million examples respectively.", "labels": [], "entities": []}, {"text": "We call the combined dataset DISCOFUSE.", "labels": [], "entities": [{"text": "DISCOFUSE", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.6905184388160706}]}, {"text": "We extensively analyze the quality of our dataset with crowdsourcing, and find that workers understand the text after splitting in 85% of the cases, and the other 15% are due to either the original text being unclear or errors in rule application.", "labels": [], "entities": []}, {"text": "We trained a state-of-the-art sequence-tosequence model ( and analyzed the fusion phenomena in which the model struggles.", "labels": [], "entities": []}, {"text": "We found that the model succeeds in fusing sentences through structural constructions such as apposition or relative clauses, but performs badly when fusion involves inserting a particular discourse connective, or selecting pronominals.", "labels": [], "entities": []}, {"text": "Last, we performed transfer learning by training on DISCOFUSE and then fine-tuning on a smaller dataset from a different distribution.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9818134903907776}, {"text": "DISCOFUSE", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8439286947250366}]}, {"text": "To this end, we utilize WEBSPLIT, a recent dataset for sentence splitting, viewing WEBSPLIT as a sentence fusion task.", "labels": [], "entities": [{"text": "WEBSPLIT", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.7523369789123535}, {"text": "sentence splitting", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7464075088500977}, {"text": "sentence fusion task", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7753166258335114}]}, {"text": "We found that pre-training on DISCOFUSE substantially improves the performance of a fusion model in this setup.", "labels": [], "entities": []}, {"text": "To conclude, our contributions are: 1.", "labels": [], "entities": []}, {"text": "DISCOFUSE: a dataset of 60 million sentence fusion examples from two different corpora.", "labels": [], "entities": [{"text": "DISCOFUSE", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7049481272697449}, {"text": "sentence fusion examples", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8086978197097778}]}, {"text": "2. A method for automatically generating sentence fusion examples from raw text.", "labels": [], "entities": [{"text": "automatically generating sentence fusion examples from raw text", "start_pos": 16, "end_pos": 79, "type": "TASK", "confidence": 0.7580932378768921}]}, {"text": "3. Automatic and human evaluation of the Transformer model on the fusion task.", "labels": [], "entities": []}, {"text": "4. A transfer learning setting in which model performance improves when pre-trained with DIS-COFUSE.", "labels": [], "entities": []}, {"text": "The DISCOFUSE dataset is publicly available at: https://discofuse.page.link/data.", "labels": [], "entities": [{"text": "DISCOFUSE dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.906315803527832}]}], "datasetContent": [{"text": "We next describe our process for building DIS-COFUSE, which contains 60 million sentence fusion examples from two different document collections: Wikipedia and Web articles about sports.: Example generation rule for apposition.", "labels": [], "entities": []}, {"text": "Given an input text and its dependency tree, we check fora match with the apposition pattern.", "labels": [], "entities": []}, {"text": "We then use the dependency tree to split the sentence and create anew example.", "labels": [], "entities": []}, {"text": "To create DISCOFUSE we retrieved the latest Wikipedia release and crawled the Web for several million sports articles.", "labels": [], "entities": []}, {"text": "Documents were annotated with dependency trees and coreference resolution using Google Cloud Natural Language.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9214346706867218}, {"text": "Google Cloud Natural Language", "start_pos": 80, "end_pos": 109, "type": "DATASET", "confidence": 0.9219750165939331}]}, {"text": "We considered each sentence and pair of consecutive sentences in each document as candidates, applying the example generation process described in Section 3.1.", "labels": [], "entities": [{"text": "example generation", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7458744049072266}]}, {"text": "Additionally, we added as examples sentence pairs from the original corpus that did not match any rule, that is (s 1 , s 2 ) = t, so that a trained model would also learn when not to change the input.", "labels": [], "entities": []}, {"text": "We filtered out examples with sentence length \u2264 6 tokens, and examples with non-ASCII characters.", "labels": [], "entities": []}, {"text": "This process resulted in 44, 177, 443 sports examples and 16, 642, 323 Wikipedia examples.", "labels": [], "entities": []}, {"text": "We randomly split these examples into 98% train, 1% dev, and 1% test sets,   making sure that each document contributes examples to only one of the split sets.", "labels": [], "entities": []}, {"text": "Like prior work (, we observed a skewed distribution of discourse phenomena in the data.", "labels": [], "entities": []}, {"text": "Specifically, examples with anaphora or the connectives \"and\" and \"but\" constitute 99.7% of Sports and 59% of Wikipedia examples.", "labels": [], "entities": [{"text": "Sports", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.934246838092804}]}, {"text": "Such a skewed distribution is likely to bias models and will fail to elucidate the ability of models to capture a wide range of linguistic phenomena.", "labels": [], "entities": []}, {"text": "Therefore, we constructed aversion of DISCOFUSE by down-sampling examples containing \"and\" and \"but\" or anaphora.", "labels": [], "entities": []}, {"text": "The downsampled dataset contains 12,080,513 Sports examples and 4,581,352 Wikipedia examples.", "labels": [], "entities": []}, {"text": "The resulting distributions of discourse types and most common connectives in the two parts of DISCOFUSE are provided in Appendix A.2.", "labels": [], "entities": []}, {"text": "We will release both the original and the downsampled versions of DISCOFUSE.: Rater evaluation understandability of the text after splitting.", "labels": [], "entities": []}, {"text": "For each example, the majority of 5 raters was taken as the final rater selection.", "labels": [], "entities": []}, {"text": "To assess the quality of the generated fusion examples in DISCOFUSE, we randomly selected 500 examples from each of the development sets of the Wikipedia and the Sports parts.", "labels": [], "entities": [{"text": "DISCOFUSE", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8761396408081055}]}, {"text": "We then conducted a crowdsourcing experiment in which each example was rated by 5 proficient English speakers, limiting each rater to at most 6 items.", "labels": [], "entities": []}, {"text": "Each rater was presented with the two independent sentences in the example and was asked to indicate whether the text is understandable.", "labels": [], "entities": []}, {"text": "If the rater answered \"yes\", she was then asked to characterize the relation between the sentences and how she would fuse them.", "labels": [], "entities": []}, {"text": "We next detail the results.: Examples for three possible reasons for not understanding the text.", "labels": [], "entities": []}, {"text": "In each example, (A) is the original text and (a) and (b) are the two sentences generated by our rules.", "labels": [], "entities": []}, {"text": "Next, we evaluated agreement on the fusion task for the 847 examples marked as understandable in Section 4.1.", "labels": [], "entities": []}, {"text": "Because there are many ways in which sentences can be fused, one cannot expect raters to produce the original text t verbatim.", "labels": [], "entities": []}, {"text": "Instead, we analyzed three central decisions and estimated whether people agree on those: (a) whether to merge the two sentences into a single one or keep them separate; (b) whether there are entities in the text that should be replaced with nominal or pronominal anaphors or cataphors; and (c) which discourse connective to add (if any).", "labels": [], "entities": []}, {"text": "For the last question, we presented raters with one connective from each of the four coarsegrained senses for discourse connectives defined by the PDTB (: comparison, expansion, contingency and temporal, as well as a no-connective option.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.8956028819084167}]}, {"text": "If the original text in the example includes a connective, we provided it as one of the options.", "labels": [], "entities": []}, {"text": "We observed a strong bias among raters towards refraining from performing any changes.", "labels": [], "entities": []}, {"text": "E.g., while only 38% of the examples did not contain a connective int, the raters chose not to add a connective in 69.2% of the cases.", "labels": [], "entities": []}, {"text": "Similarly, only in 29.1% of the examples the two sentences were not merged into a single one, while the raters chose not to merge in 53.1% of the examples.", "labels": [], "entities": []}, {"text": "Similar behavior was also observed by and.", "labels": [], "entities": []}, {"text": "We further looked at the agreement between the rater majority and the 'gold' fusion decision.", "labels": [], "entities": []}, {"text": "This analysis is shown in.", "labels": [], "entities": []}, {"text": "Agreement on merging the input sentences into one is almost random (52%), since usually both options are valid.", "labels": [], "entities": []}, {"text": "Consensus on whether to add an anaphor is higher, but not very high (63%), especially in sentences when the anaphor int is a nominal rather than a pronoun.", "labels": [], "entities": []}, {"text": "Finally, there is higher agreement on selecting the connective category (57%), for which the random baseline is 20%.", "labels": [], "entities": []}, {"text": "As mentioned, raters tend to keep the sentences unchanged.", "labels": [], "entities": [{"text": "raters", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9284155964851379}]}, {"text": "But in cases where raters agree to add a connective, agreement figures increase substantially.", "labels": [], "entities": [{"text": "agreement", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9648792147636414}]}, {"text": "Specifically, when it is clear that a connective is needed, there is also high agreement for picking the right one (76%), for deciding whether: Average agreement for each fusion decision between the gold annotation and rater majority on examples marked as understandable by the raters.", "labels": [], "entities": [{"text": "agreement", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9583650827407837}]}, {"text": "The right column considers only examples in which both the 'gold' and rater majority agreed that a connective should be added.", "labels": [], "entities": []}, {"text": "to add an anaphor (70%), and for deciding whether to merge the sentences or not (70%).", "labels": [], "entities": []}, {"text": "In this experiment, we looked at the recently released WEBSPLIT dataset 1.0 (.", "labels": [], "entities": [{"text": "WEBSPLIT dataset 1.0", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9090351661046346}]}, {"text": "It consists of examples (t, {s i } n i=1 ), where t is a sentence that verbalizes the same set of RDF triples as {s i } n i=1 . We note that WEBSPLIT was originally developed for sentence splitting, from t to {s i } n i=1 , but here we view its examples for the reverse fusion task: from {s i } n i=1 tot.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.7595539391040802}]}, {"text": "We only considered examples where {s i } n i=1 corresponds to exactly two simpler sentences (n = 2).", "labels": [], "entities": []}, {"text": "This leaves us with 135K training, 8K validation, and 8K test samples.", "labels": [], "entities": []}, {"text": "We tokenized the data using byte-pair-encoding () and compared three models: (i) The COPY baseline that concatenates the two input sentences, (ii) a model trained on WEB-SPLIT alone, and (iii) a model pre-trained on DFWIKI and fine-tuned on WEBSPLIT.", "labels": [], "entities": [{"text": "WEBSPLIT", "start_pos": 241, "end_pos": 249, "type": "DATASET", "confidence": 0.9485533237457275}]}, {"text": "For the last two models, we use the CopyNet architecture (, which is similar  to state-of-the-art models for the splitting task on WEBSPLIT (.", "labels": [], "entities": [{"text": "splitting task", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.8865844905376434}, {"text": "WEBSPLIT", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.8797218799591064}]}, {"text": "While the Transformer outperformed this model on our main experiments, here it overfit on the small training set of WEBSPLIT.", "labels": [], "entities": [{"text": "WEBSPLIT", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.7696782946586609}]}, {"text": "The training details are provided in Appendix A.3.", "labels": [], "entities": [{"text": "Appendix A.3", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.7904811799526215}]}, {"text": "shows the results of the experiment.", "labels": [], "entities": []}, {"text": "Similarly to Section 5, we measured the model performance using SARI.", "labels": [], "entities": [{"text": "SARI", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.8596194982528687}]}, {"text": "Pre-training with DFWIKI improves SARI score by 9% compared to using WEBSPLIT alone.", "labels": [], "entities": [{"text": "DFWIKI", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.6886731386184692}, {"text": "SARI score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.94949871301651}, {"text": "WEBSPLIT", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.6127326488494873}]}, {"text": "In particular, the F1 of the 'kept' and 'added' n-grams is significantly higher, by 23% and 33% respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9990843534469604}]}, {"text": "Specifically, 'added' tokens refer also to correctly choosing discourse connectives, to which the large-scale examples in DISCOFUSE were likely helpful.", "labels": [], "entities": []}, {"text": "We note that even with pre-training, the SARI 'add' score is only 10.4.", "labels": [], "entities": [{"text": "SARI 'add' score", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.9181240558624267}]}, {"text": "This is probably due to the large amount of paraphrasing done in WEB-SPLIT, which makes it problematic for fusion evaluation (see also Section 2).", "labels": [], "entities": [{"text": "WEB-SPLIT", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.8508274555206299}, {"text": "fusion evaluation", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9451569616794586}]}, {"text": "For example:  We evaluated model performance using two automatic metrics.", "labels": [], "entities": []}, {"text": "The first is Exact Match (Exact) to see how often the model generates the exact same   text as the gold fusion.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.7448877990245819}]}, {"text": "The second is SARI (, which computes the set of added, removed, and kept n-grams in the model output, comparing the output both with the gold text and the input text.", "labels": [], "entities": [{"text": "SARI", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9884318709373474}]}, {"text": "Then it computes the F 1 scores for these three sets and averages the scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9672260284423828}]}, {"text": "We compute SARI on up to 4-grams, as in.", "labels": [], "entities": [{"text": "SARI", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9832131266593933}]}, {"text": "We refrained from using metrics like BLEU because in fusion there is large overlap between the input sentences and their fused version, and such metrics do not capture well fine-grained differences of only a single word.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9932717680931091}]}, {"text": "We note that our definition of SARI 2 slightly differs from the one given by) in two aspects: (i) We define 0 0 = 1 when computing precision and recall, otherwise SARI could be less than 1 even if the output matches the gold text exactly.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9981794357299805}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9971171617507935}, {"text": "SARI", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9024955034255981}]}, {"text": "(ii) Instead of considering only the precision of deleted n-grams, we use F 1 for all three sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9976083040237427}, {"text": "F 1", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9805905818939209}]}, {"text": "Otherwise, SARI will give high scores to models that merely copy everything in the input, without even trying to infer what to delete.", "labels": [], "entities": []}, {"text": "We next turn to cross-domain evaluation.", "labels": [], "entities": [{"text": "cross-domain evaluation", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.7684667706489563}]}, {"text": "When applying a model trained on one domain to the other domain performance drops.", "labels": [], "entities": []}, {"text": "This shows that the discourse phenomena distribution differs between the domains, indicating that transfer learning is not trivial even with these large datasets.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8797529935836792}]}, {"text": "This is especially evident when applying DFWIKI to Sports, where Exact falls from 42% to 32% on the full dataset and from 50% to 40% on the downsampled one.", "labels": [], "entities": [{"text": "Sports", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9068633913993835}, {"text": "Exact", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.8457595705986023}]}, {"text": "Interestingly, when learning on the mixed training set, performance on both domains is close to in-domain performance, showing that the model has the capacity to handle both domains.", "labels": [], "entities": []}, {"text": "Finally, we take advantage of the provided annotation of the different discourse phenomena within each example in DISCOFUSE.", "labels": [], "entities": [{"text": "DISCOFUSE", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.8637634515762329}]}, {"text": "We conducted a detailed analysis of in-domain model performance by discourse type, presented in.", "labels": [], "entities": []}, {"text": "Results show that structural discourse types, such as apposition and relative clause, are easier to learn with both high exact match and SARI scores.", "labels": [], "entities": [{"text": "exact match", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.9522522687911987}, {"text": "SARI", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9540116190910339}]}, {"text": "While differences with respect to SARI scores are not large between phenomena, exact match varies more.", "labels": [], "entities": [{"text": "SARI", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.5488888621330261}, {"text": "exact match", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9518423676490784}]}, {"text": "Anaphora and verb phrase coordination are more challenging, but still require matching of the same noun (the named entity or the subject).", "labels": [], "entities": [{"text": "verb phrase coordination", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6047497193018595}]}, {"text": "On the other hand, discourse types that involve connective prediction, such as sentence coordination and discourse connective, require semantic understanding, and performance is significantly lower.", "labels": [], "entities": [{"text": "connective prediction", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7345508337020874}, {"text": "sentence coordination", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7307538539171219}]}, {"text": "In addition, when two discourse types are required for fusion, performance drops dramatically.", "labels": [], "entities": []}, {"text": "Detection for cases when model output differed from the gold, and cases when they were identical.", "labels": [], "entities": [{"text": "Detection", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.922658383846283}]}, {"text": "As our second experiment we employed crowdsourcing to test how distinguishable the fusion model outputs are from the gold fused texts.", "labels": [], "entities": []}, {"text": "Concretely, we present raters an independent sentence pair from DISCOFUSE and two fused versionsthe gold version and one generated by a model.", "labels": [], "entities": []}, {"text": "Raters were asked to detect the gold version.", "labels": [], "entities": []}, {"text": "For each example, we took the majority of 5 raters as the final choice.", "labels": [], "entities": []}, {"text": "This experiment mitigates the difficulties of automatic text generation evaluation, where many outputs are valid fora single input.", "labels": [], "entities": [{"text": "text generation evaluation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.8108819425106049}]}, {"text": "We sampled 1000 random examples from each development set of the two domains and applied the in-domain model to both.", "labels": [], "entities": []}, {"text": "The raters were presented only with examples where the model output was different from the gold fusion, and assumed 50% detection accuracy otherwise.", "labels": [], "entities": [{"text": "detection", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.7707934379577637}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8401534557342529}]}, {"text": "Out of cases when model output differed from the gold, raters were able to identify the human version in 65% of Sports examples and 61% of Wikipedia examples.", "labels": [], "entities": []}, {"text": "Looking at the entire set, humans were able to identify the human version in 57% (Sports) and 55% (Wikipedia) of the cases.", "labels": [], "entities": []}, {"text": "This shows that our Transformer model, applied over a dataset of millions of examples, is able to learn good fusions in general.", "labels": [], "entities": []}, {"text": "Nevertheless, models are still far from perfect -human accuracy is clearly better than random and this improvement is statistically significant at a level of p < 10 \u22125 for Sports and p < 10 \u22123 for Wikipedia.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9925956130027771}, {"text": "Wikipedia", "start_pos": 197, "end_pos": 206, "type": "DATASET", "confidence": 0.972027063369751}]}, {"text": "With the DISCOFUSE approach we can collect a large amount of examples automatically.", "labels": [], "entities": []}, {"text": "Still, these examples only reflect the manual rules that identify discourse phenomena.", "labels": [], "entities": []}, {"text": "We wanted to see if DISCOFUSE covers enough cases such that a trained model would be helpful for testing on fusion datasets generated by different approaches.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generated fusion examples for different phenomena. The input text is marked in uppercase blue, and the  generated sentence pair is marked in lowercase red. We show in boldface parts that allow us to detect the target  phenomenon.", "labels": [], "entities": []}, {"text": " Table 2: Rater evaluation understandability of the text  after splitting. For each example, the majority of 5  raters was taken as the final rater selection.", "labels": [], "entities": []}, {"text": " Table 5: Average agreement for each fusion decision  between the gold annotation and rater majority on ex- amples marked as understandable by the raters. The  right column considers only examples in which both  the 'gold' and rater majority agreed that a connective  should be added.", "labels": [], "entities": [{"text": "Average agreement", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8368910849094391}]}, {"text": " Table 6: Exact and SARI scores of DFSPORT, DFWIKI,  DFS+W and COPY, on the test sets of DISCOFUSE be- fore (Full) and after down-sampling (Sampled).", "labels": [], "entities": [{"text": "SARI", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9944248199462891}, {"text": "DFWIKI", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.6141301989555359}, {"text": "COPY", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.974244236946106}]}, {"text": " Table 7: In-domain evaluation with breakdown by dis- course phenomena. Performance of DFSPORT and  DFWIKI on the sports and Wikipedia development sets.", "labels": [], "entities": [{"text": "DFSPORT", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.7718854546546936}, {"text": "DFWIKI", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.7280576229095459}, {"text": "Wikipedia development sets", "start_pos": 125, "end_pos": 151, "type": "DATASET", "confidence": 0.8382260402043661}]}, {"text": " Table 8: Human detection (Detection) percentage for  DFSPORT and DFWIKI on 1000 samples from each of  the Sports and Wikipedia development sets. We report", "labels": [], "entities": [{"text": "Human detection (Detection", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6837364509701729}, {"text": "DFSPORT", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.833507239818573}, {"text": "DFWIKI", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.7596327662467957}, {"text": "Sports and Wikipedia development sets", "start_pos": 107, "end_pos": 144, "type": "DATASET", "confidence": 0.9222721457481384}]}, {"text": " Table 9: Alignment-based connective prediction accu- racy for the most common connectives. When a model  did not add a connective, the token other is used.", "labels": [], "entities": [{"text": "Alignment-based connective prediction", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.656773696343104}]}, {"text": " Table 10: Fusion results on WEBSPLIT, measured by  SARI and the F1 scores that compose it.", "labels": [], "entities": [{"text": "Fusion", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.81358402967453}, {"text": "WEBSPLIT", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7158210873603821}, {"text": "SARI", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.6477435231208801}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9984548091888428}]}, {"text": " Table 16: Most common connectives in DISCOFUSE  after down-sampling. Percentages are with respect to  the entire dataset, including examples without a con- nective.", "labels": [], "entities": []}, {"text": " Table 17: Parameters and hyperparameters of the mod- els DFSPORT, DFWIKI, DFS+W.", "labels": [], "entities": [{"text": "DFSPORT", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.7861844301223755}, {"text": "DFWIKI", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.4844754934310913}]}, {"text": " Table 18: Parameters and hyperparameters of the  CopyNet models used for transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.9472814798355103}]}]}