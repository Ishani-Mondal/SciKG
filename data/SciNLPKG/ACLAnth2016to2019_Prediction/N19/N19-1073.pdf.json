{"title": [{"text": "Serial Recall Effects in Neural Language Modeling", "labels": [], "entities": [{"text": "Serial Recall", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8575364053249359}, {"text": "Neural Language Modeling", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6805122296015421}]}], "abstractContent": [{"text": "Serial recall experiments study the ability of humans to recall words in the order in which they occurred.", "labels": [], "entities": []}, {"text": "The following serial recall effects are generally investigated in studies with humans: word length and frequency, primacy and recency, semantic confusion, repetition, and transposition effects.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8582947254180908}, {"text": "repetition", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.8781851530075073}]}, {"text": "In this research, we investigate LSTM language models in the context of these serial recall effects.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.862647533416748}]}, {"text": "Our work provides a framework to better understand and analyze neural language models and opens anew window to develop accurate language models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of language modeling is to estimate the probability of a sequence of words in natural language, typically allowing one to make probabilistic predictions of the next word given preceding ones ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7292053699493408}]}, {"text": "For several years now, Long Short-Term Memory (LSTM) language models have demonstrated state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Recent studies have begun to shed light on the information encoded by LSTM networks.", "labels": [], "entities": []}, {"text": "These models can effectively use distant history (about 200 tokens of context) and are sensitive to word order, replacement, or removal (, can learn function words much better than content words (, can remember sentence lengths, word identity, and word order (, can capture syntactic structures () such as subject-verb agreement ().", "labels": [], "entities": []}, {"text": "These characteristics are often attributed to LSTM's ability in overcoming the curse of dimensionality-by associating a distributed feature vector to each-and modeling longrange dependencies in faraway context (.", "labels": [], "entities": []}, {"text": "The goal of our research is to complement the prior work to provide a richer understanding about how LSTM language models use prior linguistic context.", "labels": [], "entities": []}, {"text": "Inspired by investigations in cognitive psychology about serial recall in humans)-where participants are asked to recall a sequence of items in order in which they were presented, we investigate how word length or frequency (wordfrequency effect), word position (primacy, recency, and transposition effects), word similarity (semantic confusion effect), and word repetition (repetition effect) influence learning in LSTM language models.", "labels": [], "entities": [{"text": "word repetition (repetition effect)", "start_pos": 358, "end_pos": 393, "type": "METRIC", "confidence": 0.6268090506394705}]}, {"text": "Our investigation provides a framework to better understand and analyze language models at a considerably finer-grained level than previous studies, and opens anew window to develop more accurate language models.", "labels": [], "entities": []}, {"text": "We find that LSTM language models (a) can learn frequent/shorter words considerably better than infrequent/longer ones, (b) can learn recent words in sequences better than words in earlier positions, 1 (c) have a tendency to predict words that are semantically similar to target words -indicating that these networks have a tendency to group semantically similar words while suggesting one specific word as target based on prior context, (d) predict as output the words that are observed in prior context, i.e. repeat words from prior context, and (e) may transpose (switch adjacent) words in output depending on word syntactic function.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two benchmark language modeling datasets: Penn Treebank (PTB) and WikiText-2 (WT2) (.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.9637364387512207}]}, {"text": "PTB and WT2 have vocabulary sizes of 10K and 33K respectively.", "labels": [], "entities": [{"text": "PTB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9665675759315491}, {"text": "WT2", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7990971803665161}]}, {"text": "We use the POS-tagged versions of these datasets provided by, and treat nouns (NN), verbs (VB), adjectives (JJ), and adverbs (RB) as content words, and others word classes as function words, see details in.", "labels": [], "entities": []}, {"text": "We set LSTM's parameters as suggested in () for PTB and follow its suggested parameter tuning procedure for WT2.", "labels": [], "entities": [{"text": "PTB", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9376011490821838}, {"text": "WT2", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9164919853210449}]}, {"text": "For both datasets, we set context size ton = 100 obtained from {5, 20, 50, 100, 200} and validation data; note that the number of samples are equal for different sequence lengths.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion matrix for repetition effect. Rows  and columns show POS tag classes of target and pre- dicted words respectively.", "labels": [], "entities": [{"text": "repetition effect", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7916947901248932}]}, {"text": " Table 2: Average distance of tokens in POS classes.", "labels": [], "entities": [{"text": "Average distance", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8189782202243805}]}, {"text": " Table 3: Confusion matrix for transpositions in PTB  at POS tag level. LSTM transposes 'RB NN' and 'JJ  VB|JJ|RB,' more than others pairs.", "labels": [], "entities": [{"text": "PTB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8049412369728088}]}]}