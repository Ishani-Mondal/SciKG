{"title": [], "abstractContent": [{"text": "Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not constrained by the semantic graph.", "labels": [], "entities": [{"text": "Generating from Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8684982880949974}]}, {"text": "To explicitly account for this underspecification, we breakdown generating from AMR into two steps: first generate a syntactic structure, and then generate the surface form.", "labels": [], "entities": []}, {"text": "We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional un-labelled data.", "labels": [], "entities": []}, {"text": "We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic annotation framework which abstracts away from the surface form of text to capture the core 'who did what to whom' structure.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8479166924953461}]}, {"text": "As a result, generating from AMR is underspecified (see for an example).", "labels": [], "entities": [{"text": "AMR", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.7114064693450928}]}, {"text": "Single-step approaches to AMR generation () therefore have to decide the syntax and surface form of the AMR realisation in one go.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9875454008579254}]}, {"text": "We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (.", "labels": [], "entities": []}, {"text": "First, we generate a delexicalised constituency structure from the AMR graph using a syntax model.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.8770803809165955}]}, {"text": "Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.855327844619751}]}, {"text": "Breaking down the AMR generation process this way provides us with several advantages: we disentangle the variance caused by the choice of syntax from that caused by the choice of words.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.9655631482601166}]}, {"text": "We can therefore realise the same AMR graph with a variety of syntactic structures by sampling from the syntax model, and deterministically decoding using the lexicalisation model.", "labels": [], "entities": []}, {"text": "We hypothesise that this generates better paraphrases of the reference realisation than sampling from a singlestep model.", "labels": [], "entities": []}, {"text": "We linearise both the AMR graphs () and constituency trees () to allow us to use sequence-to-sequence models for the syntax and lexicalisation models.", "labels": [], "entities": []}, {"text": "Further, as the AMR dataset is relatively small, we have issues with data sparsity causing poor parameter estimation for rarely seen words.", "labels": [], "entities": [{"text": "AMR dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8382290005683899}]}, {"text": "We deal with this by anonymizing named entities, and including a copy mechanism ( into our decoder, which allows open-vocabulary token generation.", "labels": [], "entities": [{"text": "open-vocabulary token generation", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.6551633377869924}]}, {"text": "We show that factorising the generation process in this way leads to improvements in AMR generation, setting anew state of the art for single-model AMR generation performance training only on labelled data.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.982142299413681}, {"text": "AMR generation", "start_pos": 148, "end_pos": 162, "type": "TASK", "confidence": 0.863229364156723}]}, {"text": "We also verify our diverse generation hypothesis with a human annotation study.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first investigate how much information AMR contains about possible syntactic realisations.", "labels": [], "entities": []}, {"text": "We train two seq2seq models of the above architecture to predict the delexicalised constituency tree of an example given either the AMR graph or the text.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 132, "end_pos": 141, "type": "DATASET", "confidence": 0.9099575281143188}]}, {"text": "We then evaluate both models on labelled and unlabelled F1 score on the dev split of the corpus.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.972174882888794}]}, {"text": "As neither model is guaranteed to produce trees with the right number of terminals, we first run an insert/delete aligner between the predicted and reference terminals (i.e. POS tags) before calculating span F1s.", "labels": [], "entities": [{"text": "F1s", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.511441171169281}]}, {"text": "We also report the results of running our aligner on the most probable parse tree as estimated by an unconditional LSTM as a baseline both to control for our aligner and also to see how much extra signal is in the AMR graph.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 214, "end_pos": 223, "type": "DATASET", "confidence": 0.8097893297672272}]}, {"text": "The results in show that predicting a syntactic structure from an AMR graph is a much harder task than predicting from the text, but there is information in the AMR graph to improve over a blind baseline.", "labels": [], "entities": [{"text": "predicting a syntactic structure", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8398066312074661}]}, {"text": "show that adding syntax into the model dramatically boosts performance, resulting in state-ofthe-art single model performance on both datasets without using external training data.", "labels": [], "entities": []}, {"text": "As an oracle experiment, we also generate from the realisation model conditioned on the ground truth parse.", "labels": [], "entities": []}, {"text": "The outstanding result here -BLEU scores in the 50s -demonstrates that being able to predict the gold reference parse tree is a bottleneck in the performance of our model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.998883068561554}]}, {"text": "However, given the inherent difficulty of predicting a single syntax realisation (cf. Section 4), we suspect that there is an intrinsic limit to how well generating from an AMR graph can replicate the reference realisation.", "labels": [], "entities": []}, {"text": "We further note that we do not use models tailored to graph-structured data or character-level features as in;, or additional unlabelled data to perform semi-supervised learning (.", "labels": [], "entities": []}, {"text": "We believe that we can improve our results even further if we use these techniques.", "labels": [], "entities": []}, {"text": "Our model explicitly disentangles variation caused by syntax choice from that caused by lexical choice.", "labels": [], "entities": []}, {"text": "This means that we can generate diverse realisations of the same AMR graph by sampling from the syntax model and deterministically decoding from the realisation model.", "labels": [], "entities": []}, {"text": "We hypothesise that this procedure generates more meaningpreserving realisations than just sampling from a straight AMR-to-text model, which can result in incoherent output.", "labels": [], "entities": []}, {"text": "We selected the first 50 AMR graphs in the dev set on linearised length between 15 and 40 with coherent reference realisations and generated 5 different realisations with our joint model and our baseline model.", "labels": [], "entities": []}, {"text": "For our joint model, we first sampled 3 parse structures from the syntax model with temperature 0.3.", "labels": [], "entities": []}, {"text": "This means we divide the pertimestep logits of the syntax decoder by 0.3; this serves to sharpen the outputs of the syntax model and constrains the sampling process to produce relatively high-probability syntactic structures for the given AMR.", "labels": [], "entities": []}, {"text": "Then, we realised each parse deterministically with the lexicalisation model.", "labels": [], "entities": []}, {"text": "For the baseline model, we sample 3 realisations from the decoder with the same temperature.", "labels": [], "entities": []}, {"text": "This gave us 100 examples in total.", "labels": [], "entities": []}, {"text": "We then crowdsourced acceptability judgments for each example from 100 annotators: we showed the reference realisation of an AMR graph, together with model realisations, and asked each annotator to mark all the grammatical realisations which have the same meaning as the reference realisation.", "labels": [], "entities": []}, {"text": "Each annotator was presented 30 examples selected randomly.", "labels": [], "entities": []}, {"text": "Our results in show that the joint model can generate more meaning-preserving realisations compared to a syntax-agnostic baseline.", "labels": [], "entities": []}, {"text": "This shows the utility of separating out syntactic and lexical variation: we model explicitly meaning-preserving invariances, and can therefore generate better paraphrases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing scores on LDC2017T10 dev set.", "labels": [], "entities": [{"text": "LDC2017T10 dev set", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9470011393229166}]}, {"text": " Table 2: Average number of acceptable realisations out  of 3. The difference is significant with p < 0.001.", "labels": [], "entities": [{"text": "Average number", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9659980535507202}]}, {"text": " Table 3: BLEU results for generation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987381100654602}]}]}