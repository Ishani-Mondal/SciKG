{"title": [{"text": "Ranking-Based Autoencoder for Extreme Multi-label Classification", "labels": [], "entities": [{"text": "Multi-label Classification", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6804282665252686}]}], "abstractContent": [{"text": "Extreme Multi-label classification (XML) is an important yet challenging machine learning task, that assigns to each instance its most relevant candidate labels from an extremely large label collection, where the numbers of labels , features and instances could be thousands or millions.", "labels": [], "entities": [{"text": "Multi-label classification (XML)", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.8553055405616761}]}, {"text": "XML is more and more on demand in the Internet industries, accompanied with the increasing business scale / scope and data accumulation.", "labels": [], "entities": [{"text": "data accumulation", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.698318138718605}]}, {"text": "The extremely large label collections yield challenges such as computational complexity, inter-label dependency and noisy labeling.", "labels": [], "entities": []}, {"text": "Many methods have been proposed to tackle these challenges, based on different mathematical formulations.", "labels": [], "entities": []}, {"text": "In this paper , we propose a deep learning XML method, with a word-vector-based self-attention, followed by a ranking-based AutoEncoder architecture.", "labels": [], "entities": []}, {"text": "The proposed method has three major advantages: 1) the autoencoder simultaneously considers the inter-label dependencies and the feature-label dependencies, by projecting labels and features onto a common embedding space; 2) the ranking loss not only improves the training efficiency and accuracy but also can be extended to handle noisy labeled data; 3) the efficient attention mechanism improves feature representation by highlighting feature importance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9971242547035217}]}, {"text": "Experimental results on benchmark datasets show the proposed method is competitive to state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our experiments are conducted on six extreme multi-label datasets and their characteristics are shown in, among which IMDb is crawled from online movie database 2 and the rest five datasets are downloaded from the extreme classification repository 3 . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.", "labels": [], "entities": [{"text": "IMDb", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.833418071269989}]}, {"text": "For all datasets, we reserve another 20% of training data as validation for tuning hyper-parameters.", "labels": [], "entities": []}, {"text": "After tuning, all models are trained on the entire training set.", "labels": [], "entities": []}, {"text": "Among these datasets, three of them are only provided with BoW feature matrix: Delicious, Mediamill (dense feature matrix extracted from image data) and RCV, which are only feasible for the non-deep learning methods (SLEEC, FastXML, PDSparse) and Rank-AE.", "labels": [], "entities": [{"text": "BoW feature matrix", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.6116574803988138}]}, {"text": "We provide both feature matrix and raw documents for IMDb, EURLex and Wiki10, which are feasible for both deep learning and non-deep learning methods.", "labels": [], "entities": [{"text": "EURLex", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9271103739738464}]}, {"text": "For those data with both formats, we remove the words from the raw documents that do not have corresponding BoW features so that the vocabulary size is the same for both deep and non-deep learning methods.", "labels": [], "entities": []}, {"text": "To evaluate the performances of each model, we adopt the metrics that have been widely used in XML: Precision at top k (P @k), and the Normalized Discounted Cummulated Gains at top k (n@k) (.", "labels": [], "entities": [{"text": "Precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.992180347442627}]}, {"text": "P @k is a measure based on the fraction of correct predictions in the top k predicted scoring labels and n@k is a normalized metric for Discounted Cumulative Gain: wherein the rank k returns k largest indices of the prediction\u02c6yprediction\u02c6 prediction\u02c6y in a descending order, and |y| is the number of positive labels in ground truth.", "labels": [], "entities": [{"text": "Discounted Cumulative Gain", "start_pos": 136, "end_pos": 162, "type": "TASK", "confidence": 0.6905305584271749}]}, {"text": "In the results, we report the average P @k and n@k on testing set with k = 1, 3, 5 respectively.", "labels": [], "entities": []}, {"text": "In Rank-AE, we use the fixed neural network architecture, with two fully connected layers in both Encoder and Decoder, and one fully connected layer following Embedding & Atten network in Feature Embedding.", "labels": [], "entities": []}, {"text": "We also fix most of the hyper-parameters, including hidden dimension h (100 for small number of labels data and 200 for large ones), word embedding size C = 100, and reduction ratio r = 4.", "labels": [], "entities": [{"text": "reduction ratio r", "start_pos": 166, "end_pos": 183, "type": "METRIC", "confidence": 0.9553646643956503}]}, {"text": "The remaining hyper-parameters, such as balance \u03bb between L hand L ae , margin min L ae , and others (decay, learning rate) in the optimization algorithms, are tuned on validation set.", "labels": [], "entities": [{"text": "balance \u03bb between L hand L ae", "start_pos": 40, "end_pos": 69, "type": "METRIC", "confidence": 0.88541122845241}, {"text": "margin min L ae", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9434435814619064}]}, {"text": "In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove 4 , a pre-trained word embeddings of 100 dimensions; if it is not, e.g. Mediamill, Delicious and RCV, a random initialization is employed.", "labels": [], "entities": [{"text": "BoW", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9017776250839233}, {"text": "Mediamill", "start_pos": 200, "end_pos": 209, "type": "DATASET", "confidence": 0.9477739334106445}]}, {"text": "For the existing methods with the same train/test split, we take the scores from the original papers for SLEEC, FastXML and PD-Sparse directly.", "labels": [], "entities": [{"text": "SLEEC", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.7440924644470215}]}, {"text": "For the new datasets and splits, the hyperparameters are tuned on the validation set for all methods, as suggested in their papers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset Characteristics: train, test, label and  feature are the numbers of training, testing, labels and  features respectively; cardinality is the average number  of label per instance; inst/L is the average number of  instances per label. * indicates only feature matrix is  provided and no raw document.", "labels": [], "entities": []}, {"text": " Table 2: Comparisons with other methods (P @k and n@k are reported in the top and bottom tables respectively).  '-' indicates unavailable due to raw documents are not available for these deep learning methods, and number in  bold is the best result in the line.", "labels": [], "entities": []}]}