{"title": [], "abstractContent": [{"text": "Self-attention networks (SANs) have drawn increasing interest due to their high paral-lelization in computation and flexibility in modeling dependencies.", "labels": [], "entities": [{"text": "Self-attention networks (SANs)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6141426265239716}]}, {"text": "SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces.", "labels": [], "entities": [{"text": "SANs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9035205245018005}]}, {"text": "In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads.", "labels": [], "entities": []}, {"text": "Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer base-line and other existing models on enhancing the locality of SANs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7415202558040619}]}, {"text": "Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Self-attention networks (SANs) ( have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation (, natural language inference (), and acoustic modeling (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.817424476146698}, {"text": "acoustic modeling", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.7223158478736877}]}, {"text": "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.", "labels": [], "entities": []}, {"text": "In addition, the performance of SANs can be improved by multi-head attention (, which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.", "labels": [], "entities": []}, {"text": "Despite their success, SANs have two major limitations.", "labels": [], "entities": [{"text": "SANs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9607844352722168}]}, {"text": "First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.", "labels": [], "entities": []}, {"text": "This work was conducted when Baosong Yang was interning at Tencent AI Lab.", "labels": [], "entities": [{"text": "Tencent AI Lab", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.581982026497523}]}, {"text": "count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (.", "labels": [], "entities": []}, {"text": "Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion, which fails to exploit useful interactions across different heads.", "labels": [], "entities": []}, {"text": "Recent work shows that better features can be learned if different sets of representations are present at feature learning time.", "labels": [], "entities": []}, {"text": "To this end, we propose novel convolutional self-attention networks (CSANs), which model locality for self-attention model and interactions between features learned by different attention heads in an unified framework.", "labels": [], "entities": []}, {"text": "Specifically, in order to pay more attention to a local part of the input sequence, we restrict the attention scope to a window of neighboring elements.", "labels": [], "entities": []}, {"text": "The localness is therefore enhanced via a parameter-free 1-dimensional convolution.", "labels": [], "entities": []}, {"text": "Moreover, we extend the convolution to a 2-dimensional area with the axis of attention head.", "labels": [], "entities": []}, {"text": "Thus, the proposed model allows each head to interact local features with its adjacent subspaces at attention time.", "labels": [], "entities": []}, {"text": "We expect that the interaction across different subspaces can further improve the performance of SANs.", "labels": [], "entities": [{"text": "SANs", "start_pos": 97, "end_pos": 101, "type": "TASK", "confidence": 0.9548673629760742}]}, {"text": "We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9177560806274414}, {"text": "WMT17 Chineseto-English", "start_pos": 117, "end_pos": 140, "type": "DATASET", "confidence": 0.8440275192260742}, {"text": "WAT17", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.817841112613678}]}, {"text": "Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model ( across language pairs.", "labels": [], "entities": []}, {"text": "Comparing with previous work on modeling locality for SANs (e.g., our model boosts performance on both translation quality and training efficiency.: Illustration of (a) vanilla SANs; (b) 1-dimensional convolution with the window size being 3; and (c) 2-dimensional convolution with the area being 3 \u00d7 3.", "labels": [], "entities": []}, {"text": "Different colors and patterns represent different subspaces modeled by multi-head attention, and transparent colors denote masked tokens that are invisible to SANs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments with the Transformer model () on English\u21d2German (En\u21d2De), Chinese\u21d2English (Zh\u21d2En) and Japanese\u21d2English (Ja\u21d2En) translation tasks.", "labels": [], "entities": [{"text": "Japanese\u21d2English (Ja\u21d2En) translation", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.6745424800448947}]}, {"text": "For the En\u21d2De and Zh\u21d2En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8809287548065186}]}, {"text": "Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.", "labels": [], "entities": [{"text": "WAT17 corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9707880914211273}]}, {"text": "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding) with 32K merge operations.", "labels": [], "entities": []}, {"text": "Following, we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.", "labels": [], "entities": []}, {"text": "Prior studies revealed that modeling locality in lower layers can achieve better performance), we applied our approach to the lowest three layers of the encoder.", "labels": [], "entities": []}, {"text": "About configurations of NMT models, we used the Base and Big settings same as, and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.", "labels": [], "entities": [{"text": "Base", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9458030462265015}]}], "tableCaptions": [{"text": " Table 1: Comparing with the existing approaches on WMT14 En\u21d2De translation task. For a fair comparison,  we re-implemented the existing locality approaches under the same framework. \"Parameter\" denotes the number  of model parameters (M = million) and \"Speed\" denotes the training speed (steps/second). \"\" column denotes  performance improvements over the Transformer baseline.", "labels": [], "entities": [{"text": "WMT14 En", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8666882216930389}, {"text": "Parameter", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9684333205223083}, {"text": "Speed", "start_pos": 254, "end_pos": 259, "type": "METRIC", "confidence": 0.9708347320556641}]}, {"text": " Table 2: Experimental results on WMT14 En\u21d2De, WMT17 Zh\u21d2En and WAT17 Ja\u21d2En test sets. \"Speed\"  denotes the training speed (steps/second). \"\u2191 / \u21d1\" indicates statistically significant difference from the vanilla  self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling", "labels": [], "entities": [{"text": "WMT14", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7897623777389526}]}]}