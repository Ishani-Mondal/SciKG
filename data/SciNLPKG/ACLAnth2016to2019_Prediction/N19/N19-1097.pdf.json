{"title": [{"text": "A Systematic Study of Leveraging Subword Information for Learning Word Representations", "labels": [], "entities": [{"text": "Learning Word Representations", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.6455920934677124}]}], "abstractContent": [{"text": "The use of subword-level information (e.g., characters, character n-grams, morphemes) has become ubiquitous in modern word representation learning.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.814789613087972}]}, {"text": "Its importance is attested especially for morphologically rich languages which generate a large number of rare words.", "labels": [], "entities": []}, {"text": "Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typo-logically diverse languages and different tasks is still missing.", "labels": [], "entities": []}, {"text": "In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models: 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations.", "labels": [], "entities": []}, {"text": "We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position em-beddings and self-attention.", "labels": [], "entities": []}, {"text": "Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (gen-eral and rare word similarity, dependency parsing , fine-grained entity typing) for 5 languages representing 3 language types.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7718155384063721}]}, {"text": "Our main results clearly indicate that there is no \"one-size-fits-all\" configuration, as performance is both language-and task-dependent.", "labels": [], "entities": []}, {"text": "We also show that configurations based on unsupervised seg-mentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.7386163473129272}]}], "introductionContent": [{"text": "Word representations are central to a wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Standard word representation models are based on the distributional hypothesis and induce representations from large unlabeled corpora using word co-occurrence statistics ().", "labels": [], "entities": []}, {"text": "However, as pointed out by recent work (, mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which leads to several fundamental limitations.", "labels": [], "entities": []}, {"text": "The standard approaches ignore the internal structure of words, that is, the syntactic or semantic composition from subwords or morphemes to words, and are incapable of parameter sharing at the level of subword units.", "labels": [], "entities": []}, {"text": "Assigning only a single vector to each word causes the data sparsity problem, especially in resource-poor settings where huge amounts of training data cannot be guaranteed.", "labels": [], "entities": []}, {"text": "The issue is also prominent for morphologically rich languages (e.g., Finnish) with productive morphological systems that generate a large number of infrequent/rare words.", "labels": [], "entities": []}, {"text": "Although potentially useful information on word relationships is hidden in their internal subwordlevel structure, 1 subword-agnostic word representation models do not take these structure features into account and are effectively unable to represent rare words accurately, or unseen words at all.", "labels": [], "entities": []}, {"text": "Therefore, there has been a surge of interest in subword-informed word representation architectures aiming to address these gaps.", "labels": [], "entities": []}, {"text": "A large number of architectures has been proposed in related research, and they can be clustered over the two main axes (  Figure 1: Illustration of the general framework for learning subword-informed word representations, with the focus on two crucial components: 1) segmentation of words and 2) subword embedding composition.", "labels": [], "entities": []}, {"text": "By varying the two components, and optionally including or excluding position embeddings from the computations, we obtain a wide spectrum of different subwordinformed configurations used in the study (see \u00a72).", "labels": [], "entities": []}, {"text": "Our word-level representation model in this work is skipgram (on the top layer of the figure), but it can be replaced by any other distributional word-level model..", "labels": [], "entities": [{"text": "skipgram", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9609399437904358}]}, {"text": "First, the models differ in the chosen method for segmenting words into subwords.", "labels": [], "entities": []}, {"text": "The methods range from fully supervised approaches to e.g. unsupervised approaches based on BPE.", "labels": [], "entities": [{"text": "BPE", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.7210364937782288}]}, {"text": "Second, another crucial aspect is the subword composition function used to obtain word embeddings from the embeddings of each word's constituent subword units.", "labels": [], "entities": [{"text": "subword composition", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7649073004722595}]}, {"text": "Despite a steadily increasing interest in such subwordinformed word representations, their systematic comparative analysis across the two main axes, as well as across typologically diverse languages and different tasks is still missing.", "labels": [], "entities": []}, {"text": "In this work, we conduct a systematic study of a variety of subword-informed word representation architectures that all can be described by a general framework illustrated by.", "labels": [], "entities": []}, {"text": "The framework enables straightforward experimentation with prominent word segmentation methods (e.g., BPE, Morfessor, supervised segmentation systems) as well as subword composition functions (e.g., addition, self-attention), resulting in a large number of different subword-informed configurations.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7392895817756653}]}, {"text": "Our study aims at providing answers to the following crucial questions: Q1) How generalizable are subword-informed models across typologically diverse languages and across different downstream tasks?", "labels": [], "entities": []}, {"text": "Do different languages and tasks require different configurations to reach peak performances or is there a single best-performing configuration?", "labels": [], "entities": []}, {"text": "Q2) How important is it to choose an appropriate segmentation and composition method?", "labels": [], "entities": [{"text": "segmentation and composition", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7881388068199158}]}, {"text": "How effective are more generally applicable unsupervised segmentation methods?", "labels": [], "entities": []}, {"text": "Is it always better to resort to a supervised method, if available?", "labels": [], "entities": []}, {"text": "Q3) Is there a difference in performance with and without the full word representation?", "labels": [], "entities": []}, {"text": "Can more advanced techniques based on position embeddings and selfattention yield better task performance?", "labels": [], "entities": []}, {"text": "We evaluate subword-informed word representation configurations originating from the general framework in three different tasks using standard benchmarks and evaluation protocols: 1) general and rare word similarity and relatedness, 2) dependency parsing, and 3) fine-grained entity typing for 5 languages representing 3 language families (fusional, introflexive, agglutinative).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 236, "end_pos": 254, "type": "TASK", "confidence": 0.7913706302642822}]}, {"text": "We show that different tasks and languages indeed require diverse subword-informed configurations to reach peak performance: this calls fora more careful languageand task-dependent tuning of configuration components.", "labels": [], "entities": []}, {"text": "We also show that more sophisticated configurations are particularly useful for representing rare words, and that unsupervised segmentation methods can be competitive to supervised segmentation in tasks such as parsing or fine-grained entity typing.", "labels": [], "entities": []}, {"text": "We hope that this paper will provide useful points of comparison and comprehensive guidance for developing next-generation subword-informed word representation models for typologically diverse languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train different subword-informed model configurations on 5 languages representing 3 morphological language types: English (EN), German (DE), Finnish (FI), Turkish (TR) and Hebrew (HE), see.", "labels": [], "entities": []}, {"text": "We then evaluate the resulting subwordinformed word embeddings in three distinct tasks: 1) general and rare word similarity and relatedness, 2) syntactic parsing, and 3) fine-grained entity typing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7366777956485748}, {"text": "fine-grained entity typing", "start_pos": 170, "end_pos": 196, "type": "TASK", "confidence": 0.5789143840471903}]}, {"text": "The three tasks have been selected in particular as they require different degrees of syntactic and semantic information to be stored in the input word embeddings, ranging from a purely semantic task (word similarity) over a hybrid syntactic-semantic task of entity typing to syntactic parsing.", "labels": [], "entities": [{"text": "word similarity)", "start_pos": 201, "end_pos": 217, "type": "TASK", "confidence": 0.740096241235733}, {"text": "syntactic parsing", "start_pos": 276, "end_pos": 293, "type": "TASK", "confidence": 0.7742882668972015}]}, {"text": "Subword-Informed Configurations We train a large number of subword-informed configurations by varying the segmentation method \u03b4 ( \u00a72.2), subword embeddings W s , the inclusion of position embeddings W p and the operations on W s ( \u00a72.3), and the composition functions f \u0398 ( \u00a72.4).", "labels": [], "entities": []}, {"text": "The configurations are based on the following variations of constituent components: (1) For the segmentation \u03b4, we test a supervised morphological system CHIP-MUNK (sms), Morfessor (morf) and BPE (bpe).", "labels": [], "entities": [{"text": "BPE", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.988657534122467}]}, {"text": "A word token can be optionally inserted into the subword sequence S w for all three segmentation methods (ww) or left out (w-); (2) We can only embed the subword s for morf and bpe, while with sms we can optionally embed the concatenation of the subword and its morphotactic tag s : t (st); 10 (3) We test subword embedding learning without position embeddings (p-), or we integrate them using addition (pp) or element-wise multiplication (mp); (4) For the composition function function f \u0398 , we experiment with addition (add), single head selfattention (att), and multi-head self-attention (mtx).", "labels": [], "entities": []}, {"text": "provides an overview of all components used to construct a variety of subword-informed configurations used in our evaluation.", "labels": [], "entities": []}, {"text": "The variations of components from yield 24 different configurations in total for sms, and 18 for morf and bpe.", "labels": [], "entities": []}, {"text": "We use pretrained CHIPMUNK models for all test languages except for Hebrew, as Hebrew lacks gold segmentation data.", "labels": [], "entities": []}, {"text": "Following, we use the default parameters for Morfessor, and 10k merge operations for BPE across languages.", "labels": [], "entities": [{"text": "Morfessor", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8143556714057922}]}, {"text": "We use available BPE models pre-trained on Wikipedia by.", "labels": [], "entities": []}, {"text": "Two well-known word representation models, which can also be described by the general framework from, are used as insightful baselines: the subword-agnostic SGNS model () and fastText (FT).", "labels": [], "entities": [{"text": "fastText (FT)", "start_pos": 175, "end_pos": 188, "type": "METRIC", "confidence": 0.7905465811491013}]}, {"text": "FT computes the target word embedding using addition as the composition function, while the segmentation is straightforward: the model simply generates all character n-grams of length 3 to 6 and adds them to S w along with the full word.", "labels": [], "entities": [{"text": "FT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8387155532836914}]}, {"text": "Training Setup Our training data for all languages is Wikipedia.", "labels": [], "entities": []}, {"text": "We lowercase all text and replace all digits with a generic tag #.", "labels": [], "entities": []}, {"text": "The statistics of the training corpora are provided in.", "labels": [], "entities": []}, {"text": "All subword-informed variants are trained on the same data and share the same parameters for the SGNS model.", "labels": [], "entities": []}, {"text": "Further, we use ADAGRAD (Duchi Once st is applied, we do not use position embeddings anymore, because the morphotactic tags are already encoded in subword embeddings, i.e., stand pp are mutually exclusive.", "labels": [], "entities": [{"text": "ADAGRAD", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9838790893554688}]}, {"text": "et al., 2011) with a linearly decaying learning rate, and do a grid search of learning rate and batch size for each \u03b4 on the German 14 WordSim-353 data set (WS;).", "labels": [], "entities": [{"text": "German 14 WordSim-353 data set (WS", "start_pos": 125, "end_pos": 159, "type": "DATASET", "confidence": 0.9190506339073181}]}, {"text": "The hyperparameters are then fixed for all other languages and evaluation runs.", "labels": [], "entities": []}, {"text": "Finally, we set the learning rate to 0.05 for sms and bpe, and 0.075 for morf, and the batch size to 1024 for all the settings.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.961882621049881}]}, {"text": "Word Similarity and Relatedness These standard intrinsic evaluation tasks test the semantics of word representations (   Fine-Grained Entity Typing The task is to map entities, which could comprise more than one entity token, to predefined entity types (.", "labels": [], "entities": [{"text": "Word Similarity and Relatedness", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7440917119383812}]}, {"text": "It is a suitable semi-semantic task to test our subword models, as the subwords of entities usually carry some semantic information from which the entity types can be inferred.", "labels": [], "entities": []}, {"text": "For example, Lincolnshire will belong to /location/county as -shire is a suffix that strongly indicates a location.", "labels": [], "entities": []}, {"text": "We rely on an entity typing dataset of built for over 250 languages by obtaining entity mentions from Wikidata and their associated FIGER-based entity types (: there only exists a one-to-one mapping between the entity and one of the 112 FIGER types.", "labels": [], "entities": []}, {"text": "We randomly sample the data to obtain a train/dev/test split with the size of 60k/20k/20k for all languages.", "labels": [], "entities": []}, {"text": "For evaluation we extend the RNNbased model of, where they stacked all the subwords of entity tokens into a flattened sequence: we use the hierarchical embedding composition instead.", "labels": [], "entities": []}, {"text": "For each entity token, we first compute its word embeddings with our subword configurations, 16 then feed the word embeddings of entity tokens to a bidirectional LSTM with 2 hidden layers of size 512, followed by a projection layer which predicts the entity type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results on word similarity and relatedness across languages. The highest score for each row is in bold,  and we choose randomly in case of a tie. All scores are obtained after computing the embeddings of OOV words.", "labels": [], "entities": []}, {"text": " Table 5: Results on the dependency parsing task. The  two best configurations are selected according to LAS.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8738726377487183}, {"text": "LAS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9566302299499512}]}, {"text": " Table 6: Test accuracy, the evaluation metric used by Heinzerling and Strube (2018), on the fine-grained entity  typing task. The results are averaged over 5 runs with random seeds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.973899781703949}, {"text": "entity  typing task", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7278483907381693}]}, {"text": " Table 7: Results on word similarity and relatedness across languages for CHIPMUNK (sms). All scores are ob- tained after computing the embeddings of OOV words.", "labels": [], "entities": []}, {"text": " Table 8: Results on word similarity and relatedness across languages for Morfessor (morf). All scores are obtained  after computing the embeddings of OOV words.", "labels": [], "entities": []}, {"text": " Table 9: Results on word similarity and relatedness across languages for BPE (bpe). All scores are obtained after  computing the embeddings of OOV words.", "labels": [], "entities": []}, {"text": " Table 10: Results on dependency parsing across languages for CHIPMUNK (sms).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8379160463809967}]}, {"text": " Table 11: Results on dependency parsing across languages for Morfessor (morf).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8255003988742828}]}, {"text": " Table 12: Results on dependency parsing across languages for BPE (bpe).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8022197186946869}]}, {"text": " Table 13: Accuracy on fine-grained entity typing across languages.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.985960841178894}]}]}