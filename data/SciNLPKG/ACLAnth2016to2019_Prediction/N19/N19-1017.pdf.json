{"title": [], "abstractContent": [{"text": "Languages evolve and diverge overtime.", "labels": [], "entities": []}, {"text": "Their evolutionary history is often depicted in the shape of a phylogenetic tree.", "labels": [], "entities": []}, {"text": "Assuming parsing models are representations of their languages grammars, their evolution should follow a structure similar to that of the phylo-genetic tree.", "labels": [], "entities": []}, {"text": "In this paper, drawing inspiration from multi-task learning, we make use of the phylogenetic tree to guide the learning of multilingual dependency parsers leverag-ing languages structural similarities.", "labels": [], "entities": []}, {"text": "Experiments on data from the Universal Dependency project show that phylogenetic training is beneficial to low resourced languages and to well furnished languages families.", "labels": [], "entities": [{"text": "phylogenetic training", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.916473776102066}]}, {"text": "As aside product of phylogenetic training, our model is able to perform zero-shot parsing of previously unseen languages.", "labels": [], "entities": [{"text": "zero-shot parsing of previously unseen languages", "start_pos": 72, "end_pos": 120, "type": "TASK", "confidence": 0.7948806335528692}]}], "introductionContent": [{"text": "Languages change and evolve overtime.", "labels": [], "entities": []}, {"text": "A community that spoke once a single language can be split geographically or politically, and if the separation is long enough their language will diverge in direction different enough so that at some point they might not be intelligible to each other.", "labels": [], "entities": []}, {"text": "The most striking differences between related languages are often of lexical and phonological order but grammars also changeover time.", "labels": [], "entities": []}, {"text": "Those divergent histories are often depicted in the shape of a tree in which related languages whose common history stopped earlier branch off higher than languages that have shared a longer common trajectory.", "labels": [], "entities": []}, {"text": "We hypothesize that building on this shared history is beneficial when learning dependency parsing models.", "labels": [], "entities": [{"text": "learning dependency parsing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6427973806858063}]}, {"text": "We thus propose to use the phylogenetic structure to guide the training of multi-lingual graph-based neural dependency parsers that will tie parameters between languages according to their common history.", "labels": [], "entities": [{"text": "multi-lingual graph-based neural dependency parsers", "start_pos": 75, "end_pos": 126, "type": "TASK", "confidence": 0.6522488713264465}]}, {"text": "As our phylogenetic learning induces parsing models for every inner node in the phylogenetic tree, it can also perform zero-shot dependency parsing of unseen languages.", "labels": [], "entities": [{"text": "zero-shot dependency parsing of unseen languages", "start_pos": 119, "end_pos": 167, "type": "TASK", "confidence": 0.7555514971415201}]}, {"text": "Indeed, one can use the model of the lowest ancestor (in the tree) of anew language as an approximation of that language grammar.", "labels": [], "entities": []}, {"text": "We assess the potential of phylogenetic training with experiments on data from the Universal Dependencies project version 2.2.", "labels": [], "entities": [{"text": "phylogenetic training", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.9666740894317627}]}, {"text": "Our results show that parsers indeed benefit from this multi-lingual training regime as models trained with the phylogenetic tree outperform independently learned models.", "labels": [], "entities": []}, {"text": "The results on zero-shot parsing show that a number of factors such as the genre of the data and the writing system have a significant impact on the quality of the analysis of an unseen language, with morphological analysis being of great help.", "labels": [], "entities": [{"text": "zero-shot parsing", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7027744948863983}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents both the neural parsing model as well as the phylogenetic training procedure.", "labels": [], "entities": [{"text": "neural parsing", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7390556931495667}]}, {"text": "Section 3 presents some experiments over data from UD 2.2.", "labels": [], "entities": [{"text": "UD 2.2", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.7011900842189789}]}, {"text": "Section 4 presents some related works on multi-task learning and multilingual parsing.", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7337096631526947}]}, {"text": "Finally, Section 5 closes the paper and gives some future perspectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the potential of phylogenetic training both in terms of multi-task learning and zero-shot parsing capabilities, we experimented with data from the Universal Dependencies project version 2.2 (.", "labels": [], "entities": [{"text": "phylogenetic training", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.9437400996685028}, {"text": "zero-shot parsing", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.6871108263731003}]}, {"text": "When several corpora are available fora language, we chose one to keep a good balance between morphological annotation and number of sentences.", "labels": [], "entities": []}, {"text": "For example, the Portuguese GSD treebank has slightly more sentences than the Bosque treebank but it is not well morphologically annotated.", "labels": [], "entities": [{"text": "Portuguese GSD treebank", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.829992949962616}, {"text": "Bosque treebank", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9262210726737976}]}, {"text": "The zero-shot parsing models have been directly tested on languages that lack of training set.", "labels": [], "entities": [{"text": "zero-shot parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6357129514217377}]}, {"text": "The treebanks names are given in the tree 4 and the result table 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results for languages with a training set  for phylogenetic models and independent models. The  training set size of languages without a developpement  set are reported in brackets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of languages without a training set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9701743721961975}]}]}