{"title": [{"text": "Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6625919491052628}]}], "abstractContent": [{"text": "Fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains.", "labels": [], "entities": []}, {"text": "Ina standard fine-tuning scheme, source and target problems are trained using the same architecture.", "labels": [], "entities": []}, {"text": "Although capable of adapting to new domains , pre-trained units struggle with learning uncommon target-specific patterns.", "labels": [], "entities": []}, {"text": "In this paper , we propose to augment the target-network with normalised, weighted and randomly ini-tialised units that beget a better adaptation while maintaining the valuable source knowledge.", "labels": [], "entities": []}, {"text": "Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.", "labels": [], "entities": [{"text": "POS tagging of social media texts", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8785790403683981}]}], "introductionContent": [{"text": "POS tagging is a sequence labelling problem, that consists on assigning to each sentence' word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8153460025787354}]}, {"text": "Such information is useful for higher-level applications, such as machine-translation or cross-lingual information retrieval (.", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.6313036878903707}]}, {"text": "One of the best approaches for POS tagging of social media text (, is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest (target-dataset).", "labels": [], "entities": [{"text": "POS tagging of social media text", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.9348692099253336}]}, {"text": "While this approach is known to be very effective, because it takes benefit from pre-trained neurons, it has one main drawback by design.", "labels": [], "entities": []}, {"text": "Indeed, it has been shown in computervision () that, when fine-tuning on scenes a model pre-trained on objects, it is the neuron firing on the white dog object that became highly sensitive to the white waterfall scene.", "labels": [], "entities": []}, {"text": "Simply said, pre-trained neurons are biased by what they have learned in the source-dataset.", "labels": [], "entities": []}, {"text": "This is: Given a word representation xi , a BiLSTM (\u03a6) models the sequence, and a FC layer (\u03a8) performs classification.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8660834431648254}]}, {"text": "In standard fine-tuning, the units are pre-trained on a large source-dataset then adapted to the target one.", "labels": [], "entities": []}, {"text": "In this work, we propose to add randomly initialised units (green branch) and jointly adapt them with pre-trained ones (gray branch).", "labels": [], "entities": []}, {"text": "An elementwise sum is further applied to merge the two branches.", "labels": [], "entities": []}, {"text": "Before merging, we balance the different behaviours of pre-trained and random units, using an independent normalisation (N ).", "labels": [], "entities": [{"text": "independent normalisation (N )", "start_pos": 94, "end_pos": 124, "type": "METRIC", "confidence": 0.7923773050308227}]}, {"text": "Finally we let the network learn which of pre-trained or random neurons are more suited for every class, by adding learnable weighting vectors (u and v initialised with 1-values) on the FC layers.", "labels": [], "entities": []}, {"text": "also the case on NLP (see experiments).", "labels": [], "entities": []}, {"text": "Consequently, pre-trained units struggle with learning patterns specific to the target-dataset (e.g., \"wanna\" or \"gonna\" in the Tweets domain).", "labels": [], "entities": []}, {"text": "This last is non-desirable, since it has been shown recently () that such specific units are important for performance.", "labels": [], "entities": []}, {"text": "To overcome this drawback, one can propose to take benefit from randomly initialised units, that are by design nonbiased.", "labels": [], "entities": []}, {"text": "However, it is common to face small target-datasets that contain too few data to learn such neurons from scratch.", "labels": [], "entities": []}, {"text": "Hence, in such setting, it is hard to learn random units that fire on specific patterns and generalise well.", "labels": [], "entities": []}, {"text": "In this article, we propose a hybrid method that takes benefit from both worlds, without their drawbacks.", "labels": [], "entities": []}, {"text": "It consists in augmenting the source-network (set of pre-trained units) with randomly initialised units and jointly learn them.", "labels": [], "entities": []}, {"text": "We call our method PretRand (Pretrained and Random units) and illustrate it in.", "labels": [], "entities": [{"text": "PretRand", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8132994771003723}]}, {"text": "The main difficulty is forcing the network to consider random units, because they have different behaviours than pretrained ones.", "labels": [], "entities": []}, {"text": "Indeed, while these last strongly fire discriminatively on many words, these first do not fire on any word at the initial stage of fine-tuning.", "labels": [], "entities": []}, {"text": "Therefore, random units do not significantly contribute to the computation of gradients and are thus slowly updated.", "labels": [], "entities": []}, {"text": "To overcome this problem, we proposed to independently normalise pre-trained and random layers.", "labels": [], "entities": []}, {"text": "This last balances their range of activations and thus forces the network to consider them, both.", "labels": [], "entities": []}, {"text": "Last but not least, we do not know which of pre-trained and random units are the best for every class-predictor, thus we propose to learn weighting vectors on top of each branch.", "labels": [], "entities": []}, {"text": "Evaluation was carried on 3 POS tagging Tweets datasets in a transfer-learning setting.", "labels": [], "entities": [{"text": "POS tagging Tweets datasets", "start_pos": 28, "end_pos": 55, "type": "DATASET", "confidence": 0.7840172350406647}]}, {"text": "Our method outperforms SOTA methods and significantly surpasses fairly comparable baselines.", "labels": [], "entities": [{"text": "SOTA", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9139074087142944}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of tokens in every used dataset.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of our method to state-of-the-art (top) and baselines (bottom) in terms of token-level accuracy  (in %) on 3 Tweets datasets. Note that, baselines are more fairly comparable to our method. In the second and last  columns, we respectively highlighted the number of parameters and the average performance on the 3 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8599764704704285}]}, {"text": " Table 3: Ablation study. Token level accuracy (in %)  when progressively ablating PretRand components.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9927123785018921}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7407739162445068}]}]}