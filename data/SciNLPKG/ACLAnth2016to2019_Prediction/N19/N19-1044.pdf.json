{"title": [{"text": "Code-Switching for Enhancing NMT with Pre-Specified Translation", "labels": [], "entities": []}], "abstractContent": [{"text": "Leveraging user-provided translation to constrain NMT has practical significance.", "labels": [], "entities": []}, {"text": "Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding.", "labels": [], "entities": []}, {"text": "Both methods can hurt translation fidelity for various reasons.", "labels": [], "entities": [{"text": "translation fidelity", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.973044365644455}]}, {"text": "We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations.", "labels": [], "entities": []}, {"text": "Our method does not change the NMT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words.", "labels": [], "entities": []}, {"text": "Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.", "labels": [], "entities": [{"text": "translation of constrained words", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.8664622008800507}]}], "introductionContent": [{"text": "One important research question in domainspecific machine translation () is how to impose translation constraints (.", "labels": [], "entities": [{"text": "domainspecific machine translation", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.6345540285110474}]}, {"text": "As shown in (a), the word \"breadboard\" can be translated into \"\u5207\u9762 \u5305\u677f (a wooden board that is used to cut bread on)\" in the food domain, but \"\u7535 \u8def \u677f (a construction base for prototyping of electronics)\" in the electronic domain.", "labels": [], "entities": []}, {"text": "To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8521648049354553}]}, {"text": "We investigate the method of leveraging pre-specified translation for NMT using such a lexicon.", "labels": [], "entities": []}, {"text": "For leveraging pre-specified translation, one existing approach uses placeholder tags to substitute named entities () or rare words (Luong et al., ) on both the source and target sides during training, so that a model can translate such words by learning to translate placeholder tags.", "labels": [], "entities": []}, {"text": "For example, the i-th named entity in the source sentence is replaced with \"tag i \", as well as its corresponding translation in the target side.", "labels": [], "entities": []}, {"text": "Placeholder tags in the output are replaced with pre-specified translation as a post-processing step.", "labels": [], "entities": []}, {"text": "One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output.", "labels": [], "entities": []}, {"text": "Another approach imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding.", "labels": [], "entities": []}, {"text": "This method ensures that pre-specified translations appear in the output.", "labels": [], "entities": []}, {"text": "A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity).", "labels": [], "entities": []}, {"text": "There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows.", "labels": [], "entities": []}, {"text": "We investigate a novel method based on data augmentation, which combines the advantages of both methods above.", "labels": [], "entities": []}, {"text": "The idea is to construct synthetic parallel sentences from the original paral-lel training data.", "labels": [], "entities": []}, {"text": "The synthetic sentence pairs resemble code-switched source sentences and their translations, where certain source words are replaced with their corresponding target translations.", "labels": [], "entities": []}, {"text": "The motivation is to make the model learn to \"translate\" embedded pre-specified translations by copying them from the modified source.", "labels": [], "entities": []}, {"text": "During decoding, the source is similarly modified as a preprocessing step.", "labels": [], "entities": []}, {"text": "As shown in, translation is executed over the code-switched source, without further constraints or post-processing.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9763956069946289}]}, {"text": "In contrast to the placeholder method, our method keeps lexical semantic information (i.e. target words v.s. placeholder tags) in the source, which can lead to more adequate translations.", "labels": [], "entities": []}, {"text": "Compared with the lexical constraint method, prespecified translation is learned because such information is available both in training and decoding.", "labels": [], "entities": []}, {"text": "As a data augmentation method, it can be used on any NMT architecture.", "labels": [], "entities": []}, {"text": "In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences.", "labels": [], "entities": [{"text": "translate code-switched source sentences", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.8512769043445587}, {"text": "translating un-replaced sentences", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.8279114961624146}]}, {"text": "To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system.", "labels": [], "entities": [{"text": "copying", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9832891821861267}]}, {"text": "Second, we integrate pointer network ( into the decoder.", "labels": [], "entities": []}, {"text": "The copy mechanism was firstly proposed to copy source words.", "labels": [], "entities": []}, {"text": "In our method, it is further used to copy source-side target words.", "labels": [], "entities": []}, {"text": "Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer () model on various test sets across different domains.", "labels": [], "entities": []}, {"text": "We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism.", "labels": [], "entities": []}, {"text": "We release four high-quality En-Ru e-commerce test sets translated by Russian language experts, totalling 7169 sentences with an average length of 21 1 .", "labels": [], "entities": [{"text": "En-Ru e-commerce test sets translated", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.8704622983932495}]}], "datasetContent": [{"text": "We compare our method with strong baselines on large-scale En-Ru and Ch-En tasks on various test sets across different domains, using a strongly optimized Transformer ().", "labels": [], "entities": []}, {"text": "BLEU () is used for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9760357141494751}]}, {"text": "We use six self-attention layers for both the encoder and the decoder.", "labels": [], "entities": []}, {"text": "The embedding size and the hidden size are set to 512.", "labels": [], "entities": []}, {"text": "Eight heads are used for self-attention.", "labels": [], "entities": []}, {"text": "A feed-forward layer with 2048 cells and Swish () is used as the activation function.) is used for training; warmup step is 16000; the learning rate is 0.0003.", "labels": [], "entities": []}, {"text": "We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out ( probabilities are set to 0.1.", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7277560532093048}]}, {"text": "We extract a SMT phrase table on the bilingual training corpus by using moses () with default setting, which is used for matching sentence pairs to generate augmented training data.", "labels": [], "entities": [{"text": "SMT phrase", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.8859353363513947}]}, {"text": "We apply count-based pruning () to the phrase table, the threshold is set to 10.", "labels": [], "entities": []}, {"text": "During decoding, similar to, and Post and Vilar (2018), we make use of references to obtain gold constraints.", "labels": [], "entities": []}, {"text": "Following previous work, prespecified translations for each source sentence are sampled from references and used by all systems for fair comparison.", "labels": [], "entities": []}, {"text": "In all the baseline systems, the vocabulary size is set to 50K on both sides.", "labels": [], "entities": []}, {"text": "For \"Data augmentation\", to allow the source-side dictionary to cover target-side words, the target-and source-side vocabularies are merged fora new source vocabulary.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7195821553468704}]}, {"text": "For \"Shared embeddings\", the source vocabulary remains the same as the baselines, where the source-side target words use embeddings from target-side vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on En-Ru, one or two source phrases of each sentence have pre-specified translation. \"Trans- former\" is our in-house vanilla Transformer baseline. \"Marian\" is the implementation of Transformer by Junczys- Dowmunt et al. (2018), which is used as a reference of our Transformer implementation.", "labels": [], "entities": []}, {"text": " Table 2: Results on Ch-En, one or two source phrases of each sentence have pre-specified translation.", "labels": [], "entities": []}, {"text": " Table 3: Decoding speed (words/sec), Ch-En dev set.", "labels": [], "entities": []}, {"text": " Table 4: Copy success rate on Ch-En test sets.", "labels": [], "entities": [{"text": "Copy success rate", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.935521682103475}, {"text": "Ch-En test sets", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.7482206722100576}]}, {"text": " Table 5: BLEU scores of non code-switched (original)  input on En-Ru test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985516667366028}, {"text": "En-Ru test sets", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9248639345169067}]}]}