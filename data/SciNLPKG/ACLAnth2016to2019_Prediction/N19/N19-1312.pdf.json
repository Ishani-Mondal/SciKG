{"title": [{"text": "Improving Domain Adaptation Translation with Domain Invariant and Specific Information", "labels": [], "entities": [{"text": "Improving Domain Adaptation Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9251120984554291}]}], "abstractContent": [{"text": "In domain adaptation for neural machine translation , translation performance can benefit from separating features into domain-specific features and common features.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7164328843355179}, {"text": "neural machine translation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6948808431625366}]}, {"text": "In this paper , we propose a method to explicitly model the two kinds of information in the encoder-decoder framework so as to exploit out-of-domain data in in-domain training.", "labels": [], "entities": []}, {"text": "In our method, we maintain a private encoder and a private decoder for each domain which are used to model domain-specific information.", "labels": [], "entities": []}, {"text": "In the meantime, we introduce a common en-coder and a common decoder shared by all the domains which can only have domain-independent information flow through.", "labels": [], "entities": []}, {"text": "Besides , we add a discriminator to the shared en-coder and employ adversarial training for the whole model to reinforce the performance of information separation and machine translation simultaneously.", "labels": [], "entities": [{"text": "information separation", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7515710592269897}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7464929521083832}]}, {"text": "Experiment results show that our method can outperform competitive baselines greatly on multiple data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT)) has made great progress and drawn much attention recently.", "labels": [], "entities": [{"text": "Neural machine translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8041198750336965}]}, {"text": "Most NMT models are based on the encoder-decoder architecture, where all the sentence pairs share the same set of parameters for the encoder and decoder which makes NMT models have a tendency towards overfitting to frequent observations (e.g., words, word cooccurrences, translation patterns), but overlooking special cases that are not frequently observed.", "labels": [], "entities": []}, {"text": "However, in practical applications, NMT models usually need to perform translation for some specific domain with only a small quantity of in-*Corresponding Author domain training data but a large amount of out-ofdomain data.", "labels": [], "entities": []}, {"text": "Simply combining in-domain training data with out-of-domain data will lead to overfitting to the out-of-domain data.", "labels": [], "entities": []}, {"text": "Therefore, some domain adaptation technique should be adopted to improve in-domain translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8039777874946594}]}, {"text": "Fortunately, out-of-domain data still embodies common knowledge shared between domains.", "labels": [], "entities": []}, {"text": "And incorporating the common knowledge from out-of-domain data can help in-domain translation.", "labels": [], "entities": []}, {"text": "have done this kind of attempts and managed to improve in-domain translation.", "labels": [], "entities": []}, {"text": "The common architecture of this method is to share a single encoder and decoder among all the domains and add a discriminator to the encoder to distinguish the domains of the input sentences.", "labels": [], "entities": []}, {"text": "The training is based on adversarial learning between the discriminator and the translation , ensuring the encoder can learn common knowledge across domains that can help to generate target translation.", "labels": [], "entities": []}, {"text": "extend this line of work by introducing a private encoder to learn some domain specific knowledge.", "labels": [], "entities": []}, {"text": "They have proven that domain specific knowledge is a complement to domain invariant knowledge and indispensable for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7601158916950226}]}, {"text": "Intuitively, besides the encoder, the knowledge inferred by the decoder can also be divided into domain specific and domain invariant and further improvement will be achieved by employing private In this paper, in order to produce in-domain translation with not only common knowledge but in-domain knowledge, we employ a common encoder and decoder among all the domains and also a private encoder and decoder for each domain separately.", "labels": [], "entities": []}, {"text": "The differences between our method and the above methods are in two points: first, we employ multiple private encoders rather where all the domains only have one private encoder; sec-ond, we also introduce multiple private decoders contrast to no private decoder.", "labels": [], "entities": []}, {"text": "This architecture is based on the consideration that out-of-domain data is far more than in-domain data and only using one private encoder and/or decoder has the risk of overfitting.", "labels": [], "entities": []}, {"text": "Under the framework of our method, the translation of each domain is predicted on the output of both the common decoder and its private decoder.", "labels": [], "entities": []}, {"text": "In this way, the in-domain private decoder has direct influence to the generation of in-domain translation and the out-ofdomain decoder is used to help train the common encoder and decoder better which can also help indomain translation.", "labels": [], "entities": [{"text": "indomain translation", "start_pos": 216, "end_pos": 236, "type": "TASK", "confidence": 0.7666778266429901}]}, {"text": "We conducted experiments on English\u2192Chinese and English\u2194German domain adaptation tasks for machine translation under the framework of RNNSearch ( ) and Transformer () and get consistently significant improvements over several strong baselines.", "labels": [], "entities": [{"text": "English\u2194German domain adaptation", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.634237140417099}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7622932195663452}, {"text": "RNNSearch", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.7042890787124634}]}], "datasetContent": [{"text": "We evaluated our method on the English\u2192Chinese (En-Zh), English\u2192German (En-De) and German\u2192English (De-En) domain adaptation translation task.", "labels": [], "entities": [{"text": "German\u2192English (De-En) domain adaptation translation", "start_pos": 83, "end_pos": 135, "type": "TASK", "confidence": 0.6675736440552605}]}, {"text": "We made some some detailed analysis to empirically show the effectiveness of our model based on En-Zh translation task.", "labels": [], "entities": [{"text": "En-Zh translation", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.4819650202989578}]}], "tableCaptions": [{"text": " Table 1: Results of the en-zh translation experiments.  The marks indicate whether the proposed methods  were significantly better than the best performed con- trast models(**: better at significance level \u03b1 = 0.01,  *:\u03b1 = 0.05)(", "labels": [], "entities": []}, {"text": " Table 2: Results of the WMT 07 en-de translation ex- periments.", "labels": [], "entities": [{"text": "WMT 07 en-de translation", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.7972142994403839}]}, {"text": " Table 3: Results of the IWSLT 15 en-de experiments.  The second part results were directly taken from their  papers.", "labels": [], "entities": [{"text": "IWSLT 15 en-de experiments", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.8204780668020248}]}, {"text": " Table 4: Results of the ablation study. \"DCN\"  means the discriminator and \"Private\" means the pri- vate encoder-decoder.", "labels": [], "entities": []}, {"text": " Table 6: Results of the out-of-domain translation task.  The test sets are from the NIST test sets but we ex- change the translation directions.", "labels": [], "entities": [{"text": "out-of-domain translation task", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.7448212305704752}, {"text": "NIST test sets", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.9710573752721151}]}, {"text": " Table 7: Results of the En-Zh experiments based on the  transformer model.", "labels": [], "entities": []}]}