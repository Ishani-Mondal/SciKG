{"title": [{"text": "Combining Distant and Direct Supervision for Neural Relation Extraction", "labels": [], "entities": [{"text": "Neural Relation Extraction", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.868158221244812}]}], "abstractContent": [{"text": "In relation extraction with distant supervision, noisy labels make it difficult to train quality models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.9058992862701416}]}, {"text": "Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations.", "labels": [], "entities": []}, {"text": "We improve such models by combining the distant supervision data with an additional directly-supervised data, which we use as supervision for the attention weights.", "labels": [], "entities": []}, {"text": "We find that joint training on both types of supervision leads to a better model because it improves the model's ability to identify noisy sentences.", "labels": [], "entities": []}, {"text": "In addition, we find that sigmoidal attention weights with max pooling achieves better performance over the commonly used weighted average attention in this setup.", "labels": [], "entities": []}, {"text": "Our proposed method 1 achieves anew state-of-the-art result on the widely used FB-NYT dataset.", "labels": [], "entities": [{"text": "FB-NYT dataset", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9831512868404388}]}], "introductionContent": [{"text": "Early work in relation extraction from text used directly supervised methods, e.g.,, which motivated the development of relatively small datasets with sentence-level annotations such as ACE.", "labels": [], "entities": [{"text": "relation extraction from text", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.9088564813137054}]}, {"text": "Recognizing the difficulty of annotating text with relations, especially when the number of relation types of interest is large, others) introduced the distant supervision approach of relation extraction, where a knowledge base (KB) and a text corpus are used to automatically generate a large dataset of labeled bags of sentences (a set of sentences that might express the relation) which are then used to train a relation classifier.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.7687839865684509}]}, {"text": "The large number of labeled instances produced with distant supervision make it a practical alternative to manual annotations.", "labels": [], "entities": []}, {"text": "However, distant supervision implicitly assumes that all the KB facts are mentioned in the text (at least one of the sentences in each bag expresses the relation) and that all relevant facts are in the KB (use entities that are not related in the KB as negative examples).", "labels": [], "entities": []}, {"text": "These two assumptions are generally not true, which introduces many noisy examples in the training set.", "labels": [], "entities": []}, {"text": "Although many methods have been proposed to deal with such noisy training data (e.g.,), a rather obvious approach has been understudied: combine distant supervision data with additional direct supervision.", "labels": [], "entities": []}, {"text": "Intuitively, directly supervising the model can improve its performance by helping it identify which of the input sentences fora given pair of entities are more likely to express a relation.", "labels": [], "entities": []}, {"text": "A straightforward way to combine distant and direct supervision is to concatenate instances from both datasets into one large dataset.", "labels": [], "entities": []}, {"text": "We show in Section 4.2 that this approach doesn't help the model.", "labels": [], "entities": []}, {"text": "also observed similar results; instead, they train a graphical model on the distantly supervised instances while using the directly labeled instances to supervise a subcomponent of the model.", "labels": [], "entities": []}, {"text": "We discuss prior work in more detail in Section 5.", "labels": [], "entities": []}, {"text": "In our paper, we demonstrate a similar approach with neural networks.", "labels": [], "entities": []}, {"text": "Specifically, our neural model attends over sentences to distinguish between sentences that are likely to express some relation between the entities and sentences that do not.", "labels": [], "entities": []}, {"text": "We use the additional direct supervision to supervise these attention weights.", "labels": [], "entities": []}, {"text": "We train this model jointly on both types of supervision in a multitask learning setup.", "labels": [], "entities": []}, {"text": "In addition, we experimentally find that sigmoidal attention weights with max pooling achieves better perfor-  Figure 1: An overview of our approach for combining distant and direct supervision.", "labels": [], "entities": []}, {"text": "The left side shows one sentence in the labeled data and how it is used to provide direct supervision for the sentence encoder.", "labels": [], "entities": []}, {"text": "The right side shows snippets of the text corpus and the knowledge base, which are then combined to construct one training instance for the model, with a bag of three input sentences and two active relations: 'founder of' and 'ceo of'.", "labels": [], "entities": []}, {"text": "mance in this model setup than the commonly used weighted average attention.", "labels": [], "entities": [{"text": "mance", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9600436687469482}]}, {"text": "The contributions of this paper are as follows: \u2022 We propose an effective neural network model for improving distant supervision by combining it with a directly supervised data in the form of sentence-level annotations.", "labels": [], "entities": []}, {"text": "The model is trained jointly on both types of supervision in a multitask learning setup, where the direct supervision data is employed as supervision for attention weights.", "labels": [], "entities": []}, {"text": "\u2022 We show experimentally that our model setup benefits from sigmoidal attention weights with max pooling over the commonly used softmaxbased weighted averaging attention.", "labels": [], "entities": []}, {"text": "\u2022 Our best model achieves anew state-of-the-art result on the FB-NYT dataset, previously used by ;.", "labels": [], "entities": [{"text": "FB-NYT dataset", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9865076243877411}]}, {"text": "Specifically, combining both forms of supervision achieves a 4.4% relative AUC increase than our baseline without the additional supervision.", "labels": [], "entities": [{"text": "AUC", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9786321520805359}]}, {"text": "The following section defines the notation we use, describes the problem and provides an overview of our approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Controlled experiments for a) how the su- pervised data is used in the model, b) which function  is used to compute attention weights, c) how sentence  embeddings are aggregated", "labels": [], "entities": []}, {"text": " Table 2: Weights assigned to sentences by our baseline and our best model. The baseline incorrectly predicts  \"no relation\", while our best model correctly predicts \"neighbourhood of\" for this bag.", "labels": [], "entities": []}]}