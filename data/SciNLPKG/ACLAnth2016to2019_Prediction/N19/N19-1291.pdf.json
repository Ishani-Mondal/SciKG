{"title": [{"text": "Scalable Collapsed Inference for High-Dimensional Topic Models", "labels": [], "entities": [{"text": "Scalable Collapsed Inference", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8538813988367716}]}], "abstractContent": [{"text": "The bigger the corpus, the more topics it can potentially support.", "labels": [], "entities": []}, {"text": "To truly make full use of massive text corpora, a topic model inference algorithm must therefore scale efficiently in 1) documents and 2) topics, while 3) achieving accurate inference.", "labels": [], "entities": []}, {"text": "Previous methods have achieved two out of three of these criteria simultaneously , but never all three at once.", "labels": [], "entities": []}, {"text": "In this paper, we develop an online inference algorithm for topic models which leverages stochasticity to scale well in the number of documents, sparsity to scale well in the number of topics, and which operates in the collapsed representation of the topic model for improved accuracy and run-time performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9982002973556519}]}, {"text": "We use a Monte Carlo inner loop in the on-line setting to approximate the collapsed vari-ational Bayes updates in a sparse and efficient way, which we accomplish via the Metropolis-Hastings Walker method.", "labels": [], "entities": []}, {"text": "We showcase our algorithm on LDA and the recently proposed mixed membership skip-gram topic model.", "labels": [], "entities": [{"text": "LDA", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7895273566246033}]}, {"text": "Our method requires only amortized O(k d) computation per word token instead of O(K) operations, where the number of topics occurring fora particular document k d \u226a the total number of topics in the corpus K, to converge to a high-quality solution.", "labels": [], "entities": [{"text": "O", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.898296594619751}]}], "introductionContent": [{"text": "Topic models are powerful tools for analyzing today's massive, constantly expanding digital text information by representing high-dimensional data in a low-dimensional subspace.", "labels": [], "entities": []}, {"text": "We can recover the main themes of a corpus by using topic models such as latent Dirichlet allocation (LDA) to organize, understand, search, and explore the documents ().", "labels": [], "entities": []}, {"text": "Traditional LDA inference techniques such as variational Bayes and collapsed Gibbs sampling do not readily scale to corpora containing millions of documents.", "labels": [], "entities": []}, {"text": "To scale up inference, the main approaches are distributed algorithms and stochastic algorithms.", "labels": [], "entities": []}, {"text": "Stochastic algorithms, such as stochastic variational inference (SVI), operate in an online fashion, and hence do not need to see all of the documents before updating the topics, so they can be applied to corpora of any size, without expensive distributed hardware.", "labels": [], "entities": [{"text": "stochastic variational inference (SVI)", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.7399213512738546}]}, {"text": "The \"collapsed\" representation of topic models is also frequently important, as it leads to faster convergence, efficient updates, and lower variance in estimation ().", "labels": [], "entities": [{"text": "estimation", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9495546817779541}]}, {"text": "The stochastic collapsed variational Bayesian inference (SCVB0) algorithm, proposed by (, combines the benefits of stochastic and collapsed inference.", "labels": [], "entities": []}, {"text": "Larger corpora typically support more topics, which brings the additional efficiency challenge of training a larger model (.", "labels": [], "entities": []}, {"text": "This challenge has been addressed by exploiting sparsity to perform updates in time sublinear in the number of topics.", "labels": [], "entities": []}, {"text": "A sparse variant of the SVI algorithm for LDA, SSVI, proposed by, is scalable to large numbers of topics, but does not fully exploit the collapsed representation of LDA, which is important for faster convergence and improved inference accuracy, due to a better variational bound ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.9883688688278198}]}, {"text": "The Metropolis Hastings Walker (MHW) method () scales well in the number of topics, and uses a collapsed inference algorithm, but it operates in the batch setting, so it is not scalable to large corpora. is a distributed approach to the MHW method which adopts a data-and-model-parallel strategy to maximize memory and CPU efficiency.", "labels": [], "entities": []}, {"text": "However, it is not an online approach, and furthermore requires multiple expensive computer clusters to converge faster.", "labels": [], "entities": []}, {"text": "Tensor methods are another approach to speeding up topic models (, which theoretically guarantee the recovery of the true parameters by overcoming the problem of local optima.", "labels": [], "entities": []}, {"text": "These techniques use the method of moments instead of maximum likelihood estimation or Bayesian inference, which leads to lower data efficiency, and sometimes unreliable performance.", "labels": [], "entities": []}, {"text": "In this work, we propose a highly efficient and scalable inference algorithm for topic models.", "labels": [], "entities": []}, {"text": "We develop an online algorithm which leverages stochasticity to scale well in the number of documents, sparsity to scale well in the number of topics, and which operates in the collapsed representation of topic models.", "labels": [], "entities": []}, {"text": "We thereby combine the individual benefits of SVI, SSVI, SCVB0, and MHW into a single algorithm.", "labels": [], "entities": [{"text": "MHW", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7126924991607666}]}, {"text": "Our approach is to develop a sparse version of SCVB0.", "labels": [], "entities": [{"text": "SCVB0", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9438591003417969}]}, {"text": "Inspired by SSVI, we use a Monte Carlo inner loop to approximate the SCVB0 variational distribution updates in a sparse and efficient way, which we accomplish via MHW method.", "labels": [], "entities": []}, {"text": "To show the generality of our algorithm, we explore the benefits of our inference method for LDA and another recently proposed topic model, MMSGTM, with experiments on both small and large-scale datasets.", "labels": [], "entities": [{"text": "MMSGTM", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.8261781930923462}]}], "datasetContent": [{"text": "In this section we study the performance of our SparseSCVB0 1 algorithm, on small as well as large corpora to validate the proposed method for topic models such as LDA and MMSGTM, and to compare with other state-of-the-art algorithms.", "labels": [], "entities": [{"text": "MMSGTM", "start_pos": 172, "end_pos": 178, "type": "DATASET", "confidence": 0.7919821739196777}]}, {"text": "We compared SparseSCVB0 to SCVB0 and SVI.", "labels": [], "entities": [{"text": "SparseSCVB0", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8471695780754089}]}, {"text": "For a fair comparison, we implemented all of them in the fast high-level language Julia V0.6..", "labels": [], "entities": []}, {"text": "We conducted all experiments on a computer with 64GB memory and an Intel Xeon E5-2623 V4 processor with 2.60 GHz clock rate, 8\u00d7256KB L2 Cache and 10MB L3 Cache.", "labels": [], "entities": []}, {"text": "As we only use one single thread for sampling across all experiments, only one CPU core is active throughout the experiment with only 256KB available L2 Cache.", "labels": [], "entities": []}, {"text": "We used NIPS, Reuters-150, PubMed Central, and Wikipedia as representative very small, small, medium, and large-scale datasets, respectively.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9558947086334229}, {"text": "Reuters-150", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8884739875793457}, {"text": "PubMed Central", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9223474860191345}, {"text": "Wikipedia", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.9725702404975891}]}, {"text": "The NIPS corpus has 1740 scientific articles from years 1987-1999 with 2.3M tokens, due to Sam Roweis.", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9835996627807617}]}, {"text": "The newswire corpus Reuters-150 contains 15, 500 articles with dictionary size of 8, 350 words.", "labels": [], "entities": [{"text": "newswire corpus Reuters-150", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.8158302704493204}]}, {"text": "PubMed Central has 320M tokens across 165, 000 scientific articles and a vocabulary size of around 38, 500 words.", "labels": [], "entities": [{"text": "PubMed Central", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.958307296037674}]}, {"text": "The Wikipedia corpus contained 4.6 million articles from the online network data communication information communicate connection prison prisoners prisoner imprisoned jail escaped detained guards dog dogs shepherd hounds bred coat scent instinct eating companion song sung sing singing sings sang songs recorded melody tune votes vote cast elections voted candidate parties majority election wind winds blowing speed blows direction high low blown chill hour hours noontime daylight minutes midnight morning seconds: Randomly selected topics from a 10, 000-topic model trained using SparseSCVB0 on Wikipedia encyclopedia.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.806683361530304}, {"text": "shepherd hounds bred coat scent instinct eating companion song sung sing singing sings sang songs recorded melody tune votes vote cast elections voted candidate parties majority election", "start_pos": 205, "end_pos": 391, "type": "TASK", "confidence": 0.8291785496252554}]}, {"text": "We used the dictionary of 7, 700 words which was extracted by.", "labels": [], "entities": []}, {"text": "There were 811M tokens in the corpus.", "labels": [], "entities": []}], "tableCaptions": []}