{"title": [{"text": "Towards Content Transfer through Grounded Text Generation", "labels": [], "entities": [{"text": "Content Transfer", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.764871746301651}, {"text": "Grounded Text Generation", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6821821530659994}]}], "abstractContent": [{"text": "Recent work in neural generation has attracted significant interest in controlling the form of text, such as style, persona, and politeness.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7770541906356812}]}, {"text": "However, there has been less work on controlling neural text generation for content.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7432971994082133}]}, {"text": "This paper introduces the notion of Content Transfer for long-form text generation, where the task is to generate a next sentence in a document that both fits its context and is grounded in a content-rich external textual source such as a news story.", "labels": [], "entities": [{"text": "Content Transfer", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7258635014295578}, {"text": "long-form text generation", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6809346477190653}]}, {"text": "Our experiments on Wikipedia data show significant improvements against competitive baselines.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9417350590229034}]}, {"text": "As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style), affect (), politeness), persona ( voice, grammatical correctness (, and length (.", "labels": [], "entities": [{"text": "neural natural language generation (NLG)", "start_pos": 15, "end_pos": 55, "type": "TASK", "confidence": 0.8071754234177726}, {"text": "length", "start_pos": 249, "end_pos": 255, "type": "METRIC", "confidence": 0.9606701135635376}]}, {"text": "This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness.", "labels": [], "entities": []}, {"text": "What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.", "labels": [], "entities": []}, {"text": "Consider for example, which illustrates a situation where an author edits a document (here a Wikipedia article),: Example of content transfer: Given existing curated text (yellow) and a document with additional relevant information (green), the task is to update the curated text (orange) to reflect the most salient updates. and the goal is to generate or suggest a next sentence (shown in orange) to the author.", "labels": [], "entities": [{"text": "content transfer", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7832367420196533}]}, {"text": "This type of unconstrained, long-form text generation task () is of course extremely difficult.", "labels": [], "entities": [{"text": "text generation task", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7897369762261709}]}, {"text": "Free-form generation can easily go astray due to two opposing factors.", "labels": [], "entities": []}, {"text": "On one hand, ensuring that the generated output is of relatively good quality often comes at the cost of making it bland and devoid of factual content ().", "labels": [], "entities": []}, {"text": "On the other hand, existing techniques can help steer neural models away from blandness in order to produce more contentful outputs (using temperature sampling,), etc.), but often at the cost of \"hallucinating\" words or concepts that are totally irrelevant.", "labels": [], "entities": []}, {"text": "Neither situation provides a compelling experience to the user.", "labels": [], "entities": []}, {"text": "What is clearly missing from the aforementioned authoring scenario is the notion of grounding: there is often a profusion of online resources that bear at least some relevance to any given document currently being written.", "labels": [], "entities": []}, {"text": "Much of the general-purpose world knowledge is available in the form of encyclopedias (e.g., Wikipedia), books (e.g., Project Gutenberg, Google Books), and news articles.", "labels": [], "entities": []}, {"text": "While the generation of good quality texts without any conditioning on \"external\" sources) might bean interesting research endeavor on its own, we argue that grounding can make the generation task much easier, e.g., as shown in where a passage of a news article (green) can be reformulated considering the current context of the document (yellow) in order to produce a natural next sentence.", "labels": [], "entities": []}, {"text": "In light of this desideratum, this paper addresses the problem of grounded text generation, where the goal is to infuse the content or knowledge from an external source (e.g., a news article as in) in order to generate a follow-up sentence of an existing document.", "labels": [], "entities": [{"text": "grounded text generation", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7320881883303324}]}, {"text": "We see this as a form of Content Transfer, as other characteristics of the external source-such as style and linguistic form-are not controlled.", "labels": [], "entities": [{"text": "Content Transfer", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7656521499156952}]}, {"text": "In addition to formulating this new task, our work makes the following contributions: We provide a large dataset of 640k instances that contain parallel data of a source document (news articles), a context, and sentence to be produced.", "labels": [], "entities": []}, {"text": "The latter two are extracted from Wikipedia, which is an attractive dataset for grounded generation as many of the statements in Wikipedia cite external sources (i.e., grounded in an external article).", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9753198623657227}]}, {"text": "Finally, we also provide simple yet efficient models that condition both on the external article and the context of the current document.", "labels": [], "entities": []}, {"text": "We compare our models against extractive and abstractive baselines, including summarization methods that simply try to condense the external article without considering the context of the document.", "labels": [], "entities": []}, {"text": "Our experiments show that our models which incorporate the context gain 7.0 ROUGE-L F1 points -in other words, treating our task as a summarization problem is not enough.", "labels": [], "entities": [{"text": "ROUGE-L F1 points", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.9622037410736084}, {"text": "summarization", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.9623309373855591}]}, {"text": "Our human evaluations also show that models that are aware of the context generate relevant and fluent sentences that are coherent to the context.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our ideal dataset would capture the edits made to some curated reference text in light of a stream of new articles describing changes.", "labels": [], "entities": []}, {"text": "For instance, one might maintain reference software documentation about a system, making additions or changes in light of incoming emails describing updates or additions.", "labels": [], "entities": []}, {"text": "This type of data is unfortunately difficult to obtain due to privacy considerations.", "labels": [], "entities": []}, {"text": "However, Wikipedia can provide a naturallyoccurring body of text with references to primary sources.", "labels": [], "entities": []}, {"text": "A substantial fraction of Wikipedia sentences include citations to supporting documentation, a ripe source of data for content transfer.", "labels": [], "entities": [{"text": "content transfer", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7664699554443359}]}, {"text": "That said, some of the citations are quite difficult to follow or trust: broken URLs might lead to lost information; citations to books are difficult to consume given the large scope of information; etc.", "labels": [], "entities": []}, {"text": "Therefore, we only consider cases where the reference links to some well-known news sources.", "labels": [], "entities": []}, {"text": "Based on citation frequency, we selected a list of 86 domains, 3 primarily news outlets.", "labels": [], "entities": []}, {"text": "During the data creation process we only considered citations belonging to one of these eighty six domains.", "labels": [], "entities": []}, {"text": "We make this simplifying assumption for several reasons.", "labels": [], "entities": []}, {"text": "First, our English Wikipedia dump contained approximately 23.7 million citation URLS belonging to 1.6 million domains; fine-grained filtering would be a daunting task.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.7878143787384033}]}, {"text": "Our hand-vetted list of domains is a high-precision (albeit lowrecall) means of selecting clean data.", "labels": [], "entities": []}, {"text": "Second, we wanted to ground the generated text on credible, consistent, and well-written sources of information.", "labels": [], "entities": []}, {"text": "Furthermore, well-known domains are readily available on Common Crawl, 4 leading to an easily-reproducible dataset.", "labels": [], "entities": [{"text": "Common Crawl, 4", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9636350274085999}]}, {"text": "illustrates the procedure used to create a dataset for the task described in Section 2 from Wikipedia.", "labels": [], "entities": []}, {"text": "For each Wikipedia article, we extracted the plain text without markdown.", "labels": [], "entities": []}, {"text": "When encountering a citation belonging to a selected domain, we considered the sentence just before the citation to be generated based on the content of the citation.", "labels": [], "entities": []}, {"text": "This sentence became our reference update sentence: the additional update sentence x added to the curated text s to produce the new text s . The k sentences prior to the target sentence in the Wikipedia article were considered to be the curated text s.", "labels": [], "entities": []}, {"text": "In our case, we used a window of k = 3 sentences to select our context.", "labels": [], "entities": []}, {"text": "The cited article acted as the document d, from which the appropriate update x can be generated.", "labels": [], "entities": []}, {"text": "The HTML source of the citation was down-loaded from Common Crawl for reproducibility and consistency.", "labels": [], "entities": [{"text": "Common Crawl", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9304299652576447}, {"text": "consistency", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9656261801719666}]}, {"text": "The HTML derived from Common Crawl is then processed to get the plain text of the news article.", "labels": [], "entities": []}, {"text": "The resulting dataset C consists of aligned tuples , where n is the total number of samples in the dataset.", "labels": [], "entities": []}, {"text": "Alternatively, one might rely on Wikipedia edit history to create a dataset.", "labels": [], "entities": []}, {"text": "In this setting, edits which include anew citation would act as the update x.", "labels": [], "entities": []}, {"text": "Although this has the upside of identifying potentially complex, multi-sentence updates, preliminary analysis suggested that these edits are noisy.", "labels": [], "entities": []}, {"text": "Editors may first generate the content in one edit, then add the citation in a subsequent edit, they may only rephrase apart of the text while adding the citation, or they may check in a range of changes across the document in a single edit.", "labels": [], "entities": []}, {"text": "Our simpler sentence-based approach leads to an interesting dataset with fewer complications.", "labels": [], "entities": []}, {"text": "describes some key statistics of our dataset and how it compares with other datasets used for similar tasks.", "labels": [], "entities": []}, {"text": "The ROUGE-1 recall scores of reference output x against document d suggest this task will be difficult for conventional extractive summarization techniques.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9917096495628357}, {"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.773117184638977}, {"text": "extractive summarization", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.5862488746643066}]}, {"text": "We hypothesize that during content transfer, the language in document d often undergoes substantial transformations to fit the curated text s.", "labels": [], "entities": []}, {"text": "The average unigram overlap (after stopword removal) between the document d and the reference update sentence x is 55.79%; overlap of the curated text sand the reference update sentence x is 30.12%.", "labels": [], "entities": [{"text": "unigram overlap", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.7034595906734467}, {"text": "overlap", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9776796698570251}]}, {"text": "This suggests the reference update sentence x can be derived from the document d, though not extracted directly.", "labels": [], "entities": []}, {"text": "Furthermore, the content of x is very different from the content of s but appears topically related.", "labels": [], "entities": []}, {"text": "Our dataset consists of approximately 290k unique Wikipedia articles.", "labels": [], "entities": []}, {"text": "Some heavily-cited articles include 'Timeline of investigations into', 'List of England Test cricketers', and '2013 in science'.", "labels": [], "entities": [{"text": "Timeline", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.5388460755348206}, {"text": "List of England Test cricketers", "start_pos": 72, "end_pos": 103, "type": "DATASET", "confidence": 0.6281086146831513}]}, {"text": "We randomly split the dataset into 580k training instances, 6049 validation instances, and 50k test instances, ensuring that any Wikipedia article appearing in the train set must not appear in validation or test.", "labels": [], "entities": []}, {"text": "We evaluate our models using both automated metrics and, fora subset of promising systems, human assessment.", "labels": [], "entities": []}, {"text": "One key evaluation is the similarity between the model generated update sentence and reference update sentence.", "labels": [], "entities": []}, {"text": "We also ask human judges to assess grammaticality and coherence.", "labels": [], "entities": []}, {"text": "Hyper-parameter settings: For all our experiments with generative models, we have used bidirectional encoder, 2 layers in encoder and decoder, RNN size of 128, word vector size of 100.", "labels": [], "entities": []}, {"text": "We have used sentencepiece toolkit 6 to use byte-pairencoding (BPE) with a vocabulary size of 32k.", "labels": [], "entities": []}, {"text": "We used stochastic gradient descent optimizer and the stopping criterion was perplexity on the validation set.", "labels": [], "entities": []}, {"text": "We filtered our dataset to contain instances which have length of the document between 50 and 2000 tokens, length of the curated text between 20 and 500 tokens and the length of the update sentence between 5 and 200 tokens.", "labels": [], "entities": [{"text": "length", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9612714052200317}]}, {"text": "Our primary automated evaluation metric for system-generated update sentences is ROUGE-L F1 against reference update sentence, 7 though we also include BLEU () and METEOR (Denkowski and Lavie, 2011) as additional indicators.", "labels": [], "entities": [{"text": "ROUGE-L F1", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.8870865404605865}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9989194869995117}, {"text": "METEOR", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.973556637763977}]}, {"text": "ROUGE is a standard family of metrics for summarization tasks; ROUGE-L measures the longest common subsequence between the system and the reference, capturing both lexical selection and word order.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9311015903949738}]}, {"text": "illustrates that this task is quite difficult for extractive techniques.", "labels": [], "entities": []}, {"text": "Furthermore, the results emphasize the importance of having curated text as context when generating the update.", "labels": [], "entities": []}, {"text": "In all experimental conditions, models aware of context perform much better than models agnostic of it.", "labels": [], "entities": []}, {"text": "In contrast to  outperformed hybrid, likely because we only had a single input document.", "labels": [], "entities": []}, {"text": "Extractive CAG, CIG, and CRG all outperformed both Sum-Basic and the context informed variant.", "labels": [], "entities": []}, {"text": "Extractive CAG was on-par with generative CAG, suggesting the generated sentences were of reasonable quality.", "labels": [], "entities": [{"text": "Extractive CAG", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6934916377067566}, {"text": "generative CAG", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9519909024238586}]}, {"text": "However, generative CIG and CRG were substantially better: rewriting to match context was beneficial.", "labels": [], "entities": [{"text": "generative CIG", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7895047068595886}]}, {"text": "The Oracle system of aims to establish an upper limit attainable by extractive methods, using the following oracle experiment: For each test instance d i , s i , xi , we enumerate each extracted sentence e of document d i and select the one with highest ROUGE-L score as Oracle's update sentenc\u00ea xi (i.e., \u02c6 xi = arg max e\u2208d i ROUGE-L(x i , e)).", "labels": [], "entities": [{"text": "ROUGE-L score", "start_pos": 254, "end_pos": 267, "type": "METRIC", "confidence": 0.9537825882434845}]}, {"text": "Note this yields a very optimistic upper bound, as the same ground truth xi is used both to select an extractive sentence from a large pool of candidates and for final automatic metric scoring.", "labels": [], "entities": []}, {"text": "8 Nevertheless, these oracle results let us draw two conclusions: (1) They give us better perspective to assess the non-oracle systems, and we believe that their seemingly low automatic evaluation scores are quite reasonable relative to the optimistic upper bound (e.g., CIGs ROUGE-Ls score is 55% of the oracle).", "labels": [], "entities": [{"text": "ROUGE-Ls score", "start_pos": 276, "end_pos": 290, "type": "METRIC", "confidence": 0.8919739723205566}]}, {"text": "The oracle results suggest that humans are substantially changing the surface realization as they summarize for Wikipedia, as otherwise the oracle results would be much closer to maximum metric scores (i.e., 100%).", "labels": [], "entities": []}, {"text": "This shows that extractive methods are not enough for this task, justifying our use of generation techniques.", "labels": [], "entities": []}, {"text": "For careful evaluation of the performance of the most promising configurations (CAG and CIG models) we also asked human judges for quality assessments.", "labels": [], "entities": []}, {"text": "We solicited several types of evaluation, including two relative comparisons between pairs of system outputs and an absolute quality evaluation of individual system outputs.", "labels": [], "entities": []}, {"text": "Close to reference (Relative): The first relative comparison measured how accurately the generated update reflected information in the reference update.", "labels": [], "entities": [{"text": "Relative)", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9288854598999023}]}, {"text": "Here, the annotators saw only the reference update sentence and the outputs of two systems labeled A and B in a randomized order.", "labels": [], "entities": []}, {"text": "We asked the annotators \"Which system output is closest in meaning to the reference update?\"", "labels": [], "entities": []}, {"text": "The annotators could pick system A, system B, or indicate that neither was preferred.", "labels": [], "entities": []}, {"text": "This is a simple evaluation task though potentially biased toward the sole reference update.", "labels": [], "entities": []}, {"text": "Coherent to context (Relative): The second relative comparison measured whether the generated output contained salient information from the document written in a manner appropriate to the curated text.", "labels": [], "entities": [{"text": "Relative)", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9310648143291473}]}, {"text": "The annotators saw the document d, the curated text s, and the outputs of the two systems A and B, again in a random order.", "labels": [], "entities": []}, {"text": "They were asked, \"Which system output is more accurate relative to the background information given in the snippet of the article?\"", "labels": [], "entities": []}, {"text": "Each judge had to consider whether the information fits with the curated text and also whether system-generated content could be supported by the document.", "labels": [], "entities": []}, {"text": "Four human judges each annotated 30 unique output pairs for these two relative comparison settings, a total of 240 relative judgments.", "labels": [], "entities": []}, {"text": "shows the results: the context-aware CIG system was substantially better in both settings.", "labels": [], "entities": []}, {"text": "DUC Guidelines (Absolute): In addition, we performed an absolute quality evaluation following the guidelines from DUC 2007.", "labels": [], "entities": [{"text": "DUC Guidelines", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9811132550239563}, {"text": "Absolute", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9909480214118958}, {"text": "DUC 2007", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9730403423309326}]}, {"text": "Each judge was presented with a single system output, then they were asked to evaluate five aspects of system output: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence.", "labels": [], "entities": []}, {"text": "For each aspect, the judge provided an assessment on a five-point scale: (1) Very Poor, (2) Poor, (3) Barely Acceptable, (4) Good, (5) Very Good.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.7504886984825134}]}, {"text": "We gathered 120 additional judgments in this setting (4 judges, 30 outputs).", "labels": [], "entities": []}, {"text": "Again, context-aware CIG substantially outperforms CAG across the board, as seen in.", "labels": [], "entities": []}, {"text": "Observations: Systems unaware of the curated text s tend to generate long updates with repeated frequent words or phrases.", "labels": [], "entities": [{"text": "Observations", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8342542052268982}]}, {"text": "Consider the ratio of unique tokens over the total number of tokens in the generated output, which we denote by R. A small R indicates many repeated tokens.", "labels": [], "entities": []}, {"text": "We find that 88% of the time this ratio R falls below 0.5 for the CAG model, i.e. for 88% instances, more than 50% of the words in the generated output are repeats.", "labels": [], "entities": [{"text": "R", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9409270286560059}]}, {"text": "This number is relatively small -14% for CIG and 20% for CRG -in context aware models.", "labels": [], "entities": []}, {"text": "In the reference updates only 0.21% instances repeat more than 50% of words.", "labels": [], "entities": []}, {"text": "show good and bad examples generated by the CIG model along with the document, curated text and the reference update. has a set of updates generated by the CIG model as Document (News Article) sequels are fairly new to bollywood, but director sanjay gadhvi realised there was cash to be made from resurrecting his hit action thriller dhoom, by casting sexy young stars like hrithik rosha, aishwarya rai and abhishek bachchan in an even bigger game of cops and robbes...that the twist in dhoom 2's tail is not explained is yet another shortcoming.", "labels": [], "entities": []}, {"text": "it's only roshan's charismatic performance as the criminal mastermind, and the sizzling chemistry he shares with rai's sassy cohort, that rescues this adventure from becoming an elongated tourism commercial.", "labels": [], "entities": []}, {"text": "Curated Text (Wikipedia Context) it makes no lasting contributions to world cinema, but if twoand-a-half hours of disposable entertainment are all you're after, you could do far worse.", "labels": [], "entities": []}, {"text": "\"l.a. weekly's david chute stated the film was, \"a movie meal as satisfying as this one can make you feel that nothing else matters.\" jaspreet pandohar of the bbc gave it a two-star rating, writing \"by roping in acclaimed action director alan amin to take care of the thrills and spills, you'd expect gadhvi to have spent time crafting out a sophisticated storyline instead of simply sending his cast on a catand-mouse chase around the globe.", "labels": [], "entities": []}, {"text": "Reference Update it's only roshan's charismatic performance as the criminal mastermind, and the sizzling chemistry he shares with rai's sassy cohort, that rescues this adventure from becoming an elongated tourism commercial.\"", "labels": [], "entities": []}, {"text": "Generated Update it's only roshan's finest performance as the criminal terrorist, and the sizzling chemistry he shares with rai's sassy anatomy, that attues this adventure from becoming an elongated tourism commercial.\" well as the reference update.", "labels": [], "entities": [{"text": "Generated Update", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8442587852478027}]}, {"text": "As we can see in examples 3 and 4, the CIG model misplaces the date but correctly generates the remaining content.", "labels": [], "entities": []}, {"text": "In examples 1 and 2, the CIG model appears to successfully select the correct pronouns for coreference resolution, though it gets confused as to when to use the pronoun or the named entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.9695396721363068}]}, {"text": "Examples 5 and 6 represent failure cases due to missing words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Key characteristics of the dataset: approximate size of input and output instances, approximate dataset  size, and recall of reference output against the source material, as a measure of dataset difficulty.", "labels": [], "entities": [{"text": "approximate dataset  size", "start_pos": 94, "end_pos": 119, "type": "METRIC", "confidence": 0.8065775434176127}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9990013241767883}]}, {"text": " Table 2: Automated metrics; 95% confidence interval  in parentheses.", "labels": [], "entities": [{"text": "95% confidence interval", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.7442790269851685}]}, {"text": " Table 3: Human preferences of CAG vs. CIG.", "labels": [], "entities": []}, {"text": " Table 4: Human absolute quality assessments.", "labels": [], "entities": [{"text": "Human absolute quality assessments", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.5070242807269096}]}]}