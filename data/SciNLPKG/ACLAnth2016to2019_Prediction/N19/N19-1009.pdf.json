{"title": [], "abstractContent": [{"text": "We report on adaptation of multilingual end-to-end speech recognition models trained on as many as 100 languages.", "labels": [], "entities": [{"text": "multilingual end-to-end speech recognition", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.6096348464488983}]}, {"text": "Our findings shed light on the relative importance of similarity between the target and pretraining languages along the dimensions of phonetics, phonology , language family, geographical location, and orthography.", "labels": [], "entities": []}, {"text": "In this context, experiments demonstrate the effectiveness of two additional pretraining objectives in encouraging language-independent encoder representations: a context-independent phoneme objective paired with a language-adversarial classification objective.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main difficulty in creating automatic speech recognition (ASR) systems fora large number of the world's 7,000 languages is alack of training data.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.8143505950768789}]}, {"text": "Such data comes in the form of speech paired with transcriptions, a pronunciation lexicon, and text for language model training.", "labels": [], "entities": []}, {"text": "A common technique in data-constrained settings is to learn language-independent representations of speech via multilingual training.", "labels": [], "entities": []}, {"text": "Popular approaches include the use of multilingual bottleneck features ( as well as multilingual model training before fine-tuning to a target language (.", "labels": [], "entities": []}, {"text": "Prior work in multilingual and cross-lingual speech recognition has been restricted to a small handful of the world's most-spoken languages, relying on multilingual corpora such as GlobalPhone (), the IARPA Babel corpora (), or the VoxForge 1 corpora.", "labels": [], "entities": [{"text": "cross-lingual speech recognition", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6229533950487772}, {"text": "GlobalPhone", "start_pos": 181, "end_pos": 192, "type": "DATASET", "confidence": 0.9501335024833679}, {"text": "IARPA Babel corpora", "start_pos": 201, "end_pos": 220, "type": "DATASET", "confidence": 0.8302906552950541}, {"text": "VoxForge 1 corpora", "start_pos": 232, "end_pos": 250, "type": "DATASET", "confidence": 0.8306996623675028}]}, {"text": "Most work typically only reports on models trained on a subset of these languages.", "labels": [], "entities": []}, {"text": "In this paper we explore pretraining multilingual ASR models using speech from as many as voxforge.org 100 languages from the CMU Wilderness Multilingual Speech Dataset.", "labels": [], "entities": [{"text": "ASR", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8456670045852661}, {"text": "CMU Wilderness Multilingual Speech Dataset", "start_pos": 126, "end_pos": 168, "type": "DATASET", "confidence": 0.9321386337280273}]}, {"text": "To the best of our knowledge, this is the greatest number of languages that has been used in multilingual ASR model training to date.", "labels": [], "entities": [{"text": "ASR model training", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9097357789675394}]}, {"text": "We perform experiments to guide the choice of languages used when pretraining the model and assess the relative importance of similarity between the pretraining languages and target language in terms of geographic location, phonology, phonetic inventory, language family and orthography.", "labels": [], "entities": []}, {"text": "We examine these variables in the context of two experimental setups: one where models are adapted to target language and target speakers, and one where models are adapted to target language but non-target speakers.", "labels": [], "entities": []}, {"text": "The first task is relevant to language documentation contexts, which often involves transcribing speech of specific speakers for which there already exists some transcribed speech as training data.", "labels": [], "entities": [{"text": "language documentation contexts", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.7547302742799123}]}, {"text": "The second case is relevant to incident response as modelled by LORELEI (, where there may only be a single target-language consultant available for which transcribed speech can be elicited, but the goal is to have an ASR model that generalizes to multiple speakers.", "labels": [], "entities": [{"text": "incident response", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8555276095867157}, {"text": "LORELEI", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9825998544692993}, {"text": "ASR", "start_pos": 218, "end_pos": 221, "type": "TASK", "confidence": 0.9122648239135742}]}, {"text": "Multilingual ASR training on such a scale presents challenges because of this language diversity.", "labels": [], "entities": [{"text": "ASR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6870354413986206}]}, {"text": "In order to guide the model to learn language-independent representations that are more amenable to adaptation, we experiment with two auxiliary training tasks.", "labels": [], "entities": []}, {"text": "The first is context-independent phoneme sequence prediction to help bridge orthographic inconsistencies between languages.", "labels": [], "entities": [{"text": "context-independent phoneme sequence prediction", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.6039336025714874}]}, {"text": "The second is a domainadversarial classification objective () over languages to encourage invariance of the model with respect to language-specific phenomena.", "labels": [], "entities": [{"text": "domainadversarial classification", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.7094179689884186}]}, {"text": "The hierarchical combination of grapheme and phoneme objectives has only been used in monolingual end-to-end frameworks.", "labels": [], "entities": []}, {"text": "Languageadversarial training in ASR ( has not been done at this scale before, nor in an endto-end framework.", "labels": [], "entities": [{"text": "ASR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9738048315048218}]}, {"text": "Our experiments are designed to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "Is there benefit in scaling multilingual model training to a large number of languages?", "labels": [], "entities": []}, {"text": "2. In what circumstances, if any, does the addition of a phoneme and/or languageadversarial objective improve multilingual models?", "labels": [], "entities": []}, {"text": "3. How should we choose languages with which to pretrain a multilingual model?", "labels": [], "entities": []}, {"text": "4. Do the answers to the above questions change when adapting to target versus non-target speakers in the target language?", "labels": [], "entities": []}, {"text": "We find that using the auxiliary objectives in pretraining facilitates model transfer to unseen languages, especially when the pretraining languages are very dissimilar (Section 6).", "labels": [], "entities": []}, {"text": "When the target speakers are seen in adaptation (Section 7), similarity of the pretraining languages and the target language is more important than quantity of pretraining languages.", "labels": [], "entities": [{"text": "similarity", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9806315898895264}]}, {"text": "Choosing as pretraining languages geographically proximal languages tends to help more than phonetically and phonologically similar but otherwise distant languages.", "labels": [], "entities": []}, {"text": "However, when adapting to a handful of non-target speakers of the target language (Section 8), the domain mismatch caused by the unseen speaker, language, or recording environment degrades performance.", "labels": [], "entities": []}, {"text": "Exposing the model to as many pretraining languages as possible becomes vital to minimize this mismatch.", "labels": [], "entities": []}, {"text": "Results on this task demonstrate that a massively multilingual seed model substantially outperforms other seed models trained on languages similar to the target.", "labels": [], "entities": []}, {"text": "We have provided an ESPnet recipe to train and test our models.", "labels": [], "entities": []}], "datasetContent": [{"text": "While the dataset includes only a single reading of the Bible for most languages, there area number with two or more.", "labels": [], "entities": []}, {"text": "We evaluate on languages for which we can find two or more readings.", "labels": [], "entities": []}, {"text": "This is so that we can compare adaptation to a target language but not the speakers of the target reading (we refer to this task as language adaptation, as explored in Section 8) with adaptation to the target language as well as the target reading (we refer to this task as reading adaptation).", "labels": [], "entities": [{"text": "language adaptation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7279543578624725}]}, {"text": "We additionally restricted the evaluation languages to those that have at least one good or very good reading in terms of alignment quality.", "labels": [], "entities": []}, {"text": "presents the evaluation languages and readings grouped by family or geographic location, along with their durations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Word error rate (%) comparison of multilingual models adapted to target languages, with and without  auxiliary training objectives (relative change in parentheses). Additionally including Cyrillic-script languages  in pretraining (CYR) doesn't consistently improve over a model pretrained on Quechuan languages (QUE) unless  additional phoneme and language-adversarial objectives (+phn and +adv) are used in combination (+phn+adv).  The auxiliary objectives help when pretraining languages are varied, but hinder when they are very similar. The  final four columns suggest that the objectives are complementary. Average relative word error rate change for each  pretraining set when adding in the auxiliary objectives (versus no aditional objectives) is indicated by Avg. rel. \u2206.", "labels": [], "entities": [{"text": "Word error rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7863224546114603}, {"text": "word error rate", "start_pos": 639, "end_pos": 654, "type": "METRIC", "confidence": 0.8578576644261678}, {"text": "Avg. rel. \u2206", "start_pos": 777, "end_pos": 788, "type": "DATASET", "confidence": 0.8783511072397232}]}, {"text": " Table 3: Word error rate (%) comparison of adaptation of models pretrained on: Quechuan and Cyrillic-script  languages (QUE+CYR), languages phonologically and phonetically similar to the target (PHON/INV), geograph- ically proximate languages (GEO), and a massively multilingual set of languages (100-LANG). In each case we  compared the average relative WER change when adding auxiliary phoneme and language-adversarial objectives  (+phn+adv). Dashed entries had phonology and phonetic inventories that weren't well attested in URIEL, so  were not assessed.", "labels": [], "entities": [{"text": "Word error rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7803711692492167}, {"text": "WER", "start_pos": 356, "end_pos": 359, "type": "METRIC", "confidence": 0.9979436993598938}]}, {"text": " Table 4: Adaptation to the non-target reading in the  target language. All language sets use the auxiliary  training objectives, which again exhibited an relative  gain over the corresponding model without. The rel- ative deltas of 100-LANG are with respect to the next  closest model on a language-by-language basis.", "labels": [], "entities": []}]}