{"title": [{"text": "Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section Title Generation", "labels": [], "entities": [{"text": "Improving Human Text Comprehension", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8876822143793106}, {"text": "Semi-Markov CRF-based Neural Section Title Generation", "start_pos": 43, "end_pos": 96, "type": "TASK", "confidence": 0.7016467700401942}]}], "abstractContent": [{"text": "Titles of short sections within long documents support readers by guiding their focus towards relevant passages and by providing anchor-points that help to understand the progression of the document.", "labels": [], "entities": []}, {"text": "The positive effects of section titles are even more pronounced when measured on readers with less developed reading abilities, for example in communities with limited labeled text resources.", "labels": [], "entities": []}, {"text": "We, therefore, aim to develop techniques to generate section titles in low-resource environments.", "labels": [], "entities": []}, {"text": "In particular, we present an extractive pipeline for section title generation by first selecting the most salient sentence and then applying deletion-based compression.", "labels": [], "entities": [{"text": "section title generation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7704679369926453}]}, {"text": "Our compression approach is based on a Semi-Markov Conditional Random Field that leverages unsu-pervised word-representations such as ELMo or BERT, eliminating the need fora complex encoder-decoder architecture.", "labels": [], "entities": [{"text": "BERT", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9732471704483032}]}, {"text": "The results show that this approach leads to competitive performance with sequence-to-sequence models with high resources, while strongly outper-forming it with low resources.", "labels": [], "entities": []}, {"text": "Ina human-subjects study across subjects with varying reading abilities, we find that our section titles improve the speed of completing comprehension tasks while retaining similar accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9948267340660095}]}], "introductionContent": [{"text": "Section titles in long documents that explain the content of the section improve the recall of content () while simultaneously increasing the reading speed.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9981763362884521}]}, {"text": "Additionally, they can provide a context to allow ambiguous words to be understood more easily) and to better understand the overall text).", "labels": [], "entities": []}, {"text": "However, most documents do not include titles for short segments or only provide a very abstract description of their topics, e.g. \"Geography\" When another old cave is discovered in the south of France, it is not usually news.", "labels": [], "entities": []}, {"text": "Rather, it is an ordinary event.", "labels": [], "entities": []}, {"text": "Such discoveries are so frequent these days that hardly anybody pays heed to them.", "labels": [], "entities": []}, {"text": "However, when the Lascaux cave complex was discovered in 1940, the world was amazed.", "labels": [], "entities": []}, {"text": "Painted directly on its walls were hundreds of scenes showing how people lived thousands of years ago.", "labels": [], "entities": []}, {"text": "The scenes show people hunting animals, such as bison or wild cats.", "labels": [], "entities": []}, {"text": "Other images depict birds and, most noticeably, horses, which appear in more than 300 wall images, by far outnumbering all other animals.", "labels": [], "entities": []}, {"text": "Early artists drawing these animals accomplished a monumental and difficult task.", "labels": [], "entities": []}, {"text": "They did not limit themselves to the easily accessible walls but carried their painting materials to spaces that required climbing steep walls or crawling into narrow passages in the Lascaux complex.", "labels": [], "entities": []}, {"text": "Unfortunately, the paintings have been exposed to the destructive action of water and temperature changes, which easily wear the images away.", "labels": [], "entities": []}, {"text": "Because the Lascaux caves have many entrances, air movement has also damaged the images inside.", "labels": [], "entities": []}, {"text": "Although they are not out in the open air, where natural light would have destroyed them long ago, many of the images have deteriorated and are barely recognizable.", "labels": [], "entities": []}, {"text": "To prevent further damage, the site was closed to tourists in 1963, 23 years after it was discovered.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SELECTOR is trained on the CNN-DM corpus (, which is the most commonly used corpus for news summarization.", "labels": [], "entities": [{"text": "SELECTOR", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.5823297500610352}, {"text": "CNN-DM corpus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9664826095104218}, {"text": "news summarization", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.6628174632787704}]}, {"text": "Each summary comprises a number of bullet points for an article, with an average length of 66 tokens and 4.9 bullet points.", "labels": [], "entities": []}, {"text": "The COMPRESSOR is trained on the Google sentence compression dataset, which comprises 200,000 sentence-headline pairs from news articles.", "labels": [], "entities": [{"text": "COMPRESSOR", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.6410892009735107}, {"text": "Google sentence compression dataset", "start_pos": 33, "end_pos": 68, "type": "DATASET", "confidence": 0.7045637890696526}]}, {"text": "The deletion-only version of the headlines was created by pruning the syntactic tree of the sentence and aligning the words with the headline.", "labels": [], "entities": []}, {"text": "The largest comparable corpus does not include deletion-only headlines.", "labels": [], "entities": []}, {"text": "We limit the vocabulary size to 50,000 words for both corpora.", "labels": [], "entities": []}, {"text": "Both SELECTOR and COM-PRESSOR use a two-layer bidirectional LSTM with 64 hidden dimensions for each direction, and a word-embedding size of 200.", "labels": [], "entities": [{"text": "SELECTOR", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.5608922243118286}]}, {"text": "Each linguistic feature is embedded into 30-dimensional space.", "labels": [], "entities": []}, {"text": "During training, the dropout probability is set to 0.5 ().", "labels": [], "entities": [{"text": "dropout probability", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.9365864396095276}]}, {"text": "The model is trained for up to 50 epochs or until the validation loss does not decrease for three consecutive epochs.", "labels": [], "entities": [{"text": "validation", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.911258339881897}]}, {"text": "We additionally halve the learning rate every time the validation loss does not decrease for two epochs.", "labels": [], "entities": [{"text": "validation", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.963013231754303}]}, {"text": "We use Adam () with AMSGrad (, an initial learning rate of 0.003, and a l2-penalty weight of 0.001.", "labels": [], "entities": [{"text": "AMSGrad", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.6041085720062256}, {"text": "initial learning rate", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.726466159025828}]}, {"text": "The RANKER uses the same LSTM configuration, but we optimize it with SGD with 0.9 momentum, and an initial learning rate of 0.25.", "labels": [], "entities": [{"text": "RANKER", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.4525166153907776}]}, {"text": "The S2S models have 64 hidden dimensions for each direction of the encoder, and 128 dimensions for the decoder LSTM.", "labels": [], "entities": []}, {"text": "They use one layer, and the decoder is initialized with the final state of the encoder.", "labels": [], "entities": []}, {"text": "Our optimizer for this task is adagrad with an initial learning rate of 0.15, and an accumulator value of 0.1 (Duchi et al., 2011).", "labels": [], "entities": [{"text": "accumulator", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9920786023139954}]}, {"text": "In the automated evaluation, we focus on the compression models and first conduct experiments with the full dataset to compute an upper bound on the performance of our approach.", "labels": [], "entities": []}, {"text": "This experiment functions as a benchmark to investigate how much better the S2S based approaches perform with sufficient data.", "labels": [], "entities": []}, {"text": "The next experiment investigates a scenario, in which data availability is limited and ranges from 100 to 1000 training examples.", "labels": [], "entities": []}, {"text": "We compare results with and without linguistic features to further evaluate whether these features improve the performance or whether contextual embeddings area sufficient representation.", "labels": [], "entities": []}, {"text": "In each experiment, we measure precision, recall, and F1-score of the predictions compared to the human reference, as well as the ROUGE-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9997826218605042}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9994274377822876}, {"text": "F1-score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9997497200965881}, {"text": "ROUGE-score", "start_pos": 130, "end_pos": 141, "type": "METRIC", "confidence": 0.9901708960533142}]}, {"text": "We additionally measure the length of the compressions to investigate whether methods delete a sufficient number of words.", "labels": [], "entities": []}, {"text": "We evaluated the effect of our generated titles in a between-subjects study on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.9619239370028178}]}, {"text": "We compared three different conditions: no titles, human-generated titles, and algorithmically generated titles by our SCRF+Ranking model.", "labels": [], "entities": []}, {"text": "Every participant kept their randomly assigned condition throughout all tasks.", "labels": [], "entities": []}, {"text": "We defined the following three tasks to approximately measure the effect of short section titles on (1) retention of text, (2) comprehension of text, and (3) retrieval of information.", "labels": [], "entities": [{"text": "retrieval of information", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.8573965628941854}]}, {"text": "(Retention) We first presented a text and then asked participants three questions about facts in the text.", "labels": [], "entities": []}, {"text": "(Comprehension) We showed a text and then asked the participants to generate a three-sentence summary of the text.", "labels": [], "entities": []}, {"text": "(Retrieval) We first presented two questions and then the text, prompting participants to find the answers.", "labels": [], "entities": []}, {"text": "Previous findings indicate that titles help with retention only when presented towards the beginning of a text).", "labels": [], "entities": [{"text": "retention", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.917742908000946}]}, {"text": "Thus, we place texts in the left margin at the top of a paragraph as shown in the example in.", "labels": [], "entities": []}, {"text": "This further avoids interrupting the reading flow of the long text while being integrated into the natural left-to-right reading process.", "labels": [], "entities": []}, {"text": "Although reading comprehension is well studied in natural language processing, most datasets focus on machine comprehension ().", "labels": [], "entities": []}, {"text": "Therefore, we adapted texts from the interactive reading practice by National Geographic, written by Helen Stephenson  texts include Geography, Science, Anthropology, and History; their length ranges from four to seven paragraphs.", "labels": [], "entities": []}, {"text": "Each text is accompanied by reading comprehension questions, which we utilized in the retention and retrieval tasks.", "labels": [], "entities": []}, {"text": "We first excluded those questions where the answer was part of either human-or algorithmically generated summary.", "labels": [], "entities": []}, {"text": "Of the remaining questions, we randomly selected three questions for each of the retention and retrieval tasks.", "labels": [], "entities": [{"text": "retention and retrieval", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.5423805812994639}]}, {"text": "The same questions were shown in either of the conditions.", "labels": [], "entities": []}, {"text": "Every participant completed six tasks, two for every possible task, one with intermediate and one with advanced difficulty.", "labels": [], "entities": []}, {"text": "To account for the different backgrounds of participants, we also asked participants about their perceived difficulty for each task on a 5-point Likert scale.", "labels": [], "entities": []}, {"text": "The total time to complete all tasks was limited to 30 minutes, and Turkers were paid $5.", "labels": [], "entities": []}, {"text": "In total, we recruited 144 participants who self-reported that they fluently spoke English, uniformly distributed over the three conditions.", "labels": [], "entities": []}, {"text": "They answered on average 68.25% of questions correctly and took 16.5 minutes to complete all six tasks.", "labels": [], "entities": []}, {"text": "This is approximately 30% faster than the fastest graduate student we recruited for pilot-testing, indicating that Turkers aimed to complete the tasks as fast as possible, possibly by only skimming the text.", "labels": [], "entities": []}, {"text": "We omitted results from participants with an answer accuracy of less than 25% (n=21), and excluded individual replies given in under 15 seconds (n=10) or over 10 minutes (n=5), leaving a total of 701 completed tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.647139847278595}]}, {"text": "After excluding outliers, the correct answer average was 75.64%, while the time to completion increased by 15 seconds to 16.75 minutes.", "labels": [], "entities": [{"text": "correct answer average", "start_pos": 30, "end_pos": 52, "type": "METRIC", "confidence": 0.8517550627390543}]}], "tableCaptions": [{"text": " Table 1: Results of our models on the large dataset comprising 200,000 compression examples.", "labels": [], "entities": []}, {"text": " Table 2: The causal effects of the human and algorithmic section titles on different measures differ across tasks. All  the shown effect sizes are measured in comparison to the baseline without any shown titles. Significant p-values at  a 0.05 level are marked with a *.", "labels": [], "entities": []}]}