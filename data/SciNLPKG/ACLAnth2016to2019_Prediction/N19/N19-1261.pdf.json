{"title": [{"text": "Automatic learner summary assessment for reading comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Automating the assessment of learner summaries provides a useful tool for assessing learner reading comprehension.", "labels": [], "entities": [{"text": "assessment of learner summaries", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6178978830575943}]}, {"text": "We present a summarization task for evaluating non-native reading comprehension and propose three novel approaches to automatically assess the learner summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9784227013587952}]}, {"text": "We evaluate our models on two datasets we created and show that our models outperform traditional approaches that rely on exact word match on this task.", "labels": [], "entities": []}, {"text": "Our best model produces quality assessments close to professional examiners.", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarization is a well-established method of measuring reading proficiency in traditional English as a second or other language (ESOL) assessment.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8660205006599426}]}, {"text": "It is considered an effective approach to test both cognitive and contextual dimensions of reading ().", "labels": [], "entities": []}, {"text": "However, due to the high time and cost demands of manual summary assessment, modern English exams usually replace the summarization task with multiple choice or short answer questions that are easier to score).", "labels": [], "entities": [{"text": "summarization task", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8716552257537842}]}, {"text": "Automating the assessment of learner summarization skills provides an efficient evaluation method for the quality of the learner summary and can lead to effective educational applications to enhance reading comprehension tasks.", "labels": [], "entities": [{"text": "learner summarization skills", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.6169004042943319}]}, {"text": "In this paper, we present a summarization task for evaluating non-native reading comprehension and propose three novel machine learning approaches to assessing learner summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.983089804649353}]}, {"text": "First, we extract features to measure the content similarity between the reading passage and the summary.", "labels": [], "entities": []}, {"text": "Secondly, we calculate a similarity matrix based on sentence-to-sentence similarity between * The work by the first author was done at the University of Cambridge prior to joining Amazon Research.", "labels": [], "entities": [{"text": "Amazon Research", "start_pos": 180, "end_pos": 195, "type": "DATASET", "confidence": 0.8963846266269684}]}, {"text": "the reading passage and the summary, and apply a Convolutional Neural Network (CNN) model to assess the summary quality using the similarity matrix.", "labels": [], "entities": []}, {"text": "Thirdly, we build an end-to-end summarization assessment model using the Long Short Term Memory (LSTM) model.", "labels": [], "entities": [{"text": "summarization assessment", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8660853505134583}]}, {"text": "Finally, we combine the three approaches in a single system using a simple parallel ensemble modeling technique.", "labels": [], "entities": []}, {"text": "We compiled two datasets to evaluate our models, and we release this data with the paper.", "labels": [], "entities": []}, {"text": "We show that our models outperform traditional approaches that rely on exact word match on the task and that our best model produces quality assessments close to professional examiners.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on the real learner data and on the simulated learner data, for consistency, using 5-fold cross validation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.9829548001289368}]}, {"text": "In each fold, 60% of the data is used as the training set, 20% as the development set, and 20% as the test set.", "labels": [], "entities": []}, {"text": "We compare our models against five baselines: most frequent baseline, random baseline, ROUGE  baseline, BLEU baseline, and ROUGE + BLEU baseline.", "labels": [], "entities": [{"text": "ROUGE  baseline", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9564844369888306}, {"text": "BLEU baseline", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.9698990881443024}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.7193325757980347}]}, {"text": "We use accuracy to evaluate the models on the simulated learner data, and on the real learner data, we report scores of two evaluation metrics: Pearson correlation coefficient (PCC) and Root Mean Square Error (RMSE), which are commonly used for evaluating regression models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9989827275276184}, {"text": "Pearson correlation coefficient (PCC)", "start_pos": 144, "end_pos": 181, "type": "METRIC", "confidence": 0.9622492094834646}, {"text": "Root Mean Square Error (RMSE)", "start_pos": 186, "end_pos": 215, "type": "METRIC", "confidence": 0.8747449006353106}]}, {"text": "shows the results of the baseline and the four types of models on the simulated learner data, and reports the results of the models on the real learner data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of the regression model performance on the learner data. We use the bold font to indicate the best  model for each method. The asterisk sign indicates the best performance across all models.", "labels": [], "entities": [{"text": "learner data", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7338705658912659}]}]}