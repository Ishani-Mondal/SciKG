{"title": [{"text": "What just happened? Evaluating retrofitted distributional word vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has attempted to enhance vector space representations using information from structured semantic resources.", "labels": [], "entities": []}, {"text": "This process, dubbed retrofitting Faruqui et al.", "labels": [], "entities": []}, {"text": "(2015), has yielded improvements in word similarity performance.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.733431488275528}]}, {"text": "Research has largely focused on the retrofitting algorithm, or on the kind of struc-tured semantic resources used, but little research has explored why some resources perform better than others.", "labels": [], "entities": []}, {"text": "We conducted a fine-grained analysis of the original retrofitting process , and found that the utility of different lexical resources for retrofitting depends on two factors: the coverage of the resource and the evaluation metric.", "labels": [], "entities": []}, {"text": "Our assessment suggests that the common practice of using correlation measures to evaluate increases in performance against full word similarity benchmarks 1) obscures the benefits offered by smaller resources , and 2) overlooks incremental gains in word similarity performance.", "labels": [], "entities": []}, {"text": "We propose root-mean-square error (RMSE) as an alternative evaluation metric, and demonstrate that correlation measures and RMSE sometimes yield opposite conclusions concerning the efficacy of retrofitting.", "labels": [], "entities": [{"text": "root-mean-square error (RMSE)", "start_pos": 11, "end_pos": 40, "type": "METRIC", "confidence": 0.856237530708313}, {"text": "RMSE", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.8969749212265015}]}, {"text": "This point is illustrated byword vectors retrofitted with novel treatments of the FrameNet data (Fillmore and Baker, 2010).", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.934334397315979}]}], "introductionContent": [{"text": "One of the most challenging tasks in the field of Natural Language Processing (NLP) is accurately encoding meaning into a computational system.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7705996433893839}]}, {"text": "Currently, the predominant approach is to represent the meanings of linguistic units, such as words or phrases, as vectors in a high-dimensional space.", "labels": [], "entities": []}, {"text": "Vector embeddings are trained overlarge text corpora using machine-learning techniques, and have proven useful fora wide range of applications, such as named entity recognition, semantic role labeling), sentiment analysis ( , and machine translation (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.6208378573258718}, {"text": "semantic role labeling", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.6607667009035746}, {"text": "sentiment analysis", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.9779343008995056}, {"text": "machine translation", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.8065614402294159}]}, {"text": "Word vectors are typically trained solely on the distributional information from text corpora.", "labels": [], "entities": []}, {"text": "Recent work has attempted to improve word vectors by infusing them with information from semantic resources in a post-processing step.", "labels": [], "entities": []}, {"text": "This technique, referred to as retrofitting, was introduced by.", "labels": [], "entities": []}, {"text": "They adjusted pretrained embeddings based on lexical relations in WordNet,, and the Paraphrase Database ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9709015488624573}]}, {"text": "In some cases, this method yielded gains in word similarity performance.", "labels": [], "entities": []}, {"text": "Retrofitting has been extended in a variety of ways.", "labels": [], "entities": []}, {"text": "Briefly, these include 1) adding word-toword relations to encompass more than just similarity relations, such as by directly introducing antonymy relations, or by explicitly modeling the pairwise relations between items (); 2) increasing the size of the output vocabulary (, or extending the process to affect the word vectors of words outside of the semantic resource; and 3) constructing sense-specific word vectors using a word sense ontology ( , or word sense information learned from parallel text corpora.", "labels": [], "entities": []}, {"text": "However, while has certainly spawned a productive line of research into improving pre-trained word vectors, the original study contained a puzzling finding: retrofitting with certain semantic resources actually appeared to harm the quality of the word embeddings.", "labels": [], "entities": []}, {"text": "In principle, if semantic resources contain information that is not already captured by the word vectors, then retrofitting should always improve them.", "labels": [], "entities": []}, {"text": "In order to understand why some semantic resources appear better suited for retrofitting word vectors, we conducted a fine-grained analysis ofs original technique.", "labels": [], "entities": []}, {"text": "Given their popularity, we focused on word similarity evaluations.", "labels": [], "entities": [{"text": "word similarity evaluations", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8195874492327372}]}, {"text": "We observe that the perceived usefulness of a semantic resource depends on its coverage of the words in the evaluation benchmark.", "labels": [], "entities": []}, {"text": "Furthermore, we report that the choice of evaluation metric can lead to different conclusions.", "labels": [], "entities": []}, {"text": "We note that some gains in performance are not captured by correlation measures, and propose that root-mean-square error (RMSE) is more appropriate for measuring changes in word similarity performance.", "labels": [], "entities": [{"text": "root-mean-square error (RMSE)", "start_pos": 98, "end_pos": 127, "type": "METRIC", "confidence": 0.8779014945030212}]}], "datasetContent": [{"text": "The standard approach to evaluate the performance of word vectors on word similarity judgments is to compute the cosine similarity values between each pair of words in the dataset and then calculate the correlation between these values and the similarity scores collected from human raters.", "labels": [], "entities": [{"text": "word similarity judgments", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7263921697934469}]}, {"text": "A similar technique is used to assess the utility of different semantic resources in retrofitting word vectors: increases in correlation are taken to be indicative that information from the resource has been successfully injected into the word vectors.", "labels": [], "entities": []}, {"text": "For both types of evaluations, Spearman correlation has become the preferred correlation measure.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 31, "end_pos": 51, "type": "METRIC", "confidence": 0.7813028991222382}, {"text": "correlation", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9552730321884155}]}, {"text": "However, there are several reasons that this method maybe misleading.", "labels": [], "entities": []}, {"text": "The first concerns the issue of the relative coverage of each resource.", "labels": [], "entities": []}, {"text": "Simply put, not every resource contains all of the words in the evaluation dataset.", "labels": [], "entities": []}, {"text": "If a resource lacks the words fora particular similarity judgment, then the predicted score will be the same for both the baseline and retrofitted vectors.", "labels": [], "entities": []}, {"text": "This may have important consequences on the evaluation metric: the fixed scores can throw off the global ranking of the predicted scores, which is measured by the Spearman correlation.", "labels": [], "entities": []}, {"text": "For every word pair in a word similarity dataset, a resource can contain 1) both words, 2) one of the words, or 3) neither of the words.", "labels": [], "entities": []}, {"text": "If the goal of the evaluation is to determine whether the knowledge of particular semantic resources can be added to word vectors, then it seems reasonable to only evaluate the resource on the word pairs it covers.", "labels": [], "entities": []}, {"text": "In this case, the resource will either group the two words together or place them in separate groups, which can be interpreted as explicitly indicating whether the two words are semantically related or not.", "labels": [], "entities": []}, {"text": "Conversely, it is obvious that retrofitting will not improve the vectors for the word pairs for which neither word is in the semantic resource.", "labels": [], "entities": []}, {"text": "The situation where only one word is present is more complicated.", "labels": [], "entities": []}, {"text": "For example, imagine that a resource contained the word view but not the word skyline.", "labels": [], "entities": []}, {"text": "Following retrofitting the vector for view will move while the vector for skyline will stay the same.", "labels": [], "entities": []}, {"text": "The relationship between view and skyline will either become more accurate or less accurate, but this change does not directly stem from the semantic resource.", "labels": [], "entities": []}, {"text": "If the goal of the retrofitting evaluation is to assess the usefulness of particular semantic resources, then including these kinds of word pairs is misleading, since the observed changes are incidental and do not reflect the semantic groupings in the resource.", "labels": [], "entities": []}, {"text": "In our analyses, \"all pairs\" shows the performance of the word vectors using all of the word similarity judgments, and \"pairs in resource\" shows their performance using only the subset comprised of judgments for which both words were contained in the semantic resource.", "labels": [], "entities": []}, {"text": "Our more radical proposal is to consider an entirely different evaluation metric altogether.", "labels": [], "entities": []}, {"text": "Measures of correlation indicate how well word vectors are able to predict the similarity judgments.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9821828007698059}]}, {"text": "Spearman correlation specifically measures how well word vectors are able to predict the correct rankings of similarity judgments.", "labels": [], "entities": []}, {"text": "For example, according to the MEN3K dataset, brick and construction should be ranked as less similar than town and village.", "labels": [], "entities": [{"text": "MEN3K dataset", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9860262274742126}]}, {"text": "Another conceivable way to test the word vectors ability to capture word similarity knowledge would be to directly compare the word vectors' predicted score with the human score.", "labels": [], "entities": []}, {"text": "According to MEN3K, the average rated similarity for town and village was 43 out of 50.", "labels": [], "entities": [{"text": "MEN3K", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9587811231613159}, {"text": "similarity", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9342218637466431}]}, {"text": "Taken literally, after normalizing the original scores the cosine similarity should be exactly 0.86.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 59, "end_pos": 76, "type": "METRIC", "confidence": 0.8923899531364441}]}, {"text": "We operationalized this by evaluating word vectors using root-mean-square error (RMSE).", "labels": [], "entities": [{"text": "root-mean-square error (RMSE)", "start_pos": 57, "end_pos": 86, "type": "METRIC", "confidence": 0.8197720050811768}]}, {"text": "This approach seems particularly appealing for measuring the effects of retrofitting because each similarity judgment contributes independently to the RMSE score.", "labels": [], "entities": [{"text": "RMSE score", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.7762549519538879}]}, {"text": "One may wonder whether Pearson correlation, which measures linear association, might serve as a better comparison to RMSE.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.7605689167976379}]}, {"text": "To address this concern, we employed the harmonic mean of the Pearson and Spearman correlations as our correlation measure.", "labels": [], "entities": []}, {"text": "This blends the linear measure (Pearson) with the standardly-employed measure (Spearman).", "labels": [], "entities": [{"text": "linear measure (Pearson)", "start_pos": 16, "end_pos": 40, "type": "METRIC", "confidence": 0.9022708058357238}]}, {"text": "However, we note that the resulting baseline and retrofitted scores were very similar across correlation measures, and so our conclusions regarding the choice of evaluation metric were unaffected by this decision.", "labels": [], "entities": []}, {"text": "In the analysis that follows, we considered the effect of resource coverage and evaluation metric  on the results of retrofitting.", "labels": [], "entities": []}, {"text": "There were four conditions: 1) Correlation, all word pairs in the benchmark, 2) Correlation, only those pairs in which both words were in resource, 3) RMSE, all word pairs in the benchmark, and 4) RMSE, only those pairs in which both words were in resource.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.958437442779541}, {"text": "RMSE", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.8517568111419678}, {"text": "RMSE", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.871679425239563}]}, {"text": "If one of the words in a word pair was missing from the word vectors, then it was assigned a predicted cosine similarity of zero.", "labels": [], "entities": []}, {"text": "(This only occurred with the RW dataset, and was limited to the all pairs conditions.) shows the baseline word similarity performance according to the harmonic mean of the correlation measures and RMSE.", "labels": [], "entities": [{"text": "RW dataset", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9540175795555115}, {"text": "RMSE", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.6468890905380249}]}, {"text": "As a reference, we include the NumberBatch (NB) vectors, which recently demonstrated state-of-the-art word similarity performance).", "labels": [], "entities": []}, {"text": "Correlation and RMSE give similar baseline results among the vector sets and their ability to predict the four similarity benchmarks: NB performs the best.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9452847242355347}, {"text": "RMSE", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.7494698762893677}]}, {"text": "The exception is that SG scores a slightly better RMSE score on the MEN3K dataset.", "labels": [], "entities": [{"text": "SG", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.7417060732841492}, {"text": "RMSE score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9459033012390137}, {"text": "MEN3K dataset", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9838335812091827}]}, {"text": "shows the measured improvements in correlation due to retrofitting.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9915502071380615}]}, {"text": "This mirrors's original finding that the PPDB offers the most improvements, and that grouping words by FrameNet frames (FN) usually leads to worse performance.", "labels": [], "entities": []}, {"text": "Note that this finding is observed after correcting for the issue from Faruqui et al. which omitted data from FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9392619729042053}]}, {"text": "This plot also suggests that using FrameNet frame elements (FN-ANNO) to group words is very detrimental to word vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of word forms and word groupings  per semantic resource.", "labels": [], "entities": []}, {"text": " Table 2: Baseline word vector similarity performance.  Scores are listed in the form \"Correlation/RMSE\".  Bold face indicates the best-performing set of vectors  for each similarity dataset for their correlation score  and for their RMSE score. Recall that lower RMSE  is better.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.6098247766494751}, {"text": "correlation score", "start_pos": 201, "end_pos": 218, "type": "METRIC", "confidence": 0.9557846784591675}, {"text": "RMSE", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9905005097389221}]}]}