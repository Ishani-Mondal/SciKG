{"title": [{"text": "Practical, Efficient, and Customizable Active Learning for Named Entity Recognition in the Digital Humanities", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7118175427118937}]}], "abstractContent": [{"text": "Scholars in inter-disciplinary fields like the Digital Humanities are increasingly interested in semantic annotation of specialized corpora.", "labels": [], "entities": []}, {"text": "Yet, under-resourced languages, imperfect or noisily structured data, and user-specific classification tasks make it difficult to meet their needs using off-the-shelf models.", "labels": [], "entities": []}, {"text": "Manual annotation of large corpora from scratch, meanwhile , can be prohibitively expensive.", "labels": [], "entities": []}, {"text": "Thus, we propose an active learning solution for named entity recognition, attempting to maximize a custom model's improvement per additional unit of manual annotation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6346428195635477}]}, {"text": "Our system robustly handles any domain or user-defined label set and requires no external resources, enabling quality named entity recognition for Humanities corpora where such resources are not available.", "labels": [], "entities": []}, {"text": "Evaluating on typologically dis-parate languages and datasets, we reduce required annotation by 20-60% and greatly out-perform a competitive active learning baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reaping the benefits of recent advances in Named Entity Recognition (NER) is challenging when dealing with under-resourced languages, niche domains, imperfect or noisily structured data, or user-specific classification tasks.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.7983384331067404}]}, {"text": "Scholars in interdisciplinary fields like the Digital Humanities (DH) are increasingly interested in semantic annotation of specialized corpora that invoke many of these challenges.", "labels": [], "entities": []}, {"text": "Thus, such corpora cannot easily be annotated automatically with blackbox, off-the-shelf NER models.", "labels": [], "entities": []}, {"text": "Manual annotation of large corpora from scratch, meanwhile, can be prohibitively costly.", "labels": [], "entities": []}, {"text": "Successful DH initiatives like the Pelagios Commons (, which collects geospatial data from historical sources, often require extensive funding, relying on considerable manual annotation (.", "labels": [], "entities": []}, {"text": "To this end, we introduce the Humanities Entity Recognizer (HER), 1 a whitebox toolkit for buildyour-own NER models, freely available for public use.", "labels": [], "entities": [{"text": "Humanities Entity Recognizer (HER)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.5884406367937723}]}, {"text": "HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model.", "labels": [], "entities": []}, {"text": "Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by.", "labels": [], "entities": []}, {"text": "In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (, we observe significant improvements by integrating all three criteria.", "labels": [], "entities": []}, {"text": "In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework.", "labels": [], "entities": []}, {"text": "This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991982579231262}]}, {"text": "The standard, exclusive evaluation framework, by contrast, only measures the accuracy of the final trained model's predictions on a held out test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9971957206726074}]}, {"text": "Thus, the inclusive framework is relevant to the user who wants to annotate a finite corpus as fast and as accurately as possible by any means necessary, whereas the exclusive framework is relevant to the user who wants to build an NER tool that can generalize well to other corpora.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments comparing several combinations of active learning algorithms and NER model architectures in both frameworks across many typologically diverse languages and domains.", "labels": [], "entities": []}, {"text": "The systematic differences between inclusive and exclusive results demonstrate that while deep NER model architectures are highly preferable for tagging held out sentences, shallow models) perform better on sentences that could have been chosen for manual annotation but were not selected by the active learning algorithm.", "labels": [], "entities": [{"text": "tagging held out sentences", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.8629534244537354}]}, {"text": "We argue for the importance of considering both frameworks when evaluating an active learning approach, as the intended application determines which framework is more relevant and thus, which model should be employed.", "labels": [], "entities": []}, {"text": "Controlling for the NER model, HER's active learning sentence ranking component achieves significant improvement over a competitive baseline.", "labels": [], "entities": []}, {"text": "Because HER does not reference the inference model during sentence ranking, this provides counter evidence to's hypothesis that non-native active learning is suboptimal.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe several experiments evaluating HER's performance on diverse corpora.", "labels": [], "entities": [{"text": "HER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7350619435310364}]}, {"text": "When a standard test set is available, we perform inclusive evaluation on the combined train and dev sets and evaluate exclusively on test.", "labels": [], "entities": []}, {"text": "Otherwise, we only evaluate inclusively.", "labels": [], "entities": []}, {"text": "In both settings, we compare multiple combinations of ranking systems and taggers over a learning curve, reporting F1 exact match accuracy of identified entities.", "labels": [], "entities": [{"text": "F1 exact match accuracy", "start_pos": 115, "end_pos": 138, "type": "METRIC", "confidence": 0.8560439348220825}]}, {"text": "In all figures, line dashing (contiguous, dashed, or dotted) denotes inference model (CRF, BiLSTM-CRF, or CNN-BiLSTM), whereas line accents (stars, circles, triangles, or squares) denotes active learning method.", "labels": [], "entities": []}, {"text": "Besides PTDL, we also consider a random active learning method (RAND), MNLP, and Erdmann et al.", "labels": [], "entities": [{"text": "PTDL", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.6494958400726318}, {"text": "MNLP", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7511855959892273}]}, {"text": "(2016)'s CAP algorithm.", "labels": [], "entities": []}, {"text": "Like PTDL, CAP ranks sentences based on frequency weighted OOVs, but calculates weights based on capitalization patterns, prioritizing capitalized OOVs occurring in non-sentence initial position.", "labels": [], "entities": []}, {"text": "Quantity of training data is reported as percentage of the entire corpus for inclusive evaluations, and as tokens actively annotated (i.e., not counting the random seed sentences) for exclusive evaluations.", "labels": [], "entities": [{"text": "Quantity", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9183034896850586}]}, {"text": "For consistency, following seed annotation, we always fetch additional annotation batches at the following intervals, in tokens: 1K, 4K, 5K, 10K, 20K until we reach 100K total tokens, 50K until 300K total, 100K until 500K total, and 250K from there.", "labels": [], "entities": []}, {"text": "For all experiments leveraging neural taggers, we use freely available pretrained embeddings (, except for Latin, where we train fasttext () embeddings on the Perseus () and Latin Library collections with default parameters (using pretrained embeddings yield small performance boosts that decrease with additional training data).", "labels": [], "entities": [{"text": "neural taggers", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7774686813354492}, {"text": "Latin Library collections", "start_pos": 174, "end_pos": 199, "type": "DATASET", "confidence": 0.8866032958030701}]}, {"text": "We conclude this section with a direct comparison to the recently proposed active learning pipeline of and their MNLP ranking algorithm.", "labels": [], "entities": []}, {"text": "In the inclusive framework, regardless of corpus size, BiLSTM-CRFs do not surpass CRFs until the accuracy is so high that the distinction is negligible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9988711476325989}]}, {"text": "Promoting overfitting by reducing dropout did not significantly affect this result.", "labels": [], "entities": []}, {"text": "In the exclusive framework, BiLSTM-CRF surpasses CRF around 50K tokens annotated.", "labels": [], "entities": []}, {"text": "This holds for all languages and corpora we investigate, suggesting quantity of data annotated is more predictive of exclusive performance trends, whereas proportion of the corpus annotated better predicts inclusive trends.", "labels": [], "entities": []}], "tableCaptions": []}