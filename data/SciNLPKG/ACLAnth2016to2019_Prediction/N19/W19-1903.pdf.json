{"title": [{"text": "Extracting Adverse Drug Event Information with Minimal Engineering", "labels": [], "entities": [{"text": "Extracting Adverse Drug Event Information", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8863981008529663}]}], "abstractContent": [{"text": "In this paper we describe an evaluation of the potential of classical information extraction methods to extract drug-related attributes, including adverse drug events, and compare to more recently developed neural methods.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7765701115131378}]}, {"text": "We use the 2018 N2C2 shared task data as our gold standard data set for training.", "labels": [], "entities": [{"text": "N2C2 shared task data", "start_pos": 16, "end_pos": 37, "type": "DATASET", "confidence": 0.8330229967832565}]}, {"text": "We train support vector machine classifiers to detect drug and drug attribute spans, and pair these detected entities as training instances for an SVM relation classifier, with both systems using standard features.", "labels": [], "entities": [{"text": "SVM relation classifier", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7459984024365743}]}, {"text": "We compare to baseline neu-ral methods that use standard contextualized embedding representations for entity and relation extraction.", "labels": [], "entities": [{"text": "entity and relation extraction", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.6446060463786125}]}, {"text": "The SVM-based system and a neural system obtain comparable results, with the SVM system doing better on concepts and the neural system performing better on relation extraction tasks.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7631901502609253}]}, {"text": "The neural system obtains surprisingly strong results compared to the system based on years of research in developing features for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7991716265678406}]}], "introductionContent": [{"text": "Adverse drug events (ADEs) describe undesirable signs and symptoms that occur consequent to administration of a medication.", "labels": [], "entities": [{"text": "Adverse drug events (ADEs)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6301573266585668}]}, {"text": "ADEs maybe identified in randomized controlled trials (RCTs), observational studies, spontaneous reports such as those gathered in the Food and Drug Administrations (FDAs) Adverse Event Reporting System (FAERS), or manual chart review of data in electronic health records (EHRs).", "labels": [], "entities": []}, {"text": "RCTs have notable limitations for pharmacoepidemiology, including strict inclusion and exclusion criteria that limit their generalizability, small cohort sizes that make them under-powered for detecting rarer ADEs, and time-limited study periods that prevent detection of ADEs that occur with longer drug administration.", "labels": [], "entities": []}, {"text": "Although drug manufacturers are required to submit postmarket adverse event reports to the FDA, this information is not uniformly available to clinicians (.", "labels": [], "entities": []}, {"text": "Therefore, the 21st Century Cures Act directs the FDA to use realworld data (RWD) in the drug approval process.", "labels": [], "entities": [{"text": "21st Century Cures", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.5249582131703695}]}, {"text": "Use of RWD is particularly important for medications that are commonly used off-label, for example, those targeted for treatment of rare diseases such as pulmonary hypertension in children (.", "labels": [], "entities": [{"text": "RWD", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.6592442393302917}]}, {"text": "Electronic health records (EHRs) provide an opportunity to capture such data reflecting real-world use of approved medications.", "labels": [], "entities": []}, {"text": "Most studies of pharmacovigilance using RWD are based on healthcare insurance claims-for instance, the FDAs Sentinel program-because claims data contains longitudinal information about medication dispensing and clinical diagnoses.", "labels": [], "entities": [{"text": "FDAs Sentinel program-because claims data", "start_pos": 103, "end_pos": 144, "type": "DATASET", "confidence": 0.8561928510665894}, {"text": "medication dispensing and clinical diagnoses", "start_pos": 185, "end_pos": 229, "type": "TASK", "confidence": 0.6248763501644135}]}, {"text": "However, claims data may lack sensitivity for identification of ADEs, since not all signs and symptoms are submitted to insurers for billing purposes.", "labels": [], "entities": [{"text": "identification of ADEs", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7684695521990458}]}, {"text": "Reliance on claims data may also lead to incongruous results, such as a Mini-Sentinel study that found-contrary to data from several large RCTs-that dabigatran was associated with a lower risk of gastrointestinal bleeding than warfarin ().", "labels": [], "entities": []}, {"text": "Limiting studies using RWD to structured data alone neglects the rich data that maybe found in the unstructured, free text portion of the EHR.", "labels": [], "entities": [{"text": "EHR", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.9667609333992004}]}, {"text": "However, this data is not readily available for computation.", "labels": [], "entities": []}, {"text": "Extracting this information requires natural language processing (NLP) methods.", "labels": [], "entities": [{"text": "Extracting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9583098888397217}]}, {"text": "The NLP sub-task of information extraction is concerned with finding concepts in text and the relations between them ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.8124069571495056}]}, {"text": "Examples of information extraction are named entity recognition (e.g., finding the names of peo-ple, organizations, etc.) and relation extraction (e.g., determining whether the employment relation holds between a detected person like Tim Cook and a detected organization like Apple).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7397869378328323}, {"text": "entity recognition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7654297053813934}, {"text": "relation extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7693636417388916}]}, {"text": "A recent National NLP Clinical Challenge (n2c2)-hosted shared task annotated ADEs in clinical text in a style that is amenable to an information extraction approach.", "labels": [], "entities": [{"text": "National NLP Clinical Challenge (n2c2)-hosted shared task annotated ADEs", "start_pos": 9, "end_pos": 81, "type": "DATASET", "confidence": 0.910824735959371}, {"text": "information extraction", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.7748138308525085}]}, {"text": "Specifically, annotations for things like drug names or drug attributes, including dosages, routes, and adverse events are entitylike spans, while the pairing of attributes and drugs are naturally represented as relations to be extracted.", "labels": [], "entities": []}, {"text": "The benefit of framing the ADE task as an information extraction task is that decades of research in information extraction can be brought to bear on the task, before even considering the specifics of the domain or the task.", "labels": [], "entities": [{"text": "information extraction task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7985045611858368}, {"text": "information extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7367715239524841}]}, {"text": "In this work, we sought to evaluate a number of standard information extraction methods, including both standard clinical NLP tools and general domain methods, with the goals of setting strong baselines, learning how much performance is dependent on domain knowledge, and comparing classical machine learning to new deep learning approaches.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7294431179761887}]}], "datasetContent": [{"text": "The tables show results on the concept extraction, relation classification, and end-to-end relation extraction.", "labels": [], "entities": [{"text": "concept extraction", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7289952486753464}, {"text": "relation classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.9163882732391357}, {"text": "relation extraction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7441067397594452}]}, {"text": "In: Results of relation extraction experiments (system-generated entity arguments) with SVM vs. Neural systems.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7607700824737549}]}, {"text": "the concept extraction task, the systems perform very similarly on average, with the SVM featureengineered approach obtaining a micro-averaged F-score of 0.91 and the neural system scoring 0.90 (final row).", "labels": [], "entities": [{"text": "concept extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7191678583621979}, {"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9369783401489258}]}, {"text": "By comparison, the best performing system at the n2c2 shared task scored 0.94 on the concept extraction task.", "labels": [], "entities": [{"text": "concept extraction task", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.7716167767842611}]}, {"text": "The middle rows of show the performance for different concept types.", "labels": [], "entities": []}, {"text": "The two systems perform similarly across concept types, except that the SVM-based system performs much better on Route, while the neural system is much better at extracting Reason and Duration concepts.", "labels": [], "entities": []}, {"text": "For relation classification with gold standard concepts given as input, top), the neural system is at least as good as the SVM-based system for every relation type, and the microaveraged neural system is 0.93 compared to the 0.91 for the SVM-based system.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9054798781871796}]}, {"text": "Most improvement is seen in the Drug-Duration and DrugFrequency categories.", "labels": [], "entities": []}, {"text": "By comparison, the best performing system in the n2c2 challenge scored 0.96 on Track 2.", "labels": [], "entities": []}, {"text": "In the end-to-end relation extraction task (Table 3, bottom), the neural system is again two points better than the SVM in F1 score.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8132764101028442}, {"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9854921698570251}]}, {"text": "The SVM performs better on Drug-Route and Drug-ADE, while the neural system performs better in DrugDuration and Drug-Reason.", "labels": [], "entities": []}, {"text": "The best performing system in the n2c2 challenge scored 0.89 on Track 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of entity recognition experiments with  SVM vs. Neural systems.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7639537155628204}]}, {"text": " Table 2: Results of relation classification experiments  (gold standard entity arguments) with SVM vs. Neural  systems.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7945762276649475}]}, {"text": " Table 3: Results of relation extraction experiments  (system-generated entity arguments) with SVM vs.  Neural systems.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7650429308414459}]}]}