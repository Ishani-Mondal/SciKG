{"title": [{"text": "Predicting the Type and Target of Offensive Posts in Social Media", "labels": [], "entities": []}], "abstractContent": [{"text": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages.", "labels": [], "entities": []}, {"text": "However, previous work on this topic did not consider the problem as a whole, but rather fo-cused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression.", "labels": [], "entities": []}, {"text": "In contrast, here we target several different kinds of offensive content.", "labels": [], "entities": []}, {"text": "In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media.", "labels": [], "entities": []}, {"text": "For this purpose, we complied the Offensive Language Identification Dataset (OLID), anew dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available.", "labels": [], "entities": []}, {"text": "We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks.", "labels": [], "entities": [{"text": "hate speech identification", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.8069206674893697}, {"text": "aggression detection", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.7130419909954071}]}, {"text": "We further experiment with and we compare the performance of different machine learning models on OLID.", "labels": [], "entities": [{"text": "OLID", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8411540985107422}]}], "introductionContent": [{"text": "Offensive content has become pervasive in social media and thus a serious concern for government organizations, online communities, and social media platforms.", "labels": [], "entities": []}, {"text": "One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation.", "labels": [], "entities": []}, {"text": "In the last few years, there have been several studies on the application of computational methods to deal with this problem.", "labels": [], "entities": []}, {"text": "Prior work has studied offensive language in Twitter (, Wikipedia comments, and Facebook posts (.", "labels": [], "entities": []}, {"text": "Previous studies have looked into different aspects of offensive language such as the use of abusive language (, (cyber-)aggression (, (cyber-)bullying (, toxic comments 1 , hate speech (, and offensive language (.", "labels": [], "entities": []}, {"text": "Recently, analyzed the similarities between different approaches proposed in previous work and argued that there was a need fora typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit.", "labels": [], "entities": []}, {"text": "further applied this idea to German tweets.", "labels": [], "entities": []}, {"text": "They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further sub-classifying the offensive tweets as profanity, insult, or abuse.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, no prior work has explored the target of the offensive language, which might be important in many scenarios, e.g., when studying hate speech with respect to a specific target.", "labels": [], "entities": []}, {"text": "Below, we aim at bridging this gap.", "labels": [], "entities": []}, {"text": "More generally, in this paper, we expand on the above ideas by proposing a novel three-level hierarchical annotation schema that encompasses the following three general categories:: Four tweets from the dataset, with their labels for each level of the annotation schema.", "labels": [], "entities": []}, {"text": "We further use the above schema to annotate a large dataset of English tweets, which we make publicly available online.", "labels": [], "entities": []}, {"text": "The key contributions of this paper can be summarized as follows: \u2022 We propose anew three-level hierarchical annotation schema for abusive language detection and characterization.", "labels": [], "entities": [{"text": "abusive language detection and characterization", "start_pos": 131, "end_pos": 178, "type": "TASK", "confidence": 0.6814142107963562}]}, {"text": "\u2022 We apply the schema to create Offensive Language Identification Dataset (OLID), anew large-scale dataset of English tweets with high-quality annotation of the target and type of offenses.", "labels": [], "entities": []}, {"text": "\u2022 We perform experiments on OLID using different machine learning models for each level of the annotation, thus setting important baselines to compare to in future work.", "labels": [], "entities": []}, {"text": "While each of these sub-tasks tackles a particular type of abuse or offense, they share similar properties and the hierarchical annotation model proposed in this paper aims to capture this.", "labels": [], "entities": []}, {"text": "Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we believe that OLID's use of a hierarchical annotation schema makes it a useful resource for various offensive language identification and characterization tasks.", "labels": [], "entities": [{"text": "offensive language identification and characterization", "start_pos": 266, "end_pos": 320, "type": "TASK", "confidence": 0.7929910778999328}]}], "datasetContent": [{"text": "We experiment with various models: SVM Our simplest machine learning model is a linear SVM trained on word unigrams.", "labels": [], "entities": []}, {"text": "SVMs have achieved state-of-the-art results for many text classification tasks ( ).", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8703410625457764}]}, {"text": "BiLSTM We also experiment with a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from a pre-existing model for sentiment analysis.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8347464799880981}, {"text": "sentiment analysis", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.9606920778751373}]}, {"text": "The model consists of (i) an input embedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input features.", "labels": [], "entities": []}, {"text": "The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the final prediction.", "labels": [], "entities": []}, {"text": "We set two input channels for the input embedding layers: pre-trained FastText embeddings, as well as updatable embeddings learned by the model during training.", "labels": [], "entities": []}, {"text": "CNN Finally, we experiment with a Convolutional Neural Network (CNN) model based on the architecture of, and using the same multi-channel inputs as the above BiLSTM.", "labels": [], "entities": []}, {"text": "Our models are trained on the training dataset, and evaluated by predicting the labels for the held-out test set.", "labels": [], "entities": []}, {"text": "As the label distribution is highly imbalanced (see), we evaluate and we compare the performance of the different models using macro-averaged F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9577758312225342}]}, {"text": "We further report per-class Precision (P), Recall (R), and F1-score (F1), and weighted average.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 28, "end_pos": 41, "type": "METRIC", "confidence": 0.9503067582845688}, {"text": "Recall (R)", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9664825350046158}, {"text": "F1-score (F1)", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9027684181928635}]}, {"text": "Finally, we compare the performance of the models against simple majority and minority class baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The keywords from the full dataset (except  for the first three rows) and the percentage of offensive  tweets for each keyword.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of label combinations in OLID.", "labels": [], "entities": [{"text": "OLID", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6285606622695923}]}, {"text": " Table 4: Results for offensive language detection (Level A). We report Precision (P), Recall (R), and F1 for each  model/baseline on all classes (NOT, OFF), and weighted averages. Macro-F1 is also listed (best in bold).", "labels": [], "entities": [{"text": "offensive language detection", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6225307484467825}, {"text": "Precision (P)", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9672164171934128}, {"text": "Recall (R)", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9581334292888641}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9984058737754822}, {"text": "OFF", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.940010666847229}]}, {"text": " Table 5: Results for offensive language categorization (level B). We report Precision (P), Recall (R), and F1 for  each model/baseline on all classes (TIN, UNT), and weighted averages. Macro-F1 is also listed (best in bold).", "labels": [], "entities": [{"text": "offensive language categorization", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6562795837720236}, {"text": "Precision (P)", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9633862674236298}, {"text": "Recall (R)", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9516476690769196}, {"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9985902905464172}]}, {"text": " Table 6: Results for offense target identification (level C). We report Precision (P), Recall (R), and F1 for each  model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold).", "labels": [], "entities": [{"text": "offense target identification", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7648979226748148}, {"text": "Precision (P)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9646691381931305}, {"text": "Recall (R)", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9552683979272842}, {"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9966394901275635}]}]}