{"title": [{"text": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach", "labels": [], "entities": [{"text": "Weak Supervision", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8294681906700134}]}], "abstractContent": [{"text": "Negation scope detection is widely performed as a supervised learning task which relies upon negation labels at word level.", "labels": [], "entities": [{"text": "Negation scope detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.945339560508728}]}, {"text": "This suffers from two key drawbacks: (1) such granular annotations are costly and (2) highly subjective, since, due to the absence of explicit linguistic resolution rules, human annotators often disagree in the perceived negation scopes.", "labels": [], "entities": [{"text": "linguistic resolution", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.785085141658783}]}, {"text": "To the best of our knowledge, our work presents the first approach that eliminates the need for word-level negation labels, replacing it instead with document-level sentiment annotations.", "labels": [], "entities": [{"text": "word-level negation labels", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6894216239452362}]}, {"text": "For this, we present a novel strategy for learning fully interpretable negation rules via weak supervision: we apply reinforcement learning to find a policy that reconstructs negation rules from sentiment predictions at document level.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules.", "labels": [], "entities": []}, {"text": "Furthermore, an out-of-sample evaluation via sentiment analysis reveals consistent improvements (of up to 4.66 %) over both a sentiment analysis with (i) no negation handling and (ii) the use of word-level annotations from humans.", "labels": [], "entities": [{"text": "negation handling", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7215598523616791}]}, {"text": "Moreover , the inferred negation rules are fully in-terpretable.", "labels": [], "entities": []}], "introductionContent": [{"text": "Negations area frequently utilized linguistic tool for expressing disapproval or framing negative content with positive words.", "labels": [], "entities": []}, {"text": "Neglecting negations can lead to false attributions ( and, moreover, impair accuracy when analyzing natural language; e. g., in information retrieval and especially in sentiment analysis (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.99471116065979}, {"text": "information retrieval", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.7246165871620178}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.9487822949886322}]}, {"text": "Hence, even simple heuristics for identifying negation scopes can yield substantial improvements in such cases (.", "labels": [], "entities": [{"text": "negation scopes", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7949871718883514}]}, {"text": "Negation scope detection is sometimes implemented as unsupervised learning (e. g.,, while a better performance is commonly achieved via supervised learning (see our supplements fora detailed overview): the resulting models thus learn to identify negation scopes from word-level annotations (e. g.,.", "labels": [], "entities": [{"text": "Negation scope detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.932081937789917}]}, {"text": "We argue that this approach suffers from inherent drawbacks.", "labels": [], "entities": []}, {"text": "(1) Such granular annotations are costly and, especially at word level, a considerable number of them is needed.", "labels": [], "entities": []}, {"text": "(2) Negation scope detection is highly subjective (. Due to the absence of explicit linguistic rules for resolutions, existing corpora often come with annotation guidelines).", "labels": [], "entities": [{"text": "Negation scope detection", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.96769646803538}]}, {"text": "Yet there are considerable differences: some corpora were labeled in away that negation scopes consist of single text spans, while others allowed disjoint spans (.", "labels": [], "entities": []}, {"text": "More importantly, given the absence of universal rules, human annotators largely disagree in their perception of what words should be labeled as negated.", "labels": [], "entities": []}, {"text": "Since prevalent corpora were labeled only by a single rater, we now establish the severity of between-rater discrepancies.", "labels": [], "entities": [{"text": "severity", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9602866172790527}]}, {"text": "For this, we carried out an initial analysis of 500 sentences from movie reviews.", "labels": [], "entities": []}, {"text": "Each sentence contained at least one explicit negation phrase from the list of, such as \"not\" or \"no.\"", "labels": [], "entities": []}, {"text": "Two human raters were then asked to annotate negation scopes.", "labels": [], "entities": [{"text": "negation scopes", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.9383479356765747}]}, {"text": "They could choose an arbitrary selection of words and were not restricted to a single subspan, as recommended by.", "labels": [], "entities": []}, {"text": "The annotations resulted in large differences: only 50.20 % of the words were simultaneously labeled as \"negated\" by both raters.", "labels": [], "entities": []}, {"text": "Based on this experimental evidence, we showcase there is no universal definition of negation scopes (rather, human annotations are likely to be noisy or even error-prone) and thus highlight the need for further research.", "labels": [], "entities": [{"text": "negation scopes", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.9365309476852417}]}, {"text": "To the best of our knowledge, our work presents the first approach that eliminates the need for word-level annotations of negation labels.", "labels": [], "entities": []}, {"text": "Instead, we perform negation scope detection merely by utilizing shallow annotations at document level in the form of sentiment labels (e. g., from user reviews).", "labels": [], "entities": [{"text": "negation scope detection", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.9611501892407736}]}, {"text": "Our novel strategy learns interpretable negation rules via weak supervision: we apply reinforcement learning to find a policy that reconstructs negation rules based on sentiment prediction at document level (as opposed to conventional word-level annotations).", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.6973740011453629}]}, {"text": "In our approach, a single document d comes with a sentiment label yd . The document consists of Nd words, w d,1 , . .", "labels": [], "entities": []}, {"text": ", w d,N d , where the number of words can easily surpass several hundreds.", "labels": [], "entities": []}, {"text": "Based on the sentiment value, we then need to make a decision (especially out-of-sample) for each of the Nd words, whether or not it should be negated.", "labels": [], "entities": []}, {"text": "In this case, a single sentiment value is outnumbered by potentially hundreds of negation decisions, thus pinpointing to the difficulty of this task.", "labels": [], "entities": []}, {"text": "Formally, the goal is to learn individual labels a d,i \u2208 {Negated, \u00acNegated} for each word w d,i . Rewards are the errors in sentiment prediction at document level.", "labels": [], "entities": [{"text": "Rewards", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9143164157867432}, {"text": "sentiment prediction", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.9357209205627441}]}, {"text": "Our approach exhibits several favorable features that overcome shortcomings found in prior works.", "labels": [], "entities": []}, {"text": "Among them, it eliminates the need for manual word-level labels.", "labels": [], "entities": []}, {"text": "It thus avoids the detrimental influence of subjectivity and misinterpretation.", "labels": [], "entities": []}, {"text": "Instead, our model is solely trained on a document-level variable and can thus learn domain-specific particularities of the given prose.", "labels": [], "entities": []}, {"text": "The inferred negation rules are fully interpretable while documents can contain multiple instances of negations with arbitrary complexity, sometimes nested or consisting out of disjoint text spans.", "labels": [], "entities": []}, {"text": "Despite facing several times more negation decisions than sentiment labels, our experiments demonstrate that this problem can be effectively learned through reinforcement learning.", "labels": [], "entities": []}, {"text": "Given the considerable inconsistencies inhuman annotations of negation scopes and the lack of universal rules, we regard the \"true\" negation scopes as unobservable.", "labels": [], "entities": [{"text": "negation scopes", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.9153099954128265}]}, {"text": "Hence, we later compare the identified negation scopes with those from rater 1 and 2 only as a sensitivity check because of the fact that both raters have only 50.2 % overlap.", "labels": [], "entities": []}, {"text": "Instead, we choose the following evaluation strategy.", "labels": [], "entities": []}, {"text": "We concentrate on the performance of negation scope detection as a supporting tool in natural language processing where its primary role is to facilitate more complex learning tasks such as sentiment analysis.", "labels": [], "entities": [{"text": "negation scope detection", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.9552642107009888}, {"text": "sentiment analysis", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.9594487249851227}]}, {"text": "Therefore, we report the performance improvements in sentiment analysis resulting from our approach.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9555902481079102}]}, {"text": "For a fair comparison, we use baselines that only rely upon the same information as our weak supervision (and thus have no access to word-level negation labels).", "labels": [], "entities": []}, {"text": "Our performance is even on par with a supervised classifier that can exploit richer labels during training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the following benchmark datasets with document-level annotations from the literature (cf.: IMDb: movie reviews from the Internet Movie Database archive, each annotated with an overall rating at document level (Pang and).", "labels": [], "entities": [{"text": "Internet Movie Database archive", "start_pos": 127, "end_pos": 158, "type": "DATASET", "confidence": 0.8293120265007019}, {"text": "Pang", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.8306894898414612}]}, {"text": "Airport: user reviews of airports from Skytrax, each annotated with an overall rating at document level (.", "labels": [], "entities": [{"text": "Skytrax", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9500608444213867}]}, {"text": "Ad hoc: financial announcements with complex, domain-specific language, labeled with the daily abnormal return of the corresponding stock.", "labels": [], "entities": []}, {"text": "We perform 4000 learning iterations with a higher exploration rate as given by the following parameters 3 : exploration \u03b5 = 0.001, discount factor \u03b3 = 0 and learning rate \u03b1 = 0.005.", "labels": [], "entities": [{"text": "exploration \u03b5", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9561938643455505}, {"text": "discount factor \u03b3", "start_pos": 131, "end_pos": 148, "type": "METRIC", "confidence": 0.9673039714495341}, {"text": "learning rate \u03b1", "start_pos": 157, "end_pos": 172, "type": "METRIC", "confidence": 0.9145219922065735}]}, {"text": "Ina second phase, we run 1000 iterations for fine-tuning with exploration \u03b5 = 0.0001, discount factor \u03b3 = 0 and learning rate \u03b1 = 0.001.", "labels": [], "entities": [{"text": "discount factor \u03b3", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9756306807200114}, {"text": "learning rate \u03b1", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.9347121914227804}]}, {"text": "For each dataset, the reinforcement learning process converges to a stationary policy that shows reward fluctuations below 0.05 %.", "labels": [], "entities": []}, {"text": "As part of a benchmark, we study the mean squared error (MSE) between yd and the predicted sentiment S 0 d when leaving negations untreated as our benchmark.", "labels": [], "entities": [{"text": "mean squared error (MSE) between yd", "start_pos": 37, "end_pos": 72, "type": "METRIC", "confidence": 0.9491347521543503}]}, {"text": "For all datasets, the in-sample MSE decreases substantially (see), demonstrating the effectiveness of our learning approach.", "labels": [], "entities": []}, {"text": "The reductions number to 14.93 % (IMDb), 16.77 % (airport), and 0.91 % (ad hoc).", "labels": [], "entities": [{"text": "airport", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9780856370925903}]}, {"text": "The latter is a result of the considerably more complex language in financial statements.", "labels": [], "entities": []}, {"text": "We use 10-fold cross validation to compare the out-ofsample performance in sentiment analysis of reinforcement learning to benchmarks without wordlevel labels from previous works.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9292529821395874}]}, {"text": "The benchmarks consists of rules), which search for the occurrence of specific cues based on pre-defined lists and then invert the meaning of a fixed number of surrounding words.", "labels": [], "entities": []}, {"text": "compares the out-ofsample MSE between predicted sentiment and the", "labels": [], "entities": [{"text": "out-ofsample MSE", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.7265438139438629}]}], "tableCaptions": [{"text": " Table 1: Out-of-sample MSE between sentiment S \u03c0", "labels": [], "entities": [{"text": "MSE", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8209261894226074}]}, {"text": " Table 2: Excerpt of state-action function Q(s i , a i ) ac- tions A = {Negated, \u00acNegated} and the learned policy  \u03c0  *  for IMDb reviews.", "labels": [], "entities": []}]}