{"title": [{"text": "VCWE: Visual Character-Enhanced Word Embeddings", "labels": [], "entities": [{"text": "VCWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.865838885307312}]}], "abstractContent": [{"text": "Chinese is a logographic writing system, and the shape of Chinese characters contain rich syntactic and semantic information.", "labels": [], "entities": []}, {"text": "In this paper , we propose a model to learn Chinese word embeddings via three-level composition: (1) a convolutional neural network to extract the intra-character compositionality from the visual shape of a character; (2) a recurrent neural network with self-attention to compose character representation into word embeddings; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information.", "labels": [], "entities": []}, {"text": "Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7467581629753113}, {"text": "sentiment analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9368607699871063}, {"text": "named entity recognition", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.6473877628644308}, {"text": "part-of-speech tagging", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7117012590169907}]}], "introductionContent": [{"text": "Distributed representations of words, namely word embeddings, encode both semantic and syntactic information into a dense vector.", "labels": [], "entities": []}, {"text": "Currently, word embeddings have been playing a pivotal role in many natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 68, "end_pos": 107, "type": "TASK", "confidence": 0.7776537707873753}]}, {"text": "Most of these NLP tasks also benefit from the pre-trained word embeddings, such as word2vec () and GloVe), which are based on the distributional hypothesis: words that occur in the same contexts tend to have similar meanings.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.8078987002372742}]}, {"text": "Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words.", "labels": [], "entities": []}, {"text": "To improve the performance of word embeddings, sub-word information has been employed (.", "labels": [], "entities": []}, {"text": "Compositionality is more critical for Chinese, since Chinese is a logographic writing system.", "labels": [], "entities": []}, {"text": "In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information.", "labels": [], "entities": []}, {"text": "For example, Chinese character \"\u4f11\" (rest) is composed of the characters for \"\u4eba\" (person) and \"\u6728\" (tree), with the intended idea of someone leaning against a tree, i.e., resting.", "labels": [], "entities": []}, {"text": "Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings.", "labels": [], "entities": []}, {"text": "These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (, radical 2 ( or strokes.", "labels": [], "entities": []}, {"text": "However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality.", "labels": [], "entities": []}, {"text": "2) The other kind of methods learns word embeddings with the visual information of the character.", "labels": [], "entities": []}, {"text": "learn character embedding based on its visual characteristics in the text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8261987566947937}]}, {"text": "also introduce a pixel-based model that learns character features from font images.", "labels": [], "entities": []}, {"text": "However, their model is not shown to be better than word2vec model because it has little flexibility and fixed character features.", "labels": [], "entities": []}, {"text": "Besides, most of these methods pay less attention to the non-compositionality.", "labels": [], "entities": []}, {"text": "For example, the semantic of Chinese word \"\u6c99\u53d1\" (sofa) cannot be composed by its contained characters \"\u6c99\" (sand) and \"\u53d1\" (hair).", "labels": [], "entities": []}, {"text": "In this paper, we fully consider the compositionality and non-compositionality of Chinese words and propose a visual character-enhanced word embedding model (VCWE) to learn Chinese word embeddings.", "labels": [], "entities": []}, {"text": "VCWE learns Chinese word embeddings via three-level composition: \u2022 The first level is to learn the intra-character composition, which gains the representation of each character from its visual appearance via a convolutional neural network; \u2022 The second level is to learn the intercharacter composition, where a bidirectional long short-term neural network (Bi-LSTM)) with self-attention to compose character representation into word embeddings; \u2022 The third level is to learn the noncompositionality, we can learn the contextual information because the overall framework of our model is based on the skip-gram.", "labels": [], "entities": []}, {"text": "Evaluations demonstrate the superior performance of our model on four tasks such as word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7261641621589661}, {"text": "sentiment analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9519765377044678}, {"text": "named entity recognition", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.6298864781856537}, {"text": "part-of-speech tagging", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.7295605838298798}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman correlation for word similarity datasets, \"-CNN\" represents replacing the CNN and image  information with randomly initialized character embedding, \"-LSTM\" represents replacing Bi-LSTM network and  self-attention with the averaging operation. For each dataset, we boldface the score with the best performance  across all models.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy for Sentiment analysis task. The configurations are the same of the ones used in", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955489635467529}, {"text": "Sentiment analysis", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9546636939048767}]}, {"text": " Table 3: Chinese NER and POS tagging results for dif- ferent pretrained embeddings. The configurations are  the same of the ones used in Table 1.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8362382054328918}, {"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6472627222537994}]}]}