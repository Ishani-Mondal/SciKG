{"title": [{"text": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "labels": [], "entities": [{"text": "Discovering Diverse Perspectives about Claims", "start_pos": 38, "end_pos": 83, "type": "TASK", "confidence": 0.8207411050796509}]}], "abstractContent": [{"text": "One key consequence of the information revolution is a significant increase and a contamination of our information supply.", "labels": [], "entities": []}, {"text": "The practice of fact-checking won't suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us.", "labels": [], "entities": []}, {"text": "To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives.", "labels": [], "entities": []}, {"text": "For example, there are many ways to respond to a claim such as \"animals should have lawful rights\", and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it.", "labels": [], "entities": []}, {"text": "Inherently, this is a natural language understanding task, and we propose to address it as such.", "labels": [], "entities": [{"text": "natural language understanding task", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7478686049580574}]}, {"text": "Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim.", "labels": [], "entities": [{"text": "substantiated perspective discovery", "start_pos": 37, "end_pos": 72, "type": "TASK", "confidence": 0.6288810869057974}]}, {"text": "Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts.", "labels": [], "entities": []}, {"text": "We construct PERSPECTRUM, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to filter out noise and ensure high-quality data.", "labels": [], "entities": []}, {"text": "Our dataset contains 1k claims, accompanied by pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.", "labels": [], "entities": []}, {"text": "We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform machine baselines built upon state-of-the-art NLP techniques.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7297086715698242}]}, {"text": "This poses a challenge and an opportunity for the NLP community to address.", "labels": [], "entities": []}, {"text": "Figure 1: Given a claim, a hypothetical system is expected to discover various perspectives that are substantiated with evidence and their stance with respect to the claim.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding most nontrivial claims requires insights from various perspectives.", "labels": [], "entities": []}, {"text": "Today, we make use of search engines or recommendation systems to retrieve information relevant to a claim, but this process carries multiple forms of bias.", "labels": [], "entities": []}, {"text": "In particular, they are optimized relative to the claim (query) presented, and the popularity of the relevant documents returned, rather than with respect to the diversity of the perspectives presented in them or whether they are supported by evidence.", "labels": [], "entities": []}, {"text": "In this paper, we explore an approach to mitigating this selection bias) when studying (disputed) claims.", "labels": [], "entities": []}, {"text": "Consider the claim shown in: \"animals should have lawful rights.\"", "labels": [], "entities": []}, {"text": "One might compare the biological similarities/differences between humans and other an-imals to support/oppose the claim.", "labels": [], "entities": []}, {"text": "Alternatively, one can base an argument on morality and rationality of animals, or lack thereof.", "labels": [], "entities": []}, {"text": "Each of these arguments, which we refer to as perspectives throughout the paper, is an opinion, possibly conditional, in support of a given claim or against it.", "labels": [], "entities": []}, {"text": "A perspective thus constitutes a particular attitude towards a given claim.", "labels": [], "entities": []}, {"text": "Natural language understanding is at the heart of developing an ability to identify diverse perspectives for claims.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6637239853541056}]}, {"text": "In this work, we propose and study a setting that would facilitate discovering diverse perspectives and their supporting evidence with respect to a given claim.", "labels": [], "entities": []}, {"text": "Our goal is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges.", "labels": [], "entities": []}, {"text": "For example, for the claim in, multiple (nonredundant) perspectives should be retrieved from a pool of perspectives; one of them is \"animals have no interest or rationality\", a perspective that should be identified as taking an opposing stance with respect to the claim.", "labels": [], "entities": []}, {"text": "Each perspective should also be well-supported by evidence found in a pool of potential pieces of evidence.", "labels": [], "entities": []}, {"text": "While it might be impractical to provide an exhaustive spectrum of ideas with respect to a claim, presenting a small but diverse set of perspectives could bean important step towards addressing the selection bias problem.", "labels": [], "entities": []}, {"text": "Moreover, it would be impractical to develop an exhaustive pool of evidence for all perspectives, from a diverse set of credible sources.", "labels": [], "entities": []}, {"text": "We are not attempting to do that.", "labels": [], "entities": []}, {"text": "We aim at formulating the core NLP problems, and developing a dataset that will facilitate studying these problems from the NLP angle, realizing that using the outcomes of this research in practice requires addressing issues such as trustworthiness) and possibly others.", "labels": [], "entities": []}, {"text": "Inherently, our objective requires understanding the relations between perspectives and claims, the nuances in the meaning of various perspectives in the context of claims, and relations between perspectives and evidence.", "labels": [], "entities": []}, {"text": "This, we argue, can be done with a diverse enough, but not exhaustive, dataset.", "labels": [], "entities": []}, {"text": "And it can be done without attending to the legitimacy and credibility of sources contributing evidence, an important problem but orthogonal to the one studied here.", "labels": [], "entities": []}, {"text": "To facilitate the research towards developing solutions to such challenging issues, we propose PERSPECTRUM, a dataset of claims, perspectives and evidence paragraphs.", "labels": [], "entities": [{"text": "PERSPECTRUM", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9826108813285828}]}, {"text": "For a given claim and pools of perspectives and evidence paragraphs, a hypothetical system is expected to select the relevant perspectives and their supporting paragraphs.", "labels": [], "entities": []}, {"text": "Our dataset contains 907 claims, 11,164 perspectives and 8,092 evidence paragraphs.", "labels": [], "entities": []}, {"text": "In constructing it, we use online debate websites as our initial seed data, and augment it with search data and paraphrases to make it richer and more challenging.", "labels": [], "entities": []}, {"text": "We make extensive use of crowdsourcing to increase the quality of the data and clean it from annotation noise.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: \u2022 To facilitate making progress towards the problem of substantiated perspective discovery, we create a high-quality dataset for this task.", "labels": [], "entities": [{"text": "substantiated perspective discovery", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.6059984167416891}]}, {"text": "1 \u2022 We identify and formulate multiple NLP tasks that are at the core of addressing the substantiated perspective discovery problem.", "labels": [], "entities": [{"text": "perspective discovery problem", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.7681285242239634}]}, {"text": "We show that humans can achieve high scores on these tasks.", "labels": [], "entities": []}, {"text": "\u2022 We develop competitive baseline systems for each sub-task, using state-of-the-art techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe a multi-step process, constructed with detailed analysis, substantial refinements and multiple pilots studies.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to annotate different aspects of the dataset.", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate.", "labels": [], "entities": [{"text": "acceptance rate", "start_pos": 220, "end_pos": 235, "type": "METRIC", "confidence": 0.9805108606815338}]}, {"text": "To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators.", "labels": [], "entities": []}, {"text": "For any of the annotations steps described below, the users are guided to an external platform where they first read the instructions and try a verification step to make sure they have understood the instructions.", "labels": [], "entities": []}, {"text": "Only after successful completion are they allowed to start the annotation tasks.", "labels": [], "entities": []}, {"text": "Throughout our annotations, it is our aim to make sure that the workers are responding objectively to the tasks (as opposed to using their personal opinions or preferences).", "labels": [], "entities": []}, {"text": "The screen-shots of the annotation interfaces for each step are included in the Appendix (Section A.3).", "labels": [], "entities": []}, {"text": "In the steps outlined below, we filter out a subset of the data with low rater-rater agreement \u03c1 (see.", "labels": [], "entities": [{"text": "rater-rater agreement \u03c1", "start_pos": 73, "end_pos": 96, "type": "METRIC", "confidence": 0.849145789941152}]}, {"text": "In certain steps, we use an information retrieval (IR) system 2 to generate the best candidates for the task at hand.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.8108521819114685}]}, {"text": "Step 1: The initial data collection.", "labels": [], "entities": []}, {"text": "We start by crawling the content of a few notable debating websites: idebate.com, debatewise.org, procon.org.", "labels": [], "entities": []}, {"text": "This yields \u223c 1k claims, \u223c 8k perspectives and \u223c 8k evidence paragraphs (for complete statistics, see in the Appendix).", "labels": [], "entities": [{"text": "Appendix", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.8626639246940613}]}, {"text": "This data is significantly noisy and lacks the structure we would like.", "labels": [], "entities": []}, {"text": "In the following steps we explain how we denoise it and augment it with additional data.", "labels": [], "entities": []}, {"text": "Step 2a: Perspective verification.", "labels": [], "entities": [{"text": "Perspective verification", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.9643563330173492}]}, {"text": "For each perspective we verify that it is a complete English sentence, with a clear stance with respect to the given claim.", "labels": [], "entities": []}, {"text": "For a fixed pair of claim and perspective, we ask the crowd-workers to label the perspective with one of the five categories of support, oppose, mildly-support, mildly-oppose, or not a valid perspective.", "labels": [], "entities": []}, {"text": "The reason that we ask for two levels of intensity is to distinguish mild or conditional arguments from those that express stronger positions.", "labels": [], "entities": []}, {"text": "Every 10 claims (and their relevant perspectives) are bundled to form a HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8436093330383301}]}, {"text": "Three independent annotators solve a HIT, and each gets paid $1.5-2 per HIT.", "labels": [], "entities": []}, {"text": "To get rid of the ambiguous/noisy perspectives we measure rater-rater agreement on the resulting data and retain only the subset which has a significant agreement of \u03c1 \u2265 0.5.", "labels": [], "entities": []}, {"text": "To account for minor disagreements in the intensity of perspective stances, before measuring any notion of agreement, we collapse the five labels into three labels, by collapsing mildly-support and mildlyoppose into support and oppose, respectively.", "labels": [], "entities": []}, {"text": "To assess the quality of these annotations, two of the authors independently annotate a random subset of instances in the previous step (328 perspectives for 10 claims).", "labels": [], "entities": []}, {"text": "Afterwards, the differences were adjudicated.", "labels": [], "entities": []}, {"text": "We measure the accuracy adjudicated results with AMT annotations to estimate the quality of our annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9978585839271545}]}, {"text": "This results in an accuracy of 94%, which shows high-agreement with the crowdsourced annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997101426124573}]}, {"text": "Step 2b: Perspective paraphrases.", "labels": [], "entities": [{"text": "Perspective paraphrases", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.8910544812679291}]}, {"text": "To enrich the ways the perspectives are phrased, we crowdsource paraphrases of our perspectives.", "labels": [], "entities": []}, {"text": "We ask annotators to generate two paraphrases for each of the 15 perspectives in each HIT, fora reward of $1.50.", "labels": [], "entities": [{"text": "HIT", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8673390746116638}]}, {"text": "Subsequently, we perform another round of crowdsourcing to verify the generated paraphrases.", "labels": [], "entities": []}, {"text": "We create HITs of 24 candidate paraphrases to be verified, with a reward of $1.", "labels": [], "entities": []}, {"text": "Overall, this process gives us \u223c 4.5 paraphrased perspectives.", "labels": [], "entities": []}, {"text": "The collected paraphrases form clusters of equivalent perspectives, which we refine further in the later steps.", "labels": [], "entities": []}, {"text": "Step 2c: Web perspectives.", "labels": [], "entities": []}, {"text": "In order to ensure that our dataset contains more realistic sentences, we use web search to augment our pool of perspectives with additional sentences that are topically related to what we already have.", "labels": [], "entities": []}, {"text": "Specifically, we use Bing search to extract sentences that are similar to our current pool of perspectives, by querying \"claim+perspective\".", "labels": [], "entities": []}, {"text": "We create a pool of relevant web sentences and use an IR system (introduced earlier) to retrieve the 10 most similar sentences.", "labels": [], "entities": []}, {"text": "These candidate perspectives are annotated using (similar to step 2a) and only those that were agreed upon are retained.", "labels": [], "entities": []}, {"text": "Step 2d: Final perspective trimming.", "labels": [], "entities": [{"text": "perspective trimming", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.653214618563652}]}, {"text": "Ina final round of annotation for perspectives, an expert annotator went overall the claims in order to verify that all the equivalent perspectives are clustered together.", "labels": [], "entities": []}, {"text": "Subsequently, the expert annotator went over the most similar claim-pairs (and their perspectives), in order to annotate the missing perspectives shared between the two claims.", "labels": [], "entities": []}, {"text": "To cut the space of claim pairs, the annotation was done on the top 350 most similar claim pairs retrieved: A summary of PERSPECTRUM statistics by the IR system.", "labels": [], "entities": [{"text": "PERSPECTRUM", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.8934034109115601}, {"text": "IR system", "start_pos": 151, "end_pos": 160, "type": "DATASET", "confidence": 0.7914385497570038}]}, {"text": "Step 3: Evidence verification.", "labels": [], "entities": [{"text": "Evidence verification", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.790032833814621}]}, {"text": "The goal of this step is to decide whether a given evidence paragraph provides enough substantiations fora perspective or not.", "labels": [], "entities": []}, {"text": "Performing these annotations exhaustively for any perspective-evidence pair is not possible.", "labels": [], "entities": []}, {"text": "Instead, we make use of a retrieval system to annotate only the relevant pairs.", "labels": [], "entities": []}, {"text": "In particular, we create an index of all the perspectives retained from step 2a.", "labels": [], "entities": []}, {"text": "For a given evidence paragraph, we retrieve the top relevant perspectives.", "labels": [], "entities": []}, {"text": "We ask the annotators to note whether a given evidence paragraph supports a given perspective or not.", "labels": [], "entities": []}, {"text": "Each HIT contains a 20 evidence paragraphs and their top 8 relevant candidate perspectives.", "labels": [], "entities": []}, {"text": "Each HIT is paid $1 and annotated by at least 4 independent annotators.", "labels": [], "entities": []}, {"text": "In order to assess the quality of our annotations, a random subset of instances (4 evidenceperspective pairs) are annotated by two independent authors and the differences are adjudicated.", "labels": [], "entities": []}, {"text": "We measure the accuracy of our adjudicated labels versus AMT labels, resulting in 87.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9993903636932373}]}, {"text": "This indicates the high quality of the crowdsourced data.", "labels": [], "entities": []}, {"text": "We now provide a brief summary of PERSPECTRUM.", "labels": [], "entities": [{"text": "PERSPECTRUM", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9449225068092346}]}, {"text": "The dataset contains about 1k claims with a significant length diversity.", "labels": [], "entities": []}, {"text": "Additionally, the dataset comes with \u223c 12k perspectives, most of which were generated through paraphrasing (step 2b).", "labels": [], "entities": []}, {"text": "The perspectives which convey the same point with respect to a claim are grouped into clusters.", "labels": [], "entities": []}, {"text": "On average, each cluster has a size of 2.3 which shows that, on average, many perspectives have equivalents.", "labels": [], "entities": []}, {"text": "More granular details are available in.", "labels": [], "entities": []}, {"text": "To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \"topics\" associated with each claim (e.g., Law, Ethics, etc.)", "labels": [], "entities": []}, {"text": "We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (.", "labels": [], "entities": []}, {"text": "Additionally, the included claims touch upon 10+ different topics.", "labels": [], "entities": []}, {"text": "depicts a few popular categories and sampled questions from each.", "labels": [], "entities": []}, {"text": "We perform evaluations on four different subtasks in our dataset.", "labels": [], "entities": []}, {"text": "In all of the following evaluations, the systems are given the two pools of perspectives U p and evidences U e . T1: Perspective extraction.", "labels": [], "entities": [{"text": "Perspective extraction", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7244502753019333}]}, {"text": "A system is expected to return the collection of mutually disjoint perspectives with respect to a given claim.", "labels": [], "entities": []}, {"text": "Let\u02c6PLet\u02c6 Let\u02c6P (c) be the set of output perspectives.", "labels": [], "entities": []}, {"text": "Define the precision and recall as Pre(c) =   T2: Perspective stance classification.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999592125415802}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9994286894798279}, {"text": "Pre", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9920591115951538}, {"text": "T2", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9163448810577393}]}, {"text": "Given a claim, a system is expected to label every perspective in P (c) with one of two labels support or oppose.", "labels": [], "entities": []}, {"text": "We use the well-established definitions of precision-recall for this binary classification task.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9976919889450073}]}, {"text": "A system is expected to decide whether two given perspectives are equivalent or not, with respect to a given claim.", "labels": [], "entities": []}, {"text": "We evaluate this task in away similar to a clustering problem.", "labels": [], "entities": []}, {"text": "For a pair of perspectives p 1 , p 2 \u2208 P (c), a system predicts whether the two are in the same cluster or not.", "labels": [], "entities": []}, {"text": "The ground-truth is whether there is a cluster which contains both of the perspectives or not: \u2203\u02dcp\u2203\u02dcp s.t.", "labels": [], "entities": []}, {"text": "We use this pairwise definition for all the pairs in P (c) \u00d7 P (c), for any claim c in the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A summary of PERSPECTRUM statistics", "labels": [], "entities": [{"text": "PERSPECTRUM", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.6503011584281921}]}, {"text": " Table 3: Quality of different baselines on different sub- tasks (Section 5). All the numbers are in percentage.  Top machine baselines are in bold.", "labels": [], "entities": []}]}