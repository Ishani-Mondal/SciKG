{"title": [{"text": "SC-UPB at the VarDial 2019 Evaluation Campaign: Moldavian vs. Romanian Cross-Dialect Topic Identification", "labels": [], "entities": [{"text": "Moldavian vs. Romanian Cross-Dialect Topic Identification", "start_pos": 48, "end_pos": 105, "type": "TASK", "confidence": 0.4656309833129247}]}], "abstractContent": [{"text": "This paper describes our models for the Mol-davian vs. Romanian Cross-Topic Identification (MRC) evaluation campaign, part of the VarDial 2019 workshop.", "labels": [], "entities": [{"text": "Mol-davian vs. Romanian Cross-Topic Identification (MRC) evaluation", "start_pos": 40, "end_pos": 107, "type": "TASK", "confidence": 0.7414089706208971}, {"text": "VarDial 2019 workshop", "start_pos": 130, "end_pos": 151, "type": "DATASET", "confidence": 0.7520588636398315}]}, {"text": "We focus on the three subtasks for MRC: binary classification between the Moldavian (MD) and the Roma-nian (RO) dialects and two cross-dialect multi-class classification between six news topics, MD to RO and RO to MD.", "labels": [], "entities": [{"text": "MRC", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9852896332740784}]}, {"text": "We propose several deep learning models based on long short-term memory cells, Bidirectional Gated Recurrent Unit (BiGRU) and Hierarchical Attention Networks (HAN).", "labels": [], "entities": []}, {"text": "We also employ three word embedding models to represent the text as a low dimensional vector.", "labels": [], "entities": []}, {"text": "Our official submission includes two runs of the BiGRU and HAN models for each of the three subtasks.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8362217545509338}, {"text": "HAN", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8574471473693848}]}, {"text": "The best submitted model obtained the following macro-averaged F 1 scores: 0.708 for subtask 1, 0.481 for subtask 2 and 0.480 for the last one.", "labels": [], "entities": [{"text": "F 1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.915530800819397}]}, {"text": "Due to a read error caused by the quoting behaviour over the test file, our final submissions contained a smaller number of items than expected.", "labels": [], "entities": []}, {"text": "More than 50% of the submission files were corrupted.", "labels": [], "entities": []}, {"text": "Thus, we also present the results obtained with the corrected labels for which the HAN model achieves the following results: 0.930 for subtask 1, 0.590 for subtask 2 and 0.687 for the third one.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of discriminating between two dialects or different languages is a popular research topic which has attracted a lot of interest from the research community.", "labels": [], "entities": []}, {"text": "Specifically, the VarDial competition proposed in recent years a number of shared tasks on different languages such as dialect identification for Arabic or German, Indo-Aryan language identification, distinguish between Mainland and Taiwan Mandarin or discriminating between.", "labels": [], "entities": [{"text": "Indo-Aryan language identification", "start_pos": 164, "end_pos": 198, "type": "TASK", "confidence": 0.6333306729793549}]}, {"text": "This year (, the problem of discriminating between Romanian and Moldavian dialects was introduced as a series of three subtasks.", "labels": [], "entities": []}, {"text": "It involves the processing of the MOROCO dataset (  to construct several language classification models.", "labels": [], "entities": [{"text": "MOROCO dataset", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8242668807506561}]}, {"text": "The dataset contains text samples from online news outlets in the Romanian (RO) language or the Moldavian (MD) dialect.", "labels": [], "entities": []}, {"text": "All the subtasks are closed, meaning that the use of external datasets is not allowed.", "labels": [], "entities": []}, {"text": "Additionally, internal data, available for the MRC subtasks, must not be used between tasks.", "labels": [], "entities": [{"text": "MRC subtasks", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.613184243440628}]}, {"text": "Thus, the first subtask is a binary classification between the two dialects.", "labels": [], "entities": []}, {"text": "The second subtask involves a cross-dialect multi-class classification between six topics.", "labels": [], "entities": []}, {"text": "More precisely, the classifier is trained using Moldavian dialect in order to classify samples from the Romanian dialect.", "labels": [], "entities": []}, {"text": "The third subtask is similar to the second one, here the use of the dialects is reversed.", "labels": [], "entities": []}, {"text": "Generally, such tasks are approached using traditional machine learning algorithms, which unfortunately require handcrafted features.", "labels": [], "entities": []}, {"text": "Recently, deep learning methods, where features are learned from the data, have been proposed.", "labels": [], "entities": []}, {"text": "To address the MRC shared task, we propose the use of three state of the art deep learning architectures for text classification: Long Short-Term Memory cells (LSTM), Bidirectional Gated Recurrent Unit (Bi-GRU) (Graves and) and Hierarchical Attention Networks (HAN) (.", "labels": [], "entities": [{"text": "MRC shared task", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.9059524536132812}, {"text": "text classification", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.821561336517334}]}, {"text": "The submission results are based only on BiGRU and HAN models for each of the three subtasks.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.8097200989723206}, {"text": "HAN", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8407799601554871}]}, {"text": "After the competition deadline an error, caused by the quoting behaviour over the test file, was discovered.", "labels": [], "entities": []}, {"text": "As a result our final submissions contained a smaller number of labels than expected, with approximately 50% of the files being corrupted.", "labels": [], "entities": []}, {"text": "Thus, we present both the official: Dataset sample distribution between training, validation and test for each of the three subtasks.", "labels": [], "entities": []}, {"text": "submissions as well as later work, that includes the correction of this problem.", "labels": [], "entities": []}, {"text": "The study of Romanian dialects was first approached by.", "labels": [], "entities": []}, {"text": "They construct binary classifiers to distinguish between Romanian and three dialects (Macedo-Romanian, Megleno-Romanian and Istro-Romanian) by exploring information provided by a set of 108 word pairs.", "labels": [], "entities": []}, {"text": "Consequently,  proposed a first Moldavian and Romanian Dialectal Corpus (MOROCO) assembled from multiple news websites.", "labels": [], "entities": [{"text": "Moldavian and Romanian Dialectal Corpus (MOROCO)", "start_pos": 32, "end_pos": 80, "type": "DATASET", "confidence": 0.610140286386013}]}, {"text": "On top of this dataset they construct several deep learning models for dialect identification: Character level Convolutional Neural Network (CharCNN) and an improvement CNN model using squeeze and excitation blocks (.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.799124538898468}]}, {"text": "Additionally, they also investigate shallow string kernel methods (.", "labels": [], "entities": []}, {"text": "They conclude that string kernels achieve best performance among the studied methods.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly discuss the dataset for the three tasks.", "labels": [], "entities": []}, {"text": "Section 3 describes the methodology behind our solution, while the experimental setup and the results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 contains details regarding our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The MOROCO dataset contains Moldavian and Romanian samples collected from one of the following news categories: culture, finance, politics, science, sports and technology.", "labels": [], "entities": [{"text": "MOROCO dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7566808760166168}]}, {"text": "It is divided between training, validation and test for each of the three tasks as described in.", "labels": [], "entities": [{"text": "validation", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.9491665959358215}]}, {"text": "The test set is combined for all the subtasks such that the labels for the first task cannot be inferred.", "labels": [], "entities": []}, {"text": "This is necessary because the second and the third subtasks are based entirely on just one of the dialects.", "labels": [], "entities": []}, {"text": "The data samples are provided preprocessed by replacing named entities, which could act as biases for the classifiers, with a special identifier: $NE$.", "labels": [], "entities": []}, {"text": "Besides the default processing, we also took extra steps to cleanup the dataset.", "labels": [], "entities": []}, {"text": "Text usually contains expressions which carry little to no meaning, thus, we choose to remove the following: stop words, special characters and punctuation marks all except end of sentence.", "labels": [], "entities": []}, {"text": "Additionally, we remove the named entity identifier as they interfere with the text representations.", "labels": [], "entities": []}, {"text": "Another important aspect is given by how we deal with the diacritics.", "labels": [], "entities": []}, {"text": "During our experiments we analyze their impact on the performance.", "labels": [], "entities": []}, {"text": "We aim to provide classification models for all the subtasks from the challenge.", "labels": [], "entities": []}, {"text": "The solutions are based on word embeddings which are used as a preprocessing step to create inputs for the classifiers.", "labels": [], "entities": []}, {"text": "In order to achieve this, we rely on a number of pretrained word vector models: Romanian Language Corpus (CoRoLa) introduced by, Nordic Language Processing Laboratory (NLPL) word embedding repository ( and Common Crawl (CC) word vectors ().", "labels": [], "entities": [{"text": "Romanian Language Corpus (CoRoLa)", "start_pos": 80, "end_pos": 113, "type": "DATASET", "confidence": 0.8923582832018534}, {"text": "Nordic Language Processing Laboratory (NLPL) word embedding repository", "start_pos": 129, "end_pos": 199, "type": "DATASET", "confidence": 0.7353811979293823}]}, {"text": "The relevant details for each word vector representation model can be viewed in.", "labels": [], "entities": []}, {"text": "The input for the RNN flavour models is computed by taking the mean of all word embeddings present in the text.", "labels": [], "entities": []}, {"text": "Missing words are considered zero valued vectors.", "labels": [], "entities": []}, {"text": "The result is a representation of the whole news item as a single embedding vector.", "labels": [], "entities": []}, {"text": "The LSTM architecture consists of a starting LSTM layer of size 256.", "labels": [], "entities": []}, {"text": "This is followed by a secondary LSTM layer of 512 neurons.", "labels": [], "entities": []}, {"text": "Next, we use dropout as a regularization technique for reducing overfitting in neural networks ().", "labels": [], "entities": []}, {"text": "The method refers to dropping out individual units during training with a probability p = 0.3.", "labels": [], "entities": []}, {"text": "We use a fully connected layer consisting of 512 neurons between the recurrent layers and the output one.", "labels": [], "entities": []}, {"text": "All LSTM layers use the tanh activation function while the fully connected one uses Rectified Linear Unit (ReLU), both empirically chosen.", "labels": [], "entities": [{"text": "Rectified Linear Unit (ReLU)", "start_pos": 84, "end_pos": 112, "type": "METRIC", "confidence": 0.9184393286705017}]}, {"text": "Finally, the output consists of a softmax activation layer of variable size depending on the subtask, 2 dimensions for the first and 6 for the second and third.", "labels": [], "entities": []}, {"text": "The BiGRU model is similar, it uses an initial GRU layer of 256 size followed by a bidirectional GRU layer of size 512.", "labels": [], "entities": []}, {"text": "For both layers we apply batch normalization to accelerate the training.", "labels": [], "entities": []}, {"text": "Similarly, we use an empirically chosen tanh activation function.", "labels": [], "entities": []}, {"text": "This connects to two fully connected layers of 1024 and 512 neurons, both with dropout mechanism with p = 0.3.", "labels": [], "entities": []}, {"text": "The output layer is the same as for the LSTM architecture.", "labels": [], "entities": []}, {"text": "Due to the two-level hierarchical attention architecture, the HAN model learns the importance of the words as a weighted sum be-  tween the word embeddings.", "labels": [], "entities": []}, {"text": "Thus, it can create its own sentence and document models.", "labels": [], "entities": []}, {"text": "For a consistent input, not dependent on different document and sentence sizes, the model requires two hyper parameters: maximum sentence length (number of words) and maximum document length (number of sentences).", "labels": [], "entities": []}, {"text": "We choose these parameters by inspecting the statistics of the whole dataset to create an initial estimate which was later improved via a grid search.", "labels": [], "entities": []}, {"text": "The best performance was achieved with a maximum sentence length of 150 words and a maximum document size of 20 sentences.", "labels": [], "entities": []}, {"text": "Besides these parameters, we also used a grid search in order to choose a size of 200 neurons for the attention layer as well as for the BiGRU.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.6943181753158569}]}, {"text": "Similarly to the previous architectures, the output consists of 2 or 6 neurons depending on the subtask.", "labels": [], "entities": []}, {"text": "We train the model using the Adam optimizer () with the default hyper parameters.", "labels": [], "entities": []}, {"text": "For the learning rate, we use \u03b1 = 0.0005 which was chosen using a grid search as well.", "labels": [], "entities": []}, {"text": "We work with the training-validation split recommended by the organizers.", "labels": [], "entities": []}, {"text": "mean of precision and recall, F 1 score, in.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9933567643165588}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995009899139404}, {"text": "F 1 score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.984199325243632}]}, {"text": "Overall the HAN model outperforms the others for the first subtask and BiGRU with NLPL embeddings offers the best results for the second and third subtasks.", "labels": [], "entities": [{"text": "HAN", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.6284561157226562}, {"text": "BiGRU", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9058079123497009}]}, {"text": "For the official results, the best model, BiGRU with CC embeddings, obtained macro-averaged F 1 scores as follows: 0.708 for subtask 1, 0.481 for subtask 2 and 0.480 for the third one.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.5061069130897522}, {"text": "F 1", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8988035023212433}]}, {"text": "After the correction, the HAN with CC embedding model achieved 0.930 for subtask 1 while BiGRU with NLPL obtained 0.701 for subtask 2 and 0.803 for subtask 3.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.6237625479698181}]}, {"text": "Additionally, unlike the CC embeddings, for subtasks 2 and 3 the model that obtained the best results uses embeddings without diacritics.", "labels": [], "entities": []}, {"text": "To better visualize and understand the misclassification behaviour we present the confusion matrices for the three subtasks in.", "labels": [], "entities": []}, {"text": "The matrices are created using the models which achieved the best results over the test dataset.", "labels": [], "entities": []}, {"text": "For the first subtask the error is consistent across both classes, RO and MD.", "labels": [], "entities": [{"text": "RO", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9601778388023376}, {"text": "MD", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9020898342132568}]}, {"text": "Next, for the second subtask we observe high misclassification between the following classes: finance (FIN) -politics (POL), technology (TEC) -FIN and TEC -science (SCI).", "labels": [], "entities": [{"text": "FIN", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.7544558048248291}]}, {"text": "For the last subtask we notice that the misclassification errors from subtask 2 hold, as well as the addition of a high error between the TEC -POL classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset sample distribution between training,  validation and test for each of the three subtasks.", "labels": [], "entities": []}, {"text": " Table 2: Word embeddings: statistics regarding training methods and dataset/parameters details.", "labels": [], "entities": []}, {"text": " Table 3: Results obtained for: subtask 1 (top), subtask 2 (middle) and subtask 3 (bottom). The best results are  presented in bold. The first lines represent the official results, BiGRU represents the first run and HAN the second  one. All results are grouped by model type as well as the embeddings used. We include both the results for training  and evaluation datasets. Since the HAN model is computationally complex, we included only the embedding which  provided the best results with the previous architectures, namely, FastText Common Crawl (CC) word vectors.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 181, "end_pos": 186, "type": "METRIC", "confidence": 0.9924216866493225}, {"text": "HAN", "start_pos": 216, "end_pos": 219, "type": "METRIC", "confidence": 0.7238912582397461}]}, {"text": " Table 4: Confusion matrices for: subtask 1 (top), sub- task 2 (middle) and subtask 3 (bottom) constructed us- ing the models which obtained the best results over the  test dataset. Subtask 1 represents the classification be- tween the Moldavian (MD) and the Romanian (RO) di- alects. Subtask 2 and 3 are cross-dialect multi-class  classification between: culture (CUL), finance (FIN),  politics (POL), science (SCI), sports (SPO) and tech- nology (TEC).", "labels": [], "entities": []}, {"text": " Table 4. The matri- ces are created using the models which achieved  the best results over the test dataset. For the first  subtask the error is consistent across both classes,  RO and MD. Next, for the second subtask we ob- serve high misclassification between the following  classes: finance (FIN) -politics (POL), technol- ogy (TEC) -FIN and TEC -science (SCI). For  the last subtask we notice that the misclassification", "labels": [], "entities": [{"text": "RO", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9867497086524963}, {"text": "FIN", "start_pos": 338, "end_pos": 341, "type": "METRIC", "confidence": 0.5063695311546326}]}]}