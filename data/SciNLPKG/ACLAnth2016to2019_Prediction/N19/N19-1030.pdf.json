{"title": [{"text": "Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering", "labels": [], "entities": [{"text": "Learning to Attend On Essential Terms", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7842764357725779}, {"text": "Open-domain Question Answering", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6056134700775146}]}], "abstractContent": [{"text": "Open-domain question answering remains a challenging task as it requires models that are capable of understanding questions and answers , collecting useful information, and reasoning over evidence.", "labels": [], "entities": [{"text": "Open-domain question answering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6885217626889547}]}, {"text": "Previous work typically formulates this task as a reading comprehension or entailment problem given evidence retrieved from search engines.", "labels": [], "entities": []}, {"text": "However , existing techniques struggle to retrieve indirectly related evidence when no directly related evidence is provided, especially for complex questions where it is hard to parse precisely what the question asks.", "labels": [], "entities": []}, {"text": "In this paper we propose a retriever-reader model that learns to attend on essential terms during the question answering process.", "labels": [], "entities": [{"text": "question answering process", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.8595113952954611}]}, {"text": "We build (1) an essential term selector which first identifies the most important words in a question, then refor-mulates the query and searches for related evidence ; and (2) an enhanced reader that distinguishes between essential terms and distracting words to predict the answer.", "labels": [], "entities": []}, {"text": "We evaluate our model on multiple open-domain multiple-choice QA datasets, notably performing at the level of the state-of-the-art on the AI2 Reasoning Challenge (ARC) dataset.", "labels": [], "entities": [{"text": "AI2 Reasoning Challenge (ARC) dataset", "start_pos": 138, "end_pos": 175, "type": "DATASET", "confidence": 0.5343976233686719}]}], "introductionContent": [{"text": "Open-domain question answering (QA) has been extensively studied in recent years.", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8163878520329794}]}, {"text": "Many existing works have followed the 'search-and-answer' strategy and achieved strong performance () spanning multiple QA datasets such as TriviaQA ( However, open-domain QA tasks become inherently more difficult when (1) dealing with questions with little available evidence; (2) solving * Most of the work was done during internship at Microsoft, questions where the answer type is free-form text (e.g. multiple-choice) rather than a span among existing passages (i.e., 'answer span'); or when (3) the need arises to understand long and complex questions and reason over multiple passages, rather than simple text matching.", "labels": [], "entities": []}, {"text": "As a result, it is essential to incorporate commonsense knowledge or to improve retrieval capability to better capture partially related evidence.", "labels": [], "entities": []}, {"text": "As shown in, the TriviaQA, SQuAD, and MS-Macro datasets all provide passages within which the correct answer is guaranteed to exist.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8785507082939148}, {"text": "MS-Macro datasets", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9627323746681213}]}, {"text": "However, this assumption ignores the difficulty of retrieving question-related evidence from a large volume of open-domain resources, especially when considering complex questions which require reasoning or commonsense knowledge.", "labels": [], "entities": []}, {"text": "On the other hand, ARC does not provide passages known to contain the correct answer.", "labels": [], "entities": [{"text": "ARC", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.679029107093811}]}, {"text": "Instead, the task of identifying relevant passages is left to the solver.", "labels": [], "entities": []}, {"text": "However, questions in ARC have multiple answer choices that provide indirect information that can help solve the question.", "labels": [], "entities": []}, {"text": "As such an effective model needs to account for relations among passages, questions, and answer choices.", "labels": [], "entities": []}, {"text": "Real-world datasets such as Amazon-QA (a corpus of user queries from Amazon)) also exhibit the same challenge, i.e., the need to surface related evidence from which to extractor summarize an answer.", "labels": [], "entities": []}, {"text": "shows an example of a question in the ARC dataset and demonstrates the difficulties in retrieval and reading comprehension.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.926021933555603}]}, {"text": "As shown for Choice 1 (C1), a simple concatenation of the For SQuAD and TriviaQA, since the questions are paired with span-type answers, it is convenient to obtain ranking supervision where retrieved passages are relevant via distant supervision; however free-form questions in ARC and Amazon-QA result in alack of supervision which makes the problem more difficult.", "labels": [], "entities": [{"text": "ARC", "start_pos": 278, "end_pos": 281, "type": "DATASET", "confidence": 0.9173128604888916}]}, {"text": "For MS-Macro, the dataset is designed to annotate relevant passages though it has free-form answers.", "labels": [], "entities": []}], "datasetContent": [{"text": "Example questions  In this section, we first discuss the performance of the essential term selector, ET-Net, on a public dataset.", "labels": [], "entities": []}, {"text": "We then discuss the performance of the whole retriever-reader pipeline, ET-RR, on multiple open-domain datasets.For both the ET-Net and ET-RR models, we use 96-dimensional hidden states and 1-layer BiLSTMs in the sequence modeling layer.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.4 is applied for the embedding layer and the BiLSTMs' output layer.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9332141280174255}]}, {"text": "We use adamax) with a learning rate of 0.02 and batch size of 32.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.930717408657074}]}, {"text": "The model is trained for 100 epochs.", "labels": [], "entities": []}, {"text": "Our code is released at https://github.com/ nijianmo/arc-etrr-code.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of different selectors.", "labels": [], "entities": []}, {"text": " Table 5:  Statistics on ARC, RACE-Open,  MCScript-Open and Amazon-QA. Corpus size is  the number of sentences.", "labels": [], "entities": [{"text": "ARC", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7695618271827698}]}, {"text": " Table 7: Accuracy on multiple-choice selection on ARC,  RACE-Open and MCScript-Open.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9721946716308594}, {"text": "ARC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8939211368560791}]}, {"text": " Table 9: Accuracy on multiple-choice selection on  three product categoris of Amazon-QA.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9898174405097961}, {"text": "Amazon-QA", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.957173228263855}]}, {"text": " Table 10: Experimental results for reader on RACE.", "labels": [], "entities": [{"text": "RACE", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.9201855063438416}]}, {"text": " Table 10. As shown, the re- cently proposed Reading Strategies and OpenAI  GPT models, that finetune generative pre-trained  models achieve the highest scores. Among non- pre-trained models, our reader outperforms other", "labels": [], "entities": [{"text": "OpenAI  GPT", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8307049870491028}]}, {"text": " Table 12: Comparison of query formulation methods  and amounts of retrieved evidence (i.e., top K) on the  ARC dataset, in terms of percentage accuracy.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.6835234761238098}, {"text": "ARC dataset", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.9666922092437744}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9512627720832825}]}]}