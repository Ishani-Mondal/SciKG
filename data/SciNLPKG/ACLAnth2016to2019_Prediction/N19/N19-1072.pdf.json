{"title": [{"text": "Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation", "labels": [], "entities": [{"text": "Manual Summary Evaluation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6817058026790619}]}], "abstractContent": [{"text": "Conducting a manual evaluation is considered an essential part of summary evaluation methodology.", "labels": [], "entities": []}, {"text": "Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores.", "labels": [], "entities": []}, {"text": "Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as Responsiveness and pairwise comparison, attainable via crowdsourcing.", "labels": [], "entities": []}, {"text": "We revisit the Pyramid approach , proposing a lightweight sampling-based version that is crowdsourcable.", "labels": [], "entities": []}, {"text": "We analyze the performance of our method in comparison to original expert-based Pyramid evaluations , showing higher correlation relative to the common Responsiveness method.", "labels": [], "entities": []}, {"text": "We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating content quality of summaries is an integral part of summarization research.", "labels": [], "entities": [{"text": "summaries", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.9372215270996094}, {"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9851378202438354}]}, {"text": "Measuring the performance of a summarization system can be done through either automatic or manual evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.97551029920578}]}, {"text": "An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing.", "labels": [], "entities": []}, {"text": "Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient.", "labels": [], "entities": [{"text": "reliability", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9727296829223633}]}, {"text": "This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment.", "labels": [], "entities": []}, {"text": "The Pyramid method) is a prominent manual evaluation methodology that is considered highly reliable for comparing summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9121245741844177}]}, {"text": "It relies on a small set of manually-crafted reference summaries, out of which all summary content units (SCUs) are manually extracted.", "labels": [], "entities": []}, {"text": "System summaries are then manually checked for coverage of each individual SCU, from which an overall system score is derived.", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9869678616523743}]}, {"text": "The Pyramid evaluation method's reliability comes at a cost.", "labels": [], "entities": []}, {"text": "It requires laborious manual work performed by annotators who must browse through non-trivial guidelines).", "labels": [], "entities": []}, {"text": "Due to these drawbacks, it was only used in a few DUC and TAC benchmarks.", "labels": [], "entities": [{"text": "DUC and TAC benchmarks", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.6931814178824425}]}, {"text": "Instead, summarization work in recent years has mostly employed simpler manual evaluation approaches, such as Responsiveness and pairwise comparison, which do not rely on reference summaries and can be attained via crowdsourcing.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9899260997772217}]}, {"text": "Yet, these methods are quite subjective, since evaluators need to provide only a single global judgment for the quality of a summary (or a pair of summaries).", "labels": [], "entities": []}, {"text": "Such judgments are far more subjective than the Pyramid score, which is derived from many, more objective, local decisions, each judging independently the presence of an individual SCU.", "labels": [], "entities": []}, {"text": "Indeed, it was shown that the above subjective crowdsourcing-based evaluation methods are not reliable enough to produce consistent scores across experiments.", "labels": [], "entities": []}, {"text": "We propose a simplified crowdsourcable and reproducible version of the Pyramid method, that suggests appealing advantages over prior crowdsourcable evaluation methods.", "labels": [], "entities": []}, {"text": "Like the original Pyramid, our method leverages the strong signal of the reference summaries and similarly bases its score on less subjective SCU judgments.", "labels": [], "entities": []}, {"text": "In contrast to the original Pyramid, we rely on statistical sampling rather than exhaustive SCU extraction and testing, lowering overall cost.", "labels": [], "entities": [{"text": "SCU extraction", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.7814750671386719}]}, {"text": "Empirically, our method correlates with the original Pyra-mid scores better than the common Responsiveness method, and shows better stability.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Pyramid method () consists of two manual phases.", "labels": [], "entities": []}, {"text": "The first phase is pyramid creation, performed once when a dataset is constructed, per each input topic to be summarized (either a single document or a set of documents).", "labels": [], "entities": [{"text": "pyramid creation", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.745390772819519}]}, {"text": "In this phase, experts exhaustively extract all SCU contributors (\"mentions\"), each being a text span describing an individual fact.", "labels": [], "entities": []}, {"text": "SCU contributors are extracted from several reference summaries of the source text.", "labels": [], "entities": []}, {"text": "Coreferring SCU contributors across reference summaries are then merged into a single SCU, which is given a representative label.", "labels": [], "entities": []}, {"text": "Each SCU is then assigned a weight, equal to the number of reference summaries in which it was found, indicating its salience.", "labels": [], "entities": []}, {"text": "The second phase is system evaluation, performed over the summaries produced by the evaluated system.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8067677617073059}]}, {"text": "Each Pyramid SCU for the source text is manually checked for its presence in the given system summary, whose Pyramid score is then computed as a normalized sum of the weights of the SCUs it contains.", "labels": [], "entities": [{"text": "Pyramid score", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.8485815227031708}]}, {"text": "The overall system score is defined as the average Pyramid score overall its evaluated summaries.", "labels": [], "entities": []}, {"text": "Although certain normalization variants attempt to weigh in SCU precision, the score is essentially an absolute \"recallstyle\" interpretation reflecting the system's ability to cover the content units found in the reference summaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9049912095069885}, {"text": "recallstyle", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9943910241127014}]}, {"text": "Such a fairly robust score allows, in principle, system comparison across experiments ().", "labels": [], "entities": []}, {"text": "We note that due to the Pyramid method's reliability, some research has been carried out on simulating the Pyramid method as a fully automatic one (.", "labels": [], "entities": []}, {"text": "The hope of such a line of work is to find an automatic evaluation method that is more reliable than the commonly used ones, by taking the reference summary semantic content into account.", "labels": [], "entities": []}, {"text": "Despite these efforts, automated Pyramid evaluations did not make their way yet to mainstream summary evaluation practices, where variants of the ROUGE metric) still prevail.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9846030473709106}]}, {"text": "In any case, as this paper focuses on manual evaluation, we compare our results to those of the manual Pyramid.", "labels": [], "entities": []}, {"text": "The Responsiveness method, introduced in DUC 2003, does not require reference summaries.", "labels": [], "entities": [{"text": "DUC 2003", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9383744895458221}]}, {"text": "Instead, human evaluators typically read both the source text and the system summary.", "labels": [], "entities": []}, {"text": "They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question.", "labels": [], "entities": []}, {"text": "Finally, compared systems are ranked by the average score of their summaries.", "labels": [], "entities": []}, {"text": "This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (.", "labels": [], "entities": []}, {"text": "Another common crowdsourcable evaluation method is pairwise comparison (: an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text.", "labels": [], "entities": []}, {"text": "This protocol allows comparing only two systems at a time, where the superior is determined by the total votes overall input texts.", "labels": [], "entities": []}, {"text": "The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores.", "labels": [], "entities": []}, {"text": "Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (.", "labels": [], "entities": []}, {"text": "The lightweight crowdsourcable Pyramid version we propose aims to preserve the interpretability and relative objectiveness of the Pyramid scores.", "labels": [], "entities": []}, {"text": "This could provide absolute scores for comparing multiple systems, which the pairwise method does not, in a more reliable manner than Responsiveness evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlations to the original Pyramid scores,  for our crowdsourced method and for expert Respon- siveness method, for DUC '05 and '06.", "labels": [], "entities": [{"text": "DUC '05", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.8784792621930441}]}]}