{"title": [{"text": "Mining Discourse Markers for Unsupervised Sentence Representation Learning", "labels": [], "entities": [{"text": "Mining Discourse Markers", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.4947791596253713}, {"text": "Sentence Representation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.8021526336669922}]}], "abstractContent": [{"text": "Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct.", "labels": [], "entities": []}, {"text": "Very little work adequately exploits unannotated data-such as discourse markers between sentences-mainly because of data sparseness and ineffective extraction methods.", "labels": [], "entities": []}, {"text": "In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data.", "labels": [], "entities": []}, {"text": "Our resulting dataset contains 174 discourse markers with at least 10K examples each, even for rare markers such as coincidentally or amazingly.", "labels": [], "entities": []}, {"text": "We use the resulting data as supervision for learning transferable sentence embeddings.", "labels": [], "entities": []}, {"text": "In addition, we show that even though sentence representation learning through prediction of discourse markers yields state of the art results across different transfer tasks, it is not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7899529337882996}]}, {"text": "Our datasets are publicly available 1", "labels": [], "entities": []}], "introductionContent": [{"text": "An important challenge within the domain of natural language processing is the construction of adequate semantic representations for textual unitsfrom words over sentences to whole documents.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.67719038327535}]}, {"text": "Recently, numerous approaches have been proposed for the construction of vector-based representations for larger textual units, especially sentences.", "labels": [], "entities": []}, {"text": "One of the most popular frameworks aims to induce sentence embeddings as an intermediate representation for predicting relations between sentence pairs.", "labels": [], "entities": [{"text": "predicting relations between sentence pairs", "start_pos": 108, "end_pos": 151, "type": "TASK", "confidence": 0.8540318727493286}]}, {"text": "For instance, similarity judgements (paraphrases) or inference relations have been used as prediction tasks, and the resulting embeddings perform well in practice, even when the representations are transfered to other semantic tasks (.", "labels": [], "entities": []}, {"text": "However, the kind of annotated data that is needed for such supervised approaches is costly to obtain, prone to bias, and arguably fairly limited with regard to the kind of semantic information captured, as they single out a narrow aspect of the entire semantic content.", "labels": [], "entities": []}, {"text": "Unsupervised approaches have also been proposed, based on sentence distributions in large corpora in relation to their discourse context.", "labels": [], "entities": []}, {"text": "For instance, construct sentence representations by trying to reconstruct neighbouring sentences, which allows them to take into account different contextual aspects of sentence meaning.", "labels": [], "entities": []}, {"text": "In the same vein, propose to predict if two sentences are consecutive, even though such local coherence can be straightforwardly predicted with relatively shallow features (.", "labels": [], "entities": []}, {"text": "A more elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing.", "labels": [], "entities": [{"text": "prediction of the semantic or rhetorical relation between two sentences", "start_pos": 32, "end_pos": 103, "type": "TASK", "confidence": 0.8233343482017517}, {"text": "discourse parsing", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7197958379983902}]}, {"text": "A number of annotated corpora exist, such as RST-DT () and PDTB (, but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult.", "labels": [], "entities": [{"text": "discourse relation prediction", "start_pos": 136, "end_pos": 165, "type": "TASK", "confidence": 0.7022275527318319}]}, {"text": "The problem, however, is much easier when there is a marker that makes the semantic link explicit, and this observation has often been used in a semi-supervised setting to predict discourse relations in general.", "labels": [], "entities": []}, {"text": "Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly.", "labels": [], "entities": []}, {"text": "Consider the following sentence pair: I live in Paris.", "labels": [], "entities": []}, {"text": "But I'm often abroad.", "labels": [], "entities": []}, {"text": "The discourse marker but highlights an opposition between the first sentence (the speaker s1 Paul Prudhomme's Louisiana Kitchen created a sensation when it was published in 1984.", "labels": [], "entities": []}, {"text": "c happily, s2' This family collective cookbook is just as good: Sample from our Discovery dataset lives in Paris) and the second sentence (the speaker is often abroad).", "labels": [], "entities": [{"text": "Discovery dataset", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.8599847555160522}]}, {"text": "The marker can thus be straightforwardly used as a label between sentence pairs.", "labels": [], "entities": []}, {"text": "In this case, the task is to predict c = but (among other markers) for the pair (I live in Paris, I'm often abroad).", "labels": [], "entities": []}, {"text": "Note that discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (c = therefore), subjectivity analysis (c = personally) or sentiment analysis (c = sadly).", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.6980958878993988}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.9475403726100922}]}, {"text": "More generally, discourse markers indicate how a sentence contributes to the meaning of a text, and they provide an appealing supervision signal for sentence representation learning based on language use.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 149, "end_pos": 181, "type": "TASK", "confidence": 0.8043333888053894}]}, {"text": "A wide variety of discourse usages would be desirable in order to learn general sentence representations.", "labels": [], "entities": []}, {"text": "Extensive research in linguistics has resulted in elaborate discourse marker inventories for many languages.", "labels": [], "entities": []}, {"text": "These inventories were created by manual corpus exploration or annotation of small-scale corpora: the largest annotated corpus, the English PDTB consists of a few tens of thousand examples, and provides a list of about 100 discourse markers, organized in a number of categories.", "labels": [], "entities": [{"text": "English PDTB", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.8376849293708801}]}, {"text": "Previous work on sentence representation learning with discourse markers makes use of even more restricted sets of discourse markers, as shown in table 2.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.805355171362559}]}, {"text": "use 9 categories as labels, accounting for 40 discourse markers in total.", "labels": [], "entities": []}, {"text": "It should be noted that the aggregate labels do not allow for any fine-grained distinctions; for instance, the TIME label includes both now and next, which is likely to impair the supervision.", "labels": [], "entities": [{"text": "TIME", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.609233558177948}]}, {"text": "Moreover, discourse markers maybe ambiguous; for example now can be used to express contrast.", "labels": [], "entities": []}, {"text": "On the other hand, make use of 15 discourse markers, 5 of which are accounting for more than 80% of their training data.", "labels": [], "entities": []}, {"text": "In order to ensure the quality of their examples, they only select pairs matching a dependency pattern manually specified for each marker.", "labels": [], "entities": []}, {"text": "As such, See for instance a sample of language on the Textlink project website: http://www.textlink.ii.metu.", "labels": [], "entities": [{"text": "Textlink project website", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.9161416093508402}]}, {"text": "edu.tr/dsd-view both of these studies use a restricted or impoverished set of discourse markers; they also both use the BookCorpus dataset, whose size (4.7M sentences that contain a discourse marker, according to) is prohibitively small for the prediction of rare discourse markers.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9907698929309845}]}, {"text": "In this work we use web-scale data in order to explore the prediction of a wide range of discourse markers, with more balanced frequency distributions, along with application to sentence representation learning.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 178, "end_pos": 210, "type": "TASK", "confidence": 0.8309817512830099}]}, {"text": "We use English data for the experiments, but the same method could be applied to any language that bears a typological resemblance with regard to discourse usage, and has sufficient amounts of textual data available (e.g. German or French).", "labels": [], "entities": []}, {"text": "Inspired by recent work () on the unexpected properties of recent manually labelled datasets (e.g. SNLI), we will also analyze our dataset to check whether labels are easy to guess, and whether the proposed model architectures make use of high-level reasoning for their predictions.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: -we propose a simple and efficient method to discover new discourse markers, and present a curated list of 174 markers for English; -we provide evidence that many connectives can be predicted with only simple lexical features; -we investigate whether relation prediction actually makes use of the relation between sentences; -we carryout extensive experiments based on the Infersent/SentEval framework.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 285, "end_pos": 304, "type": "TASK", "confidence": 0.8096955418586731}]}], "datasetContent": [{"text": "In the following, we call our method Discovery.", "labels": [], "entities": []}, {"text": "We create several variations of the sentence pairs dataset.", "labels": [], "entities": []}, {"text": "In DiscoveryHard, we remove examples where the candidate marker was among the top 5 predictions in our Fasttext shallow model and keep only the 174 candidate markers with a frequency of at least 10k.", "labels": [], "entities": []}, {"text": "Instances are then sampled randomly so that each marker appears exactly 10k times in the dataset.", "labels": [], "entities": []}, {"text": "Subsequently, the resulting set of discourse markers is also used in the other variations of our dataset.", "labels": [], "entities": []}, {"text": "DiscoveryBase designates the dataset for which examples predicted with the Fasttext model were not removed.", "labels": [], "entities": []}, {"text": "In order to measure the extent to which the model makes use of the relation between s 1 and s 2 , we also create a DiscoveryShuffled dataset, which is the DiscoveryBase dataset subjected to the random shuffle operation described previously.", "labels": [], "entities": [{"text": "DiscoveryShuffled dataset", "start_pos": 115, "end_pos": 140, "type": "DATASET", "confidence": 0.8831985294818878}, {"text": "DiscoveryBase dataset", "start_pos": 155, "end_pos": 176, "type": "DATASET", "confidence": 0.9339468777179718}]}, {"text": "To isolate the contribution of our discovery method, the dataset DiscoveryAdv discards all discourse markers from PDTB that were not found by our method.", "labels": [], "entities": []}, {"text": "Also, in order to measure the impact of label diversity, Discovery10 uses 174k examples for each of the 10 most frequent markers, thus totalling as many instances as DiscoveryBase.", "labels": [], "entities": [{"text": "DiscoveryBase", "start_pos": 166, "end_pos": 179, "type": "DATASET", "confidence": 0.9609867334365845}]}, {"text": "Finally, DiscoveryBig contains almost twice as many instances as DiscoveryBase, i.e. 20k instances for each discourse marker (although, fora limited number of markers, the number of instances is slightly lower due to data sparseness).", "labels": [], "entities": [{"text": "DiscoveryBase", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.934604823589325}]}, {"text": "Our goal is to evaluate the effect of using our various training datasets on sentence encoding, given encoders of equivalent capacity and similar setups.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7487355470657349}]}, {"text": "Thus, we follow the exact setup of Infersent (, also used in the Dissent () model: we learn to encode sentences into h with a bi-directional LSTM sentence encoder using element-wise max pooling overtime.", "labels": [], "entities": [{"text": "Infersent", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.8946192860603333}]}, {"text": "The dimension size of h is 4096.", "labels": [], "entities": [{"text": "dimension size", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9620440900325775}]}, {"text": "Word embeddings are fixed GloVe embeddings with 300 dimensions, trained on Common Crawl 840B.", "labels": [], "entities": [{"text": "Common Crawl 840B", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.9021907846132914}]}, {"text": "A sentence pair (s 1 , s 2 ) is represented with [h 1 , h 2 , h 1 h 2 , |h 2 \u2212 h 1 |], 6 which is fed to a softmax in order to predict a marker c.", "labels": [], "entities": []}, {"text": "Our datasets are split in 90% train, 5% validation, and 5% test.", "labels": [], "entities": []}, {"text": "Optimization is done with SGD (learning rate is initialized at 0.1, decayed by 1% at each epoch and by 80% if validation accuracy decreases; learning stops when learning rate is below 10 \u22125 and the best model on training task validation loss is used for evaluation; gradient is clipped when its norm exceeds 5).", "labels": [], "entities": []}, {"text": "Once the sentence encoder has been trained on abase task, the resulting sentence embeddings are tested with the SentEval library (.", "labels": [], "entities": []}, {"text": "We evaluate the different variations of our dataset we described above in order to analyze their effect, and compare them to a number of existing models.", "labels": [], "entities": []}, {"text": "displays the tasks used for evaluation.", "labels": [], "entities": []}, {"text": "For further analysis, table 9 displays the result of Linguistic Probing using the method by.", "labels": [], "entities": [{"text": "Linguistic Probing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7230408489704132}]}, {"text": "Although these tasks are primarily designed for understanding the content of embeddings, they also focus on aspects that are desirable to perform well in general semantic tasks (e.g. prediction of tense, or number of object).", "labels": [], "entities": [{"text": "prediction of tense", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.8505596915880839}]}, {"text": "gives an overview of transfer learning evaluation, also comparing to other supervised and unsupervised approaches.", "labels": [], "entities": [{"text": "transfer learning evaluation", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.9625353614489237}]}, {"text": "Note that we outperform DisSent on all tasks except TREC 7 with less than half the amount of training examples.", "labels": [], "entities": [{"text": "TREC 7", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.559409573674202}]}, {"text": "In addition, our approach is arguably simpler and faster.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Discourse markers or classes used by previous work on unsupervised representation learning", "labels": [], "entities": []}, {"text": " Table 4: Candidate discourse markers that are the most  difficult to predict from shallow features", "labels": [], "entities": []}, {"text": " Table 5: Candidate discourse markers that are the easi- est to predict from shallow features. This shows candi- dates that are unlikely to be interesting discourse cues.", "labels": [], "entities": []}, {"text": " Table 6: SentEval evaluation results with our models trained on various datasets. The first two models are su- pervised, the other ones unsupervised. All scores are accuracy percentages, except SICK-R, which is Pearson  correlation percentage. InferSent is from Conneau et al. (2017), MTL is the multi-task learning based model from  Subramanian et al. (2018). Evaluation tasks are described in table 7, and N denotes the number of examples for  each dataset (in millions). Dissent is from Nie et al. (2017), QuickThought is from Logeswaran and Lee", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9989354014396667}, {"text": "SICK-R", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9276573657989502}, {"text": "Pearson  correlation percentage", "start_pos": 212, "end_pos": 243, "type": "METRIC", "confidence": 0.9385250012079874}, {"text": "QuickThought", "start_pos": 510, "end_pos": 522, "type": "METRIC", "confidence": 0.6586027145385742}]}, {"text": " Table 7: Transfer evaluation tasks. N is the number of  training examples and C is number of classes for each  task.", "labels": [], "entities": [{"text": "Transfer evaluation tasks", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8963146408398946}]}, {"text": " Table 8: Test results (accuracy) on implicit discursive  relation prediction task (PDTB relations level 1 and 2,  i.e coarse-grained and fine-grained) and training tasks  T . Note that scores for T are not comparable since the  test set changes for each version of the dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9992175102233887}, {"text": "implicit discursive  relation prediction task", "start_pos": 37, "end_pos": 82, "type": "TASK", "confidence": 0.6621707141399383}]}, {"text": " Table 9: Accuracy of various models on linguistic probing tasks using logistic regression on SentEval. BShift is  detection of token inversion. CoordInv is detection of clause inversion. ObjNum/SubjNum is prediction of the  number of object resp. subject. Tense is prediction of the main verb tense. Depth is prediction of parse tree depth.  TC is detection of common sequences of constituents. WC is prediction of words contained in the sentence. OddM  is detection of random replacement of verbs/nouns by other verbs/nouns. AVG is the average score of those tasks  for each model. For more details see Conneau et al. (2018). SkipThought and Infersent results come from Perone  et al. (2018), QuickThought results come from Brahma (2018).", "labels": [], "entities": [{"text": "AVG", "start_pos": 527, "end_pos": 530, "type": "METRIC", "confidence": 0.9973264932632446}, {"text": "QuickThought", "start_pos": 695, "end_pos": 707, "type": "METRIC", "confidence": 0.8629493117332458}]}]}