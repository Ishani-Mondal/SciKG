{"title": [{"text": "Integrating Semantic Knowledge to Tackle Zero-shot Text Classification", "labels": [], "entities": [{"text": "Tackle Zero-shot Text Classification", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.8650376200675964}]}], "abstractContent": [{"text": "Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8593542277812958}]}, {"text": "Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem.", "labels": [], "entities": [{"text": "zero-shot text classification", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.6446242332458496}]}, {"text": "In this paper, we propose a two-phase framework together with data augmentation and feature augmentation to solve this problem.", "labels": [], "entities": []}, {"text": "Four kinds of semantic knowledge (word embeddings, class descriptions , class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively.", "labels": [], "entities": []}, {"text": "Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.999382734298706}]}], "introductionContent": [{"text": "As one of the most fundamental problems in machine learning, automatic classification has been widely studied in several domains.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7232727408409119}, {"text": "automatic classification", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.8168465495109558}]}, {"text": "However, many approaches, proven to be effective in traditional classification tasks, cannot catch up with a dynamic and open environment where new classes can emerge after the learning stage.", "labels": [], "entities": []}, {"text": "For example, the number of topics on social media is growing rapidly, and the classification models are required to recognise the text of the new topics using only general information (e.g., descriptions of the topics) since labelled training instances are unfeasible to obtain for each new topic ().", "labels": [], "entities": []}, {"text": "This scenario holds in many real-world domains such as object recognition and medical diagnosis.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9010853469371796}, {"text": "medical diagnosis", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7360126972198486}]}, {"text": "Zero-shot learning (ZSL) for text classification aims to classify documents of classes which are absent from the learning stage.", "labels": [], "entities": [{"text": "text classification", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7856687605381012}]}, {"text": "Although it is challenging fora machine to achieve, humans are able to learn new concepts by transferring knowledge from known to unknown domains based on high-level descriptions and semantic representations.", "labels": [], "entities": []}, {"text": "Therefore, without labelled data of unseen classes, a zero-shot learning framework is expected to exploit supportive semantic knowledge (e.g., class descriptions, relations among classes, and external domain knowledge) to generally infer the features of unseen classes using patterns learned from seen classes.", "labels": [], "entities": []}, {"text": "So far, three main types of semantic knowledge have been employed in general zero-shot scenarios ( ).", "labels": [], "entities": []}, {"text": "The most widely used one is semantic attributes of classes such as visual concepts (e.g., colours, shapes) and semantic properties (e.g., behaviours, functions) (.", "labels": [], "entities": []}, {"text": "The second type is concept ontology, including class hierarchy and knowledge graphs, which represents relationships among classes and features (.", "labels": [], "entities": []}, {"text": "The third type is semantic word embeddings which capture implicit relationships between words thanks to a large training text corpus.", "labels": [], "entities": []}, {"text": "Nonetheless, concerning ZSL in text classification particularly, there are few studies exploiting one of these knowledge types and none has considered the combinations of them (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7533089518547058}]}, {"text": "Moreover, some previous works used different datasets to train and test, but there is similarity between classes in the training and testing set.", "labels": [], "entities": []}, {"text": "For example, in), the class \"imdb.com\" in the training set naturally corresponds to the class \"Movies\" in the testing set.", "labels": [], "entities": []}, {"text": "Hence, these methods are not working under a strict zero-shot scenario.", "labels": [], "entities": []}, {"text": "To tackle the zero-shot text classification problem, this paper proposes a novel two-phase framework together with data augmentation and feature augmentation).", "labels": [], "entities": [{"text": "zero-shot text classification", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6059214472770691}]}, {"text": "In addition, four kinds of semantic knowledge including word embeddings, class descriptions, class hierarchy, and a general knowledge graph (ConceptNet) are exploited in the framework to effectively learn the unseen classes.", "labels": [], "entities": []}, {"text": "Both of the two phases are based on convolutional neural networks.", "labels": [], "entities": []}, {"text": "The first phase called coarse-grained classification judges if a document is from seen or unseen classes.", "labels": [], "entities": [{"text": "coarse-grained classification", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7550954222679138}]}, {"text": "Then, the second phase, named finegrained classification, finally decides its class.", "labels": [], "entities": [{"text": "finegrained classification", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7619206607341766}]}, {"text": "Note that all the classifiers in this framework are trained using labelled data of seen classes (and augmented text data) only.", "labels": [], "entities": []}, {"text": "None of the steps learns from the labelled data of unseen classes.", "labels": [], "entities": []}, {"text": "The contributions of our work can be summarised as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel deep learning based twophase framework, including coarse-grained and fine-grained classification, to tackle the zero-shot text classification problem.", "labels": [], "entities": [{"text": "zero-shot text classification", "start_pos": 133, "end_pos": 162, "type": "TASK", "confidence": 0.585482120513916}]}, {"text": "Unlike some previous works, our framework does not require semantic correspondence between classes in a training stage and classes in an inference stage.", "labels": [], "entities": []}, {"text": "In other words, the seen and unseen classes can be clearly different.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel data augmentation technique called topic translation to strengthen the capability of our framework to detect documents from unseen classes effectively.", "labels": [], "entities": [{"text": "topic translation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7630715072154999}]}, {"text": "\u2022 We propose a method to perform feature augmentation by using integrated semantic knowledge to transfer the knowledge learned from seen to unseen classes in the zero-shot scenario.", "labels": [], "entities": [{"text": "feature augmentation", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7701631784439087}]}, {"text": "In the remainder of this paper, we firstly explain our proposed zero-shot text classification framework in section 2.", "labels": [], "entities": [{"text": "zero-shot text classification", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.6311151882012686}]}, {"text": "Experiments and results, which demonstrate the performance of our framework, are presented in section 3.", "labels": [], "entities": []}, {"text": "Related works are discussed in section 4.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes our work and mentions possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used two textual datasets for the experiments.", "labels": [], "entities": []}, {"text": "The vocabulary size of each dataset was limited by 20,000 most frequent words and all numbers were excluded.", "labels": [], "entities": []}, {"text": "(1) DBpedia ontology dataset ( ) includes 14 non-overlapping classes and textual data collected from Wikipedia.", "labels": [], "entities": [{"text": "DBpedia ontology dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7514829039573669}]}, {"text": "Each class has 40,000 training and 5,000 testing samples.", "labels": [], "entities": []}, {"text": "(2) The 20newsgroups dataset 2 has 20 topics each of which has approximately 1,000 documents.", "labels": [], "entities": [{"text": "20newsgroups dataset", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.8142720758914948}]}, {"text": "70% of the documents of each class were randomly selected for training, and the remaining 30% were used as a testing set.", "labels": [], "entities": []}, {"text": "We compared each phase and the overall framework with the following approaches and settings.", "labels": [], "entities": []}, {"text": "Phase 1: Proposed by (, DOC is a state-of-the-art open-world text classification approach which classifies anew sample into a seen class or \"reject\" if the sample does not belong to any seen classes.", "labels": [], "entities": [{"text": "open-world text classification", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6729245881239573}]}, {"text": "The DOC uses a single CNN and a 1-vs-rest sigmoid output layer with threshold adjustment.", "labels": [], "entities": [{"text": "CNN", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9003486633300781}]}, {"text": "Unlike DOC, the classifiers in the proposed Phase 1 work individually.", "labels": [], "entities": [{"text": "DOC", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.864008367061615}]}, {"text": "However, fora fair comparison, we used DOC only as a binary classifier in this phase (\u02c6 y i \u2208 C S or\u02c6yor\u02c6 or\u02c6y i / \u2208 C S ).", "labels": [], "entities": []}, {"text": "Phase 2: To see how well the augmented feature v w,c work in ZSL, we ran the zero-shot classifier with different combinations of inputs.", "labels": [], "entities": []}, {"text": "Particularly, five combinations of v w , v c , and v w,c were tested with documents from unseen classes only (traditional ZSL).", "labels": [], "entities": []}, {"text": "The whole framework: (1) Count-based model selected the class whose label appears most frequently in the document as\u02c6yas\u02c6 as\u02c6y i . (2) Label similarity () is an unsupervised approach which calculates the cosine similarity between the sum of word embeddings of each class label and the sum of word embeddings of every n-gram (n = 1, 2, 3) in the document.", "labels": [], "entities": []}, {"text": "We adopted this approach to do single-label classification by predicting the class that got the highest similarity score among all classes.", "labels": [], "entities": [{"text": "single-label classification", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7552489638328552}, {"text": "similarity score", "start_pos": 104, "end_pos": 120, "type": "METRIC", "confidence": 0.9610255658626556}]}, {"text": "(3) RNN AutoEncoder was built based on a Seq2Seq model with LSTM (512 hidden units), and it was trained to encode documents and class labels onto the same latent space.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9238787889480591}]}, {"text": "The cosine similarity was applied to select a class label closest to the document on the latent space.", "labels": [], "entities": []}, {"text": "(4) RNN+FC refers to the architecture 2 proposed in (.", "labels": [], "entities": []}, {"text": "It used an RNN layer with LSTM (512 hidden units) followed by two dense layers with 400 and 100 units respectively.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9268680214881897}]}, {"text": "(5) CNN+FC replaced the RNN in the previous model with a CNN, which has the identical structure as the zero-shot classifier in Phase 2.", "labels": [], "entities": [{"text": "CNN+FC", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7838941415150961}]}, {"text": "Both RNN+FC and CNN+FC predicted the confidence p(\u02c6 y i = c|x i ) given v wand v c . The class with the highest confidence was selected as\u02c6yas\u02c6 as\u02c6y i . For Phase 1, we used the accuracy for binary classification (y, \u02c6 y i \u2208 C S or y, \u02c6 y i / \u2208 C S ) as an evaluation metric.", "labels": [], "entities": [{"text": "CNN+FC", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9203517039616903}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9979922771453857}]}, {"text": "In contrast, for Phase 2 and the whole framework, we used the multi-class classification accuracy (\u02c6 y i = y i ) as a metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.5090497732162476}]}], "tableCaptions": [{"text": " Table 1: The rates of unseen classes and the numbers  of augmented documents (per unseen class) in the ex- periments", "labels": [], "entities": []}, {"text": " Table 2: The accuracy of the whole framework compared with the baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996488094329834}]}, {"text": " Table 3: The accuracy of Phase 1 with and without aug- mented data compared with DOC .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997349381446838}, {"text": "DOC", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.7700634002685547}]}, {"text": " Table 5: The accuracy of the traditional classifier in  Phase 2 given documents from seen classes only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996447563171387}]}, {"text": " Table 6: The accuracy of the zero-shot classifier in  Phase 2 given documents from unseen classes only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996410608291626}]}]}