{"title": [{"text": "A Large-Scale Comparison of Historical Text Normalization Systems", "labels": [], "entities": [{"text": "Historical Text Normalization", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.5530091524124146}]}], "abstractContent": [{"text": "There is no consensus on the state-of-the-art approach to historical text normalization.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.6651196877161661}]}, {"text": "Many techniques have been proposed, including rule-based methods, distance metrics, character-based statistical machine translation, and neural encoder-decoder models, but studies have used different datasets, different evaluation methods, and have come to different conclusions.", "labels": [], "entities": [{"text": "character-based statistical machine translation", "start_pos": 84, "end_pos": 131, "type": "TASK", "confidence": 0.6195731684565544}]}, {"text": "This paper presents the largest study of historical text normalization done so far.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6595214903354645}]}, {"text": "We critically survey the existing literature and report experiments on eight languages, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods.", "labels": [], "entities": []}, {"text": "The datasets and scripts are made publicly available.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Systems The selection of normalization systems follows two goals: (i) to include at least one system for each major category as identified in Sec.", "labels": [], "entities": []}, {"text": "2; and (ii) to use only freely available tools in order to facilitate reproduction and application of the described methods.", "labels": [], "entities": []}, {"text": "To that effect, this study compares the following approaches: \u2022 Norma 3 (Bollmann, 2012), which combines substitution lists, a rule-based normalizer, and a distance-based algorithm, with the option of running them separately or combined.", "labels": [], "entities": []}, {"text": "Importantly, it implements supervised learning algorithms for all of these components and is not restricted to a particular language.", "labels": [], "entities": []}, {"text": "\u2022 cSMTiser 4, which implements a normalization pipeline using character-based statistical machine translation (CSMT) using the Moses toolkit ().", "labels": [], "entities": [{"text": "character-based statistical machine translation (CSMT)", "start_pos": 62, "end_pos": 116, "type": "TASK", "confidence": 0.7039875984191895}]}, {"text": "\u2022 Neural machine translation (NMT), in the form of two publicly available implementations: (i) the model by, also used in; 5 and (ii) the model by.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 2, "end_pos": 34, "type": "TASK", "confidence": 0.7723943144083023}]}, {"text": "Two systems were chosen for the NMT approach as they use very different hyperparameters, despite both using comparable neural encoderdecoder models: Bollmann  Datasets gives an overview of the historical datasets.", "labels": [], "entities": [{"text": "Bollmann  Datasets", "start_pos": 149, "end_pos": 167, "type": "DATASET", "confidence": 0.8784353733062744}]}, {"text": "They are taken from and represent the largest and most varied collection of datasets used for historical text normalization so far, covering eight languages from different language families-English, German, Hungarian, Icelandic, Spanish, Portuguese, Slovene, and Swedish-as well as different text genres and time periods.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6548710366090139}]}, {"text": "Furthermore, most of these have also been used in previous work, such as the English, Hungarian, Icelandic, and Swedish datasets (e.g.,) and the Slovene datasets (e.g., Ljube\u0161i\u00b4c.", "labels": [], "entities": [{"text": "Slovene datasets", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.6464610695838928}]}, {"text": "Additionally, contemporary datasets are required for the rule-based and distance-based components of Norma, as they expect a list of valid target word forms to function properly.", "labels": [], "entities": []}, {"text": "For this, we want to choose resources that are readily available for many languages and are reliable, i.e., consist of carefully edited text.", "labels": [], "entities": []}, {"text": "Here, I choose a combination of three sources: 7 (i) the normalizations in the training sets, (ii) the Europarl corpus (, and (iii) the parallel Bible corpus by.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.9893864989280701}]}, {"text": "The only exception is Icelandic, which is not covered by Europarl; here, we can follow Pettersson (2016) instead by using data from two specialized resources, the B\u00cdN database) and the M\u00cdM corpus (.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9851940274238586}, {"text": "B\u00cdN database", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.7848629057407379}, {"text": "M\u00cdM corpus", "start_pos": 185, "end_pos": 195, "type": "DATASET", "confidence": 0.9686170220375061}]}, {"text": "This way, we obtain full-form lexica of: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent; best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from the best result using McNemar's test at p < 0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7515544891357422}, {"text": "McNemar's test", "start_pos": 280, "end_pos": 294, "type": "DATASET", "confidence": 0.8956549564997355}]}, {"text": "\u2020 indicates scores that were not (re)produced here, but reported in previous work; they might not be strictly comparable due to differences in data preprocessing (cf. Sec. 3).", "labels": [], "entities": []}, {"text": "Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the theoretical maximum accuracy with purely token-level methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9995032548904419}, {"text": "Maximum", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.8744131922721863}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9972611665725708}]}, {"text": "component, while the latter is reported to produce the best results (Bollmann, 2012).", "labels": [], "entities": []}, {"text": "For cSMTiser, the authors suggest using additional monolingual data to improve the language model; the contemporary datasets are used for this purpose and the model is trained both without and with this additional data; the latter is denoted cSMTiser +LM . For NMT, the model by is evaluated using an ensemble of five models; the model by is trained on character-level input using the default settings provided by their implementation.", "labels": [], "entities": []}, {"text": "To illustrate how challenging the normalization task is on different datasets, we can additionally look at the identity baseline-i.e., the percentage of tokens that do not need to be normalized-as well as the maximum accuracy obtainable if each word type was mapped to its most frequently occurring normalization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9982798099517822}]}, {"text": "The latter gives an indication of the extent of ambiguity in the datasets and the disadvantage of not considering token context (cf. Sec. 2.6).", "labels": [], "entities": []}, {"text": "Results shows the results of this evaluation.", "labels": [], "entities": []}, {"text": "The extent of spelling variation varies greatly between datasets, with less than 15% of tokens requiring normalization (SL G ) to more than 80% (HU).", "labels": [], "entities": [{"text": "normalization (SL G )", "start_pos": 105, "end_pos": 126, "type": "METRIC", "confidence": 0.7204673290252686}]}, {"text": "The maximum accuracy is above 97% for most datasets, suggesting that we can obtain high normalization accuracy in principle even without considering token context.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992208480834961}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9017415642738342}]}, {"text": "For the normalization systems, we observe significantly better word accuracy with SMT than NMT on four of the datasets, and non-significant differences on five others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9183463454246521}, {"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9579907059669495}]}, {"text": "There is only one dataset (DE A ) where the NMT system by gets significantly better word accuracy than other systems.", "labels": [], "entities": [{"text": "DE A )", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9324175119400024}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8811770081520081}]}, {"text": "This somewhat contradicts the results from, who find NMT to usually outperform the SMT baseline by.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9588772058486938}]}, {"text": "However, note that the results for the cSMTiser system are often significantly better than reported in previous work: e.g., on Hungarian, cSMTiser obtains 91.7% accuracy, but only 80.1% with the SMT system from Pettersson et al..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9992535710334778}, {"text": "SMT", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.9654858112335205}]}, {"text": "Overall, the deep NMT model by consistently outperforms the shallow one by.", "labels": [], "entities": []}, {"text": "cSMTiser seems to benefit from the added contemporary data for language modelling, though the effect is not significant on any individual dataset.", "labels": [], "entities": [{"text": "cSMTiser", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.955860435962677}, {"text": "language modelling", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7946844398975372}]}, {"text": "Finally, while Norma does produce competitive results on sev-: Evaluations on the subset of incorrect normalizations only; best results for each dataset in bold.", "labels": [], "entities": []}, {"text": "Note that this subset is different for each system, so for comparisons between systems, these numbers should be considered in conjunction with word accuracy scores from eral datasets (particularly in the \"combined\" setting), it is generally significantly behind the SMT and NMT methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.6628881096839905}, {"text": "SMT", "start_pos": 266, "end_pos": 269, "type": "TASK", "confidence": 0.8420278429985046}]}], "tableCaptions": [{"text": " Table 1: Historical datasets used in the experiments", "labels": [], "entities": []}, {"text": " Table 2: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent;  best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from the  best result using McNemar's test at p < 0.05.  \u2020 indicates scores that were not (re)produced here, but reported  in previous work; they might not be strictly comparable due to differences in data preprocessing (cf. Sec. 3).  Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the  theoretical maximum accuracy with purely token-level methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8338998556137085}, {"text": "McNemar's test", "start_pos": 251, "end_pos": 265, "type": "DATASET", "confidence": 0.8389839132626852}, {"text": "accuracy", "start_pos": 491, "end_pos": 499, "type": "METRIC", "confidence": 0.9992287158966064}, {"text": "accuracy", "start_pos": 584, "end_pos": 592, "type": "METRIC", "confidence": 0.9920185208320618}]}, {"text": " Table 3: Evaluations on the subset of incorrect normalizations only; best results for each dataset in bold. Note that  this subset is different for each system, so for comparisons between systems, these numbers should be considered  in conjunction with word accuracy scores from", "labels": [], "entities": [{"text": "accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.732634425163269}]}, {"text": " Table 4: Word accuracy for seen/unseen tokens separately (cf. Sec. 5.3); best results for each dataset in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9359113574028015}]}, {"text": " Table 5: Word accuracy for the \"lookup on seen tokens, learned models on unseen tokens\" strategy, following  Robertson and Goldwater (2018) (cf. Sec. 5.3), compared to the best result without this strategy (according to  Table 2). Best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from  the best result using McNemar's test at p < 0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9184211492538452}, {"text": "McNemar's test", "start_pos": 366, "end_pos": 380, "type": "DATASET", "confidence": 0.8480341037114462}]}]}