{"title": [], "abstractContent": [{"text": "Recent work on visually grounded language learning has focused on broader applications of grounded representations, such as visual question answering and multimodal machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.695937916636467}, {"text": "multimodal machine translation", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.6126865049203237}]}, {"text": "In this paper we consider grounded word sense translation, i.e. the task of correctly translating an ambiguous source word given the corresponding textual and visual context.", "labels": [], "entities": [{"text": "grounded word sense translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.67573631554842}]}, {"text": "Our main objective is to investigate the extent to which images help improve word-level (lexical) translation quality.", "labels": [], "entities": [{"text": "word-level (lexical) translation quality", "start_pos": 77, "end_pos": 117, "type": "TASK", "confidence": 0.6579151401917139}]}, {"text": "We do so by first studying the dataset for this task to understand the scope and challenges of the task.", "labels": [], "entities": []}, {"text": "We then explore different data settings, image features, and ways of grounding to investigate the gain from using images in each of the combinations.", "labels": [], "entities": []}, {"text": "We find that grounding on the image is specially beneficial in weaker uni-directional recurrent translation models.", "labels": [], "entities": []}, {"text": "We observe that adding structured image information leads to stronger gains in lexical translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.7923628687858582}]}], "introductionContent": [{"text": "The multimodal machine translation (MMT) shared task has been conducted for the past three years () with the main goal of investigating the effectiveness of information from images in machine translation (MT).", "labels": [], "entities": [{"text": "multimodal machine translation (MMT) shared task", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.8070005141198635}, {"text": "machine translation (MT)", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.8571272253990173}]}, {"text": "However, as acknowledged in, it has been difficult to evaluate the impact of multimodality (images) on the sentence-level translation quality, since the changes incurred by having an additional modality can be quite subtle.", "labels": [], "entities": []}, {"text": "The MMT shared task consists of translating English sentences that describe an image into a target language given the English sentence itself and the image that it describes.", "labels": [], "entities": [{"text": "MMT shared task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9008804162343343}]}, {"text": "Recently proposed, the multimodal lexical translation (MLT) ( ) is a  similar task but focused at the word level and only at ambiguous words.", "labels": [], "entities": [{"text": "multimodal lexical translation (MLT)", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.7565254668394724}]}, {"text": "In MLT, the objective is to correctly translate each ambiguous word in the English source sentence into a corresponding word in the target language given the word itself, the English sentence in which it occurs and the image being described by that sentence.", "labels": [], "entities": [{"text": "MLT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9605263471603394}]}, {"text": "This is similar to the task of Visual Sense Disambiguation () where the objective is to disambiguate the ambiguous verbs using text and image contexts.", "labels": [], "entities": [{"text": "Visual Sense Disambiguation", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6249009768168131}]}, {"text": "The authors of MLT proposed to define a word in the source language to be ambiguous if it has multiple translations in the target language with different meanings in the dataset.", "labels": [], "entities": []}, {"text": "However, they did not suggest any models for that.", "labels": [], "entities": []}, {"text": "In this paper, we propose to treat MLT as a sequence labeling task, as depicted by the example in, similar to part-of-speech tagging or named entity recognition.", "labels": [], "entities": [{"text": "MLT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9725960493087769}, {"text": "sequence labeling task", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7018587191899618}, {"text": "part-of-speech tagging", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7132503241300583}, {"text": "named entity recognition", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.5883121192455292}]}, {"text": "Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation () and approaches to ground machine translation).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7381410201390585}, {"text": "ground machine translation", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.7260134617487589}]}, {"text": "More specifically, we propose and empirically evaluate grounded translation disambiguation models based on recurrent sequential units for the task of MLT.", "labels": [], "entities": [{"text": "translation disambiguation", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.8666007816791534}, {"text": "MLT", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9439337253570557}]}, {"text": "Our primary contributions are: \u2022 An investigation of the MLT dataset to understand the scope and challenges of the task: we find the task is challenging because of the skewed distribution of translation candidates in the training set and that the scope of improvements from images is about 7.8% for English-German and 8.6% for EnglishFrench.", "labels": [], "entities": [{"text": "MLT dataset", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8103407919406891}, {"text": "EnglishFrench", "start_pos": 327, "end_pos": 340, "type": "DATASET", "confidence": 0.9647721648216248}]}, {"text": "\u2022 An investigation into data settings for the task: we find that models trained to tag all words, irrespective of their ambiguity level, perform better than other settings.", "labels": [], "entities": []}, {"text": "\u2022 A study on the effect of visual representations for grounded recurrent models: we find that simple unidirectional recurrent models gain more with conditioning of visual information than stronger bidirectional recurrent models.", "labels": [], "entities": []}, {"text": "\u2022 An investigation on different visual representations for the task: we find that structured image information (in the form of objects) perform better than the popularly used ResNet pool5 image features.", "labels": [], "entities": []}, {"text": "extract the MLT dataset from the Multi30K (.", "labels": [], "entities": [{"text": "MLT dataset", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8679748773574829}, {"text": "Multi30K", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9384515285491943}]}, {"text": "MLT was also used to compute Lexical Translation Accuracy for systems submitted to the WMT18 multimodal translation shared task ().", "labels": [], "entities": [{"text": "MLT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5828860402107239}, {"text": "Lexical Translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.9061145484447479}, {"text": "Accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.5770249962806702}, {"text": "WMT18 multimodal translation shared task", "start_pos": 87, "end_pos": 127, "type": "TASK", "confidence": 0.6838737010955811}]}, {"text": "The dataset consists of 31,014 images with one English description per image, where the ambiguous words in the description, if any, are labeled to their corresponding lexical translations in the target language conforming to the given context (see).", "labels": [], "entities": []}, {"text": "The dataset is split into training, validation and test sets in the same way as in the WMT's MMT task in 2016 (see).", "labels": [], "entities": [{"text": "WMT's MMT task in 2016", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.8734039068222046}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.", "labels": [], "entities": [{"text": "MLT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9055701494216919}, {"text": "UA", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9007179737091064}, {"text": "APS", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9398961663246155}, {"text": "APHW", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9308140873908997}, {"text": "TCPA", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.8030613660812378}, {"text": "SR", "start_pos": 221, "end_pos": 223, "type": "METRIC", "confidence": 0.9953706860542297}, {"text": "Skewness Ratio", "start_pos": 225, "end_pos": 239, "type": "METRIC", "confidence": 0.9119144976139069}, {"text": "WSR", "start_pos": 271, "end_pos": 274, "type": "METRIC", "confidence": 0.9869953989982605}, {"text": "Weigthed average of SRs", "start_pos": 276, "end_pos": 299, "type": "METRIC", "confidence": 0.9346403777599335}]}, {"text": " Table 3: Results of the Human Experiment. Ins: In- stances with ambiguous words. Img: the Ins instances  where the Image was used. Img-MFT: the Img in- stances where the Most Frequent Translation was not  selected (filtered out) by the annotators. Img-MFT /  Ins (Scope): the ratio of Img-MFT to Ins expressed in  percentage; and as discussed in Section 2.2.2 this re- flects the Scope of improvement at Lexical Translation  using Images.", "labels": [], "entities": [{"text": "Lexical Translation", "start_pos": 405, "end_pos": 424, "type": "TASK", "confidence": 0.8438745141029358}]}, {"text": " Table 4: Comparing multimodal models with their text- only counterparts in different data settings. We observe  ULSTM benefits more from the ResNet-50 global im- age feature as compared to BLSTM.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.8422767519950867}]}, {"text": " Table 5: Comparing object-based grounding BLSTM  models with other BLSTM models in different data set- tings.", "labels": [], "entities": []}]}