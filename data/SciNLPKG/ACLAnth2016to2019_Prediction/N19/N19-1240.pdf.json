{"title": [{"text": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.870214432477951}]}], "abstractContent": [{"text": "Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs.", "labels": [], "entities": [{"text": "reading comprehension", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8611379563808441}]}, {"text": "We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents.", "labels": [], "entities": []}, {"text": "We frame it as an inference problem on a graph.", "labels": [], "entities": []}, {"text": "Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within-and cross-document coreference).", "labels": [], "entities": []}, {"text": "Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning.", "labels": [], "entities": []}, {"text": "Our Entity-GCN method is scalable and compact , and it achieves state-of-the-art results on a multi-document question answering dataset, WIKIHOP (Welbl et al., 2018).", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.8495367765426636}]}], "introductionContent": [{"text": "The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6574630041917165}]}, {"text": "Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD ( and CNN/Daily Mail (, enabling end-to-end training of neural models ().", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8341193199157715}, {"text": "CNN/Daily Mail", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9274240285158157}]}, {"text": "These systems, given a text and a question, need to answer the query relying on the given document.", "labels": [], "entities": []}, {"text": "Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence).", "labels": [], "entities": []}, {"text": "The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (, TriviaQA (, and RACE (, have been created in such away as to address this shortcoming and to ensure that systems  relying only on local information cannot achieve competitive performance.", "labels": [], "entities": []}, {"text": "Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7944126129150391}]}, {"text": "The WIKIHOP dataset ( was explicitly created to facilitate the development of systems dealing with these scenarios.", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9495877027511597}]}, {"text": "Each example in WIKIHOP consists of a collection of documents, a query and a set of candidate answers ().", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.8837605118751526}]}, {"text": "Though there is no guarantee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using a chain of reasoning crossing document boundaries.", "labels": [], "entities": []}, {"text": "Though an important practical problem, the multi-hop setting has so far received little attention.", "labels": [], "entities": []}, {"text": "The methods reported by approach the task by merely concatenating all documents into a single long text and training a standard RNN-based reading comprehension model, namely, BiDAF ( and.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 175, "end_pos": 180, "type": "METRIC", "confidence": 0.8744260668754578}]}, {"text": "Document concatenation in this setting is also used in Weaver () and MHPGM ().", "labels": [], "entities": [{"text": "Document concatenation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8182760775089264}, {"text": "Weaver", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.9539674520492554}, {"text": "MHPGM", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8502405881881714}]}, {"text": "The only published paper which goes beyond concatenation is due to, where they augment RNNs with jump-links corresponding to co-reference edges.", "labels": [], "entities": []}, {"text": "Though these edges provide a structural bias, the RNN states are still tasked with passing the information across the document and performing multihop reasoning.", "labels": [], "entities": []}, {"text": "Instead, we frame question answering as an inference problem on a graph representing the document collection.", "labels": [], "entities": [{"text": "question answering", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7401076704263687}]}, {"text": "Nodes in this graph correspond to named entities in a document whereas edges encode relations between them (e.g., crossand within-document coreference links or simply co-occurrence in a document).", "labels": [], "entities": []}, {"text": "We assume that reasoning chains can be captured by propagating local contextual information along edges in this graph using a graph convolutional network (GCN).", "labels": [], "entities": []}, {"text": "The multi-document setting imposes scalability challenges.", "labels": [], "entities": []}, {"text": "In realistic scenarios, a system needs to learn to answer a query fora given collection (e.g., Wikipedia or a domain-specific set of documents).", "labels": [], "entities": []}, {"text": "In such scenarios one cannot afford to run expensive document encoders (e.g., RNN or transformer-like self-attention (), unless the computation can be preprocessed both at train and test time.", "labels": [], "entities": []}, {"text": "Even if (similarly to WIKIHOP creators) one considers a coarse-to-fine approach, where a set of potentially relevant documents is provided, re-encoding them in a query-specific way remains the bottleneck.", "labels": [], "entities": []}, {"text": "In contrast to other proposed methods (e.g.,), we avoid training expensive document encoders.", "labels": [], "entities": []}, {"text": "In our approach, only a small query encoder, the GCN layers and a simple feed-forward answer selection component are learned.", "labels": [], "entities": []}, {"text": "Instead of training RNN encoders, we use contextualized embeddings (ELMo) to obtain initial (local) representations of nodes.", "labels": [], "entities": []}, {"text": "This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed.", "labels": [], "entities": []}, {"text": "Even in the somewhat contrived WIKIHOP setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF.", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.6548704504966736}, {"text": "BiDAF", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.4935965836048126}]}, {"text": "1 Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par When compared to the 'small' and hence fast BiDAF model reported in, which is 25% less accurate than our Entity-GCN.", "labels": [], "entities": []}, {"text": "Larger RNN models are problematic also because of GPU memory constraints. with many techniques that use expensive questionaware recurrent document encoders.", "labels": [], "entities": []}, {"text": "Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results.", "labels": [], "entities": []}, {"text": "As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by.", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows: \u2022 we present a novel approach for multi-hop QA that relies on a (pre-trained) document encoder and information propagation across multiple documents using graph neural networks; \u2022 we provide an efficient training technique which relies on a slower offline and a faster on-line computation that does not require expensive document processing; \u2022 we empirically show that our algorithm is effective, presenting an improvement over previous results.", "labels": [], "entities": [{"text": "multi-hop QA", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.5484214127063751}]}], "datasetContent": [{"text": "Data The WIKIHOP dataset comprises of tuples q, Sq , C q , a where: q is a query/question, Sq is a set of supporting documents, C q is a set of candidate answers (all of which are entities mentioned in Sq ), and a \u2208 C q is the entity that correctly answers the question.", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.9527587592601776}]}, {"text": "WIKIHOP is assembled assuming that there exists a corpus and a knowledge base (KB) related to each other.", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8674392104148865}]}, {"text": "The KB contains triples s, r, o where sis a subject entity, o an object entity, and r a unidirectional relation between them.", "labels": [], "entities": []}, {"text": "used WIKIPEDIA as corpus and WIKIDATA as KB.", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 5, "end_pos": 14, "type": "DATASET", "confidence": 0.9345033764839172}, {"text": "WIKIDATA", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.924800455570221}]}, {"text": "The KB is only used for constructing WIKIHOP: retrieved the supporting documents Sq from the corpus looking at mentions of subject and object entities in the text.", "labels": [], "entities": [{"text": "WIKIHOP", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7577855587005615}]}, {"text": "Note that the set Sq (not the KB) is provided to the QA system, and not all of the supporting documents are relevant for the query but some of them act as distractors.", "labels": [], "entities": []}, {"text": "Queries, on the other hand, are not expressed in natural language, but instead consist of tuples s, r, ? where the object entity is unknown and it has to be inferred by reading the support documents.", "labels": [], "entities": []}, {"text": "Therefore, answering a query corresponds to finding the entity a that is the object of a tuple in the KB with subject sand relation r among the provided set of candidate answers C q . Task The goal is to learn a model that can identify the correct answer a from the set of supporting documents Sq . To that end, we exploit the available supervision to train a neural network that computes scores for candidates in C q . We estimate the parameters of the architecture by maximizing the likelihood of observations.", "labels": [], "entities": []}, {"text": "For prediction, we then output the candidate that achieves the highest probability.", "labels": [], "entities": [{"text": "prediction", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9713727235794067}]}, {"text": "In the following, we present our model discussing the design decisions that enable multi-step reasoning and an efficient computation.", "labels": [], "entities": []}, {"text": "In this section, we compare our method against recent work as well as preforming an ablation study using the WIKIHOP dataset ( standard (unmasked) one and a masked one.", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.9569867551326752}]}, {"text": "The masked version was created by the authors to test whether methods are able to learn lexical abstraction.", "labels": [], "entities": []}, {"text": "In this version, all candidates and all mentions of them in the support documents are replaced by random but consistent placeholder tokens.", "labels": [], "entities": []}, {"text": "Thus, in the masked version, mentions are always referred to via unambiguous surface forms.", "labels": [], "entities": []}, {"text": "We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WIKIHOP dataset statistics from Welbl et al.  (2018): number of candidates and documents per sam- ple and document length.", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9164225459098816}]}, {"text": " Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN  outperforms recent prior work without learning any language model to process the input but relying on a pre- trained one (ELMo -without fine-tunning it) and applying R-GCN to reason among entities in the text. * with  coreference for unmasked dataset and without coreference for the masked one.", "labels": [], "entities": [{"text": "WIKIHOP closed test set", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.9121627956628799}]}, {"text": " Table 3: Ablation study on WIKIHOP validation set.  The full model is our Entity-GCN with all of its com- ponents and other rows indicate models trained without  a component of interest. We also report baselines using  GloVe instead of ELMo with and without R-GCN. For  the full model we report mean \u00b11 std over 5 runs.", "labels": [], "entities": [{"text": "WIKIHOP validation set", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8989077210426331}, {"text": "mean \u00b11 std", "start_pos": 296, "end_pos": 307, "type": "METRIC", "confidence": 0.9432321339845657}]}, {"text": " Table 4: Accuracy and precision at K (P@K in the table) analysis overall and per query type. Avg. |C q | indicates  the average number of candidates with one standard deviation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987963438034058}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9990479350090027}, {"text": "Avg", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9971501231193542}]}]}