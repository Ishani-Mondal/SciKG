{"title": [{"text": "The Emergence of Number and Syntax Units in LSTM Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement.", "labels": [], "entities": []}, {"text": "We have however no mechanistic understanding of how they accomplish this remarkable feat.", "labels": [], "entities": []}, {"text": "Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account.", "labels": [], "entities": []}, {"text": "We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level.", "labels": [], "entities": [{"text": "number tracking", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7136033475399017}]}, {"text": "We discover that long-distance number information is largely managed by two \"number units\".", "labels": [], "entities": []}, {"text": "Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure.", "labels": [], "entities": []}, {"text": "We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last years, recurrent neural networks (RNNs), and particularly long-short-term-memory (LSTM) architectures (Hochreiter and Schmidhuber, 1997), have been successfully applied to a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "This has spurred interest in whether these generic sequence-processing devices are discovering genuine structural properties of language in their training data, or whether their success can be explained by opportunistic surfacepattern-based heuristics.", "labels": [], "entities": []}, {"text": "Until now, this debate has mostly relied on \"behavioural\" evidence: The LSTM had been treated as a black box, and its capacities had been indirectly inferred by its performance on linguistic tasks.", "labels": [], "entities": []}, {"text": "In this study, we took a complementary approach inspired by neuroscience: We thoroughly investigated the inner dynamics of an LSTM language model performing a number agreement task, striving to achieve a mechanistic understanding of how it accomplishes it.", "labels": [], "entities": []}, {"text": "We found that the LSTM had specialized two \"grandmother\" cells to carry number features from the subject to the verb across the intervening material.", "labels": [], "entities": []}, {"text": "Interestingly, the LSTM also possesses a more distributed mechanism to predict number when subject and verb are close, with the grandmother number cells only playing a crucial role in more difficult long-distance cases.", "labels": [], "entities": []}, {"text": "Crucially, we independently identified a set of cells tracking syntactic structure, and found that one of them encodes the presence of an embedded phrase separating the main subject-verb dependency, and has strong efferent connections to the long-distance number cells, suggesting that the network relies on genuine syntactic information to regulate agreement-feature percolation.", "labels": [], "entities": []}, {"text": "Our analysis thus provides direct evidence for the claim that LSTMs trained on unannotated corpus data, despite lacking significant linguistic priors, learn to perform structure-dependent linguistic operations.", "labels": [], "entities": []}, {"text": "In turn, this suggests that raw linguistic input and generic memory mechanisms, such as those implemented in LSTMs, may suffice to trigger the induction of non-trivial grammatical rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "To successfully perform the NA-task, the LSTM should: (1) encode and store the grammatical number of the subject; and (2) track the main subject-verb syntactic dependency.", "labels": [], "entities": []}, {"text": "The latter information is important for identifying the time period during which subject number should be stored, output and then updated by the network.", "labels": [], "entities": []}, {"text": "This section describes the 'neural circuit' that encodes and processes this information in the LSTM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ablation-experiments results: Percentage ac- curacy in all NA-tasks. Full: non-ablated model, C:  condition, S: singular, P: plural. Pink (dark lines in  B&W printing): plural subject, Light blue: singular  subject. Performance reduction less than 10% is de- noted by '-'.", "labels": [], "entities": [{"text": "Ablation-experiments", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9971446394920349}, {"text": "Percentage ac- curacy", "start_pos": 40, "end_pos": 61, "type": "METRIC", "confidence": 0.8649996966123581}, {"text": "Performance reduction", "start_pos": 226, "end_pos": 247, "type": "METRIC", "confidence": 0.9800185859203339}]}]}