{"title": [{"text": "Neural Text Simplification in Low-Resource Conditions Using Weak Supervision", "labels": [], "entities": [{"text": "Neural Text Simplification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8865439494450887}, {"text": "Weak Supervision", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6970410197973251}]}], "abstractContent": [{"text": "Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-to-sequence learning.", "labels": [], "entities": [{"text": "Neural text simplification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8818699518839518}]}, {"text": "Most recent efforts with such a data-demanding paradigm have dealt with the English language, for which sizeable training datasets are currently available to deploy competitive models.", "labels": [], "entities": []}, {"text": "Similar improvements on less resource-rich languages are conditioned either to intensive manual work to create training data, or to the design of effective automatic generation techniques to bypass the data acquisition bottleneck.", "labels": [], "entities": []}, {"text": "Inspired by the machine translation field, in which synthetic parallel pairs generated from monolin-gual data yield significant improvements to neural models, in this paper we exploit large amounts of heterogeneous data to automatically select simple sentences, which are then used to create synthetic simplification pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7714226841926575}]}, {"text": "We also evaluate other solutions, such as over-sampling and the use of external word em-beddings to be fed to the neural simplification system.", "labels": [], "entities": []}, {"text": "Our approach is evaluated on Italian and Spanish, for which few thousand gold sentence pairs are available.", "labels": [], "entities": []}, {"text": "The results show that these techniques yield performance improvements over a baseline sequence-to-sequence configuration.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text simplification aims at making a text more readable by reducing its lexical and structural complexity while preserving the meaning..", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7939888536930084}]}, {"text": "Neural approaches to the task have gained increasing attention in the NLP community thanks to recent advancements of deep, sequence-to-sequence approaches.", "labels": [], "entities": []}, {"text": "However, all recent improvements have dealt with English.", "labels": [], "entities": []}, {"text": "The main reason is that such data-hungry approaches require large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language.", "labels": [], "entities": []}, {"text": "Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela () and the aligned version of simple and standard English Wikipedia (.", "labels": [], "entities": [{"text": "Newsela", "start_pos": 168, "end_pos": 175, "type": "DATASET", "confidence": 0.9707311391830444}, {"text": "English Wikipedia", "start_pos": 226, "end_pos": 243, "type": "DATASET", "confidence": 0.8869945108890533}]}, {"text": "These data have become the common benchmark for evaluating new approaches to neural text simplification.", "labels": [], "entities": [{"text": "neural text simplification", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6392893095811208}]}, {"text": "These methods rely on the use of deep reinforcement learning (, memory-augmented neural networks (, the combination of semantic parsing and neural approaches) and the personalisation to specific grade levels.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.754746288061142}]}, {"text": "Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (.", "labels": [], "entities": []}, {"text": "The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time anew language (or anew domain with specific idiosyncrasies) has to be covered.", "labels": [], "entities": []}, {"text": "To alleviate the data bottleneck issue, enabling the development of neural solutions also for languages other than English, we explore data augmentation techniques for creating task-specific training data.", "labels": [], "entities": []}, {"text": "Our experiments range from simple oversampling techniques to weakly supervised data augmentation methods inspired by recent works in other NLP tasks, in particular Machine Translation (MT) ().", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.8489957571029663}]}, {"text": "Ina nutshell, taking an opposite direction to simplification, we proceed by i) automatically selecting simple sentences from a large pool of monolingual data, and ii) synthetically creating complex sentences.", "labels": [], "entities": []}, {"text": "These artificially created sentences will be then used as the \"source\" side of new difficultsimple training pairs fed into an MT-like encoderdecoder architecture.", "labels": [], "entities": []}, {"text": "Our hypothesis is that, though sub-optimal due to possible errors introduced in the automatic generation of complex sentences, these training pairs represent useful material for building our sequence-to-sequence text simplification models.", "labels": [], "entities": []}, {"text": "Under this hypothesis, any noise in the source side of the pairs can still be treated as an approximation of text difficulty that, paired with its correct simplified counterpart, can contribute to model training.", "labels": [], "entities": []}, {"text": "We run our experiments on Italian and Spanish, two languages for which only small datasets of manually curated simplifications are available.", "labels": [], "entities": []}, {"text": "The main contributions of this work are: \u2022 We explore different approaches for augmenting training data for neural text simplification using weak supervision; \u2022 We test them in under-resourced conditions on Italian and Spanish.", "labels": [], "entities": [{"text": "neural text simplification", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.6423067450523376}]}], "datasetContent": [{"text": "We run our experiments on two languages, Italian and Spanish.", "labels": [], "entities": []}, {"text": "Below, we describe for each language the gold standard and the simple monolingual data extraction process to augment our training data.", "labels": [], "entities": []}, {"text": "We report in the results of Italian and Spanish text simplification using different settings and data augmentation techniques.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6802752166986465}]}, {"text": "For each language, we evaluate the results using the gold training set as is, and expanding it through oversampling (i.e. repetition of the same sentence pairs 5 and 10 times).", "labels": [], "entities": []}, {"text": "In addition, we evaluate the impact of: i) adding pre-trained word embeddings built on large monolingual corpora (Pretr.Emb), ii) using the simple-to-simple pairs for data augmentation (Copied), and iii) using, for the same purpose, the simple-to-complex synthetic pairs (Complic.).", "labels": [], "entities": []}, {"text": "We also explore the addition of different combinations of the aforementioned resources.", "labels": [], "entities": []}, {"text": "The evaluation is performed by computing the SARI score () on the test set.", "labels": [], "entities": [{"text": "SARI score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9565620720386505}]}, {"text": "Our results show that adding only pre-trained word embeddings trained on large monolingual corpora achieves, in general, better performance than the baseline (max: +2.73 SARI points for Italian, +0.8 for Spanish).", "labels": [], "entities": [{"text": "SARI", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9970163106918335}]}, {"text": "Our experiments show also that the usefulness of simple-to-simple pairs cannot be generalised: they are beneficial for all results on Italian and SPAx10, while they are harmful for SPAx1 and SPAx5.", "labels": [], "entities": [{"text": "Italian", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9342963695526123}, {"text": "SPAx10", "start_pos": 146, "end_pos": 152, "type": "DATASET", "confidence": 0.5044381022453308}, {"text": "SPAx5", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.8770133256912231}]}, {"text": "Our intuition is that the copied data pushed the system in the direction of learning to copy the source sentence in the output instead of simplifying it, which can create some instability in the model during training.", "labels": [], "entities": []}, {"text": "The addition of simple-to-simple pairs and of pretrained word embeddings does not yield large improvements, confirming the idea that the copied pairs mainly affect the quality of the word embedding representations instead of the relation between complex and simple sentences (i.e. attention network).", "labels": [], "entities": []}, {"text": "The largest gains in performance are obtained when using the simple-to-complex synthetic pairs.", "labels": [], "entities": []}, {"text": "Both in isolation and when paired with pre-trained embeddings, they make the neural model able to outperform the baseline up to +3.4 SARI points.", "labels": [], "entities": [{"text": "SARI", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9729713797569275}]}, {"text": "The best results for both languages are obtained by multiplying the training data by 10 and adding the simple-to-complex synthetic data.", "labels": [], "entities": []}, {"text": "These configurations outperform the standard settings (ITAx1 and SPAx1) by +5.4 SARI points for Italian and and +2.4 for Spanish.", "labels": [], "entities": [{"text": "ITAx1", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8424083590507507}, {"text": "SARI", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9978805780410767}]}, {"text": "When concatenating all the synthetic and real data, and the pre-trained embeddings are used, the performance is comparable with the one obtained using the simple-to-complex synthetic pairs, but at the cost of using a larger quantity of training data.", "labels": [], "entities": []}, {"text": "Although we cannot make a direct comparison of the SARI scores across different languages, Italian and Spanish are typologically very similar, and therefore we can argue that our models for neural simplification in Italian works better than the Spanish ones.", "labels": [], "entities": [{"text": "SARI", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.8544034361839294}]}, {"text": "This may depend on several reasons.", "labels": [], "entities": []}, {"text": "For Italian, the selection of 500, 000 simple sentences is based on sentence-specific features correlated with high readability, emerged from the analysis in).", "labels": [], "entities": []}, {"text": "On the contrary, extracting simple monolingual sentences based on the readability score at document level, as we did for Spanish, is more prone to inconsistencies.", "labels": [], "entities": []}, {"text": "Other differences maybe due to the quality of gold standard data: although the Spanish gold standard is bigger than the Italian one (55, 890 complex-simple sentence pairs vs. 32, 210 pairs respectively), its language is generally more complex, since it contains news articles, while the Italian gold standard includes to a large extent stories for children and textbooks.", "labels": [], "entities": [{"text": "Spanish gold standard", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.7553002536296844}, {"text": "Italian gold standard", "start_pos": 287, "end_pos": 308, "type": "DATASET", "confidence": 0.7909290194511414}]}, {"text": "Besides, while some of the Italian sentences were manually aligned, the Spanish gold data were obtained by automatically extracting complex-to-simple pairs from the Newsela corpus, in which the alignment had been done at document level.", "labels": [], "entities": [{"text": "Spanish gold data", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.7760777473449707}, {"text": "Newsela corpus", "start_pos": 165, "end_pos": 179, "type": "DATASET", "confidence": 0.9753788113594055}]}, {"text": "As a comparison, we evaluate on the same test set also the MUSST syntactic simplifier), a freely available system implementing a set of simplification rules for Italian and Spanish.", "labels": [], "entities": []}, {"text": "We obtain 20.16 SARI for Italian and 21.24 for Spanish.", "labels": [], "entities": [{"text": "SARI", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9987398982048035}]}, {"text": "Our results show that, despite some issues described before, low-resource neural simplification is still a promising research direction to pursue, especially with data augmentation.", "labels": [], "entities": [{"text": "low-resource neural simplification", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.6407266557216644}]}, {"text": "This is particularly true for Spanish MUSST, which includes a richer set of rules than the Italian version, but that achieves nevertheless -9.56 SARI points than the best neural model for Spanish.", "labels": [], "entities": [{"text": "Spanish MUSST", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.6750331223011017}, {"text": "SARI", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9962892532348633}]}], "tableCaptions": [{"text": " Table 1: Results of neural simplification experiments on Italian and Spanish data (SARI)", "labels": [], "entities": [{"text": "neural simplification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.6857765316963196}, {"text": "Italian and Spanish data (SARI)", "start_pos": 58, "end_pos": 89, "type": "DATASET", "confidence": 0.6762829210077014}]}]}