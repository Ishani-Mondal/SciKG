{"title": [{"text": "Opinion Mining with Deep Contextualized Embeddings", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7875581383705139}]}], "abstractContent": [{"text": "Detecting opinion expression is a potential and essential task in opinion mining that can be extended to advanced tasks.", "labels": [], "entities": [{"text": "Detecting opinion expression", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9146751761436462}, {"text": "opinion mining", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.858616054058075}]}, {"text": "In this paper, we considered opinion expression detection as a sequence labeling task and exploited different deep contextualized embedders into the state-of-the-art architecture, composed of bidirec-tional long short-term memory (BiLSTM) and conditional random field (CRF).", "labels": [], "entities": [{"text": "opinion expression detection", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8426542083422343}, {"text": "sequence labeling task", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7736974557240804}]}, {"text": "Our experimental results show that using different word embeddings can cause contrasting results, and the model can achieve remarkable scores with deep contextualized embeddings.", "labels": [], "entities": []}, {"text": "Especially, using BERT embedder can significantly exceed using ELMo embedder.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9740756750106812}]}], "introductionContent": [{"text": "One of the crucial tasks in sentiment analysis field is opinion mining, which right now becomes more and more popular for survey.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9711038768291473}, {"text": "opinion mining", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.866915225982666}]}, {"text": "The purpose of opinion mining is to detect the emotional expression from a sentence or an entire document.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8492995798587799}]}, {"text": "To be more specific, those expressions usually contain human beings' emotions, interests, even attitudes via natural language.", "labels": [], "entities": []}, {"text": "Fine-grained opinion mining is not only fundamental but also important because bountiful Natural Language Processing (NLP) applications can benefit from it.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7127912044525146}]}, {"text": "For example, detecting opinion expression can be extended to identify opinion entity, such as;;, recognize stance, and extract aspect (.", "labels": [], "entities": [{"text": "detecting opinion expression", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.8692369858423868}]}, {"text": "Opinion expression detection can be viewed as a linguistic sequence labeling problem.", "labels": [], "entities": [{"text": "Opinion expression detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.892234186331431}, {"text": "linguistic sequence labeling", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6423399349053701}]}, {"text": "Therefore, recognizing opinionated span from a sentence can be designed as a token-level sequence tagging problem.", "labels": [], "entities": [{"text": "token-level sequence tagging", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.5993516147136688}]}, {"text": "In this case, standard BIO encoding is usually applied in the same way in \u02d9 Irsoy  and Cardie (2014);.", "labels": [], "entities": [{"text": "BIO encoding", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.8489386439323425}]}, {"text": "Thus, we used the dataset tagged with B, I, and O characters, which represent the beginning, inside, and outside respectively.", "labels": [], "entities": []}, {"text": "The first token in each opinionated span is attached to B character, and then the rest of the span are assigned to I character. is an example of BIO scheme.", "labels": [], "entities": []}, {"text": "Out of exactness, in this study, we chose the dataset MPQA 1.2 used in Xie (2017); \u02d9.", "labels": [], "entities": [{"text": "exactness", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9641348719596863}, {"text": "MPQA 1.2", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8961523175239563}]}, {"text": "To estimate the generalization of our model, we also took another opinion-oriented dataset MOAT from the organization NTCIR7 (.", "labels": [], "entities": [{"text": "opinion-oriented dataset MOAT from the organization NTCIR7", "start_pos": 66, "end_pos": 124, "type": "DATASET", "confidence": 0.7470173410006932}]}, {"text": "Opinion expression usually contains a speaker's emotion; hence, we assume that semantics contributes more than syntax.", "labels": [], "entities": []}, {"text": "Even though pretrained word embedding can improve the performance, it still doesn't fully utilize word meaning and its context.", "labels": [], "entities": []}, {"text": "Therefore, generating word representations based on the contexts is critical.", "labels": [], "entities": []}, {"text": "Owing to deep neural network, model can produce dynamic word representation, such as;;, on the contrary to fixed representation.", "labels": [], "entities": []}, {"text": "In this paper, we applied two state-of-the-art and innovative models,) and BERT, to produce word representations and compared the performances.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9993900060653687}]}, {"text": "After obtaining the contextualized word representations, we fed them into a robust neural network.", "labels": [], "entities": []}], "datasetContent": [{"text": "Count DSE 14492 ESE 14492 NTCIR7-MOAT 3376: The number of sentences for each dataset.", "labels": [], "entities": [{"text": "DSE 14492 ESE 14492 NTCIR7-MOAT 3376", "start_pos": 6, "end_pos": 42, "type": "DATASET", "confidence": 0.7057231465975443}]}, {"text": "In the evaluation, we calculated precision, recall, and F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9998102784156799}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9996954202651978}, {"text": "F1-score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993232488632202}]}, {"text": "However, in order to evaluate our model comprehensively, we measured our model not only by token basis but also span basis.", "labels": [], "entities": []}, {"text": "This        is because it is difficult to define the boundaries of expressions, even for human annotators.", "labels": [], "entities": []}, {"text": "To put it another way, for token-based evaluation, F1-score pays attention to whether the individual tag is predicted correctly or not.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9989826083183289}]}, {"text": "In contrast, spanbased evaluation cares about the count of overlap; hence, we used binary overlap along with proportional overlap.", "labels": [], "entities": []}, {"text": "Binary overlap computes the number of matching overlaps between predicted sequence and ground-truth sequence.", "labels": [], "entities": []}, {"text": "As long as predicted span overlaps ground-truth span, binary overlap views it as a correct prediction.", "labels": [], "entities": []}, {"text": "To refine the evaluation, proportional overlap considers the length of overlap and imparts a partial correctness to each match, which is able to assess the model more accurate.", "labels": [], "entities": []}, {"text": "We used these three evaluations to measure our models., 5, and 6 display the results of the data sets individually.", "labels": [], "entities": []}, {"text": "The scores show that using BERT embedder does achieve better performance than using ELMo embedder by dramatic difference, which implies that BERT is promising in opinion mining tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9302013516426086}, {"text": "BERT", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9956861734390259}, {"text": "opinion mining tasks", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.8440624872843424}]}, {"text": "More interestingly, adding attention mechanism on BiLSTM can increase the performance slightly.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.797089695930481}]}], "tableCaptions": [{"text": " Table 3: The results of applying a variety of embeddings on DSEs. ELMo embedder outperforms others signifi- cantly. Besides, stacking a CRF layer as decoder can increase the performance slightly.", "labels": [], "entities": []}, {"text": " Table 4: Experimental evaluation of models for DSE.", "labels": [], "entities": [{"text": "DSE", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.691293478012085}]}, {"text": " Table 5: Experimental evaluation of models for ESE.", "labels": [], "entities": [{"text": "ESE", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7830809354782104}]}, {"text": " Table 6: Experimental evaluation of models for NTCIR-MOAT7. * means the same scores.", "labels": [], "entities": [{"text": "NTCIR-MOAT7", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.6367194652557373}]}]}