{"title": [{"text": "Pre-trained Language Model Representations for Language Generation", "labels": [], "entities": [{"text": "Language Model Representations", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.6539514462153116}]}], "abstractContent": [{"text": "Pre-trained language model representations have been successful in a wide range of language understanding tasks.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.731738954782486}]}, {"text": "In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summariza-tion.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.6886032621065775}]}, {"text": "We find that pre-trained representations are most effective when added to the en-coder network which slows inference by only 14%.", "labels": [], "entities": []}, {"text": "Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7916393876075745}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9989308714866638}]}, {"text": "While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available.", "labels": [], "entities": []}, {"text": "Finally, on abstractive summa-rization we achieve anew state of the art on the full text version of CNN-DailyMail.", "labels": [], "entities": [{"text": "full text version of CNN-DailyMail", "start_pos": 79, "end_pos": 113, "type": "DATASET", "confidence": 0.7362788915634155}]}], "introductionContent": [{"text": "Pre-training of language models has been shown to provide large improvements fora range of language understanding tasks (.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7027647793292999}]}, {"text": "The key idea is to train a large generative model on vast corpora and use the resulting representations on tasks for which only limited amounts of labeled data is available.", "labels": [], "entities": []}, {"text": "Pre-training of sequence to sequence models has been previously investigated for text classification but not for text generation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.814984917640686}, {"text": "text generation", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.7928942441940308}]}, {"text": "In neural machine translation, there has been work on transferring representations from high-resource language pairs to low-resource settings (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6561722854773203}]}, {"text": "In this paper, we apply pre-trained representations from language models to language genera-tion tasks that can be modeled by sequence to sequence architectures.", "labels": [], "entities": []}, {"text": "Previous work on integrating language models with sequence to sequence models focused on the decoder network and added language model representations right before the output of the decoder ().", "labels": [], "entities": []}, {"text": "We extend their study by investigating several other strategies such as inputting ELMo-style representations ( or fine-tuning the language model ( \u00a72).", "labels": [], "entities": [{"text": "inputting ELMo-style representations", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.7590499917666117}]}, {"text": "Our experiments rely on strong transformerbased language models trained on up to six billion tokens ( \u00a73).", "labels": [], "entities": []}, {"text": "We present a detailed study of various strategies in different simulated labeled training data scenarios and observe the largest improvements in low-resource settings but gains of over 1 BLEU are still possible when five million sentence-pairs are available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.998668909072876}]}, {"text": "The most successful strategy to integrate pre-trained representations is as input to the encoder network ( \u00a74).", "labels": [], "entities": []}], "datasetContent": [{"text": "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens.", "labels": [], "entities": [{"text": "German newscrawl distributed by WMT'18", "start_pos": 73, "end_pos": 111, "type": "DATASET", "confidence": 0.8496894478797913}]}, {"text": "Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens.", "labels": [], "entities": [{"text": "English newscrawl data", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8080514073371887}]}, {"text": "We learn a joint Byte-Pair-Encoding (BPE; vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary.", "labels": [], "entities": [{"text": "German and English newscrawl", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.6535760685801506}]}, {"text": "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 EnglishTurkish (en-tr) news task.", "labels": [], "entities": [{"text": "WMT'18 English-German (en-de) news translation task", "start_pos": 60, "end_pos": 111, "type": "TASK", "confidence": 0.8170841410756111}, {"text": "WMT'18 EnglishTurkish (en-tr) news task", "start_pos": 148, "end_pos": 187, "type": "DATASET", "confidence": 0.910466628415244}]}, {"text": "For WMT'18 EnglishGerman, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5.", "labels": [], "entities": [{"text": "WMT'18 EnglishGerman", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9104442894458771}, {"text": "ParaCrawl corpus", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.8855738043785095}]}, {"text": "This results in 5.18M sentence pairs.", "labels": [], "entities": []}, {"text": "We tokenize all data with the Moses tokenizer ( and apply the BPE vocabulary learned on the monolingual corpora.", "labels": [], "entities": [{"text": "BPE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8133413195610046}]}, {"text": "For WMT'18 English-Turkish, we use all of the available bitext comprising 208K sentence-pairs without any filtering.", "labels": [], "entities": [{"text": "WMT'18 English-Turkish", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9122790992259979}]}, {"text": "We develop on newstest2017 and test on newstest2018.", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.9555862545967102}, {"text": "newstest2018", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.967029869556427}]}, {"text": "For en-tr we only experiment with adding representations to the encoder and therefore apply the language model vocabulary to the source side.", "labels": [], "entities": []}, {"text": "For the target vocabulary we learn a BPE code with 32K merge operations on the Turkish side of the bitext.", "labels": [], "entities": [{"text": "BPE code", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.679993212223053}]}, {"text": "Both datasets are evaluated in terms of case-sensitive de-tokenized BLEU (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9898276329040527}]}, {"text": "We consider the CNNDailyMail abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries.", "labels": [], "entities": [{"text": "CNNDailyMail abstractive document summarization", "start_pos": 16, "end_pos": 63, "type": "TASK", "confidence": 0.69975546002388}]}, {"text": "CNN-DailyMail is a widely used dataset for abstractive text summarization., we report results on the non-anonymized version of CNNDailyMail rather than the entity-anonymized version () because the language model was trained on full text.", "labels": [], "entities": [{"text": "CNN-DailyMail", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9608361721038818}, {"text": "abstractive text summarization.", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6982875963052114}, {"text": "CNNDailyMail", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.9346031546592712}]}, {"text": "Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types).", "labels": [], "entities": [{"text": "BPE vocabulary", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.6890931725502014}]}, {"text": "We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).", "labels": [], "entities": [{"text": "F1-Rouge", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9784925580024719}]}], "tableCaptions": [{"text": " Table 1: BLEU on newstest2018 of WMT English- German in three simulated bitext size scenarios.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991857409477234}, {"text": "WMT English- German", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.902091383934021}]}, {"text": " Table 2: WMT English-Turkish translation results in  terms of BLEU on newstest2017 (valid) and new- stest2018 (test) with ELMo inputs to the encoder.", "labels": [], "entities": [{"text": "WMT English-Turkish translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6717057824134827}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9990992546081543}, {"text": "newstest2017", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9373621940612793}]}, {"text": " Table 3: Abstractive summarization results on CNN- DailyMail. ELMo inputs achieve a new state of the art.", "labels": [], "entities": [{"text": "Abstractive", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9848461151123047}, {"text": "CNN- DailyMail", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9391858379046122}]}]}