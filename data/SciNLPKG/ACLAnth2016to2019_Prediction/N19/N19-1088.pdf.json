{"title": [], "abstractContent": [{"text": "In this paper, we present a method for ad-versarial decomposition of text representation.", "labels": [], "entities": [{"text": "text representation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7008374333381653}]}, {"text": "This method can be used to decompose a representation of an input sentence into several independent vectors, each of them responsible fora specific aspect of the input sentence.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method on two case studies: the conversion between different social registers and diachronic language change.", "labels": [], "entities": []}, {"text": "We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence.", "labels": [], "entities": []}, {"text": "It is also learning a continuous (rather than categorical) representation of the style of the sentence, which is more linguistically realistic.", "labels": [], "entities": []}, {"text": "The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition.", "labels": [], "entities": []}, {"text": "Furthermore, we evaluate the obtained meaning embeddings on a downstream task of paraphrase detection and show that they significantly outperform the embed-dings of a regular autoencoder.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.9021366536617279}]}], "introductionContent": [{"text": "Despite the recent successes in using neural models for representation learning for natural language text, learning a meaningful representation of input sentences remains an open research problem.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.9136033058166504}]}, {"text": "A variety of approaches, from sequence-to-sequence models that followed the work of to the more recent proposals ( share one common drawback.", "labels": [], "entities": []}, {"text": "Namely, all of them encode the input sentence into just one single vector of a fixed size.", "labels": [], "entities": []}, {"text": "One way to bypass the limitations of a single vector representation is to use an attention mechanism (.", "labels": [], "entities": []}, {"text": "We propose to approach this problem differently and design a method for adversarial decomposition of the learned input representation into multiple components.", "labels": [], "entities": []}, {"text": "Our method encodes the input sentence into several vectors, where each vector is responsible fora specific aspect of the sentence.", "labels": [], "entities": []}, {"text": "In terms of learning different separable components of input representation, our work most closely relates to the style transfer work, which has been applied to a variety of different aspects of language, from diachronic language differences () to authors' personalities () and even sentiment ().", "labels": [], "entities": []}, {"text": "The style transfer work effectively relies on the more classical distinction between meaning and form (, which accounts for the fact that multiple surface realizations are possible for the same meaning.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.713673785328865}]}, {"text": "For simplicity, we will use this terminology throughout the rest of the paper.", "labels": [], "entities": []}, {"text": "Consider encoding an input sentence into a meaning vector and a form vector.", "labels": [], "entities": []}, {"text": "This enables a controllable change of meaning or form by a simple change applied to these vectors.", "labels": [], "entities": []}, {"text": "For example, we can encode two sentences written in two different styles, then swap the form vectors while leaving the meaning vectors intact.", "labels": [], "entities": []}, {"text": "We can then generate new unique sentences with the original meaning, but written in a different style.", "labels": [], "entities": []}, {"text": "We propose a novel model for this type of decomposition based on adversarial-motivational training, GAN architecture () and adversarial autoencoders (.", "labels": [], "entities": []}, {"text": "In addition to the adversarial loss, we use a special motivator (, which, in contrast to the discriminator, is used to provide a motivational loss to encourage better decomposition of the meaning and the form.", "labels": [], "entities": []}, {"text": "All the code is available on GitHub . We evaluate the proposed methods for learning separate aspects of input representation in the following case studies: 1.", "labels": [], "entities": []}, {"text": "Specifically, we consider the Early Modern English (e.g. What would she have?) and the contemporary English ( What does she want?).", "labels": [], "entities": []}, {"text": "2. Social register (, i.e. subsets of language appropriate in a given context or characteristic of a certain group of speakers.", "labels": [], "entities": []}, {"text": "Social registers include formal vs informal language, the language used in different genres (e.g., fiction vs. newspapers vs. academic texts), different dialects, and literary idiostyles.", "labels": [], "entities": []}, {"text": "We experiment with the titles of scientific papers vs. newspaper articles.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similarly to the evaluation of style transfer in CV ( ), evaluation of this task is difficult.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7680107653141022}]}, {"text": "We follow the approach of ; and recently proposed by methods of evaluation of \"transfer strength\" and \"content preservation\".", "labels": [], "entities": [{"text": "content preservation", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7718349397182465}]}, {"text": "The authors showed that the proposed automatic metrics correlate with human judgment to a large degree and can serve as a proxy.", "labels": [], "entities": []}, {"text": "Below we give an overview of these metrics.", "labels": [], "entities": []}, {"text": "The goal of this metric is to capture whether the form has been changed successfully.", "labels": [], "entities": []}, {"text": "To do that, a classifier C is trained on the two corpora, X a and X b to recognize the linguistic \"form\" typical of each of them.", "labels": [], "entities": []}, {"text": "After that a sentence, for which the form/meaning has been changed, is passed to the classifier.", "labels": [], "entities": []}, {"text": "The overall accuracy reflects the degree of success of changing the form/meaning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992340803146362}]}, {"text": "This approach is widely used in CV ( , and was applied in NLP as well).", "labels": [], "entities": []}, {"text": "In our experiments we used a GRU unit followed by four fully-connected layers with ELU activation functions between them as the classifier.", "labels": [], "entities": []}, {"text": "Content preservation Note that the transfer strength by itself does not capture the overall quality of a changed sentence.", "labels": [], "entities": [{"text": "Content preservation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7227084189653397}]}, {"text": "A extremely overfitted model that produces the most characteristic sentence of one corpus all the time would have a high score according to this metric.", "labels": [], "entities": []}, {"text": "Thus, we need to measure how much of the meaning was preserved while changing the form.", "labels": [], "entities": []}, {"text": "To do that, proposed to use a cosine similarity based metric using pretrained word embeddings.", "labels": [], "entities": []}, {"text": "First, a sentence embedding is computed by concatenation of max, mean, and average pooling over the timesteps: Next, the cosine similarity score s i between the embedding v s i of the original source sentence and the target sentence with the changed form v ti is computed, and the scores across the dataset are averaged to obtain the total score s.", "labels": [], "entities": [{"text": "cosine similarity score s", "start_pos": 121, "end_pos": 146, "type": "METRIC", "confidence": 0.7399209961295128}]}, {"text": "We evaluated the proposed method on several datasets that reflect different changes of meaning and form.", "labels": [], "entities": []}, {"text": "This experiment is conducted with a dataset of titles of scientific papers and news articles published by.", "labels": [], "entities": []}, {"text": "This dataset (referred to as \"Headlines\") contains titles of scientific articles crawled from online digital libraries, such as \"ACM Digital Library\" and \"arXiv\".", "labels": [], "entities": [{"text": "ACM Digital Library", "start_pos": 129, "end_pos": 148, "type": "DATASET", "confidence": 0.9633944829305013}]}], "tableCaptions": [{"text": " Table 3: F1 scores on the task of paraphrase detection  using the SentEval toolkit (", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991088509559631}, {"text": "paraphrase detection", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.970618486404419}]}]}