{"title": [{"text": "Better, Faster, Stronger Sequence Tagging Constituent Parsers", "labels": [], "entities": [{"text": "Sequence Tagging Constituent Parsers", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.9061049073934555}]}], "abstractContent": [{"text": "Sequence tagging models for constituent parsing are faster, but less accurate than other types of parsers.", "labels": [], "entities": [{"text": "Sequence tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8967706561088562}, {"text": "constituent parsing", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6266890168190002}]}, {"text": "In this work, we address the following weaknesses of such constituent parsers: (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.7130707204341888}]}, {"text": "To effectively close brackets, we train a model that learns to switch between tagging schemes.", "labels": [], "entities": []}, {"text": "To reduce sparsity, we decompose the label set and use multi-task learning to jointly learn to predict sublabels.", "labels": [], "entities": []}, {"text": "Finally, we mitigate issues from greedy decoding through auxiliary losses and sentence-level fine-tuning with policy gradient.", "labels": [], "entities": []}, {"text": "Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chi-nese Penn Treebanks, and reduce their parsing time even further.", "labels": [], "entities": [{"text": "sequence tagging constituent parsers", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.8074222803115845}, {"text": "English and Chi-nese Penn Treebanks", "start_pos": 110, "end_pos": 145, "type": "DATASET", "confidence": 0.9007641911506653}]}, {"text": "On the SPMRL datasets, we observe even greater improvements across the board, including anew state of the art on Basque, Hebrew, Polish and Swedish.", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7512475550174713}]}], "introductionContent": [{"text": "Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications.", "labels": [], "entities": [{"text": "Constituent parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8986155390739441}, {"text": "natural language processing (NLP)", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.7820315460364023}]}, {"text": "Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments ().", "labels": [], "entities": []}, {"text": "Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce ( or chart-based parsers)., for example, presented a fast shift-reduce parser with transitions learned by a SVM classifier.", "labels": [], "entities": []}, {"text": "Similarly, introduced a fast GPU implementation for, and significantly improved the speed of the greedy top-down algorithm, by learning to predict a list of syntactic distances that determine the order in which the sentence should be split.", "labels": [], "entities": []}, {"text": "In an alternative line of work, some authors have proposed new parsing paradigms that aim to both reduce the complexity of existing parsers and improve their speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9658182859420776}, {"text": "speed", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.960928738117218}]}, {"text": "proposed a machine translation-inspired sequence-tosequence approach to constituent parsing, where the input is the raw sentence, and the 'translation' is a parenthesized version of its tree.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7034432888031006}]}, {"text": "reduced constituent parsing to sequence tagging, where only n tagging actions need to be made, and obtained one of the fastest parsers to date.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.5639360547065735}, {"text": "sequence tagging", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6489775031805038}]}, {"text": "However, the performance is well below the state of the art).", "labels": [], "entities": []}, {"text": "Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results.", "labels": [], "entities": [{"text": "sequence tagging constituent parsers", "start_pos": 61, "end_pos": 97, "type": "TASK", "confidence": 0.7959159165620804}]}, {"text": "These include: high error rates when long constituents need to be closed, label sparsity, and error propagation arising from greedy inference.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7418886125087738}]}, {"text": "We then present the technical contributions of the work.", "labels": [], "entities": []}, {"text": "To effectively close brackets of long constituents, we combine the relative-scale tagging scheme used by with a secondary top-down absolute-scale scheme.", "labels": [], "entities": [{"text": "relative-scale tagging", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.6684887707233429}]}, {"text": "This makes it possible to train a model that learns how to switch between two encodings, depending on which one is more suitable at each time step.", "labels": [], "entities": []}, {"text": "To reduce label sparsity, we recast the constituent-parsing-assequence-tagging problem as multi-task learning (MTL), to decompose a large label space and also obtain speed ups.", "labels": [], "entities": []}, {"text": "Finally, we mitigate error propagation using two strategies that come at no cost to inference efficiency: auxiliary tasks and policy gradient fine-tuning.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.722632572054863}]}], "datasetContent": [{"text": "We now review the impact of the proposed techniques on a wide variety of settings.", "labels": [], "entities": []}, {"text": "3 different labels in the MTL setting.", "labels": [], "entities": [{"text": "MTL setting", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.7678805291652679}]}, {"text": "Datasets We use the English Penn Treebank (PTB)) and the Chinese Penn Treebank (CTB) ().", "labels": [], "entities": [{"text": "English Penn Treebank (PTB))", "start_pos": 20, "end_pos": 48, "type": "DATASET", "confidence": 0.8951856195926666}, {"text": "Chinese Penn Treebank (CTB)", "start_pos": 57, "end_pos": 84, "type": "DATASET", "confidence": 0.8882077634334564}]}, {"text": "For these, we use the same predicted PoS tags as.", "labels": [], "entities": []}, {"text": "We also provide detailed results on the SPMRL treebanks (), 5 a set of datasets for constituent parsing on morphologically rich languages.", "labels": [], "entities": [{"text": "SPMRL treebanks", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7694792151451111}, {"text": "constituent parsing", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.6603526622056961}]}, {"text": "For these, we use the predicted PoS tags provided together with the corpora.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we provide the first evaluation on the SPMRL datasets for sequence tagging constituent parsers.", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.8459919393062592}, {"text": "sequence tagging constituent parsers", "start_pos": 88, "end_pos": 124, "type": "TASK", "confidence": 0.8385754823684692}]}, {"text": "Metrics We report bracketing F-scores, using the EVALB and the EVAL-SPMRL scripts.", "labels": [], "entities": [{"text": "bracketing F-scores", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6764251887798309}, {"text": "EVALB", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9550302028656006}]}, {"text": "We measure the speed in terms of sentences per second.", "labels": [], "entities": [{"text": "speed", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9670643210411072}]}, {"text": "Setup We use, for direct comparison against.", "labels": [], "entities": []}, {"text": "We adopt bracketing F-score instead of label accuracy for model selection and report this performance as our second baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9158545732498169}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.7498114705085754}, {"text": "model selection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.6548800021409988}]}, {"text": "After 100 epochs, we select the model that fared best on the development set.", "labels": [], "entities": []}, {"text": "We use GloVe embeddings () for our English models and zzgiga embeddings ( for the Chinese models, fora more homogeneous comparison against other parsers).", "labels": [], "entities": []}, {"text": "ELMo (Peters et al., 2018) or BERT ( could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9255484938621521}, {"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9976509213447571}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9996191263198853}]}, {"text": "For SPMRL, no pretrained embeddings are used, following.", "labels": [], "entities": [{"text": "SPMRL", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.9518516659736633}]}, {"text": "As aside note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 shared task pretrained word embeddings ( or even the multilingual BERT model 6 . Our models are run on a single CPU 7 (and optionally on a consumer-grade GPU for further comparison) using a batch size of 128 for testing.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task pretrained word embeddings", "start_pos": 95, "end_pos": 144, "type": "DATASET", "confidence": 0.8921726175716945}, {"text": "BERT", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9306412935256958}]}, {"text": "Additional hyperparameters can be found in Appendix A.: Results on the PTB dev set, compared against.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8567397594451904}, {"text": "PTB dev set", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9092151721318563}]}, {"text": "DE refers to dynamic encoding and MTL to a model that additionally casts the problem as multi-task learning.", "labels": [], "entities": [{"text": "dynamic encoding", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7215389907360077}]}, {"text": "Each auxiliary task is added separately to the baseline with DE and MTL.", "labels": [], "entities": [{"text": "DE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.983913004398346}, {"text": "MTL", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.48998820781707764}]}, {"text": "Policy gradient fine-tunes the model that includes the best auxiliary task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the PTB dev set, compared against", "labels": [], "entities": [{"text": "PTB dev set", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9582148392995199}]}, {"text": " Table 2: Results on the CTB and SPMRL dev sets", "labels": [], "entities": [{"text": "CTB", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.961647629737854}, {"text": "SPMRL dev", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.7395005822181702}]}, {"text": " Table 4: Comparison on the CTB test set", "labels": [], "entities": [{"text": "CTB test set", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9598884979883829}]}, {"text": " Table 5: Comparison on the test SPMRL datasets (except Arabic). Kitaev and Klein (2018b) are results pub- lished after this work was submitted (italics represent the cases where they obtain a new state of the art on the  corresponding language).", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.8065784573554993}]}, {"text": " Table 6: Comparison of speeds on the SPMRL datasets", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.760422557592392}]}]}