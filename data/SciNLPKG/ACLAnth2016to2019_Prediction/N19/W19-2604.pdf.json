{"title": [], "abstractContent": [{"text": "Datasets are integral artifacts of empirical scientific research.", "labels": [], "entities": []}, {"text": "However, due to natural language variation, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications.", "labels": [], "entities": []}, {"text": "We report our approach to the Coleridge Initiative's Rich Context Competition , which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification).", "labels": [], "entities": [{"text": "Coleridge Initiative's Rich Context Competition", "start_pos": 30, "end_pos": 77, "type": "DATASET", "confidence": 0.9342575073242188}, {"text": "dataset mention extraction)", "start_pos": 145, "end_pos": 172, "type": "TASK", "confidence": 0.7911462485790253}]}, {"text": "In this work, we propose various neural base-lines and evaluate these model on one-plus and zero-shot classification scenarios.", "labels": [], "entities": []}, {"text": "We further explore various joint learning approaches-exploring the synergy between the tasks-and report the issues with such techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "The modern scientific method hinges on replicability and falsifiability.", "labels": [], "entities": [{"text": "replicability", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9711752533912659}]}, {"text": "Datasets are an essential aspect of enabling such analysis in much of modern empirical studies.", "labels": [], "entities": []}, {"text": "Datasets themselves are varied -in size, complexity, substructure, and scope -and references to them are also varied -in naming convention and subsequent reference or citation, both within and across documents.", "labels": [], "entities": []}, {"text": "Dataset mention extraction and classification has thus become more critical not only to facilitate the identification of proper target datasets for testing hypotheses but also to benchmark incremental research by extension.", "labels": [], "entities": [{"text": "Dataset mention extraction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6189599732557932}]}, {"text": "In this work, we explore various neural approaches to identifying cited surface forms associated with a dataset and interlinking them.", "labels": [], "entities": []}, {"text": "We benchmark our approach on the Coleridge Initiative's Rich Text Context Competition (RTCC), released in 2018, which we participated in, whose dataset comprises of social science publications exemplify such confusability problems with multiple surface dataset citations.", "labels": [], "entities": [{"text": "Coleridge Initiative", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9474450945854187}, {"text": "Rich Text Context Competition (RTCC)", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.7607042023113796}]}], "datasetContent": [{"text": "We elaborate on the complete experimental setup, which has the following configuration: Hyper-parameters.", "labels": [], "entities": []}, {"text": "As the documents in the corpus have 7K tokens on average, the sequence lengths are too long for any model to process directly.", "labels": [], "entities": []}, {"text": "We split the documents into shorter text fragment (T F i ) for training and inference.", "labels": [], "entities": []}, {"text": "Most fragments do not contain any dataset mentions; these segments we term \"negative segments\".", "labels": [], "entities": []}, {"text": "The document collection is thus highly skewed, with only 0.4% positive tokens (similarly for positive segments).", "labels": [], "entities": []}, {"text": "We under-sample to lessen the effect of data skew, by only considering some of the negative segments during training.", "labels": [], "entities": []}, {"text": "We sample all \"positive segments\", those with dataset mentions.", "labels": [], "entities": []}, {"text": "Our processing methods involve two hyperparameters -the segment length and the sampling rate of negative segments.", "labels": [], "entities": []}, {"text": "Both hyper-parameters affect the ratio of negative tokens sampled in the training set, which in turn impacts performance.", "labels": [], "entities": []}, {"text": "We experiment with the CRF baseline model (trigram model, whose hand-tuned features include uppercasing and digits) to analyze the effect of these hyper-parameters and select optimal values (cf..", "labels": [], "entities": []}, {"text": "For example, a negative sampling rate (NSR) of 0.05 means that we sample 5% of the total number of negative segments from the original dataset for training; conversely, NSR=0 means every training segment contains at least one dataset mention.", "labels": [], "entities": [{"text": "negative sampling rate (NSR)", "start_pos": 15, "end_pos": 43, "type": "METRIC", "confidence": 0.7942745784918467}]}, {"text": "Note that even for NSR=0, there are still many negative tokens as each segment only contains a few short mention phrases (4.7 tokens per mention on average), with the rest negative.", "labels": [], "entities": []}, {"text": "From the table, we can see that the model generally works better when the negative token rate is small.", "labels": [], "entities": []}, {"text": "We use the optimal segment length 40 and NSR=0.015 (1.5%) for all neural models in this paper.", "labels": [], "entities": [{"text": "NSR", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9979515671730042}]}, {"text": "For all models, we use the 300-dimensional GloVe () word embeddings.", "labels": [], "entities": []}, {"text": "All models are trained with Adam optimizer.", "labels": [], "entities": []}, {"text": "For dataset mention extraction, the task-specific parameters are as follows.", "labels": [], "entities": [{"text": "dataset mention extraction", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6064385771751404}]}, {"text": "For the base BiLSTM, we use a hidden size of 100 and a dropout rate of 0.2 on word embeddings.", "labels": [], "entities": []}, {"text": "We then used a dense layer with sigmoid activation to determine the probability of the input being part of a dataset mention.", "labels": [], "entities": []}, {"text": "For the character embedding CNN, we use character embedding dimension 300, 1D convolution 300 filters, window size 6, and a dropout rate of 0.4.", "labels": [], "entities": []}, {"text": "For the CNN-BiLSTM-CRF model, we add a CRF layer on top of the BiLSTM instead of a dense layer.", "labels": [], "entities": []}, {"text": "For dataset classification, the task-specific parameters are as follows.", "labels": [], "entities": [{"text": "dataset classification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7081910669803619}]}, {"text": "For the CNN model, we use 1D convolution with 256 kernels, with window size 6, followed by global max pooling, and a dense layer for the final classification output.", "labels": [], "entities": []}, {"text": "For the LSTM based model, we use a BiLSTM with hidden dimension 100 to encode the input sequence and use a dense layer on the final state of the BiLSTM for the final dataset classification.", "labels": [], "entities": []}, {"text": "We use a sigmoid for the final non-linear activation function.", "labels": [], "entities": []}, {"text": "As explained earlier, the rationale to use sigmoid is to allow the model to associate a single mention to multiple datasets which appear commonly in the dataset (see the example in).", "labels": [], "entities": []}, {"text": "We evaluate our model on the development set, the test set and on the zero-shot test set.", "labels": [], "entities": []}, {"text": "We first randomly held out 7% of the datasets from the corpus and select the publications (219 documents in total) containing these datasets to form the zero-shot test set.", "labels": [], "entities": []}, {"text": "To be clear, the datasets in the zero-shot test set are not seen at all within the training set.", "labels": [], "entities": []}, {"text": "We then ran-  domly holdout 225 publications to form the test set.", "labels": [], "entities": []}, {"text": "The datasets mentioned in these testing documents may have other mentions in the training set as well.", "labels": [], "entities": []}, {"text": "The dev set is split from the training set (5%) and has the same distribution and length as the training set.", "labels": [], "entities": [{"text": "length", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9773208498954773}]}, {"text": "Since the test set and zero-shot test set contain complete documents and do not have any sampling, the distribution is different from the sampled training set.", "labels": [], "entities": []}, {"text": "During the evaluation, we do not sample.", "labels": [], "entities": []}, {"text": "We first split the test documents into text segments of the same length as the training segments and perform inference with our trained model on these segments.", "labels": [], "entities": []}, {"text": "We combine the predicted results as the prediction for the entire test document.", "labels": [], "entities": []}, {"text": "We employ precision (P), recall (R) and F 1 score as our evaluation metrics.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9517512917518616}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.964099571108818}, {"text": "F 1 score", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9787305990854899}]}, {"text": "For dataset mention subtask, these metrics can be interpreted in a relaxed or strict manner, with respect to true token coverage.", "labels": [], "entities": []}, {"text": "The relaxed, partial match metric attributes a true positive count if any of the ground truth tokens are correctly predicted by the model as a mention phrase.", "labels": [], "entities": []}, {"text": "The strict, exact match metric attributes a true positive only when if every token in the mention is predicted correctly.", "labels": [], "entities": []}, {"text": "We also report exact match P, R, F 1 at the document level.", "labels": [], "entities": [{"text": "exact match P", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.8920828302701315}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9340599179267883}]}], "tableCaptions": [{"text": " Table 1: Mention Extraction Subtask performance. Segment length 40, negative sampling rate: 0.015.", "labels": [], "entities": [{"text": "negative sampling rate", "start_pos": 69, "end_pos": 91, "type": "METRIC", "confidence": 0.7471955617268881}]}, {"text": " Table 2: Dataset Classification Subtask performance. Segment length 40, negative sampling rate 0.015.", "labels": [], "entities": []}]}