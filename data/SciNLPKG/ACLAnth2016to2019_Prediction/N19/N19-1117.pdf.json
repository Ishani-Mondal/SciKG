{"title": [{"text": "Knowledge-Augmented Language Model and Its Application to Unsupervised Named-Entity Recognition", "labels": [], "entities": [{"text": "Named-Entity Recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.746182769536972}]}], "abstractContent": [{"text": "Traditional language models are unable to efficiently model entity names observed in text.", "labels": [], "entities": []}, {"text": "All but the most popular named entities appear infrequently in text providing insufficient context.", "labels": [], "entities": []}, {"text": "Recent efforts have recognized that context can be generalized between entity names that share the same type (e.g., person or location) and have equipped language models with access to an external knowledge base (KB).", "labels": [], "entities": []}, {"text": "Our Knowledge-Augmented Language Model (KALM) continues this line of work by augmenting a traditional model with a KB.", "labels": [], "entities": []}, {"text": "Unlike previous methods, however, we train with an end-to-end predictive objective optimizing the perplexity of text.", "labels": [], "entities": []}, {"text": "We do not require any additional information such as named entity tags.", "labels": [], "entities": []}, {"text": "In addition to improving language modeling performance, KALM learns to recognize named entities in an entirely unsuper-vised way by using entity type information latent in the model.", "labels": [], "entities": []}, {"text": "On a Named Entity Recognition (NER) task, KALM achieves performance comparable with state-of-the-art supervised models.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) task", "start_pos": 5, "end_pos": 40, "type": "TASK", "confidence": 0.8385093297277179}]}, {"text": "Our work demonstrates that named entities (and possibly other types of world knowledge) can be modeled successfully using predictive learning and training on large corpora of text without any additional information .", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is a form of unsupervised learning that allows language properties to be learned from large amounts of unlabeled text.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7369887679815292}]}, {"text": "As components, language models are useful for many Natural Language Processing (NLP) tasks such as generation () and machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8124613165855408}]}, {"text": "Additionally, the form of predictive learning that language modeling uses is useful to acquire text representations that can be used successfully to improve a number of downstream NLP tasks (.", "labels": [], "entities": []}, {"text": "In fact, models pre-trained with a predictive objective have provided anew stateof-the-art by a large margin.", "labels": [], "entities": []}, {"text": "Current language models are unable to encode and decode factual knowledge such as the information about entities and their relations.", "labels": [], "entities": []}, {"text": "Names of entities are an open class.", "labels": [], "entities": []}, {"text": "While classes of named entities (e.g., person or location) occur frequently, each individual name (e.g, Atherton or Zhouzhuang) maybe observed infrequently even in a very large corpus of text.", "labels": [], "entities": []}, {"text": "As a result, language models learn to represent accurately only the most popular named entities.", "labels": [], "entities": []}, {"text": "In the presence of external knowledge about named entities, language models should be able to learn to generalize across entity classes.", "labels": [], "entities": []}, {"text": "For example, knowing that Alice is a name used to refer to a person should give ample information about the context in which the word may occur (e.g.,.", "labels": [], "entities": []}, {"text": "In this work, we propose Knowledge Augmented Language Model (KALM), a language model with access to information available in a KB.", "labels": [], "entities": []}, {"text": "Unlike previous work, we make no assumptions about the availability of additional components (such as Named Entity Taggers) or annotations.", "labels": [], "entities": [{"text": "Named Entity Taggers", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7018942832946777}]}, {"text": "Instead, we enhance a traditional LM with a gating mechanism that controls whether a particular word is modeled as a general word or as a reference to an entity.", "labels": [], "entities": []}, {"text": "We train the model end-to-end with only the traditional predictive language modeling perplexity objective.", "labels": [], "entities": [{"text": "predictive language modeling perplexity", "start_pos": 56, "end_pos": 95, "type": "TASK", "confidence": 0.8555236905813217}]}, {"text": "As a result, our system can model named entities in text more accurately as demonstrated by reduced perplexities compared to traditional LM baselines.", "labels": [], "entities": []}, {"text": "In addition, KALM learns to recognize named entities completely unsupervised by interpreting the predictions of the gating mechanism attest time.", "labels": [], "entities": []}, {"text": "In fact, KALM learns an unsupervised named entity tagger that rivals inaccuracy supervised counterparts.", "labels": [], "entities": [{"text": "KALM", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.593835711479187}]}, {"text": "KALM works by providing a language model with the option to generate words from a set of entities from a database.", "labels": [], "entities": []}, {"text": "An individual word can either come from a general word dictionary as in traditional language model or be generated as a name of an entity from a database.", "labels": [], "entities": []}, {"text": "Entities in the database are partitioned by type.", "labels": [], "entities": []}, {"text": "The decision of whether the word is a general term or a named entity from a given type is controlled by a gating mechanism conditioned on the context observed so far.", "labels": [], "entities": []}, {"text": "Thus, KALM learns to predict whether the context observed is indicative of a named entity of a given type and what tokens are likely to be entities of a given type.", "labels": [], "entities": []}, {"text": "The gating mechanism at the core of KALM is similar to attention in Neural Machine Translation ().", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6838897069295248}]}, {"text": "As in translation, the gating mechanism allows the LM to represent additional latent information that is useful for the end task of modeling language.", "labels": [], "entities": []}, {"text": "The gating mechanism (in our case entity type prediction) is latent and learned in an end-to-end manner to maximize the probability of observed text.", "labels": [], "entities": []}, {"text": "Experiments with named entity recognition show that the latent mechanism learns the information that we expect while LM experiments show that it is beneficial for the overall language modeling task.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.6622165342171987}, {"text": "language modeling task", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7889042596022288}]}, {"text": "This paper makes the following contributions: \u2022 Our model, KALM, achieves anew stateof-the art for Language Modeling on several benchmarks as measured by perplexity.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.776281863451004}]}, {"text": "\u2022 We learn a named entity recognizer without any explicit supervision by using only plain text.", "labels": [], "entities": []}, {"text": "Our unsupervised named entity recognizer achieves a performance on par with the state-of-the supervised methods.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate that predictive learning combined with a gating mechanism can be utilized efficiently for generative training of deep learning systems beyond representation pre-training.", "labels": [], "entities": [{"text": "generative training", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.9334264993667603}]}], "datasetContent": [{"text": "We evaluate KALM on two tasks: language modeling and NER.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7742665410041809}]}, {"text": "We use two datasets: Recipe used only for LM evaluation and CoNLL 2003 used for both the LM and NER evaluations.", "labels": [], "entities": [{"text": "LM evaluation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8068383038043976}, {"text": "CoNLL 2003", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.7371407747268677}]}], "tableCaptions": [{"text": " Table 1: Statistics of recipe and CoNLL 2003 datasets", "labels": [], "entities": [{"text": "recipe and CoNLL 2003 datasets", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.8640966176986694}]}, {"text": " Table 2: Statistics of recipe and CoNLL 2003 KBs", "labels": [], "entities": [{"text": "recipe and CoNLL 2003 KBs", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.714587140083313}]}, {"text": " Table 3: Language modeling results on the recipe and CoNLL 2003 datasets", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7447394132614136}, {"text": "recipe and CoNLL 2003 datasets", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8236848294734955}]}, {"text": " Table 4: Characterization of WikiText-2 relative to CoNLL 2003 training set. Entities extracted from WikiText  cover 92.80% of the entities in CoNLL 2003 overall, and cover 82.56% of the unique entities. WikiText's size is  2.62 times as large. And adding WikiText to CoNLL training reduces the perplexity from 4.69 to 2.29.", "labels": [], "entities": [{"text": "CoNLL 2003 training set", "start_pos": 53, "end_pos": 76, "type": "DATASET", "confidence": 0.9403305351734161}]}, {"text": " Table 5: Results of KALM (above the double line in the table) and supervised NER models (under the double  line). Superscript annotations: 0: The basic bidirectional KALM with type embedding features as described in  Section 4.1. 1: Adding P (\u03c4 |y) in decoding, as described in Section 4.2.1, where \u03b1 and \u03b2 are tuned to be 0.4 and  0.6. 2: The basic model trained on CoNLL 2003 concatenated with WikiText-2. 3: Adding (\u03c4 |y) in decoding, with  the model trained on CoNLL 2003 concatenated with WikiText-2.", "labels": [], "entities": []}]}