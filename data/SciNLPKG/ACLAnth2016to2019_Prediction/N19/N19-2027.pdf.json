{"title": [{"text": "Generate, Filter, and Rank: Grammaticality Classification for Production-Ready NLG Systems", "labels": [], "entities": [{"text": "Rank", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9504042267799377}]}], "abstractContent": [{"text": "Neural approaches to Natural Language Generation (NLG) have been promising for goal-oriented dialogue.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.8171006143093109}]}, {"text": "One of the challenges of productionizing these approaches, however, is the ability to control response quality, and ensure that generated responses are acceptable.", "labels": [], "entities": []}, {"text": "We propose the use of a generate, filter, and rank framework, in which candidate responses are first filtered to eliminate unacceptable responses, and then ranked to select the best response.", "labels": [], "entities": []}, {"text": "While acceptability includes grammatical correctness and semantic correct-ness, we focus only on grammaticality classification in this paper, and show that existing datasets for grammatical error correction don't correctly capture the distribution of errors that data-driven generators are likely to make.", "labels": [], "entities": [{"text": "grammaticality classification", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.7192016839981079}, {"text": "grammatical error correction", "start_pos": 178, "end_pos": 206, "type": "TASK", "confidence": 0.6842235922813416}]}, {"text": "We release a grammatical classification and semantic correctness classification dataset for the weather domain that consists of responses generated by 3 data-driven NLG systems.", "labels": [], "entities": [{"text": "grammatical classification and semantic correctness classification", "start_pos": 13, "end_pos": 79, "type": "TASK", "confidence": 0.7043128708998362}]}, {"text": "We then explore two supervised learning approaches (CNNs and GBDTs) for classifying grammaticality.", "labels": [], "entities": [{"text": "classifying grammaticality", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.87846440076828}]}, {"text": "Our experiments show that grammaticality classification is very sensitive to the distribution of errors in the data, and that these distributions vary significantly with both the source of the response as well as the domain.", "labels": [], "entities": [{"text": "grammaticality classification", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.9095309674739838}]}, {"text": "We show that it's possible to achieve high precision with reasonable recall on our dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9982576966285706}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.998712420463562}]}], "introductionContent": [{"text": "In recent years, neural network-based approaches have been increasingly promising in the context of goal-oriented Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.8356673916180929}]}, {"text": "These approaches can effectively learn to generate responses of desired complexity and detail from unaligned data.", "labels": [], "entities": []}, {"text": "Additionally, these approaches can be scaled with relatively low effort * Equal contribution to new domains and use cases.", "labels": [], "entities": []}, {"text": "However, they are less robust to mistakes and have poor worst case performance.", "labels": [], "entities": []}, {"text": "Consistently achieving acceptable response quality in a customer facing product is an immediate blocker to using such models widely.", "labels": [], "entities": []}, {"text": "Controlling quality at generation time in these models is challenging, and there are no guarantees that any of the generated responses are suitable to surface to an end user.", "labels": [], "entities": []}, {"text": "Additionally, quality is hard to enforce at data collection time, given the increasingly widespread dependence on large pools of untrained annotators.", "labels": [], "entities": [{"text": "quality", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9830894470214844}]}, {"text": "As a result, classifying acceptability with high precision is extremely desirable.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9952200055122375}]}, {"text": "It can be used to establish safe fallbacks to acceptable, but potentially less ideal, responses that are generated by more traditional NLG systems like templates.", "labels": [], "entities": []}, {"text": "Such responses are likely to be grammatically and semantically correct, but may sacrifice detail, variety, and naturalness; this trade-off may sometimes be necessary in a consumer-facing product.", "labels": [], "entities": [{"text": "detail", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9794633388519287}, {"text": "variety", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9645155668258667}]}, {"text": "For example, the system could respond with \"Here's your weather forecast\", and show a card with relevant weather information, rather than generate an incoherent weather forecast.", "labels": [], "entities": []}, {"text": "Some key aspects of acceptability are grammaticality and semantic correctness.", "labels": [], "entities": []}, {"text": "A grammatical response is one that is well-formed, and a semantically correct response is one that correctly expresses the information that needs to be conveyed.", "labels": [], "entities": []}, {"text": "Systems that generate ungrammatical or incorrect responses run the risk of seeming unreliable or unintelligent.", "labels": [], "entities": []}, {"text": "Another important facet of acceptability is the naturalnesss (or human likeness) of the response, that can improve the usability of chatbots and other dialogue systems.", "labels": [], "entities": []}, {"text": "In this paper, we first propose the inclusion of a filtering step that performs acceptability classification in the more widely used generate & rank framework.", "labels": [], "entities": [{"text": "acceptability classification", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.8714178502559662}]}, {"text": "Then, we narrow our focus to grammaticality classification, and show how this problem calls for datasets of a different nature than typical grammatical error correction (GEC) datasets.", "labels": [], "entities": [{"text": "grammaticality classification", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.8491370677947998}, {"text": "grammatical error correction (GEC)", "start_pos": 140, "end_pos": 174, "type": "TASK", "confidence": 0.7293990850448608}]}, {"text": "We also show that state-of-the-art GEC models trained on general corpora fail to generalize to this problem.", "labels": [], "entities": []}, {"text": "Finally, we introduce a dataset of system-generated grammatical errors for the weather domain, and demonstrate the performance of some strong baselines for grammatical classification on this data.", "labels": [], "entities": [{"text": "grammatical classification", "start_pos": 156, "end_pos": 182, "type": "TASK", "confidence": 0.7605990171432495}]}, {"text": "This dataset can also be used for further research on semantic correctness classification.", "labels": [], "entities": [{"text": "semantic correctness classification", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.8910021781921387}]}, {"text": "Our experiments also reinforce the need for the new framework we propose.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first collected a dataset of human-generated responses for the weather domain, using a process similar to the one used in.", "labels": [], "entities": []}, {"text": "Each of the collected responses is conditioned on a scenario, consisting of a goal (the intent to be expressed) and arguments (information to be expressed).", "labels": [], "entities": []}, {"text": "In collecting the dataset, we restricted ourselves to the goals inform current condition and inform forecast.", "labels": [], "entities": [{"text": "forecast", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8172881603240967}]}, {"text": "An example scenario is \"requested location\": \"London\", \"temp\": \"32\", \"temp scale\": \"fahrenheit\", \"precip summary\": \"Heavy Blowing Snow\" A possible response for this scenario is In London, it's currently 32 degrees Fahrenheit with heavy snow..", "labels": [], "entities": [{"text": "London", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9016069769859314}]}, {"text": "We then trained some standard NLG models on this corpus.", "labels": [], "entities": []}, {"text": "Two of these (sc-LSTM Lex and sc-LSTM Delex) are semantically conditioned LSTMs as described in; the genLSTM model is a vanilla LSTM decoder; and IR is a simple retrieval-based generator.", "labels": [], "entities": []}, {"text": "The details of these are described in Appendix A.1.", "labels": [], "entities": [{"text": "Appendix A.1", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.686163067817688}]}, {"text": "We generated n = 3 responses from each of these models for each scenario in a held out data set, and deduped generated candidates that differed by a single character (often punctuation).", "labels": [], "entities": []}, {"text": "We then asked crowdworkers to judge the grammaticality of these responses.", "labels": [], "entities": []}, {"text": "Our final dataset 1 consists of 33K model-generated responses with grammaticality and semantic correctness judgments.", "labels": [], "entities": []}, {"text": "shows a detailed breakdown of grammatical and ungrammatical responses per model.", "labels": [], "entities": []}, {"text": "github.com/facebookresearch/momi  We try different combinations of NUCLE corpus and our dataset as train and test sets to learn a grammaticality classifier for model-generated weather responses.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9198723435401917}]}, {"text": "lists the results of these experiments described above.", "labels": [], "entities": []}, {"text": "As discussed before, since the goal is to build a classifier for use in production systems, we report the recall of models for grammatical class when the precision is very high (98%", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9969281554222107}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9990367889404297}]}], "tableCaptions": [{"text": " Table 2: Comparison of weather responses dataset  against the NUCLE corpus", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9788175225257874}]}, {"text": " Table 1: Mistakes involving grammatical errors and other cases of unacceptability in model-generated weather  responses", "labels": [], "entities": []}, {"text": " Table 3: Distribution of positive and negative examples in weather responses dataset. # gr and # ungr denote  number of grammatical and ungrammatical samples respectively.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of number of times the top ranked  response is unacceptable with and without filtering.", "labels": [], "entities": []}]}