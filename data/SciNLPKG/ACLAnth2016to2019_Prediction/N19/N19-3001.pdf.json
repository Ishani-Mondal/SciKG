{"title": [{"text": "Is it Dish Washer Safe? Automatically Answering \"Yes/No\" Questions using Customer Reviews", "labels": [], "entities": [{"text": "Dish Washer", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.8869890570640564}, {"text": "Answering \"Yes/No\" Questions", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8357189978872027}]}], "abstractContent": [{"text": "It has become commonplace for people to share their opinions about all kinds of products by posting reviews online.", "labels": [], "entities": []}, {"text": "It has also become commonplace for potential customers to do research about the quality and limitations of these products by posting questions online.", "labels": [], "entities": []}, {"text": "We test the extent to which reviews are useful in question-answering by combining two Amazon datasets, and focusing our attention on yes/no questions.", "labels": [], "entities": [{"text": "Amazon datasets", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.915575236082077}]}, {"text": "A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time.", "labels": [], "entities": []}, {"text": "Preliminary reading comprehension experiments with this dataset prove inconclusive , with accuracy in the range 50-66%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994810223579407}]}], "introductionContent": [{"text": "Consumers often carryout online research about a product before purchasing.", "labels": [], "entities": []}, {"text": "This can take the form of reading consumer reviews and/or asking specific questions on online fora.", "labels": [], "entities": []}, {"text": "In this paper we ask whether a question-answering (QA) system can utilize the information in consumer reviews when answering yes/no questions about a product.", "labels": [], "entities": []}, {"text": "We compile a dataset of questions about Amazon products together with consumer reviews of the same products, and manually analyse a sample of 100 questions from four domains.", "labels": [], "entities": []}, {"text": "We find that the reviews contain the answer in only 45% of cases.", "labels": [], "entities": []}, {"text": "In 36% of cases, the answer is directly expressed in at least one of the reviews, and 9% of the time, it is indirectly expressed.", "labels": [], "entities": []}, {"text": "This suggests that reviews can sometimes be useful and so we goon to experiment with QA systems that use the reviews in addition to the question.", "labels": [], "entities": []}, {"text": "We focus on yes/no questions.", "labels": [], "entities": []}, {"text": "Being able to answer these is not only an indicator of whether reviews will be useful for other question types but is also a signal of how much comprehension is actually taking place.", "labels": [], "entities": []}, {"text": "In our preliminary experiments with three domains from this new dataset, we compare systems which attempt to answer a yes/no question based on the question alone to those that also use related reviews.", "labels": [], "entities": []}, {"text": "We experiment with two methods for selecting relevant sentences from the reviews, and with various representations for encoding the questions and reviews including bag-ofwords, word2vec), ELMO (, and BERT).", "labels": [], "entities": [{"text": "ELMO", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9836955070495605}, {"text": "BERT", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9983386993408203}]}, {"text": "On the development set, our systems tend to outperform the chance baseline but not by a large margin -our development set results range from 50 to 66%.", "labels": [], "entities": []}, {"text": "Over the three domains, we also find that the question-only systems tend to perform as well as and sometimes outperform those which also use the reviews, suggesting that separating the answers from the noise in these reviews is not straightforward.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare systems which use the review and question text to systems which just use the question text.", "labels": [], "entities": []}, {"text": "We select three of the four domains used for manual analysis.We exclude Clothing Shoes & Jewellery due to the small number of questions.", "labels": [], "entities": [{"text": "manual analysis.We", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6649523079395294}]}, {"text": "represents the number of questions in the training, development and test set and the ratio of \"Yes\" and \"No\" questions in each of them.", "labels": [], "entities": []}, {"text": "The evaluation metric is accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994407296180725}]}, {"text": "For the bow and word2vec experiments, we normalize the text and remove stop words.", "labels": [], "entities": []}, {"text": "We use a pre-trained Google News word2vec model.", "labels": [], "entities": []}, {"text": "Every text is encoded as a sum of its word vectors and normalized.", "labels": [], "entities": []}, {"text": "For the ELMO representation we average three layers of ELMO output and represent a sentence as concatenation of its words vectors.", "labels": [], "entities": []}, {"text": "In the question-only BERT experiment, we perform single-sentence classification,).", "labels": [], "entities": [{"text": "BERT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9697855114936829}, {"text": "single-sentence classification", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.724601998925209}]}, {"text": "In the experiments where the reviews are used, we perform sentence 7 pair classification where the question is the first sentence and the review text the second (Devlin et al., 2018,.", "labels": [], "entities": [{"text": "sentence 7 pair classification", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6019350290298462}]}, {"text": "We use the pretrained models B base and B large . Both of them are uncased.: Results on test set with best-scoring question and review+question systems on development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Balanced yes/no dataset statistics per domain: Number of products (P) which have yes/no questions,  number of questions (# Question), count of sentences in questions (S), total number of words in questions (W),  total number of reviews (# Reviews), all number of sentences in reviews, total number of words in reviews.", "labels": [], "entities": []}, {"text": " Table 3: Selection of 100 questions from 4 domains for manual analysis. The last column contains the percentage  of the analysed questions from each domain (eg. 100 is 4.6% of the Baby question data, 3.6% of Beauty, etc. ).", "labels": [], "entities": [{"text": "Baby question data", "start_pos": 181, "end_pos": 199, "type": "DATASET", "confidence": 0.8547467390696207}]}, {"text": " Table 4: Domain split of the training, development and test sets using the number of questions and the ratio of  \"Yes\" and \"No\" questions in each of them.", "labels": [], "entities": []}, {"text": " Table 5: Results on development set of Logistic Regression (LR) applied to bag of word (bow), word2vec (w2v)  and ELMO (elmo) representations, and BERT models (B base and B large) for 3 domains. The best question-only  (Q only) and question+review (Q+Review) systems are in bold.", "labels": [], "entities": [{"text": "Logistic Regression (LR)", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.754772675037384}, {"text": "BERT", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9923611283302307}]}, {"text": " Table 6: Results on test set with best-scoring question  and review+question systems on development set.", "labels": [], "entities": []}]}