{"title": [{"text": "Measuring Immediate Adaptation Performance for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7239426970481873}]}], "abstractContent": [{"text": "Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve system performance for interactive machine translation.", "labels": [], "entities": [{"text": "Incremental domain adaptation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6475891868273417}, {"text": "interactive machine translation", "start_pos": 199, "end_pos": 230, "type": "TASK", "confidence": 0.6168276866277059}]}, {"text": "Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected.", "labels": [], "entities": []}, {"text": "Adaptation is most commonly assessed using corpus-level BLEU-or TER-derived metrics that do not explicitly take adaptation speed into account.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9808697700500488}, {"text": "BLEU-or TER-derived metrics", "start_pos": 56, "end_pos": 83, "type": "METRIC", "confidence": 0.8450294137001038}]}, {"text": "We find that these metrics often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items.", "labels": [], "entities": []}, {"text": "To this end, we propose new metrics that directly evaluate immediate adaptation performance for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8050802052021027}]}, {"text": "We use these metrics to choose the most suitable adaptation method from a range of different adaptation techniques for neural machine translation systems .", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.70367564757665}]}], "introductionContent": [{"text": "Incremental domain adaptation, or online adaptation, has been shown to improve statistical machine translation and especially neural machine translation (NMT) systems significantly () (inter-alia).", "labels": [], "entities": [{"text": "Incremental domain adaptation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7734041412671407}, {"text": "online adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7597156763076782}, {"text": "statistical machine translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6696658134460449}, {"text": "neural machine translation (NMT)", "start_pos": 126, "end_pos": 158, "type": "TASK", "confidence": 0.7932153244813284}]}, {"text": "The natural use case is a computeraided translation (CAT) scenario, where a user and a machine translation system collaborate to translate a document.", "labels": [], "entities": [{"text": "computeraided translation (CAT)", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.87022944688797}]}, {"text": "Each user translation is immediately used as anew training example to adapt the machine translation system to the specific document.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.697078600525856}]}, {"text": "Adaptation techniques for MT are typically evaluated by their corpus translation quality, but such evaluations may not capture prominent aspects of the user experience in a collaborative translation scenario.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9912318587303162}, {"text": "corpus translation", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.6956790089607239}]}, {"text": "This paper focuses on directly measuring the speed of lexical acquisition for in-domain vocabulary.", "labels": [], "entities": []}, {"text": "To that end, we propose three related metrics that are designed to reflect the responsiveness of adaptation.", "labels": [], "entities": []}, {"text": "An ideal system would immediately acquire indomain lexical items upon observing their translations.", "labels": [], "entities": []}, {"text": "Moreover, one might expect a neural system to generalize from one corrected translation to related terms.", "labels": [], "entities": []}, {"text": "Once a user translates \"bank\" to German \"Bank\" (institution) instead of \"Ufer\" (shore) in a document, the system should also correctly translate \"banks\" to \"Banken\" instead of \"Ufer\" (the plural is identical to the singular in German) in future sentences.", "labels": [], "entities": []}, {"text": "We measure both one-shot vocabulary acquisition for terms that have appeared once in a previous target sentence, as well as zeroshot vocabulary acquisition for terms that have not previously appeared.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows some surprising results.", "labels": [], "entities": []}, {"text": "Methods that appear to have comparable performance using corpus quality metrics such as BLEU can differ substantially in zero-shot and one-shot vocabulary acquisition.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9896690249443054}, {"text": "vocabulary acquisition", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7075789421796799}]}, {"text": "In addition, we find that fine-tuning a neural model tends to improve one-shot vocabulary recall while degrading zero-shot vocabulary recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9038298726081848}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.7582116723060608}]}, {"text": "We evaluate several adaptation techniques on a range of online adaptation datasets.", "labels": [], "entities": []}, {"text": "Fine tuning applied to all parameters in the NMT model maximizes one-shot acquisition, but shows a worrisome degradation in zero-shot recall.", "labels": [], "entities": [{"text": "one-shot acquisition", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.519774541258812}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9783474206924438}]}, {"text": "By contrast, fine tuning with group lasso regularization, a technique recently proposed to improve the space efficiency of adapted models, achieves an appealing balance of zero-shot and one-shot vocabulary acquisition as well as high corpus-level translation quality.", "labels": [], "entities": [{"text": "group lasso regularization", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6017886598904928}, {"text": "vocabulary acquisition", "start_pos": 195, "end_pos": 217, "type": "TASK", "confidence": 0.7318815886974335}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the Autodesk test set for tradi- tional MT quality metrics. SBLEU refers to an average  of sentence-wise BLEU scores as described by Nakov  et al. (2012). The best result in each column is denoted  with bold font.", "labels": [], "entities": [{"text": "Autodesk test set", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9169850945472717}, {"text": "MT quality metrics", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7778392632802328}, {"text": "SBLEU", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9282563328742981}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9647238850593567}]}, {"text": " Table 2: Results on the Autodesk test set for the pro- posed metrics R0, R1, and R0+1.", "labels": [], "entities": [{"text": "Autodesk test set", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9325366218884786}, {"text": "R0", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.7221211194992065}]}, {"text": " Table 3: Results on Autodesk data calculating the met- rics only for truly novel content words, i.e. ones that do  not occur in the training data.", "labels": [], "entities": []}, {"text": " Table 4: Results on Autodesk data calculating the met- rics with subwords.", "labels": [], "entities": [{"text": "Autodesk data", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9185821115970612}]}, {"text": " Table 5: BLEU, sentence-wise BLEU, TER, R0+1, R0, and R1 metrics for a number of data sets, comparing  different adaptation methods as described in Section 4. Baseline results are given as absolute scores, results for  adaptation are given as relative differences. Best viewed in color.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982049465179443}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.958685576915741}, {"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9968376159667969}, {"text": "R0", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.8695086240768433}]}]}