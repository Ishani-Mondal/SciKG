{"title": [{"text": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models", "labels": [], "entities": [{"text": "Transfer Learning from Pretrained Language", "start_pos": 38, "end_pos": 80, "type": "TASK", "confidence": 0.9178074121475219}]}], "abstractContent": [{"text": "A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora.", "labels": [], "entities": []}, {"text": "In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting.", "labels": [], "entities": []}, {"text": "Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process.", "labels": [], "entities": []}, {"text": "This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task.", "labels": [], "entities": []}, {"text": "Our method does not require pre-training or finetuning separate components of the network and we train our models end-to-end in a single step.", "labels": [], "entities": []}, {"text": "We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity .", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7893495361010233}]}], "introductionContent": [{"text": "Pretrained word representations captured by Language Models (LMs) have recently become popular in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Pretrained word representations captured by Language Models (LMs)", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.6105536609888077}, {"text": "Natural Language Processing (NLP)", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7015994985898336}]}, {"text": "Pretrained LMs encode contextual information and high-level features of language, modeling syntax and semantics, producing state-of-the-art results across a wide range of tasks, such as named entity recognition (, machine translation () and text classification.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 186, "end_pos": 210, "type": "TASK", "confidence": 0.613471786181132}, {"text": "machine translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.7909041345119476}, {"text": "text classification", "start_pos": 241, "end_pos": 260, "type": "TASK", "confidence": 0.8061209321022034}]}, {"text": "However, in cases where contextual embeddings from language models are used as additional features (e.g. ELMo (), results come at a high computational cost and require task-specific architectures.", "labels": [], "entities": []}, {"text": "At the same time, approaches that rely on fine-tuning a LM to the task at hand (e.g.) depend on pretraining the model on an extensive vocabulary and on employing a sophisticated slanted triangular learning rate scheme to adapt the parameters of the LM to the target dataset.", "labels": [], "entities": []}, {"text": "We propose a simple and effective transfer learning approach, that leverages LM contextual representations and does not require any elaborate scheduling schemes during training.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.894160658121109}]}, {"text": "We initially train a LM on a Twitter corpus and then transfer its weights.", "labels": [], "entities": []}, {"text": "We add a task-specific recurrent layer and a classification layer.", "labels": [], "entities": []}, {"text": "The transferred model is trained end-to-end using an auxiliary LM loss, which allows us to explicitly control the weighting of the pretrained part of the model and ensure that the distilled knowledge it encodes is preserved.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: 1) We show that transfer learning from language models can achieve competitive results, while also being intuitively simple and computationally effective.", "labels": [], "entities": []}, {"text": "2) We address the problem of catastrophic forgetting, by adding an auxiliary LM objective and using an unfreezing method.", "labels": [], "entities": [{"text": "catastrophic forgetting", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.5537849962711334}]}, {"text": "3) Our results show that our approach is competitive with more sophisticated transfer learning methods.", "labels": [], "entities": []}, {"text": "We make our code widely available.", "labels": [], "entities": []}], "datasetContent": [{"text": "To pretrain the language model, we collect a dataset of 20 million English Twitter messages, including approximately 2M unique tokens.", "labels": [], "entities": []}, {"text": "We use the 70K most frequent tokens as vocabulary.", "labels": [], "entities": []}, {"text": "We evaluate our model on five datasets: Sent17 for sentiment analysis (, PsychExp for emotion recognition, Irony18 for irony detection (Van Hee et al., 2018), SCv1 and SCv2 for sarcasm detection (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.958147257566452}, {"text": "emotion recognition", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7150667011737823}, {"text": "irony detection", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.8606591820716858}, {"text": "sarcasm detection", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.9206511080265045}]}, {"text": "More details about the datasets can be found in.", "labels": [], "entities": []}, {"text": "To preprocess the tweets, we use Ekphrasis (.", "labels": [], "entities": []}, {"text": "For the generic datasets, we use NLTK ().", "labels": [], "entities": []}, {"text": "For the NBoW baseline, we use word2vec () 300-dimensional embeddings as features.", "labels": [], "entities": [{"text": "NBoW baseline", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9714756011962891}]}, {"text": "47.0 \u00b1 1.1 66.5 \u00b1 0.2 75.0 \u00b1 0.7 56.8 \u00b1 2.0 45.8 \u00b1 1.6 ULMFiT (Wiki-103) 23.6 \u00b1 1.6 60.5 \u00b1 0.5 68.7 \u00b1 0.6 56.6 \u00b1 0.5 21.8 \u00b1 0.3 ULMFiT (Twitter) 41.6 \u00b1 0.7 65.6 \u00b1 0.4 67.2 \u00b1 0.9 44.0 \u00b1 0.7 40.2 \u00b1 1.", "labels": [], "entities": [{"text": "ULMFiT (Twitter) 41.6 \u00b1 0.7 65.6 \u00b1 0.4 67.2", "start_pos": 128, "end_pos": 171, "type": "DATASET", "confidence": 0.8784249533306469}]}, {"text": "For the neural models, we use an LM with an embedding size of 400, 2 hidden layers, 1000 neurons per layer, embedding dropout 0.1, hidden dropout 0.3 and batch size 32.", "labels": [], "entities": []}, {"text": "We add Gaussian noise of size 0.01 to the embedding layer.", "labels": [], "entities": []}, {"text": "A clip norm of 5 is applied, as an extra safety measure against exploding gradients.", "labels": [], "entities": [{"text": "clip norm", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9534969627857208}]}, {"text": "For each text classification neural network, we add on top of the transferred LM an LSTM layer of size 100 with self-attention and a softmax classification layer.", "labels": [], "entities": [{"text": "text classification neural", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.8242138028144836}]}, {"text": "In the pretraining step, SGD with a learning rate of 0.0001 is employed.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.976188987493515}]}, {"text": "In the transferred model, SGD with the same learning rate is used for the pretrained layers.", "labels": [], "entities": []}, {"text": "However, we use Adam () with a learning rate of 0.0005 for the newly added LSTM and classification layers.", "labels": [], "entities": []}, {"text": "For developing our models, we use PyTorch ( and Scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.5120830535888672}]}], "tableCaptions": [{"text": " Table 1: Datasets used for the downstream tasks.", "labels": [], "entities": []}, {"text": " Table 2: Ablation study on various downstream datasets. Average over five runs with standard deviation. BoW  stands for Bag of Words, NBoW for Neural Bag of Words. P-LM stands for a classifier initialized with our  pretrained LM, su for sequential unfreezing and aux for the auxiliary LM loss. In all cases, F 1 is employed.", "labels": [], "entities": [{"text": "F 1", "start_pos": 309, "end_pos": 312, "type": "METRIC", "confidence": 0.9890416860580444}]}]}