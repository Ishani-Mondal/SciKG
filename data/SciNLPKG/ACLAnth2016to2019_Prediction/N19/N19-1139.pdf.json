{"title": [{"text": "White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks", "labels": [], "entities": [{"text": "Efficient Distillation of Black-Box Adversarial Attacks", "start_pos": 16, "end_pos": 71, "type": "TASK", "confidence": 0.6966790109872818}]}], "abstractContent": [{"text": "Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adver-sarial training.", "labels": [], "entities": []}, {"text": "Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018).", "labels": [], "entities": []}, {"text": "In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network.", "labels": [], "entities": []}, {"text": "We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples.", "labels": [], "entities": []}, {"text": "Moreover, it reduces adversarial example generation time by 19x-39x.", "labels": [], "entities": []}, {"text": "We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability.", "labels": [], "entities": []}, {"text": "Our attack flips the API-predicted label in 42% of the generated examples, while humans maintain high-accuracy in predicting the gold label.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adversarial examples () have gained tremendous attention recently, as they elucidate model limitations, and expose vulnerabilities in deployed systems.", "labels": [], "entities": []}, {"text": "Work in natural language processing (NLP) either (a) used simple heuristics for generating adversarial examples, or (b) assumed white-box access, where the attacker has access to gradients of the model with respect to the input (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.801886518796285}]}, {"text": "In this approach, adversarial examples are constructed through an optimization process that uses gradient descent to search for input examples that maximally change the predictions of a model.", "labels": [], "entities": []}, {"text": "However, developing attacks with only black-box access to a model (no access to gradients) is still under-explored in NLP.", "labels": [], "entities": []}, {"text": "* Equal contribution Inspired by work in computer vision, we show in this work that a neural network can learn to emulate the optimization process of a white-box attack and generalize well to new examples.", "labels": [], "entities": []}, {"text": "gives a high-level overview of our approach.", "labels": [], "entities": []}, {"text": "We assume a text classification model and a white-box attack that flips characters in the input to modify the model prediction (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7389360070228577}]}, {"text": "We generate output adversarial examples using the white-box attack and train a neural network from these inputoutput examples to imitate the white-box attack.", "labels": [], "entities": []}, {"text": "This results in a much more efficient attack whose run-time is independent of the optimization process.", "labels": [], "entities": []}, {"text": "Moreover, assuming adversarial examples transfer between different models, our distilled model can now be used to generate adversarial examples for black-box attacks directly.", "labels": [], "entities": []}, {"text": "We use our approach to attack a toxicity classifier, aimed at detecting toxic language on social media (.", "labels": [], "entities": []}, {"text": "We show that our model achieves a speed-up of 19x-39x in generating adversarial examples while maintaining similar attack quality, compared to an optimization-based method.", "labels": [], "entities": []}, {"text": "We then use our model fora black-box attack against Google Perspective API for detecting toxic sentences, and find that 42% of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic.", "labels": [], "entities": []}, {"text": "Our code can be downloaded from http:// github.com/orgoro/white-2-black.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now empirically investigate whether our method can be used to attack classifiers for detecting \"toxic\" language on social media.", "labels": [], "entities": []}, {"text": "Recently a challenge by Alphabet aimed to improve labeling of toxic comments that are rude and disrespectful.", "labels": [], "entities": [{"text": "Alphabet", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9823753237724304}]}, {"text": "Alphabet released a dataset 2 of 160K comments from Wikipedia discussions, and classified each comment to six labels.", "labels": [], "entities": [{"text": "Alphabet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.965813159942627}]}, {"text": "We focus on the Toxic label only, which represents 9.5% of the dataset.", "labels": [], "entities": []}, {"text": "We used the dataset to train the source model S(\u00b7), which runs a 2-layer bidirectional GRU () over the input x, and then uses an attention () pooling layer to obtain a fixed dimensional vector for x.", "labels": [], "entities": []}, {"text": "This vector is passed through a feed-forward layer to compute the probability that x is toxic.", "labels": [], "entities": []}, {"text": "The accuracy of the source model is 96.5% AUC -comparable to the top submissions in the challenge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997029900550842}, {"text": "AUC", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9996302127838135}]}, {"text": "We used the 13,815 toxic-labeled sentences from the training set to generate adversarial examples for training the attacker as described above.", "labels": [], "entities": []}, {"text": "Dataset generation depends on the search procedure, and we experiment with 3 setups: (a) HOTFLIP-5: beam-search with K=5, (b) HOTFLIP-10: beamsearch with K=10, and (C) HOTFLIP+: a more expensive search procedure that calls S(\u00b7) more frequently.", "labels": [], "entities": [{"text": "Dataset generation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6247164458036423}]}, {"text": "We describe the details of this procedure in Appendix A. Because our attacker is independent of the search procedure, inference time is not affected by the search procedure at data generation time.", "labels": [], "entities": []}, {"text": "This results in three distilled models: DISTFLIP-5, DISTFLIP-10, and DISTFLIP+.", "labels": [], "entities": [{"text": "DISTFLIP-5", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8130533695220947}]}, {"text": "Attacking the source model We compare the performance of DISTFLIP variants against HOT-FLIP variant, including HOTFLIP-1 (K=1).", "labels": [], "entities": []}, {"text": "We also compare to a RANDOM baseline, which chooses a position and target character randomly, and to an ATTENTION baseline, which uses the attention layer of S(\u00b7) to choose the character position with maximum attention to flip, and selects a target character randomly.", "labels": [], "entities": [{"text": "RANDOM baseline", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.7354598343372345}, {"text": "ATTENTION", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9947452545166016}]}, {"text": "We report the average number of flips required to change the prediction of toxic sentences in the source model, the slow-down per single character-flip, and the slowdown per attack, which is computed by multiplying slow-down per flip by the ratio of the number of flips required per attack.", "labels": [], "entities": []}, {"text": "Because roughly 15% of the examples in the dataset mostly contain repeated profanities that require many flips, we also report the average number of flips for the other 85% of the examples.", "labels": [], "entities": []}, {"text": "We observe that HOTFLIP+ requires the fewest flips to change model prediction, but attacks are very slow.", "labels": [], "entities": [{"text": "model prediction", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.687632292509079}]}, {"text": "The number of flips per attack for DISTFLIP+ is comparable to HOTFLIP-5 and HOTFLIP-10, but it achieves a speed-up of 19x-39x.", "labels": [], "entities": [{"text": "HOTFLIP-10", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8316717743873596}]}, {"text": "DISTFLIP-5 and DISTFLIP-10 require a few more flips compared to DISTFLIP+.", "labels": [], "entities": [{"text": "DISTFLIP-5", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9590420126914978}, {"text": "DISTFLIP-10", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.873728334903717}]}, {"text": "Overall attack quality is high, with less than two flips necessary for 85% of the examples for DISTFLIP+.  that a sentence is toxic, where probability > 0.7 is classified as toxic, < 0.3 is non-toxic, and otherwise uncertain.", "labels": [], "entities": []}, {"text": "The model itself is not publicly available.", "labels": [], "entities": []}, {"text": "We randomly selected 136 toxic examples from the validation set and attacked them with DISTFLIP+ until the source model probability was < 0.5.", "labels": [], "entities": []}, {"text": "We measured the toxicity probability before and after our attack and found that the average toxicity probability decreased from 0.9 to 0.67, with an average of 5.0 flips per sentence.", "labels": [], "entities": []}, {"text": "The label is flipped from toxic to uncertain or non-toxic in 42% of these examples.", "labels": [], "entities": []}, {"text": "Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).", "labels": [], "entities": [{"text": "DISTFLIP-5", "start_pos": 235, "end_pos": 245, "type": "DATASET", "confidence": 0.8843913078308105}]}, {"text": "The same annotator was never shown a toxic sentence and its attacked counterpart.", "labels": [], "entities": []}, {"text": "We asked annotators whether sentences are toxic, and measured average annotated toxicity.", "labels": [], "entities": []}, {"text": "We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.", "labels": [], "entities": []}, {"text": "This shows that humans clearly view the sentences as toxic, even after our attack.", "labels": [], "entities": []}, {"text": "shows examples for sentences attacked by DISTFLIP-10 and the change in toxicity score according to The Google Perspective API.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average number of flips to change the prediction of toxic sentences, average number of flips to change  the prediction for 85% of the examples that do not contain repeated profanities, slow-down per flip compared  to DISTFLIP+, and slow-down per sentence attack compared to DISTFLIP+. One character-flip using DISTFLIP  takes 12ms on an Nvidia GTX1080Ti.", "labels": [], "entities": []}]}