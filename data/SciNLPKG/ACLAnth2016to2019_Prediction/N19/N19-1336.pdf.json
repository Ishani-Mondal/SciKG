{"title": [{"text": "Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users.", "labels": [], "entities": []}, {"text": "In real world applications, however, it is important to ensure that the agent performs smoothly interacting with not only regular users but also those malicious ones who would attack the system through interactions in order to achieve goals for their own advantage.", "labels": [], "entities": []}, {"text": "In this paper, we develop algorithms to evaluate the robust-ness of a dialogue agent by carefully designed attacks using adversarial agents.", "labels": [], "entities": []}, {"text": "Those attacks are performed in both black-box and white-box settings.", "labels": [], "entities": []}, {"text": "Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the robustness of a goal-oriented dialogue system.", "labels": [], "entities": []}, {"text": "On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to \u22125.76 on a scale from \u221210 to 10 for randomized goals.", "labels": [], "entities": []}, {"text": "Moreover, with the proposed adversar-ial training, we are able to improve the robust-ness of negotiation agents by 1.5 points on average against all our attacks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Crafting an intelligent agent to communicate in the dialogue system using natural languages has been a long-standing problem in AI.", "labels": [], "entities": []}, {"text": "It requires designing an agent to understand, plan and generate natural language to achieve different goals such as question-answering, cooperation, negotiation etc.", "labels": [], "entities": []}, {"text": "Inspired by recent successes in deep neural networks, ( has recently developed an end-to-end learning framework to train a recurrent neural network (RNN)-based negotiation agent in goal-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "This NNbased technique has been identified as one of the state-of-the-arts and has been applied to several other tasks (.", "labels": [], "entities": []}, {"text": "Although NN-based dialogue agents have shown convincing performance on several tasks, it is not clear whether they also work well when facing malicious users or agents.", "labels": [], "entities": []}, {"text": "To answer this question, we study how to evaluate the robustness of a goal-oriented dialogue system.", "labels": [], "entities": []}, {"text": "For simplicity, we consider a goal-oriented agent A that aims to maximize some score, and define the \"robustness\" of A as the worst-case performance under any feasible agent A . We also call A an adversarial agent that tries to \"attack\" A since it aims to minimize A's score.", "labels": [], "entities": []}, {"text": "The problem of evaluating the robustness of A can then be solved by designing an adversarial agent to attack A.", "labels": [], "entities": []}, {"text": "For instance, considering a negotiation agent that can decide when to make a deal, we say the agent is not robust if an adversarial agent can fool the target agent to make a deal with significant lower scores.", "labels": [], "entities": []}, {"text": "Ideally, before deploying an agent into real systems, we need to ensure it performs smoothly under strong adversarial attacks.", "labels": [], "entities": []}, {"text": "The concept of adversarial agent is related to recent studies on adversarial examples for image classifiers-it has been shown that a carefully designed small perturbation can easily make neural networks mis-classify (, and several recent works has extended these attacks to natural language processing models such as sentiment analysis ( and machine translation (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 317, "end_pos": 335, "type": "TASK", "confidence": 0.8847320377826691}, {"text": "machine translation", "start_pos": 342, "end_pos": 361, "type": "TASK", "confidence": 0.7741928994655609}]}, {"text": "However, all of the previous work consider attacking a static model, where except input im-age/sentence there is no interaction between the attacker and the target model.", "labels": [], "entities": []}, {"text": "Instead, we investigate a much more challenging problem, where there can be many turns of interactions between adversarial and target agents.", "labels": [], "entities": []}, {"text": "This leads to several difficulties including 1) How to lead the target agent to a bad state and 2) how to force the target agent to make a wrong decision.", "labels": [], "entities": []}, {"text": "Therefore, previous methods for attacking static models cannot be directly applied.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the aforementioned challenges by proposing several novel ways to design an adversarial agent to evaluate the robustness of goal-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "We highlight our major contributions as follows: \u2022 We propose a framework to generate adversarial agents in both black-box and white-box settings.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on crafting adversarial agents instead of adversarial examples in an interactive dialogue system.", "labels": [], "entities": []}, {"text": "\u2022 We conduct a series of studies on the negotiation agent proposed in (.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed strategies can successfully attack existing negotiation agents to significantly reduce their average score.", "labels": [], "entities": []}, {"text": "For instance, our attacks can reduce the average advantage of the RL-based negotiation agent from 2.68 to \u22125.76 on random problems with the total value of 10.", "labels": [], "entities": []}, {"text": "\u2022 We also show that through the proposed iterative adversarial training procedure, we could significantly improve the robustness of a goal-oriented agent against various attacks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform extensive experiments on evaluating the robustness of the negotiation agents developed in ().", "labels": [], "entities": []}, {"text": "Furthermore, we show that the robustness of negotiation agents can be significantly improved using the proposed adversarial training procedure.", "labels": [], "entities": []}, {"text": "Our codes are publicly available at https://github.com/cmhcbb/ Robustness-of-Dialogue-systems.", "labels": [], "entities": []}, {"text": "We use the code released by the authors ( and follow their instructions to get the target end-to-end negotiation agents.", "labels": [], "entities": []}, {"text": "More specifically, we first train the model on 5808 dialogues, based on 2236 unique scenarios in supervised way to imitate the actions of human users.", "labels": [], "entities": []}, {"text": "We call this model supervised model (SV agent).", "labels": [], "entities": []}, {"text": "Then we use reinforcement learning to conduct goal-oriented training in order to maximize the agent' reward.", "labels": [], "entities": []}, {"text": "The second model is called the reinforcement learning model (RL agent).", "labels": [], "entities": []}, {"text": "As a result, when doing selfplay between RL agent and SV agent, we could get RL agent with 5.86 perplexity, 89.57% agreement and 7.23 average score, while SV agent achieves 5.47 perplexity and 4.55 average score.", "labels": [], "entities": [{"text": "agreement", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9866085052490234}]}, {"text": "These numbers are similar to the numbers reported in (.", "labels": [], "entities": []}, {"text": "To evaluate the robustness of these agents, we conduct all the proposed attacks on both supervised model (SV agent) and reinforcement learning model (RL agent).", "labels": [], "entities": []}, {"text": "The successfulness of an attack is measured by average score advantage and positive advantage rate (PAR).", "labels": [], "entities": [{"text": "average score advantage", "start_pos": 47, "end_pos": 70, "type": "METRIC", "confidence": 0.9108370939890543}, {"text": "positive advantage rate (PAR)", "start_pos": 75, "end_pos": 104, "type": "METRIC", "confidence": 0.8987016975879669}]}, {"text": "Average score advantage is defined by averaged adversarial agent's score minus average target agent's score.", "labels": [], "entities": [{"text": "Average score advantage", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.8885193864504496}]}, {"text": "The value is in the region of since the total values are controlled to be 10 for both sides, and a larger advantage indicates a more successful attack.", "labels": [], "entities": []}, {"text": "Also, we define positive advantage rate (PAR) as the ratio of dialogues that the adversarial agent gets a higher score than the target agent.", "labels": [], "entities": [{"text": "positive advantage rate (PAR)", "start_pos": 16, "end_pos": 45, "type": "METRIC", "confidence": 0.8724267582098643}]}, {"text": "We will see that most attacks developed in this paper will improve both average score advantage and PAR.", "labels": [], "entities": [{"text": "average score advantage", "start_pos": 72, "end_pos": 95, "type": "METRIC", "confidence": 0.951966921488444}, {"text": "PAR", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9863221645355225}]}, {"text": "Note that this is the first work on attacking a goal-oriented dialogue agent so there is no previous method that could be included in the comparisons.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Competitive negotiation dialogue generated  between agent and human.", "labels": [], "entities": [{"text": "Competitive negotiation dialogue generated  between agent and human", "start_pos": 10, "end_pos": 77, "type": "TASK", "confidence": 0.8400377780199051}]}, {"text": " Table 2: Negotiation task evaluation with different adversarial agent on 2000 randomly generated scenarios, against  the supervised model and reinforcement learning model. The maximum socre is 10. When agents failed to agree,  all agents get 0 score. PAR stands for positive advantage rate. RA+PA+DA stands for the combination of re- active attack, preemptive attacka and delayed attack. RA+PA+TA stands for the combination of reactive attack,  preemptive attacka and transfer attack.", "labels": [], "entities": [{"text": "RA+PA+TA", "start_pos": 389, "end_pos": 397, "type": "METRIC", "confidence": 0.53651043176651}]}, {"text": " Table 3: Dialogue example generated by black-box RL  attack agent against RL agent.", "labels": [], "entities": []}, {"text": " Table 4: Dialogue example generated by reactive attack  agent against RL agent.", "labels": [], "entities": []}, {"text": " Table 5: Dialogue example generated by RA+PA+DA  attack agent against RL agent.", "labels": [], "entities": []}, {"text": " Table  6. First, we observe that the adversarial trained  model achieves much better performance against  black-box RL attack; the advantage of RL attack  drops from 2.32 to \u22121.8. Moreover, the model  achieves consistently better performance against  other white-box attacks. For instance, the advan- tage of the strongest RA+PA+DA attack is reduced  from 5.78 to 3.98.", "labels": [], "entities": []}, {"text": " Table 6: Negotiation task evaluation with different ad- versarial agent on 2000 randomly generated scenarios,  against adversarial trained model.", "labels": [], "entities": []}, {"text": " Table 7. We observe that the performance of  white-box attacks are quite consistent with differ- ent choices of n. This probably indicates that there  the best n varies for different cases. Therefore, if  we could change the n from case to case adap- tively, which is done by delayed attack, we could  see a performance boost.", "labels": [], "entities": []}, {"text": " Table 7: Negotiation task evaluation with different  choices of n against RL model.", "labels": [], "entities": []}]}