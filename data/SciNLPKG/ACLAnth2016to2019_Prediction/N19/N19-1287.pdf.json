{"title": [{"text": "Document-Level Event Factuality Identification via Adversarial Neural Network", "labels": [], "entities": [{"text": "Document-Level Event Factuality Identification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.838611900806427}]}], "abstractContent": [{"text": "Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Document-level event factuality identification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7268646210432053}, {"text": "discourse understanding", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7121557593345642}]}, {"text": "Previous studies mainly suffer from the scarcity of suitable corpus and effective methods.", "labels": [], "entities": []}, {"text": "To solve these two issues, we first construct a corpus annotated with both document-and sentence-level event factuality information on both En-glish and Chinese texts.", "labels": [], "entities": []}, {"text": "Then we present an LSTM neural network based on adversarial training with both intra-and inter-sequence attentions to identify document-level event fac-tuality.", "labels": [], "entities": []}, {"text": "Experimental results show that our neural network model can outperform various baselines on the constructed corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document-level event factuality identification is the task of deciding the commitment of relevant sources towards the factual nature of an event, and to determine whether an event is a fact, a possibility, or an impossible situation from the view of document.", "labels": [], "entities": [{"text": "Document-level event factuality identification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7142847031354904}]}, {"text": "Identifying document-level factuality of events requires comprehensive understanding of documents.", "labels": [], "entities": []}, {"text": "As illustrated in where events are in bold, the event \"reach\" (including its other forms) have various factuality values in different sentences.", "labels": [], "entities": []}, {"text": "For example, in paragraph 2, \"reach\" is impossible/CT-according to the negative word \"denied\", while in paragraph 3, \"reach\" is possible/PS+ due to the speculative word \"may\".", "labels": [], "entities": [{"text": "CT-according", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.9872139692306519}]}, {"text": "The main contents of this document is \"Mexico denied that they will reach an agreement with the U.S. on the new trade deal\", and the documentlevel factuality of the event \"reach\" is CT-.", "labels": [], "entities": [{"text": "CT-", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9787240326404572}]}, {"text": "Document-level event factuality identification is fundamental for document-level NLP applications, such as machine reading comprehension, which aims to have machines read a text passage According to Politico.com, it is said the United States will reach(CT+) an agreement with Mexico on the new trade deal that will replace North American Free Trade Agreement (NAFTA) before December, 2017.", "labels": [], "entities": [{"text": "Document-level event factuality identification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7842339724302292}]}, {"text": "However, Mexican Economy Minister Ildefonso Guajardo denied that they plan to reach(CT-) any agreement with the U.S. on the trade deal talks.", "labels": [], "entities": []}, {"text": "\"We are not going to sacrifice the quality of an agreement because of pressure of time.", "labels": [], "entities": []}, {"text": "We will keep engaged.\" he said.", "labels": [], "entities": []}, {"text": "Just two days ago, Guajardo said the two sides may reach(PS+) an agreement within hours.", "labels": [], "entities": []}, {"text": "The government has not been informed that any agreement will be reached(CT-) yet, said another two Mexican officials.", "labels": [], "entities": [{"text": "CT-)", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.8685383200645447}]}, {"text": "During the past few weeks, the U.S. has been negotiating with Mexico on the new trade deal and has achieved much progress.", "labels": [], "entities": []}, {"text": "Thus, some media speculate that they will possibly reach(PS+) an agreement.", "labels": [], "entities": []}, {"text": "But now it seems that the negotiations will continue before they can get a good deal.", "labels": [], "entities": []}, {"text": "(Time: November, 2017) (Document-level factuality of the event \"reach\" is CT-.): An example document with both sentenceand document-level event factuality. and then answer questions about the text.", "labels": [], "entities": []}, {"text": "According to the document in, the answer of the following question should be \"No\", which is consistent with the document-level factuality of the event \"reach\" (CT-): Q: Does the U.S. reach an agreement with Mexico on the new trade deal before December 2017?", "labels": [], "entities": []}, {"text": "A: No. Previous studies mostly reported on sentencelevel event factuality identification tasks.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9929707646369934}, {"text": "sentencelevel event factuality identification tasks", "start_pos": 43, "end_pos": 94, "type": "TASK", "confidence": 0.7017066180706024}]}, {"text": "On one hand, due to the scarcity of document-level event factuality corpus, these studies only considered the corpora annotated with sentence-level event factuality information, such as ACE 2005 1 , LU (,, and UDS-IH2 (.", "labels": [], "entities": [{"text": "ACE 2005 1", "start_pos": 186, "end_pos": 196, "type": "DATASET", "confidence": 0.9366154670715332}]}, {"text": "On the other hand, previous studies only con-sidered information within sentences, using rules, machine learning models, and combinations of them () for modeling.", "labels": [], "entities": []}, {"text": "Neural network models have also recently been used for the sentence-level event factuality identification (.", "labels": [], "entities": [{"text": "sentence-level event factuality identification", "start_pos": 59, "end_pos": 105, "type": "TASK", "confidence": 0.6679367870092392}]}, {"text": "According to, document-level event factuality cannot be deduced from each sentence-level factuality separately, but depends on the comprehensive semantic information of sentences.", "labels": [], "entities": []}, {"text": "However, no suitable model for document-level task has been proposed yet.", "labels": [], "entities": []}, {"text": "To solve the issues above, this paper focuses on document-level event factuality identification.", "labels": [], "entities": [{"text": "document-level event factuality identification", "start_pos": 49, "end_pos": 95, "type": "TASK", "confidence": 0.6286768168210983}]}, {"text": "Our contributions can be summarized as follows.", "labels": [], "entities": []}, {"text": "1) We construct a document-level event factuality corpus, i.e. DLEF, on both English and Chinese texts.", "labels": [], "entities": []}, {"text": "To our best knowledge, this is the first document-level event factuality corpus.", "labels": [], "entities": []}, {"text": "The statistics on the corpora and the experimental results show that our corpus can sufficiently reflect linguistic characteristics of news texts, and provide adequate support on resource for research.", "labels": [], "entities": []}, {"text": "2) We propose an LSTM neural network with both intra-and inter-sequence attentions to identify document-level event factuality, and consider dependency paths from speculative and negative cues to the event and sentences containing the event as features.", "labels": [], "entities": []}, {"text": "Due to the diversity of various contents of the texts in DLEF corpus, we employ Adversarial Training to improve the robustness of our model.", "labels": [], "entities": [{"text": "DLEF corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8725564181804657}]}, {"text": "Experimental results show that our model is superior to various baselines.", "labels": [], "entities": []}, {"text": "The corpus and code of this paper will be released at https://github.com/qz011/dlef.", "labels": [], "entities": []}], "datasetContent": [{"text": "We introduce the experimental settings and the baselines, finally presenting the experimental results and analysis in detail.", "labels": [], "entities": []}, {"text": "The PS-and Uu documents only cover 1.39% and 1.20% in our English and Chinese corpus, respectively.", "labels": [], "entities": [{"text": "PS-and Uu documents", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.6657291054725647}]}, {"text": "Therefore, we mainly focus on the performance of CT+, CT-, and PS+.", "labels": [], "entities": []}, {"text": "For fair comparison, we perform 10-fold crossvalidation on English and Chinese corpora, respectively.", "labels": [], "entities": []}, {"text": "In addition to Precision, Recall, and F1-measure for each category of factuality value, we consider macro-and micro-averaging to obtain the overall performance of all the categories of factuality values.", "labels": [], "entities": [{"text": "Precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9986416697502136}, {"text": "Recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9565173983573914}, {"text": "F1-measure", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9987427592277527}]}, {"text": "The hidden units of LSTM are set as n h = 50.", "labels": [], "entities": []}, {"text": "We initialize word embeddings via Word2Vec (), setting the dimensions as d 0 = 100, and fine-tuning them during training.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9706459641456604}]}, {"text": "SGD with momentum is applied to optimize our models.", "labels": [], "entities": []}, {"text": "Att 2 and Att 2+AT are the models proposed in Section 3 that consider the contexts, i.e., one sentence before and after the current sentence containing the event as the input.", "labels": [], "entities": [{"text": "AT", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.6676805019378662}]}, {"text": "Compared to Att 2, Att 2+AT considers Adversarial Training (AT, the same below).", "labels": [], "entities": [{"text": "AT", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9034743905067444}, {"text": "AT", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.990955650806427}]}, {"text": "We also consider the following baselines for the comparison with our models: MaxEntVote is a maximum entropy model that only considers the view of AUTHOR (de Marneffe et al., 2012).", "labels": [], "entities": [{"text": "AUTHOR", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.5547374486923218}]}, {"text": "We use maximum entropy model to identify sentence-level event factuality, and consider voting mechanism, i.e., choose the value committed by the most sentences as the document-level factuality value.", "labels": [], "entities": []}, {"text": "We also consider other machine learning models, e.g., but obtain lower micro-/macro-averaged F1 on and Chinese corpus.", "labels": [], "entities": [{"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.7994298934936523}, {"text": "Chinese corpus", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9401925802230835}]}, {"text": "SentVote identifies sentence-level event factuality, and does not consider inter-sequence attention in the model proposed in Section 3.", "labels": [], "entities": []}, {"text": "Similar to MaxEntVote model, voting mechanism is used to identify document-level event factuality in this SentVote model.", "labels": [], "entities": []}, {"text": "MP 2 considers Max-Pooling instead of attention compared with Att 2.", "labels": [], "entities": []}, {"text": "Att 1 considers only intra-sequence attention, but not the inter-sequence attention.", "labels": [], "entities": []}, {"text": "For an event, we concatenate its i dependency paths and j sentences into one path and one sentence as the input, respectively.", "labels": [], "entities": []}, {"text": "presents the performances of our models and baselines.", "labels": [], "entities": []}, {"text": "MaxEntVote gives relatively lower results than other models, especially on CTand PS+.", "labels": [], "entities": [{"text": "CTand PS+", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8474273085594177}]}, {"text": "SentVote models are better than MaxEntVote, but still obtain lower results than Att 2, which can prove that inter-sequence attention is more useful than voting.", "labels": [], "entities": []}, {"text": "Max-pooling only selects the most active information for each dimension of features, While attention takes into account all the features and assigns weights for them according their degrees of importance.", "labels": [], "entities": []}, {"text": "Hence, Att 2 gets better results than MP L2.", "labels": [], "entities": []}, {"text": "Att 1 only considers the intra-sequence attention and obtains lower results than Att 2, which proves the effectiveness of inter-sequence attention.", "labels": [], "entities": []}, {"text": "Att 2 and Att 2+AT achieve better results than other baselines.", "labels": [], "entities": [{"text": "AT", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.801841676235199}]}, {"text": "Compared to Att 2, Att 2+AT considers the adversarial perturbation and training that can alleviate overfitting.", "labels": [], "entities": [{"text": "AT", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8395374417304993}]}, {"text": "Therefore, Att 2+AT is superior to Att 2, which can prove the effectiveness of adversarial training.", "labels": [], "entities": [{"text": "AT", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.7787808775901794}]}], "tableCaptions": [{"text": " Table 2: Statistics of the documents in DLEF corpus  with n types of sentence-level event factuality values.", "labels": [], "entities": [{"text": "DLEF corpus", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9333421587944031}]}, {"text": " Table 3: Statistics of DLEF corpus. The units of length  of English and Chinese texts are tokens and Chinese  characters, respectively.", "labels": [], "entities": [{"text": "DLEF corpus", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9264765381813049}]}, {"text": " Table 4: Inter-annotator agreement of event factuality.", "labels": [], "entities": []}, {"text": " Table 5: F1-measures of baselines and our model.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9978442192077637}]}, {"text": " Table 6: F1-measures of Att 2+AT with different input features.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9980272650718689}, {"text": "AT", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.7265716791152954}]}, {"text": " Table 6. Att 2+AT achieves the best re- sults when considering both paths and sentences", "labels": [], "entities": [{"text": "AT", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8381388187408447}, {"text": "re-", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9560824632644653}]}, {"text": " Table 7: F1-measures of Att 2+AT on the documents with n types of sentence-level factuality values.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.998746395111084}, {"text": "Att 2+AT", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8288450688123703}]}, {"text": " Table 8: F1-measures of Att 2+AT with joint optimization.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9986435770988464}, {"text": "Att", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9710469841957092}, {"text": "AT", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.4996437728404999}]}]}