{"title": [{"text": "Incorporating Context and External Knowledge for Pronoun Coreference Resolution", "labels": [], "entities": [{"text": "Pronoun Coreference Resolution", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8932667970657349}]}], "abstractContent": [{"text": "Linking pronominal expressions to the correct references requires, in many cases, better analysis of the contextual information and external knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we propose a two-layer model for pronoun coreference resolution that leverages both context and external knowledge, where a knowledge attention mechanism is designed to ensure the model leveraging the appropriate source of external knowledge based on different context.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.883283774058024}]}, {"text": "Experimental results demonstrate the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin.", "labels": [], "entities": [{"text": "validity", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9579454660415649}]}], "introductionContent": [{"text": "The question of how human beings resolve pronouns has long been of interest to both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning and brings challenges in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 233, "end_pos": 263, "type": "TASK", "confidence": 0.6864444216092428}]}, {"text": "To explore solutions for that question, pronoun coreference resolution was proposed.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8622065981229147}]}, {"text": "As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference fora given pronominal anaphor in the context and has been shown to be crucial fora series of downstream tasks, including machine translation (, summarization (), information extraction (, and dialog systems (.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.8930006821950277}, {"text": "pronoun coreference resolution", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.7935391863187155}, {"text": "machine translation", "start_pos": 264, "end_pos": 283, "type": "TASK", "confidence": 0.83737912774086}, {"text": "summarization", "start_pos": 287, "end_pos": 300, "type": "TASK", "confidence": 0.9862638711929321}, {"text": "information extraction", "start_pos": 305, "end_pos": 327, "type": "TASK", "confidence": 0.8490066230297089}]}, {"text": "Conventionally, people design rules or use features () to resolve the pronoun coreferences.", "labels": [], "entities": []}, {"text": "* This work was done during the internship of the first author in Tencent AI Lab.", "labels": [], "entities": [{"text": "Tencent AI Lab", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.6349233587582906}]}, {"text": "These methods heavily rely on the coverage and quality of the manually defined rules and features.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9613630771636963}]}, {"text": "Until recently, end-to-end solution ( ) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information.", "labels": [], "entities": []}, {"text": "However, training such models on annotated corpora can be biased and normally does not consider external knowledge.", "labels": [], "entities": []}, {"text": "Despite the great efforts made in this area in the past few decades, pronoun coreference resolution remains challenging.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.8691177368164062}]}, {"text": "The reason behind is that the correct resolution of pronouns can be influenced by many factors; many resolution decisions require reasoning upon different contextual and external knowledge, which is also proved in other NLP tasks.", "labels": [], "entities": []}, {"text": "demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that 'them' refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where 'she' can only refer to a female person (girl); Example C requires a more general type of knowledge 1 that 'cats can climb trees but a dog normally does not'.", "labels": [], "entities": []}, {"text": "All of these knowledge are difficult to be learned from training data.", "labels": [], "entities": []}, {"text": "Considering the importance of both contextual information and external human knowledge, how to jointly leverage them becomes an important question for pronoun coreference resolution.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.8441842993100485}]}, {"text": "In this paper, we propose a two-layer model to address the question while solving two challenges of incorporating external knowledge into deep models for pronoun coreference resolution, where the challenges include: first, different cases have their knowledge preference, i.e., some knowledge is exclusively important for certain cases, which requires the model to be flexible in selecting appropriate knowledge per case; second, the availability of knowledge resources is limited and such resources normally contain noise, which requires the model to be robust in learning from them.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.8576843937238058}]}, {"text": "Consequently, in our model, the first layer predicts the relations between candidate noun phrases and the target pronoun based on the contextual information learned by neural networks.", "labels": [], "entities": []}, {"text": "The second layer compares the candidates pair-wisely, in which we propose a knowledge attention module to focus on appropriate knowledge based on the given context.", "labels": [], "entities": []}, {"text": "Moreover, a softmax pruning is placed in between the two layers to select high confident candidates.", "labels": [], "entities": []}, {"text": "The architecture ensures the model being able to leverage both context and external knowledge.", "labels": [], "entities": []}, {"text": "Especially, compared with conventional approaches that simply treat external knowledge as rules or features, our model is not only more flexible and effective but also interpretable as it reflects which knowledge source has the higher weight in order to make the decision.", "labels": [], "entities": []}, {"text": "Experiments are conducted on a widely used evaluation dataset, where the results prove that the proposed model outperforms all baseline models by a great margin.", "labels": [], "entities": []}, {"text": "Above all, to summarize, this paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a two-layer neural model to combine contextual information and external: The architecture of the two-layer model for pronoun coreference resolution.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 128, "end_pos": 158, "type": "TASK", "confidence": 0.866365393002828}]}, {"text": "The first layer encodes the contextual information for computing F c . The second layer leverages external knowledge to score F k . A pruning layer is applied in between the two layers to control computational complexity.", "labels": [], "entities": []}, {"text": "The dashed boxes in the first and second layer refer to span representation and knowledge scoring, respectively.", "labels": [], "entities": [{"text": "span representation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8368651270866394}, {"text": "knowledge scoring", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7555217742919922}]}, {"text": "knowledge for the pronoun coreference resolution task.", "labels": [], "entities": [{"text": "pronoun coreference resolution task", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7584245055913925}]}, {"text": "2. We propose a knowledge attention mechanism that allows the model to select salient knowledge for different context, which predicts more precisely and can be interpretable through the learned attention scores.", "labels": [], "entities": []}, {"text": "3. With our proposed model, the performance of pronoun coreference resolution is boosted by a great margin over the state-of-the-art models.", "labels": [], "entities": [{"text": "pronoun coreference resolution", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.8473991354306539}]}], "datasetContent": [{"text": "Moreover, the comparison between the two variants of our models is also interesting, where the final two-layer model outperforms the Feature Concatenation model.", "labels": [], "entities": []}, {"text": "It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework.", "labels": [], "entities": []}, {"text": "The reason   behind this result is mainly from the noise in the knowledge source, e.g., parsing error, incorrectly identified relations, etc.", "labels": [], "entities": []}, {"text": "For example, the plurality of 17% noun phrases are wrongly labeled in the test data.", "labels": [], "entities": []}, {"text": "As a comparison, our knowledge attention might contribute to alleviate such noise when incorporating all knowledge sources.", "labels": [], "entities": []}, {"text": "Effect of Different Knowledge To illustrate the importance of different knowledge sources and the knowledge attention mechanism, we ablate various components of our model and report the corresponding F1 scores on the test data.", "labels": [], "entities": [{"text": "F1", "start_pos": 200, "end_pos": 202, "type": "METRIC", "confidence": 0.9976885318756104}]}, {"text": "The results are shown in, which clearly show the necessity of the knowledge.", "labels": [], "entities": []}, {"text": "Interestingly, AG contributes the most among all knowledge types, which indicates that potentially more cases in the evaluation dataset demand on the AG knowledge than others.", "labels": [], "entities": []}, {"text": "More importantly, the results also prove the effectiveness of the knowledge attention module, which contributes to the performance gap between our model and the Feature Concatenation one.", "labels": [], "entities": []}, {"text": "Effect of Different Pruning Thresholds We try different thresholds t for the softmax pruning in selecting reliable candidates.", "labels": [], "entities": []}, {"text": "The effects of different thresholds on reducing candidates and overall performance are shown in and 6 respectively.", "labels": [], "entities": []}, {"text": "Along with the increase oft, both the max and the average number of pruned candidates drop quickly, so that the space complexity of the model can be reduced accordingly.", "labels": [], "entities": [{"text": "max", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9977188110351562}]}, {"text": "Particularly, there are as much as 80% candidates can be filtered out when t = 10 \u22121 . Meanwhile, when referring to, it is observed that the model performs stable with the decreasing of candidate numbers.", "labels": [], "entities": []}, {"text": "Not surprisingly, the precision rises when reducing candidate numbers, yet the recall drops dramatically, eventually results in the drop of F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996676445007324}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9995255470275879}, {"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9996616840362549}]}, {"text": "With the above observations, the reason we sett = 10 \u22127 as the default threshold is straightforward: on this value, one-third candidates are pruned with almost no influence on the model performance in terms of precision, recall, and the F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9995761513710022}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9996567964553833}, {"text": "F1 score", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9850838780403137}]}], "tableCaptions": [{"text": " Table 1: Statistics of the evaluation dataset. Number of  selected pronouns are reported.", "labels": [], "entities": []}, {"text": " Table 2: Pronoun coreference resolution performance of different models on the evaluation dataset. Precision (P),  recall (R), and F1 score are reported, with the best one in each F1 column marked bold.", "labels": [], "entities": [{"text": "Pronoun coreference resolution", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7927236557006836}, {"text": "Precision (P)", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.9656146466732025}, {"text": "recall (R)", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9619591981172562}, {"text": "F1 score", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9886956214904785}, {"text": "F1", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.9517161846160889}]}]}