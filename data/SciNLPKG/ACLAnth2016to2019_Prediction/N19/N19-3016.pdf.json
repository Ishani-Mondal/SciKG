{"title": [{"text": "Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "labels": [], "entities": [{"text": "Word-level Word Representations", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.5643336375554403}]}], "abstractContent": [{"text": "In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations.", "labels": [], "entities": []}, {"text": "We provide strong empirical evidence that model-ing characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words.", "labels": [], "entities": []}, {"text": "We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets.", "labels": [], "entities": []}, {"text": "Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks.", "labels": [], "entities": []}, {"text": "Our code is available at https: //github.com/jabalazs/gating.", "labels": [], "entities": []}], "introductionContent": [{"text": "Incorporating sub-word structures like substrings, morphemes and characters to the creation of word representations significantly increases their quality as reflected both by intrinsic metrics and performance in a wide range of downstream tasks (.", "labels": [], "entities": []}, {"text": "The reason for this improvement is related to sub-word structures containing information that is usually ignored by standard word-level models.", "labels": [], "entities": []}, {"text": "Indeed, when representing words as vectors extracted from a lookup table, semantically related words resulting from inflectional processes such as surf, surfing, and surfed, are treated as being independent from one another . Further, wordlevel embeddings do not account for derivational processes resulting in syntactically-similar words with different meanings such as break, breakable, and unbreakable.", "labels": [], "entities": []}, {"text": "This causes derived words, which are usually less frequent, to have lowerquality (or no) vector representations.", "labels": [], "entities": []}, {"text": "Previous works have successfully combined character-level and word-level word representations, obtaining overall better results than using only word-level representations.", "labels": [], "entities": []}, {"text": "For example achieved state-of-the-art results in a machine translation task by representing unknown words as a composition of their characters.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.787199874718984}]}, {"text": "What these works have in common is that the models they describe first learn how to represent subword information, at character (, morpheme, or substring () levels, and then combine these learned representations at the word level.", "labels": [], "entities": []}, {"text": "The incorporation of information at a finer-grained hierarchy results in higher-quality modeling of rare words, morphological processes, and semantics.", "labels": [], "entities": []}, {"text": "There is no consensus, however, on which combination method works better in which case, or how the choice of a combination method affects downstream performance, either measured intrinsically at the word level, or extrinsically at the sen-tence level.", "labels": [], "entities": []}, {"text": "In this paper we aim to provide some intuitions about how the choice of mechanism for combining character-level with word-level representations influences the quality of the final word representations, and the subsequent effect these have in the performance of downstream tasks.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We show that a feature-wise sigmoidal gating mechanism is the best at combining representations at the character and word-level hierarchies, as measured byword similarity tasks.", "labels": [], "entities": []}, {"text": "\u2022 We provide evidence that this mechanism learns that to properly model increasingly infrequent words, it has to increasingly rely on character-level information.", "labels": [], "entities": []}, {"text": "\u2022 We finally show that despite the increased expressivity of word representations it offers, it has no clear effect in sentence representations, as measured by sentence evaluation tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our models for solving the Natural Language Inference (NLI) task in two datasets, SNLI (Bowman et al., 2015) and MultiNLI (, and validated them in each corresponding development set (including the matched and mismatched development sets of MultiNLI).", "labels": [], "entities": [{"text": "solving the Natural Language Inference (NLI) task", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.7067998515235053}]}, {"text": "For each dataset-method combination we trained 7 models initialized with different random seeds, and saved each when it reached its best validation accuracy . We then evaluated the quality of each trained model's word representations vi in 10 word similarity tasks, using the system created by . Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks ().", "labels": [], "entities": []}, {"text": "Word-level Semantic Similarity A desirable property of vector representations of words is that semantically similar words should have similar vector representations.", "labels": [], "entities": [{"text": "Word-level Semantic Similarity", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5629081825415293}]}, {"text": "Assessing whether a set of word representations possesses this quality is referred to as the semantic similarity task.", "labels": [], "entities": []}, {"text": "This is the most widely-used evaluation method for evaluating word representations, despite its shortcomings (.", "labels": [], "entities": [{"text": "evaluating word representations", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.5967552264531454}]}, {"text": "This task consists of comparing the similarity between word vectors measured by a distance si ] for each i, and both \u2212 \u2192 si and \u2190 \u2212 si \u2208 R ds 2 . We found that models validated on the matched development set of MultiNLI, rather than the mismatched, yielded best results, although the differences were not statistically significant.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 211, "end_pos": 219, "type": "DATASET", "confidence": 0.9257338643074036}]}, {"text": "metric (usually cosine distance), with a similarity score obtained from human judgements.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.9711814522743225}]}, {"text": "High correlation between these similarities is an indicator of good performance.", "labels": [], "entities": []}, {"text": "A problem with this formulation though, is that the definition of \"similarity\" often confounds the meaning of both similarity and relatedness.", "labels": [], "entities": []}, {"text": "For example, cup and tea are related but dissimilar words, and this type of distinction is not always clear (.", "labels": [], "entities": []}, {"text": "To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and somewhere the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353).", "labels": [], "entities": [{"text": "WS353R", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.8976397514343262}, {"text": "WS353S", "start_pos": 184, "end_pos": 190, "type": "DATASET", "confidence": 0.9435251951217651}, {"text": "MEN", "start_pos": 261, "end_pos": 264, "type": "DATASET", "confidence": 0.8531267046928406}, {"text": "MTurk287", "start_pos": 266, "end_pos": 274, "type": "DATASET", "confidence": 0.9189863801002502}, {"text": "MTurk771", "start_pos": 276, "end_pos": 284, "type": "DATASET", "confidence": 0.9039307832717896}, {"text": "WS353", "start_pos": 290, "end_pos": 295, "type": "DATASET", "confidence": 0.882710337638855}]}, {"text": "We also included the RareWords (RW) dataset for evaluating the quality of rare word representations.", "labels": [], "entities": [{"text": "RareWords (RW) dataset", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7931298971176147}]}, {"text": "See appendix B fora more complete description of the datasets we used.", "labels": [], "entities": []}, {"text": "Sentence-level Evaluation Tasks Unlike wordlevel representations, there is no consensus on the desirable properties sentence representations should have.", "labels": [], "entities": [{"text": "Sentence-level Evaluation Tasks", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8224741617838541}]}, {"text": "In response to this, created SentEval 7 , a sentence representation evaluation benchmark designed for assessing how well sentence representations perform in various downstream tasks (.", "labels": [], "entities": []}, {"text": "Some of the datasets included in SentEval correspond to sentiment classification (CR, MPQA, MR, SST2, and SST5), subjectivity classification (SUBJ), question-type classification (TREC), recognizing textual entailment (SICK E), estimating semantic relatedness (SICK R), and measuring textual semantic similarity (STS16, STSB).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8206404447555542}, {"text": "question-type classification", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.7052470147609711}]}, {"text": "The datasets are described by, and we provide pointers to their original sources in the appendix table B.2.", "labels": [], "entities": []}, {"text": "To evaluate these sentence representations SentEval trained a linear model on top of them, and evaluated their performance in the validation sets accompanying each dataset.", "labels": [], "entities": []}, {"text": "The only exception was the STS16 task, in which our representations were evaluated directly.", "labels": [], "entities": []}, {"text": "Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset.", "labels": [], "entities": []}, {"text": "For each task and dataset, every best-performing method was significantly different to other methods (p < 0.05), except for w trained in SNLI at the MTurk287 task.", "labels": [], "entities": [{"text": "MTurk287 task", "start_pos": 149, "end_pos": 162, "type": "DATASET", "confidence": 0.823472410440445}]}, {"text": "Statistical significance was obtained with a two-sided Welch's t-test for two independent samples without assuming equal variance.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.6116631031036377}]}, {"text": "We can observe the same trend mentioned in section 4.1, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in SNLI at a statistically significant level, confirming the findings of.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.8241541981697083}]}, {"text": "In other words, training sentence encoders on MultiNLI yields more general sentence representations than doing soon SNLI.", "labels": [], "entities": []}, {"text": "The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI.", "labels": [], "entities": [{"text": "SICKE", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7104965448379517}, {"text": "SICKR", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.6493932008743286}, {"text": "SNLI", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.920497715473175}]}, {"text": "We hypothesize this is again due to both SNLI and SICK) having similar data distributions . Additionally, there was no method that significantly outperformed the word only baseline in classification tasks.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.7911215424537659}]}, {"text": "This means that the added expressivity offered by explicitly modeling characters, be it through concatenation or gating, was not significantly better than simply fine-tuning the pre-trained GloVe embeddings for this type of task.", "labels": [], "entities": []}, {"text": "We hypothesize this is due to the conflation of two effects.", "labels": [], "entities": []}, {"text": "First, the fact that morphological processes might not encode important information for solving these tasks; and second, that SNLI and MultiNLI belong to domains that are too dissimilar to the domains in which the sentence representations are being tested.", "labels": [], "entities": []}, {"text": "On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI.", "labels": [], "entities": [{"text": "STSB task", "start_pos": 88, "end_pos": 97, "type": "TASK", "confidence": 0.7762146294116974}, {"text": "SNLI", "start_pos": 167, "end_pos": 171, "type": "DATASET", "confidence": 0.7693136930465698}]}, {"text": "This again hints at this method being capable of modeling phenomena at the word level, resulting in improved semantic representations at the sentence level.", "labels": [], "entities": []}, {"text": "However, the same cannot be said about sentence-level evaluation performance; there is no clear correlation between word similarity tasks and sentence-evaluation tasks.", "labels": [], "entities": []}, {"text": "This is clearly illustrated by performance in the STSBenchmark, the only in which the vector gate was significantly superior, not being correlated with performance in any word-similarity dataset.", "labels": [], "entities": []}, {"text": "This can be interpreted simply as word-level representations capturing word-similarity not being a sufficient condition for good performance in sentence-level tasks.", "labels": [], "entities": []}, {"text": "In general, shows that there are no general correlation effects spanning both training datasets and combination mechanisms.", "labels": [], "entities": []}, {"text": "For example, shows that, for both word-only and concat models trained in SNLI, performance in word similarity tasks correlates positively with performance inmost sentence evaluation tasks, however, this does not happen as clearly for the same models trained in MultiNLI ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models  initialized with different random seeds. Correlations were scaled to the [\u2212100; 100] range for easier reading.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 75, "end_pos": 94, "type": "METRIC", "confidence": 0.9690433144569397}]}, {"text": " Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized  with different random seeds. Values represent accuracy (%) unless indicated by  \u2020, in which case they represent  Pearson correlation scaled to the range [\u2212100, 100] for easier reading. Bold values represent the best method  per training dataset, per task; underlined values represent the best-performing method per task, independent of  training dataset. Values marked with an asterisk (  *  ) are significantly different to the average performance of the  best model trained on the same dataset (p < 0.05). Results for every best-performing method trained on one  dataset are significantly different to the best-performing method trained on the other. Statistical significance was  obtained in the same way as described in table 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9991940855979919}, {"text": "Pearson correlation", "start_pos": 232, "end_pos": 251, "type": "METRIC", "confidence": 0.9594326019287109}]}]}