{"title": [{"text": "Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation", "labels": [], "entities": [{"text": "Vector of Locally-Aggregated Word Embeddings (VLAWE)", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.5580355115234852}]}], "abstractContent": [{"text": "In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document em-beddings.", "labels": [], "entities": []}, {"text": "Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows.", "labels": [], "entities": [{"text": "image representation", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7252519577741623}]}, {"text": "First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings.", "labels": [], "entities": []}, {"text": "Each word embedding is then associated to its nearest cluster centroid (codeword).", "labels": [], "entities": []}, {"text": "The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword.", "labels": [], "entities": []}, {"text": "We plug the VLAWE representation, which is learned in an unsuper-vised manner, into a classifier and show that it is useful fora diverse set of text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 144, "end_pos": 169, "type": "TASK", "confidence": 0.7921552260716757}]}, {"text": "We compare our approach with abroad range of recent state-of-the-art methods , demonstrating the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.", "labels": [], "entities": [{"text": "Movie Review data set", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.9334373325109482}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9997106194496155}]}, {"text": "Our code is available at https://github.com/raduionescu/vlawe-boswe/.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, word embeddings () have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis, information retrieval and word sense disambiguation (), among many others.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7992630402247111}, {"text": "sentiment analysis", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.9578616619110107}, {"text": "information retrieval", "start_pos": 171, "end_pos": 192, "type": "TASK", "confidence": 0.8196689784526825}, {"text": "word sense disambiguation", "start_pos": 197, "end_pos": 222, "type": "TASK", "confidence": 0.6739024122556051}]}, {"text": "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations (.", "labels": [], "entities": []}, {"text": "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity, it seems that more complex approaches usually yield better performance (.", "labels": [], "entities": []}, {"text": "To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.", "labels": [], "entities": []}, {"text": "Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) () used in computer vision to efficiently represent images for various image classification and retrieval tasks.", "labels": [], "entities": [{"text": "image classification and retrieval", "start_pos": 151, "end_pos": 185, "type": "TASK", "confidence": 0.8082378655672073}]}, {"text": "To our knowledge, we are the first to adapt and use VLAD in the text domain.", "labels": [], "entities": []}, {"text": "Our document-level representation is constructed as follows.", "labels": [], "entities": []}, {"text": "First, we apply a pre-trained word embedding model, such as GloVe (), on all the words from a set of training documents in order to obtain a set of training word vectors.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.7820500135421753}]}, {"text": "The word vectors are clustered by k-means in order to learn a codebook of semnatically-related word embeddings.", "labels": [], "entities": []}, {"text": "Each word embedding is then associated to its nearest cluster centroid (codeword).", "labels": [], "entities": []}, {"text": "The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a text document is then com-puted by accumulating the differences between each codeword vector and each word vector that is both present in the document and associated to the respective codeword.", "labels": [], "entities": []}, {"text": "Since our approach considers cluster centroids as reference for building the representation, it can easily accommodate new words, not seen during k-means training, simply by associating them to the nearest cluster centroids.", "labels": [], "entities": []}, {"text": "Thus, VLAWE is robust to vocabulary distribution gaps between training and test, which can appear when the training set is particularly smaller or from a different domain.", "labels": [], "entities": []}, {"text": "Certainly, the robustness holds as long as the word embeddings are pretrained on a very large set of documents, e.g. the entire Wikipedia.", "labels": [], "entities": []}, {"text": "We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful fora diverse set of text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 182, "end_pos": 207, "type": "TASK", "confidence": 0.7914278904596964}]}, {"text": "We consider five benchmark data sets: Reuters-21578, RT-2k (), MR (Pang and), TREC () and Subj ().", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9541596174240112}, {"text": "RT-2k", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.506932258605957}, {"text": "MR", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9928194880485535}, {"text": "TREC", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9930015802383423}]}, {"text": "We compare VLAWE with recent stateof-the-art methods (, demonstrating the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of by almost 10%.", "labels": [], "entities": [{"text": "Movie Review (MR) data set", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.6817986156259265}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present related works on learning documentlevel representations in Section 2.", "labels": [], "entities": []}, {"text": "We describe the Vector of Locally-Aggregated Word Embeddings in Section 3.", "labels": [], "entities": []}, {"text": "We present experiments and results on various text classification tasks in Section 4.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8577695687611898}]}, {"text": "Finally, we draw our conclusion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we used the pre-trained word embeddings computed with the GloVe toolkit provided by.", "labels": [], "entities": []}, {"text": "The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases.", "labels": [], "entities": []}, {"text": "Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library.", "labels": [], "entities": []}, {"text": "We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k \u00b7 d = 10 \u00b7 300 = 3000 components.", "labels": [], "entities": [{"text": "VLAWE", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.7558388113975525}]}, {"text": "Similar to, we set \u03b1 = 0.5 for the power normalization step in Equation   Support Vector Machines (SVM) implementation provided by).", "labels": [], "entities": []}, {"text": "We set the SVM regularization parameter to C = 1 in all our experiments.", "labels": [], "entities": [{"text": "SVM regularization", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7134126126766205}]}, {"text": "In the SVM, we use the linear kernel.", "labels": [], "entities": []}, {"text": "For optimal results, the VLAWE representation is combined with the BOSWE representation ( , which is based on the PQ kernel ().", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9815215468406677}]}, {"text": "We follow the same evaluation procedure as and, using 10-fold cross-validation when a train and test split is not pre-defined fora given data set.", "labels": [], "entities": []}, {"text": "As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9092541138331095}, {"text": "Reuters-21578 data set", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9914679328600565}, {"text": "classification accuracy", "start_pos": 112, "end_pos": 135, "type": "METRIC", "confidence": 0.5818528831005096}, {"text": "RT-2k", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9348849058151245}, {"text": "TREC", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.5582912564277649}, {"text": "Subj data sets", "start_pos": 176, "end_pos": 190, "type": "DATASET", "confidence": 0.9327255884806315}]}], "tableCaptions": []}