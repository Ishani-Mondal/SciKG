{"title": [{"text": "An Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search", "labels": [], "entities": []}], "abstractContent": [{"text": "Globally normalized neural sequence models are considered superior to their locally normalized equivalents because they may ameliorate the effects of label bias.", "labels": [], "entities": []}, {"text": "However, when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing.", "labels": [], "entities": []}, {"text": "Thus, the practical advantage of global normalization in the context of modern neural methods remains unclear.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7004625201225281}]}, {"text": "In this paper, we attempt to shed light on this problem through an empirical study.", "labels": [], "entities": []}, {"text": "We extend an approach for search-aware training via a continuous relaxation of beam search (Goyal et al., 2017b) in order to enable training of globally normalized recurrent sequence models through simple backpropagation.", "labels": [], "entities": []}, {"text": "We then use this technique to conduct an empirical study of the interaction between global normalization, high-capacity encoders, and search-aware optimization.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.6874914467334747}, {"text": "search-aware optimization", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.710777536034584}]}, {"text": "We observe that in the context of inexact search, globally normalized neural models are still more effective than their locally normalized counterparts.", "labels": [], "entities": []}, {"text": "Further, since our training approach is sensitive to warm-starting with pre-trained models , we also propose a novel initialization strategy based on self-normalization for pre-training globally normalized models.", "labels": [], "entities": []}, {"text": "We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global nor-malization under different conditions while using search-aware training.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8403719365596771}]}], "introductionContent": [{"text": "Neural encoder-decoder models have been tremendously successful at a variety of NLP tasks, such as machine translation), parsing), summarization, dialog generation (, and image captioning (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7218636572360992}, {"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.9743959903717041}, {"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9901907444000244}, {"text": "dialog generation", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.8443517684936523}, {"text": "image captioning", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7534042298793793}]}, {"text": "With these models, the target sequence is generated in a left-to-right step-wise manner with the predictions at every step being conditioned on the input sequence and the whole prediction history.", "labels": [], "entities": []}, {"text": "This long-distance memory precludes exact search for the maximally scoring sequence according to the model and therefore, approximate algorithms like greedy search or beam search are necessary in practice during decoding.", "labels": [], "entities": []}, {"text": "In this scenario, it is natural to resort to search-aware learning techniques for these models which makes the optimization objective sensitive to any potential errors that could occur due to inexact search in these models.", "labels": [], "entities": []}, {"text": "This work focuses on comparison between search-aware locally normalized sequence models that involve projecting the scores of items in the vocabulary onto a probability simplex at each step and globally normalized/unnormalized sequence models that involve scoring sequences without explicit normalization at each step.", "labels": [], "entities": []}, {"text": "When conditioned on the the full input sequence and the entire prediction history, both locally normalized and globally normalized conditional models should have same expressive power under a highcapacity neural parametrization in theory, as they can both model same set of distributions overall finite length output sequences.", "labels": [], "entities": []}, {"text": "However, locally normalized models are constrained in how they respond to search errors during training since the scores at each decoding step must sum to one.", "labels": [], "entities": []}, {"text": "To let a search-aware training setup have the most flexibility, abandoning this constraint maybe useful for easier optimization.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that the interaction between approximate inference and nonconvex parameter optimization results in more robust training and better performance for models with global normalization compared to those with the more common locally normalized parametrization.", "labels": [], "entities": []}, {"text": "We posit that this difference is due to label bias arising from the interaction of approximate search and search-aware optimization in locally normalized models.", "labels": [], "entities": []}, {"text": "A commonly understood source of label bias in locally normalized sequence models is an effect of conditioning only on partial input (for example, only the history of the input) at each step during decoding.", "labels": [], "entities": []}, {"text": "We discus another potential source of label bias arising from approximate search with locally normalized models that maybe present even with access to the full input at each step.", "labels": [], "entities": []}, {"text": "To this end, we train search-aware globally and locally normalized models in an end-to-end (sub)-differentiable manner using a continuous relaxation to the discontinuous beam search procedure introduced by.", "labels": [], "entities": []}, {"text": "This approach requires initialization with a suitable globally normalized model to work in practice.", "labels": [], "entities": []}, {"text": "Hence, we also propose an initialization strategy based upon self-normalization for pre-training globally normalized models.", "labels": [], "entities": []}, {"text": "We demonstrate the effect of both sources of label bias through our experiments on two common sequence tasks: CCG supertagging and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7764566242694855}]}, {"text": "We find that label bias can be eliminated by both, using a powerful encoder, and using a globally normalized model.", "labels": [], "entities": []}, {"text": "We observe that global normalization yields performance gains over local normalization and is able to ameliorate label bias especially in scenarios that involve a very large hypothesis space.", "labels": [], "entities": []}], "datasetContent": [{"text": "To empirically analyze the interaction between label bias arising from different sources, searchaware training, and global normalization, we conducted experiments on two tasks with vastly different sizes of output space: CCG supertagging and Machine Translation.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7175524830818176}, {"text": "Machine Translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8153370916843414}]}, {"text": "As described in the next section, the task of tagging allows us to perform controlled experiments which explicitly study the effect of amount of input information available to the decoder at each step, we analyze the scenarios in which search aware training and global normalization are expected to improve the model performance.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 262, "end_pos": 282, "type": "TASK", "confidence": 0.6521270275115967}]}, {"text": "In all our experiments, we report results on training with standard teacher forcing optimization and self-normalization as our baselines.", "labels": [], "entities": []}, {"text": "We report results with both search-aware locally and globally normalized models (Section 3.1) after warm starting with both cross entropy trained models and self-normalized models to study the effects of search-aware optimization and global normalization.", "labels": [], "entities": []}, {"text": "We follow and use the decomposable Hamming loss approximation with search-aware optimization for both the tasks and decode via soft beam search decoding method which involves continuous beam search with soft backpointers for the LSTM Beam search dynamics as described in Section 3, but using identifiable backpointers and labels (using MAP estimates of soft backpointers and labels) to decode.", "labels": [], "entities": []}, {"text": "We tune hyperparameters like learning rate and annealing schedule by observing performance on development sets for both the tasks.", "labels": [], "entities": []}, {"text": "We performed at least three random restarts for each class and report results based on best development performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of logZ between cross entropy  trained models (CE) and self normalized models (L2)  for CCG supertagging and Machine Translation tasks.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8218455612659454}]}, {"text": " Table 3: Accuracy results on CCG supertagging  when initialized with a self normalized model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988517761230469}]}, {"text": " Table 4: BLEU results on de-en Machine Transla- tion. Regular and Self-normalized refer to the initiza- tion scheme for soft beam search training. pretrain- greedy and pretrain-beam refer to the output of decod- ing the initializer model. locally normalized and glob- ally normalized refer to search-aware soft-beam mod- els", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9956929087638855}]}, {"text": " Table 5: Breakdown of BLEU results on de-en  Machine Translation dev set. Reported on Self- normalized initialization", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9949284791946411}, {"text": "initialization", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.5478273630142212}]}, {"text": " Table 6: BLEU scores with different length inputs  on dev set Reported on Self-normalized initialization.  The header specifies the range of length of the input  sentences", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979836940765381}]}]}