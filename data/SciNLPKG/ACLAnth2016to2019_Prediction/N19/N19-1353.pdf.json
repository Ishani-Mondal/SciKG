{"title": [{"text": "Subword-based Compact Reconstruction of Word Embeddings", "labels": [], "entities": [{"text": "Subword-based Compact Reconstruction of Word Embeddings", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7742514361937841}]}], "abstractContent": [{"text": "The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space.", "labels": [], "entities": []}, {"text": "The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism.", "labels": [], "entities": []}, {"text": "Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embed-dings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words.", "labels": [], "entities": []}, {"text": "We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained word embeddings (or embedding vectors), especially those trained on avast amount of text data, such as the Common Crawl (CC) corpus 2 , are now considered as highly beneficial, fundamental language resources.", "labels": [], "entities": [{"text": "Common Crawl (CC) corpus 2", "start_pos": 119, "end_pos": 145, "type": "DATASET", "confidence": 0.7563216005052839}]}, {"text": "Typical examples of large, well-trained word embeddings are those trained on the CC corpus with 600 billion tokens by fastText ( and with 840 billion tokens by), which we refer to as fastText.600B and GloVe.840B 4 , respectively.", "labels": [], "entities": [{"text": "CC corpus", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.8975883424282074}]}, {"text": "In fact, we often Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (, discourse parsing (, semantic parsing (, and semantic role labeling (.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 430, "end_pos": 450, "type": "TASK", "confidence": 0.7731287181377411}, {"text": "discourse parsing", "start_pos": 454, "end_pos": 471, "type": "TASK", "confidence": 0.7209016233682632}, {"text": "semantic parsing", "start_pos": 475, "end_pos": 491, "type": "TASK", "confidence": 0.7196682393550873}, {"text": "semantic role labeling", "start_pos": 499, "end_pos": 521, "type": "TASK", "confidence": 0.6276438335577647}]}, {"text": "Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages.", "labels": [], "entities": []}, {"text": "In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "It is crucial to address such issues, especially when applying them to real-world open systems.", "labels": [], "entities": []}, {"text": "The total number of embeddings (i.e., the total memory requirement of such word embeddings) often becomes unacceptably large, especially in limited-memory environments, including GPUs, since the vocabulary size is more than 2 million words, which require at least 2 gigabytes (GB) of memory for storage.", "labels": [], "entities": []}, {"text": "One possible solution is to merely discard (less important) words from the vocabulary, which can straightforwardly reduce the memory requirement.", "labels": [], "entities": []}, {"text": "However, such a naive method can cause another well-known drawback regarding the inapplicability of OOV words.", "labels": [], "entities": []}, {"text": "The applicability of OOV words is highly desirable in real systems since input words can be uncontrollably diverse.", "labels": [], "entities": []}, {"text": "Therefore, there is a trade-off between the number of embedding vectors and the applicability of OOV words; thus, our goal is to investigate and develop a method that simultaneously has less memory requirement and high applicability of OOV words, which are both desirable properties for word embeddings in real-world open systems.", "labels": [], "entities": []}, {"text": "Recently, methods that leverage subword information have been proposed and have become popular for overcoming the OOV word issue.", "labels": [], "entities": [{"text": "OOV word issue", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.7560098767280579}]}, {"text": "Con-ceptually, the subword-based approach can coverall the words that can be constructed by a combination of subwords.", "labels": [], "entities": []}, {"text": "Thus, the subword-based approach can greatly mitigate (or solve) the OOV word issue.", "labels": [], "entities": [{"text": "OOV word issue", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7312951683998108}]}, {"text": "We extend this approach to simultaneously enabling a reduction in the total number of embedding vectors through the reconstruction of word embeddings by subwords.", "labels": [], "entities": []}, {"text": "The key techniques of our approach are twofold: memoryshared embeddings and a variant of the key-valuequery (KVQ) self-attention mechanism.", "labels": [], "entities": []}, {"text": "That is, our approach reconstructs well-trained word embeddings using a limited number of embedding vectors that are shared by all the subwords with an effective weighting calculated by the self-attention mechanism.", "labels": [], "entities": []}, {"text": "In our experiments, we show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings, such as fastText.600B and GloVe.840B, in a small fixed space while preventing quality degradation across several linguistic benchmark datasets from word similarity and analogy tasks.", "labels": [], "entities": []}, {"text": "We also demonstrate the effectiveness of our reconstructed embeddings for representing the embeddings of OOV words.", "labels": [], "entities": []}, {"text": "Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task.", "labels": [], "entities": [{"text": "named entity recognition task", "start_pos": 104, "end_pos": 133, "type": "TASK", "confidence": 0.6959900185465813}]}], "datasetContent": [{"text": "This section describes our experiments for evaluating the performance of the model shrinkage.", "labels": [], "entities": []}, {"text": "This section describes our experiments for evaluating the performance of OOV word embeddings.", "labels": [], "entities": []}, {"text": "To investigate the effectiveness of our reconstucted embeddings in downstream tasks, we evaluated them in the named entity recognition (NER) and the textual entailment (TE) tasks.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.793780247370402}]}], "tableCaptions": [{"text": " Table 2: Evaluation datasets used in our experi- ments. MEM (Bruni et al., 2014), M&C (Miller  and Charles, 1991), MTurk (Radinsky et al., 2011),  RW (Luong et al., 2013), R&G (Rubenstein and Good- enough, 1965), SCWS (Huang et al., 2012), SLex (Hill  et al., 2014), WSR and WSS (Agirre et al., 2009),  GL (Mikolov et al., 2013a), and MSYN (Mikolov et al.,  2013b).", "labels": [], "entities": [{"text": "WSR", "start_pos": 268, "end_pos": 271, "type": "METRIC", "confidence": 0.6994433403015137}, {"text": "GL", "start_pos": 304, "end_pos": 306, "type": "METRIC", "confidence": 0.985160231590271}, {"text": "MSYN", "start_pos": 336, "end_pos": 340, "type": "DATASET", "confidence": 0.8544926047325134}]}, {"text": " Table 4: Results of model shrinkage experiments by reconstructing the fastText.600B embeddings. Each  dataset in WordSim and Analogy was evaluated by Spearman's rho and accuracy, respectively. 'Macro' and 'Micro'  represent the macro-average of Spearman's rho over all WordSim datasets and the micro-average of accuracy over  all Analogy datasets.", "labels": [], "entities": [{"text": "WordSim", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9644821882247925}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9993526339530945}, {"text": "WordSim datasets", "start_pos": 270, "end_pos": 286, "type": "DATASET", "confidence": 0.9778794646263123}, {"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.9977715015411377}, {"text": "Analogy datasets", "start_pos": 331, "end_pos": 347, "type": "DATASET", "confidence": 0.73394675552845}]}, {"text": " Table 7: Results of the NER experiments on the  CoNLL-2003 dataset.", "labels": [], "entities": [{"text": "NER", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9324014782905579}, {"text": "CoNLL-2003 dataset", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9844455122947693}]}, {"text": " Table 8: Results of the TE experiments on the SNLI  dataset.", "labels": [], "entities": [{"text": "TE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9067820310592651}, {"text": "SNLI  dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9493176639080048}]}]}