{"title": [{"text": "Naive Bayes and BiLSTM Ensemble for Discriminating between Mainland and Taiwan Variation of Mandarin Chinese", "labels": [], "entities": [{"text": "BiLSTM Ensemble", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.7413353025913239}, {"text": "Discriminating between Mainland and Taiwan Variation of Mandarin Chinese", "start_pos": 36, "end_pos": 108, "type": "TASK", "confidence": 0.5624475429455439}]}], "abstractContent": [{"text": "Automatic dialect identification is a more challenging task than language identification, as it requires the ability to discriminate between varieties of one language.", "labels": [], "entities": [{"text": "Automatic dialect identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6832400461037954}, {"text": "language identification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7345415204763412}]}, {"text": "In this paper, we propose an ensemble based system, which combines traditional machine learning models trained on bag of n-gram fetures, with deep learning models trained on word em-beddings, to solve the Discriminating between Mainland and Taiwan Variation of Mandarin Chinese (DMT) shared task at VarDial 2019.", "labels": [], "entities": [{"text": "Mainland and Taiwan Variation of Mandarin Chinese (DMT) shared task at VarDial 2019", "start_pos": 228, "end_pos": 311, "type": "DATASET", "confidence": 0.6617861449718475}]}, {"text": "Our experiments show that a character bigram-trigram combination based Naive Bayes is a very strong model for identifying varieties of Mandarin Chinense.", "labels": [], "entities": [{"text": "Mandarin Chinense", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.7362393736839294}]}, {"text": "Through further ensemble of Navie Bayes and BiLSTM, our system (team: itsalexyang) achived an macro-averaged F1 score of 0.8530 and 0.8687 in two tracks.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.7234010100364685}, {"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9776981472969055}]}], "introductionContent": [{"text": "Dialect identification, which aims at distinguishing related languages or varieties of a specific language, is a special case of language identification.", "labels": [], "entities": [{"text": "Dialect identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8323422372341156}, {"text": "language identification", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.7241433709859848}]}, {"text": "Accurate detection of dialects is an important step for many NLP piplines and applications, such as automatic speech recognition, machine translation and multilingual data acquisition.", "labels": [], "entities": [{"text": "Accurate detection of dialects", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7587018683552742}, {"text": "automatic speech recognition", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.6215866307417551}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8014262020587921}, {"text": "multilingual data acquisition", "start_pos": 154, "end_pos": 183, "type": "TASK", "confidence": 0.6325374742348989}]}, {"text": "While there are effective solutions to language identification, dialect identification remains a tough problem to be tackled.", "labels": [], "entities": [{"text": "language identification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7138623893260956}, {"text": "dialect identification", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7326546311378479}]}, {"text": "As linguistic differences among related languages are less obvious than those among different languages, dialect identification is more subtle and complex, and therefore has become an attractive topic for many researchers in recent years.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7290095686912537}]}, {"text": "Mandarin Chinese is a group of related varieties of Chinese spoken across many different regions.", "labels": [], "entities": []}, {"text": "The group includes Putonghua, the offical language of Mainland China, and Guoyu, another", "labels": [], "entities": []}], "datasetContent": [{"text": "We use scikit-learn library 1 for the implementation of the n-gram features based models and the ensemble meta-classifier.", "labels": [], "entities": []}, {"text": "As for deep learning models, we implement them using Keras 2 library with Tensorflow backend.", "labels": [], "entities": [{"text": "Keras 2 library", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8901160955429077}]}, {"text": "We used Adam () method as the optimizer, setting the first momentum to be 0.9 , the second momentum 0.999 and the initial learning 0.001.", "labels": [], "entities": []}, {"text": "The bacth size is 32.", "labels": [], "entities": []}, {"text": "All hidden states of LSTMs, feature maps of CNNs and word embeddings have 300 dimensions.", "labels": [], "entities": []}, {"text": "Word embeddings are fine tuned during training process.", "labels": [], "entities": []}, {"text": "All models are trained separately on dataset of traditional and simplified version, and evaluated using macroweighted f1 score.", "labels": [], "entities": []}, {"text": "Our code for all experiments is publicly available 3 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of dataset for each variety. Sentence lengths are calculated based on word-level tokens from  training and validation set.", "labels": [], "entities": []}, {"text": " Table 3: Macro-weighted f1 scores of LR, SVM, MNB using individual or combined features as input, both on  dataset of simplified and traditional version.", "labels": [], "entities": []}, {"text": " Table 4: Macro-weighted f1 scores of deep learning models using word embeddings as input, both on dataset of  simplified and traditional version.", "labels": [], "entities": []}, {"text": " Table 5: Macro-weighted f1 scores of 4 ensemble strategies combining different base classifiers, both on dataset of  simplified and traditional version. \"all ML\" and \"all DL\" refer to combine all machine learning models and deep  learning models respectively. All machine learning models use character bigram-trigram combination as input.", "labels": [], "entities": []}, {"text": " Table 6: Macro-weighted f1 scores of 3 submissions on  test sets (team: itsalexyang).", "labels": [], "entities": []}]}