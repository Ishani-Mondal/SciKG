{"title": [{"text": "Joint Multi-Label Attention Networks for Social Text Annotation", "labels": [], "entities": [{"text": "Social Text Annotation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6410102943579356}]}], "abstractContent": [{"text": "We propose a novel attention network for document annotation with user-generated tags.", "labels": [], "entities": [{"text": "document annotation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7294822931289673}]}, {"text": "The network is designed according to the human reading and annotation behaviour.", "labels": [], "entities": []}, {"text": "Usually , users try to digest the title and obtain a rough idea about the topic first, and then read the content of the document.", "labels": [], "entities": []}, {"text": "Present research shows that the title metadata could largely affect the social annotation.", "labels": [], "entities": []}, {"text": "To better utilise this information, we design a framework that separates the title from the content of a document and apply a title-guided attention mechanism over each sentence in the content.", "labels": [], "entities": []}, {"text": "We also propose two semantic-based loss regularisers that enforce the output of the network to conform to label semantics , i.e. similarity and subsumption.", "labels": [], "entities": []}, {"text": "We analyse each part of the proposed system with two real-world open datasets on publication and question annotation.", "labels": [], "entities": []}, {"text": "The integrated approach , Joint Multi-label Attention Network (JMAN), significantly outperformed the Bidi-rectional Gated Recurrent Unit (Bi-GRU) by around 13%-26% and the Hierarchical Attention Network (HAN) by around 4%-12% on both datasets, with around 10%-30% reduction of training time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social annotation, or tagging, is a popular functionality allowing users to assign \"keywords\" to online resources for better semantic search and recommendation).", "labels": [], "entities": []}, {"text": "Common socially annotated textual resources include questions, papers, (micro-)blogs, product reviews, etc.", "labels": [], "entities": []}, {"text": "In practice, however, only a limited number of resources is annotated with tags.", "labels": [], "entities": []}, {"text": "Annotating a large number of documents requires much cognitive effort and can be time-consuming.", "labels": [], "entities": [{"text": "Annotating a large number of documents", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.750613292058309}]}, {"text": "This has driven research on document annotation based on existing tag sets).", "labels": [], "entities": []}, {"text": "Recent studies formalise the automated social annotation task as a multi-label classification problem ( and apply deep learning approaches (.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7045036852359772}]}, {"text": "A strong baseline is the use of Bi-directional RNN) with GRU ( or LSTM.", "labels": [], "entities": [{"text": "GRU", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9923399686813354}]}, {"text": "Another more recent improvement is achieved through Hierarchical Attention Network (HAN) () which discriminates important words and sentences from others, as adapted in () for annotation.", "labels": [], "entities": []}, {"text": "These models, however, suffer from two issues: (i) simply scanning over the words and sentences, the models do not fully mimic the way users read and annotate documents, and (ii) semantic relations, similarity and subsumption, among the labels are not considered.", "labels": [], "entities": []}, {"text": "Our model focuses on simulating users' reading and annotation behaviour with attention mechanisms.", "labels": [], "entities": []}, {"text": "The title of a document is highly abstract while informative about the topics and has a direct impact on users' annotation choice, showing high descriptive capacity and effectiveness for annotation (; the content provides complementary information for annotation.", "labels": [], "entities": []}, {"text": "Usually, users firstly read the title, and based on their understanding of the title, proceed to the content of the document.", "labels": [], "entities": []}, {"text": "To simulate this behaviour, we propose an attention network with separated inputs (title and content) and parallelled attention layers at both the wordlevel and the sentence-level.", "labels": [], "entities": []}, {"text": "One major distinction to previous approaches is to represent the content with a title-guided attention mechanism; this enables the network to discriminate among sentences based on its understanding of the title.", "labels": [], "entities": []}, {"text": "In addition, in the social context, users tend to annotate documents collectively with tags of various semantic forms and granularities).", "labels": [], "entities": []}, {"text": "One challenging issue is how to exploit the relations among labels (user-generated tags) ( to improve the learning performance.", "labels": [], "entities": []}, {"text": "Among neural network based methods, a recent attempt is to initialise weights for dedicated neurons in the last layer to memorise the label relations (, however, the limitation is the large number of neurons to be assigned, making it inefficient (or inapplicable) for systems with large number of labels.", "labels": [], "entities": []}, {"text": "To incorporate the label semantics inferred from the data or from external knowledge bases into the network, we design two loss regularisers, for similarity and subsumption relations, respectively.", "labels": [], "entities": []}, {"text": "The regularisers enforce the output layer of the network to satisfy the semantic constraints of the labels.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed approach for automated social annotation on two representative open datasets in social tagging, Bibsonomy 1 (academic publication annotation) and Zhihu 2 (general domain social question annotation).", "labels": [], "entities": [{"text": "Bibsonomy", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.8319140076637268}, {"text": "general domain social question annotation", "start_pos": 180, "end_pos": 221, "type": "TASK", "confidence": 0.6347542881965638}]}, {"text": "For Bibsonomy, we used the cleaned dataset from (Dong et al.,", "labels": [], "entities": [{"text": "Bibsonomy", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.6157936453819275}]}], "tableCaptions": [{"text": " Table 1: Comparison Results on the Bibsonomy dataset", "labels": [], "entities": [{"text": "Bibsonomy dataset", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.686860665678978}]}, {"text": " Table 2: Comparison Results on the Zhihu dataset", "labels": [], "entities": [{"text": "Zhihu dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9140527248382568}]}]}