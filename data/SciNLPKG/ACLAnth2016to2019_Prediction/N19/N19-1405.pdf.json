{"title": [{"text": "Understanding Dataset Design Choices for Multi-hop Reasoning", "labels": [], "entities": [{"text": "Multi-hop Reasoning", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7608182430267334}]}], "abstractContent": [{"text": "Learning multi-hop reasoning has been a key challenge for reading comprehension models, leading to the design of datasets that explicitly focus on it.", "labels": [], "entities": []}, {"text": "Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning.", "labels": [], "entities": [{"text": "multi-hop question answering task", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7241489440202713}]}, {"text": "In this paper, we investigate two recently proposed datasets, WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018).", "labels": [], "entities": [{"text": "WikiHop", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9153986573219299}]}, {"text": "First, we explore sentence-factored models for these tasks; by design, these models cannot do multi-hop reasoning, but they are still able to solve a large number of examples in both datasets.", "labels": [], "entities": [{"text": "multi-hop reasoning", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.6860822439193726}]}, {"text": "Furthermore, we find spurious correlations in the unmasked version of WikiHop, which make it easy to achieve high performance considering only the questions and answers.", "labels": [], "entities": []}, {"text": "Finally, we investigate one key difference between these datasets, namely span-based vs. multiple-choice formulations of the QA task.", "labels": [], "entities": []}, {"text": "Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting.", "labels": [], "entities": []}, {"text": "Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering from text () is a key challenge problem for NLP that tests whether models can extract information based on a query.", "labels": [], "entities": [{"text": "Question answering from text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8992996662855148}]}, {"text": "However, even sophisticated models that perform well on QA benchmarks () may only be doing shallow pattern matching of the question against the supporting passage ().", "labels": [], "entities": []}, {"text": "More recent work () has emphasized gathering information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning.", "labels": [], "entities": []}, {"text": "Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop ( and HotpotQA ( . In this paper, we seek to answer two main questions.", "labels": [], "entities": []}, {"text": "First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them?", "labels": [], "entities": []}, {"text": "Recent work has shown that largescale QA datasets often do not exhibit their advertised properties.", "labels": [], "entities": []}, {"text": "We devise a test setting to see whether multi-hop reasoning is necessary: can a model which treats each sentence independently select the sentence containing the answer?", "labels": [], "entities": []}, {"text": "This provides a rough estimate of the fraction of questions solvable by a non-multi-hop system.", "labels": [], "entities": []}, {"text": "Our results show that more than half of the questions in WikiHop and HotpotQA do not require multihop reasoning to solve.", "labels": [], "entities": []}, {"text": "Surprisingly, we find that a simple baseline which ignores the passage and only uses the question and answer can achieve strong results on WikiHop and a modified version of HotpotQA, further confirming this view.", "labels": [], "entities": [{"text": "WikiHop", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.9397743344306946}]}, {"text": "Second, we study the nature of the supervision on the two datasets.", "labels": [], "entities": []}, {"text": "One critical difference is that HotpotQA is span-based (the answer is a span of the passage) while WikiHop is multiplechoice.", "labels": [], "entities": []}, {"text": "How does this difference affect learning and evaluation of multi-hop reasoning systems?", "labels": [], "entities": []}, {"text": "We show that a multiple-choice version of HotpotQA is vulnerable to the same baseline that performs well on WikiHop, showing that this distinction maybe important from an evaluation standpoint.", "labels": [], "entities": []}, {"text": "Furthermore, we show that a state-of-theart model, BiDAF++, trained on span-based HotpotQA and adapted to the multiple-choice setting outperforms the same model trained natively on the multiple-choice setting.", "labels": [], "entities": [{"text": "HotpotQA", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.8553192615509033}]}, {"text": "However, even in the span-based setting, the high performance of the sentence-factored models raises questions about whether multi-hop reasoning is being learned.", "labels": [], "entities": []}, {"text": "Our conclusions are as follows: (1) Many examples in both WikiHop and HotpotQA do not require multi-hop reasoning to solve, as the sentence-factored model can find the answers.", "labels": [], "entities": []}, {"text": "(2) On WikiHop and a multiple-choice version of HotpotQA, a no context baseline does very well.", "labels": [], "entities": []}, {"text": "(3) Span-based supervision provides a harder testbed than multiple choice by having more answers to choose from, but given the strong performance of the sentence-factored models, it is unclear whether any of the proposed models are doing a good job at multi-hop reasoning in any setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "WikiHop introduced this English dataset specially designed for text understanding across multiple documents.", "labels": [], "entities": [{"text": "text understanding across multiple documents", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.8228886604309082}]}, {"text": "The dataset consists of 40k+ questions, answers, and passages, where each passage consists of several documents collected from Wikipedia.", "labels": [], "entities": []}, {"text": "Questions are posed as a query of a relation r followed by ahead entity h, with the task being to find the tail entity t from a set of entity candidates E.", "labels": [], "entities": []}, {"text": "Annotators followed links between documents and were required to use multiple documents to get the answer.", "labels": [], "entities": []}, {"text": "HotpotQA  proposed anew dataset with 113k English Wikipedia-based question-answer pairs.", "labels": [], "entities": [{"text": "HotpotQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9688474535942078}]}, {"text": "The questions are diverse, falling into several categories, but all require finding and reasoning over multiple supporting documents to answer.", "labels": [], "entities": []}, {"text": "Models should choose answers by selecting variable-length spans from these documents.", "labels": [], "entities": []}, {"text": "Sentences relevant to finding the answer are annotated in the dataset as \"supporting facts\" so models can use these at training time as well.", "labels": [], "entities": []}, {"text": "In this section, we seek to answer whether multihop reasoning is really needed to solve these two multi-hop datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The accuracy of our proposed sentence- factored models on identifying answer location in the  development sets of WikiHop, HotpotQA and SQuAD.  Random: we randomly pick a sentence in the passage  to see whether it contains the answer. Factored and  Factored BiDAF refer to the models of Section 3.1. As  expected, these models perform better on SQuAD than  the other two datasets, but the model can nevertheless  find many answers in WikiHop especially.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9986894726753235}]}, {"text": " Table 2: The results of our no-context baseline com- pared with Coref-GRU (Dhingra et al., 2018), MHQA- GRN (Song et al., 2018), and Entity-GCN (De Cao  et al., 2018) on the WikiHop dev set.", "labels": [], "entities": [{"text": "MHQA- GRN", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.7606897155443827}, {"text": "WikiHop dev set", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.8970911304155985}]}, {"text": " Table 3: The performance of different models on the  dev sets of WikiHop and HotpotQA. MC denotes using  both the multiple-choice dataset and model. Span2MC  means we train the model with span-based supervision  and evaluate the model on a multiple choice setting.  Our models only mildly outperform the no-context  baseline in all settings.", "labels": [], "entities": []}, {"text": " Table 4: The performance of different models on the  dev sets of WikiHop and HotpotQA. Span denotes us- ing both span-based dataset and model. BiDAF++ de- notes the performance reported in HotpotQA (Yang  et al., 2018).", "labels": [], "entities": []}]}