{"title": [{"text": "On the Idiosyncrasies of the Mandarin Chinese Classifier System", "labels": [], "entities": [{"text": "Idiosyncrasies", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9633263349533081}]}], "abstractContent": [{"text": "While idiosyncrasies of the Chinese classi-fier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify.", "labels": [], "entities": []}, {"text": "Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.6340324978033701}]}, {"text": "We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classi-fier choice, and find that it is not fully idiosyn-cratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many of the world's languages make use of numeral classifiers).", "labels": [], "entities": []}, {"text": "While theoretical debate still rages on the function of numeral classifiers, it is generally accepted that they need to be present for nouns to be modified by numerals, quantifiers, demonstratives, or other qualifiers ().", "labels": [], "entities": []}, {"text": "In Mandarin Chinese, for instance, the phrase one person translates as \u4e00\u4e2a\u4eba (y\u00af \u0131 g` e r\u00e9n); the classifier \u4e2a (g` e) has no clear translation in English, yet, nevertheless, it is necessary to place it between the numeral \u4e00 (y\u00af \u0131) and the word for person \u4eba (r\u00e9n).", "labels": [], "entities": []}, {"text": "There are hundreds of numeral classifiers in the Mandarin lexicon (Po-Ching and Rimmington, 2015, gives some canonical examples), and classifier choices are often argued to be based on inherent, possibly universal, semantic properties associated with the noun, such as shape (.", "labels": [], "entities": []}, {"text": "Indeed, in a summary article, writes: \"Chinese classifier systems are cognitively based, rather than arbitrary systems of classification.\"", "labels": [], "entities": []}, {"text": "If classifier choice were solely based on conceptual features of a given noun, then we might expect it to be nearly determinate-like gender-marking on nouns in German, Slavic, or Romance languages)-and perhaps even fixed for all of a given noun's synonyms.", "labels": [], "entities": []}, {"text": "However, selecting which classifers go with nouns in practice is an idiosyncratic process, often with several grammatical options (see for two examples).", "labels": [], "entities": []}, {"text": "Moreover, knowing what a noun means doesn't always mean you can guess the classifier.", "labels": [], "entities": []}, {"text": "For example, most nouns referring to animals, such as \u9a74 (l\u00fcl\u00fc\u00b4l\u00fc\u00fa, donkey) or \u7f8a (y\u00e1ng, goat), use the classifier \u53ea (zh\u00af \u0131).", "labels": [], "entities": []}, {"text": "However, horses 0.0205 0.0486 0.0002 0.2570 Everything else 0.1178 0.2661: Empirical distribution of selected classifiers over two nouns: \u4eba\u58eb (r\u00e9n sh`sh`\u0131, people) and \u5de5\u7a0b (g\u00af ong ch\u00e9ng, project).", "labels": [], "entities": []}, {"text": "Most of the probability mass is allocated to only a few classifiers for both nouns (bolded).", "labels": [], "entities": []}, {"text": "cannot use \u53ea (zh\u00af \u0131), despite being semantically similar to goats and donkeys, and instead must appear with \u5339 (p\u02c7\u0131p\u02c7\u0131).", "labels": [], "entities": []}, {"text": "Conversely, knowing which particular subset of noun meaning is reflected in the classifer also doesn't mean that you can use that classifier with any noun that seems to have the right semantics.", "labels": [], "entities": []}, {"text": "For example, classifier \u6761 (ti\u00e1o) can be used with nouns referring to long and narrow objects, like rivers, snakes, fish, pants, and certain breeds of dogs-but never cats, regardless of how long and narrow they might be!", "labels": [], "entities": []}, {"text": "In general, classifiers carve up the semantic space in a very idiosyncratic manner that is neither fully arbitrary, nor fully predictable from semantic features.", "labels": [], "entities": []}, {"text": "Given this, we can ask: precisely how idiosyncratic is the Mandarin Chinese classifier system?", "labels": [], "entities": []}, {"text": "For a given noun, how predictable is the set of classifiers that can be grammatically employed?", "labels": [], "entities": []}, {"text": "For instance, had we not known that the Mandarin word for horse \u9a6c (m\u02c7am\u02c7a) predominantly takes the classifier \u5339 (p\u02c7\u0131p\u02c7\u0131), how likely would we have been to guess it over the much more common animal classifier \u53ea (zh\u00af \u0131)?", "labels": [], "entities": []}, {"text": "Is it more important to know that a noun is \u9a6c (m\u02c7am\u02c7a) or simply that the noun is an animal noun?", "labels": [], "entities": []}, {"text": "We address these questions by computing how much the uncertainty in the distribution over classifiers can be reduced by knowing information about nouns and noun semantics.", "labels": [], "entities": []}, {"text": "We quantify this notion of classifier idiosyncrasy by calculating the mutual information between classifiers and nouns, and also between classifiers and several sets that are relevant to noun meaning (i.e., categories of noun senses, sets of noun synonyms, adjectives, and categories of adjective senses).", "labels": [], "entities": []}, {"text": "Our results yield concrete, quantitative measures of idiosyncrasy in bits, that can supplement existing hand-annotated, intuition-based approaches that organize Mandarin classifiers into an ontology.", "labels": [], "entities": []}, {"text": "Why investigate the idiosyncrasy of the Mandarin Chinese classifier system?", "labels": [], "entities": []}, {"text": "How idiosyncratic or predictable natural language is has captivated researchers since Shannon (1951) originally proposed the question in the context of printed English text.", "labels": [], "entities": []}, {"text": "Indeed, looking at predictability directly relates to the complexity of language-a fundamental question in linguistics)-which has also been claimed to have consequences learnability and processing.", "labels": [], "entities": []}, {"text": "For example, how hard it is fora learner to master irregularity, say, in the English past tense might be affected by predictability, and highly predictable noun-adjacent words, such as gender affixes in German and prenominal adjectives in English, are also shown to confer online processing advantages (.", "labels": [], "entities": []}, {"text": "Within the Chinese classifier system itself, the very common, general-purpose classifier \u4e2a (g` e) is acquired by children earlier than rarer, more semantically rich ones.", "labels": [], "entities": []}, {"text": "General classifiers are also found to occur more often in corpora with nouns that are less predictable in context (i.e., nouns with high surprisal; Hale 2001) (, providing initial evidence that predictability likely plays a role in classifiernoun pairing more generally.", "labels": [], "entities": [{"text": "classifiernoun pairing", "start_pos": 232, "end_pos": 254, "type": "TASK", "confidence": 0.7715790867805481}]}, {"text": "Furthermore, providing classifiers improves participants' recall of nouns in laboratory experiments () (but see Huang and Chen 2014)-but, it isn't known whether classifiers do so by modulating noun predictability.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9891397953033447}, {"text": "noun predictability", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.6948517262935638}]}], "datasetContent": [{"text": "We apply an existing neural Mandarin word segmenter) to the Chinese Gigaword corpus (), and then feed the segmented corpus to a neural dependency parser, using Google's pretrained Parsey Uni-: Mutual information between classifiers and nouns I(C; N ), noun senses I(C; S), and adjectives I(C; A), is compared to their entropies.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.6832501888275146}]}, {"text": "The model is trained on Universal Dependencies datasets v1.3.", "labels": [], "entities": [{"text": "Universal Dependencies datasets v1.3", "start_pos": 24, "end_pos": 60, "type": "DATASET", "confidence": 0.8345544189214706}]}, {"text": "We extract classifier-noun pairs and adjective-classifiernoun triples from sentences, where the adjective and the classifier modify the same noun-this is easily determined from the parse.", "labels": [], "entities": []}, {"text": "We also record the tuple counts, and use them to compute an empirical distribution over classifiers that modify nouns, and noun-adjective pairs, respectively.", "labels": [], "entities": []}, {"text": "Since no annotated supersense list exists for Mandarin, we first use CC-CEDICT 3 as a Mandarin Chinese-to-English dictionary to translate nouns and adjectives into English.", "labels": [], "entities": []}, {"text": "Acknowledging that translating might introduce noise, we subsequently categorize our words into different senses using the SemCor supersense data for nouns, and adjectives ().", "labels": [], "entities": []}, {"text": "After that, we calculate the mutual information under each noun, and adjective supersense.", "labels": [], "entities": []}, {"text": "As this contribution is the first to investigate classifier predictability, we make several simplifying assumptions.", "labels": [], "entities": [{"text": "classifier predictability", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.8891550302505493}]}, {"text": "Extracting distributions over classifiers from a large corpus, as we do, ignores sentential context, which means we ignore the fact that some nouns (i.e., relational nouns, like m\u00af am\u00af a, Mom) are more likely to be found in verb frames or other constructions where classifiers are not needed.", "labels": [], "entities": []}, {"text": "We also ignore singularplural, which might affect classifier choice, and the mass-count distinction (, to the extent that it is not encoded in noun superset categories (e.g., substance includes mass nouns).", "labels": [], "entities": []}, {"text": "We also assume that every classifier-noun or classifier-adjective pairing we extract is equally acceptable to native speakers.", "labels": [], "entities": []}, {"text": "However, it's possible that native speakers differ in either their knowledge of classifier-noun distributions or confidence in particular combinations.", "labels": [], "entities": []}, {"text": "Whether and how such human knowledge interacts with our calculations would bean interesting future avenue.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Mutual information between classifiers and nouns I(C; N ), noun senses I(C; S), and adjectives I(C; A),  is compared to their entropies.", "labels": [], "entities": []}]}