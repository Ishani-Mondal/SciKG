{"title": [{"text": "Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling", "labels": [], "entities": [{"text": "Target-oriented Opinion Words Extraction", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5192484259605408}, {"text": "Neural Sequence Labeling", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6618132591247559}]}], "abstractContent": [{"text": "Opinion target extraction and opinion words extraction are two fundamental subtasks in Aspect Based Sentiment Analysis (ABSA).", "labels": [], "entities": [{"text": "Opinion target extraction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.728329082330068}, {"text": "opinion words extraction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6426892975966135}, {"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.7475492145333972}]}, {"text": "Recently , many methods have made progress on these two tasks.", "labels": [], "entities": []}, {"text": "However, few works aim at extracting opinion targets and opinion words as pairs.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel sequence labeling subtask for ABSA named TOWE (Target-oriented Opinion Words Extraction), which aims at extracting the corresponding opinion words fora given opinion target.", "labels": [], "entities": [{"text": "TOWE", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.8654389381408691}, {"text": "Target-oriented Opinion Words Extraction)", "start_pos": 81, "end_pos": 122, "type": "TASK", "confidence": 0.6510333001613617}]}, {"text": "A target-fused sequence labeling neu-ral network model is designed to perform this task.", "labels": [], "entities": []}, {"text": "The opinion target information is well encoded into context by an Inward-Outward LSTM.", "labels": [], "entities": []}, {"text": "Then left and right contexts of the opinion target and the global context are combined to find the corresponding opinion words.", "labels": [], "entities": []}, {"text": "We build four datasets for TOWE based on several popular ABSA benchmarks from laptop and restaurant reviews.", "labels": [], "entities": [{"text": "TOWE", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7878761291503906}]}, {"text": "The experimental results show that our proposed model outperforms the other compared methods significantly.", "labels": [], "entities": []}, {"text": "We believe that our work may not only be helpful for downstream sentiment analysis task, but can also be used for pair-wise opinion summariza-tion.", "labels": [], "entities": [{"text": "downstream sentiment analysis", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.7204683224360148}, {"text": "pair-wise opinion summariza-tion", "start_pos": 114, "end_pos": 146, "type": "TASK", "confidence": 0.5547594626744589}]}], "introductionContent": [{"text": "Sentiment analysis, also known as opinion mining (, has drawn increasing attention of researchers and industries in recent years.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9659293591976166}, {"text": "opinion mining", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8021630644798279}]}, {"text": "It can provide valuable information from user-generated reviews.", "labels": [], "entities": []}, {"text": "However, sentiment analysis at sentence level or document level sometimes cannot provide more detailed information, thus a finer-grained task, Aspect-Based Sentiment Analysis (ABSA) (, is proposed to identify the opinions of a specific target or aspect * Corresponding author.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9547056257724762}, {"text": "Aspect-Based Sentiment Analysis (ABSA)", "start_pos": 143, "end_pos": 181, "type": "TASK", "confidence": 0.6844033896923065}]}], "datasetContent": [{"text": "We build the datasets based on the SemEval challenge 2014 Task4, SemEval Challenge 2015 task 12 and SemEval Challenge 2016 task 5 (.", "labels": [], "entities": [{"text": "SemEval Challenge 2015 task 12", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.5644425392150879}]}, {"text": "The SemEval challenge provides several datasets from restaurant and laptop domain.", "labels": [], "entities": []}, {"text": "These datasets are very popular benchmarks for many ABSA subtasks, including Aspect category detection, Opinion Target Extraction, Opinion Words Extraction and Target-Dependent Sentiment Analysis (TDSA).", "labels": [], "entities": [{"text": "ABSA subtasks", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8935460448265076}, {"text": "Aspect category detection", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.755792478720347}, {"text": "Opinion Words Extraction", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.5202827652295431}, {"text": "Target-Dependent Sentiment Analysis (TDSA)", "start_pos": 160, "end_pos": 202, "type": "TASK", "confidence": 0.7107788374026617}]}, {"text": "In the original datasets of SemEval challenge, the opinion targets (aspect terms) are annotated, but the opinion words and the correspondence with targets are not provided.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7603855431079865}]}, {"text": "So we annotate the corresponding opinion words for the annotated targets.", "labels": [], "entities": []}, {"text": "Every sentence is annotated by two people, and the conflicts will be checked.", "labels": [], "entities": []}, {"text": "Each instance of the datasets consists of a sentence, the position of the target and the positions of the corresponding opinion words.", "labels": [], "entities": []}, {"text": "Note that we only keep the sentences that contain pairs of target and opinion words.", "labels": [], "entities": []}, {"text": "The sentences without targets or with implicit opinion expressions are not included.", "labels": [], "entities": []}, {"text": "Finally, we generate four datasets: 14res and 14lap from SemEval 2014, 15res from SemEval 2015 and 16res from SemEval 2016.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8823947310447693}]}, {"text": "14res, 15res, and 16res contain reviews from restaurant domain.", "labels": [], "entities": []}, {"text": "The sentences in 14lap come from laptop domain.", "labels": [], "entities": []}, {"text": "The statistics of the four datasets is shown in Table 1.", "labels": [], "entities": []}, {"text": "Precision, recall and F1 score are used as the metrics to measure the performance of models.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9963184595108032}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9989960789680481}, {"text": "F1 score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9902574121952057}]}, {"text": "An extracted opinion words span is regarded as a correct prediction when the starting and ending offset of the predicted span are both identical to those of a golden opinion words span.", "labels": [], "entities": []}, {"text": "We compute Precision, Recall and F1 with the span as the unit.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9986429810523987}, {"text": "Recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9982427358627319}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9992532134056091}]}], "tableCaptions": [{"text": " Table 1: Statistics of datasets. The number of targets is  identical to the number of pairs and instances", "labels": [], "entities": []}, {"text": " Table 2: Main Results in terms of Precsion, Recall and F1-score. Best results are in bold. IOG outperms all the  baselines significantly (p < 0.01).", "labels": [], "entities": [{"text": "Precsion", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993082284927368}, {"text": "Recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9956401586532593}, {"text": "F1-score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9991639852523804}, {"text": "IOG", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9414465427398682}]}, {"text": " Table 3: Comparisions for different model design in terms of Precsion, Recall and F1-score.", "labels": [], "entities": [{"text": "Precsion", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.997322142124176}, {"text": "Recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9944851994514465}, {"text": "F1-score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.998216450214386}]}]}