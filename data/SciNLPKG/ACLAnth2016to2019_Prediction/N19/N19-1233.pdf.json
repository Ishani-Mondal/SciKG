{"title": [{"text": "Evaluating Text GANs as Language Models", "labels": [], "entities": [{"text": "Evaluating Text GANs", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7844149867693583}]}], "abstractContent": [{"text": "Generative Adversarial Networks (GANs) area promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \"exposure bias\".", "labels": [], "entities": [{"text": "text generation", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8319045603275299}]}, {"text": "However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric.", "labels": [], "entities": [{"text": "GANs", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.949312686920166}, {"text": "text generation", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8411698937416077}]}, {"text": "In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics.", "labels": [], "entities": []}, {"text": "We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs.", "labels": [], "entities": []}, {"text": "Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.8815118273099264}]}], "introductionContent": [{"text": "Neural networks have revolutionized the field of text generation, in machine translation (, image captioning () and many other applications.", "labels": [], "entities": [{"text": "text generation", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7776804864406586}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7949416637420654}, {"text": "image captioning", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7180131822824478}]}, {"text": "Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective.", "labels": [], "entities": [{"text": "text generation", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7430576980113983}, {"text": "language modeling (LM)", "start_pos": 212, "end_pos": 234, "type": "TASK", "confidence": 0.7581584930419922}]}, {"text": "A commonly discussed drawback of such LM-based text generation is exposure bias: during training, the model predicts the next token conditioned on the ground truth history, while attest time prediction is based on predicted tokens, causing a train-test mismatch.", "labels": [], "entities": [{"text": "LM-based text generation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8678469856580099}]}, {"text": "Models trained in this manner often struggle to overcome previous prediction errors.", "labels": [], "entities": []}, {"text": "Generative Adversarial Networks () offer a solution for exposure bias.", "labels": [], "entities": [{"text": "exposure bias", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7825207412242889}]}, {"text": "* The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss.", "labels": [], "entities": []}, {"text": "In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator.", "labels": [], "entities": []}, {"text": "This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) (, or a Convolutional Neural Network (CNN) (.", "labels": [], "entities": [{"text": "text generation", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7770259082317352}]}, {"text": "However, evaluating GANs is more difficult than evaluating LMs.", "labels": [], "entities": [{"text": "GANs", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.8994478583335876}]}, {"text": "While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.826163649559021}]}, {"text": "Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation with semantic coherence (.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.9101255734761556}]}, {"text": "In this paper, we propose a method for evaluating GANs with standard probability-based evaluation metrics.", "labels": [], "entities": [{"text": "GANs", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.9683957099914551}]}, {"text": "We show that the expected prediction of a GAN generator can be viewed as a LM, and suggest a simple Monte-Carlo method for approximating it.", "labels": [], "entities": []}, {"text": "The approximated probability distribution can then be evaluated with standard LM metrics such as perplexity or Bits Per Character (BPC).", "labels": [], "entities": [{"text": "Bits Per Character (BPC)", "start_pos": 111, "end_pos": 135, "type": "METRIC", "confidence": 0.8838388621807098}]}, {"text": "To empirically establish our claim, we implement our evaluation on several.", "labels": [], "entities": []}, {"text": "We find that all models have substantially lower BPC compared to state-of-the-art LMs.", "labels": [], "entities": [{"text": "BPC", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9585068821907043}]}, {"text": "By directly comparing to LMs, we put in perspective the current performance of RNN-based GANs for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7921805083751678}]}, {"text": "Our results are also inline with recent concurrent work by, who reached a similar conclusion by comparing the performance of textual GANs to that of LMs using metrics suggested for GAN evaluation.", "labels": [], "entities": [{"text": "GAN evaluation", "start_pos": 181, "end_pos": 195, "type": "TASK", "confidence": 0.8999628722667694}]}, {"text": "Our code is available at: http: //github.com/GuyTevet/SeqGAN-eval and http://github.com/GuyTevet/ rnn-gan-eval.", "labels": [], "entities": [{"text": "GuyTevet/SeqGAN-eval", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.8192102511723837}, {"text": "GuyTevet", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9679253101348877}]}], "datasetContent": [{"text": "Input: Gt(\u00b7): the generator function at time step t (x0, ..., xt): previous gold tokens xt+1: the gold next token (as ground truth) f (\u00b7, \u00b7): a LM evaluation metric N : number of samples 1: for n \u2190 1 to N do 2: gt,n \u2190\u2212 sample from Gt(x0...xt) be gained either by using a noise vector as the initial state h 0 (Press et al., 2017), or by sampling from the GAN's internal distribution over possible output tokens (.", "labels": [], "entities": []}, {"text": "Since h 0 is constant or a noise vector that makes G t stochastic, we can omit it to get G t (x 0 . .", "labels": [], "entities": []}, {"text": "In such a setup, the expected value ] is a distribution q over the next vocabulary token at : To empirically approximate q, we can sample from it N i.i.d samples, and compute an approximatio\u00f1 , where g t,n is one sample from G t (x 0 ...x t ).", "labels": [], "entities": []}, {"text": "Then, according to the strong law of large numbers: Given this approximate LM distribution, we can evaluate a GAN using perplexity or BPC.", "labels": [], "entities": [{"text": "BPC", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9680744409561157}]}, {"text": "We summarize the evaluation procedure in Algorithm 1.", "labels": [], "entities": []}, {"text": "To compare to prior work in LM, we follow the common setup and train on the text8 dataset.", "labels": [], "entities": [{"text": "text8 dataset", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8694405257701874}]}, {"text": "The dataset is derived from Wikipedia, and includes 26 English characters plus spaces.", "labels": [], "entities": []}, {"text": "We use the standard 90/5/5 split to train/validation/test.", "labels": [], "entities": []}, {"text": "Finally, we measure performance with BPC.", "labels": [], "entities": [{"text": "BPC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9818058609962463}]}, {"text": "We tuned hyper-parameters on the validation set, including sequence length to generate attest time, 1000 for).", "labels": [], "entities": []}, {"text": "We chose the number of samples N empirically for each model, as described in Section 4.2.", "labels": [], "entities": []}, {"text": "We set \u03b1 to 10, and the boundary to \u03b3 = 10 \u22123 as a good trade-off between accuracy and run-time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9996132254600525}]}, {"text": "plots the approximate error\u02dcG error\u02dc error\u02dcG t,N \u2212\u03b1 \u2212 \u02dc G t,N \u221e as a function of N . For both models, N > 1600 satisfies this condition (red line in.", "labels": [], "entities": [{"text": "approximate error", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9366941750049591}, {"text": "G error\u02dc error", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.8310782760381699}]}, {"text": "To be safe, we used N = 2000.", "labels": [], "entities": []}, {"text": "1. four zero five two memaire in afulie war formally dream the living of the centuries to quickly can f 2.", "labels": [], "entities": []}, {"text": "part of the pract the name in one nine seven were mustring of the airports tex works to eroses exten 3.", "labels": [], "entities": []}, {"text": "eight fourth jania lpa ore nine zero zero zero sport for tail concents englished a possible for po", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set evaluation of different character-based models on the text8 dataset. State-of-the-art results are  taken from https://github.com/sebastianruder/NLP-progress/blob/master/language_  modeling.md. The uniform distribution is equivalent to guessing the next character out of |V | = 27 characters.", "labels": [], "entities": [{"text": "text8 dataset", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8257281184196472}]}]}