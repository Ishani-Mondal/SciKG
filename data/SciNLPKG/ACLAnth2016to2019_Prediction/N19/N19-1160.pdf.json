{"title": [], "abstractContent": [{"text": "Combinatory categorial grammars are linguistically motivated and useful for semantic parsing , but costly to acquire in a supervised way and difficult to acquire in an unsupervised way.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7708558440208435}]}, {"text": "We propose an alternative making use of cross-lingual learning: an existing source-language parser is used together with a parallel corpus to induce a grammar and parsing model fora target language.", "labels": [], "entities": []}, {"text": "On the PASCAL benchmark, cross-lingual CCG induction out-performs CCG induction from gold-standard POS tags on 3 out of 8 languages, and un-supervised CCG induction on 6 out of 8 languages.", "labels": [], "entities": [{"text": "PASCAL benchmark", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8229575455188751}, {"text": "CCG induction", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.7236462235450745}, {"text": "CCG induction", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8198327720165253}]}, {"text": "We also show that cross-lingually induced CCGs reflect known syntactic properties of the target languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Combinatory Categorial Grammar (CCG)) is a grammar formalism known for its linguistic elegance and computational efficiency.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG))", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7599172095457712}]}, {"text": "It has been successfully used for statistical syntactic parsing ( and has emerged as a leading grammar formalism in semantic parsing.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8135584195454916}, {"text": "semantic parsing", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7995410859584808}]}, {"text": "Semantic parsing is important because it translates natural language utterances to something that a computer can understand, e.g., database queries, computer commands, or logical formulas, enabling next-generation information systems and knowledge extraction from text, among other applications.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8671104311943054}, {"text": "knowledge extraction from text", "start_pos": 238, "end_pos": 268, "type": "TASK", "confidence": 0.8202965557575226}]}, {"text": "CCGs used inmost work to date are either hand-crafted ( or extracted from large syntactically annotated corpora ().", "labels": [], "entities": []}, {"text": "In either case language-specific human effort is required.", "labels": [], "entities": []}, {"text": "Acquiring CCGs in an unsupervised way is difficult and does not reach the performance of supervised methods.", "labels": [], "entities": []}, {"text": "As a result, most research focuses on English and other languages are neglected, meaning that speakers of other languages have delayed or no access to CCG-based semantic parsing technology.", "labels": [], "entities": [{"text": "CCG-based semantic parsing", "start_pos": 151, "end_pos": 177, "type": "TASK", "confidence": 0.5337976217269897}]}, {"text": "We propose to overcome this bottleneck by inducing CCGs cross-lingually, i.e., transferring an existing grammar from English to other languages via unannotated parallel data.", "labels": [], "entities": []}, {"text": "The process is illustrated for one English-Italian sentence pair in: the English sentence is parsed by an existing CCG parser and word-aligned to the Italian sentence.", "labels": [], "entities": []}, {"text": "Italian words receive categories equivalent to those of the aligned English words, and a semantically equivalent derivation is built for the Italian sentence.", "labels": [], "entities": []}, {"text": "With enough derivations projected in this way, they can be used to extract a CCG lexicon and to estimate parameter weights for parsing the target language.", "labels": [], "entities": []}, {"text": "Figure 2: Two examples of CCG derivations.", "labels": [], "entities": [{"text": "CCG derivations", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7140169143676758}]}, {"text": "Unlike previous competitive methods for CCG induction such as, our method does not require the training data to be POS-tagged.", "labels": [], "entities": [{"text": "CCG induction", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9343355000019073}]}, {"text": "It also induces more fine-grained labels.", "labels": [], "entities": []}, {"text": "In this paper, we compare the performance of parsers trained using our method to previous induced CCG parsers.", "labels": [], "entities": []}, {"text": "We also investigate whether the cross-lingually induced CCG lexicons correspond with linguistic insights about the target languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "Target Languages Following prior work, we evaluate the induced CCG parsers in terms of unlabeled attachment score (UAS) on the data of the PASCAL unsupervised grammar induction challenge (, which includes eight different languages other than English: Arabic, Czech, Danish, Basque, Dutch, Portuguese, Slovenian, and Swedish.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.8090179165204366}, {"text": "PASCAL unsupervised grammar induction challenge", "start_pos": 139, "end_pos": 186, "type": "TASK", "confidence": 0.6353912949562073}]}, {"text": "For qualitative evaluation, we use German, Italian, and Dutch.", "labels": [], "entities": []}, {"text": "We acknowledge the importance of testing our approach on a more typologically diverse range of languages, but leave this for future work..", "labels": [], "entities": []}, {"text": "However, these treebanks use special categories for punctuation and conjunctions, which would complicate derivation projection.", "labels": [], "entities": [{"text": "derivation projection", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.795902818441391}]}, {"text": "We thus took CCGrebank, automatically transformed it to use normal categories for these cases (an example is shown in), and trained the EasyCCG parser () on that.", "labels": [], "entities": [{"text": "CCGrebank", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.9456397891044617}]}, {"text": "The resulting model was used to produce parses for the English portions of our parallel training corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentence pairs and average num- ber of tokens per target-language sentence in the data  extracted from Tatoeba.", "labels": [], "entities": [{"text": "Tatoeba", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.6036584377288818}]}, {"text": " Table 3: Effects of varying the projection hyperparameter n: percentage of successfully projected source deriva- tions, mean ambiguity (how many target derivations are found per projected source derivation), and UAS of the  trained system on the PASCAL development data (max sentence length 15, not counting punctuation).", "labels": [], "entities": [{"text": "UAS", "start_pos": 213, "end_pos": 216, "type": "METRIC", "confidence": 0.999430239200592}, {"text": "PASCAL development data", "start_pos": 247, "end_pos": 270, "type": "DATASET", "confidence": 0.8302661180496216}]}, {"text": " Table 4: UAS of different systems on the PASCAL test data (max sentence length 15, not counting punctuation).", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9328222274780273}, {"text": "PASCAL test data", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9128606716791788}]}, {"text": " Table 5: Frequency (per sentence) of lexical categories in the output of different parsers when applied to the  Tatoeba data, illustrating learned language-specifics. The averages for English are calculated over all three parallel  training corpora.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9877883791923523}, {"text": "Tatoeba data", "start_pos": 113, "end_pos": 125, "type": "DATASET", "confidence": 0.9704894721508026}]}]}