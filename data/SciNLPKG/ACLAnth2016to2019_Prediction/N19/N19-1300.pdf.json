{"title": [{"text": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "labels": [], "entities": [{"text": "BoolQ", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7749221920967102}]}], "abstractContent": [{"text": "In this paper we study yes/no questions that are naturally occurring-meaning that they are generated in unprompted and unconstrained settings.", "labels": [], "entities": []}, {"text": "We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging.", "labels": [], "entities": [{"text": "BoolQ", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9139247536659241}]}, {"text": "They often query for complex, non-factoid information , and require difficult entailment-like inference to solve.", "labels": [], "entities": []}, {"text": "We also explore the effectiveness of a range of transfer learning baselines.", "labels": [], "entities": []}, {"text": "We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly , continues to be very beneficial even when starting from massive pre-trained language models such as BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 250, "end_pos": 254, "type": "DATASET", "confidence": 0.5040969252586365}]}, {"text": "Our best method trains BERT on MultiNLI and then retrains it on our train set.", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.6698963046073914}, {"text": "MultiNLI", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9565693140029907}]}, {"text": "It achieves 80.4% accuracy compared to 90% accuracy of human anno-tators (and 62% majority-baseline), leaving a significant gap for future work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9992586970329285}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9988786578178406}]}], "introductionContent": [{"text": "Understanding what facts can be inferred to be true or false from text is an essential part of natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.6498051981131235}]}, {"text": "In many cases, these inferences can go well beyond what is immediately stated in the text.", "labels": [], "entities": []}, {"text": "For example, a simple sentence like \"Hanna Huyskova won the gold medal for Belarus in freestyle skiing.\" implies that (1) Belarus is a country, (2) Hanna Huyskova is an athlete, (3) Belarus won at least one Olympic event, (4) the USA did not win the freestyle skiing event, and soon.", "labels": [], "entities": []}, {"text": "To test a model's ability to make these kinds of inferences, previous work in natural language in-Q: Has the UK been hit by a hurricane?", "labels": [], "entities": []}, {"text": "P: The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . .", "labels": [], "entities": []}, {"text": "ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage.", "labels": [], "entities": [{"text": "ference", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9366092681884766}]}, {"text": "However, in practice, generating candidate statements that test for complex inferential abilities is challenging.", "labels": [], "entities": []}, {"text": "For instance, evidence suggests () that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning.", "labels": [], "entities": []}, {"text": "In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions.", "labels": [], "entities": []}, {"text": "That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking.", "labels": [], "entities": []}, {"text": "contains some examples from our dataset.", "labels": [], "entities": []}, {"text": "We find such questions often query for non-factoid information, and that human annotators need to apply a wide range of inferential abilities when answering them.", "labels": [], "entities": []}, {"text": "As a result, they can be used to construct highly inferential reading comprehension datasets that have the added benefit of being directly related to the practical end-task of answering user yes/no questions.", "labels": [], "entities": []}, {"text": "Yes/No questions do appear as a subset of some existing datasets ().", "labels": [], "entities": []}, {"text": "However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.833773386478424}]}, {"text": "We follow the data collection method used by Natural Questions (NQ) ( to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions).", "labels": [], "entities": []}, {"text": "Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer.", "labels": [], "entities": []}, {"text": "The task is then to take a question and passage as input, and to return \"yes\" or \"no\" as output.", "labels": [], "entities": []}, {"text": "Following recent work (, we focus on using transfer learning to establish baselines for our dataset.", "labels": [], "entities": []}, {"text": "Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing.", "labels": [], "entities": [{"text": "Yes/No QA", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.7644432783126831}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7741303741931915}]}, {"text": "Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT) or ELMo ().", "labels": [], "entities": [{"text": "BERT", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9423961043357849}]}, {"text": "We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few other supervised datasets.", "labels": [], "entities": []}, {"text": "We found that transferring from MultiNLI, and the unsupervised pre-training in BERT, gave us the best results.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9167288541793823}, {"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9800564646720886}]}, {"text": "Notably, we found these approaches are surprisingly complementary and can be combined to achieve a large gain in performance.", "labels": [], "entities": []}, {"text": "Overall, our best model reaches 80.43% accuracy, compared to 62.31% for the majority baseline and 90% human accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.999630331993103}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9754053354263306}]}, {"text": "In light of the fact BERT on its own has achieved human-like performance on several NLP tasks, this demonstrates the high degree of difficulty of our dataset.", "labels": [], "entities": [{"text": "BERT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.7886281609535217}]}, {"text": "We present our data and code at https://goo.gl/boolq.", "labels": [], "entities": []}], "datasetContent": [{"text": "An example in our dataset consists of a question, a paragraph from a Wikipedia article, the title of the article, and an answer, which is either \"yes\" or \"no\".", "labels": [], "entities": []}, {"text": "We include the article title since it can potentially help resolve ambiguities (e.g., coreferent phrases) in the passage, although none of the models presented in this paper make use of them.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Question categorization of BoolQ. Question topics are shown in the top half and question types are shown  in the bottom half.", "labels": [], "entities": [{"text": "BoolQ", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.5731940865516663}]}, {"text": " Table 3: Transfer learning results on the BoolQ dev set after fine-tuning on the BoolQ training set. Results are  averaged over five runs. In all cases directly using the pre-trained model without fine-tuning did not achieve results  better than the majority baseline, so we do not include them here.", "labels": [], "entities": [{"text": "BoolQ dev set", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9346758524576823}, {"text": "BoolQ training set", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9513895511627197}]}]}