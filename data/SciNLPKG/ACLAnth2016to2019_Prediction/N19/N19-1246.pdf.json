{"title": [{"text": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "labels": [], "entities": [{"text": "Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "start_pos": 8, "end_pos": 84, "type": "TASK", "confidence": 0.704627949744463}]}], "abstractContent": [{"text": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.", "labels": [], "entities": [{"text": "Reading comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8593054711818695}]}, {"text": "However , a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.", "labels": [], "entities": []}, {"text": "We introduce anew English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs.", "labels": [], "entities": [{"text": "DROP", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8780028820037842}]}, {"text": "In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).", "labels": [], "entities": [{"text": "addition, counting, or sorting", "start_pos": 205, "end_pos": 235, "type": "TASK", "confidence": 0.6234624683856964}]}, {"text": "These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.", "labels": [], "entities": []}, {"text": "We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F 1 on our generalized accuracy metric, while expert human performance is 96.4%.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7323050200939178}, {"text": "F 1", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9891597032546997}, {"text": "accuracy metric", "start_pos": 192, "end_pos": 207, "type": "METRIC", "confidence": 0.9506431818008423}]}, {"text": "We additionally present anew model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F 1 .", "labels": [], "entities": [{"text": "F 1", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9931943416595459}]}], "introductionContent": [{"text": "The task of reading comprehension, where systems must understand a single passage of text well enough to answer arbitrary questions about it, has seen significant progress in the last few years, so much that the most popular datasets available for this task have been solved).", "labels": [], "entities": [{"text": "reading comprehension", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.923682302236557}]}, {"text": "We introduce a substantially more challenging English reading comprehension dataset aimed at pushing the field towards more comprehensive analysis of paragraphs of text.", "labels": [], "entities": []}, {"text": "In * Work done as an intern at the Allen Institute for Artificial Intelligence in this new benchmark, which we call DROP, a system is given a paragraph and a question and must perform some kind of Discrete Reasoning Over the text in the Paragraph to obtain the correct answer.", "labels": [], "entities": []}, {"text": "These questions that require discrete reasoning (such as addition, sorting, or counting; see) are inspired by the complex, compositional questions commonly found in the semantic parsing literature.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7552789449691772}]}, {"text": "We focus on this type of questions because they force a structured analysis of the content of the paragraph that is detailed enough to permit reasoning.", "labels": [], "entities": []}, {"text": "Our goal is to further paragraph understanding; complex questions allow us to test a system's understanding of the paragraph's semantics.", "labels": [], "entities": [{"text": "paragraph understanding", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8531407713890076}]}, {"text": "DROP is also designed to further research on methods that combine distributed representations with symbolic, discrete reasoning.", "labels": [], "entities": [{"text": "DROP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7980924844741821}]}, {"text": "In order to do well on this dataset, a system must be able to find multiple occurrences of an event described in a question (presumably using some kind of soft matching), extract arguments from the events, then perform a numerical operation such as a sort, to answer a question like \"Who threw the longest touchdown pass?\".", "labels": [], "entities": []}, {"text": "We constructed this dataset through crowdsourcing, first collecting passages from Wikipedia that are easy to ask hard questions about, then encouraging crowd workers to produce challenging questions.", "labels": [], "entities": []}, {"text": "This encouragement was partially through instructions given to workers, and partially through the use of an adversarial baseline: we ran a baseline reading comprehension method (BiDAF) () in the background as crowd workers were writing questions, requiring them to give questions that the baseline system could not correctly answer.", "labels": [], "entities": [{"text": "baseline reading comprehension method (BiDAF)", "start_pos": 139, "end_pos": 184, "type": "METRIC", "confidence": 0.6330269149371556}]}, {"text": "This resulted in a dataset of 96,567 questions from a variety of categories in Wikipedia, with a particular emphasis on sports game summaries and history passages.", "labels": [], "entities": []}, {"text": "The answers to the questions are required to be spans in the passage or question, numbers, or dates, which allows for easy and accurate evaluation metrics.", "labels": [], "entities": []}, {"text": "We present an analysis of the resulting dataset to show what phenomena are present.", "labels": [], "entities": []}, {"text": "We find that many questions combine complex question semantics with SQuAD-style argument finding; e.g., in the first question in, BiDAF correctly finds the amount the painting sold for, but does not understand the question semantics and cannot perform the numerical reasoning required to answer the question.", "labels": [], "entities": [{"text": "SQuAD-style argument finding", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.7454352378845215}, {"text": "BiDAF", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.8533127903938293}]}, {"text": "Other questions, such as the fifth question in, require finding all events in the passage that match a description in the question, then aggregating them somehow (in this instance, by counting them and then performing an argmax).", "labels": [], "entities": []}, {"text": "Very often entity coreference is required.", "labels": [], "entities": [{"text": "entity coreference", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8020122051239014}]}, {"text": "gives a number of different phenomena, with their proportions in the dataset.", "labels": [], "entities": []}, {"text": "We used three types of systems to judge baseline performance on DROP: (1) heuristic baselines, to check for biases in the data; (2) SQuAD-style reading comprehension methods; and (3) semantic parsers operating on a pipelined analysis of the passage.", "labels": [], "entities": []}, {"text": "The reading comprehension methods perform the best, with our best baseline achieving 32.7% F 1 on our generalized accuracy metric, while expert human performance is 96.4%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9907721281051636}, {"text": "accuracy metric", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.867707371711731}]}, {"text": "Finally, we contribute anew model for this task that combines limited numerical reasoning with standard reading comprehension methods, allowing the model to answer questions involving counting, addition and subtraction.", "labels": [], "entities": []}, {"text": "This model reaches 47% F 1 , a 14.3% absolute increase over the best baseline system.", "labels": [], "entities": [{"text": "F 1", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9956145286560059}]}, {"text": "The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example questions and answers from the DROP dataset, showing the relevant parts of the associated  passage and the reasoning required to answer the question.", "labels": [], "entities": [{"text": "DROP dataset", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9409275054931641}]}, {"text": " Table 3: Distribution of answer types in training set,  according to an automatic named entity recognition.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the different models on our de- velopment and test set, in terms of Exact Match (EM),  and numerically-focused F 1 ( \u00a75). Both metrics are cal- culated as the maximum against a set of gold answers.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.9600188970565796}, {"text": "F 1", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.8833860456943512}]}, {"text": " Table 5: Representative examples from our model's error analysis. We list the identified semantic phenomenon,  the relevant passage highlights, a gold question-answer pair, and the erroneous prediction by our model.", "labels": [], "entities": []}, {"text": " Table 6: Dev set performance breakdown by different  answer types; our model (NAQANet, marked as QN+)  vs. BERT, the best-performing baseline.", "labels": [], "entities": [{"text": "BERT", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9976050853729248}]}]}