{"title": [], "abstractContent": [{"text": "Critical to natural language generation is the production of correctly inflected text.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7069468100865682}]}, {"text": "In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemma-tized version.", "labels": [], "entities": [{"text": "predicting a fully inflected sentence", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8357734441757202}]}, {"text": "Unlike traditional morphological inflection or surface realization, our task input does not provide \"gold\" tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7851011455059052}]}, {"text": "We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation.", "labels": [], "entities": []}, {"text": "We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP systems are often required to generate grammatical text, e.g., in machine translation, summarization, dialogue, and grammar correction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7707645893096924}, {"text": "summarization", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.9810583591461182}, {"text": "grammar correction", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.6674178242683411}]}, {"text": "One component of grammaticality is the use of contextually appropriate closed-class morphemes.", "labels": [], "entities": []}, {"text": "In this work, we study contextual inflection, which has been recently introduced in the CoNLL-SIGMORPHON 2018 shared task () to directly investigate context-dependent morphology in NLP.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 shared task", "start_pos": 88, "end_pos": 121, "type": "DATASET", "confidence": 0.7883133292198181}]}, {"text": "There, a system must inflect partially lemmatized tokens in sentential context.", "labels": [], "entities": []}, {"text": "For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting.", "labels": [], "entities": []}, {"text": "Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s.", "labels": [], "entities": []}, {"text": "Most past work in supervised computational morphology-including the previous CoNLL-SIGMORPHON shared tasks on morphological reinflection ()-has focused mainly on step (3) above.", "labels": [], "entities": []}, {"text": "As the task has been introduced into the literature only recently, we provide some background.", "labels": [], "entities": []}, {"text": "Contextual inflection amounts to a highly constrained version of language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7169291228055954}]}, {"text": "Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metricperplexity-is dominated by the language model's ability to predict content, which is where most of the uncertainty lies.", "labels": [], "entities": [{"text": "Language modeling predicts all words of a sentence", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8099791593849659}]}, {"text": "Our task focuses on just the ability to reconstruct certain missing parts of the sentence-inflectional morphemes and their orthographic realization.", "labels": [], "entities": []}, {"text": "This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take aback seat in current language models (see.", "labels": [], "entities": []}, {"text": "Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab.", "labels": [], "entities": [{"text": "content modeling", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.685993492603302}, {"text": "Tab.", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9291318356990814}]}, {"text": "1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar-the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input.", "labels": [], "entities": []}, {"text": "Conversely, this example does still require predicting some content-the semantic choice of past tense is not given by the input and must be guessed by the system.", "labels": [], "entities": []}, {"text": "The primary contribution of this paper is a novel structured neural model for contextual inflection.", "labels": [], "entities": []}, {"text": "The model first predicts the sequence of morphological tags from the partially lemmatized sequence and, then, it uses the predicted tag and lemma to inflect the word.", "labels": [], "entities": []}, {"text": "We use this model to evince a simple point: models are better off jointly predicting morphological tags from context than directly learning to inflect lemmata from sentential context.", "labels": [], "entities": []}, {"text": "Indeed, none of the participants in the 2018 shared task jointly predicted tags with the inflected forms.", "labels": [], "entities": []}, {"text": "Comparing our new model to several competing systems, we show our model has the best performance on the majority of languages.", "labels": [], "entities": []}, {"text": "We take this as evidence that predicting morphological tags jointly with inflecting is a better method for this task.", "labels": [], "entities": [{"text": "predicting morphological tags", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8762932817141215}]}, {"text": "Furthermore, we provide an analysis discussing the role of morphological complexity in model performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Universal Dependencies v1.2 dataset (Nivre et al., 2016) for our experiments.", "labels": [], "entities": [{"text": "Universal Dependencies v1.2 dataset", "start_pos": 11, "end_pos": 46, "type": "DATASET", "confidence": 0.6279075518250465}]}, {"text": "We include all the languages with information on their lemmata and fine-grained grammar tag annotation that also have fasttext embeddings (, which are used for word embedding initialization.", "labels": [], "entities": [{"text": "word embedding initialization", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.6383556723594666}]}, {"text": "We evaluate our model's ability to predict: (i) the correct morphological tags from the lemma context, and (ii) the correct inflected forms.", "labels": [], "entities": []}, {"text": "As our evaluation metric, we report 1-best accuracy for both tags and word form prediction.", "labels": [], "entities": [{"text": "1-best", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9292157888412476}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9131966233253479}, {"text": "word form prediction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.6830624143282572}]}, {"text": "We use a word and character embedding dimensionality of 300 and 100, respectively.", "labels": [], "entities": []}, {"text": "The hidden state dimensionality is set to 200.", "labels": [], "entities": []}, {"text": "All models are trained with, with a learning rate of 0.001 for 20 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9792749285697937}]}, {"text": "We use two baseline systems: (1) the CoNLL-SIGMORPHON 2018 subtask 2 neural encoder-decoder with an attention mechanism (\"SM\"; Cotterell et al.  lemmata and tag representations, and then the decoder generates the target inflected form characterby-character; and (2) a monolingual version of the best performing system of the shared task (\"CPH\";) that augments the above encoder-decoder with full (sentence-level) left and right contexts (comprising of forms, their lemmata and morphological tags) as well as predicts morphological tags fora target form as an auxiliary task.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2018 subtask", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.9291186332702637}]}, {"text": "In both cases, the hyperparameters are set as described in.", "labels": [], "entities": []}, {"text": "We additionally evaluate the SIGMORPHON baseline system on prediction of the target form without any information on morphological tags (\"DIRECT\").", "labels": [], "entities": [{"text": "DIRECT", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9053005576133728}]}], "tableCaptions": [{"text": " Table 2: Accuracy of the models for various predic- tion settings. tag refers to tag prediction accuracy, and  form to form prediction accuracy. Our model is JOINT;  GOLD denotes form prediction conditioned on gold tar- get morphological tags; the other columns are baseline  methods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9742916226387024}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8196533918380737}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8207189440727234}, {"text": "JOINT", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.5024683475494385}, {"text": "GOLD", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9855072498321533}]}]}