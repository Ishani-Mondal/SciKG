{"title": [{"text": "Neural Grammatical Error Correction with Finite State Transducers", "labels": [], "entities": [{"text": "Neural Grammatical Error Correction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8149460405111313}]}], "abstractContent": [{"text": "Grammatical error correction (GEC) is one of the areas in natural language processing in which purely neural models have not yet superseded more traditional symbolic models.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8544292946656545}, {"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6820027629534403}]}, {"text": "Hybrid systems combining phrase-based statistical machine translation (SMT) and neural sequence models are currently among the most effective approaches to GEC.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.7361449982438769}, {"text": "GEC", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9628216028213501}]}, {"text": "However, both SMT and neural sequence-to-sequence models require large amounts of annotated data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9939295053482056}]}, {"text": "Language model based GEC (LM-GEC) is a promising alternative which does not rely on annotated training data.", "labels": [], "entities": [{"text": "Language model based GEC", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5384931862354279}]}, {"text": "We show how to improve LM-GEC by applying modelling techniques based on finite state transducers.", "labels": [], "entities": []}, {"text": "We report further gains by rescoring with neural language models.", "labels": [], "entities": []}, {"text": "We show that our methods developed for LM-GEC can also be used with SMT systems if annotated training data is available.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9868072867393494}]}, {"text": "Our best system outperforms the best published result on the CoNLL-2014 test set, and achieves far better relative improvements over the SMT baselines than previous hybrid systems.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9780797163645426}, {"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9519360661506653}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction \u2192 In such a situation].", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7693669448296229}]}, {"text": "Using neural models for GEC is becoming increasingly popular (, possibly combined with phrase-based SMT (.", "labels": [], "entities": [{"text": "GEC", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9086604714393616}, {"text": "phrase-based SMT", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.5375315696001053}]}, {"text": "A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence.", "labels": [], "entities": []}, {"text": "GEC is -compared to machine translation -a highly constrained problem as corrections tend to be very local, and lexical choices are usually limited.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.851780354976654}, {"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7275822907686234}]}, {"text": "Finite state transducers (FSTs) are an efficient way to represent large structured search spaces.", "labels": [], "entities": [{"text": "Finite state transducers (FSTs)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7271364231904348}]}, {"text": "In this paper, we propose to construct a hypothesis space using standard FST operations like composition, and then constrain the output of a neural GEC system to that space.", "labels": [], "entities": [{"text": "FST", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8424307107925415}]}, {"text": "We study two different scenarios: In the first scenario, we do not have access to annotated training data, and only use a small development set for tuning.", "labels": [], "entities": []}, {"text": "In this scenario, we construct the hypothesis space using word-level context-independent confusion sets) based on spell checkers and morphology databases, and rescore it with count-based and neural language models (NLMs).", "labels": [], "entities": []}, {"text": "In the second scenario, we assume to have enough training data available to train SMT and neural machine translation (NMT) systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9956497550010681}, {"text": "neural machine translation (NMT)", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7976518968741099}]}, {"text": "In this case, we make additional use of the SMT lattice and rescore with an NLM-NMT ensemble.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9736602902412415}]}, {"text": "Our contributions are: \u2022 We present an FST-based adaptation of the work of which allows exact inference, and does not require annotated training data.", "labels": [], "entities": [{"text": "FST-based adaptation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.785918265581131}]}, {"text": "We report large gains from rescoring with a neural language model.", "labels": [], "entities": []}, {"text": "\u2022 Our technique beats the best published result with comparable amounts of training data on the) test set when applied to SMT lattices.", "labels": [], "entities": [{"text": "SMT lattices", "start_pos": 122, "end_pos": 134, "type": "TASK", "confidence": 0.9264603555202484}]}, {"text": "Our combination strategy yields larger gains over the SMT baselines than simpler rescoring or pipelining used in prior work on hybrid systems (Grundkiewicz and Junczys-Dowmunt, 2018).", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9816338419914246}]}], "datasetContent": [{"text": "Experimental setup In our experiments with annotated training data we use the SMT system of Junczys-Dowmunt and to create 1000-best lists from which we derive the input lattices I.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.955828845500946}]}, {"text": "All our LMs are trained on the One Billion Word Benchmark dataset ().", "labels": [], "entities": [{"text": "One Billion Word Benchmark dataset", "start_pos": 31, "end_pos": 65, "type": "DATASET", "confidence": 0.5951909601688385}]}, {"text": "Our neural LM is a Transformer decoder architecture in the transformer base configuration trained with).", "labels": [], "entities": []}, {"text": "Our NMT model is a Transformer model (transformer base) trained on the concatenation of the NUCLE corpus () and the Lang-8 Corpus of Learner English v1.0 ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8759720325469971}, {"text": "NUCLE corpus", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9820958971977234}, {"text": "Lang-8 Corpus of Learner English v1.0", "start_pos": 116, "end_pos": 153, "type": "DATASET", "confidence": 0.9600425461928049}]}, {"text": "We only keep sentences with at least one correction (659K sentences in total).", "labels": [], "entities": []}, {"text": "Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.5500702857971191}]}, {"text": "We delay SGD updates by 2 on four physical GPUs as suggested by Similarly to, even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning.", "labels": [], "entities": []}, {"text": "https://github.com/grammatical/ baselines-emnlp2016.", "labels": [], "entities": []}, {"text": "We decode with beam size 12 using the SGNMT decoder (.", "labels": [], "entities": [{"text": "SGNMT decoder", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.867923378944397}]}, {"text": "We evaluate on) and JFLEG-Test, using) and JFLEGDev as development sets.", "labels": [], "entities": [{"text": "JFLEG-Test", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.791236937046051}, {"text": "JFLEGDev", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8650669455528259}]}, {"text": "Our evaluation metrics are GLEU () and M2.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9977667331695557}]}, {"text": "We generated M2 files using ERRANT) for JFLEG and Tab.", "labels": [], "entities": [{"text": "ERRANT", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9354124665260315}, {"text": "Tab", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7872651219367981}]}, {"text": "1 to be comparable to, but used the official M2 files in Tab.", "labels": [], "entities": [{"text": "M2 files in Tab", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.8425553143024445}]}, {"text": "2 to be comparable to.", "labels": [], "entities": []}, {"text": "Results Our LM-based GEC results without using annotated training data are summarized in Tab.", "labels": [], "entities": [{"text": "GEC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6123762130737305}, {"text": "Tab.", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9742011427879333}]}, {"text": "1. Even when we use the same resources (same LM and same confusion sets) as, we see gains on JFLEG (rows 1 vs. 2), probably because we avoid search errors in our FST-based scheme.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.7687573432922363}]}, {"text": "Adding an NLM yields significant gains across the board.", "labels": [], "entities": []}, {"text": "2 shows that adding confusion sets to SMT lattices is effective even without neural models (rows 3 vs. 4).", "labels": [], "entities": [{"text": "SMT lattices", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.9393219649791718}]}, {"text": "Rescoring with neural models also benefits from the confusion sets (rows 5 vs. 6).", "labels": [], "entities": []}, {"text": "With our ensemble systems (rows 7 and 8) we are able to outperform prior work   come within 3 GLEU on JFLEG.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9978686571121216}, {"text": "JFLEG", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8852576017379761}]}, {"text": "Since the baseline SMT systems of were better than the ones we used, we achieve even higher relative gains over the respective SMT baselines (Tab. 3).", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9817080497741699}, {"text": "SMT", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.9446694254875183}]}, {"text": "Error type analysis We also carried out a more detailed error type analysis of the best CoNLL-2014 M2 system with/without training data using ERRANT (Tab. 4).", "labels": [], "entities": [{"text": "CoNLL-2014 M2", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8462153077125549}, {"text": "ERRANT", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.8725588321685791}]}, {"text": "Specifically, this table shows that while the trained system was consistently better than the untrained system, the degree of the improvement differs significantly depending on the error type.", "labels": [], "entities": []}, {"text": "In particular, since the untrained system was only designed to handle Replacement word errors, much of the improvement in the trained system comes from the ability to correct Missing and Unnecessary word errors.", "labels": [], "entities": []}, {"text": "The trained system nevertheless still improves upon the untrained system in terms of replacement errors by 10 F 0.5 (45.53 vs. 55.63).", "labels": [], "entities": [{"text": "replacement errors", "start_pos": 85, "end_pos": 103, "type": "METRIC", "confidence": 0.9328925311565399}]}, {"text": "In terms of more specific error types, the trained system was also able to capture a wider variety of error types, including content word errors (adjectives, adverbs, nouns and verbs) and other categories such as pronouns and punctuation.", "labels": [], "entities": []}, {"text": "Since the untrained system only targets spelling, orthographic and morphological errors however, it is interesting to note that the difference in scores between these categories tends to be smaller than others; e.g. noun number (53.43 vs 64.96), orthography (62.77 vs 74.07), spelling (67.91 vs 75.21) and subject-verb agreement (66.67 vs 68.39).", "labels": [], "entities": []}, {"text": "This suggests that an untrained system is already able to capture the majority of these error types.", "labels": [], "entities": []}, {"text": "Oracle experiments Our FST-based composition cascade is designed to enrich the search space to allow the neural models to find better hypotheses.", "labels": [], "entities": [{"text": "FST-based composition cascade", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6730815966924032}]}, {"text": "5 reports the oracle sentence error rate for different configurations, i.e. the fraction of reference sentences in the test set which are not in the FSTs.", "labels": [], "entities": [{"text": "error rate", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8237576186656952}, {"text": "FSTs", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8789353966712952}]}, {"text": "Expanding the SMT lattice significantly reduces the oracle error rate from 55.63% to 48.17%.", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.975186288356781}, {"text": "error rate", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9352754950523376}]}], "tableCaptions": [{"text": " Table 1: Results without using annotated training data. Systems are tuned with respect to the metric highlighted in  gray. Input lattices I are derived from the source sentence as in", "labels": [], "entities": []}, {"text": " Table 2: Results with using annotated training data. Systems are tuned with respect to the metric highlighted in  gray. Input lattices I are derived from the Moses 1000-best list as in", "labels": [], "entities": [{"text": "Moses 1000-best list", "start_pos": 159, "end_pos": 179, "type": "DATASET", "confidence": 0.8408336043357849}]}, {"text": " Table 3: Improvements over SMT baselines.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9829902648925781}]}, {"text": " Table 4: A selection of ERRANT F 0.5 error type  scores comparing the best CoNLL-2014 system with  and without training data. A dash means the system  did not attempt to correct the error type.", "labels": [], "entities": [{"text": "ERRANT F 0.5 error type", "start_pos": 25, "end_pos": 48, "type": "METRIC", "confidence": 0.9175112962722778}, {"text": "CoNLL-2014", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.853473961353302}]}, {"text": " Table 5: Oracle error rates for different hypothesis  spaces using the first annotator in CoNLL-2014.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.9408524036407471}]}]}