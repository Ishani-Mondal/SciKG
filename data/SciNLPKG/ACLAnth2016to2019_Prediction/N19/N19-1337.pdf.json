{"title": [{"text": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "labels": [], "entities": [{"text": "Interpretability of Link Prediction", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.5781664103269577}]}], "abstractContent": [{"text": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.", "labels": [], "entities": []}, {"text": "Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9982264637947083}]}, {"text": "In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction fora target fact after the model is retrained.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7194020599126816}]}, {"text": "Using these single modifications of the graph, we identify the most influential fact fora predicted link and evaluate the sensitivity of the model to the addition of fake facts.", "labels": [], "entities": []}, {"text": "We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes.", "labels": [], "entities": []}, {"text": "To avoid the combinato-rial search overall possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification.", "labels": [], "entities": []}, {"text": "We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7023103833198547}]}], "introductionContent": [{"text": "Knowledge graphs (KG) play a critical role in many real-world applications such as search, structured data management, recommendations, and question answering.", "labels": [], "entities": [{"text": "structured data management", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7099821368853251}, {"text": "question answering", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.8861674070358276}]}, {"text": "Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts.", "labels": [], "entities": []}, {"text": "These dense representation models for link prediction include tensor factorization, algebraic operations, multiple embeddings [, and complex neural models.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7864998877048492}]}, {"text": "However, there are only a few studies that investigate the quality of the different KG models.", "labels": [], "entities": []}, {"text": "There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.998440682888031}, {"text": "link prediction", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8012276291847229}]}, {"text": "In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are relearned, which we collectively call Completion Robustness and Interpretability via Adversarial Graph Edits (CRIAGE).", "labels": [], "entities": []}, {"text": "First, we consider perturbations that remove a neighboring link for the target fact, thus identifying the most influential related fact, providing an explanation for the model's prediction.", "labels": [], "entities": []}, {"text": "As an example, consider the excerpt from a KG in with two observed facts, and a target predicted fact that Princes Henriette is the parent of Violante Bavaria.", "labels": [], "entities": []}, {"text": "Our proposed graph perturbation, shown in, identifies the existing fact that Ferdinal Maria is the father of Violante Bavaria as the one when removed and model retrained, will change the prediction of Princes Henriette's child.", "labels": [], "entities": []}, {"text": "We also study attacks that add anew, fake fact into the KG to evaluate the robustness and sensitivity of link prediction models to small additions to the graph.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.7080170065164566}]}, {"text": "An example attack for the original graph in, is depicted in.", "labels": [], "entities": []}, {"text": "Such perturbations to the the training data are from a family of adversarial modifications that have been applied to other machine learning tasks, known as poisoning, Biggio: Completion Robustness and Interpretability via Adversarial Graph Edits (CRIAGE): Change in the graph structure that changes the prediction of the retrained model, where (a) is the original sub-graph of the KG, (b) removes a neighboring link of the target, resulting in a change in the prediction, and (c) shows the effect of adding an attack triple on the target.", "labels": [], "entities": []}, {"text": "These modifications were identified by our proposed approach.", "labels": [], "entities": []}, {"text": "Since the setting is quite different from traditional adversarial attacks, search for link prediction adversaries brings up unique challenges.", "labels": [], "entities": [{"text": "link prediction adversaries", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.7628331979115804}]}, {"text": "To find these minimal changes fora target link, we need to identify the fact that, when added into or removed from the graph, will have the biggest impact on the predicted score of the target fact.", "labels": [], "entities": []}, {"text": "Unfortunately, computing this change in the score is expensive since it involves retraining the model to recompute the embeddings.", "labels": [], "entities": []}, {"text": "We propose an efficient estimate of this score change by approximating the change in the embeddings using Taylor expansion.", "labels": [], "entities": []}, {"text": "The other challenge in identifying adversarial modifications for link prediction, especially when considering addition of fake facts, is the combinatorial search space over possible facts, which is intractable to enumerate.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.8074828088283539}]}, {"text": "We introduce an inverter of the original embedding model, to decode the embeddings to their corresponding graph components, making the search of facts tractable by performing efficient gradient-based continuous optimization.", "labels": [], "entities": []}, {"text": "We evaluate our proposed methods through following experiments.", "labels": [], "entities": []}, {"text": "First, on relatively small KGs, we show that our approximations are accurate compared to the true change in the score.", "labels": [], "entities": []}, {"text": "Second, we show that our additive attacks can effectively reduce the performance of state of the art models up to 27.3% and 50.7% in Hits@1 for two large KGs: WN18 and YAGO3-10.", "labels": [], "entities": [{"text": "WN18", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.8868013024330139}, {"text": "YAGO3-10", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.8869956731796265}]}, {"text": "We also explore the utility of adversarial modifications in explaining the model predictions by presenting rule-like descriptions of the most influential neighbors.", "labels": [], "entities": []}, {"text": "Finally, we use adversaries to detect errors in the KG, obtaining up to 55% accuracy in detecting errors.", "labels": [], "entities": [{"text": "KG", "start_pos": 52, "end_pos": 54, "type": "DATASET", "confidence": 0.8519628047943115}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.999471127986908}]}], "datasetContent": [{"text": "Datasets To evaluate our method, we conduct several experiments on four widely used KGs.", "labels": [], "entities": []}, {"text": "To validate the accuracy of the approximations, we use smaller sized Kinship and Nations KGs for which we can make comparisons against more expensive but less approximate approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993914365768433}, {"text": "Kinship and Nations KGs", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.7611734420061111}]}, {"text": "For the remaining experiments, we use YAGO3-10 and WN18 KGs, which are closer to real-world KGs in their size and characteristics (see).", "labels": [], "entities": [{"text": "YAGO3-10", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.7221469879150391}, {"text": "WN18 KGs", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9073835015296936}]}, {"text": "Models We implement all methods using the same loss and optimization for training, i.e., AdaGrad and the binary cross-entropy loss.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7453965544700623}]}, {"text": "We use validation data to tune the hyperparameters and use a grid search to find the best hyperparameters, such as regularization parameter, and learning rate of the gradient-based method.", "labels": [], "entities": []}, {"text": "To capture the effect of our method on link prediction task, we study the change in commonly-used metrics for evaluation in this task: mean reciprocal rank (MRR) and Hits@K. Further, we use the same hyperparameters as in Dettmers et al. for training link prediction models for these knowledge graphs.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8816955884297689}, {"text": "mean reciprocal rank (MRR)", "start_pos": 135, "end_pos": 161, "type": "METRIC", "confidence": 0.9340595304965973}, {"text": "link prediction", "start_pos": 250, "end_pos": 265, "type": "TASK", "confidence": 0.7565275132656097}]}, {"text": "Influence Function We also compare our method with influence function (IF).", "labels": [], "entities": []}, {"text": "The influence function approximates the effect of upweighting a training sample on the loss fora specific test point.", "labels": [], "entities": []}, {"text": "We use IF to approximate the change in the loss after removing a triple as:  We evaluate CRIAGE by (6.1) comparing CRIAGE estimate with the actual effect of the attacks, (6.2) studying the effect of adversarial attacks on evaluation metrics, (6.3) exploring its application to the interpretability of KG representations, and (6.4) detecting incorrect triples.", "labels": [], "entities": [{"text": "IF", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9650070071220398}, {"text": "interpretability of KG representations", "start_pos": 281, "end_pos": 319, "type": "TASK", "confidence": 0.7608774602413177}]}], "tableCaptions": [{"text": " Table 1: Inverter Functions Accuracy, we calculate  the accuracy of our inverter networks in correctly re- covering the pairs of subject and relation from the test  set of our benchmarks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8850667476654053}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9994113445281982}]}, {"text": " Table 2: Data Statistics of the benchmarks.", "labels": [], "entities": []}, {"text": " Table 3: Ranking modifications by their impact on the target. We compare the true ranking of candidate triples  with a number of approximations using ranking correlation coefficients. We compare our method with influence  function (IF) with and without Hessian, and ranking the candidates based on their score, on two KGs (d = 10,  averaged over 10 random targets). For the sake of brevity, we represent the Spearman's \u03c1 and Kendall's \u03c4 rank  correlation coefficients simply as \u03c1 and \u03c4 .", "labels": [], "entities": [{"text": "influence  function (IF)", "start_pos": 212, "end_pos": 236, "type": "METRIC", "confidence": 0.7971642136573791}, {"text": "Kendall's \u03c4 rank  correlation", "start_pos": 426, "end_pos": 455, "type": "METRIC", "confidence": 0.6327751219272614}]}, {"text": " Table 4: Robustness of Representation Models, the effect of adversarial attack on link prediction task. We  consider two scenario for the target triples, 1) choosing the whole test dataset as the targets (All-Test) and 2)  choosing a subset of test data that models are uncertain about them (Uncertain-Test).", "labels": [], "entities": [{"text": "link prediction", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.8155095875263214}]}, {"text": " Table 6: Error Detection Accuracy in the neighbor- hood of 100 chosen samples. We choose the neighbor  with the least value of \u2206 (s ,r ) (s, r, o) as the incorrect  fact. This experiment assumes we know each target  fact has exactly one error.", "labels": [], "entities": [{"text": "Error Detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6993600577116013}, {"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.5025091767311096}]}]}