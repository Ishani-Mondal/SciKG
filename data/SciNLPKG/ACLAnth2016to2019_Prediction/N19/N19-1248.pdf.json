{"title": [{"text": "Highly Effective Arabic Diacritization using Sequence to Sequence Modeling", "labels": [], "entities": [{"text": "Arabic Diacritization", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6024751961231232}, {"text": "Sequence to Sequence Modeling", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6216750293970108}]}], "abstractContent": [{"text": "Arabic text is typically written without short vowels (or diacritics).", "labels": [], "entities": []}, {"text": "However, their presence is required for properly verbalizing Ara-bic and is hence essential for applications such as text to speech.", "labels": [], "entities": []}, {"text": "There are two types of dia-critics, namely core-word diacritics and case-endings.", "labels": [], "entities": []}, {"text": "Most previous works on automatic Arabic diacritic recovery rely on a large number of manually engineered features, particularly for case-endings.", "labels": [], "entities": [{"text": "automatic Arabic diacritic recovery", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.6156492531299591}]}, {"text": "In this work, we present a unified character level sequence-to-sequence deep learning model that recovers both types of diacritics without the use of explicit feature engineering.", "labels": [], "entities": []}, {"text": "Specifically, we employ a standard neural machine translation setup on overlapping windows of words (broken down into characters), and then we use voting to select the most likely diacritized form of a word.", "labels": [], "entities": []}, {"text": "The proposed model outperforms all previous state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Our best settings achieve a word error rate (WER) of 4.49% compared to the state-of-the-art of 12.25% on a standard dataset.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 28, "end_pos": 49, "type": "METRIC", "confidence": 0.8957590560118357}]}], "introductionContent": [{"text": "Arabic uses two types of vowels, namely long vowels, which are explicitly placed in the text, and short vowels, which are diacritic marks that are typically omitted during writing.", "labels": [], "entities": []}, {"text": "In order to read Arabic words properly, readers need to reintroduce the missing diacritics.", "labels": [], "entities": []}, {"text": "Therefore, accurate diacritic recovery is essential for some applications such as text-to-speech.", "labels": [], "entities": [{"text": "diacritic recovery", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6596667766571045}]}, {"text": "There are, in turn, two types of Arabic diacritics, namely coreword diacritics (CW), which specify lexical selection, and case endings (CE), which typically indicate syntactic role.", "labels": [], "entities": [{"text": "case endings (CE)", "start_pos": 122, "end_pos": 139, "type": "METRIC", "confidence": 0.6630163967609406}]}, {"text": "For example, the word \"AlElm\" 1 can accept many possible core-word diacritics depending on the intended meaning In this paper, we use Buckwalter transliteration.", "labels": [], "entities": []}, {"text": "such as: \"AaloEalam\" (the flag) and \"AaloEilom\" (the knowledge/science).", "labels": [], "entities": []}, {"text": "In our training corpus, 17.1% of the word-cores have more than one valid diacritized form.", "labels": [], "entities": []}, {"text": "In the sentence \"Zahara AaloEalamu\" (the flag appeared), \"AaloEalamu\" is the subject and takes the case ending \"u\", and in the sentence \"ra>ayotu AaloEalama\" (I saw the flag), \"AaloEalama\" is the object and takes the case ending \"a\".", "labels": [], "entities": []}, {"text": "Aside from function words, past tense and accusative verb forms, and foreign names, most words can accept different caseendings depending on context.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a unified model for both diacritic types while improving upon the state-of-the-art.", "labels": [], "entities": []}, {"text": "Specifically, we approached the task as a sequence-to-sequence (seq2seq) problem ( ; taking advantage of the recent advancements in Neural Machine Translation (NMT) ( among other applications where seq2seq models made a breakthrough (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.8310324947039286}]}, {"text": "Using the analogy of translation which employs a sequential encoder and a sequential decoder, the input undiacritized text will be encoded and then decoded into diacritized form.", "labels": [], "entities": []}, {"text": "As we show later, directly applying a seq2seq model at sentence level using word or character representations produces nearly unusable results that are much worse than the state-of-the-art due to word insertions, omissions, and substitutions.", "labels": [], "entities": [{"text": "word insertions", "start_pos": 196, "end_pos": 211, "type": "TASK", "confidence": 0.6920672208070755}]}, {"text": "Such problems are exaggerated when using word-based models due to Out-Of-Vocabulary words (OOVs).", "labels": [], "entities": []}, {"text": "Conversely, character-based models suffer from not learning long-term dependencies.", "labels": [], "entities": []}, {"text": "To avoid these problems, we train a seq2seq model on a sliding window of words that are represented using characters, and we employ voting to pick the best most likely diacritized form from different windows.", "labels": [], "entities": []}, {"text": "In doing so, we provide suffi-cient context to properly guess proper diacritized forms, while stinting the aforementioned undesirable word operations.", "labels": [], "entities": []}, {"text": "Further, the use of voting has the effect of picking the most frequent diacritized form obtained from applying the model on different contexts.", "labels": [], "entities": []}, {"text": "The resultant system makes full use of NMT machinery to achieve error rates that are 63.3% lower than the best state-of-the-art system 2 . The contributions in this paper are: \u2022 Adaptation of neural machine translation for Arabic diacritic recovery with voting.", "labels": [], "entities": [{"text": "error rates", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9665875136852264}, {"text": "neural machine translation", "start_pos": 192, "end_pos": 218, "type": "TASK", "confidence": 0.7518427769343058}, {"text": "Arabic diacritic recovery", "start_pos": 223, "end_pos": 248, "type": "TASK", "confidence": 0.6330491801102957}]}, {"text": "\u2022 Unified model to handle both core-diacritics and case-endings.", "labels": [], "entities": []}, {"text": "\u2022 Substantial improvement over state-of-theart with 4.49% word error rate compared to 12.25%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.7687908212343851}]}], "datasetContent": [{"text": "We used a modern diacritized corpus of 4.5 million tokens that covers a wide range of topics such as politics, religion, sport, health, and economics.", "labels": [], "entities": []}, {"text": "For testing, we used the freely available WikiNews corpus (18,300 words) () as a test set, which covers a variety of genres.", "labels": [], "entities": [{"text": "WikiNews corpus", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9103361368179321}]}, {"text": "reports the size of the training and test sets including the unique diacritized and undiacritized tokens and the percentage of OOVs in the test set that don't appear in the training set.", "labels": [], "entities": []}, {"text": "We randomly used 10% of the train data for validation and the rest for training.", "labels": [], "entities": [{"text": "validation", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9657924771308899}]}, {"text": "We used a sequence length of 100, 500 and 7 tokens for word-, character-, and window-based systems respectively.", "labels": [], "entities": []}, {"text": "The vocabulary is restricted to 100k words types and 1,000 character units.", "labels": [], "entities": []}, {"text": "The settings for LSTM-based Seq2Seq model were: word embeddings and LSTM states = 512; 2 layer unidirectional LSTM;   System Runs.", "labels": [], "entities": []}, {"text": "We conducted a variety of experiments as follows, namely: Word-level experiments where the input is a sequence of words and the output is a sequence of diacritized words: -Baseline Word: uses the full sentences and shows the deficiency of using NMT directly.", "labels": [], "entities": []}, {"text": "-Word 7g: uses non-overlapping windows of 7 words to compare to our best character-level model, which also uses a window of length 7.", "labels": [], "entities": []}, {"text": "-Word 7g+overlap: uses a sliding window of 7 words.", "labels": [], "entities": [{"text": "overlap", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.7132447957992554}]}, {"text": "Character-level experiments where the input is represented as a sequence of character and the output as a sequence of diacritized characters: -Baseline Char: uses the full sentence.", "labels": [], "entities": []}, {"text": "-Char 7g: uses non-overlapping sequences of 7 words.", "labels": [], "entities": []}, {"text": "-Char 7g+overlap: uses a sliding window of 7 words without voting.", "labels": [], "entities": [{"text": "overlap", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.8882988095283508}]}, {"text": "-Char ng-overlap+voting: uses a sliding window of n words with voting, where we varied n to equal 3, 5, 7, and 11.", "labels": [], "entities": []}, {"text": "When n = 7, we experimented with a seq2seq model with attention, a Transformer model, and a combination of both.", "labels": [], "entities": []}, {"text": "12.76 RDI ( 15.95 MADAMIRA ( 19.02 MIT ( 30.50 fer also from excessive repetition of characters that are often meaningless hallucination (e.g. \"AalofaA}iti AaloHaAdiy waAlt\u223cawaAliy Aaloayoiy AloanohaAti AaloanohaAti\").", "labels": [], "entities": [{"text": "RDI", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.996881365776062}, {"text": "MADAMIRA", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9872118830680847}, {"text": "MIT", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9877360463142395}]}, {"text": "When we limited the context to 7 words, the results improved dramatically, nonetheless, the output still suffered from a high ratio of OOVs.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.97719407081604}]}, {"text": "Using characters instead alleviated the OOV problem.", "labels": [], "entities": [{"text": "OOV", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.5775589942932129}]}, {"text": "The results improved dramatically with contexts of length 7 yielding the best results.", "labels": [], "entities": []}, {"text": "Using voting lowered WER rate further, leading to a 4.77% WER.", "labels": [], "entities": [{"text": "WER rate", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8668123185634613}, {"text": "WER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9838806986808777}]}, {"text": "Using a Transformer model led to nearly identical WER to using our NMT model with attention.", "labels": [], "entities": [{"text": "WER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9541149139404297}]}, {"text": "However their results are somewhat complimentary.", "labels": [], "entities": []}, {"text": "Thus, voting on the predictions across both systems improved the results further with a 4.49% WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.998258650302887}]}, {"text": "compares our best system with other systems.", "labels": [], "entities": []}, {"text": "The WER of our best system is 63.3% lower than the state-of-the-art.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9995720982551575}]}, {"text": "we randomly selected 100 errors word-core and 100 case-ending errors we ascertain the most common error types.", "labels": [], "entities": []}, {"text": "For caseending, the top 4 error types were: long-distance dependency (e.g. coordination or verb subj/obj), which is an artifact of using limited context -24% of errors; confusion between different syntactic functions (e.g. N N vs. N ADJ or V Subj vs. V Obj) -22%; wrong selection of morphological analysis (e.g. present tense vs. past tense) -20%; and named entities (NEs) -16%.", "labels": [], "entities": []}, {"text": "For long distance dependencies, increasing context size may help in some case, but may introduce additional errors (see).", "labels": [], "entities": []}, {"text": "Perhaps combining multiple context sizes may help.", "labels": [], "entities": []}, {"text": "As for word-core, the top 4 errors were: incorrect selection for ambiguous words, where most of these errors were related to active vs. passive voice -60%; NEs -32%; borrowed words -4%; and words with multiple valid diacritized words -4%.", "labels": [], "entities": []}], "tableCaptions": []}