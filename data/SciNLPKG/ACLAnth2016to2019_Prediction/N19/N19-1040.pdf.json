{"title": [{"text": "Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual Supervision", "labels": [], "entities": [{"text": "Learning Bilingual Sentiment-Specific Word Embeddings", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6980470180511474}]}], "abstractContent": [{"text": "Word embeddings learned in two languages can be mapped to a common space to produce Bilingual Word Embeddings (BWE).", "labels": [], "entities": []}, {"text": "Un-supervised BWE methods learn such a mapping without any parallel data.", "labels": [], "entities": []}, {"text": "However, these methods are mainly evaluated on tasks of word translation or word similarity.", "labels": [], "entities": [{"text": "word translation", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7551018297672272}, {"text": "word similarity", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6744064837694168}]}, {"text": "We show that these methods fail to capture the sentiment information and do not perform well enough on cross-lingual sentiment analysis.", "labels": [], "entities": [{"text": "cross-lingual sentiment analysis", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.7688500086466471}]}, {"text": "In this work, we propose UBiSE (Un-supervised Bilingual Sentiment Embeddings), which learns sentiment-specific word representations for two languages in a common space without any cross-lingual supervision.", "labels": [], "entities": [{"text": "Un-supervised Bilingual Sentiment Embeddings)", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.6560306787490845}]}, {"text": "Our method only requires a sentiment corpus in the source language and pretrained mono-lingual embeddings of both languages.", "labels": [], "entities": []}, {"text": "We evaluate our method on three language pairs for cross-lingual sentiment analysis.", "labels": [], "entities": [{"text": "cross-lingual sentiment analysis", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8653551340103149}]}, {"text": "Experimental results show that our method out-performs previous unsupervised BWE methods and even supervised BWE methods.", "labels": [], "entities": []}, {"text": "Our method succeeds fora distant language pair English-Basque.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lack of annotated corpora degrades the quality of sentiment analysis in low-resource languages.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9378394484519958}]}, {"text": "Cross-lingual sentiment analysis tackles this problem by adapting the sentiment resource in a resource-rich language (the source language) to a resource-poor language (the target language).", "labels": [], "entities": [{"text": "Cross-lingual sentiment analysis", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.750406434138616}]}, {"text": "Bilingual Word Embeddings (BWE) provide away to transfer the sentiment information from the source language to the target language.", "labels": [], "entities": [{"text": "Bilingual Word Embeddings (BWE)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.584874634941419}]}, {"text": "There has been an increasing interest in BWE methods in recent years, including both supervised methods and unsupervised methods.", "labels": [], "entities": [{"text": "BWE", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7477971315383911}]}, {"text": "Supervised BWE methods map the word vectors of the two languages in a common space by exploiting either a bilingual seed dictionary or other parallel data, while unsupervised BWE methods do not utilize any form of bilingual supervision.", "labels": [], "entities": []}, {"text": "Yet, these methods are mostly evaluated on tasks of word translation or word similarity, and do not perform well enough on cross-lingual sentiment analysis as shown in Section 4.", "labels": [], "entities": [{"text": "word translation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7514723241329193}, {"text": "word similarity", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6988835781812668}, {"text": "cross-lingual sentiment analysis", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.7979619900385538}]}, {"text": "Consider the case where we want to perform sentiment analysis on the target language with merely an annotated sentiment corpus in the source language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9412819743156433}]}, {"text": "We assume pretrained monolingual embeddings of both languages are available to us.", "labels": [], "entities": []}, {"text": "One solution is to first align the embeddings of both languages in a common space using unsupervised BWE methods, then train a classifier based on the source sentiment corpus.", "labels": [], "entities": [{"text": "BWE", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.7207946181297302}]}, {"text": "In this solution, no sentiment information is utilized to learn the alignment.", "labels": [], "entities": []}, {"text": "In this paper, we propose to exploit the sentiment information and learn sentiment-specific alignment.", "labels": [], "entities": [{"text": "sentiment-specific alignment", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.7664420008659363}]}, {"text": "The sentiment information is gradually incorporated into the BWE through an iterative constraint relaxation procedure.", "labels": [], "entities": [{"text": "BWE", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.5266622304916382}]}, {"text": "Unlike previous work which performed alignment in a single direction by linearly mapping the source vectors to the target vector space, we propose an alignment model that maps the vectors of the two languages to anew shared space with two non-linear transformations.", "labels": [], "entities": []}, {"text": "Our model is able to separate positive vectors from negative vectors in the bilingual space and allow such sentiment information to be transferred to the target language.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose a novel approach to learn bilingual sentiment-specific word embeddings without any cross-lingual supervision and perform cross-lingual sentiment analysis with minimum resource requirement.", "labels": [], "entities": [{"text": "cross-lingual sentiment analysis", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.7459689974784851}]}, {"text": "We propose an iterative constraint relaxation pro-cedure that gradually incorporates the sentiment information into the BWE.", "labels": [], "entities": [{"text": "BWE", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.6372125148773193}]}, {"text": "Our proposed approach achieves state-of-the-art results.", "labels": [], "entities": []}, {"text": "2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier.", "labels": [], "entities": []}, {"text": "Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches.", "labels": [], "entities": []}, {"text": "3. We introduce an alignment-specific objective and a simple re-normalization trick.", "labels": [], "entities": []}, {"text": "Unlike previous BWE methods that learn orthogonal mappings, we introduce non-orthogonal mappings which enable the transfer of sentiment information from the source language to the target language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the multilingual sentiment dataset provided by (.", "labels": [], "entities": []}, {"text": "It contains annotated hotel reviews in English (EN), Spanish (ES), Catalan (CA) and Basque (EU).", "labels": [], "entities": []}, {"text": "In our experiment, we use EN as the source language and ES, CA, EU as the target languages.", "labels": [], "entities": []}, {"text": "For each target language, the dataset is divided into a target development set and a target test set.", "labels": [], "entities": []}, {"text": "We also combine the strong and weak labels to produce a binary setup.", "labels": [], "entities": []}, {"text": "We use as the classifier to preform cross-lingual sentiment analysis.", "labels": [], "entities": [{"text": "preform cross-lingual sentiment analysis", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.7100609838962555}]}, {"text": "The loss of each instance is weighted by its inverse class frequency to address the class imbalance problem.", "labels": [], "entities": []}, {"text": "For each method, the dropout rate is fixed at 0.3 and the l 2 -regularization strength is tuned on the target development set 5 . We train five classifiers for each method and report the average macro-F1 on the target test set.", "labels": [], "entities": []}, {"text": "presents the results of different BWE methods.", "labels": [], "entities": []}, {"text": "UBISE outperforms all unsupervised methods on all six tasks and outperforms all baselines on four out of six tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Macro F1 of different BWE approaches. The best score for each language pair is shown in bold. The  best score among unsupervised BWE methods for each language pair is underlined.", "labels": [], "entities": []}, {"text": " Table 2:  Comparison between UBISE and  UBISE MIN", "labels": [], "entities": [{"text": "UBISE", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8321767449378967}, {"text": "MIN", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.49017706513404846}]}]}