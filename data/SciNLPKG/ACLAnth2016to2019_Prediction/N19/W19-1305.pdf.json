{"title": [], "abstractContent": [{"text": "We investigate the relationship between basic principles of human morality and the expression of opinions in user-generated text data.", "labels": [], "entities": []}, {"text": "We assume that people's backgrounds, culture, and values are associated with their perceptions and expressions of everyday topics, and that people's language use reflects these perceptions.", "labels": [], "entities": []}, {"text": "While personal values and social effects are abstract and complex concepts, they have practical implications and are relevant fora wide range of NLP applications.", "labels": [], "entities": []}, {"text": "To extract human values (in this paper, morality) and measure social effects (morality and stance), we empirically evaluate the usage of amorality lexicon that we expanded via a quality controlled, human in the loop process.", "labels": [], "entities": []}, {"text": "As a result, we enhanced the Moral Foundations Dictionary in size (from 324 to 4,636 syntactically disambiguated entries) and scope.", "labels": [], "entities": [{"text": "Moral Foundations Dictionary", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7584850986798605}]}, {"text": "We used both lexica for feature-based and deep learning classification (SVM, RF, and LSTM) to test their usefulness for measuring social effects.", "labels": [], "entities": []}, {"text": "We find that the enhancement of the original lexicon led to measurable improvements in prediction accuracy for the selected NLP tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9241377115249634}]}], "introductionContent": [{"text": "User-generated text data are used in various fields to study, analyze, and extract people's culture, behavior, opinions, and emotions.", "labels": [], "entities": []}, {"text": "The access and popularity of social media platforms such as Twitter attract individuals to participate in online discussions or share their points of view.", "labels": [], "entities": []}, {"text": "Different beliefs and perspectives on social, political, economic, and other potentially controversial issues can lead to debates or conflicts among groups, and can result in arguments, abusive discussions, and segregated communities.", "labels": [], "entities": []}, {"text": "Given this type of behavior on online platforms, researchers have been investigating the relationship between basic principles of human values and the expression of opinions in usergenerated text data by using (lexical) resources developed for this purpose and domain.", "labels": [], "entities": []}, {"text": "This is done as part of stance analysis, analysis of controversial topics, sentiment analysis), and other standard NLP tasks.", "labels": [], "entities": [{"text": "stance analysis", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.9872161746025085}, {"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9617211520671844}]}, {"text": "Following this line of research, in this paper, we operationalize and extract morality as a basic principle of human decision making and interaction guideline for people, e.g., when expressing themselves related to social or political topics.", "labels": [], "entities": []}, {"text": "Our research is based on the assumption that people's backgrounds, cultures, and values affect their perception and expression of knowledge and beliefs about everyday topics.", "labels": [], "entities": []}, {"text": "These personal idiosyncrasies and differences manifest themselves in people's social discourse and everyday use of language, and can be helpful in analyzing or measuring people's positions or values regarding various social issues.", "labels": [], "entities": []}, {"text": "Concepts such as morality are challenging to measure as they require reliable operationalization and identification of regularities, and accounting for context and meaning.", "labels": [], "entities": []}, {"text": "To measure such concepts, we need to make sure that our results are -as much as possible -a reflection of the behavioral effect we want to study, not of the tools we use.", "labels": [], "entities": []}, {"text": "The same is true fora wide range of social concepts that have been measured by applying lexicons to text data, such as opinion), emotions", "labels": [], "entities": []}], "datasetContent": [{"text": "For the stance datasets, the results are shown in the first 12 columns of.", "labels": [], "entities": [{"text": "stance datasets", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8830715715885162}]}, {"text": "Depending on the sub-topic, our baseline accuracy ranged from 45.07% (RF, Trump, stance hardest to predict) to 69.54% (SVM, atheism, stances easiest to predict).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9801077842712402}]}, {"text": "As observed for the Baltimore data, adding lexical morality features to stance increased accuracy over our baseline in all but one case (Climate, RF) cases.", "labels": [], "entities": [{"text": "Baltimore data", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.9672646224498749}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9989877343177795}]}, {"text": "The results for the LSTM model for both datasets are shown in.", "labels": [], "entities": []}, {"text": "As mentioned before, we used two sets of neuron sizes for the hidden layer.", "labels": [], "entities": []}, {"text": "For the Baltimore dataset, using the MFDE achieved better performance in both implemented models.", "labels": [], "entities": [{"text": "Baltimore dataset", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9773828685283661}, {"text": "MFDE", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8856837749481201}]}, {"text": "The highest accuracy was obtained by the enhanced LSTM model using enhanced morality words (EM), 86.61% (N=100).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993900060653687}]}, {"text": "For the stance dataset, adding morality embedding to the output of LSTM (baseline) resulted in outperforming the baseline in 83.33% of cases (10 out of 12).", "labels": [], "entities": []}, {"text": "Does using morality as a lexical feature improve prediction accuracy for the selected NLP tasks?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.936936616897583}]}, {"text": "Comparing the baseline to any models that include morality, we conclude that adding morality as a lexical feature increases accuracy in 13 out of 14 cases (93%) for feature-based learning (considering RF and SVM models for each topic) and in 12 out of 14 cases (85.7%) for deep learning (considering experiments with two sets of neurons for each topic).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9990604519844055}]}, {"text": "This finding suggests that using the morality as a feature is helpful for standard NLP tasks -and possibly other tasks as well, which would need to be explored in future work.", "labels": [], "entities": []}, {"text": "Does expanding the MFDO pay off?", "labels": [], "entities": [{"text": "MFDO", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.6043460965156555}]}, {"text": "We find that for feature-based learning, in 29 out of 42 cases (69.05%), the accuracy with any MFDE feature outperforms the models with MFDO features, in 21.43% of the cases, MFDO outperforms MFDE, and in 9.52% of the cases, both versions of the dictionary lead to equal results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992604851722717}, {"text": "MFDE", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.7976803183555603}]}, {"text": "For the LSTM, 9 out of 14 models (64.28%) had better performance when using MFDE, while 14.28% of models (2 models) worked better with MFDO ().", "labels": [], "entities": [{"text": "LSTM", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.6070011854171753}, {"text": "MFDE", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7536093592643738}, {"text": "MFDO", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.7882817983627319}]}, {"text": "From that, we conclude that lexicon expansion is worthwhile as it improves prediction accuracy in the majority of our experiments, especially for feature-based learning.", "labels": [], "entities": [{"text": "lexicon expansion", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7771115601062775}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.937919020652771}]}, {"text": "Does disambiguating word sense in the MFDO via POS pay off?", "labels": [], "entities": [{"text": "MFDO", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6470563411712646}]}, {"text": "Based on the results in and 3, we found that syntactic disambiguating of lexicon entries leads to only minor quantitative improvements.", "labels": [], "entities": []}, {"text": "We believe that the usefulness of POS tags can be further tested with other types of user-generated data that follow more conventional grammatical rules.", "labels": [], "entities": []}, {"text": "In addition, beyond what we measured in this paper, this additional layer of information might further boost the quality of the data.", "labels": [], "entities": []}, {"text": "Based on the results of all implemented models, highlighted in and 3, we found that using MFDE results in higher performance compared to other models (MFDO and BM).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)", "labels": [], "entities": [{"text": "predicting stance", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8716040551662445}, {"text": "morality", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9181661009788513}, {"text": "RF", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9658578634262085}, {"text": "Baltimore datasets", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.7349212169647217}, {"text": "Accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9983491897583008}, {"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9960424900054932}]}, {"text": " Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)", "labels": [], "entities": [{"text": "predicting stance", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8726898729801178}, {"text": "morality", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9375287890434265}, {"text": "Baltimore datasets", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9087973535060883}, {"text": "Accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9978641867637634}, {"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9941482543945312}]}]}