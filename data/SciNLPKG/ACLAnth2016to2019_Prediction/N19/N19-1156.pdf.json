{"title": [{"text": "A Probabilistic Generative Model of Linguistic Typology", "labels": [], "entities": []}], "abstractContent": [{"text": "In the principles-and-parameters framework, the structural features of languages depend on parameters that maybe toggled on or off, with a single parameter often dictating the status of multiple features.", "labels": [], "entities": []}, {"text": "The implied co-variance between features inspires our prob-abilisation of this line of linguistic inquiry-we develop a generative model of language based on exponential-family matrix factorisa-tion.", "labels": [], "entities": []}, {"text": "By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9978668093681335}]}, {"text": "Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages.", "labels": [], "entities": []}, {"text": "This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typo-logical features and languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic typologists dissect and analyse languages in terms of their structural properties).", "labels": [], "entities": []}, {"text": "For instance, consider the phonological property of word-final obstruent decoding: German devoices word-final obstruents (Zug is pronounced /zuk/), whereas English does not (dog is pronounced /d6g/).", "labels": [], "entities": []}, {"text": "In the tradition of generative linguistics, one line of typological analysis is the principles-and-parameters framework, which posits the existence of a set of universal parameters, switches as it were, that languages toggle.", "labels": [], "entities": [{"text": "generative linguistics", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9599957168102264}]}, {"text": "One arrives at a kind of factorial typology, to borrow terminology from optimality theory, through different settings of the parameters.", "labels": [], "entities": []}, {"text": "Within the principleand-parameters research program, then, the goal is to identify the parameters that serve as axes, along which languages may vary.", "labels": [], "entities": []}, {"text": "It is not enough, however, to simply write down the set of parameters available to language.", "labels": [], "entities": []}, {"text": "Indeed, one of the most interesting facets of typology is that different parameters are correlated.", "labels": [], "entities": []}, {"text": "To illustrate this point, we show a heatmap in Figure 1 that shows the correlation between the values of selected parameters taken from a typological knowledge base (KB).", "labels": [], "entities": []}, {"text": "Notice how head-final word order, for example, highly correlates with strong suffixation.", "labels": [], "entities": []}, {"text": "The primary contribution of this work is a probabilisation of typology inspired by the principles-and-parameters framework.", "labels": [], "entities": []}, {"text": "We assume a given set of typological parameters and develop a generative model of a language's parameters, casting the problem as a form of exponential-family matrix factorisation.", "labels": [], "entities": []}, {"text": "We observe a binary matrix that encodes the settings of each parameter for each language.", "labels": [], "entities": []}, {"text": "For example, the Manchu head-final entry of this matrix would beset to 1, because Manchu is a head-final language.", "labels": [], "entities": []}, {"text": "The goal of our model is to explain each entry of matrix as arising through the dot product of a language embedding and a parameter embedding passed through a sigmoid.", "labels": [], "entities": []}, {"text": "We test our model on The World Atlas of Language Structures (WALS), the largest available knowledge base of typological parameters at the lexical, phonological, syntactic and semantic level.", "labels": [], "entities": [{"text": "World Atlas of Language Structures (WALS)", "start_pos": 25, "end_pos": 66, "type": "DATASET", "confidence": 0.9275769740343094}]}, {"text": "Our contributions are: (i) We develop a probabilisation of typology inspired by the principles-and-parameters framework.", "labels": [], "entities": []}, {"text": "(ii) We introduce the novel task of typological collaborative filtering, where we observe some of a language's parameters, but hold some out.", "labels": [], "entities": [{"text": "typological collaborative filtering", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.6533666253089905}]}, {"text": "At evaluation time, we predict the held-out parameters using the generative model.", "labels": [], "entities": []}, {"text": "(iii) We develop a semi-supervised extension, in which we incorporate language embeddings output by a neural language model, thus improving performance with unlabelled data.", "labels": [], "entities": []}, {"text": "Indeed, when we partially observe some of the typological parameters of a language, we achieve near-perfect (97%) accuracy on the prediction of held-out parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9986666440963745}]}, {"text": "(iv) We perform an extensive qualitative and quantitative analysis of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "As described in \u00a76, we binarise WALS.", "labels": [], "entities": [{"text": "WALS", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9118316769599915}]}, {"text": "In order to compare directly with our semi-supervised extension, we limit our evaluation to the subset of languages which is the intersection of the languages for which we have Bible data, and the languages which are present in WALS.", "labels": [], "entities": []}, {"text": "Finally, we observe that some languages have very few features encoded, and some features are encoded for very few languages.", "labels": [], "entities": []}, {"text": "For instance, feature 10B (Nasal Vowels in West Africa), is only encoded fora total of 40 languages, and only one feature value appears for more than 10 languages.", "labels": [], "entities": []}, {"text": "Because of this, we restrict our evaluation to languages and feature values which occur at least 10 times.", "labels": [], "entities": []}, {"text": "Note that we evaluate on the original parameters, and not the binarised ones.", "labels": [], "entities": []}, {"text": "Our general experimental set-up is as follows.", "labels": [], "entities": []}, {"text": "We first split the languages in WALS into each language branch (genus using WALS terminology).", "labels": [], "entities": []}, {"text": "This gives us, e.g., a set of Germanic languages, a set of Romance languages, a set of Berber languages, and soon.", "labels": [], "entities": []}, {"text": "(We note that this does not correspond to the notion of a language family, e.g., the Indo-European language family.)", "labels": [], "entities": []}, {"text": "We wish to evaluate on this type of held-out set, as it is both relatively challenging: If we know the parameters of Portuguese, predicting the parameters for Spanish is a much easier task.", "labels": [], "entities": []}, {"text": "This setup will both give us a critical estimate of how well we can predict features overall, in addition to mimicking a scenario in which we either have a poorly covered language or branch, which we wish to add to WALS.", "labels": [], "entities": []}], "tableCaptions": []}