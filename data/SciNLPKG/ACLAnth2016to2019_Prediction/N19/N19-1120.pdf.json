{"title": [{"text": "Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7182988325754801}]}], "abstractContent": [{"text": "The overreliance on large parallel corpora significantly limits the applicability of machine translation systems to the majority of language pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7141173928976059}]}, {"text": "Back-translation has been dominantly used in previous approaches for un-supervised neural machine translation, where pseudo sentence pairs are generated to train the models with a reconstruction loss.", "labels": [], "entities": [{"text": "un-supervised neural machine translation", "start_pos": 69, "end_pos": 109, "type": "TASK", "confidence": 0.7061765938997269}]}, {"text": "However , the pseudo sentences are usually of low quality as translation errors accumulate during training.", "labels": [], "entities": []}, {"text": "To avoid this fundamental issue , we propose an alternative but more effective approach, extract-edit, to extract and then edit real sentences from the target monolingual corpora.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a comparative translation loss to evaluate the translated target sentences and thus train the un-supervised translation systems.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed approach consistently outperforms the previous state-of-the-art un-supervised machine translation systems across two benchmarks (English-French and English-German) and two low-resource language pairs (English-Romanian and English-Russian) by more than 2 (up to 3.63) BLEU points.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7216098606586456}, {"text": "BLEU", "start_pos": 302, "end_pos": 306, "type": "METRIC", "confidence": 0.9989970326423645}]}], "introductionContent": [{"text": "Promising results have been achieved in Neural Machine Translation (NMT) by representation learning ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7607946544885635}]}, {"text": "But recent studies ( highlight the overreliance of current NMT systems on large parallel corpora.", "labels": [], "entities": []}, {"text": "In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (.", "labels": [], "entities": []}, {"text": "While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, lat- When training the source-to-target (s-t) translation model, instead of using the t-s back-translated sentences to train the model, we directly set the extractededited sentences as pivotal points to guide the training.", "labels": [], "entities": []}, {"text": "est approaches () focus on the fully unsupervised scenario.", "labels": [], "entities": []}, {"text": "Back-translation has been dominantly used in these approaches, where pseudo sentence pairs are generated to train the translation systems with a reconstruction loss.", "labels": [], "entities": []}, {"text": "However, it is inefficient because the generated pseudo sentence pairs are usually of low quality.", "labels": [], "entities": []}, {"text": "During the dual learning of back-translation, the errors could easily accumulate and thus the learned target language distribution would gradually deviate from the real target distribution.", "labels": [], "entities": []}, {"text": "This critical drawback hinders the further development of the unsupervised NMT systems.", "labels": [], "entities": []}, {"text": "An alternative solution is to extract real parallel sentences from comparable monolingual corpora, and then use them to train the NMT systems.", "labels": [], "entities": []}, {"text": "Recently, neural-based methods ( aim to select potential parallel sentences from monolingual corpora in the same domain.", "labels": [], "entities": []}, {"text": "However, these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision.", "labels": [], "entities": []}, {"text": "In this paper, we propose a radically different approach for unsupervised NMT-extract-edit, a powerful alternative to back-translation (see.", "labels": [], "entities": []}, {"text": "Specifically, to train the source-to-target translation model, we first extract potential parallel sentence candidates in the target language space given a source language sentence.", "labels": [], "entities": [{"text": "source-to-target translation", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7623220682144165}]}, {"text": "Since it cannot be guaranteed that there always exist potential parallel sentence pairs in monolingual corpora, we further propose a simple but effective editing mechanism to revise the extracted sentences, making them aligned with the source language sentence.", "labels": [], "entities": []}, {"text": "Then a comparative translation loss is introduced to evaluate the translated sentence based on the extracted-and-edited ones and train the translation model.", "labels": [], "entities": []}, {"text": "Compared to backtranslation, extract-edit avoids the distribution deviation issue by extracting and editing real sentences from the target language space.", "labels": [], "entities": []}, {"text": "Those extracted-and-edited sentences serve as pivotal points in the target language space to guide the unsupervised learning.", "labels": [], "entities": []}, {"text": "Thus, the learned target language distribution could be closer to the real one.", "labels": [], "entities": []}, {"text": "The extract-edit model and the translation model, the two major parts of our method, can be jointly trained in a fully unsupervised way.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9636579751968384}]}, {"text": "Empirical results on popular benchmarks show that exact-edit consistently outperforms the stateof-the-art unsupervised NMT system () with back-translation across four different languages pairs.", "labels": [], "entities": []}, {"text": "In summary, our main contributions are three-fold 1 : \u2022 We propose a more effective alternative paradigm to back-translation, extract-edit, to train the unsupervised NMT systems with potentially real sentence pairs; \u2022 We introduce a comparative translation loss for unsupervised learning, which optimizes the translated sentence by maximizing its relative similarity with the source sentence among the extracted-and-edited pairs; \u2022 Our method advances the previous state-ofthe-art NMT systems across four different The source code can be found in this repository: https://github.com/jiaweiw/ Extract-Edit-Unsupervised-NMT language pairs under monolingual corpora only scenario.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider four language pairs: English-French (en-fr), English-German (en-de), English-Russian (en-ru) and English-Romanian (en-ro) for evaluation.", "labels": [], "entities": []}, {"text": "For a fair comparison, we use the same corpora as in    German and Russian, all the available sentences are used from the WMT monolingual News Crawl datasets from years 2007 through 2017.", "labels": [], "entities": [{"text": "WMT monolingual News Crawl datasets from years 2007", "start_pos": 122, "end_pos": 173, "type": "DATASET", "confidence": 0.9018463492393494}]}, {"text": "As for Romanian, we combine the News Crawl dataset and WMT'16 monolingual dataset.", "labels": [], "entities": [{"text": "News Crawl dataset", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9600658416748047}, {"text": "WMT'16 monolingual dataset", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.8910944064458212}]}, {"text": "The translation results are evaluated on newstest 2014 for en-fr, and newstest 2016 for en-de, en-ro and en-ru.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The experimental results of parallel sentence mining on the newstest 2012 en \u2192 fr translation dataset with  different levels of added sentence noises. Metric: The percentage of Hits@k.", "labels": [], "entities": [{"text": "newstest 2012 en \u2192 fr translation dataset", "start_pos": 70, "end_pos": 111, "type": "DATASET", "confidence": 0.9414003235953194}, {"text": "Metric", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.980316698551178}]}, {"text": " Table 3: The performance of the unsupervised NMT  systems with different learning objectives on en \u2192 fr  newstest 2014.", "labels": [], "entities": [{"text": "en \u2192 fr  newstest 2014", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8397555947303772}]}]}