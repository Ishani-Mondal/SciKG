{"title": [{"text": "An Analysis of Deep Contextual Word Embeddings and Neural Architectures for Toponym Mention Detection in Scientific Publications", "labels": [], "entities": [{"text": "Toponym Mention Detection", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.8445016145706177}]}], "abstractContent": [{"text": "Toponym detection in scientific papers is an open task and a key first step in place entity enrichment of documents.", "labels": [], "entities": [{"text": "Toponym detection in scientific papers", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9076933145523072}, {"text": "place entity enrichment of documents", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.7365234792232513}]}, {"text": "We examine three common neural architectures in NLP: 1) convolutional neural network, 2) multi-layer perceptron (both applied in a sliding window context) and 3) bi-directional LSTM and apply contextual and non-contextual word embedding layers to these models.", "labels": [], "entities": []}, {"text": "We find that deep contextual word embeddings improve the performance of the bi-LSTM with CRF neural architecture achieving the best performance when multiple layers of deep contex-tual embeddings are concatenated.", "labels": [], "entities": []}, {"text": "Our best performing model achieves an average F1 of 0.910 when evaluated on overlap macro exceeding previous state-of-the-art models in the toponym detection task.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9997151494026184}, {"text": "toponym detection task", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.8410583933194479}]}], "introductionContent": [{"text": "The available scientific knowledge is growing everyday.", "labels": [], "entities": []}, {"text": "Yet, this knowledge is often locked into publications in pdf format, that are not condusive to machine-reading or automated analyses.", "labels": [], "entities": []}, {"text": "In this work we take a step towards automated knowledge extraction that is compatible with extraction and visualization frameworks for scientific publications (.", "labels": [], "entities": [{"text": "automated knowledge extraction", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.6415075759092966}]}, {"text": "Many scientific publications contain geographic references which are commonly confused by extractors with other entities such as people or proteins whose name contains references to places.", "labels": [], "entities": []}, {"text": "Extracting such placenames, or toponyms, has several important applications such as the identification of virus outbreak locations, treatment adherence (, and mapping of research findings).", "labels": [], "entities": [{"text": "identification of virus outbreak locations", "start_pos": 88, "end_pos": 130, "type": "TASK", "confidence": 0.8882194519042969}, {"text": "mapping of research findings", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.8430965542793274}]}, {"text": "Toponyms are textual spans of text that identify geospatial locations.", "labels": [], "entities": []}, {"text": "This can range from the canonical name of populated places, such as \"Chengdu\" to director indirect mentions of geographic entities, including \"Cho Oyu\" or \"5 km south of Mirnyy\".", "labels": [], "entities": []}, {"text": "The parsing of geographic locations from unstructured text is often addressed with gazeteers.", "labels": [], "entities": [{"text": "parsing of geographic locations from unstructured text", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.8604857666151864}]}, {"text": "It is generally very difficult to achieve high accuracy due to domain diversity, place name ambiguity, metonymic language and limited contextual cues (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9946030974388123}]}, {"text": "Furthermore, major challenges to toponym detection in scientific texts come from the fact that names of institutions, viruses and proteins often contain geographic references.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8112795948982239}]}, {"text": "Moreover, the extractor needs to handle the overall noisy nature of scientific articles after PDF extraction-with challenges include associating figures and tables as well as handling character encodings.", "labels": [], "entities": [{"text": "PDF extraction-with", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7115881741046906}]}, {"text": "Given the text of a scientific publication (as extracted from the PDF), the task is to extract character offset locations of true toponyms.", "labels": [], "entities": []}, {"text": "This location is referred to as a toponym mention in the following.", "labels": [], "entities": []}, {"text": "A toponym is defined to include proper names and geographic entities but to exclude indirect mentions of places and metonyms.", "labels": [], "entities": []}, {"text": "Toponym detection is a first step towards toponym resolution where each toponym mention is to be aligned to a geospatial location.", "labels": [], "entities": [{"text": "Toponym detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8746137022972107}, {"text": "toponym resolution", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7518892586231232}]}, {"text": "In this work we focus on toponym detection and evaluate different neural specialization models for word embeddings on this task.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8354910016059875}]}, {"text": "This approach has benefitted many natural language processing (NLP) tasks, such as named entity recognition).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6079437434673309}]}, {"text": "Previous work in toponym detection has mostly focused on non-contextual word embeddings (.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8304024338722229}]}, {"text": "Here we study which neural model and which word embed-ding types are best suited for the detection of toponyms in scientific publications.", "labels": [], "entities": []}, {"text": "We also demonstrate the benefits of neural architectures in comparison to Tagme, a state-of-the-art entity linker, from which we isolate toponym spots based on DBpedia categories.", "labels": [], "entities": []}, {"text": "The contribution of our work lies in answering the following research questions in regards to the task of toponym detection in scientific papers: RQ1 Independent of the neural model architecture for specialization, which embedding demonstrates better performance: A taskindependent deep contextual embeddings or a non-contextual embedding trained on a scientific-domain specific corpus?", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7275623083114624}]}, {"text": "RQ2 Given an optimal embedding, which neural specialization architecture is optimal for the task?", "labels": [], "entities": []}, {"text": "RQ3 Given an optimal word embedding and neural architecture, what are the performance impacts of different combinations of the embedding and the classifier?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9182984828948975}]}, {"text": "Our findings show that the best performance on toponym detection is achieved by deep contextual embeddings (even though trained on a nonscientific corpus) when using bidirectional LSTMs with CRFs as the specialization architecture, while concatenating the layers of the embeddings.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8042614161968231}]}, {"text": "However, other deep contextual configurations including weighted average, and single layer selection also yield similar average performance.", "labels": [], "entities": []}, {"text": "We also find that handcrafted orthographic features did not impact bi-LSTM model performance, but did positively impact MLP and negatively impacted CNN.", "labels": [], "entities": [{"text": "MLP", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.7957829236984253}, {"text": "CNN", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.4479890763759613}]}, {"text": "In Section 2 we discuss related work.", "labels": [], "entities": []}, {"text": "Section 3 explains the neural models types included in our analysis and discusses word embedding types.", "labels": [], "entities": []}, {"text": "In Section 4, we provide details on the approaches examined in our study.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss the data, metrics, and results obtained.", "labels": [], "entities": []}, {"text": "We finish with a conclusion about the research questions posed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following we describe our experimental evaluation using data and metrics from the SemEval Toponym resolution task.", "labels": [], "entities": [{"text": "SemEval Toponym resolution task", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7736325860023499}]}], "tableCaptions": [{"text": " Table 1: Gold Standard Corpus Statistics.", "labels": [], "entities": [{"text": "Gold Standard Corpus Statistics", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.9813984483480453}]}, {"text": " Table 2: Comparison of different architectures and em- beddings.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of variations of bi-LSTM with  ELMo embeddings.", "labels": [], "entities": []}]}