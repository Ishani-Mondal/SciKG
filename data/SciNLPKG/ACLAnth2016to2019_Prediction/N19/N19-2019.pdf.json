{"title": [{"text": "A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling", "labels": [], "entities": [{"text": "Approach", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9413127303123474}, {"text": "Multi-level Sequence Labeling", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6954081257184347}]}], "abstractContent": [{"text": "In this paper we present anew method for spoken language understanding to support a spoken dialogue system handling complex dialogues in the food ordering domain.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6862346728642782}]}, {"text": "Using a small amount of authentic food ordering dialogues yields better results than a large amount of synthetic ones.", "labels": [], "entities": []}, {"text": "The size of the data makes this approach amenable to cold start projects in the multi-level sequence labeling domain.", "labels": [], "entities": [{"text": "multi-level sequence labeling", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.5651710629463196}]}, {"text": "We used windowed word n-grams, POS tag sequences and pre-trained word embeddings as features.", "labels": [], "entities": []}, {"text": "Results show that a heterogeneous feature set with the k-NN learner performs competitively against the state-of-the-art results and achieve an F-score of 60.71.", "labels": [], "entities": [{"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9996730089187622}]}], "introductionContent": [{"text": "Handling complex dialogues between customers and agents is hard, especially in the food ordering domain where there area lot of hesitations and noise involved.", "labels": [], "entities": [{"text": "Handling complex dialogues between customers and agents", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.819130369595119}, {"text": "food ordering", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.7148523777723312}]}, {"text": "A sample dialogue with only the customer side available is shown in.", "labels": [], "entities": []}, {"text": "The agent side is not available since our software setup does not include an Automatic Speech Recognition (ASR) component on the agent side.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.7167386760314306}]}, {"text": "The complexity stems from the fact that food ordering dialogues are mixed initiative, and individual customer utterances may contain multiple intents and refer to food items with complex structure.", "labels": [], "entities": [{"text": "food ordering dialogues", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7764779130617777}]}, {"text": "For example, a customer might say \"Can I get a deluxe burger with large fries and oh put extra mayo on the burger would you?\"", "labels": [], "entities": []}, {"text": "Since essentially we are trying to give each word an IOB tag with some sub-label referring to a specific class, it is natural that we approach this task as a multi-level sequence labeling problem.", "labels": [], "entities": []}, {"text": "Additionally, this must be performed with limited authentic training data, since we are starting from scratch and not many dialogues have been collected from our customers yet.", "labels": [], "entities": []}, {"text": "uh give me a medium order of onion rings and i 'd like to have them well done no tartar no lettuce but i 'd like to have uh mustard pickles on yes that 's it Both traditional methods such as Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs), or Conditional Random Fields (CRFs) and newer methods like Deep Neural Networks (DNNs) or Bi-directional Long Short-Term Memories (BiLSTMs) typically use only homogeneous feature sets.", "labels": [], "entities": []}, {"text": "Here homogeneous feature set refers to the type of feature set within which there is only one type of feature, for example, the presence/absence word ngrams.", "labels": [], "entities": []}, {"text": "Heterogeneous feature set, on the other hand, refers to the type of feature set within which at least more than one type of feature are used, for example, the presence/absence of word n-grams and pre-trained word embeddings, one being symbolic and the other being vectorized.", "labels": [], "entities": []}, {"text": "Newer methods perform better but also require considerably more data.", "labels": [], "entities": []}, {"text": "Previous research has synthesized data to obtain the required amounts for training.", "labels": [], "entities": []}, {"text": "We use a k-NN learner with a heterogeneous feature set.", "labels": [], "entities": []}, {"text": "Instead of using a massive amount of synthetic dialogues, we are able to achieve superior results by annotating less than 1% of the authentic ones.", "labels": [], "entities": []}, {"text": "This is within a reasonable budget of time and effort fora cold start project.", "labels": [], "entities": []}, {"text": "To incorporate traditional linguistic knowledge and distributional word representation, we used windowed word n-grams, POS tag sequences and pre-trained word embeddings as features.", "labels": [], "entities": [{"text": "distributional word representation", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.6165381968021393}]}, {"text": "We performed experiments comparing the use of synthetic and authentic customer data while also performing semisupervised self-training to obtain additional labeled data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our series of experiment contains four subsets of experiments.", "labels": [], "entities": []}, {"text": "The results are presented in Section 4 and discussed in Section 5.", "labels": [], "entities": []}, {"text": "The first set of experiments uses synthesized training data provided by the synthetic data generator and both human transcribed (HT) and automatic speech recognition (ASR) customergenerated test data.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 137, "end_pos": 171, "type": "TASK", "confidence": 0.7451148927211761}]}, {"text": "In the second set of experiments, we switched from using synthesized training data to using human transcribed training data.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In the Experiment I, using synthesized training data and human transcribed (HT in) test data produced the best result.", "labels": [], "entities": []}, {"text": "Overall we achieved an F-score of 41.25.", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9996328353881836}]}, {"text": "In the Experiment II, the leave-one-out experiment performed on human transcribed data is used to show the upper bound of the task (italicized in).", "labels": [], "entities": []}, {"text": "This is the ideal situation in which we do not introduce errors from automatic speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7120635509490967}]}, {"text": "The distribution of training set and the test set is identical.", "labels": [], "entities": []}, {"text": "In this best scenario, our classifier achieved an F-score of 61.33.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9996930360794067}]}, {"text": "In the Experiment III, we have two different settings.", "labels": [], "entities": []}, {"text": "One of them is to use both human transcribed data and additional self-training labeled data, the other is to only use human transcribed data as the seed while the training set itself is the sole self-training labeled data.", "labels": [], "entities": []}, {"text": "The second setting is closer to real world scenario especially when we are dealing with situations like a cold start.", "labels": [], "entities": []}, {"text": "In this set, human transcribed data plus additional selftraining labeled data reached an F-score of 48.30, outperforming only the additional data by slightly over four points.", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9997074007987976}]}, {"text": "In the Experiment IV, we show that with the accuracy improvement from ASR, our F-score is improving as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9995705485343933}, {"text": "ASR", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8130928874015808}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.999099850654602}]}, {"text": "By performing post-processing, which is a process that is designed strictly to correct errors without compromising the performance in the long run, we manage to achieve close to upper bound performance with an F-score of 60.71.", "labels": [], "entities": [{"text": "F-score", "start_pos": 210, "end_pos": 217, "type": "METRIC", "confidence": 0.999423623085022}]}, {"text": "The purpose of this set of experiment is to show that errors introduced early on in the pipeline will propagate through to cause performance decrease   later.", "labels": [], "entities": []}, {"text": "Compared to testing on human transcribed data, testing on ASR data performed much worse with a decrease of almost fourteen points in terms of F-score.", "labels": [], "entities": [{"text": "ASR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9339831471443176}, {"text": "F-score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9990480542182922}]}, {"text": "Human transcribed data tend to be more grammatical and relevant.", "labels": [], "entities": []}, {"text": "As we can see in our first example, ASR data has a lot more noise.", "labels": [], "entities": [{"text": "ASR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9716430306434631}]}, {"text": "The word sequences occurred in the sentence are also less common.", "labels": [], "entities": []}, {"text": "Words that sound similar can be mistaken from each other.", "labels": [], "entities": []}, {"text": "However, this poses some difficulty on the POS tagger when it tries to tag the words based on the left and right context.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.6863774955272675}]}, {"text": "We believe this is one of the reasons we saw a big performance difference here.", "labels": [], "entities": []}, {"text": "The leave-one-out experiment with the human transcribed data serves as the upper bound, the ideal situation where the distribution is identical and no error is introduced externally.", "labels": [], "entities": []}, {"text": "However, it is unlikely to achieve such result in real life scenario since user input is noisy and external errors will be introduced along the pipeline inevitably.", "labels": [], "entities": []}, {"text": "What is interesting is the second experiment.", "labels": [], "entities": []}, {"text": "In this particular experiment, we used human transcribed data as our training set and ASR data as our test set.", "labels": [], "entities": [{"text": "ASR data", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8034782707691193}]}, {"text": "This is a more realistic scenario.", "labels": [], "entities": []}, {"text": "While we only had less than 1% of the data compared to the previous experiments in Experiment I, we achieved a much better F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9988609552383423}]}, {"text": "We believe   this shows that with very little training data we can still achieve rather decent results.", "labels": [], "entities": []}, {"text": "The closer the distribution of both data sets have, the more likely we can achieve higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9981706142425537}]}, {"text": "Having just a little bit of human transcribed and annotated data is a reasonable cost for higher quality prediction.", "labels": [], "entities": []}, {"text": "This is a set of experiment that is rather a reinforcement of the second experiment.", "labels": [], "entities": []}, {"text": "Though the F-score dropped minimally due to the added noise from the additional data, it is still obvious that by using real world data we are improving the accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9990390539169312}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9987767338752747}]}, {"text": "The second half of this set of experiment provides a potential alternative to our approach.", "labels": [], "entities": []}, {"text": "As much as the result is not as good as the first half of this experiment or the second set of experiments, it is still even more accurate than the first set of experiments.", "labels": [], "entities": []}, {"text": "With better ASR quality we believe semi-supervised self-training can and should help.", "labels": [], "entities": [{"text": "ASR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9621132612228394}]}, {"text": "This is our best and state-of-the-art result of this task.", "labels": [], "entities": []}, {"text": "With the help from ASR quality improvement, the classifier receives a significant boost in performance from an F-score of 48.76 to 57.87.", "labels": [], "entities": [{"text": "ASR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.6426981687545776}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9996023774147034}]}, {"text": "It is especially worth noticing that by postprocessing we achieve 3 points even more.", "labels": [], "entities": []}, {"text": "Postprocessing is strictly used to correct the inconsistency in the predictions as each individual decision is made independent of each other.", "labels": [], "entities": [{"text": "Postprocessing", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9614793062210083}]}, {"text": "It will not hurt the performance in the long run.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experiment Results (F-score)", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9968708157539368}]}]}