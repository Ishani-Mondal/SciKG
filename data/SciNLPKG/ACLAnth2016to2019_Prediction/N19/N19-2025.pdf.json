{"title": [{"text": "Audio De-identification: A New Entity Recognition Task", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7986510396003723}]}], "abstractContent": [{"text": "Named Entity Recognition (NER) has been mostly studied in the context of written text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7868799368540446}]}, {"text": "Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9865717887878418}]}, {"text": "In such recordings , audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text.", "labels": [], "entities": []}, {"text": "The application of NER in the context of audio de-identification has yet to be fully investigated.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9824801683425903}]}, {"text": "To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected.", "labels": [], "entities": []}, {"text": "We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.7562785297632217}, {"text": "text-to-audio alignment", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.709898367524147}]}, {"text": "Finally , we introduce a novel metric for audio de-ID and anew evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline's results on it.", "labels": [], "entities": [{"text": "Switchboard and Fisher audio datasets", "start_pos": 129, "end_pos": 166, "type": "DATASET", "confidence": 0.689460813999176}]}], "introductionContent": [{"text": "Personal data in general, and clinical records data in particular, is a major driving force in today's scientific research.", "labels": [], "entities": []}, {"text": "Despite its abundance, the presence of Personal Health Identifiers (PHI) hinders data availability for researchers.", "labels": [], "entities": [{"text": "Personal Health Identifiers (PHI)", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.6850793311993281}]}, {"text": "Therefore, data de-identification (de-ID) is a critical component in any plan to make such data available.", "labels": [], "entities": [{"text": "data de-identification (de-ID)", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.58480304479599}]}, {"text": "However, the amount of data involved makes it prohibitively expensive to employ domain experts to tag and redact PHI manually, providing a good opportunity for automatic de-identification tools.", "labels": [], "entities": []}, {"text": "Indeed, high performance tools for the de-identification of medical text notes have been developed;.", "labels": [], "entities": []}, {"text": "Due to the rise of tele-medicine (, clinical records consist of many other types of data, such as audio conversations (, scanned documents, video, and im- ages.", "labels": [], "entities": []}, {"text": "In this work, we direct our attention towards the task of de-identifying clinical audio data.", "labels": [], "entities": []}, {"text": "This task is expected to become increasingly more important, as Machine Learning applications in telemedicine are growing in popularity.", "labels": [], "entities": []}, {"text": "Given an input audio stream, the objective is to produce a modified audio stream, where all PHI is redacted, while the rest of the stream is kept unchanged.", "labels": [], "entities": [{"text": "PHI", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9563204646110535}]}, {"text": "To the best of our knowledge, de-identifying audio is anew task, requiring anew benchmark.", "labels": [], "entities": []}, {"text": "We define and publish 1 a benchmark consisting of the following: 1.", "labels": [], "entities": []}, {"text": "A large labeled subset of the Switchboard () and Fisher () conversational English audio datasets, denoted as SWFI.", "labels": [], "entities": []}, {"text": "2. A new evaluation metric, measuring how well the PHI words in the input audio were identified and redacted, and how well the rest of the audio was preserved.", "labels": [], "entities": []}, {"text": "To better understand the challenges of the audio de-id task, we evaluate it both end-to-end and by breaking it down and solving it using individual components.", "labels": [], "entities": []}, {"text": "Our pipeline () first produces transcripts from the audio using ASR, proceeds by running text-based NER tagging, and then redacts PHI tokens, using the aligned token boundaries determined by ASR.", "labels": [], "entities": [{"text": "NER tagging", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.8126133680343628}]}, {"text": "Our tagger relies on the state-of-the-art techniques for solving the audio NER problem of recognizing entities in audio transcripts (.", "labels": [], "entities": []}, {"text": "We leverage the available Automatic Speech Recognition (ASR) technology, and use its component of alignment back to audio.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.7666176507870356}]}, {"text": "Finally, we evaluate our pipeline and describe its performance, both end-to-end and percomponent.", "labels": [], "entities": []}, {"text": "Although results on audio are worse than NER performance on text, the pipeline achieves better results than expected despite the compounding pipeline errors.", "labels": [], "entities": []}, {"text": "Last, we analyze our performance and provide insights for next steps.", "labels": [], "entities": []}], "datasetContent": [{"text": "To create a benchmark for the audio de-ID task, we use three datasets from two distinct domains: conversational English and medical records.", "labels": [], "entities": []}, {"text": "We summarize the main dataset statistics in  In the domain of medical datasets, we use I2B2'14 (), which consists of identified textual medical notes with PHI tagging, and the Audio Medical Conversations dataset from (, denoted AMC'17, which contains de-identified audio of doctor-patient conversations and their corresponding manual transcripts.", "labels": [], "entities": [{"text": "I2B2'14", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8506999015808105}]}, {"text": "Processing the AMC'17 conversations was facilitated by the fact that it is a de-identified dataset, which provides us with the locations of the PHI in the audio and the transcripts.", "labels": [], "entities": [{"text": "AMC'17 conversations", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.7499205470085144}]}, {"text": "Three PHI types: names, dates and ages were redacted, preserving type information, and synthetic data was generated using dictionaries and context-aware rules.", "labels": [], "entities": []}, {"text": "First names were drawn from the US Social Security Administration babies names registry 2 and last names were drawn from the Frequently Occurring Surnames list from the 1990's US Census . Human annotators used surrounding context to resolve the other PHI types and filled in fake appropriate identifiers.", "labels": [], "entities": [{"text": "US Social Security Administration babies names registry", "start_pos": 32, "end_pos": 87, "type": "DATASET", "confidence": 0.6412204248564584}, {"text": "Frequently Occurring Surnames list from the 1990's US Census", "start_pos": 125, "end_pos": 185, "type": "DATASET", "confidence": 0.7644500255584716}]}, {"text": "Notably, neither of the above-mentioned medical datasets could serve as a benchmark for the audio de-ID task, as I2B2'14 is text-based, and AMC'17 contains only redacted audio conversations and is not publicly available.", "labels": [], "entities": [{"text": "AMC'17", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.9430642127990723}]}, {"text": "Therefore, we focused on the conversational English domain, where we generated a combined dataset SWFI from the Switchboard ( and Fisher () datasets.", "labels": [], "entities": []}, {"text": "These datasets include hundreds of conversations in English about a variety of subjects, along with their transcripts.", "labels": [], "entities": []}, {"text": "To enable proper training and evaluation for the audio de-ID task, we annotated all 250 Switchboard conversations, and 326 from Fisher.", "labels": [], "entities": []}, {"text": "Annotation included named PHI labels, and the time intervals ) matching each named PHI back into the audio.", "labels": [], "entities": []}, {"text": "This dataset is publicly available 1 to allow for standardized evaluation of novel approaches to this task.", "labels": [], "entities": []}, {"text": "The annotation process began by tokenization of the transcripts provided in both datasets using white-space separators, removing special transcript characters and keeping word capitalization in its original form.", "labels": [], "entities": []}, {"text": "Following that, PHI word annotation was performed manually.", "labels": [], "entities": [{"text": "PHI word annotation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.644691119591395}]}, {"text": "The results can be seen in.", "labels": [], "entities": []}, {"text": "As performing temporal labeling manually is an arduous process, we opt fora semi-automatic ASR-based procedure.", "labels": [], "entities": [{"text": "temporal labeling", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.6088115572929382}, {"text": "ASR-based", "start_pos": 91, "end_pos": 100, "type": "TASK", "confidence": 0.9743542075157166}]}, {"text": "To this end, we determine word start and end times by aligning the manual transcripts to audio intervals.", "labels": [], "entities": []}, {"text": "We assess the quality of this semi-automatic labeling scheme using human evaluation.", "labels": [], "entities": []}, {"text": "For a random sample of 6 SWFI conversations (3 Switchboard and 3 Fisher), we slice the audio according to the aligned interval times per transcript word, and measure both the quality of the transcription, and that of the alignment.", "labels": [], "entities": []}, {"text": "shows the distribution of alignment errors of the tokens from the sample conversations.", "labels": [], "entities": []}, {"text": "These are denoted as good alignment, short (i.e. ASR interval is shorter than actual word) and extended (i.e. interval is longer than expected) where all alignment errors are in the scale of 30-60ms (1-2 audio frames).", "labels": [], "entities": [{"text": "ASR interval", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9616603851318359}]}, {"text": "To test the performance of our models on the audio de-ID task, we conducted a number of experiments, described next.", "labels": [], "entities": []}, {"text": "Section 7 then details our results.", "labels": [], "entities": []}, {"text": "We report Recall, P recision, and F 1 scores for all experiments, which are significantly more informative than accuracy due to a low PHI/non-PHI ratio.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.996452808380127}, {"text": "P recision", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.849513977766037}, {"text": "F 1 scores", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9762990474700928}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9991739392280579}, {"text": "PHI", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.979562520980835}]}, {"text": "We report results on the SWFI test set using the tags which are shown in bold in.", "labels": [], "entities": [{"text": "SWFI test set", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8920305768648783}]}, {"text": "We evaluate our performance against the coverage threshold \u03c1 \u2208 [0, 1] which is defined in Section 3.", "labels": [], "entities": [{"text": "coverage threshold \u03c1", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.9681409398714701}]}, {"text": "Specifically, we focus on type-less metrics, as we care more about the tokens' redaction than their type classification.", "labels": [], "entities": [{"text": "type classification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7183515131473541}]}, {"text": "Our first experiment evaluates the performance of M AMC , M SWFI , and M I2B2 on the SWFI test set.", "labels": [], "entities": [{"text": "SWFI test set", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.8391218185424805}]}, {"text": "First, to decouple their tagging performance from the other pipeline errors, we measure their tagging performance on the manually annotated transcripts (referred to as NER score).", "labels": [], "entities": [{"text": "NER score", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.7432361543178558}]}, {"text": "NER errors may arise due to train-test disparity, where the train and test data are from different domains or different mediums (e.g. text vs. audio), which results in different discriminative models.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9236989617347717}]}, {"text": "Additionally, we measure their overall end-to-end score.", "labels": [], "entities": [{"text": "end-to-end score", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.9654706716537476}]}, {"text": "We analyze the complex behavior of the models' precision by inspecting the coverage distribution of PHI and non-PHI tokens.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.997528612613678}]}, {"text": "Our second experiment evaluates the effect: Error analysis of a sample of M SWFI F N errors, including errors from all components across the pipeline and even occasional manual transcription errors which contribute to both F P and F N errors. of two significant hyperparameters on pipeline performance using the SWFI test set: \u2022 The number of alternative hypotheses passed on from the ASR to the NER tagger.", "labels": [], "entities": [{"text": "SWFI test set", "start_pos": 312, "end_pos": 325, "type": "DATASET", "confidence": 0.7959266503651937}]}, {"text": "\u2022 The amount of padding added around each detection by the alignment component.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics for train and test sets, show- ing the number of notes (written or spoken), token  count, and percent of tokens which are PHI.", "labels": [], "entities": [{"text": "PHI", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.8184422254562378}]}, {"text": " Table 3: ASR WER and token-audio alignment distri- bution on sample conversations from the SWFI dataset.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7910963892936707}, {"text": "WER", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8399180769920349}, {"text": "token-audio alignment", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7005106955766678}, {"text": "SWFI dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9270540475845337}]}, {"text": " Table 4: NER score of the different NER models, and  their end-to-end F 1 in their optimal choice of \u03c1.", "labels": [], "entities": [{"text": "F 1", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9518065452575684}]}, {"text": " Table 5: Error analysis of a sample of M SWFI F N er- rors, including errors from all components across the  pipeline and even occasional manual transcription er- rors which contribute to both F P and F N errors.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9692539572715759}]}]}