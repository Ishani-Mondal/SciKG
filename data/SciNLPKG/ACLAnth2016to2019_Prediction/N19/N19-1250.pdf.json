{"title": [{"text": "Learning to Denoise Distantly-Labeled Data for Entity Typing", "labels": [], "entities": [{"text": "Entity Typing", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7069840431213379}]}], "abstractContent": [{"text": "Distantly-labeled data can be used to scale up training of statistical models, but it is typically noisy and that noise can vary with the distant labeling technique.", "labels": [], "entities": []}, {"text": "In this work, we propose a two-stage procedure for handling this type of data: denoise it with a learned model, then train our final model on clean and denoised distant data with standard supervised training.", "labels": [], "entities": []}, {"text": "Our denoising approach consists of two parts.", "labels": [], "entities": []}, {"text": "First, a filtering function discards examples from the distantly labeled data that are wholly unusable.", "labels": [], "entities": []}, {"text": "Second, a rela-beling function repairs noisy labels for the retained examples.", "labels": [], "entities": []}, {"text": "Each of these components is a model trained on synthetically-noised examples generated from a small manually-labeled set.", "labels": [], "entities": []}, {"text": "We investigate this approach on the ultra-fine entity typing task of Choi et al.", "labels": [], "entities": []}, {"text": "Our baseline model is an extension of their model with pre-trained ELMo representations, which already achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Adding distant data that has been de-noised with our learned models gives further performance gains over this base model, out-performing models trained on raw distant data or heuristically-denoised distant data.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rise of data-hungry neural network models, system designers have turned increasingly to unlabeled and weakly-labeled data in order to scale up model training.", "labels": [], "entities": []}, {"text": "For information extraction tasks such as relation extraction and entity typing, distant supervision () is a powerful approach for adding more data, using a knowledge base) or heuristics () to automatically label instances.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8173490166664124}, {"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8036375045776367}, {"text": "entity typing", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7462003529071808}]}, {"text": "One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models (, capturing its interaction with other supervision ( or breaking down aspects of a task on which it is reliable ().", "labels": [], "entities": []}, {"text": "However, these approaches often require sophisticated probabilistic inference for training of the final model.", "labels": [], "entities": []}, {"text": "Ideally, we want a technique that handles distant data just like supervised data, so we can treat our final model and its training procedure as black boxes.", "labels": [], "entities": []}, {"text": "This paper tackles the problem of exploiting weakly-labeled data in a structured setting with a two-stage denoising approach.", "labels": [], "entities": []}, {"text": "We can view a distant instance's label as a noisy version of a true underlying label.", "labels": [], "entities": []}, {"text": "We therefore learn a model to turn a noisy label into a more accurate label, then apply it to each distant example and add the resulting denoised examples to the supervised training set.", "labels": [], "entities": []}, {"text": "Critically, the denoising model can condition on both the example and its noisy label, allowing it to fully leverage the noisy labels, the structure of the label space, and easily learnable correspondences between the instance and the label.", "labels": [], "entities": []}, {"text": "Concretely, we implement our approach for the task of fine-grained entity typing, where a single entity maybe assigned many labels.", "labels": [], "entities": []}, {"text": "We learn two denoising functions: a relabeling function takes an entity mention with a noisy set of types and returns a cleaner set of types, closer to what manually labeled data has.", "labels": [], "entities": []}, {"text": "A filtering function discards examples which are deemed too noisy to be useful.", "labels": [], "entities": []}, {"text": "These functions are learned by taking manuallylabeled training data, synthetically adding noise to it, and learning to denoise, similar to a conditional variant of a denoising autoencoder (.", "labels": [], "entities": []}, {"text": "Our denoising models embed both entities and labels to make their predictions, mirroring the structure of the final entity typing model itself.", "labels": [], "entities": []}, {"text": "We evaluate our model following.", "labels": [], "entities": []}, {"text": "We chiefly focus on their ultra-fine en-tity typing scenario and use the same two distant supervision sources as them, based on entity linking and head words.", "labels": [], "entities": []}, {"text": "On top of an adapted model from incorporating ELMo (, na\u00a8\u0131velyna\u00a8\u0131vely adding distant data actually hurts performance.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.5942702293395996}]}, {"text": "However, when our learned denoising model is applied to the data, performance improves, and it improves more than heuristic denoising approaches tailored to this dataset.", "labels": [], "entities": []}, {"text": "Our strongest denoising model gives again of 3 F 1 absolute over the ELMo baseline, and a 4.4 F 1 improvement over naive incorporation of distant data.", "labels": [], "entities": [{"text": "F 1 absolute", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9701243837674459}, {"text": "ELMo baseline", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.8461305201053619}, {"text": "F 1", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9437792003154755}]}, {"text": "This establishes anew state-ofthe-art on the test set, outperforming concurrently published work () and matching the performance of a BERT model on this task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9881953001022339}]}, {"text": "Finally, we show that denoising helps even when the label set is projected onto the OntoNotes label set (), outperforming the method of in that setting as well.", "labels": [], "entities": [{"text": "OntoNotes label set", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.8501527905464172}]}], "datasetContent": [{"text": "Ultra-Fine Entity Typing We evaluate our approach on the ultra-fine entity typing dataset from.", "labels": [], "entities": []}, {"text": "The 6K manually-annotated English examples are equally split into the training, development, and test examples by the authors of the dataset.", "labels": [], "entities": []}, {"text": "We generate syntheticallynoised data, D error and D drop , using the 2K training set to train the filtering and relabeling functions, f and h.", "labels": [], "entities": [{"text": "D error", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.8863562643527985}, {"text": "D drop", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.910862922668457}, {"text": "2K training set", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.8734366099039713}]}, {"text": "We randomly select 1M EL and 1M HEAD examples and use them as the noisy data D . Our augmented training data is a combination of the manually-annotated data D and D denoised . OntoNotes In addition, we investigate if denoising leads to better performance on another dataset.", "labels": [], "entities": []}, {"text": "We use the English OntoNotes dataset (), which is a widely used benchmark for fine-grained entity typing systems.", "labels": [], "entities": [{"text": "English OntoNotes dataset", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.6785735686620077}]}, {"text": "The original training, development, and test splits contain 250K, 2K, and 9K examples respectively.", "labels": [], "entities": []}, {"text": "created an augmented training set that has 3.4M examples.", "labels": [], "entities": []}, {"text": "We also construct our own augmented training sets with/without denoising using our noisy data D , using the same label mapping from ultra-fine types to OntoNotes types described in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) with various types of  augmentation added. The customized loss from Choi et al. (2018) actually causes a decrease in performance from  adding any of the datasets. Heuristics can improve incorporation of this data: a relabeling heuristic (Pair) helps on  HEAD and a filtering heuristic (Overlap) is helpful in both settings. However, our trainable filtering and relabeling  models outperform both of these techniques.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9235455989837646}, {"text": "entity typing task", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7464960118134817}]}, {"text": " Table 4: Test results on OntoNotes. Denoising helps  substantially even in this reduced setting. Using fewer  distant examples, we nearly match the performance us- ing the data from Choi et al. (2018) (see text).", "labels": [], "entities": []}, {"text": " Table 5: The average number of types added or deleted  by the relabeling function per example. The right-most  column shows that the rate of examples discarded by  the filtering function.", "labels": [], "entities": []}]}