{"title": [{"text": "Unsupervised Deep Structured Semantic Models for Commonsense Reasoning", "labels": [], "entities": []}], "abstractContent": [{"text": "Commonsense reasoning is fundamental to natural language understanding.", "labels": [], "entities": [{"text": "Commonsense reasoning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7812590599060059}, {"text": "natural language understanding", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6688403884569804}]}, {"text": "While traditional methods rely heavily on human-crafted features and knowledge bases, we explore learning commonsense knowledge from a large amount of raw text via unsupervised learning.", "labels": [], "entities": []}, {"text": "We propose two neural network models based on the Deep Structured Semantic Models (DSSM) framework to tackle two classic commonsense reasoning tasks, Wino-grad Schema challenges (WSC) and Pronoun Disambiguation (PDP).", "labels": [], "entities": [{"text": "Pronoun Disambiguation (PDP)", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.7763531267642975}]}, {"text": "Evaluation shows that the proposed models effectively capture con-textual information in the sentence and co-reference information between pronouns and nouns, and achieve significant improvement over previous state-of-the-art approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter everyday.", "labels": [], "entities": [{"text": "Commonsense reasoning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7905659079551697}]}, {"text": "It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6809005935986837}]}, {"text": "However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved.", "labels": [], "entities": [{"text": "commonsense reasoning tasks", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.7351657748222351}, {"text": "Winograd Schema Challenge (Levesque et al., 2011)", "start_pos": 133, "end_pos": 182, "type": "TASK", "confidence": 0.5863194704055786}]}, {"text": "In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP).", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.8755649924278259}, {"text": "Pronoun Disambiguation Problems (PDP)", "start_pos": 184, "end_pos": 221, "type": "TASK", "confidence": 0.7586152950922648}]}, {"text": "Compared to other commonsense reasoning tasks, * Work done when the author was at Microsoft 1.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.698806494474411}]}, {"text": "The city councilmen refused the demonstrators a permit because they feared violence.", "labels": [], "entities": []}, {"text": "A. The city councilmen B.", "labels": [], "entities": []}, {"text": "The city councilmen refused the demonstrators a permit because they advocated violence.", "labels": [], "entities": []}, {"text": "A. The city councilmen B.", "labels": [], "entities": []}, {"text": "The demonstrators: Examples from Winograd Schema Challenge (WSC).", "labels": [], "entities": [{"text": "Winograd Schema Challenge (WSC)", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.7871094147364298}]}, {"text": "The task is to identify the reference of the pronoun in bold.", "labels": [], "entities": []}, {"text": "WSC and PDP better approximate real human reasoning, and can be more easily solved by native English-speaking adults ().", "labels": [], "entities": [{"text": "WSC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5703470706939697}]}, {"text": "In addition, they are also technically challenging.", "labels": [], "entities": []}, {"text": "For example, the best reported result on WSC is only 20 percentage points better than random guess inaccuracy (.", "labels": [], "entities": [{"text": "WSC", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6338387131690979}]}, {"text": "shows two examples from WSC.", "labels": [], "entities": [{"text": "WSC", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7810357809066772}]}, {"text": "In order to resolve the co-reference in these two examples, one cannot predict what \"they\" refers to unless she is equipped with the commonsense knowledge that \"demonstrators usually cause violence and city councilmen usually fear violence\".", "labels": [], "entities": []}, {"text": "As no labeled training data is available for these tasks, previous approaches are based on either hand-crafted knowledge bases or large-scale language models.", "labels": [], "entities": []}, {"text": "For example, used existing knowledge bases such as ConceptNet () and WordNet for external supervision to train word embeddings and solve the WSC challenge.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9617677927017212}, {"text": "WSC challenge", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.8325693309307098}]}, {"text": "Recently, first used raw text from books/news to train a neural Language Model (LM), and then em-ployed the trained model to compare the probabilities of the sequences, where the pronouns are replaced by each of the candidate references, and to pick the candidate that leads to the highest probability as the answer.", "labels": [], "entities": []}, {"text": "Because none of the existing hand-crafted knowledge bases is comprehensive enough to coverall the world knowledge 1 , we focus on building unsupervised models that can learn commonsense knowledge directly from unlimited raw text.", "labels": [], "entities": []}, {"text": "Different from the neural language models, our models are optimized for co-reference resolution and achieve much better results on both the PDP and WSC tasks.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.7272432744503021}]}, {"text": "In this work we formulate the two commonsense reasoning tasks in WSC and PDP as a pairwise ranking problem.", "labels": [], "entities": [{"text": "WSC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.7459346055984497}]}, {"text": "As the first example in, we want to develop a pair-wise scoring model Score \u03b8 (x i , y) that scores the correct antecedent-pronoun pair (\"councilmen\", \"they\") higher than the incorrect one (\"demonstrators\", \"they\").", "labels": [], "entities": []}, {"text": "These scores depend to a large degree upon the contextual information of the pronoun (e.g., they) and the candidate antecedent (e.g., councilmen).", "labels": [], "entities": []}, {"text": "In other words, it requires to capture the semantic meaning of the pronoun and the candidate antecedent based on the sentences where they occur, respectively.", "labels": [], "entities": []}, {"text": "To tackle this issue, we propose two models based on the framework of Deep Structured Similarity Model (DSSM) (), as shown in(a).", "labels": [], "entities": [{"text": "Deep Structured Similarity Model (DSSM)", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.6596482864448002}]}, {"text": "Formally, let S x be the sentence containing the candidate antecedent xi and S y the sentence containing the pronoun y which we're interested in.", "labels": [], "entities": []}, {"text": "DSSM measures the semantic similarity of a pair of inputs (x i , y) by 1) mapping xi and y, together with their context information, into two vectors in a semantic space using deep neural networks f 1 and f 2 , parameterized by \u03b8; and 2) computing cosine similarity 2 between them.", "labels": [], "entities": []}, {"text": "In our case, we need to learn a task-specific semantic space where the distance between two vectors measures how likely they co-refer.", "labels": [], "entities": []}, {"text": "Commonsense knowledge such as \"demonstrators usually cause violence\" can be implicitly captured in the semantic space through DSSM, which is trained on a large amount of raw text.", "labels": [], "entities": []}, {"text": "DSSM requires labeled pairs for training.Since there is no labeled data for our tasks, we propose two unsupervised DSSMs, or UDSSMs.", "labels": [], "entities": []}, {"text": "As shown in(b) and 1(c), (S x , S y ) are encoded into contextual representations by deep neural networks f 1 and f 2 ; then we compute pair-wise their co-reference scores.", "labels": [], "entities": []}, {"text": "In what follows, we will describe two assumptions we propose to harvest training data from raw text.", "labels": [], "entities": []}, {"text": "Assumption I: A pronoun refers to one of its preceding nouns in the same sentence.", "labels": [], "entities": [{"text": "Assumption I", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7674123048782349}]}, {"text": "The sentences generated by this assumption will be used for training UDSSM-I.", "labels": [], "entities": [{"text": "UDSSM-I", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.5311025381088257}]}, {"text": "Some examples will be shown in the \"data generation\" section.", "labels": [], "entities": [{"text": "data generation", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7206530272960663}]}, {"text": "Assumption II: Ina sentence, pronouns of the same gender and plurality are more likely to refer to the same antecedent than other pronouns.", "labels": [], "entities": []}, {"text": "Similarly, the sentences following the assumption will be used for training UDSSM-II.", "labels": [], "entities": []}, {"text": "Note that the two models, UDSSM-I and UDSSM-II are trained on different types of pairwise training data, thus the model structures are different, as illustrated in(b) and 1(c), respectively.", "labels": [], "entities": [{"text": "UDSSM-I", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7994727492332458}, {"text": "UDSSM-II", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8283318877220154}]}, {"text": "Experiments demonstrated that our methods outperform stat-of-the-art performance on the tasks of WSC and PDP.", "labels": [], "entities": [{"text": "WSC", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.5365419983863831}]}], "datasetContent": [{"text": "In this section, we will introduce the datasets to train and evaluate our models for commonsense reasoning, the hyper-parameters of our model, and the analysis of our results.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8854260146617889}]}, {"text": "Training Corpus We make use of the raw text from Gutenberg 7 , a corpus offerring over 57,000 free eBooks, and 1 Billion Word 8 , a corpus of news, to train our model.", "labels": [], "entities": []}, {"text": "We first ignore the sentences that contain less than 10 tokens or longer than 50 tokens.", "labels": [], "entities": []}, {"text": "Then, for the model UDSSM-I, we collect all the sentences with the pronoun before which there're at least two nouns.", "labels": [], "entities": [{"text": "UDSSM-I", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.7202144265174866}]}, {"text": "For UDSSM-II, we collect all the sentences with at least 2 pronouns.", "labels": [], "entities": [{"text": "UDSSM-II", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.47668135166168213}]}, {"text": "In total, we collect around 4 million training pairs from each corpus for our proposed method respectively, and we split 5% as validation set.", "labels": [], "entities": []}, {"text": "Evaluation Dataset We evaluate our model on the commonsense reasoning datasets, Pronoun The best models reported in the works of and are trained on a much larger corpus from Common Crawl.", "labels": [], "entities": [{"text": "Evaluation Dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.6317944228649139}, {"text": "Common Crawl", "start_pos": 174, "end_pos": 186, "type": "DATASET", "confidence": 0.960827648639679}]}, {"text": "7 http://www.gutenberg.org 8 https://github.com/ciprian-chelba/ 1-billion-word-language-modeling-benchmark Disambiguation Problems (PDP) and Winograd Schema challenges (WSC) , which include 60 and 285 questions respectively.", "labels": [], "entities": []}, {"text": "Both of the tasks are constructed for testing commonsense reasoning and all the questions from these challenges are obvious for human beings to solve with commonsense knowledge, but hard for machines to solve with statistical techniques.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6883647590875626}]}, {"text": "We use the same setting for both our models.", "labels": [], "entities": []}, {"text": "The hidden state dimension of a single-directional LSTM is set to be 300.", "labels": [], "entities": []}, {"text": "We use 300 dimensional GloVe embeddings 11 for initialization.", "labels": [], "entities": []}, {"text": "We use Adamax to optimise the model, set learning rate to be 0.002, dropout rate on all layers are tuned from [0, 0.1, 0.2] and the batch size from.", "labels": [], "entities": [{"text": "Adamax", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.9729962348937988}]}, {"text": "For the model UDSSM-I, in one batch, we treat all sequence pairs not from the same sentence as negative cases.", "labels": [], "entities": []}, {"text": "And it takes around 30 hours on a single K40 GPU to train our models, which are much faster than training a large LM () taking weeks on multiple GPUs.", "labels": [], "entities": []}, {"text": "The experiment results are shown in.", "labels": [], "entities": []}, {"text": "Most of the performance in the top of the are the models trained with external knowledge bases, such as Cause-Effect (, WordNet, ConceptNet () knowledge bases.", "labels": [], "entities": []}, {"text": "Unsupervised Semantic Similarity Method (USSM) () is based on the skip-gram model () to train word embeddings and the embeddings of all the words connected by knowledge bases are optimized to be closer.", "labels": [], "entities": [{"text": "Unsupervised Semantic Similarity Method (USSM)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.5801374784537724}]}, {"text": "Neural Knowledge Activated Method (NKAM) ( trained a binary classification model based on whether the word pairs appear in the knowledge base.", "labels": [], "entities": []}, {"text": "One limitation of these methods is that they rely heavily on the external knowledge bases.", "labels": [], "entities": []}, {"text": "Another limitation is that they just linearly aggregate the embeddings of the words in the context, and that's hard to integrate the word order information.", "labels": [], "entities": []}, {"text": "Instead, our model with LSTM can better represent the contextual information.", "labels": [], "entities": []}, {"text": "Besides, our model don't need any external knowledge bases, and achieve a significant improvement on both of the datasets.", "labels": [], "entities": []}, {"text": "We further compare our models with the unsupervised baselines, which selects the candidate based on the cosine similarity of the hidden states of noun and pronoun.", "labels": [], "entities": []}, {"text": "Another unsupervised baseline, Google Language Model for commonsense reasoning, which compares the perplexities of the new sentences by replacing the pronoun with candidates.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8460648059844971}]}, {"text": "To make a fair comparison to's work, we also train our single model on the corpus of Gutenberg only.", "labels": [], "entities": []}, {"text": "We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset.", "labels": [], "entities": [{"text": "PDP dataset", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9181206226348877}, {"text": "UDSSM-II", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8345866203308105}, {"text": "WSC dataset", "start_pos": 144, "end_pos": 155, "type": "DATASET", "confidence": 0.891865611076355}]}, {"text": "We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus.", "labels": [], "entities": []}, {"text": "Finally, we also compare to the pre-trained Coreference Resolution Tool , and we can see that it doesn't adapt to our commonsense reasoning tasks and can't tell https://github.com/huggingface/ neuralcoref the difference between each pair of sentences from WSC.", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8757348358631134}, {"text": "WSC", "start_pos": 256, "end_pos": 259, "type": "DATASET", "confidence": 0.8682506680488586}]}, {"text": "In this way, our model can get much better performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The experiment results on PDP and WSC datasets. We compare our models to Goolge LM trained on the  same corpus 6 .", "labels": [], "entities": [{"text": "WSC datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9217454195022583}, {"text": "Goolge LM", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8838594257831573}]}]}