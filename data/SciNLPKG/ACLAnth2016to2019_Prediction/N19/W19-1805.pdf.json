{"title": [{"text": "The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models", "labels": [], "entities": [{"text": "Steep Road to Happily Ever After", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.6643282622098923}]}], "abstractContent": [{"text": "Visual storytelling is an intriguing and complex task that only recently entered the research arena.", "labels": [], "entities": [{"text": "Visual storytelling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7890468835830688}]}, {"text": "In this work, we survey relevant work to date, and conduct a thorough error analysis of three very recent approaches to visual storytelling.", "labels": [], "entities": []}, {"text": "We categorize and provide examples of common types of errors, and identify key shortcomings in current work.", "labels": [], "entities": []}, {"text": "Finally , we make recommendations for addressing these limitations in the future.", "labels": [], "entities": []}], "introductionContent": [{"text": "Artificial intelligence continues to evolve, making it increasingly plausible to develop models that interpret vision and language in a humanlike manner.", "labels": [], "entities": []}, {"text": "A crucial element of such models is the capacity to not only match images with surface-level descriptions, but to infer deeper contextual meaning.", "labels": [], "entities": []}, {"text": "Recent literature has begun to refer to this task as visual storytelling: the generation of a cohesive, sequential set of natural-language descriptions across multiple images (.", "labels": [], "entities": []}, {"text": "Visual storytelling is distinct from image captioning in that the text generated is oftentimes subjective, hinges on contextual image order, and typically employs more abstract and dynamic terms.", "labels": [], "entities": [{"text": "Visual storytelling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.782825767993927}, {"text": "image captioning", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.749827116727829}]}, {"text": "We illustrate the dichotomy between the two more concretely in terms of possible sets of sentences 1 for the images in.", "labels": [], "entities": []}, {"text": "Sentence Set 1: (1) A woman looking at a collection of tribal masks on the wall.", "labels": [], "entities": []}, {"text": "(2) Three skulls of varying sizes ordered from largest to smallest.", "labels": [], "entities": []}, {"text": "(3) A top view of a book about mythical creatures.", "labels": [], "entities": []}, {"text": "(4) Three people standing in a store looking at the products.", "labels": [], "entities": []}, {"text": "(5) An old traveling wagon that is on display.", "labels": [], "entities": []}, {"text": "1 Real samples (with punctuation and capitalization edited in some cases to increase readability) from the VIST dataset: http://visionandlanguage.net/VIST/ dataset.html  The first is a set of traditional image captions, whereas the latter represents a visual story.", "labels": [], "entities": [{"text": "VIST dataset", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.8155673742294312}]}, {"text": "Note that the former presents factual descriptions of the images in isolation from one another.", "labels": [], "entities": []}, {"text": "The latter also describes the images, but places stronger emphasis on the development of a cohesive narrative underlying the image sequence.", "labels": [], "entities": []}, {"text": "High-performing visual storytelling approaches will enable growth fora variety of applications, many of which are associated with language understanding tasks.", "labels": [], "entities": [{"text": "language understanding tasks", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.7918093999226888}]}, {"text": "They may also hold promise as a tool for assistive technology.", "labels": [], "entities": []}, {"text": "For instance, it is relatively common for users to upload large photo albums to social media platforms without including any image descriptions at all, making these albums inaccessible to those with sight impairments.", "labels": [], "entities": []}, {"text": "Visual storytelling could bridge this gap by automatically generating descriptive narratives for these albums.", "labels": [], "entities": []}, {"text": "Despite recent interest in visual storytelling, fu-eled by the 2018 Visual Storytelling Challenge, 2 this research area is still quite nascent.", "labels": [], "entities": [{"text": "visual storytelling", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7805694341659546}, {"text": "Visual Storytelling Challenge, 2", "start_pos": 68, "end_pos": 100, "type": "DATASET", "confidence": 0.603808319568634}]}, {"text": "To date, no comprehensive review has been made of work on the task.", "labels": [], "entities": []}, {"text": "Such an analysis is necessary to spur additional research and recommend directions for future work.", "labels": [], "entities": []}, {"text": "Here, we fill this void, making the following contributions: \u2022 We catalogue existing models for visual storytelling, comparing and contrasting them with one another.", "labels": [], "entities": []}, {"text": "\u2022 We provide a performance comparison based on the original results (when publicly available) or re-implementations (when not).", "labels": [], "entities": []}, {"text": "\u2022 We categorize errors into distinct types and compile statistics indicating their frequencies within and across models.", "labels": [], "entities": []}, {"text": "\u2022 We make recommendations for addressing these errors in future visual storytelling models.", "labels": [], "entities": []}, {"text": "We discuss relevant prior work in Section 2, and describe the dataset used for visual storytelling tasks in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we present an overview of the models included in our analysis, and in Section 5 we explain how these models were evaluated.", "labels": [], "entities": []}, {"text": "We conduct our comprehensive error analysis in Section 6, and make our recommendations based on the outcomes of this analysis in Section 7.", "labels": [], "entities": []}, {"text": "We summarize these sections and report our final conclusions in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained and evaluated AREL according to the instructions provided in its publicly available: Performance obtained when we ran AREL-s-50 and GLACNet, the two models for which we were able to obtain working implementations.", "labels": [], "entities": [{"text": "AREL", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7193894386291504}, {"text": "AREL-s-50", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.8759716153144836}]}, {"text": "However, we modified the source code slightly such that we were able to obtain the individual METEOR scores for each predicted story in the test set.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9858141541481018}]}, {"text": "This helped us in performing an in-depth error analysis of the generated stories and determining how well the automatic metrics were at scoring the stories.", "labels": [], "entities": []}, {"text": "Training the model took around 2 weeks on a 3.5 GHz Intel Core i5 CPU with 16 GB RAM.", "labels": [], "entities": []}, {"text": "The GLACNet code is also publicly available.", "labels": [], "entities": [{"text": "GLACNet code", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9258437752723694}]}, {"text": "We trained and evaluated the model using an NVIDIA Tesla P100 GPU instance on Google Cloud Platform.", "labels": [], "entities": []}, {"text": "The model took one week to finish training.", "labels": [], "entities": []}, {"text": "The original source code only provided an average METEOR score across all generated stories after testing.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9800592362880707}]}, {"text": "Thus, we added code to produce the METEOR score for each story.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9800082147121429}]}, {"text": "We will make all adapted source code publicly available online to ensure easy replicability.", "labels": [], "entities": []}, {"text": "The source code for Contextualize, Show and Tell is available online as well.", "labels": [], "entities": [{"text": "Tell", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7893034219741821}]}, {"text": "The authors personally sent us the generated stories, so we did not re-implement their model.", "labels": [], "entities": []}, {"text": "We have directly included their METEOR results in our evaluation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9616852402687073}]}, {"text": "Common metrics for evaluating visual storytelling models include METEOR (Banerjee and 7 https://github.com/littlekobe/AREL 8 Extenuating circumstances limited our hardware resources in the midst of our AREL evaluation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9871568083763123}]}, {"text": "Training would have undoubtedly been quicker using GPUs, as was done in the original paper (), BLEU (), CIDEr (, and ROUGE-L ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9983845949172974}, {"text": "CIDEr", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.5079495310783386}, {"text": "ROUGE-L", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9865788817405701}]}, {"text": "METEOR, the primary metric considered in the Visual Storytelling Challenge, calculates the alignment between the machine-generated hypotheses and the reference stories based on the exact, stem, synonym, and paraphrase matches between words and phrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8530808091163635}]}, {"text": "While AREL was evaluated using METEOR as well as the other metrics, GLACNet was evaluated using only METEOR scores and measures of perplexity.", "labels": [], "entities": [{"text": "AREL", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.898640513420105}, {"text": "METEOR", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.5666627287864685}]}, {"text": "Contextualize, Show and Tell was also evaluated using only METEOR.", "labels": [], "entities": [{"text": "Tell", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9766108989715576}, {"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.7035979628562927}]}, {"text": "We generated scores for the remaining metrics as well for GLACNet and Contextualize, Show and Tell to aid our analysis.", "labels": [], "entities": [{"text": "GLACNet", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8494460582733154}]}], "tableCaptions": [{"text": " Table 1: Performance as reported in the source papers (Wang et al., 2018; Kim et al., 2018). BLEU-RL, METEOR- RL, and CIDEr-RL were baseline reinforcement learning approaches using BLEU, METEOR, and CIDEr scores  as their reward functions, respectively (Wang et al., 2018).", "labels": [], "entities": [{"text": "BLEU-RL", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9969611763954163}, {"text": "METEOR- RL", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9409910639127096}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9835061430931091}]}, {"text": " Table 2: Performance obtained when we ran AREL-s-50 and GLACNet, the two models for which we were able  to obtain working implementations.", "labels": [], "entities": [{"text": "AREL-s-50", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.8661532998085022}]}, {"text": " Table 5: Frequency (in terms of overall percentage) of  the most common error types across all 1010 generated  test stories by AREL and GLACNet and 1938 gener- ated test stories by Contextualize, Show and Tell.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9935699701309204}, {"text": "AREL", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9010742902755737}, {"text": "GLACNet", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.8112953901290894}]}]}