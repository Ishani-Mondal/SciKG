{"title": [{"text": "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing", "labels": [], "entities": [{"text": "Imposing Label-Relational Inductive Bias", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9122243523597717}]}], "abstractContent": [{"text": "Existing entity typing systems usually exploit the type hierarchy provided by knowledge base (KB) schema to model label correlations and thus improve the overall performance.", "labels": [], "entities": []}, {"text": "Such techniques, however, are not directly applicable to more open and practical scenarios where the typeset is not restricted by KB schema and includes avast number of free-form types.", "labels": [], "entities": []}, {"text": "To model the underlying label correlations without access to manually annotated label structures, we introduce a novel label-relational inductive bias, represented by a graph propagation layer that effectively encodes both global label co-occurrence statistics and word-level similarities.", "labels": [], "entities": []}, {"text": "On a large dataset with over 10,000 free-form types, the graph-enhanced model equipped with an attention-based matching module is able to achieve a much higher recall score while maintaining a high-level precision.", "labels": [], "entities": [{"text": "recall score", "start_pos": 160, "end_pos": 172, "type": "METRIC", "confidence": 0.9865140914916992}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9959219694137573}]}, {"text": "Specifically, it achieves a 15.3% relative F1 improvement and also less inconsistency in the outputs.", "labels": [], "entities": [{"text": "F1 improvement", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9680463969707489}]}, {"text": "We further show that a simple modification of our proposed graph layer can also improve the performance on a conventional and widely-tested dataset that only includes KB-schema types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7250265628099442}]}, {"text": "In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge fora wide range of NLP tasks, such as coreference resolution), relation extraction () and question answering ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 232, "end_pos": 254, "type": "TASK", "confidence": 0.9359208345413208}, {"text": "relation extraction", "start_pos": 257, "end_pos": 276, "type": "TASK", "confidence": 0.8893491923809052}, {"text": "question answering", "start_pos": 284, "end_pos": 302, "type": "TASK", "confidence": 0.8941827714443207}]}, {"text": "In practical scenarios, a key challenge of entity typing is to correctly predict multiple ground-truth type labels from a large candidate set that covers a wide range of types in different granularities.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7246372103691101}]}, {"text": "In this sense, it is essential for models to effectively capture the inter-label correlations.", "labels": [], "entities": []}, {"text": "For instance, if an entity is identified as a \"criminal\", then the entity must also be a \"person\", but it is less likely for this entity to be a \"police officer\" at the same time.", "labels": [], "entities": []}, {"text": "When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions.", "labels": [], "entities": []}, {"text": "As shown in, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions.", "labels": [], "entities": []}, {"text": "Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions ( or enhanced type label encodings () that enable parameter sharing between related types.", "labels": [], "entities": []}, {"text": "These methods rely on the assump-tion that the underlying type structures are predefined in entity typing datasets.", "labels": [], "entities": []}, {"text": "For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types are from KB ontologies and naturally follow tree-like structures.", "labels": [], "entities": []}, {"text": "However, since knowledge bases are inherently incomplete, existing KBs only include a limited set of entity types.", "labels": [], "entities": []}, {"text": "Thus, models trained on these datasets fail to generalize to lots of unseen types.", "labels": [], "entities": []}, {"text": "In this work, we investigate entity typing in a more open scenario where the typeset is not restricted by KB schema and includes over 10,000 free-form types (.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7812204957008362}]}, {"text": "As most of the types do not follow any predefined structures, methods that explicitly incorporate type hierarchies cannot be straightforwardly applied here.", "labels": [], "entities": []}, {"text": "To effectively capture the underlying label correlations without access to known type structures, we propose a novel label-relational inductive bias, represented by a graph propagation layer that operates in the latent label space.", "labels": [], "entities": []}, {"text": "Specifically, this layer learns to incorporate a label affinity matrix derived from global type co-occurrence statistics and word-level type similarities.", "labels": [], "entities": []}, {"text": "It can be seamlessly coupled with existing models and jointly updated with other model parameters.", "labels": [], "entities": []}, {"text": "Empirically, on the Ultra-Fine dataset (, the graph layer alone can provide a significant 11.9% relative F1 improvement over previous models.", "labels": [], "entities": [{"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9965721368789673}]}, {"text": "Additionally, we show that the results can be further improved (11.9% \u2192 15.3%) with an attention-based mention-context matching module that better handles pronouns entity mentions.", "labels": [], "entities": []}, {"text": "With a simple modification, we demonstrate that the proposed graph layer is also beneficial to the widely used OntoNotes dataset, despite the fact that samples in OntoNotes have lower label multiplicity (i.e., average number of ground-truth types for each sample) and thus require less labeldependency modeling than the Ultra-Fine dataset.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.9311027526855469}]}, {"text": "To summarize, our major contribution includes: \u2022 We impose an effective label-relational bias on entity typing models with an easy-toimplement graph propagation layer, which allows the model to implicitly capture type dependencies; \u2022 We augment our graph-enhanced model with an attention-based matching module, which constructs stronger interactions between the mention and context representations; \u2022 Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets Our experiments mainly focus on the Ultra-Fine entity typing dataset which has 10,331 labels and most of them are defined as freeform text phrases.", "labels": [], "entities": []}, {"text": "The training set is annotated with heterogeneous supervisions based on KB, Wikipedia and head words in dependency trees, resulting in about 25.2M 4 training samples.", "labels": [], "entities": [{"text": "KB", "start_pos": 71, "end_pos": 73, "type": "DATASET", "confidence": 0.8695942759513855}]}, {"text": "This dataset also includes around 6,000 crowdsourced samples.", "labels": [], "entities": []}, {"text": "Each of these samples has five groundtruth labels on average.", "labels": [], "entities": []}, {"text": "For a fair comparison, we use the original test split of the crowdsourced data for evaluation.", "labels": [], "entities": []}, {"text": "To better understand the capability of our model, we also test our model on the commonly-used OntoNotes () benchmark.", "labels": [], "entities": []}, {"text": "It is worth noting that this dataset is much smaller and has lower label multiplicity than the Ultra-Fine dataset, i.e., each sample only has around 1.5 labels on average.", "labels": [], "entities": []}, {"text": "shows a comparison of these two datasets.", "labels": [], "entities": []}, {"text": "Baselines For the Ultra-Fine dataset, we compare our model with AttentiveNER () and the multi-task model proposed with the Ultra-Fine dataset.", "labels": [], "entities": [{"text": "Ultra-Fine dataset", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.7292459905147552}, {"text": "AttentiveNER", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.8630657196044922}, {"text": "Ultra-Fine dataset", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.8048007488250732}]}, {"text": "Note that other models that require pre-defined type hierarchy are not applicable to this dataset.", "labels": [], "entities": []}, {"text": "For experiments on OntoNotes, in addition to the two neural baselines for Ultra-Fine, we compare with several existing methods that explicitly utilize the pre-defined type structures in loss functions.", "labels": [], "entities": []}, {"text": "Namely, these methods are AFET (), LNR (Ren et al., 2016b) and NFETC (.", "labels": [], "entities": [{"text": "AFET", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9141713380813599}, {"text": "LNR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8194164037704468}, {"text": "NFETC", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.7880651950836182}]}, {"text": "Evaluation Metrics On Ultra-Fine, we first evaluate the mean reciprocal rank (MRR), macro precision(P), recall (R) and F1 following existing research.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 56, "end_pos": 82, "type": "METRIC", "confidence": 0.9426123797893524}, {"text": "macro precision(P)", "start_pos": 84, "end_pos": 102, "type": "METRIC", "confidence": 0.8270341277122497}, {"text": "recall (R)", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9612542688846588}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9986271858215332}]}, {"text": "As P, Rand F1 all depend on a chosen threshold on probabilities, we also consider a more transparent comparison using precisionrecall curves.", "labels": [], "entities": [{"text": "Rand F1", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.6108076870441437}, {"text": "precisionrecall", "start_pos": 118, "end_pos": 133, "type": "METRIC", "confidence": 0.9956791996955872}]}, {"text": "On OntoNotes, we use the standard metrics used by baseline models: accuracy, macro, and micro F1 scores.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.865226149559021}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995217323303223}, {"text": "micro F1 scores", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.7917372584342957}]}, {"text": "Implementation Details Most of the model hyperparameters, such as embedding dimensions, learning rate, batch size, dropout ratios on context and mention representations are consistent with existing models.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.6939375400543213}]}, {"text": "Since the mention-context matching module brings more parameters, we apply a dropout layer over the extracted feature f to avoid overfitting.", "labels": [], "entities": [{"text": "mention-context matching", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6816537976264954}]}, {"text": "We list all the hyperparameters in the appendix.", "labels": [], "entities": []}, {"text": "Models for OntoNotes are trained with standard binary cross-entropy (BCE) losses defined on all candidate labels.", "labels": [], "entities": [{"text": "cross-entropy (BCE) losses", "start_pos": 54, "end_pos": 80, "type": "METRIC", "confidence": 0.7175018966197968}]}, {"text": "When training on Ultra-Fine, we adopt the multi-task loss proposed in which divides the cross-entropy loss into three separate losses over different type granularities.", "labels": [], "entities": []}, {"text": "The multi-task objective avoids penalizing false negative types and can achieve higher recalls.", "labels": [], "entities": [{"text": "recalls", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9989534616470337}]}, {"text": "We report the results on Ultra-Fine in.", "labels": [], "entities": [{"text": "Ultra-Fine", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9106187224388123}]}, {"text": "It is worth mentioning that our model, denoted as LA-BELGCN, is trained using the unlicensed training set which is smaller than the one used by compared baselines.", "labels": [], "entities": [{"text": "LA-BELGCN", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9277759790420532}]}, {"text": "Even though our model significantly outperforms the baselines, fora fair comparison, we first test our model using the same decision threshold (0.5) used by previous models.", "labels": [], "entities": []}, {"text": "In terms of F1, our best model (LABELGCN) outperforms existing methods by a large margin.", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.998140811920166}, {"text": "LABELGCN", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.920647919178009}]}, {"text": "Compared to, our model improves on both precision and recall significantly.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9996967315673828}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9996615648269653}]}, {"text": "Compared to the AttentiveNER trained with standard BCE loss, our model achieves much higher recall but performs worse in precision.", "labels": [], "entities": [{"text": "BCE loss", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9491887986660004}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9996457099914551}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9987284541130066}]}, {"text": "This is due to the fact that when trained with BCE loss, the model usually retrieves only one label per sample and these types are mostly general types 5 which are easier to predict.", "labels": [], "entities": [{"text": "BCE loss", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9295053482055664}]}, {"text": "With higher recalls or more retrieved types, achieving high precision requires being accurate on fine-grained types, which are often harder to predict.", "labels": [], "entities": [{"text": "recalls", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9882959723472595}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9970095157623291}]}, {"text": "As the precision and recall scores both rely on the decision threshold, different models or different metrics can have different optimal thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9993494153022766}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9964373111724854}]}, {"text": "As shown by the \"LABELGCN + thresh tuning\" entry in, with threshold tuning, our model beats baselines in all metrics.", "labels": [], "entities": [{"text": "LABELGCN + thresh tuning", "start_pos": 17, "end_pos": 41, "type": "METRIC", "confidence": 0.879861444234848}]}, {"text": "We also see that recall is usually lagging behind precision on this dataset, indicating that F1 score is mainly affected: Comparison with baseline models on the Ultra-Fine dataset.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.999606192111969}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9991555213928223}, {"text": "F1 score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9902425706386566}, {"text": "Ultra-Fine dataset", "start_pos": 161, "end_pos": 179, "type": "DATASET", "confidence": 0.844496101140976}]}, {"text": "Threshold-tuning gives better performance on all metrics compared to both baselines.: Decomposed validation performance on pronouns and the other entities.", "labels": [], "entities": [{"text": "Threshold-tuning", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9596208333969116}]}, {"text": "Each entry is obtained using the best threshold among the 50 equal-interval thresholds.", "labels": [], "entities": []}, {"text": "The corresponding PR curves can be found in the appendix (). by the recall and tuning towards recall can usually lead to higher F1 scores.", "labels": [], "entities": [{"text": "PR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9161727428436279}, {"text": "appendix", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9453205466270447}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9985949397087097}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9979410767555237}, {"text": "F1 scores", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.979989618062973}]}, {"text": "For more transparent comparisons, we show the precision-recall curves in.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9990500807762146}]}, {"text": "These data points are based on the validation performance given by 50 equal-interval thresholds between 0 and 1.", "labels": [], "entities": []}, {"text": "We can see there is a clear margin between our model and the multi-task baseline method (LabelGCN vs Choi et al.).", "labels": [], "entities": []}, {"text": "To better understand the requirements for applying our model, we further evaluate on the OntoNotes dataset.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.9600488841533661}]}, {"text": "Here we do not apply the proposed mention-context matching module as this dataset does not include any pronoun entities.", "labels": [], "entities": [{"text": "mention-context matching", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6701528429985046}]}, {"text": "To obtain more reliable co-occurrence statistics, we use the augmented training data released by.", "labels": [], "entities": []}, {"text": "However, since the training set is still much smaller than that of the Ultra-Fine dataset, the derived co-occurrence statistics are relatively noisy and might introduce undesired bias.", "labels": [], "entities": [{"text": "Ultra-Fine dataset", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.6663896441459656}]}, {"text": "We thus add an additional residual connection to our graph convolution layer, which allows the model to selectively use co-occurrence statistics.", "labels": [], "entities": []}, {"text": "This indeed gives us improvements over previous state-of-thearts, as shown in.", "labels": [], "entities": []}, {"text": "However, compared to Ultra-Fine, the margin of the improvement is smaller.", "labels": [], "entities": [{"text": "margin", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9804379940032959}]}, {"text": "In view of the key differences of these two datasets, we highlight two key requirements for our proposed model to offer substantial improvements.", "labels": [], "entities": []}, {"text": "First, there should be a large-scale training set so that the derived co-occurrence statistics can reasonably reflect the true label correlations.", "labels": [], "entities": []}, {"text": "Second, the samples themselves should also have higher label multiplicity.", "labels": [], "entities": []}, {"text": "In fact, most of the samples in OntoNotes only have 1 or 2 labels.", "labels": [], "entities": []}, {"text": "This property actually alleviates the need for models to capture label dependencies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with baseline models on the Ultra-Fine dataset. Threshold-tuning gives better performance  on all metrics compared to both baselines.", "labels": [], "entities": [{"text": "Ultra-Fine dataset", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9096036553382874}, {"text": "Threshold-tuning", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.9947340488433838}]}, {"text": " Table 3: Decomposed validation performance on pro- nouns and the other entities. Each entry is obtained  using the best threshold among the 50 equal-interval  thresholds. The corresponding PR curves can be found  in the appendix (", "labels": [], "entities": []}, {"text": " Table 4: Qualitative analysis of validation samples. We use different colors and subscript symbols to mark incon- sistencies. The bottom two rows show error cases for both models.", "labels": [], "entities": []}, {"text": " Table 5: Results on OntoNotes. Upper rows show the  results of baselines that explicitly use the hierarchical  type structures.", "labels": [], "entities": []}]}