{"title": [{"text": "Top-Down Structurally-Constrained Neural Response Generation with Lexicalized Probabilistic Context-Free Grammar", "labels": [], "entities": [{"text": "Neural Response Generation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6855223079522451}]}], "abstractContent": [{"text": "We consider neural language generation under a novel problem setting: generating the words of a sentence according to the order of their first appearance in its lexicalized PCFG parse tree, in a depth-first, left-to-right manner.", "labels": [], "entities": [{"text": "neural language generation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6922133366266886}, {"text": "PCFG parse tree", "start_pos": 173, "end_pos": 188, "type": "DATASET", "confidence": 0.82523645957311}]}, {"text": "Unlike previous tree-based language generation methods, our approach is both (i) top-down and (ii) explicitly generating syntactic structure at the same time.", "labels": [], "entities": []}, {"text": "In addition, our method combines neural model with symbolic approach: word choice at each step is constrained by its predicted syntactic function.", "labels": [], "entities": []}, {"text": "We applied our model to the task of dialog response generation, and found it significantly improves over sequence-to-sequence baseline, in terms of diversity and relevance.", "labels": [], "entities": [{"text": "dialog response generation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8894154032071432}]}, {"text": "We also investigated the effect of lexicalization on language generation, and found that lexicalization schemes that give priority to content words have certain advantages over those focusing on dependency relations.", "labels": [], "entities": [{"text": "language generation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7289773225784302}]}], "introductionContent": [{"text": "Neural encoder-decoder architectures have shown promise and become very popular for natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.6578808029492696}]}, {"text": "Over the past few years, there has seen a surging interest in sequence-tosequence learning for dialog response generation using neural encoder-decoder models.", "labels": [], "entities": [{"text": "dialog response generation", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.8059568603833517}]}, {"text": "Typically, an encoder encodes conversational context (source side) information into vector representations, and a decoder auto-regressively generates word tokens conditioned on the source vectors and previously generated words.", "labels": [], "entities": []}, {"text": "Two problems arise with the standard left-toright decoding mechanism.", "labels": [], "entities": []}, {"text": "First, no future information is available at any step of the decoding process, while the study of linguistic dependency structure shows that certain words depend on the others that come right to them.", "labels": [], "entities": []}, {"text": "Second, preceding words define the context for following words in left-to-right, auto-regressive language models, while linguistic theories may prefer other hierarchies (e.g., adjectives modifying nouns, adverbs modifying verbs).", "labels": [], "entities": []}, {"text": "Psycho-linguistics studies also suggest that human may first generate the abstract representation of the things to say, and then linearize them into sentences).", "labels": [], "entities": []}, {"text": "Therefore, it is appealing to consider language generation in alternative orders.", "labels": [], "entities": [{"text": "language generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7408934533596039}]}, {"text": "This poses a greater challenge because a mechanism in extra to word generation is needed for deciding the position of each word.", "labels": [], "entities": [{"text": "word generation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.6978923231363297}]}, {"text": "Some recent works adopt a syntax-free approach to address this problem.", "labels": [], "entities": []}, {"text": "proposed a middleout decoder that starts from the middle of sentences and finishes the rest in forward and backward directions.", "labels": [], "entities": []}, {"text": "( and) start with one or two predicted keywords and generate the rest of sentences in a similar fashion.", "labels": [], "entities": []}, {"text": "Others incorporate tree structures without syntactic relations and categories.", "labels": [], "entities": []}, {"text": "() canonicalizes the dependency structures of sentences into ternary trees, and generate only the words top-down.", "labels": [], "entities": []}, {"text": "Yet another line of work aim to model the full syntactic trees.", "labels": [], "entities": []}, {"text": "generates phrase structures and part-of-speech tags along with words for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.73728147149086}]}, {"text": "( generates shiftreduce action sequences of context-free grammars in addition to words for language model and parsing.", "labels": [], "entities": []}, {"text": "But words are still generated in left-to-right order in their approaches.", "labels": [], "entities": []}, {"text": "In the domain of dialog, we believe language generation can benefit from alternative orders, for the same reasons argued earlier.", "labels": [], "entities": []}, {"text": "On the other hand, inhuman conversations, the structure of utterances usually correspond with dialog states (e.g., wh-noun or wh-adverb phrases are more likely to be used in a request state), so modelling phrase structures can potentially help capturing discourse level information.", "labels": [], "entities": []}, {"text": "In order to be able to generate complete syntactic trees, while be flexible about word generation order at the same time, the use of lexicalized grammar becomes a natural choice.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use BLEU (), ROUGE, and METEOR () scores as automatic evaluation metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9993304014205933}, {"text": "ROUGE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9960289001464844}, {"text": "METEOR", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9963281750679016}]}, {"text": "While the reliability of such metrics has been criticized (, there is also evidence that for task-oriented domains, these metrics correlate with human judgment to a certain extent (.", "labels": [], "entities": []}, {"text": "We evaluate the semantic similarity between generated responses and human responses/persona by the cosine distance of their sentence embeddings.", "labels": [], "entities": []}, {"text": "We use the word averaging approach by) to embed the responses, which has been demonstrated to be very good at capturing lexical level semantic similarity.", "labels": [], "entities": [{"text": "word averaging", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7298968583345413}, {"text": "capturing lexical level semantic similarity", "start_pos": 110, "end_pos": 153, "type": "TASK", "confidence": 0.6941101849079132}]}, {"text": "The normalizing singular vector is obtained from the responses in training set.", "labels": [], "entities": []}, {"text": "We measure word overlapping between generated responses and the responses in training set using BLEU and ROUGE as a proxy for novelty.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9989199638366699}, {"text": "ROUGE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9931167364120483}]}, {"text": "The responses in training set with most common words with generated responses are used as references.", "labels": [], "entities": []}, {"text": "For diversity, we count the number of distinct n-grams.", "labels": [], "entities": []}, {"text": "In addition, we perform a k-means clustering on the sentence embeddings of responses into 10 clusters, and measure average squared Euclidean distance between members of each cluster (Inertia).", "labels": [], "entities": [{"text": "Inertia", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.9960938096046448}]}, {"text": "The larger the number, the harder it is to separate embeddings into 10 clusters, thus the greater the diversity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average absolute positions of different type of  words.", "labels": [], "entities": []}, {"text": " Table 2: Average frequency of first five words in differ- ent generation orders.", "labels": [], "entities": [{"text": "Average frequency", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9139528870582581}]}, {"text": " Table 4: Evaluation results on Persona dataset.", "labels": [], "entities": [{"text": "Persona dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9431203901767731}]}]}