{"title": [{"text": "Deep Learning Techniques for Humor Detection in Hindi-English Code-Mixed Tweets", "labels": [], "entities": [{"text": "Humor Detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.9897308051586151}]}], "abstractContent": [{"text": "We propose bilingual word embeddings based on word2vec and fastText models (CBOW and Skip-gram) to address the problem of Humor detection in Hindi-English code-mixed tweets in combination with deep learning architec-tures.", "labels": [], "entities": [{"text": "Humor detection", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.9785812199115753}]}, {"text": "We focus on deep learning approaches which are not widely used on code-mixed data and analyzed their performance by experimenting with three different neural network models.", "labels": [], "entities": []}, {"text": "We propose convolution neural network (CNN) and bidirectional long-short term memory (biLSTM) (with and without Attention) models which take the generated bilingual embeddings as input.", "labels": [], "entities": []}, {"text": "We make use of Twitter data to create bilingual word embed-dings.", "labels": [], "entities": []}, {"text": "All our proposed architectures outper-form the state-of-the-art results, and Attention-based bidirectional LSTM model achieved an accuracy of 73.6% which is an increment of more than 4% compared to the current state-of-the-art results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9993818998336792}]}], "introductionContent": [{"text": "In the present day, we observe an exponential rise in the number of individuals using Internet Technology for different purposes like entertainment, learning and sharing their experiences.", "labels": [], "entities": []}, {"text": "This led to a tremendous increase in content generated by users on social networking and microblogging sites.) act as a platform for users to reach large masses in real-time and express their thoughts freely and sometimes anonymously amongst communities and virtual networks.", "labels": [], "entities": []}, {"text": "These natural language texts depict various linguistic elements such as aggression, irony, humor, and sarcasm.", "labels": [], "entities": []}, {"text": "In recent years, automatic detection of these elements has become a research interest for both organizations and research communities.", "labels": [], "entities": [{"text": "automatic detection", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.6906068325042725}]}, {"text": "* These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "The advancement in computer technologies places increasing emphasis on systems and models that can effectively handle natural human language.", "labels": [], "entities": []}, {"text": "So far, the majority of the research in natural language processing and deep learning is focused on the English language as individuals across the world use it widely.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6543667614459991}]}, {"text": "But, in multilingual geographies like India, it is a natural phenomenon for individuals to use more than one language words in speech and in social media sites like).", "labels": [], "entities": []}, {"text": "Data shows that in India, there are about 314.9 million bilingual speakers and most of these speakers tend to mix two languages interchangeably in their communication.", "labels": [], "entities": []}, {"text": "Researchers) defined this linguistic behavior as Code-mixingthe embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language which produces utterances consisting of words taken from the lexicons of different languages.", "labels": [], "entities": [{"text": "Code-mixingthe embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language which produces utterances consisting of words taken from the lexicons of different languages", "start_pos": 49, "end_pos": 278, "type": "Description", "confidence": 0.7173775383404323}]}, {"text": "The primary challenge with the code-mixed corpus is the lack of data in general text-corpora, for conducting experiments.", "labels": [], "entities": []}, {"text": "In this paper, we take up the task of detecting one critical element of natural language which plays a significant part in our linguistic, cognitive, and social lives, i.e., extensively studied the psychology of humor and stated that it is ubiquitous across cultures and it is a necessary part of all verbal communication.", "labels": [], "entities": []}, {"text": "The classification of some text as humor can be very subjective.", "labels": [], "entities": []}, {"text": "Also, capturing Humor in higher order structures (de) through text processing is considered as a challenging natural language problem.", "labels": [], "entities": [{"text": "capturing Humor in higher order structures (de)", "start_pos": 6, "end_pos": 53, "type": "TASK", "confidence": 0.7877768145667182}]}, {"text": "Pun detection in one-liners () and detection of humor in Yelp reviews (de have also been studied in recent years.", "labels": [], "entities": [{"text": "Pun detection", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8796584010124207}, {"text": "detection of humor in Yelp reviews", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7307563722133636}]}, {"text": "Deep learning techniques () have contributed to significant progress in various areas of research, including natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.6871546506881714}]}, {"text": "Convolutional neural network based networks have been used for sentence classification, bidirectional LSTM networks (biLSTM) were used for sequence tagging, and attention based bidirectional LSTM networks were used for relational classification () and topic-based sentiment analysis (.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8104280829429626}, {"text": "sequence tagging", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.7202129065990448}, {"text": "relational classification", "start_pos": 219, "end_pos": 244, "type": "TASK", "confidence": 0.7668726742267609}, {"text": "topic-based sentiment analysis", "start_pos": 252, "end_pos": 282, "type": "TASK", "confidence": 0.7276287078857422}]}, {"text": "In this work, we propose three deep learning networks using bilingual word embeddings as input and compare it against the classification models presented in () using their annotated corpus to detect one of the playful domains of language: Humor.", "labels": [], "entities": []}, {"text": "An example from the corpus: \"Subha ka bhula agar sham ko wapas ghar aa jaye then we must thank GPS technology..\"", "labels": [], "entities": []}, {"text": "\"(If someone is lost in the morning and returns home in the evening then we must thank GPS technology.)", "labels": [], "entities": []}, {"text": "This tweet is annotated as humorous.", "labels": [], "entities": []}, {"text": "In particular, we are focused on code-mixed data as it lacks the presence of bilingual word embeddings, commonly used, to train any deep learning model which is essential for understanding human behavior, events, reviews, studying trends as well as linguistic analysis ().", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 249, "end_pos": 268, "type": "TASK", "confidence": 0.722446471452713}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Similar meaning Hindi and English words  similarity scores with word2vec and fastText models", "labels": [], "entities": []}, {"text": " Table 2: Detailed accuracies achieved on the bench- mark dataset by different models. *Random Forest,  Naive Bayes, Extra tree, and kernel SVM accuracies  are from (Khandelwal et al., 2018)", "labels": [], "entities": [{"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9511246085166931}]}]}