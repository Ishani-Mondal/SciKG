{"title": [{"text": "ExCL: Extractive Clip Localization Using Natural Language Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "The task of retrieving clips within videos based on a given natural language query requires cross-modal reasoning over multiple frames.", "labels": [], "entities": []}, {"text": "Prior approaches such as sliding window classifiers are inefficient, while text-clip similarity driven ranking-based approaches such as segment proposal networks are far more complicated.", "labels": [], "entities": []}, {"text": "In order to select the most relevant video clip corresponding to the given text description, we propose a novel extrac-tive approach that predicts the start and end frames by leveraging cross-modal interactions between the text and video-this removes the need to retrieve and re-rank multiple proposal segments.", "labels": [], "entities": []}, {"text": "Using recurrent networks we encode the two modalities into a joint representation which is then used in different variants of start-end frame predictor networks.", "labels": [], "entities": []}, {"text": "Through extensive experimentation and ablative analysis , we demonstrate that our simple and elegant approach significantly outperforms state of the art on two datasets and has comparable performance on a third.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clip Localization is the task of selecting the relevant span of temporal frames in a video corresponding to a natural language description and has recently piqued interest in research that lies at the intersection of visual and textual modalities.", "labels": [], "entities": [{"text": "Clip Localization is the task of selecting the relevant span of temporal frames in a video corresponding to a natural language description", "start_pos": 0, "end_pos": 138, "type": "Description", "confidence": 0.7606460059230978}]}, {"text": "An example of this task is demonstrated in.", "labels": [], "entities": []}, {"text": "It requires cross-modal reasoning to ground freeform text inside the video and calls for models capable of segmenting a video into action segments as well as measuring multi-modal semantic similarity.", "labels": [], "entities": []}, {"text": "This task is inherently discriminative, i.e., there is only a single most relevant clip pertaining to * Equal contribution, randomly ordered.", "labels": [], "entities": []}, {"text": "a given query in the corresponding video.", "labels": [], "entities": []}, {"text": "However, most prior works () explore this as a ranking task over a fixed number of moments by uniformly sampling clips within a video.", "labels": [], "entities": []}, {"text": "Moreover, these approaches are restrictive in scope since they use predefined clips as candidates fora video and cannot be easily extended to videos with considerable variance in length.; apply twostage methods which rank candidate clips using a learned similarity metric.", "labels": [], "entities": []}, {"text": "propose a sliding window approach with alignment and offset regression learning objective, but it is limited by the coarseness of the windows and is thus inefficient and inflexible.", "labels": [], "entities": []}, {"text": "address this through a query-guided segment proposal network (QSPN).", "labels": [], "entities": []}, {"text": "However, the similarity metric used by these approaches is difficult to learn as it is sensitive to the choice of negative samples ( and it still does not consider the discriminative nature of the task.", "labels": [], "entities": []}, {"text": "Hence, we propose an elegant and fairly simple extractive approach.", "labels": [], "entities": []}, {"text": "Our technique is similar to text-based Machine Comprehension (  but in a multimodal setting where the video is analogous to the text passage and the targetclip is analogous to the text span corresponding to the correct answer.", "labels": [], "entities": []}, {"text": "We verify empirically that our method significantly outperforms prior work on two benchmark datasets -TACoS, ActivityNet and comparably well on the third, Charades-STA.", "labels": [], "entities": [{"text": "TACoS", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.6303885579109192}]}, {"text": "Our flexible, modular approach to Extractive Clip Localization (ExCL) can easily be extended to incorporate attention models and different variants of encoders for both visual and text modality to improve performance further.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on three datasets.", "labels": [], "entities": []}, {"text": "Note that we do not evaluate our models on   We compare the different training objectives (labeled ExCL-clf for classification and ExCL-reg for regression) and evaluate the usefulness of recurrent encoders for video representations by removing the video LSTM (labeled ExCL-clf/reg 1-{a, b, c}).", "labels": [], "entities": []}, {"text": "We also perform ablative analysis of a range of span predictor networks.", "labels": [], "entities": []}, {"text": "We compare the performance of our proposed model with three baselines which are the current SOTA for the different datsets.", "labels": [], "entities": [{"text": "SOTA", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9158294200897217}]}, {"text": "While we significantly beat the first two baselines for TACoS and Activity Net respectively, we attain comparable performance with the third for Charades-STA.", "labels": [], "entities": [{"text": "TACoS", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.8767966628074646}]}, {"text": "It is to be noted that since the approach in ( ) depends on ranking fixed number of segments in each video, it is not scalable to the other two datasets which have longer videos with greater variance in their lengths.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Clip Localization Accuracy at IoU = {0.3, 0.5, 0.7} for TACoS, Charades-STA and ActivityNet. Here  ExCL-clf represents the classification loss model and ExCL-reg represents the regression loss model. ExCL- {clf/reg} 1-m refer to models run without a video LSTM encoder, while ExCL-{clf/reg} 2-m include the video  LSTM. m = a, b, c refer to MLP, tied LSTM and conditioned LSTM span predictor networks respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9100543260574341}]}]}