{"title": [{"text": "Generating Text through Adversarial Training using Skip-Thought Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "GANs have been shown to perform exceedingly well on tasks pertaining to image generation and style transfer.", "labels": [], "entities": [{"text": "GANs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8600023984909058}, {"text": "image generation", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7464636564254761}, {"text": "style transfer", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7794233858585358}]}, {"text": "In the field of language modelling, word embeddings such as GLoVe and word2vec are state-of-the-art methods for applying neural network models on textual data.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7611701786518097}]}, {"text": "Attempts have been made to utilize GANs with word embeddings for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7528875470161438}]}, {"text": "This study presents an approach to text generation using Skip-Thought sentence embeddings with GANs based on gradient penalty functions and f-measures.", "labels": [], "entities": [{"text": "text generation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7898935973644257}]}, {"text": "The proposed architecture aims to reproduce writing style in the generated text by modelling the way of expression at a sentence level across all the works of an author.", "labels": [], "entities": []}, {"text": "Extensive experiments were run in different embedding settings on a variety of tasks including conditional text generation and language generation.", "labels": [], "entities": [{"text": "conditional text generation", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.6914648115634918}, {"text": "language generation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7657583355903625}]}, {"text": "The model outperforms baseline text generation networks across several automated evaluation metrics like BLEU-n, METEOR and ROUGE.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9961379170417786}, {"text": "METEOR", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.8798384666442871}, {"text": "ROUGE", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9412423968315125}]}, {"text": "Further, wide applicability and effectiveness in real life tasks are demonstrated through human judgement scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inducing a particular style in generated text is a promising development which can lead to producing acceptable responses in dialogue generation, image captioning and artificial chat bot systems.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8258599638938904}, {"text": "image captioning", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.7073290199041367}]}, {"text": "In unsupervised text generation, estimating the distribution of real text from a corpus is a challenging task.", "labels": [], "entities": [{"text": "text generation", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7436910271644592}]}, {"text": "Recent approaches using adversarial training have addressed this issue by trying to overcome the exposure bias that models trained for maximum likelihood suffer from.", "labels": [], "entities": []}, {"text": "This work proposes an approach for text generation using a Generative Adversarial Network (GAN) with Skip-Thought vectors (STGAN).", "labels": [], "entities": [{"text": "text generation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8377735614776611}]}, {"text": "GANs () area class of neural networks that explicitly train a generator to produce highquality samples by pitting the generator against an adversarial discriminative model.", "labels": [], "entities": []}, {"text": "GANs output differentiable values and the task of discrete text generation is challenging because of the nondifferentiable nature of generating discrete symbols.", "labels": [], "entities": [{"text": "discrete text generation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6398477554321289}]}, {"text": "Hence, in the present work, the GANs are trained with sentence embedding vectors as a differentiable input.", "labels": [], "entities": [{"text": "GANs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9228146076202393}]}, {"text": "The sentence embeddings are produced using Skip-Thought ( , a neural network model for learning fixed length representations of sentences.", "labels": [], "entities": []}, {"text": "People's way of expression and communication intention is more diverse across utterances than the vocabulary.", "labels": [], "entities": []}, {"text": "To imitate this, the proposed STGAN architecture models the variability at the utterance level in a corpus rather than at word or character level.", "labels": [], "entities": []}, {"text": "The effectiveness of this approach is evaluated on automated corpus-based metrics: BLEU-n (), METEOR (Banerjee and) and ROUGE) using different embeddings: Average GloVe (), Vector Extrema GloVe () and Skip-Thought ( . We perform an empirical study with human judgements to assess both the quality and the style reproduction in the generated text.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.997235119342804}, {"text": "METEOR", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9754219055175781}, {"text": "ROUGE", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.9940084218978882}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of models on word-overlap based automated metrics when trained with different embeddings.  Skip-Thought gives better results than GloVe for BLEU-n and ROUGE metrics, while the METEOR scores are  comparable to that when using averaged GloVe embedding with Attention BiLSTM generator.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9562820196151733}, {"text": "BLEU-n", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9951606392860413}, {"text": "METEOR", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.979927659034729}]}, {"text": " Table 2: BLEU-2, BLEU-3 METEOR and ROUGE metric scores across GAN models with different f-measures.  GloVe: GLoVe Average, ST: Skip-Thought, WGAN: Wasserstein GAN, GP: Gradient Penalty", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9981757402420044}, {"text": "BLEU-3", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9977866411209106}, {"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9267985820770264}, {"text": "ROUGE metric", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9578790068626404}, {"text": "GLoVe Average", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.8963547945022583}, {"text": "ST", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.955033540725708}, {"text": "WGAN", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.8159642815589905}, {"text": "GP: Gradient Penalty", "start_pos": 165, "end_pos": 185, "type": "METRIC", "confidence": 0.8576171100139618}]}, {"text": " Table 3: Sentences sampled from STGAN when training on CMU-SE Dataset; mode collapse is overcome by using  minibatch discrimination. Sample quality in terms of length and diversity further improved by using Wasserstein  distance f-measure with gradient penalty regularizer. WGAN: Wasserstein GAN, GP: Gradient Penalty", "labels": [], "entities": [{"text": "STGAN", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8593009114265442}, {"text": "CMU-SE Dataset", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9809595942497253}, {"text": "WGAN", "start_pos": 275, "end_pos": 279, "type": "DATASET", "confidence": 0.4655384421348572}, {"text": "GP: Gradient Penalty", "start_pos": 298, "end_pos": 318, "type": "METRIC", "confidence": 0.8045568913221359}]}, {"text": " Table 4: Weighted human scores for sentences.  |rating \u2212 3| is weight given to each sentence's rating.  39.02% of the generated samples were marked as real.", "labels": [], "entities": []}]}