{"title": [{"text": "Learning When Not to Answer: A Ternary Reward Structure for Reinforcement Learning based Question Answering", "labels": [], "entities": [{"text": "Learning When Not to Answer", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6138292670249939}, {"text": "Reinforcement Learning based Question Answering", "start_pos": 60, "end_pos": 107, "type": "TASK", "confidence": 0.8327456831932067}]}], "abstractContent": [{"text": "In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications.", "labels": [], "entities": []}, {"text": "We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings.", "labels": [], "entities": []}, {"text": "More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence.", "labels": [], "entities": []}, {"text": "We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer.", "labels": [], "entities": []}, {"text": "We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9980707764625549}]}, {"text": "Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8898360133171082}]}, {"text": "In these methods the input question is first parsed into a constituent question entity and relation.", "labels": [], "entities": []}, {"text": "The answer entity is then identified by sequentially taking a number of steps (or 'hops') over the knowledge graph (KG) starting from the question entity.", "labels": [], "entities": []}, {"text": "The agent receives a positive reward if it arrives at the correct answer entity and a negative reward for an incorrect answer entity.", "labels": [], "entities": []}, {"text": "For example, for the question \"What is the capital of France?\", the question entity is (F rance) and the goal is to find a path in the KG which connects it to (P aris).", "labels": [], "entities": [{"text": "F rance)", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9451020161310831}, {"text": "P aris)", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.8004846175511678}]}, {"text": "The relation between the answer entity and question entity in this example is (Capital of ) which is missing from the KG and has to be inferred via alternative paths.", "labels": [], "entities": []}, {"text": "A possible two-hop path to find the answer is to use the fact that (M acron) is the president of (F rance) and that he lives in (P aris).", "labels": [], "entities": []}, {"text": "However, there are many paths that lead to the entity (P aris) but also to other entities which makes finding the correct answer a non-trivial task.", "labels": [], "entities": []}, {"text": "The standard evaluation metrics used for these systems are metrics developed for web search such as Mean Reciprocal Rank (MRR) and hits@k, where k ranges from 1 to 20.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 100, "end_pos": 126, "type": "METRIC", "confidence": 0.9653068085511526}]}, {"text": "We argue that this is not a correct evaluation mechanism fora practical question-answering system (such as Alexa, Cortana, Siri, etc.) where the goal is to return a single answer for each question.", "labels": [], "entities": []}, {"text": "Moreover it is assumed that there is always an answer entity that could be reached from the question entity in limited number of steps.", "labels": [], "entities": []}, {"text": "However this cannot be guaranteed in a large-scale commercial setting and for all KGs.", "labels": [], "entities": []}, {"text": "For example, in our proprietary dataset used for the experimentation, for 15.60% of questions the answer entity cannot be reached within the limit of number of steps used by the agent.", "labels": [], "entities": []}, {"text": "Hence, we propose anew evaluation criterion, allowing systems to return 'no answer' as a response when no answer is available.", "labels": [], "entities": []}, {"text": "We demonstrate that existing state-of-the-art methods are not suited fora practical questionanswering setting and perform poorly in our evaluation setup.", "labels": [], "entities": []}, {"text": "The root-cause of poor performance is the reward structure which does not provide any incentive to learn not to answer.", "labels": [], "entities": []}, {"text": "The modified reward structure we present allows agents to learn not to answer in a principled way.", "labels": [], "entities": []}, {"text": "Rather than having only two rewards, a positive and a negative reward, we introduce a ternary reward structure that also rewards agents for not answering a question.", "labels": [], "entities": []}, {"text": "A higher reward is given to the agent for correctly answering a question compared to not answering a question.", "labels": [], "entities": []}, {"text": "In this setup the agent learns to make a trade-off between these three possibilities to obtain the highest total reward overall questions.", "labels": [], "entities": []}, {"text": "Additionally, because the search space of possible paths exponentially grows with the number of hops, we also investigate using Depth-First-Search (DFS) algorithm to collect paths that lead to the correct answer.", "labels": [], "entities": []}, {"text": "We use these paths as a supervised signal for training the neural network before the reinforcement learning algorithm is applied.", "labels": [], "entities": []}, {"text": "We show that this improves overall performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "User-facing question answering systems inherently face a trade-off between presenting an answer to a user that could potentially be incorrect, and choosing not to answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7410323917865753}]}, {"text": "However, prior work in knowledge graph question-answering (QA) only considers cases in which the answering agent always produces an answer.", "labels": [], "entities": [{"text": "knowledge graph question-answering (QA)", "start_pos": 23, "end_pos": 62, "type": "TASK", "confidence": 0.6480150371789932}]}, {"text": "This setup originates from the link prediction and knowledge base completion tasks in which the evaluation criteria are hits@k and Mean Reciprocal Rank (MRR), where k ranges from 1 to 20.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7531477510929108}, {"text": "knowledge base completion", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5716722309589386}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 131, "end_pos": 157, "type": "METRIC", "confidence": 0.967800517876943}]}, {"text": "However, these metrics are not an accurate representation of practical question-answering systems in which the goal is to return a single correct answer or not answer at all.", "labels": [], "entities": []}, {"text": "Moreover, using these metrics result in the problem of the model learning 'spurious' paths since the metrics encourage the models to make wild guesses even if the path is unlikely to lead to the correct answer.", "labels": [], "entities": []}, {"text": "We therefore propose to measure the fraction of questions the system answers (Answer Rate) and the number of correct answers out of all answers (Precision) to measure the system performance.", "labels": [], "entities": [{"text": "Answer Rate)", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.8784751892089844}, {"text": "Precision)", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9863147139549255}]}, {"text": "We combine these two metrics by taking the harmonic mean and call this the QA Score.", "labels": [], "entities": [{"text": "QA Score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.782840371131897}]}, {"text": "This can be viewed as a variant of the popular F-Score metric, with answer rate used as an analogue to recall in the original metric.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.935422420501709}, {"text": "answer rate", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.8424366414546967}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9976645708084106}]}, {"text": "We evaluate our proposed approach on a publicly available dataset,    For both datasets, we add the reverse relations of all relations in the training set in order to facilitate backward navigation following the approach of previous work.", "labels": [], "entities": [{"text": "backward navigation", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.7599043548107147}]}, {"text": "Similarly, a 'no op' relation is added for each entity between the entity and itself, which allows the agent to loop/reason multiple consecutive steps over the same entity.", "labels": [], "entities": []}, {"text": "An overview of both datasets can be found in.", "labels": [], "entities": []}, {"text": "We extend the publicly available implementation of for our experimentation.", "labels": [], "entities": []}, {"text": "We set the size of the entity and relation representations d at 100 and the hidden state at 200.", "labels": [], "entities": []}, {"text": "We use a single layer LSTM and train models with path length 3 (tuned using hyper-parameter search).", "labels": [], "entities": []}, {"text": "We optimize the neural network using Adam (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on FB15k-237 dataset.", "labels": [], "entities": [{"text": "FB15k-237 dataset", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9841510355472565}]}, {"text": " Table 2: Results on Alexa69k-378 dataset.", "labels": [], "entities": [{"text": "Alexa69k-378 dataset", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.9777317643165588}]}, {"text": " Table 3: Statistics of the datasets.", "labels": [], "entities": []}]}