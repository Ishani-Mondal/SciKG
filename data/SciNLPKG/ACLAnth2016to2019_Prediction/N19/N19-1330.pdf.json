{"title": [{"text": "Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models", "labels": [], "entities": [{"text": "Improve Recurrent Neural Language", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.7671662643551826}]}], "abstractContent": [{"text": "Recurrent neural network language models (RNNLM) form a valuable foundation for many NLP systems, but training the models can be computationally expensive, and may take days to train on a large corpus.", "labels": [], "entities": []}, {"text": "We explore a technique that uses large corpus n-gram statistics as a regularizer for training a neural network LM on a smaller corpus.", "labels": [], "entities": []}, {"text": "In experiments with the Billion-Word and Wikitext corpora , we show that the technique is effective, and more time-efficient than simply training on a larger sequential corpus.", "labels": [], "entities": []}, {"text": "We also introduce new strategies for selecting the most informative n-grams, and show that these boost efficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural network models of language (RNNLMs) form a foundation for many natural language processing systems.", "labels": [], "entities": []}, {"text": "However, the networks can be expensive to train: training a single model over several million tokens can take hours, and searching through the large hyperparameter space of RNNLMs often entails training and testing hundreds of different models.", "labels": [], "entities": []}, {"text": "This makes it burdensome to experiment with new RNNLM architectures on large corpora, or to train RNNLMs for new textual domains.", "labels": [], "entities": []}, {"text": "RNNLMs are typically trained on sequential text.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how to efficiently augment the training of RNNLMs by regularizing the models to match n-gram statistics taken from a much larger corpus.", "labels": [], "entities": []}, {"text": "The motivation is that large-corpus n-gram statistics maybe informative to an RNNLM trained on a smaller sequential corpus, but unlike RNNLM training, ngram statistics are inexpensive to compute even overlarge corpora.", "labels": [], "entities": []}, {"text": "Moreover, the statistics only need to be computed once and can be re-used for training many different smaller-corpus RNNLMs.", "labels": [], "entities": []}, {"text": "Naively, regularizing an RNNLM to match a given set of n-gram statistics is non-trivial, because the marginal probabilities that n-gram statistics represent are not parameters of the RNNLM.", "labels": [], "entities": []}, {"text": "In recent work, showed that it was possible to regularize an RNNLM to match given n-gram statistics by training the network, when started from a zero state, to match each ngram probability.", "labels": [], "entities": []}, {"text": "However, the regularization approach in that work had tractability limitationsthe time cost of the regularization was sufficiently high that using it was inferior to simply training the RNNLM on more sequential text.", "labels": [], "entities": [{"text": "regularization", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9706741571426392}, {"text": "RNNLM", "start_pos": 186, "end_pos": 191, "type": "DATASET", "confidence": 0.8261591792106628}]}, {"text": "In this paper, we present an efficient n-gram regularization technique and show that the technique can improve RNNLM training.", "labels": [], "entities": [{"text": "RNNLM training", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.8175259530544281}]}, {"text": "Our method has three distinctions from previous work that provide efficiency.", "labels": [], "entities": []}, {"text": "First, we prioritize regularizing only the n-grams that are most likely to improve the model, by focusing on cases where the RNNLM's sequential training corpus diverges significantly from the n-gram statistics.", "labels": [], "entities": [{"text": "RNNLM's sequential training corpus", "start_pos": 125, "end_pos": 159, "type": "DATASET", "confidence": 0.7992677927017212}]}, {"text": "Secondly, we regularize the entire output softmax of the RNN to match given conditional n-gram statistics, which means we can impose a large number of statistical constraints using only one softmax evaluation.", "labels": [], "entities": []}, {"text": "Finally, we use an ensemble of multiple loss functions in our regularizer, which provides an additional boost.", "labels": [], "entities": []}, {"text": "In experiments, we show how n-gram regularization with these enhancements results in better models using the same amount of training time, compared to standard sequential training.", "labels": [], "entities": []}, {"text": "We also plan to release our code base and to the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present our experiments measuring the effectiveness of conditional n-gram regularization.", "labels": [], "entities": [{"text": "conditional n-gram regularization", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.5704362491766611}]}], "tableCaptions": [{"text": " Table 1: Test perplexities of different methods and se- quential training token sizes. sq log: mean squared log  probability ratio penalty. KL: mean Kullback-Leibler  divergence penalty. Models with the combined penalty  achieve the lowest perplexities.", "labels": [], "entities": [{"text": "mean squared log  probability ratio penalty", "start_pos": 96, "end_pos": 139, "type": "METRIC", "confidence": 0.7374235987663269}, {"text": "KL", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9201376438140869}]}, {"text": " Table 2: Test perplexities of RNNLMs trained on  Wikitext-2 regularized with different numbers of bi- grams using the combined penalty. Baseline indicates  an equal-time controlled baseline, random is a regular- ized model with bigrams selected randomly, and ELC  indicates our proposed strategy.", "labels": [], "entities": [{"text": "ELC", "start_pos": 260, "end_pos": 263, "type": "METRIC", "confidence": 0.9953782558441162}]}, {"text": " Table 3: Test perplexities of RNN-LMs trained with  and without regularization on the Wikitext-2 corpus,  ensembled with a KN-smoothed trigram model trained  on the Wikitext-103 corpus.", "labels": [], "entities": [{"text": "Wikitext-2 corpus", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.9325621426105499}, {"text": "Wikitext-103 corpus", "start_pos": 166, "end_pos": 185, "type": "DATASET", "confidence": 0.9264374673366547}]}]}