{"title": [{"text": "MuST-C: a Multilingual Speech Translation Corpus", "labels": [], "entities": [{"text": "Multilingual Speech Translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6303607523441315}]}], "abstractContent": [{"text": "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora.", "labels": [], "entities": [{"text": "spoken language translation (SLT)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8324560821056366}]}, {"text": "This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation.", "labels": [], "entities": [{"text": "SLT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9797331094741821}, {"text": "automatic speech recognition", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.6095787783463796}, {"text": "machine translation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7614395916461945}]}, {"text": "To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages.", "labels": [], "entities": [{"text": "multilingual speech translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6916471918423971}, {"text": "SLT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9897533655166626}]}, {"text": "For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations.", "labels": [], "entities": []}, {"text": "Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with strong baseline system on each language direction.", "labels": [], "entities": [{"text": "corpus creation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7184700220823288}, {"text": "SLT", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.5210665464401245}]}], "introductionContent": [{"text": "Besides the increased computing power, the recent surge of neural end-to-end approaches to natural language processing tasks has been stoked by the increased availability of data.", "labels": [], "entities": []}, {"text": "For instance, when supported by sizeable training corpora, the robustness and the strong generalization capabilities of neural networks led to their dominance over previous paradigms both in automatic speech recognition (ASR ( ) and machine translation (MT ().", "labels": [], "entities": [{"text": "automatic speech recognition (ASR", "start_pos": 191, "end_pos": 224, "type": "TASK", "confidence": 0.7112556993961334}, {"text": "machine translation (MT", "start_pos": 233, "end_pos": 256, "type": "TASK", "confidence": 0.768900454044342}]}, {"text": "Compared to its two parent research areas, spoken language translation (SLT) has not shown such a steady progress yet.", "labels": [], "entities": [{"text": "spoken language translation (SLT)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8110291759173075}]}, {"text": "Despite recent claims by big industry players about the effectiveness of end-toend learning  to a stable dominance of the end-to-end paradigm also in this area is the scarcity of training corpora.", "labels": [], "entities": []}, {"text": "While cascade ASR+MT solutions can exploit the wealth of task-specific data available for each of the two tasks, 1 the situation for end-to-end model training is much less favourable.", "labels": [], "entities": [{"text": "ASR+MT", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.8315033117930094}]}, {"text": "As shown in Table 1, few publicly available corpora exist, their language coverage is rather limited and, most importantly, their size is often too small (less than 100 hours of translated audio) for training datahungry neural models.", "labels": [], "entities": []}, {"text": "To circumvent the problem, neural SLT approaches currently rely on: i) large proprietary corpora (, ii) multitask learning In resource-rich conditions, ASR and MT training often builds on thousands of hours of transcribed speech and tens of millions of parallel sentences, respectively.", "labels": [], "entities": [{"text": "ASR", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9808531403541565}, {"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9693264365196228}]}, {"text": "2 Besides the corpora reported in, several smaller (< 4 hours) freely-available datasets have been created (e.g. the IWSLT evaluation campaign development and test sets from 2010 to 2017 and the Griko-Italian corpus by).", "labels": [], "entities": [{"text": "IWSLT evaluation campaign development and test sets", "start_pos": 117, "end_pos": 168, "type": "DATASET", "confidence": 0.8912408777645656}, {"text": "Griko-Italian corpus", "start_pos": 195, "end_pos": 215, "type": "DATASET", "confidence": 0.8535065352916718}]}, {"text": "(, iii) encoder/decoder pre-training (, iv) synthesized speech data, or v) machine-translated target text data.", "labels": [], "entities": []}, {"text": "Though effective, solutions ii) and iii) assume the availability of ASR and MT data, which is not always guaranteed (especially in low-resource language settings).", "labels": [], "entities": [{"text": "ASR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9420524835586548}, {"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.8535964488983154}]}, {"text": "Solutions iv) and v), instead, rely on training material derived from sub-optimal automatic data creation/augmentation procedures.", "labels": [], "entities": []}, {"text": "This situation calls for initiatives towards the creation of large, high-quality multilingual corpora suitable to explore end-to-end SLT in more favorable conditions similar to condition i).", "labels": [], "entities": [{"text": "SLT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.917077898979187}]}, {"text": "Along this direction, our contributions are: \u2022 A large (\u223c400 hours of speech per language) multilingual corpus for SLT from English into 8 languages (German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian); \u2022 An empirical verification of its quality; \u2022 ASR, MT and SLT results computed with strong baseline systems on each language direction.", "labels": [], "entities": [{"text": "SLT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9877324104309082}, {"text": "ASR", "start_pos": 272, "end_pos": 275, "type": "METRIC", "confidence": 0.6921057105064392}, {"text": "MT", "start_pos": 277, "end_pos": 279, "type": "TASK", "confidence": 0.7720769047737122}]}, {"text": "MuST-C is released under a Creative Commons license, Attribution -Non Commercial -No Derivatives (CC BY NC ND 4.0 International), and is freely downloadable at mustc.fbk.eu", "labels": [], "entities": [{"text": "CC BY NC ND 4.0 International)", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.7218152284622192}]}], "datasetContent": [{"text": "In this section we present two sets of experiments, which are respectively aimed to: i) empirically assess the quality of the MuST-C corpus (Section 3.3) and ii) compute baseline ASR, MT, and SLT results for future comparisons (Section 3.4).", "labels": [], "entities": [{"text": "MuST-C corpus", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.8135291635990143}]}, {"text": "In these experiments, the audio-transcription alignments of MuST-C are used to train and evaluate ASR models, transcription-translation alignments are used for the MT models, and audiotranslation alignments are used for the SLT models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9412848353385925}]}, {"text": "In our experiments, texts are tokenized and punctuation is normalized.", "labels": [], "entities": []}, {"text": "Furthermore, the English texts are lowercased, while the target language texts are split into characters still preserving the word boundaries.", "labels": [], "entities": []}, {"text": "For MT, we segment the English words with the BPE algorithm () using a maximum of 30K merge operations.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9870873689651489}, {"text": "BPE", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9445397853851318}]}, {"text": "The output generation of all models is performed using beam search with abeam size of 5.", "labels": [], "entities": []}, {"text": "ASR performance is measured with word error rate (WER) computed on lower-cased, tokenized texts without punctuation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9767066240310669}, {"text": "word error rate (WER)", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.8944400548934937}]}, {"text": "MT and SLT results are computed with BLEU ().", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6805964112281799}, {"text": "SLT", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.8352307081222534}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9990313053131104}]}, {"text": "As observed in Section 2, each section of MuST-C is larger than any other existing publicly available SLT corpus.", "labels": [], "entities": []}, {"text": "The usefulness of a resource, however, is not only a matter of size but also of quality (in this case, the quality of the audio-transcription-translation alignments).", "labels": [], "entities": []}, {"text": "For an empirical verification of this aspect, we experimented with two comparable datasets.", "labels": [], "entities": []}, {"text": "One is the TED-derived English-German IWSLT18 corpus (, which is built following a pipeline that performs segment extraction and alignment based on time information (i.e. start and end position of each segment in the SubRip Text (SRT) files) instead of text-level alignments.", "labels": [], "entities": [{"text": "TED-derived English-German IWSLT18 corpus", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.7876626998186111}, {"text": "segment extraction", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7986477017402649}]}, {"text": "The other is the English-German subset of MuST-C derived from the same TED Talks used to build the IWSLT18 corpus.", "labels": [], "entities": [{"text": "IWSLT18 corpus", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9311947226524353}]}, {"text": "On one side (MuST-C), the number of segments, their length, and the overall corpus quality depend on text-level alignments.", "labels": [], "entities": []}, {"text": "On the other side (IWSLT18), they depend on matching time stamps.", "labels": [], "entities": [{"text": "IWSLT18", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8349286317825317}]}, {"text": "This strategy, however, has some drawbacks.", "labels": [], "entities": []}, {"text": "First, as pointed out by, the use of time information brings some noise in the corpus.", "labels": [], "entities": []}, {"text": "Second, it often results in utterancelevel alignment (based on speakers' pauses in the original audio).", "labels": [], "entities": [{"text": "utterancelevel alignment", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7661686539649963}]}, {"text": "Compared to sentence-level alignment, this level of granularity can be sub-optimal during model training (e.g. for MT and SLT, learning from complete sentences is easier than learning from phrases).", "labels": [], "entities": [{"text": "sentence-level alignment", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.747706800699234}]}, {"text": "Finally, time information about the recorded speech is not always available: bypassing this need would make the method replicable on other data (not only TED-like).", "labels": [], "entities": []}, {"text": "Though initialized with the same set of 1, 619 talks, the two pipelines produce different corpora.", "labels": [], "entities": []}, {"text": "As shown in, our approach filters out 58 entire talks (\u223c3.6% of the total) but the final number of segments, their corresponding audio duration and their average length (in words) are larger.: Statistics of the English-German corpora created by applying the IWSLT18 and MuST-C pipelines to the same initial set of 1, 619 TED Talks.", "labels": [], "entities": [{"text": "IWSLT18", "start_pos": 258, "end_pos": 265, "type": "DATASET", "confidence": 0.9300611019134521}]}, {"text": "Each corpus was divided into training, development and test.", "labels": [], "entities": []}, {"text": "Development and test contain segments from randomly selected common talks (i.e. those preserved by the MuST-C pipeline).", "labels": [], "entities": []}, {"text": "Their size is respectively 2.3K (from 28 talks) and 2.1K segments (from 26 talks).", "labels": [], "entities": []}, {"text": "The test portions were concatenated to create a balanced test set (4.2K segments) containing half of the instances from the IWSLT18 corpus and half from MuST-C.", "labels": [], "entities": [{"text": "IWSLT18 corpus", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.9709571003913879}, {"text": "MuST-C", "start_pos": 153, "end_pos": 159, "type": "DATASET", "confidence": 0.9186174869537354}]}, {"text": "The remaining material was used to separately train ASR, MT and SLT models on homogeneous data from either of the two corpora (i.e. three systems   per corpus).", "labels": [], "entities": [{"text": "ASR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6347739100456238}, {"text": "MT and SLT", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.5702646871407827}]}, {"text": "All the systems are evaluated on the common test set.", "labels": [], "entities": []}, {"text": "shows that the models trained on MuST-C data achieve better results on the balanced test set in all the three tasks.", "labels": [], "entities": [{"text": "MuST-C data", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.7497719824314117}]}, {"text": "In particular: i) a reduction of 10.1 WER points in ASR indicates a higher quality of audio-transcription alignments, ii) a BLEU increase of 0.56 points in MT indicates a similar quality for transcription-translation alignments, and iii) a BLEU increase of 3.31 points in SLT indicates a higher quality of audio-translation alignments.", "labels": [], "entities": [{"text": "WER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9937011003494263}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9991042017936707}, {"text": "BLEU", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.9993844032287598}]}, {"text": "We consider these results as evidence of the reliability of our corpus creation methodology.", "labels": [], "entities": [{"text": "corpus creation", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7100659161806107}]}, {"text": "Being the same for all the language pairs, we expect this procedure to end up in comparable quality for all the 8 sections of MuST-C.", "labels": [], "entities": [{"text": "MuST-C", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.8561808466911316}]}, {"text": "We finally present baseline results computed, for all the three tasks, on each section of MuST-C.", "labels": [], "entities": [{"text": "MuST-C", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.6747041940689087}]}, {"text": "Also for these experiments, development and test data are created with segments from talks that are common to all the languages.", "labels": [], "entities": []}, {"text": "Their size is respectively 1.4K (from 11 talks) and 2.5K segments (from 27 talks).", "labels": [], "entities": []}, {"text": "The remaining data (of variable size depending on the language pairs) are used for training.", "labels": [], "entities": []}, {"text": "For the sake of replicability, these splits are preserved in the released version of MuST-C.", "labels": [], "entities": [{"text": "MuST-C", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.8939960598945618}]}, {"text": "The results in lead to the following observations.", "labels": [], "entities": []}, {"text": "First, though not directly comparable since they are computed on different test sets, English-German results are inline (actually higher, since they are produced by models built on larger training data) with those presented in Section 3.3.", "labels": [], "entities": []}, {"text": "This indicates that the level of quality observed in the previous experiments with a subset of the training data is preserved by the whole material released for this language pair.", "labels": [], "entities": []}, {"text": "Second, looking at the other language pairs, ASR, MT and SLT results are comparable with the English-German scores.", "labels": [], "entities": [{"text": "ASR", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8291641473770142}, {"text": "MT", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.6639517545700073}, {"text": "SLT", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9467202425003052}]}, {"text": "Besides normal fluctuations in the optimization of the neural models, performance differences are coherent with: i) the relative difficulty of each target language (e.g. Russian is more difficult due to high inflection) and ii) the variable quantity of training data available (e.g. French has the largest training set, see).", "labels": [], "entities": []}, {"text": "Overall, these explainable differences suggest that our corpus creation methodology yields homogeneous quality for all the languages covered by MuST-C.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics for each section of MuST-C.", "labels": [], "entities": [{"text": "MuST-C", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.6565273404121399}]}, {"text": " Table 3: Statistics of the English-German corpora cre- ated by applying the IWSLT18 and MuST-C pipelines  to the same initial set of 1, 619 TED Talks.", "labels": [], "entities": [{"text": "IWSLT18", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.8913317918777466}]}, {"text": " Table 4: Performance of ASR, MT and SLT systems  trained with En-De IWSLT18 and MuST-C data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8126774430274963}, {"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8246978521347046}, {"text": "En-De IWSLT18", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.6575622260570526}, {"text": "MuST-C data", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.7912174761295319}]}, {"text": " Table 5: Baseline ASR, MT and SLT results for each  language direction.", "labels": [], "entities": [{"text": "ASR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.39294373989105225}, {"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.7044535875320435}]}]}