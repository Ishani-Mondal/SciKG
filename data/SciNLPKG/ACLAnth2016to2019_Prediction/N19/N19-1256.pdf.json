{"title": [], "abstractContent": [{"text": "In this paper, we present an adaptive convolution for text classification to give stronger flexibility to convolutional neural networks (CNNs).", "labels": [], "entities": [{"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8129023611545563}]}, {"text": "Unlike traditional convolutions that use the same set of filters regardless of different inputs, the adaptive convolution employs adaptively generated convolutional filters that are conditioned on inputs.", "labels": [], "entities": []}, {"text": "We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to convolution blocks in existing CNNs.", "labels": [], "entities": []}, {"text": "We show the efficacy of our approach in existing CNNs based on our performance evaluation.", "labels": [], "entities": []}, {"text": "Our evaluation indicates that adaptive convolutions improve all the baselines, without any exception, as much as up to 2.6 percentage point in seven benchmark text classification datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification assigns topics to texts by understanding the semantics of the texts.", "labels": [], "entities": [{"text": "Text classification assigns topics", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8494384735822678}]}, {"text": "It is one of the fundamental tasks in natural language processing (NLP) which has abroad range of applications, including web search (, contextual advertising (, and user profiling (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.8224876224994659}]}, {"text": "Traditional approaches to text classification use sparse representations of text, such as bag-of-words ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7294508218765259}]}, {"text": "To date, neural networkbased text embedding techniques, particularly convolutional neural networks (CNNs)) have shown remarkable results in text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.808597594499588}]}, {"text": "One of the driving forces of CNNs is a convolution operation.", "labels": [], "entities": []}, {"text": "It screens local information which appear in inputs (either input texts or outputs from the previous convolution block) by convolving a set of filters with inputs.", "labels": [], "entities": []}, {"text": "In the context of text classification, this operation is analogous to questions and answers.", "labels": [], "entities": [{"text": "text classification", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7675352394580841}]}, {"text": "Convolutional filters are like questions asking for the intensity of particular patterns in receptive fields.", "labels": [], "entities": []}, {"text": "Outputs of convolution operations are the answers from the inputs to the questions.", "labels": [], "entities": [{"text": "Outputs", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9584070444107056}]}, {"text": "CNNs derive the right class with stacked convolution blocks . On this point, CNNs can be likened to players of the twenty questions who guess the answers by iteratively asking questions and receiving information.", "labels": [], "entities": []}, {"text": "However, differences exist between humans and traditional CNNs in the manner in which they play this game.", "labels": [], "entities": []}, {"text": "Humans adaptively ask questions by fully utilizing information they have obtained.", "labels": [], "entities": []}, {"text": "If players have narrowed the answer down to the name of a person, they would not want to ask questions such as, \"Does that have four legs?\".", "labels": [], "entities": []}, {"text": "Rather, they would prefer questions related to the target's profession or origin that are practical for inferring the answer.", "labels": [], "entities": []}, {"text": "In contrast, typical CNNs use the same set of filters in any circumstances).", "labels": [], "entities": []}, {"text": "This may hamper CNNs from leveraging the information they have as intermediate hidden representations of inputs processed in consecutive convolution operations, and focusing their capacity on disentangling uncertainty.", "labels": [], "entities": []}, {"text": "Motivated by this, we propose an adaptive convolution to give stronger flexibility to networks and allow networks to simulate human capabilities of utilizing the information they have.", "labels": [], "entities": []}, {"text": "The adaptive convolution performs convolution operations with filters (questions) dynamically generated conditioned on inputs (outputs from the previous convolution block).", "labels": [], "entities": []}, {"text": "We achieve this by attaching filter-generating networks, carefully designed modular networks for generating filters, to convolutional blocks in CNNs.", "labels": [], "entities": []}, {"text": "Each attached filter-generating network produces filters from the input and pass the filters to its convolution block.", "labels": [], "entities": []}, {"text": "Generated filters are reflections of the information contained in the inputs and allow the networks to focus on extracting informative features.", "labels": [], "entities": []}, {"text": "We further propose a hashing technique to substantially compress the size of the filtergenerating networks, and prevent a considerable increase in the number of parameters when applying the adaptive convolution.", "labels": [], "entities": []}, {"text": "Our adaptive convolution can easily be applied to existing CNNs, because of the modularity of the filter-generating networks.", "labels": [], "entities": []}, {"text": "We demonstrate that significant gains can be realized by applying adaptive convolutions to baseline CNNs), based on a performance evaluation.", "labels": [], "entities": []}, {"text": "Our adaptive convolutions improve performance of all the baseline CNNs as much as up to 2.6 percentage point, without any exception, in seven text classification benchmark datasets.", "labels": [], "entities": []}, {"text": "To summarize, our technical contributions are three fold: \u2022 We propose an adaptive convolution which can give stronger flexibility to existing CNNs.", "labels": [], "entities": []}, {"text": "\u2022 We design a hashing technique to apply the adaptive convolution without a considerable increase in the number of required parameters.", "labels": [], "entities": []}, {"text": "\u2022 We show the effectiveness of our approach based on an evaluation on seven text classification benchmark datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7096614241600037}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related works and Section 3 describes the proposed methodology.", "labels": [], "entities": []}, {"text": "We present our evaluation in Section 4 and conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets and Data Preprocessing We employ seven datasets covering seven different classification tasks compiled by.", "labels": [], "entities": []}, {"text": "'AG','DBPedia' and 'Yahoo' are news, ontology, and topic classification datasets, respectively.", "labels": [], "entities": [{"text": "AG','DBPedia", "start_pos": 1, "end_pos": 13, "type": "DATASET", "confidence": 0.7619994481404623}, {"text": "topic classification", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6995492875576019}]}, {"text": "The others are sentiment classification datasets, where '.p'(polarity) in the dataset name indicates that labels are binary and '.f' (full) means that the labels refer to the number of stars.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8619730472564697}]}, {"text": "We tokenize each text using Stanford's CoreNLP ( ) after converting all uppercase letters to lowercase letters.", "labels": [], "entities": []}, {"text": "In building a vocabulary, we retain words that appear more than five times in a corpus.", "labels": [], "entities": []}, {"text": "We replace remaining words with the special 'UNK' tokens.", "labels": [], "entities": [{"text": "UNK' tokens", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.771517296632131}]}, {"text": "Baselines We select three baseline CNNs to which we apply our adaptive convolution.", "labels": [], "entities": []}, {"text": "First one is CNN, the basic form of CNNs consists of a single convolution block.", "labels": [], "entities": [{"text": "CNN", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.4686792492866516}]}, {"text": "The others are recently proposed DPCNN (Johnson and Zhang, 2017) and DenseCNN ( ) which employ multiple convolution blocks.", "labels": [], "entities": []}, {"text": "We reproduce these three models and apply adaptive convolutions to assess the efficacy of our methodology.", "labels": [], "entities": []}, {"text": "All of them are word-level CNNs.", "labels": [], "entities": []}, {"text": "We do not apply adaptive convolutions to character-level CNNs () because of their relatively poor performance compared to word-level CNNs (Johnson and Zhang, 2016).", "labels": [], "entities": []}, {"text": "We also compare the performance of our methodology with ACNN ( ).", "labels": [], "entities": [{"text": "ACNN", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.749325156211853}]}, {"text": "Similar to our approach, ACNN employs dynamically generated filters for convolutions.", "labels": [], "entities": []}, {"text": "Different from our approach, however, it generates filters with original inputs from a single subnetwork.", "labels": [], "entities": []}, {"text": "Note that ACNN is a specifically designed network architecture, so its filter generation approach cannot readily be applied to other existing CNNs.", "labels": [], "entities": []}, {"text": "Models other than CNNs, such as RNNs () and word embedding-based models (   the models where transfer learning is applied, such as ULMFiT ( to compare the capacity of models by themselves, not the effectiveness of transfer.", "labels": [], "entities": []}, {"text": "Training Details We implement all of the models with PyTorch ( framework.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8399308919906616}]}, {"text": "For all the models and datasets, we use 300 dimensional GloVe () vectors trained on 840 billion words for word embedding initialization and initialize outof-vocabulary words with Gaussian distribution with the standard deviation of 0.6.", "labels": [], "entities": [{"text": "word embedding initialization", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.7572919130325317}]}, {"text": "We do not use the text region embedding, for fair comparisons with other comparative models.", "labels": [], "entities": []}, {"text": "We optimize parameters using Adam () with initial learning rate of 0.01 and batch size of 128.", "labels": [], "entities": []}, {"text": "Gradients are clipped at 5.0 by norm.", "labels": [], "entities": [{"text": "Gradients", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9705578088760376}]}, {"text": "We use ReLU activation after convolution operations.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8360716700553894}]}, {"text": "Model-specific configurations are as follows: \u2022 CNN: We use the total of 300 filters, with 100 filters each having window size of 3,4 and 5.", "labels": [], "entities": [{"text": "CNN", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.587603747844696}]}, {"text": "\u2022 DPCNN: We use 100 filters with a size of 3, for each convolution operation.", "labels": [], "entities": [{"text": "DPCNN", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.6517444252967834}]}, {"text": "We set the depth to 11 for all the datasets except for the 'AG' dataset in which depth is set to 9.", "labels": [], "entities": [{"text": "AG' dataset", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.8337007164955139}]}, {"text": "\u2022 DenseCNN: We use 75 filters with a size of 3 for each convolution block.", "labels": [], "entities": []}, {"text": "Input texts are padded or truncated to a fixed length.", "labels": [], "entities": []}, {"text": "We set the fixed length to 300, except for the 'AG' in which 100 is used.", "labels": [], "entities": [{"text": "length", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8119741678237915}, {"text": "AG", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.968444287776947}]}, {"text": "For 'AG' dataset, we use six convolution blocks and seven for all the other datasets.", "labels": [], "entities": [{"text": "AG' dataset", "start_pos": 5, "end_pos": 16, "type": "DATASET", "confidence": 0.5921866993109385}]}, {"text": "These configurations are set on the validation set held out by 10% from the training data.", "labels": [], "entities": []}, {"text": "If not specified, the same configurations are used in all the datasets.", "labels": [], "entities": []}, {"text": "Once we fit model settings, we apply our adaptive convolution to those settings.", "labels": [], "entities": []}, {"text": "We use 600 for the context vector size (i.e. 300 for GRU hidden states).", "labels": [], "entities": []}, {"text": "In the hashed generation, we use 20 for the hash (shared) pool size and five for the number of importance parameters.", "labels": [], "entities": [{"text": "hashed generation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.8122970759868622}]}, {"text": "As shown in the table, adaptive convolutions improve all baseline CNNs in all datasets, with no exception.", "labels": [], "entities": []}, {"text": "The performance improvements are relatively small for datasets in which known performances are already nearly 100%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Test accuracies [%] on the seven text classification datasets. Results marked with * are reported in each  reference, while results marked with  \u2020 are re-printed following (Wang et al., 2018b). If not specified, results are  from our implementations. Values in the parentheses are from their reference, except for CNN whose performances  is reported in (Johnson and Zhang, 2016).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.8865814805030823}, {"text": "CNN", "start_pos": 324, "end_pos": 327, "type": "DATASET", "confidence": 0.9245505332946777}]}, {"text": " Table 4: Validation accuracies on Yahoo dataset for the  hashed generation-based models with different context  generation settings.", "labels": [], "entities": [{"text": "Yahoo dataset", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9600718319416046}]}]}