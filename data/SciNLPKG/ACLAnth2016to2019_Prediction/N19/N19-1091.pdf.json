{"title": [{"text": "Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses.", "labels": [], "entities": []}, {"text": "The interaction between a customer and the customer care representative contributes substantially to the overall customer experience.", "labels": [], "entities": []}, {"text": "Thus, it is imperative for customer care agents and chat-bots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention.", "labels": [], "entities": []}, {"text": "Our system aims at automatically transforming neutral customer care responses into courteous replies.", "labels": [], "entities": []}, {"text": "Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer.", "labels": [], "entities": [{"text": "stylistic transfer", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6958884000778198}]}, {"text": "Our technique is based on a reinforced pointer-generator model for the sequence to sequence task.", "labels": [], "entities": []}, {"text": "The model is also conditioned on a hierarchically encoded and emotionally aware conversational context.", "labels": [], "entities": []}, {"text": "We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: generic and courteous.", "labels": [], "entities": []}, {"text": "We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based.", "labels": [], "entities": []}, {"text": "Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content.", "labels": [], "entities": []}, {"text": "Experimental results also prove that our proposed approach performs better than the base-line models.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the advancement of artificial intelligence (AI) and natural language processing (NLP), automatic systems have made immense impact on human lives by assisting them in their everyday * * First two authors are jointly the first authors works.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7928164700667063}]}, {"text": "Human-computer interaction is pervasive in many applications such as chatbots, personal assistants and many more.", "labels": [], "entities": []}, {"text": "Natural language generation (NLG) component of such systems is an important aspect of every human computer interaction.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.738795816898346}]}, {"text": "Thus research in recent years have been on modulating biases, styles and control in text generation to enhance these interactions.", "labels": [], "entities": [{"text": "text generation", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7666657865047455}]}, {"text": "Customer care is an essential tool used by companies to provide guidance, assistance and in building stable customer relations.", "labels": [], "entities": []}, {"text": "The ease of access, ease in following-up and immediacy of social media has made it a strong platform for companies and applications to interact with their customers.", "labels": [], "entities": []}, {"text": "In this platform, we seethe usage of courteous and emphatic language, which is the center of our current study.", "labels": [], "entities": []}, {"text": "For the growth of any company or application it is necessary for the customer care agents to be cordial and amicable to the customer.", "labels": [], "entities": []}, {"text": "Thus along with handling queries, it is important for agents to provide customer satisfaction by greeting, empathizing, appreciating feedback, apologizing at the right time, and thus build a strong relation with the customer.", "labels": [], "entities": []}, {"text": "In, we showcase different situations in which an agent can behave courteously, thereby providing a good customer experience.", "labels": [], "entities": []}, {"text": "In this work, we focus on proposing an effective deep learning framework to enhance the existing NLG systems by converting their replies to courteous ones, by staying conversationally grounded, and emotionally aware of the user.", "labels": [], "entities": []}, {"text": "For any Natural Language Generation (NLG) module (generic or task oriented), courteous response can play an important role in keeping the user engaged with the system.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG) module", "start_pos": 8, "end_pos": 48, "type": "TASK", "confidence": 0.8272598811558315}]}, {"text": "Also, it will make the system more human-like while generating responses.", "labels": [], "entities": []}, {"text": "Inducing courteous behavior in responses can be fused with any existing NLG system to give them humanly essence and simultaneously make users more comfortable in using these systems leading to an increase in user association with the brand or product.", "labels": [], "entities": []}, {"text": "This would eventually lead to customer satisfaction with an increase in customer retention.", "labels": [], "entities": []}, {"text": "Moreover, such language conditioning shall ensure that responses are more human-like.", "labels": [], "entities": []}, {"text": "Thus, the major motivation behind this task is to create systems that are able to converse with humans efficiently and generate replies in accordance with the emotions of the customer.", "labels": [], "entities": []}, {"text": "Courteousness is a virtue of humans and to be able to make a machine behave courteously is a challenging task.", "labels": [], "entities": []}, {"text": "Unlike a generic NLG system that focuses in generating responses, our system adds courteous nature and emotional sense to the replies, thereby, making the responses interesting and engaging to the users.", "labels": [], "entities": []}, {"text": "Such systems have high applications in many areas/companies that employ chatbots to deal with the customers.", "labels": [], "entities": []}, {"text": "We thus propose a novel research direction of inducing courteous behavior in the natural language responses for the customer care domain whilst being contextually consistent.", "labels": [], "entities": []}, {"text": "The key contributions of our work are summarized as follows: (i) Creation of a high quality and a large conversational dataset, Courteously Yours Customer Care Dataset (CYCCD) prepared from the actual conversations on Twitter.", "labels": [], "entities": [{"text": "Courteously Yours Customer Care Dataset (CYCCD)", "start_pos": 128, "end_pos": 175, "type": "DATASET", "confidence": 0.640654593706131}]}, {"text": "We provide both forms of agent responses: generic and courteous.", "labels": [], "entities": []}, {"text": "(ii) Proposal of a strong benchmark model based on a context and emotionally aware reinforced pointer-generator approach which demonstrates very strong performance (both on quantitative and qualitative analyses) on established and task-specific metrics, both automatic and human evaluation based.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: In section 2, we discuss the related works.", "labels": [], "entities": []}, {"text": "In Section 3 we explain the proposed methodology followed by the dataset description in section 4.", "labels": [], "entities": []}, {"text": "Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively.", "labels": [], "entities": []}, {"text": "In section 7, we present the concluding remarks followed by future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the details of the dataset that we create for our experiments.", "labels": [], "entities": []}, {"text": "We use the data of the interactions between customers and professional customer care agents of companies on their Twitter handles.", "labels": [], "entities": []}, {"text": "We source the requisite Twitter data from the dataset made available on Kaggle by Thought vector . Tweets have labels of company names, anonymized user ids, time stamps, and response tweet ids -essential for reconstructing conversations, and nuanced analyses.", "labels": [], "entities": [{"text": "Kaggle by Thought vector", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.9019019454717636}, {"text": "reconstructing conversations", "start_pos": 208, "end_pos": 236, "type": "TASK", "confidence": 0.8998394310474396}]}, {"text": "We filter out conversations having multiple responses to a single tweet, and those starting by a tweet by a company.", "labels": [], "entities": []}, {"text": "This was done to ensure correct conversation flow and to acquire suggestion / complaint based exchanges, respectively.", "labels": [], "entities": []}, {"text": "Implementation Details: We use a vocabulary of size 30k for the task (as the range of courteous expressions is limited, and informative contents can be copied even if they are out-of-vocabulary-OOV).", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8167234063148499}]}, {"text": "We use 256 dimensional hidden states and 128 dimensional word embeddings (not pretrained).", "labels": [], "entities": []}, {"text": "We use AdaGrad as the optimizer with gradient clipping (magnitude 2).", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9344931840896606}]}, {"text": "We train with batches of size 16, and use the same size for beam search decoding.", "labels": [], "entities": [{"text": "beam search decoding", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.932481050491333}]}, {"text": "We monitor smoothened running loss on the validation set for early stopping and finding the best models for decoding.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7337910234928131}]}, {"text": "We use \u03b7 = 0.99 (similar to () for the joint loss.", "labels": [], "entities": []}, {"text": "For the reward function the values of \u03bb 1 and \u03bb 2 are 0.75 and 0.25, respectively.", "labels": [], "entities": []}, {"text": "Automatic Evaluation: For automatic evaluation, in addition to the standard metrics like BLEU (), ROUGE) and perplexity, we also use two task-specific metrics: 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9991590976715088}, {"text": "ROUGE", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9872713685035706}]}, {"text": "Content preservation (CP): We want to measure how much of the informative content from the original generic response(X) is reflected in the generated courteous response(Y ).", "labels": [], "entities": [{"text": "Content preservation (CP)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8710126399993896}]}, {"text": "We use a measure similar to ROUGE-L recall.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9917014241218567}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.7670678496360779}]}, {"text": "where LCS is the longest common subsequence.", "labels": [], "entities": [{"text": "LCS", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.905082106590271}]}, {"text": "2. Emotional accuracy (EA): To measure the consonance between the generated courteous expressions (source of emotion) and the gold, we find the cosine similarity between the MojiTalk emoji distributions of the two responses (X e and Y e ).", "labels": [], "entities": [{"text": "accuracy (EA)", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9221437722444534}]}, {"text": "Human Evaluation: In order to understand the quality of the responses we adopt human evaluation to compare the performance of different models.", "labels": [], "entities": []}, {"text": "We randomly sample 500 responses from the test set for human evaluation.", "labels": [], "entities": []}, {"text": "Given a generic response along with conversation history, three human annotators with post-graduate exposure were assigned to evaluate the courteous responses generated by the different models for the three metrics: The courteous response is grammatically correct and is free of any errors.", "labels": [], "entities": []}, {"text": "2. Content Adequacy (CA): The generated response contains the information present in the generic form of the response and there is no loss of information while adding the courteous part to the responses.", "labels": [], "entities": [{"text": "Content Adequacy (CA)", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.5329464137554168}]}, {"text": "3. Courtesy Appropriateness (CoA): The courtesy part added to the generic responses is in accordance to the conversation history.", "labels": [], "entities": [{"text": "Courtesy Appropriateness (CoA)", "start_pos": 3, "end_pos": 33, "type": "METRIC", "confidence": 0.8736983299255371}]}, {"text": "The scoring scheme for fluency and content adequacy is 0: incorrect or incomplete, 1: moderately correct, 2: correct, whereas for courtesy appropriateness the scoring scheme is -1: inappropriate, 0: non-courteous, 1: appropriate, respectively.", "labels": [], "entities": []}, {"text": "We computed the Fleiss' kappa for the above metrics to measure inter-rater consistency.", "labels": [], "entities": [{"text": "Fleiss'", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9359388947486877}]}, {"text": "The kappa score for fluency is 0.75 and courtesy appropriateness is 0.77 indicating \"substantial agreement\" and the score is 0.67 for content adequacy denoting \"considerable agreement\".", "labels": [], "entities": [{"text": "kappa score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.931183785200119}]}], "tableCaptions": [{"text": " Table 3: Results of various Models; P: Pointer Genera- tor Model; EE: Emotional embedding", "labels": [], "entities": [{"text": "EE", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9670647978782654}]}, {"text": " Table 4: Human evaluation results for Fluency, Con- tent Adequacy and Courtesy Appropriateness (All val- ues are in percentages.)", "labels": [], "entities": [{"text": "Fluency", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9884636998176575}, {"text": "Con- tent Adequacy", "start_pos": 48, "end_pos": 66, "type": "METRIC", "confidence": 0.8544007986783981}, {"text": "Courtesy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9339907169342041}, {"text": "Appropriateness", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.6367641687393188}]}]}