{"title": [{"text": "Investigating Speech Recognition for Improving Predictive AAC", "labels": [], "entities": [{"text": "Investigating Speech Recognition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7765664458274841}, {"text": "Improving Predictive AAC", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6923289100329081}]}], "abstractContent": [{"text": "Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices.", "labels": [], "entities": []}, {"text": "This is particularly important for real-time person-to-person conversations.", "labels": [], "entities": []}, {"text": "We investigate whether performing speech recognition on the speaking-side of a conversation can improve language model based predictions.", "labels": [], "entities": [{"text": "speech recognition on the speaking-side of a conversation", "start_pos": 34, "end_pos": 91, "type": "TASK", "confidence": 0.799577035009861}]}, {"text": "We compare the accuracy of three plausible microphone deployment options and the accuracy of two commercial speech recognition engines (Google and IBM Watson).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9992119073867798}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9978322386741638}]}, {"text": "We found that despite recognition word error rates of 7-16%, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.", "labels": [], "entities": []}], "introductionContent": [{"text": "People who are non-verbal often use some form of Augmentative and Alternative Communication (AAC).", "labels": [], "entities": [{"text": "Augmentative and Alternative Communication (AAC)", "start_pos": 49, "end_pos": 97, "type": "TASK", "confidence": 0.6768903689725059}]}, {"text": "Common forms of speaking disorders include stuttering, cluttering, apraxia, dysarthria, aphasia, Parkinson's disease, amyotrophic lateral sclerosis (ALS), or cerebral palsy.", "labels": [], "entities": []}, {"text": "An AAC device may let a user select letters, words, and phrases from its interface and a communication partner can read the text or hear it via text-to-speech.", "labels": [], "entities": []}, {"text": "The rate at which an AAC user can enter text is typically slow (often less than 10 words-per-minute) (.", "labels": [], "entities": []}, {"text": "That is why predictive AAC devices normally use a language model to try and make suggestions of likely upcoming text.", "labels": [], "entities": [{"text": "predictive AAC", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.6460690498352051}]}, {"text": "These predictions are usually made based solely on the text entered by the AAC user.", "labels": [], "entities": []}, {"text": "They typically ignore the two-way nature of conversation which can offer many contextual clues.", "labels": [], "entities": []}, {"text": "In this paper, first we investigate how to record and recognize the speech of a partner communicating with the AAC user.", "labels": [], "entities": [{"text": "recognize the speech of a partner communicating with the AAC user", "start_pos": 54, "end_pos": 119, "type": "TASK", "confidence": 0.550433646548878}]}, {"text": "Then we investigate if speech recognition on partner speech improves two-sided conversational language modeling.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.6964162290096283}, {"text": "conversational language modeling", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.6433621545632681}]}], "datasetContent": [{"text": "We performed speech recognition using two commercially available speech recognizers, Google Cloud Speech-to-Text and IBM Watson Speechto-Text.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.788920521736145}]}, {"text": "We performed speech recognition on audio from each of the three different microphones.: Word Error Rate (WER %) using different microphones and speech recognizers.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7401170134544373}, {"text": "Word Error Rate (WER %)", "start_pos": 88, "end_pos": 111, "type": "METRIC", "confidence": 0.8573351403077444}]}, {"text": "Results are formatted as mean \u00b1 95% bootstrap confidence intervals.", "labels": [], "entities": []}, {"text": "We now investigate how to use language models to better predict turns in our dialogue collection.", "labels": [], "entities": []}, {"text": "Recall we recorded both sides of 196 of the dialogues from our set of 1,154 dialogues.", "labels": [], "entities": []}, {"text": "After dropping  dialogues with numbers, we arrived at a test set of 160 dialogues with audio data.", "labels": [], "entities": []}, {"text": "We created text-only training and development sets from the remaining 958 dialogues.", "labels": [], "entities": []}, {"text": "From these dialogues, we dropped 128 that contained numbers.", "labels": [], "entities": []}, {"text": "We randomly selected 160 from the remaining dialogues as a development set and 670 as a training set.", "labels": [], "entities": []}, {"text": "Our language modeling experiments used a vocabulary of 35 K words.", "labels": [], "entities": []}, {"text": "The vocabulary consisted of the most frequent known English words occurring in 50 M words of sentences parsed from Twitter.", "labels": [], "entities": []}, {"text": "Any words not in this vocabulary were mapped to an unknown word token.", "labels": [], "entities": []}, {"text": "We converted text to lowercase and removed punctuation aside from apostrophe.", "labels": [], "entities": []}, {"text": "Throughout, we report the perword perplexity of our test set (160 dialogues, 960 turns, 7.1 K words).", "labels": [], "entities": []}, {"text": "We excluded the sentence end pseudo-word from our calculations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Word Error Rate (WER %) using different mi- crophones and speech recognizers. Results are format- ted as mean \u00b1 95% bootstrap confidence intervals.", "labels": [], "entities": [{"text": "Word Error Rate (WER %)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.802892342209816}, {"text": "speech recognizers", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7204029858112335}]}, {"text": " Table 4: Perplexities with added features. We reset the  RNNLM between each sentence or after each dialogue.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.49580177664756775}]}, {"text": " Table 5: Perplexity of models trained on two-sided di- alogues and mixtures of dialogue and twitter models.", "labels": [], "entities": []}]}