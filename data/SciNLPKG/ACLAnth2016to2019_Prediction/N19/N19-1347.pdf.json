{"title": [{"text": "Learning Hierarchical Discourse-level Structure for Fake News Detection", "labels": [], "entities": [{"text": "Fake News Detection", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7378721038500468}]}], "abstractContent": [{"text": "On the one hand, nowadays, fake news articles are easily propagated through various on-line media platforms and have become a grand threat to the trustworthiness of information.", "labels": [], "entities": []}, {"text": "On the other hand, our understanding of the language of fake news is still minimal.", "labels": [], "entities": []}, {"text": "Incorporating hierarchical discourse-level structure of fake and real news articles is one crucial step toward a better understanding of how these articles are structured.", "labels": [], "entities": []}, {"text": "Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges.", "labels": [], "entities": [{"text": "fake news detection domain", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7255488783121109}]}, {"text": "First, existing methods for capturing discourse-level structure rely on annotated corpora which are not available for fake news datasets.", "labels": [], "entities": []}, {"text": "Second, how to extract out useful information from such discovered structures is another challenge.", "labels": [], "entities": []}, {"text": "To address these challenges , we propose Hierarchical Discourse-level Structure for Fake news detection.", "labels": [], "entities": [{"text": "Fake news detection", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8555994232495626}]}, {"text": "HDSF learns and constructs a discourse-level structure for fake/real news articles in an automated and data-driven manner.", "labels": [], "entities": []}, {"text": "Moreover, we identify insightful structure-related properties , which can explain the discovered structures and boost our understating of fake news.", "labels": [], "entities": []}, {"text": "Conducted experiments show the effectiveness of the proposed approach.", "labels": [], "entities": []}, {"text": "Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse-level structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work, we focus on detecting fake news articles (hereafter referred to as documents) based on their contents.", "labels": [], "entities": [{"text": "detecting fake news articles (hereafter referred to as documents)", "start_pos": 26, "end_pos": 91, "type": "TASK", "confidence": 0.8107333183288574}]}, {"text": "Many existing linguistic approaches for fake news detection) overlook a crucial linguistic aspect of fake/real news documents i.e., the hierarchical discourselevel structure.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.6690649191538492}]}, {"text": "Usually, in a document, discourse units (e.g., sentences) are organized in a hierarchical structure e.g., a tree.", "labels": [], "entities": []}, {"text": "The importance of considering the hierarchical discourse-level structure for fake news detection is three-fold.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6969991425673167}]}, {"text": "First, previous studies () explored discourse-level structure in fake news detection and discovered that the way two discourse units of a document are connected could be quite revealing and insightful about its truthfulness.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6787516673405966}]}, {"text": "For instance, applied Rhetorical Structure Theory (RST) ( and noted that fake stories lack \"evidence\" as a defined inter-discourse relation.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.830250104268392}]}, {"text": "Second, fake news is typically produced by connecting disjoint pieces of news and unlike well-established journalism (e.g., New York Times) fake news production lacks a meticulous editorial board.", "labels": [], "entities": []}, {"text": "Therefore, by incorporating the hierarchical discourse-level structure, we can investigate the coherence of fake/real news documents (we will show this later).", "labels": [], "entities": []}, {"text": "Third, a substantial number of studies have shown that using hierarchical structures yields a better document representation in various downstream tasks whose predictions depend on the entire text ().", "labels": [], "entities": []}, {"text": "Since typically fake news detection is considered as a classification problem based on the entire text, applying discourse analysis has the potential to advance fake news detection (this will be verified later).", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.656434049208959}, {"text": "fake news detection", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.6623947421709696}]}, {"text": "On the other hand, incorporating the hierarchical structure at the discourse level for fake news detection faces tremendous challenges.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7192325592041016}]}, {"text": "First, many existing methods incorporating structural discourse () (not for fake news detection though) rely on annotated corpora such as Penn Discourse.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6858816842238108}, {"text": "Penn Discourse", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.9812926948070526}]}, {"text": "Constructing and annotating such corpora is an arduous and costly process.", "labels": [], "entities": []}, {"text": "Incor-where G is a non-linear activation function, Wis some weight matrix, b is a bias vector, and \u2299 denotes the dot product operator.", "labels": [], "entities": []}, {"text": "Further, since we need a root node in a dependency tree, we compute the probability of a sentence s j being the root node, denoted as r j , as follows.", "labels": [], "entities": []}, {"text": "where u j [y] is the y-th element of vector u j . Similarly, we calculate the root probabilities for all sentences and obtain the array of root probabilities denoted as r = {r 1 , r 2 , \u00b7 \u00b7 \u00b7 , r k } where 0 \u2264 r j \u2208 r \u2264 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify the performance of the proposed framework HDSF, we conduct a set of experiments.", "labels": [], "entities": []}, {"text": "We seek to answer the following research questions: 1.", "labels": [], "entities": []}, {"text": "How does the proposed framework perform on fake news detection?", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6568040251731873}]}, {"text": "2. How do the defined structure-related properties describe the fake and real news documents?", "labels": [], "entities": []}, {"text": "In this section, we first describe the datasets followed by presenting the experimental settings.", "labels": [], "entities": []}, {"text": "Afterward, we evaluate the performance of HDSF compared to several representative baselines.", "labels": [], "entities": []}, {"text": "Finally, we present a structural analysis of the fake/real news documents.", "labels": [], "entities": []}, {"text": "We utilize five available fake news datasets in this study.", "labels": [], "entities": []}, {"text": "The first two datasets are collected by and include online articles whose veracities have been identified by experts in BuzzFeed and PolitiFact 2 . For the next two datasets, we utilize two available online fake news datasets provided by kaggle.com 3 4 . Finally, we include the dataset constructed and shared by McIntire . Since the proposed framework HDSF is a generalpurpose framework investigating discourse-level structures of fake/real news documents based on their textual contents, we do not restrict HDSF to a particular source of data and therefore combine all datasets.", "labels": [], "entities": [{"text": "McIntire", "start_pos": 313, "end_pos": 321, "type": "DATASET", "confidence": 0.9454432725906372}]}, {"text": "Similar to the previous work (, we balance the dataset to avoid a trivial solution as well as ensuring a fair performance comparison.", "labels": [], "entities": []}, {"text": "In total, we have 3360 fake and 3360 real documents.", "labels": [], "entities": []}, {"text": "First, we pre-process the documents by removing numbers, non-English characters, stop-words (e.g., 'with'), and converting all characters to lowercase.", "labels": [], "entities": []}, {"text": "We randomly select 134 documents as the development set, (67 from each class) and 134 documents (67 from each class) as the test set.", "labels": [], "entities": []}, {"text": "The remaining 6452 documents are used for training.", "labels": [], "entities": []}, {"text": "The development set is used for tuning the hyper-parameters.", "labels": [], "entities": []}, {"text": "We initialize the word embeddings from the Google news pre-trained word2vec embeddings ( . LeakyReLU () is used as the nonlinear activation function and the number of hidden units in the BLSTM network is set to 100.", "labels": [], "entities": [{"text": "Google news pre-trained word2vec embeddings", "start_pos": 43, "end_pos": 86, "type": "DATASET", "confidence": 0.8735890865325928}, {"text": "LeakyReLU", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.9077605605125427}, {"text": "BLSTM network", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.8893149495124817}]}, {"text": "Each simulation is run for 200 steps with a random mini-batch size of 40 documents.", "labels": [], "entities": []}, {"text": "The learning rate starts at 0.01 with the decay rate of 0.9 after every 50 steps.", "labels": [], "entities": [{"text": "decay rate", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.947259783744812}]}, {"text": "We use the ADAM optimizer () to optimize the parameters.", "labels": [], "entities": []}, {"text": "The PyTorch package 7 is utilized for the implementation and the code and data are publicly available in https://github.com/ hamidkarimi/HDSF.", "labels": [], "entities": []}], "tableCaptions": []}