{"title": [], "abstractContent": [{"text": "Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space.", "labels": [], "entities": []}, {"text": "Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity.", "labels": [], "entities": [{"text": "word-level mapping", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7310788333415985}]}, {"text": "We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus.", "labels": [], "entities": []}, {"text": "We also implement cross-lingual mapping of deep contextualized word embed-dings using parallel sentences with word alignments.", "labels": [], "entities": []}, {"text": "In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval.", "labels": [], "entities": [{"text": "sentence translation retrieval", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.7867163519064585}]}, {"text": "Furthermore , the sentence-level transformation could be used for word-level mapping without loss in word translation quality.", "labels": [], "entities": [{"text": "word-level mapping", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7230341732501984}, {"text": "word translation", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7103181928396225}]}], "introductionContent": [{"text": "Cross-lingual word vector models aim to embed words from multiple languages into a shared vector space to enable cross-lingual transfer and dictionary expansion (.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7334756255149841}, {"text": "dictionary expansion", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7076247781515121}]}, {"text": "One of the most common and effective approaches for obtaining bilingual word embeddings is by fitting a linear transformation matrix on the entries of a bilingual seed dictionary (.", "labels": [], "entities": []}, {"text": "This approach is versatile and scalable: multilingual embeddings can be obtained by mapping the vector spaces of multiple languages into a shared target language, typically English.", "labels": [], "entities": []}, {"text": "In addition, imposing an orthogonality constraint on the mapping ensures that the original pair-wise distances are preserved after the transformation and results in better word translation retrieval (.", "labels": [], "entities": [{"text": "word translation retrieval", "start_pos": 172, "end_pos": 198, "type": "TASK", "confidence": 0.7776512702306112}]}, {"text": "While word vector spaces tend to be globally consistent across language variations , individual words like homographs with unrelated senses (e.g. 'bank', 'coast') and phrasal verbs ('stand up', 'stand out') are likely to behave less consistently in multilingual vector spaces due to their different usage distributions.", "labels": [], "entities": []}, {"text": "Consequently, using such words in the alignment dictionary may result in suboptimal overall mapping.", "labels": [], "entities": []}, {"text": "We propose two approaches to counteract this effect by incorporating sentential context in the mapping process without explicit word sense disambiguation or additional linguistic resources.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.6967071692148844}]}, {"text": "The first approach is based on the recently proposed contextualized embeddings from language models,.", "labels": [], "entities": []}, {"text": "Using a parallel corpus with word-alignments, we extract contextualized embeddings to construct a contextaware dictionary for mapping.", "labels": [], "entities": []}, {"text": "The second approach is to learn a transformation between sentence embeddings rather than individual word embeddings.", "labels": [], "entities": []}, {"text": "Since these embeddings include context that spans full sentences, we surmise that a mapping learned at this level would be more robust to individual word misalignments.", "labels": [], "entities": []}, {"text": "We used a constrained set of parallel sentences ranging from one hundred to a million sentences for alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.9634130597114563}]}, {"text": "We then evaluated the resultant mappings on sentence translation retrieval among English, Spanish, and German as test languages.", "labels": [], "entities": [{"text": "sentence translation retrieval", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8209186395009359}]}, {"text": "Our results show that context-aware mappings significantly outperform context-independent crosslingual word mappings using reasonably-sized parallel corpora, particularly when using contextualized word embeddings.", "labels": [], "entities": []}, {"text": "In addition, when averaging static word embeddings, the sentence-level mapping can still be used for word-level mapping without loss in word translation quality.", "labels": [], "entities": [{"text": "word-level mapping", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7367262244224548}, {"text": "word translation", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.6958809047937393}]}], "datasetContent": [{"text": "We used skip-gram with subword information, i.e FastText (, for the static word embeddings, and ELMo for contextualized word embeddings.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.8423565030097961}]}, {"text": "Sentence embeddings were calculated from ELMo as the arithmetic average of the contextualized embeddings 2 . For FastText, we applied weighted averaging using smooth inverse frequency (, which works better for sentence similarity compared to other averaging schemes ).", "labels": [], "entities": [{"text": "ELMo", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6282185316085815}]}, {"text": "We evaluated the cross-lingual mapping approaches on sentence translation retrieval, where we calculate the accuracy of retrieving the correct translation from the target side of a test parallel corpus using nearest neighbor search with cosine similarity.", "labels": [], "entities": [{"text": "sentence translation retrieval", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.827052116394043}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9993308782577515}]}, {"text": "To assess the minimum bilingual data Since we use vector averaging, it doesn't matter whether we apply the learned transformation to the word embeddings before averaging, or to the sentence embeddings after averaging.", "labels": [], "entities": []}, {"text": "We found that using the arithmetic average for ELMo yields better results than weighted averaging.", "labels": [], "entities": []}, {"text": "requirements of each approach and measure how the various models respond to additional data, we split the training parallel corpus into smaller subsets of increasing sizes, starting from 100 to a million sentences (we double the size at each step).", "labels": [], "entities": []}, {"text": "Data splits and evaluation scripts are available at https://github.com/h-aldarmaki/ sent_translation_retrieval.", "labels": [], "entities": []}, {"text": "Cross-lingual word embeddings are typically evaluated in word-translation retrieval: the precision of correctly retrieving a translation from the vocabulary of another language.", "labels": [], "entities": [{"text": "Cross-lingual word embeddings", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5999820828437805}, {"text": "word-translation retrieval", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7140990346670151}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9978669285774231}]}, {"text": "Since this is a context-free task, we evaluated the performance of static word embeddings, FastText, using word vs. sentence mapping (with 1M parallel sentences).", "labels": [], "entities": []}, {"text": "The transformation matrix learned at the sentence level is used to transform the word embeddings.", "labels": [], "entities": []}, {"text": "We used the dictionaries from ().", "labels": [], "entities": []}, {"text": "We also evaluated on the SemEval'17 cross-lingual word similarity task, which is measured using the average of Pearson and Spearman correlation coefficients against human judgements.", "labels": [], "entities": [{"text": "SemEval'17 cross-lingual word similarity", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.8187673985958099}]}, {"text": "As shown in, the mapping learned at the sentence-level yields equivalent performance to word-level mapping.", "labels": [], "entities": []}, {"text": "While word-level mapping was slightly better in translating from source languages (German and Spanish) to English, the sentence-level mapping was better when translating between the source languages.", "labels": [], "entities": []}, {"text": "In the word similarity task, sentence-level mappings performed slightly better in two out of the three cases.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8302239378293356}]}, {"text": "Overall, the performance of both models are comparable, which indicates that a single transformation matrix learned at the sentence-level can be used for both word and sentence-level tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word translation precision at k (%) using k  nearest neighbor search, with k \u2208 {1, 5}.", "labels": [], "entities": [{"text": "Word translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7613612115383148}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8750683069229126}]}, {"text": " Table 2: The harmonic mean of Pearson and Spearman  correlations with human judgment on the SemEval'17  cross-lingual word similarity task.", "labels": [], "entities": [{"text": "SemEval'17  cross-lingual word similarity task", "start_pos": 93, "end_pos": 139, "type": "TASK", "confidence": 0.8588811755180359}]}]}