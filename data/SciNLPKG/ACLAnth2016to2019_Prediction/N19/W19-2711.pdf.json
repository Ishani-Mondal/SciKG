{"title": [{"text": "Towards the Data-driven System for Rhetorical Parsing of Russian Texts", "labels": [], "entities": [{"text": "Rhetorical Parsing of Russian Texts", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.8599974989891053}]}], "abstractContent": [{"text": "Results of the first experimental evaluation of machine learning models trained on Ru-RSTreebank-first Russian corpus annotated within RST framework-are presented.", "labels": [], "entities": [{"text": "Ru-RSTreebank-first Russian corpus annotated within RST framework-are", "start_pos": 83, "end_pos": 152, "type": "DATASET", "confidence": 0.8445907746042524}]}, {"text": "Various lexical, quantitative, morphological, and semantic features were used.", "labels": [], "entities": []}, {"text": "In rhetorical relation classification, ensemble of CatBoost model with selected features and a linear SVM model provides the best score (macro F 1 = 54.67 \u00b1 0.38).", "labels": [], "entities": [{"text": "rhetorical relation classification", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.9414381782213846}, {"text": "macro F 1", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.7395889361699423}]}, {"text": "We discover that most of the important features for rhetorical relation classification are related to discourse connectives derived from the connectives lexicon for Rus-sian and from other sources.", "labels": [], "entities": [{"text": "rhetorical relation classification", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.8905664285024008}]}], "introductionContent": [{"text": "One of the widely used discourse models of text is the Rhetorical Structure Theory (RST) (.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7253747582435608}]}, {"text": "It represents a text as a constituency tree containing discourse (rhetorical) relations between text segments -discourse units (DUs).", "labels": [], "entities": []}, {"text": "These units can play different roles inside a relation: nuclei contain more important information, while satellites give supplementary information.", "labels": [], "entities": []}, {"text": "The leaves of the tree are so called elementary discourse units (EDUs), they usually are represented as clauses.", "labels": [], "entities": []}, {"text": "Discourse units of different levels are combined by the same set of relations.", "labels": [], "entities": []}, {"text": "The goal of our work is the development of a data-driven system for rhetorical parsing of Russian texts.", "labels": [], "entities": [{"text": "rhetorical parsing of Russian texts", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.9406262278556824}]}, {"text": "For training, we use recently released Ru-RSTreebank corpus ( . In this paper, we describe the pipeline of the parser, present the developed featureset for relation classification task, and present the results of the first experimental evaluation of machine learning models trained on Ru-RSTreebank.", "labels": [], "entities": [{"text": "Ru-RSTreebank corpus", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.808764785528183}, {"text": "relation classification task", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.8963890473047892}]}, {"text": "Special attention is paid to the importance of discourse connectives.", "labels": [], "entities": []}, {"text": "Discourse connectives are clues signalling that there is a definite relation between two DUs, such as \"in consequence of\" for \"Effect\" or \"because of\" for \"Cause\".", "labels": [], "entities": []}, {"text": "Some of them are functional words (primary connectives), the rest of them, secondary connectives, are less grammaticalized, but also should be presented in exhaustive lexicons of connectives.", "labels": [], "entities": []}, {"text": "We find that these cue phrases are informative features for rhetorical relation classification.", "labels": [], "entities": [{"text": "rhetorical relation classification", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.9081709980964661}]}], "datasetContent": [{"text": "For experiments, we excluded \"Elaboration\" and \"Joint\" relations, since although they are the most common relations, they are also not very informative.", "labels": [], "entities": []}, {"text": "We decided to focus on more specialized relation types.", "labels": [], "entities": []}, {"text": "We also excluded \"Same-unit\", since it was used in the annotation only for utility purposes to mark discontinuous EDUs.", "labels": [], "entities": []}, {"text": "Except aforementioned ones, we took the first 11 most representative classes, for which the dataset contains at least 320 examples.", "labels": [], "entities": []}, {"text": "We selected 8 mono-nuclear relations (\"Cause\", \"Preparation\", \"Condition\", \"Purpose\", \"Attribution\", \"Evidence\", \"Evaluation\", \"Background\") and 3 multi-nuclear relations (\"Contrast\", \"Sequence\", \"Comparison\").", "labels": [], "entities": [{"text": "Preparation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8497508764266968}]}, {"text": "The dataset for experimental evaluation contains 6,790 examples.", "labels": [], "entities": []}, {"text": "We note that the distribution of the classes is skewed.", "labels": [], "entities": []}, {"text": "Before feature extraction, we performed the following preprocessing: tokenization, lemmatization, part-of-speech tagging, and morphological analysis using MyStem tool.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7262349426746368}, {"text": "part-of-speech tagging", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.6969118863344193}]}, {"text": "The hyperparameters of our models are tuned using randomized search and overfitting detection tools builtin gradient boosting packages.", "labels": [], "entities": []}, {"text": "The evaluation scores are obtained using 5-fold cross-validation procedure with macro-averaging.", "labels": [], "entities": []}, {"text": "The results for distinguishing \"SatelliteNucleus\", \"Nucleus-Satellite\", and \"NucleusNucleus\" types of relations are presented in.", "labels": [], "entities": []}, {"text": "The experiment shows that the CatBoost model outperforms linear SVM and logistic regression classifiers.", "labels": [], "entities": []}, {"text": "iments with models for rhetorical relation classification.", "labels": [], "entities": [{"text": "rhetorical relation classification", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.9370094736417135}]}, {"text": "The results show that GBT models strongly outperform other methods.", "labels": [], "entities": [{"text": "GBT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8027035593986511}]}, {"text": "Also, we observe that training on the features selected by L1-regularized logistic regression reduces the variance of GBT models.", "labels": [], "entities": []}, {"text": "Ensembles of GBT models with selected features and a linear SVM model own the best score.", "labels": [], "entities": []}, {"text": "We should note that the qualitative performances of ensembles with LightGBM and CatBoost are almost the same, however, the computational performance of the latter is significantly better.", "labels": [], "entities": [{"text": "LightGBM", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9442657828330994}, {"text": "CatBoost", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.911525547504425}]}, {"text": "Therefore, we used CatBoost model for the assessment of the feature importance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of nuclear-satellite classification  models.", "labels": [], "entities": [{"text": "nuclear-satellite classification", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7351751178503036}]}, {"text": " Table  1. The experiment shows that the CatBoost model  outperforms linear SVM and logistic regression  classifiers.", "labels": [], "entities": []}, {"text": " Table 2: Performance of rhetorical relation classifica- tion models.", "labels": [], "entities": [{"text": "rhetorical relation classifica- tion", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.8700490951538086}]}]}