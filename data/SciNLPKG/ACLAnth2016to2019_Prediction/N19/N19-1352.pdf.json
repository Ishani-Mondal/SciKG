{"title": [{"text": "How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7349487692117691}]}], "abstractContent": [{"text": "With the rapid development in deep learning , deep neural networks have been widely adopted in many real-life natural language applications.", "labels": [], "entities": []}, {"text": "Under deep neural networks, a pre-defined vocabulary is required to vectorize text inputs.", "labels": [], "entities": []}, {"text": "The canonical approach to select pre-defined vocabulary is based on the word frequency , where a threshold is selected to cutoff the long tail distribution.", "labels": [], "entities": []}, {"text": "However, we observed that such a simple approach could easily lead to under-sized vocabulary or over-sized vocabulary issues.", "labels": [], "entities": []}, {"text": "Therefore, we are interested in understanding how the end-task classification accuracy is related to the vocabulary size and what is the minimum required vocabulary size to achieve a specific performance.", "labels": [], "entities": [{"text": "end-task classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.5619141161441803}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8394510746002197}]}, {"text": "In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to perform vocabulary selection, which can intelligently select the subset of the vocabulary to achieve the required performance.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7030311971902847}]}, {"text": "To evaluate different algorithms on the newly proposed vocabulary selection problem, we propose two new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X% Accuracy Drop.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.834063321352005}]}, {"text": "Through extensive experiments on various NLP classification tasks, our varia-tional framework is shown to significantly out-perform the frequency-based and other selection baselines on these metrics.", "labels": [], "entities": [{"text": "NLP classification tasks", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.9109919468561808}]}], "introductionContent": [{"text": "Over the past decade, deep neural networks have become arguably the most popular model choice fora vast number of natural language processing (NLP) tasks and have constantly been delivering state-of-the-art results.", "labels": [], "entities": []}, {"text": "Because neural network models assume continuous data, to apply a neural network on any text data, the first step is to  vectorize the discrete text input with a word embedding matrix through look-up operation, which in turn assumes a pre-defined vocabulary set.", "labels": [], "entities": []}, {"text": "For many NLP tasks, the vocabulary size can easily go up to the order of tens of thousands, which potentially makes the word embedding the largest portion of the trainable parameters.", "labels": [], "entities": []}, {"text": "For example, a document classification task like AG-news ( can include up to 60K unique words, with the embedding matrix accounting for 97.6% of the trainable parameters, which leads to under-representation of the neural networks' own parameters.", "labels": [], "entities": [{"text": "document classification task", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7707671523094177}]}, {"text": "Intuitively, using the full or very large vocabulary are neither economical, as it limits model applicability on computation-or memoryconstrained scenarios ( , nor necessary, as many words may contribute little to the end task and could have been safely removed from the vocabulary.", "labels": [], "entities": []}, {"text": "Therefore, how to select the best vocabulary is a problem of both theoretical and practical interests.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, this vocabulary selection problem is largely under-addressed in the literature: The de facto standard practice is to do frequency-based cutoff (, and only retain the words more frequent than a certain threshold.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6779822707176208}]}, {"text": "Although this simple heuristic has demonstrated strong empirical performance, its task-agnostic nature implies that likely it is not the optimal strategy for many tasks (or any task).", "labels": [], "entities": []}, {"text": "Task-aware vocabulary selection strategies and a systematic comparison of different strategies are still lacking.", "labels": [], "entities": [{"text": "Task-aware vocabulary selection", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6129787464936575}]}, {"text": "In this work, we present the first systematic study of the vocabulary selection problem.", "labels": [], "entities": [{"text": "vocabulary selection problem", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.8934797247250875}]}, {"text": "Our study will be based on text classification tasks, abroad family of NLP tasks including document classification (DC), natural language inference (NLI), natural language understanding in dialog systems (NLU), etc.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.8025561173756918}, {"text": "document classification (DC)", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.8168643772602081}, {"text": "natural language understanding in dialog systems (NLU)", "start_pos": 155, "end_pos": 209, "type": "TASK", "confidence": 0.8046379254923927}]}, {"text": "Specifically, we aim to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "How important a role does the vocabulary selection algorithm play in text classification?", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8116661906242371}, {"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8152063190937042}]}, {"text": "2. How to dramatically reduce the vocabulary size while retaining the accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.999091386795044}]}, {"text": "The rest of the paper is organized as follows: We first formally define the vocabulary selection problem (subsection 2.1) and present a quantitative study on classification accuracy with different vocabulary selections to showcase its importance in the end task (subsection 2.2).", "labels": [], "entities": [{"text": "vocabulary selection problem", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.8147857387860616}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8363430500030518}]}, {"text": "We also propose two new metrics for evaluating the performance of vocabulary selection in text classification tasks (subsection 2.3).", "labels": [], "entities": [{"text": "vocabulary selection in text classification tasks", "start_pos": 66, "end_pos": 115, "type": "TASK", "confidence": 0.7241452137629191}]}, {"text": "We then propose a novel, task-aware vocabulary selection algorithm called Varitional Vocabulary Dropout (VVD) (section 3) which draws on the idea of variational dropout (: If we learn a dropout probability p w for each given word win the vocabulary V during the model training on a given task, the learned dropout probabilities p w will imply the importance of word w to the end task and can, therefore, be leveraged for vocabulary selection.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 421, "end_pos": 441, "type": "TASK", "confidence": 0.7970166802406311}]}, {"text": "We propose to infer the latent dropout probabilities under a Bayesian inference framework.", "labels": [], "entities": []}, {"text": "During test time, we select the sub vocabulary\u02c6Vvocabulary\u02c6 vocabulary\u02c6V by only retaining words with dropout probability lower than a certain threshold.", "labels": [], "entities": []}, {"text": "For any words deselected using VVD, we will simply regard them as a special token with null vector representation [0, 0, \u00b7 \u00b7 \u00b7 , 0].", "labels": [], "entities": []}, {"text": "Please note that our proposed algorithm needs to re-train a word embedding matrix, thus it is tangential to the research of pre-trained word embedding like Word2Vec ( or Glove) though we can use them to initialize our embedding.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.9523560404777527}]}, {"text": "We conduct comprehensive experiments to evaluate the performance of VVD (section 4) on different end classification tasks.", "labels": [], "entities": [{"text": "VVD", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.7061397433280945}, {"text": "end classification tasks", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.7769198020299276}]}, {"text": "Specifically, we compare against an array of strong baseline selection algorithms, including the frequency-based algorithm (, TF-IDF algorithm (, and structure lasso algorithm, and demonstrate that it can consistently outperform these competing algorithms by a remarkable margin.", "labels": [], "entities": []}, {"text": "To show that the conclusions are widely held, our evaluation is based on a wide range of text classification tasks and datasets with different neural networks including Convolutional Neural Network (CNN), Bi-directional Long-Short Term Memory (BiLSTM) ( and Enhanced LSTM (ESIM).", "labels": [], "entities": [{"text": "text classification", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7077841013669968}]}, {"text": "In summary, our contributions are three-fold: 1.", "labels": [], "entities": []}, {"text": "We formally define the vocabulary selection problem, demonstrate its importance, and propose new evaluation metrics for vocabulary selection in text classification tasks.", "labels": [], "entities": [{"text": "vocabulary selection problem", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.8759341438611349}, {"text": "vocabulary selection", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.7686946392059326}, {"text": "text classification tasks", "start_pos": 144, "end_pos": 169, "type": "TASK", "confidence": 0.7740169564882914}]}, {"text": "2. We propose a novel vocabulary selection algorithm based on variational dropout by re-formulating text classification under the Bayesian inference framework.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7974697947502136}, {"text": "text classification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7310305386781693}]}, {"text": "The code will be released in Github 1 . 3. We conduct comprehensive experiments to demonstrate the superiority of the proposed vocabulary selection algorithm over a number of strong baselines.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.8244761526584625}]}], "datasetContent": [{"text": "In order to evaluate how well a given selection algorithm A performs, we propose evaluation metrics as depicted in by quantitatively studying its characteristic accuracy-vocab curve.", "labels": [], "entities": [{"text": "accuracy-vocab", "start_pos": 161, "end_pos": 175, "type": "METRIC", "confidence": 0.9949120879173279}]}, {"text": "These metrics namely Area Under Curve (AUC) and Vocab@-X% separately measure the vocabulary selection performance globally and locally.", "labels": [], "entities": [{"text": "Area Under Curve (AUC)", "start_pos": 21, "end_pos": 43, "type": "METRIC", "confidence": 0.7612655659516653}]}, {"text": "Specifically, AUC computes enclosed area by the curve, which gives an overview of how well the vocabulary selection algorithm performs.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.8116936385631561}]}, {"text": "In comparison, Vocab@-X% computes the minimum vocabulary size required if X% performance drop is allowed, which straightforwardly represents how large vocabulary is required to achieve a given accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9882404804229736}]}, {"text": "For the local evaluation metric, we mainly consider Vocab@-3% and Vocab@-5%.", "labels": [], "entities": []}, {"text": "However, we observe that directly computing AUC lays too much emphasis on the large-vocabulary region, thus unable to represent an algorithm's selection capability under the low-vocabulary conditions.", "labels": [], "entities": []}, {"text": "Therefore, we propose to take the logarithm of the vocabulary size and then compute the normalized enclosed area by: It is worth noting that Vocab@-X% takes value from range with smaller values indicate better performance.", "labels": [], "entities": []}, {"text": "Since AUC is normalized by Acc(V), it takes value from range [0, 1] regardless of the classification error.", "labels": [], "entities": [{"text": "AUC", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.7733913660049438}, {"text": "Acc(V)", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9370882511138916}]}, {"text": "We compare the proposed vocabulary selection algorithm against several strong baselines on a wide range of text classification tasks and datasets.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8468421995639801}, {"text": "text classification tasks", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.78766397635142}]}, {"text": "The main datasets we are using are listed in Table 2, which provides an overview of its description and capacities.", "labels": [], "entities": []}, {"text": "Specifically, we follow () to pre-process the document classification datasets, natural language understanding dataset and natural language inference dataset.", "labels": [], "entities": []}, {"text": "We exactly replicate their experiment settings to make our method comparable with theirs.", "labels": [], "entities": []}, {"text": "Our models is implemented with TensorFlow (.", "labels": [], "entities": []}, {"text": "In order to evaluate the generalization ability of VVD selection algorithm in deep learning architectures, we study its performance under different established architectures (depicted in.", "labels": [], "entities": [{"text": "VVD selection algorithm", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.839641273021698}]}, {"text": "In natural language understanding, we use the most recent attention-based model for intention tracking (, this model first uses BiLSTM recurrent network to leverage left-to-right and right-to-left context information to form the hidden representation, then computes self-attention weights to aggregate the hidden representation and predicts user intention.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.6480474372704824}, {"text": "intention tracking", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7133022248744965}]}, {"text": "In document classification, we mainly follow the CNN architecture) to extract n-gram features and then aggregate these features to predict document category.", "labels": [], "entities": [{"text": "document classification", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7594488859176636}]}, {"text": "In natural language inference, we follow the popular ESIM architecture (   ing the Github implementation 3 . In this structure, three main components input encoding, local inference modeling, and inference composition are used to perform sequential inference and composition to simulate the interaction between premises and hypothesis.", "labels": [], "entities": []}, {"text": "Note that, we do not apply the syntax-tree based LSTM proposed in) because we lost the parse tree ( after the vocabulary compression, instead, we follow the simpler sequential LSTM framework without any syntax parse as input.", "labels": [], "entities": []}, {"text": "Besides, the accuracy curve is obtained using the publicly available test split rather than the official online evaluation because we need to evaluate lots of times at different vocabulary capacity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993922710418701}]}, {"text": "Here we demonstrate our results in natural language understanding, document classification, and  natural language inference separately in.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.646561473608017}, {"text": "document classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7494547069072723}, {"text": "natural language inference", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.6489725013573965}]}, {"text": "From these tables, first of all, we can observe that VVD is able to maintain or even improve the reported accuracy on DC and NLU tasks, the accuracy of VVD is reported under dropping out the words with dropout rate larger than 0.95.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9518629312515259}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9993048906326294}]}, {"text": "The exception is in NLI (, where the common approach uses GloVe () for initialization, and we use random initialization, which makes our model fall slightly behind.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9233999252319336}]}, {"text": "It is worth noting that Frequency-based/TF-IDF methods are based on the model trained with cross entropy, while both Group-Lasso and VVD modify the objective function by adding additional regularization.", "labels": [], "entities": [{"text": "VVD", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.7898537516593933}]}, {"text": "It can be seen that VVD is performing very similar to the baseline models on DC and NLU tasks, while consistently outperforming the baseline methods (with random initialized embedding) on more challenging NLI and YelpReview tasks, that said, VVD can also be viewed as a generally effective regularization technique to sparsify features and alleviate the over-fitting problem in NLP tasks.", "labels": [], "entities": []}, {"text": "In terms of the vocabulary selection capability, our proposed VVD is demonstrated to outperform the competing algorithms in terms of both AUC and Vocab@-X% metrics consistently over different datasets as shown in Table 3.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7811728119850159}, {"text": "AUC", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7762810587882996}]}, {"text": "In order to better understand the margin between VVD and frequency-based method, we plot their accuracy-vocab curves in, from which we can observe that the accuracy curves start from nearly the same accuracy with the full vocabulary, by gradually decreasing the budget\u02c6V budget\u02c6 budget\u02c6V , VVD decreases at a much lower rate than the competing algorithms, which clearly reflects its superiority under limited-budget scenario.", "labels": [], "entities": [{"text": "accuracy-vocab", "start_pos": 95, "end_pos": 109, "type": "METRIC", "confidence": 0.9973134398460388}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9981764554977417}, {"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9788908362388611}, {"text": "VVD", "start_pos": 290, "end_pos": 293, "type": "METRIC", "confidence": 0.7757566571235657}]}, {"text": "From the empirical result, we can conclude that: 1) the retrieval-based selection algorithm can yield marginal improvement over the AUC metric, but the vocab@-X% metric deteriorates.", "labels": [], "entities": [{"text": "AUC", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.6841962337493896}, {"text": "vocab@-X% metric", "start_pos": 152, "end_pos": 168, "type": "METRIC", "confidence": 0.7872144341468811}]}, {"text": "2) grouplasso and VVD algorithm directly considers the connection between each word and end classification accuracy; such task-awareness can greatly in improving both evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.8795219659805298}]}, {"text": "Here we show that NLU datasets are relatively simpler, which only involves detecting key words from hu- man voice inputs to make decent decisions, a keyword vocabulary within 100 is already enough for promising accuracy.", "labels": [], "entities": [{"text": "NLU datasets", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.9065639078617096}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9807776808738708}]}, {"text": "For DC datasets, which involve better inner-sentence and inter-sentence understanding, hundred-level vocabulary is required for most cases.", "labels": [], "entities": [{"text": "DC datasets", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.825203537940979}, {"text": "inter-sentence understanding", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.6829319894313812}]}, {"text": "NLI datasets involve more complicated reasoning and interaction, which requires a thousand-level vocabulary.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.847194641828537}]}, {"text": "Case Study To provide an overview of what words are selected, we depict the selection spectrum over different NLP tasks in, from which we observe that most of the selected vocabulary are still from the high-frequency area to ensure coverage, which also explains why the frequency-based algorithm is already very strong.", "labels": [], "entities": []}, {"text": "Furthermore, we use the Snips dataset () to showcase the difference between the vocabularies selected by VVD and by frequency-based baseline.", "labels": [], "entities": [{"text": "Snips dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9175744354724884}, {"text": "VVD", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8289575576782227}]}, {"text": "The main goal of this dataset is to understand the speaker's intention such as \"BookRestaurant\", \"PlayMusic\", and \"SearchLocalEvent\".", "labels": [], "entities": [{"text": "BookRestaurant", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.943559467792511}]}, {"text": "We show the selected/unselected words by our algorithm in under a vocabulary budget of 100, it is observed that many non-informative but frequent functional words like \"get\", \"with\", and \"five\" are unselected while more task-related but less frequent words like \"neighborhood\", \"search\", \"theatre\" are selected.", "labels": [], "entities": []}, {"text": "More vividly, we demonstrate the word cloud of the selected vocabulary of Snips () in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Illustration of the frequency-based vocabulary  selection heuristic on a typical CNN-based document  classification model (Section 4.1). #Emb is the num- ber of parameters in the word embedding matrix (256  dimensions), and #CNN is that in the CNN model.", "labels": [], "entities": [{"text": "CNN-based document  classification", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.5760258138179779}]}, {"text": " Table 2: An overview of different datasets under different classification tasks including description and sizes.", "labels": [], "entities": []}, {"text": " Table 3: Experimental Results on various NLP tasks and datasets on the proposed metrics in subsection 2.3. Bold  accuracy means the result is statistically significantly better than the competitors.", "labels": [], "entities": [{"text": "Bold", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9722200036048889}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8538872599601746}]}]}