{"title": [{"text": "Structured Minimally Supervised Learning for Neural Relation Extraction", "labels": [], "entities": [{"text": "Neural Relation Extraction", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.8475273648897806}]}], "abstractContent": [{"text": "We present an approach to minimally supervised relation extraction that combines the benefits of learned representations and structured learning, and accurately predicts sentence-level relation mentions given only proposition-level supervision from a KB.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6917337328195572}, {"text": "predicts sentence-level relation mentions", "start_pos": 161, "end_pos": 202, "type": "TASK", "confidence": 0.5778716281056404}]}, {"text": "By explicitly reasoning about missing data during learning, our approach enables large-scale training of 1D convolutional neural networks while mitigating the issue of label noise inherent in distant supervision.", "labels": [], "entities": []}, {"text": "Our approach achieves state-of-the-art results on minimally supervised sentential relation extraction, out-performing a number of baselines, including a competitive approach that uses the attention layer of a purely neural model.", "labels": [], "entities": [{"text": "sentential relation extraction", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.636811375617981}]}], "introductionContent": [{"text": "Recent years have seen significant progress on tasks such as object detection, automatic speech recognition and machine translation.", "labels": [], "entities": [{"text": "object detection", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.8971979022026062}, {"text": "automatic speech recognition", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.6041773855686188}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8105281889438629}]}, {"text": "These performance advances are largely driven by the application of neural network methods on large, highquality datasets.", "labels": [], "entities": []}, {"text": "In contrast, traditional datasets for relation extraction are based on expensive and time-consuming human annotation () and are therefore relatively small.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9637863337993622}]}, {"text": "Distant supervision (), a technique which uses existing knowledge bases such as Freebase or Wikipedia as a source of weak supervision, enables learning from large quantities of unlabeled text and is a promising approach for scaling up.", "labels": [], "entities": []}, {"text": "Recent work has shown promising results from large-scale training of neural networks for relation extraction (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.9106432795524597}]}, {"text": "There are, however, significant challenges due to the inherent noise in distant supervision.", "labels": [], "entities": []}, {"text": "For example, showed that, when learning using distant supervision from a knowledge base, the portion of mis-labeled examples can vary from 13% to 31%.", "labels": [], "entities": []}, {"text": "To address this issue, another line of work has explored structured learning methods that introduce latent variables.", "labels": [], "entities": []}, {"text": "An example is MultiR (), which is based on a joint model of relations between entities in a knowledge base and those mentioned in text.", "labels": [], "entities": []}, {"text": "This structured learning approach has a number of advantages; for example, by integrating inference into the learning procedure it has the potential to overcome the challenge of missing facts by ignoring the knowledge base when mentionlevel classifiers have high confidence (.", "labels": [], "entities": []}, {"text": "Prior work on structured learning from minimal supervision has leveraged sparse feature representations, however, and has therefore not benefited from learned representations, which have recently achieved state-of-theart results on abroad range of NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach that combines the benefits of structured and neural methods for minimally supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7164305597543716}]}, {"text": "Our proposed model learns sentence representations that are computed by a 1D convolutional neural network and are used to define potentials over latent relation mention variables.", "labels": [], "entities": []}, {"text": "These mention-level variables are related to observed facts in a KB using a set of deterministic factors, followed by pairwise potentials that encourage agreement between extracted propositions and observed facts, but also enable inference to override these soft constraints during learning, allowing for the possibility of missing information.", "labels": [], "entities": []}, {"text": "Because marginal inference is intractable in this model, a MAP-based approach to learning is applied ().", "labels": [], "entities": []}, {"text": "Our approach is related to recent work structured learning with end-to-end learned representa-tions, including Structured Prediction Energy Networks (SPENs) ; the key differences are the application to minimally supervised relation extraction and the inclusion of latent variables with deterministic factors, which we demonstrate enables effective learning in the presence of missing data in distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 223, "end_pos": 242, "type": "TASK", "confidence": 0.7040599435567856}]}, {"text": "Our proposed method achieves state-of-theart results on minimally supervised sentential relation extraction, outperforming a number of baselines including one that leverages the attention layer of a purely neural model ().", "labels": [], "entities": [{"text": "sentential relation extraction", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.6420472065607706}]}], "datasetContent": [{"text": "In Section 2, we presented an approach that combines the benefits of PCNN representations and structured learning with latent variables for minimally supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7161388397216797}]}, {"text": "In this section we present the details of our evaluation methodology and experimental results.", "labels": [], "entities": []}, {"text": "Datasets: We evaluate our models on the NYTFreebase dataset ( which was created by aligning relational facts from Freebase with the New York Times corpus, and has been used in abroad range of prior work on minimally supervised relation extraction.", "labels": [], "entities": [{"text": "NYTFreebase dataset", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.9879977107048035}, {"text": "Freebase with the New York Times corpus", "start_pos": 114, "end_pos": 153, "type": "DATASET", "confidence": 0.6797128873211997}, {"text": "minimally supervised relation extraction", "start_pos": 206, "end_pos": 246, "type": "TASK", "confidence": 0.6434814557433128}]}, {"text": "Several versions of this dataset have been used in prior work; to facilitate the reproduction of prior results, we experiment with two versions of the dataset used by, we utilize word embeddings pre-trained on the NYT corpus using the word2vec tool, other parameters are initialized using the method described by.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 214, "end_pos": 224, "type": "DATASET", "confidence": 0.9800425469875336}]}, {"text": "The Hoffmann et. al. sentential evaluation dataset is split into a development and test set and grid search on the development set was used to determine optimal values for the learning rate \u03bb among {0.001, 0.01}, KB disagreement penalty scalar \u00b5 among {100, 200, \u00b7 \u00b7 \u00b7 , 2000} and \u03b2 1 /\u03b2 2 bag size threshold for the adaptive learning rate among {10, 15, \u00b7 \u00b7 \u00b7 , 40}.", "labels": [], "entities": [{"text": "Hoffmann et. al. sentential evaluation dataset", "start_pos": 4, "end_pos": 50, "type": "DATASET", "confidence": 0.5411480069160461}, {"text": "KB disagreement penalty scalar \u00b5", "start_pos": 213, "end_pos": 245, "type": "METRIC", "confidence": 0.9033881068229676}]}, {"text": "Other hyperparameters with fixed values are presented in.", "labels": [], "entities": []}, {"text": "Neural Baselines: To demonstrate the effectiveness of the our approach, we compare against colless universal schema () in addition to the PCNN+ATT model of.", "labels": [], "entities": []}, {"text": "After training the Lin et. al. model to predict observed facts in the KB, we use its attention layer to make mention-level predictions as follows: Where r j indicates the vector representation of the jth relation.", "labels": [], "entities": []}, {"text": "Structured Baselines: In addition to initializing convolutional filters used in the \u03c6 PCNN (\u00b7) factors randomly and performing structured learning of representations as in Equation 4, we also experimented with variants of MultiR and DN-MAR, which are based on the structured perceptron), using fixed sentence representations: both traditional sparse feature representations, in addition to pre-trained continuous representations generated using our bestperforming reimplementation of PCNN+ATT.", "labels": [], "entities": []}, {"text": "For the structured perceptron baselines, we also experimented with variants based on MIRA, which we found to provide consistent improvements.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.8895412683486938}]}, {"text": "More details are provided in Appendix A.  In this work, we are primarily interested in mention-level relation extraction.", "labels": [], "entities": [{"text": "mention-level relation extraction", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.6842418114344279}]}, {"text": "For our first set of experiments, we use the manually annotated dataset created by).", "labels": [], "entities": []}, {"text": "Note that sentences in the Hoffman et. al. dataset were selected from the output of systems used in their evaluation, so it is possible there are high confidence predictions made by our systems that are not present.", "labels": [], "entities": [{"text": "Hoffman et. al. dataset", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.67535001039505}]}, {"text": "Therefore, we further validate our findings, by performing a manual inspection of the highest confidence predictions in.", "labels": [], "entities": []}, {"text": "NYTFB-68K Results: As illustrated in, simply applying structured models (MultiR and DNMAR) with pre-trained sentence representations performs competitively.", "labels": [], "entities": [{"text": "NYTFB-68K", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.969490110874176}]}, {"text": "MIRA provides consistent improvements for both sparse and dense representations.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.3535175025463104}]}, {"text": "PCNN+ATT outperforms most latent-variable models on the sentential evaluation, we found this result to be surprising as the model was designed for extracting propositionlevel facts.", "labels": [], "entities": [{"text": "ATT", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.6973261833190918}]}, {"text": "Col-less universal schema does not perform very well in this evaluation; this is likely due to the fact that it was developed for the KBP slot filling evaluation (, and only uses the part of a sentence between two entities as an input representation, which can remove important context.", "labels": [], "entities": [{"text": "KBP slot filling evaluation", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.8024350851774216}]}, {"text": "Our proposed model, which jointly learns sentence representations using a structured latent-variable model that allows for the possiblity of missing data, achieves the best overall performance; its improvements overall baselines were found to be statistically significant according to a paired bootstrap test: AUC of sentential evaluation precision / recall curves for all models trained on NYTFB-280K.", "labels": [], "entities": [{"text": "AUC", "start_pos": 310, "end_pos": 313, "type": "METRIC", "confidence": 0.9993690848350525}, {"text": "precision", "start_pos": 339, "end_pos": 348, "type": "METRIC", "confidence": 0.9315560460090637}, {"text": "recall", "start_pos": 351, "end_pos": 357, "type": "METRIC", "confidence": 0.9210880994796753}, {"text": "NYTFB-280K", "start_pos": 391, "end_pos": 401, "type": "DATASET", "confidence": 0.9822235107421875}]}, {"text": "Our proposed PCNNNMAR (AdapLR) still performs the best, and the advantage over baselines is also statistically significant (p-value of bootstrap is less than 0.05).", "labels": [], "entities": []}, {"text": "tract new facts that are not already contained in a KB.", "labels": [], "entities": []}, {"text": "Furthermore, sentential extraction has the benefit of providing clear provenance for extracted facts, which is crucial in many applications.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.968349277973175}]}, {"text": "Having mentioned these limitations of the held-out evaluation metrics, however, we now present results using this approach to facilitate comparison to prior work.", "labels": [], "entities": []}, {"text": "presents precision-recall curves against held out facts from Freebase comparing PCNNN-MAR to several baselines and presents results on the larger NYTFB-280K dataset.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.9982749223709106}, {"text": "NYTFB-280K dataset", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.9922971725463867}]}, {"text": "All models perform better according to the held out evaluation metric when training on the larger dataset, which is consistent with our hypothesis, presented at the end of Section 3.1.", "labels": [], "entities": []}, {"text": "Our structured model with learned representations, PCNNNMAR (AdapLR), has lower precision when recall is high.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9988860487937927}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9946658611297607}]}, {"text": "This also fits with our hypothesis, as systems that explicitly model missing data will extract many correct facts that do not appear in the KB, resulting in an under-estimate of precision according to this metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9991683959960938}]}, {"text": "NYTFB-68K NYTFB-280K (  pairs they include.", "labels": [], "entities": [{"text": "NYTFB-68K", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9875160455703735}, {"text": "NYTFB-280K", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7614902257919312}]}, {"text": "shows that NYTFB-280K training set has around 4 times the number of sentences and entity pairs as NYTFB-68K, and the proportions of multi-sentence entity pairs in NYTFB-280K is higher.", "labels": [], "entities": [{"text": "NYTFB-280K training set", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.9525475899378458}, {"text": "NYTFB-68K", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.9644156694412231}, {"text": "NYTFB-280K", "start_pos": 163, "end_pos": 173, "type": "DATASET", "confidence": 0.9616789221763611}]}, {"text": "In, we can see that the distribution of relations in the two datasets are comparable, but NYTFB-280K has much more entity pairs for each relation.", "labels": [], "entities": [{"text": "NYTFB-280K", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.9702876210212708}]}, {"text": "Also, tells us that NYTFB-280K has a wider bag-size range and more large training bags.: Distribution of the most frequent relations in the training set of NYTFB-68K and NYTFB-280K.", "labels": [], "entities": [{"text": "NYTFB-280K", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9459191560745239}, {"text": "NYTFB-68K", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.9797987937927246}, {"text": "NYTFB-280K", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.9682103991508484}]}], "tableCaptions": [{"text": " Table 1: Number of entity pairs and sentences in  the training portion of Riedel's HELDOUT dataset  (NYTFB-68K) and Lin's dataset (NYTFB-280K).", "labels": [], "entities": [{"text": "HELDOUT dataset", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.7263109087944031}, {"text": "NYTFB-68K", "start_pos": 102, "end_pos": 111, "type": "DATASET", "confidence": 0.8871976137161255}, {"text": "Lin's dataset", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.6414265731970469}, {"text": "NYTFB-280K", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.7534905076026917}]}, {"text": " Table 2: Untuned hyperparameters in our experiments.", "labels": [], "entities": []}, {"text": " Table 3: AUC of sentential evaluation precision / recall curves for all models trained on NYTFB-68K. Continu- ous sentence representation works as well as human-engineered sentence representation, and MIRA consistently  helps structured perceptron training. PCNN+ATT performs competitively while our PCNNNMAR (AdapLR) is  statistically significantly better (p-value of bootstrap is less than 0.05)", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.999006450176239}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.8642497062683105}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9727159738540649}, {"text": "NYTFB-68K", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.9709790945053101}, {"text": "sentence representation", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.7205925583839417}, {"text": "MIRA", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9948810338973999}]}, {"text": " Table 4: AUC of sentential evaluation precision / recall curves for all models trained on NYTFB-280K. Our  proposed PCNNNMAR (AdapLR) still performs the best, and the advantage over baselines is also statistically  significant (p-value of bootstrap is less than 0.05).", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9990314245223999}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9095733761787415}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9622172713279724}, {"text": "NYTFB-280K", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.9771106243133545}]}, {"text": " Table 5: Top: P@N of 4 most frequent relations  for models trained on NYTFB-68K. Bottom: P@N  of 4 most frequent relations for models trained on  NYTFB-280K. Both models can perform well on  /location/contains relation while PCNNNMAR  (AdapLR) is consistently better over other relations.", "labels": [], "entities": [{"text": "NYTFB-68K", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9861627221107483}, {"text": "NYTFB-280K", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.9899919629096985}]}, {"text": " Table 6: Top: Sentence distribution in Hoffmann et.  al. (2011) sentential evaluation DEV dataset. Bot- tom: Sentence distribution in Hoffmann et. al. (2011)  sentential evaluation TEST dataset. There are substan- tial Out-Of-Freebase mentions which are manually la- belled as correct relational mentions.", "labels": [], "entities": [{"text": "DEV dataset", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.8759596049785614}, {"text": "TEST dataset", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.7234397530555725}]}, {"text": " Table 8: Number of entity pairs and sentences in  the training portion of Riedel's HELDOUT dataset  (NYTFB-68K) and Lin's dataset (NYTFB-280K).", "labels": [], "entities": [{"text": "HELDOUT dataset", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.7335831075906754}, {"text": "NYTFB-68K", "start_pos": 102, "end_pos": 111, "type": "DATASET", "confidence": 0.8882584571838379}, {"text": "Lin's dataset", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.6502549350261688}, {"text": "NYTFB-280K", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.7543973326683044}]}, {"text": " Table 9: Distribution of the most frequent relations in  the training set of NYTFB-68K and NYTFB-280K.", "labels": [], "entities": [{"text": "NYTFB-68K", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.9837063550949097}, {"text": "NYTFB-280K", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9717886447906494}]}, {"text": " Table 10: AUC of sentential evaluation precision / re- call curves for PCNNNMAR with three loss functions  trained on NYTFB-68K. Mention-level hamming loss  has some advantages over other two loss functions.", "labels": [], "entities": [{"text": "AUC", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9985911250114441}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9204043745994568}, {"text": "NYTFB-68K", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9833840131759644}]}]}