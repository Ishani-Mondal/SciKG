{"title": [{"text": "Entity Recognition at First Sight: Improving NER with Eye Movement Information", "labels": [], "entities": [{"text": "Entity Recognition at First Sight", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8479138493537903}, {"text": "Improving NER", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7381028234958649}]}], "abstractContent": [{"text": "Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve natural language processing models.", "labels": [], "entities": []}, {"text": "In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neu-ral model for named entity recognition (NER) with gaze embeddings.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.802851011355718}]}, {"text": "These corpora were manually annotated with named entity labels.", "labels": [], "entities": []}, {"text": "Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data attest time.", "labels": [], "entities": []}, {"text": "The gaze-augmented models for NER using token-level and type-level features outperform the baselines.", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8427047729492188}]}, {"text": "We present the benefits of eye-tracking features by evaluating the NER models on both individual datasets as well as in cross-domain settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of natural language processing includes studies of tasks of different granularity and depths of semantics: from lower level tasks such as tokenization and part-of-speech tagging up to higher level tasks of information extraction such as named entity recognition, relation extraction, and semantic role labeling).", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6699225703875223}, {"text": "tokenization and part-of-speech tagging", "start_pos": 148, "end_pos": 187, "type": "TASK", "confidence": 0.6750677973031998}, {"text": "information extraction such as named entity recognition", "start_pos": 216, "end_pos": 271, "type": "TASK", "confidence": 0.6481917798519135}, {"text": "relation extraction", "start_pos": 273, "end_pos": 292, "type": "TASK", "confidence": 0.7796915173530579}, {"text": "semantic role labeling", "start_pos": 298, "end_pos": 320, "type": "TASK", "confidence": 0.6310563186804453}]}, {"text": "As NLP systems become increasingly prevalent in society, how to take advantage of information passively collected from human readers, e.g. eye movement signals, is becoming more interesting to researchers.", "labels": [], "entities": []}, {"text": "Previous research in this area has shown promising results: Eye-tracking data has been used to improve tasks such as part-of-speech tagging (, sentiment analysis (, prediction of multiword expressions (, and word embedding evaluation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7326781451702118}, {"text": "sentiment analysis", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.9176023602485657}, {"text": "prediction of multiword expressions", "start_pos": 165, "end_pos": 200, "type": "TASK", "confidence": 0.8210285604000092}, {"text": "word embedding evaluation", "start_pos": 208, "end_pos": 233, "type": "TASK", "confidence": 0.7257301410039266}]}, {"text": "However, most of these studies focus on either relatively lower-level tasks (e.g. part-of-speech tagging and multiword expressions) or relatively global properties in the text (e.g. sentiment analysis).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7236862927675247}, {"text": "sentiment analysis", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.9197281301021576}]}, {"text": "In this paper, we test a hypothesis on a different level: Can eye movement signals also help improve higher-level semantic tasks such as extracting information from text?", "labels": [], "entities": []}, {"text": "The answer to this question is not obvious.", "labels": [], "entities": []}, {"text": "On one hand, the quality improvement attributed to eye movement signals on lower-level tasks implies that such signals do contain linguistic information.", "labels": [], "entities": []}, {"text": "On the other hand, it is not clear whether these signals can also provide significant improvement for tasks dealing with higher-level semantics.", "labels": [], "entities": []}, {"text": "Moreover, even if eye movement patterns contain signals related to higher-level tasks, as implied by a recent psycholinguistic study (, noisy as these signals are, it is not straightforward whether they would help, if not hurt, the quality of the models.", "labels": [], "entities": []}, {"text": "In this paper, we provide the first study of the impact of gaze features to automatic named entity recognition from text.", "labels": [], "entities": [{"text": "automatic named entity recognition from text", "start_pos": 76, "end_pos": 120, "type": "TASK", "confidence": 0.7142873754103979}]}, {"text": "We test the hypothesis that eye-tracking data is beneficial for entity recognition in a state-of-the-art neural named entity tagger augmented with embedding layers of gaze features.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7831089198589325}]}, {"text": "Our contributions in the current work can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "First, we manually annotate three eyetracking corpora with named entity labels to train a neural NER system with gaze features.", "labels": [], "entities": []}, {"text": "This collection of corpora facilitates future research in related topics.", "labels": [], "entities": []}, {"text": "The annotations are publicly available.", "labels": [], "entities": []}, {"text": "makes the use of eye-tracking data in NLP applications more feasible since recorded eye-tracking data for each token in context is not required anymore at prediction time.", "labels": [], "entities": []}, {"text": "Moreover, type-aggregated features appear to be particularly useful for cross-domain systems.", "labels": [], "entities": []}, {"text": "Our hypotheses are evaluated not only on the available eye-tracking corpora, but also on an external benchmark dataset, for which gaze information does not exist.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we analyzed how augmenting the named entity recognition system with eye-tracking features affects the results on the individual datasets.", "labels": [], "entities": []}, {"text": "Table 4 shows the improvements achieved by adding all 17 gaze features to the neural architecture, and training models on all three corpora, and on the combined dataset containing all sentences from the Dundee, GECO and ZuCo corpora.", "labels": [], "entities": [{"text": "Dundee, GECO and ZuCo corpora", "start_pos": 203, "end_pos": 232, "type": "DATASET", "confidence": 0.8254707554976145}]}, {"text": "Noticeably, adding token-level gaze features improves the results on all datasets individually and combined, even on the GECO corpus, which yields a high baseline due to the homogeneity of the contained named entities (see).", "labels": [], "entities": [{"text": "GECO corpus", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9557536542415619}]}, {"text": "Furthermore, also presents the results of the NER models making use of the typeaggregated features instead of token-level gaze features.", "labels": [], "entities": []}, {"text": "There are two different experiments for these type-level features: Using the features of the word types occurring in the corpus only, or using the aggregated features of all word types in the three corpora (as describe above).", "labels": [], "entities": []}, {"text": "As can be seen, the performance of the different gaze fea-: Precision (P), recall (R) and F 1 -score (F) for all models trained on individual datasets (best results in bold; * indicates statistically significant improvements on F 1 -score).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9537716656923294}, {"text": "recall (R)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9578384459018707}, {"text": "F 1 -score (F)", "start_pos": 90, "end_pos": 104, "type": "METRIC", "confidence": 0.9814245275088719}, {"text": "F 1 -score", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9677885621786118}]}, {"text": "With gaze are models trained on the original eye-tracking features on token-level, type individual are the models trained on type-aggregated gaze features of this corpus only, while type combined are the models trained with type-aggregated features computed on all datasets.", "labels": [], "entities": []}, {"text": "ture levels varies between datasets, but both the original token-level features as well as the individual and combined type-level features achieve improvements over the baselines of all datasets.", "labels": [], "entities": []}, {"text": "To sum up, the largest improvement with eyetracking features is achieved when combining all corpora into one larger dataset, where an additional 4% is gained in F 1 -score by using typeaggregated features.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9669822305440903}]}, {"text": "Evidently, a larger mixeddomain dataset benefits from the type aggregation, while the original token-level gaze features achieve the best results on the individual datasets.", "labels": [], "entities": []}, {"text": "Moreover, the additional gain when training on all datasets is due to the higher signal-to-noise ratio of type-aggregated features from multiple datasets.", "labels": [], "entities": []}, {"text": "Evaluation on CoNLL-2003 Going on step further, we evaluate the type-aggregated gaze features on an external corpus with no eye movement information available.", "labels": [], "entities": [{"text": "CoNLL-2003", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.9221047163009644}]}, {"text": "The  widely used as a benchmark dataset for NER in different shared tasks.", "labels": [], "entities": [{"text": "NER", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9549984335899353}]}, {"text": "The English part of this corpus consists of Reuters news stories and contains 302,811 tokens in 22,137 sentences.", "labels": [], "entities": [{"text": "Reuters news stories", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8588707447052002}]}, {"text": "We use this dataset as an additional corpus without gaze information.", "labels": [], "entities": []}, {"text": "Only the type-aggregated features (based on the combined eye-tracking corpora) are added to each word.", "labels": [], "entities": []}, {"text": "Merely 76% of the tokens in the CoNLL-2003 corpus also appear in the eyetracking corpora described above and thus receive type-aggregated feature values.", "labels": [], "entities": [{"text": "CoNLL-2003 corpus", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9497309029102325}]}, {"text": "The rest of the tokens without aggregated gaze information available receive a placeholder for the unknown feature values.", "labels": [], "entities": []}, {"text": "Note that to avoid overfitting we do not train on the official train/test split of the CoNLL-2003 dataset, but perform 10-fold cross validation.", "labels": [], "entities": [{"text": "CoNLL-2003 dataset", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.9822506606578827}]}, {"text": "Applying the same experiment setting, we train the augmented NER model with gaze features on the CoNLL-2003 data and compare it to a baseline model without any eye-tracking features.", "labels": [], "entities": [{"text": "CoNLL-2003 data", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9804027378559113}]}, {"text": "We achieve a minor, but nonetheless significant improvement (shown in), which strongly supports the generalizability effect of the typeaggregated features on unseen data.", "labels": [], "entities": []}, {"text": "Ina second evaluation scenario, we test the potential of eye-tracking features for NER across corpora.", "labels": [], "entities": [{"text": "NER", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9868108630180359}]}, {"text": "The goal is to leverage eye-tracking features for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8047488331794739}]}, {"text": "To show the robustness of our approach across domains, we train the models with token-level and type-level features on 100% of corpus A and a development set of 20% of corpus B and test on the remaining 80% of the corpus B, alternating only the development and the test set for each fold.", "labels": [], "entities": []}, {"text": "shows the results of this cross-corpus evaluation.", "labels": [], "entities": []}, {"text": "The impact of the eye-tracking features varies between the different combinations of datasets.", "labels": [], "entities": []}, {"text": "However, the inclusion of eye-tracking features improves the results for all combinations, except for the models trained on the ZuCo corpus: Precision (P), recall (R) and F 1 -score (F) for using type-aggregated gaze features trained on all three eye-tracking datasets and tested on the CoNLL-2003 dataset (* marks statistically significant improvement).", "labels": [], "entities": [{"text": "ZuCo corpus", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.8802981674671173}, {"text": "Precision (P)", "start_pos": 141, "end_pos": 154, "type": "METRIC", "confidence": 0.9553617835044861}, {"text": "recall (R)", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9526138752698898}, {"text": "F 1 -score (F)", "start_pos": 171, "end_pos": 185, "type": "METRIC", "confidence": 0.9778527447155544}, {"text": "CoNLL-2003 dataset", "start_pos": 287, "end_pos": 305, "type": "DATASET", "confidence": 0.9883417189121246}]}, {"text": "Evaluation on CoNLL-2003 Analogous to the individual dataset evaluation, we also test the potential of eye-tracking features in a cross-dataset scenario on an external benchmark dataset.", "labels": [], "entities": [{"text": "CoNLL-2003", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.8856015205383301}]}, {"text": "Again, we use the CoNLL-2003 corpus for this purpose.", "labels": [], "entities": [{"text": "CoNLL-2003 corpus", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.9746051132678986}]}, {"text": "We train a model on the Dundee, GECO and ZuCo corpora using type-aggregated eye-tracking features and test this model on the ConLL-2003 data.", "labels": [], "entities": [{"text": "Dundee, GECO and ZuCo corpora", "start_pos": 24, "end_pos": 53, "type": "DATASET", "confidence": 0.8547887901465098}, {"text": "ConLL-2003 data", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.9787384867668152}]}, {"text": "shows that compared to a baseline without gaze features, the results improve by 3% F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9420087486505508}]}, {"text": "These results underpin our hypothesis of the possibility of generalizing eye-tracking features on word type level, such that no recorded gaze data is required attest time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of the eye-tracking corpora, including domain, size and mean fixation and gaze  duration per token.", "labels": [], "entities": [{"text": "gaze  duration", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.6102045774459839}]}, {"text": " Table 2: Number and distribution of named entity annotations in all three eye-tracking corpora.", "labels": [], "entities": []}, {"text": " Table 4: Precision (P), recall (R) and F 1 -score (F) for  all models trained on individual datasets (best results  in bold; * indicates statistically significant improve- ments on F 1 -score). With gaze are models trained on  the original eye-tracking features on token-level, type  individual are the models trained on type-aggregated  gaze features of this corpus only, while type combined  are the models trained with type-aggregated features  computed on all datasets.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9362828284502029}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.941506564617157}, {"text": "F 1 -score (F)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9826136061123439}, {"text": "statistically significant improve- ments on F 1 -score", "start_pos": 138, "end_pos": 192, "type": "METRIC", "confidence": 0.7546911776065827}]}, {"text": " Table 5: Precision (P), recall (R) and F 1 -score (F) for  using type-aggregated gaze features on the CoNLL- 2003 dataset (* marks statistically significant improve- ment).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9425635188817978}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9506244361400604}, {"text": "F 1 -score (F)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9816927569253104}, {"text": "CoNLL- 2003 dataset", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.9443779438734055}]}, {"text": " Table 6: Cross-corpus results: Precision (P), recall (R) and F 1 -score (F) for all models trained on one dataset and  tested on another (rows = training dataset; columns = test dataset; best results in bold; * indicates statistically  significant improvements). The baseline models are trained without eye-tracking features, token models on the  original eye-tracking features, and type are the models trained with type-aggregated features computed on all  datasets.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9354111701250076}, {"text": "recall (R)", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9473349899053574}, {"text": "F 1 -score (F)", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9780704804829189}]}, {"text": " Table 7: Precision (P), recall (R) and F 1 -score (F) for  using type-aggregated gaze features trained on all three  eye-tracking datasets and tested on the CoNLL-2003  dataset (* marks statistically significant improvement).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9486895650625229}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9524699300527573}, {"text": "F 1 -score (F)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9828338027000427}, {"text": "CoNLL-2003  dataset", "start_pos": 158, "end_pos": 177, "type": "DATASET", "confidence": 0.987584263086319}]}]}