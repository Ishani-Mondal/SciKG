{"title": [{"text": "Using Natural Language Relations between Answer Choices for Machine Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "When evaluating an answer choice for Reading Comprehension task, other answer choices available for the question and the answers of related questions about the same paragraph often provide valuable information.", "labels": [], "entities": [{"text": "Reading Comprehension task", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.7964719931284586}]}, {"text": "In this paper , we propose a method to leverage the natural language relations between the answer choices, such as entailment and contradiction, to improve the performance of machine comprehension.", "labels": [], "entities": []}, {"text": "We use a stand-alone question answering (QA) system to perform QA task and a Natural Language Inference (NLI) system to identify the relations between the choice pairs.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8211254000663757}]}, {"text": "Then we perform inference using an Integer Linear Programming (ILP)-based rela-tional framework to re-evaluate the decisions made by the standalone QA system in light of the relations identified by the NLI system.", "labels": [], "entities": []}, {"text": "We also propose a multitask learning model that learns both the tasks jointly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given an input text and a set of related questions with multiple answer choices, the reading comprehension (RC) task evaluates the correctness of each answer choice.", "labels": [], "entities": []}, {"text": "Current approaches to the RC task quantify the relationship between each question and answer choice independently and pick the highest scoring option.", "labels": [], "entities": []}, {"text": "In this paper, we follow the observation that when humans approach such RC tasks, they tend to take a holistic view ensuring that their answers are consistent across the given questions and answer choices.", "labels": [], "entities": []}, {"text": "In this work we attempt to model these pragmatic inferences, by leveraging the entailment and contradiction relations between the answer choices to improve machine comprehension.", "labels": [], "entities": []}, {"text": "To help clarify these concepts, consider the following examples: How can the military benefit from the existence of the CIA?", "labels": [], "entities": []}, {"text": "c 1 : They can use them c 2 : These agencies are keenly attentive to the military's strategic and tactical requirements () c 3 : The CIA knows what intelligence the military requires and has the resources to obtain that intelligence () The above example contains multiple correct answer choices, some are easier to capture than others.", "labels": [], "entities": []}, {"text": "For example, identifying that c 3 is true might be easier than c 2 based on its alignment with the input text.", "labels": [], "entities": []}, {"text": "However, capturing that c 3 entails c 2 allows us to predict c 2 correctly as well.", "labels": [], "entities": []}, {"text": "Classification of the answer in red (marked ) could be corrected using the blue (marked ) answer choice.", "labels": [], "entities": []}, {"text": "Q1: When were the eggs added to the panto make the omelette?", "labels": [], "entities": []}, {"text": "c 1 1 : When they turned on the stove c 1 2 : When the pan was the right temperature () Q2: Why did they use stove to cook omelette?", "labels": [], "entities": []}, {"text": "c 2 1 : They didn't use the stove but a microwave c 2 2 : Because they needed to heat up the pan () Similarly, answering Q1 correctly helps in answering Q2.", "labels": [], "entities": []}, {"text": "Our goal is to leverage such inferences for machine comprehension.", "labels": [], "entities": []}, {"text": "Our approach contains three steps.", "labels": [], "entities": []}, {"text": "First, we use a stand-alone QA system to classify the answer choices as true/false.", "labels": [], "entities": []}, {"text": "Then, we classify the relation between each pair of choices fora given question as entailment, contradiction or neutral.", "labels": [], "entities": []}, {"text": "Finally, we re-evaluate the labels assigned to choices using an Integer Linear Programming based inference procedure.", "labels": [], "entities": []}, {"text": "We discuss different training protocols and representation choices for the combined decision problem.", "labels": [], "entities": []}, {"text": "We empirically evaluate on two recent datasets, MultiRC () and and show that it improves machine comprehension in both.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments in four phases.", "labels": [], "entities": []}, {"text": "In the first phase, we evaluate the stand-alone QA system.", "labels": [], "entities": []}, {"text": "In the second phase, we train the NLI system on SNLI data and evaluate the approach shown in.", "labels": [], "entities": [{"text": "SNLI data", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.8279459476470947}]}, {"text": "In the third phase, we train the NLI system using the self-training data.", "labels": [], "entities": []}, {"text": "In the fourth phase, we evaluate the proposed joint model.", "labels": [], "entities": []}, {"text": "We evaluate all models on MultiRC dataset.", "labels": [], "entities": [{"text": "MultiRC dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9666852056980133}]}, {"text": "The results are shown in table 2.", "labels": [], "entities": []}, {"text": "We evaluate the joint model on SemEval dataset, shown in table 3.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.7743569016456604}]}, {"text": "We use two datasets for our experiments, MultiRC dataset 2 and the SemEval 2018 task 11 dataset 3 . MultiRC dataset consisted of a training and development set with a hidden test set.", "labels": [], "entities": [{"text": "MultiRC dataset", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.8488925397396088}, {"text": "SemEval 2018 task 11 dataset", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.723103928565979}, {"text": "MultiRC dataset", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.6910678744316101}]}, {"text": "We split the given training set into training and development sets and use the given development set as test set.", "labels": [], "entities": []}, {"text": "Each question in the MultiRC dataset has approximately 5 choices on average.", "labels": [], "entities": [{"text": "MultiRC dataset", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.956494927406311}]}, {"text": "Multiple of them maybe true fora given question.", "labels": [], "entities": []}, {"text": "The training split of MultiRC consisted of 433 paragraphs and 4, 853 questions with 25, 818 answer choices.", "labels": [], "entities": [{"text": "MultiRC", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8617123961448669}]}, {"text": "The development split has 23 paragraphs and 275 questions with 1, 410 answer choices.", "labels": [], "entities": []}, {"text": "Test set has 83 paragraphs and 953 questions with 4, 848 answer choices.", "labels": [], "entities": [{"text": "Test set", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7657572329044342}]}, {"text": "SemEval dataset has 2 choices for each question, exactly one of them is true.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8245534598827362}]}, {"text": "The training set consists of 1, 470 paragraphs with 9, 731 questions.", "labels": [], "entities": []}, {"text": "The development set has 219 paragraphs with 1, 411 questions.", "labels": [], "entities": []}, {"text": "And the test set has 430 paragraphs with 2, 797 questions.", "labels": [], "entities": []}, {"text": "For MultiRC dataset, we use two metrics for evaluating our approach, namely EM 0 and EM 1.", "labels": [], "entities": [{"text": "MultiRC dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8255721926689148}, {"text": "EM 0", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9602183401584625}, {"text": "EM 1", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9194201529026031}]}, {"text": "EM 0 refers to the percentage of questions for which all the choices have been correctly classified.", "labels": [], "entities": [{"text": "EM 0", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8427862823009491}]}, {"text": "EM 1 is the the percentage of questions for which at most one choice is wrongly classified.", "labels": [], "entities": [{"text": "EM 1", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9784684777259827}]}, {"text": "For the SemEval dataset, we use accuracy metric.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8093670010566711}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9991633892059326}]}], "tableCaptions": [{"text": " Table 1: Accuracy of entailment and contradiction detection", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.990479052066803}]}, {"text": " Table 2: Summary of the results on MultiRC dataset.  EM 0 is the percentage of questions for which all the  choices are correct. EM 1 is the the percentage of ques- tions for which at most one choice is wrong.", "labels": [], "entities": [{"text": "MultiRC dataset", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9701026082038879}, {"text": "EM 0", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9753552973270416}, {"text": "EM", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9867488145828247}]}, {"text": " Table 3: Accuracy of various models on SemEval'18  task-11 dataset", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9960818886756897}, {"text": "SemEval'18  task-11 dataset", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.8321654200553894}]}]}