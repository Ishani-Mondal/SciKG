{"title": [{"text": "A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unan-swerable questions, (2) multi-turn interactions, and (3) abstractive answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8149733245372772}]}, {"text": "We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third.", "labels": [], "entities": []}, {"text": "Because of the datasets' structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA.", "labels": [], "entities": [{"text": "CoQA", "start_pos": 179, "end_pos": 183, "type": "DATASET", "confidence": 0.8755406737327576}]}, {"text": "Despite the similarity , models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pre-training.", "labels": [], "entities": []}, {"text": "To encourage cross-evaluation, we release code for conversion between datasets at https://github.com/my89/co-squac.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering on textual data has served as a challenge problem for the NLP community.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9038896560668945}]}, {"text": "With the development of large scale benchmarks and sufficiently simple evaluations ( progress has been rapid.", "labels": [], "entities": []}, {"text": "In recent evaluation on SQuAD (, performance exceeded that of annotators (.", "labels": [], "entities": []}, {"text": "In response to this development, there have been a flurry of new datasets.", "labels": [], "entities": []}, {"text": "In this work, we analyze three such new proposed datasets, SQuAD 2.0 (,, and CoQA (.", "labels": [], "entities": []}, {"text": "In each of these datasets, crowd workers are asked to (1) produce questions about a paragraph of text (context) and (2) produce a reply A review of other new datasets is in the related work. by either indicating there is no answer, or providing an extractive answer from the context by highlighting one contiguous span.", "labels": [], "entities": []}, {"text": "QuAC and CoQA contain two other features: questions are asked in the form of a dialog, where co-reference to previous interactions is possible and directly answering yes/no is possible.", "labels": [], "entities": [{"text": "QuAC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.852704644203186}]}, {"text": "CoQA also allows workers to edit the spans to provide abstractive answers.", "labels": [], "entities": [{"text": "CoQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9135555624961853}]}, {"text": "We compare these three datasets along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.", "labels": [], "entities": []}, {"text": "Unanswerable question coverage is complementary among datasets; SQuAD 2.0 focuses more on questions of extreme confusion, such as false premise questions, while QuAC primarily focuses on missing information.", "labels": [], "entities": []}, {"text": "QuAC and CoQA dialogs simulate different types of user behavior: QuAC dialogs often switch topics while CoQA dialogs include more queries for details.", "labels": [], "entities": []}, {"text": "Unfortunately, no dataset provides significant coverage of abstractive answers beyond yes/no answers, and we show that a method can achieve an extractive answer upper bound of 100 and 97.8 F1 on QuAC and CoQA , respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 189, "end_pos": 191, "type": "METRIC", "confidence": 0.9946351647377014}, {"text": "CoQA", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.7817768454551697}]}, {"text": "Motivated by the above analysis, we apply the baseline presented in QuAC (, BiDAF++, a model based on BiDAF (, augmented with self attention and ELMo contextualized embeddings (  CoQA contains questions that drill into details about topics and cover 60% of sentences in the context while in QuAC dialog switch topic more often and cover less than 30% of sentences.", "labels": [], "entities": []}, {"text": "Neither dataset has a significant number of returns to previous topics, clarifications, or definitional interactions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we analyze unanswerable questions, dialog features, abstractive answers in SQuAD 2.0, QuAC, and CoQA.", "labels": [], "entities": [{"text": "CoQA", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.8994929194450378}]}, {"text": "All analysis was performed by the authors, on a random sample of 50 contexts (300-700 questions) from the development set of each dataset.", "labels": [], "entities": []}, {"text": "In this section we consider whether models can benefit from transfer between SQuAD 2.0, QuAC, and CoQA, and show that the datasets, while ineffective for direct transfer, can be used as pretraining.", "labels": [], "entities": []}, {"text": "Across all of the datasets, BiDAF++ outperforms other baselines, and there exists at least one other dataset that significantly improves performance on a target dataset on average +2.1 F1.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.8999136686325073}, {"text": "F1", "start_pos": 185, "end_pos": 187, "type": "METRIC", "confidence": 0.991885781288147}]}, {"text": "Experiments do not support that direct transfer is possible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of unanswerable questions on 50 random contexts from the development set of each dataset.  SQuAD 2.0 contains a diverse set of circumstances that make questions unanswerable, QuAC focuses on informa- tion that could plausibly be in context material and CoQA does not significantly cover unanswerable questions.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of dialog features in 50 random contexts from the development set of each dataset.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of abstractive features in 50 random contexts in the develoment set of each dataset. Both  QuAC and CoQA contain yes/no questions while CoQA also contains answers that improve fluency through ab- stractive behavior. The extractive upper bound from CoQA is high because most absractivive answers involve  adding a pronoun (Coref) or inserting prepositions and changing word forms (Fluency) to existing extractive an- swers, resulting in extremely high overlap with possible extractive answers.", "labels": [], "entities": []}, {"text": " Table 4: Development set performance by training  BiDAF++ (Choi et al., 2018) models (extractive) on  CoQA data with handling yes/no and no-answer ques- tions as in QuAC. Despite being extractive, these mod- els significantly outperform reported baselines, DrQA  and DrQA + PGNet (Reddy et al., 2018).", "labels": [], "entities": [{"text": "CoQA data", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.8445649445056915}, {"text": "DrQA", "start_pos": 258, "end_pos": 262, "type": "DATASET", "confidence": 0.9586136937141418}, {"text": "DrQA + PGNet", "start_pos": 268, "end_pos": 280, "type": "DATASET", "confidence": 0.8247058590253195}]}, {"text": " Table 5: Test set results on CoQA. We report in domain  F1 (in-F1), out of domain F1 on two held out domains,  Reddit and Science (out-F1) and the overall F1 (F1).", "labels": [], "entities": [{"text": "CoQA", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.710216760635376}, {"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9313725233078003}, {"text": "F1", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.9808412194252014}]}, {"text": " Table 6: Cross dataset transfer to QuAC development  set. Models do not transfer directly (rows 3 and 4), but  after fine tuning improve performance (rows 5 and 6).", "labels": [], "entities": [{"text": "QuAC development  set", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.8348249793052673}]}, {"text": " Table 7: Cross dataset transfer to CoQA development  set. Models do not transfer directly (rows 3 and 4),  but after fine tuning improve performance (rows 5 and  6). For an explanation of why BiDAF++ outperforms  DrQA + PGNet, see Section 3.", "labels": [], "entities": [{"text": "CoQA development  set", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.8361516793568929}, {"text": "DrQA + PGNet", "start_pos": 214, "end_pos": 226, "type": "DATASET", "confidence": 0.8536799351374308}]}, {"text": " Table 8: Cross dataset transfer to SQuAD 2.0 develop- ment set. BiDAF++ (Choi et al., 2018) outperforms  the baseline, a different implementation of the same  model (Rajpurkar et al., 2018) likely because of better  hyper parameter tuning.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9418887495994568}]}]}