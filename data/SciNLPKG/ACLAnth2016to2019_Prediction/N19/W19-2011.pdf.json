{"title": [{"text": "Probing Biomedical Embeddings from Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks.", "labels": [], "entities": []}, {"text": "Pre-training on domain-specific corpora , such as biomedical articles, further improves their performance.", "labels": [], "entities": []}, {"text": "In this paper, we conduct probing experiments to determine what additional information is carried intrinsi-cally by the in-domain trained contextualized embeddings.", "labels": [], "entities": []}, {"text": "For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers.", "labels": [], "entities": []}, {"text": "We compare BERT (Devlin et al., 2018), ELMo (Pe-ters et al., 2018a), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9958115816116333}]}, {"text": "Surprisingly , while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outper-forms BioBERT in our probing tasks.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.8753504157066345}]}, {"text": "We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP has seen an upheaval in the last year, with contextual word embeddings, such as ELMo (Peters et al., 2018a) and BERT, setting state-of-the-art performance on many tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9934121966362}]}, {"text": "These empirical successes suggest that unsupervised pre-training from large corpora could be a vital part of NLP models.", "labels": [], "entities": []}, {"text": "In specific domains like biomedicine, NLP datasets are much smaller than their general-domain counterparts , which leads to a lot of ad-hoc models: some infer through knowledge bases (Chandu For example, only has about 11k training instances while the general domain NLI dataset SNLI () has 550k.", "labels": [], "entities": [{"text": "NLI dataset SNLI", "start_pos": 267, "end_pos": 283, "type": "DATASET", "confidence": 0.7576089898745219}]}, {"text": "et al., 2017), while others leverage large-scale general domain datasets for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7356661558151245}]}, {"text": "However, unlabeled biomedical texts are abundant, and their full potential has perhaps not yet been fully realized.", "labels": [], "entities": []}, {"text": "We train a domain-specific version of ELMo on 10M PubMed abstracts, called BioELMo 2 . Experiments on biomedical named entity recognition (NER) dataset BC2GM ( and biomedical natural language inference (NLI) dataset MedNLI ( clearly show the utility in training in-domain contextual word representations, but we would also like to know exactly what extra information is carried intrinsically in these embeddings.", "labels": [], "entities": [{"text": "biomedical named entity recognition (NER) dataset BC2GM", "start_pos": 102, "end_pos": 157, "type": "TASK", "confidence": 0.7603384190135531}, {"text": "biomedical natural language inference (NLI) dataset MedNLI", "start_pos": 164, "end_pos": 222, "type": "DATASET", "confidence": 0.6637852523061964}]}, {"text": "To answer this question, we design two probing tasks, one for NER and one for NLI, where contextualized embeddings are used solely as fixed feature extractors and no sequence modeling layers are allowed above the embeddings.", "labels": [], "entities": []}, {"text": "This setting prohibits the model from capturing task-specific contextual patterns, and instead only utilizes the information already present in the representations.", "labels": [], "entities": []}, {"text": "In parallel to our work of BioELMo, introduce BioBERT, which is a biomedical version of in-domain trained BERT.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.838262677192688}, {"text": "BERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.7965393662452698}]}, {"text": "We also probe BioBERT in our experiments.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.4388907849788666}]}, {"text": "Expectedly, BioELMo and BioBERT perform significantly better than their general-domain counterparts.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.591076135635376}]}, {"text": "When fine-tuned, BioBERT outperforms BioELMo, however, when used as fixed feature extractors, BioELMo is better than BioBERT in our probing tasks.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.8934916257858276}, {"text": "BioELMo", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9288095831871033}, {"text": "BioBERT", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.8554695844650269}]}, {"text": "Visualizations and nearest neighbor analyses suggest that it's because BioELMo more effectively encodes entitytypes and information about biomedical relations, such as disease and symptom interactions, than BioBERT.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: For the NER task, we use the BC2GM dataset.", "labels": [], "entities": [{"text": "NER task", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.9290660619735718}, {"text": "BC2GM dataset", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9636782705783844}]}, {"text": "BC2GM stands for BioCreative II gene mention dataset ().", "labels": [], "entities": [{"text": "BC2GM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7113177180290222}, {"text": "BioCreative II gene mention dataset", "start_pos": 17, "end_pos": 52, "type": "DATASET", "confidence": 0.6482609570026397}]}, {"text": "The task is to detect gene names in sentences.", "labels": [], "entities": []}, {"text": "It contains 15k training and 5k test sentences.", "labels": [], "entities": []}, {"text": "We also test on the general-domain CoNLL 2003 NER dataset, where the task is to detect entities such as person and location.", "labels": [], "entities": [{"text": "CoNLL 2003 NER dataset", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9362551271915436}]}, {"text": "For the NLI task, we use the MedNLI dataset (, where the task is, given a pair of sentences (premise and hypothesis), to predict whether the relation of entailment, contradiction, or neutral (no relation) holds between the Whole setting perform better than the general BERT and ELMo and biomed w2v, setting new state-of-the-art performance for this dataset.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9835126399993896}, {"text": "BERT", "start_pos": 269, "end_pos": 273, "type": "METRIC", "confidence": 0.8965467810630798}]}, {"text": "BioBERT and BioELMo remains competitive in the Probing setting, doing much better than their general domain counterparts and even general ELMo in the Whole setting.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6663480997085571}]}, {"text": "This shows that with the right pre-training, the downstream model can be considerably simplified.", "labels": [], "entities": []}, {"text": "Unsurprisingly, in the Control setting BioBERT and BioELMo do worse than their general counterparts, indicating that the gains come at the cost of losing some general-domain information.", "labels": [], "entities": [{"text": "BioELMo", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.7295994758605957}]}, {"text": "However, the performance gaps (absolute differences) between ELMo and BioELMo are larger in the biomedical domain than it is in the general domain, which is also true for BERT and BioBERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.5661765933036804}]}, {"text": "For ELMo and BioELMo, we believe it is because the PubMed corpus contains many mentions of general-domain entities whereas the reverse is not true.", "labels": [], "entities": [{"text": "PubMed corpus", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9405010342597961}]}, {"text": "Because BioBERT is initialzied with BERT and also uses general-domain corpora like Enligsh WikiPedia for pre-training, it's not surprising that BioBERT is just 0.2 worse than BERT on CoNLL 2003 NER in control setting.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9949254989624023}, {"text": "Enligsh WikiPedia", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.8515360951423645}, {"text": "BioBERT", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.870851457118988}, {"text": "BERT", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9943516254425049}, {"text": "CoNLL 2003 NER", "start_pos": 183, "end_pos": 197, "type": "DATASET", "confidence": 0.9416771729787191}]}, {"text": "BioBERT: Fine-tuned BioBERT outperforms BioELMo with biLSTM and CRF on BC2GM.", "labels": [], "entities": [{"text": "biLSTM", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9436272978782654}, {"text": "BC2GM", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9210975170135498}]}, {"text": "As a feature extractor, BioBERT is slightly worse than BioELMo in probing task of BC2GM, but outperforms BioELMo in probing task of, which can be explained by the fact that BioBERT is also pre-trained on general-domain corpora.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9740359783172607}]}], "tableCaptions": [{"text": " Table 1 show that BioBERT and BioELMo in", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.47264134883880615}]}, {"text": " Table 2: NLI test results. Whole: whole model perfor- mance on MedNLI; Probe: Probing task performance  on MedNLI; Ctrl.: Probing task performance on SNLI.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9526525735855103}, {"text": "MedNLI", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9606599807739258}]}, {"text": " Table 3: Average proportion of nearest neighbor (NN) representations that belong to the same type for different  embeddings, averaged over three random seeds. Biomed w2v performs best for number-indication relations, prob- ably because it uses a vocabulary of over 5M tokens, in which about 100k are numbers. Subset accuracy denotes  the probing task performance in the subset of MedNLI test set used for this analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 317, "end_pos": 325, "type": "METRIC", "confidence": 0.6966732144355774}, {"text": "MedNLI test set", "start_pos": 381, "end_pos": 396, "type": "DATASET", "confidence": 0.9680748383204142}]}]}