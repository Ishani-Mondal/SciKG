{"title": [{"text": "Value-based Search in Execution Space for Mapping Instructions to Programs", "labels": [], "entities": []}], "abstractContent": [{"text": "Training models to map natural language instructions to programs, given target world supervision only, requires searching for good programs at training time.", "labels": [], "entities": []}, {"text": "Search is commonly done using beam search in the space of partial programs or program trees, but as the length of the instructions grows finding a good program becomes difficult.", "labels": [], "entities": []}, {"text": "In this work, we propose a search algorithm that uses the target world state, known at training time, to train a critic network that predicts the expected reward of every search state.", "labels": [], "entities": []}, {"text": "We then score search states on the beam by interpolating their expected reward with the likelihood of programs represented by the search state.", "labels": [], "entities": []}, {"text": "Moreover , we search not in the space of programs but in a more compressed state of program executions , augmented with recent entities and actions.", "labels": [], "entities": []}, {"text": "On the SCONE dataset, we show that our algorithm dramatically improves performance on all three domains compared to standard beam search and other baselines.", "labels": [], "entities": [{"text": "SCONE dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8363244533538818}]}], "introductionContent": [{"text": "Training models that can understand natural language instructions and execute them in a realworld environment is of paramount importance for communicating with virtual assistants and robots, and therefore has attracted considerable attention (.", "labels": [], "entities": []}, {"text": "A prominent approach is to cast the problem as semantic parsing, where instructions are mapped to a high-level programming language.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7537426650524139}]}, {"text": "Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable.", "labels": [], "entities": []}, {"text": "Learning from such weak supervision results in a difficult search problem at training time.", "labels": [], "entities": []}, {"text": "The model must search fora program that when executed leads to the correct target state.", "labels": [], "entities": []}, {"text": "Early work employed lexicons and grammars to constrain the search space (, but recent success of sequence-to-sequence models () shifted most of the burden to learning.", "labels": [], "entities": []}, {"text": "Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down () or bottom-up (.", "labels": [], "entities": []}, {"text": "Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways.", "labels": [], "entities": []}, {"text": "First, we capitalize on the target world state being available at training time and train a critic network that given the language instructions, current world state, and target world state estimates the expected future reward for each search state.", "labels": [], "entities": []}, {"text": "In contrast to traditional beam search where states representing partial programs are scored based on their likelihood only, we also consider expected future reward, leading to a more targeted search at training time.", "labels": [], "entities": []}, {"text": "Second, rather than search in the space of programs, we search in a more compressed execution space, where each state is defined by a partial program's execution result.", "labels": [], "entities": []}, {"text": "We evaluated our method on the SCONE dataset, which includes three different domains where long sequences of 5 instructions are mapped to programs.", "labels": [], "entities": [{"text": "SCONE dataset", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.849604606628418}]}, {"text": "We show that while standard beam search gets stuck in local optima and is unable to discover good programs for many examples, our model is able to bootstrap, improving final performance by 20 points on average.", "labels": [], "entities": []}, {"text": "We also perform extensive analysis and show that both value-based search as well as searching in execution space contribute to the final performance.", "labels": [], "entities": []}, {"text": "Our code and data are available at http://gitlab.com/ tau-nlp/vbsix-lang2program.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our method on the three domains of SCONE with the standard accuracy metric, i.e., the proportion of test examples where the predicted program has the correct denotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9958112239837646}]}, {"text": "We trained with VBSIX, and used standard beam search (K = 32) attest time for programs' generation.", "labels": [], "entities": [{"text": "VBSIX", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9729183316230774}, {"text": "beam search (K = 32) attest time", "start_pos": 41, "end_pos": 73, "type": "METRIC", "confidence": 0.7078411612245772}]}, {"text": "Each test example contains 5 utterances, and similar to prior work we reported the model accuracy on all 5 utterances as well as the first 3 utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9904329776763916}]}, {"text": "We ran each experiment 6 times with different random seeds and reported the average accuracy and standard deviation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9958840012550354}, {"text": "standard", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9870229959487915}]}, {"text": "In contrast to prior work on SCONE (, where models were trained on all sequences of 1 or 2 utterances, and thus were exposed during training to all gold intermediate states, we trained from longer sequences keeping intermediate states latent.", "labels": [], "entities": [{"text": "SCONE", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.9046333432197571}]}, {"text": "This leads to a harder search problem that was not addressed previously, but makes our results incomparable to previous results . In SCENE and TANGRAM, we used the first 4 and 5 utterances as examples.", "labels": [], "entities": [{"text": "TANGRAM", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.4559721052646637}]}, {"text": "In ALCHEMY, we used the first utterance and 5 utterances.", "labels": [], "entities": [{"text": "ALCHEMY", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.4423885941505432}]}, {"text": "Training details To warm-start the value network, we trained it fora few thousand steps, and only then start re-ranking with its predictions.", "labels": [], "entities": []}, {"text": "Moreover, we gain efficiency by first returning K 0 (=128) states with the actor score, and then re-ranking with the actor-critic score, returning K(=32) states.", "labels": [], "entities": []}, {"text": "Last, we use the value network only in the last two utterances of every example since we found it has less effect in earlier utterances where future uncertainty is large.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer () and fixed GloVe embeddings () for utterance words.", "labels": [], "entities": []}, {"text": "Baselines We evaluated the following training methods (Hyper-parameters are in appendix B): 1.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 55, "end_pos": 71, "type": "METRIC", "confidence": 0.9462341666221619}]}, {"text": "MML: Our main baseline, where search is done with beam search and training with MML.", "labels": [], "entities": [{"text": "MML", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.933165967464447}, {"text": "MML", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.8950468301773071}]}, {"text": "We used randomized beam-search, which addsgreedy exploration to beam search, which was proposed by and performed better 4 . 2. EXPERT-MML: An alternative way of using the target denotation y at training time, based on imitation learning (, is to train an expert policy \u03c0 expert \u03b8 , which receives y as input in addition to the parsing state, and trains with the MML objective.", "labels": [], "entities": [{"text": "beam search", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.8254296183586121}, {"text": "EXPERT-MML", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.8901004791259766}]}, {"text": "Then, our policy \u03c0 \u03b8 is trained using programs found by \u03c0 expert \u03b8 . The intuition is that the expert can use y to find good programs that the policy \u03c0 \u03b8 can train from.", "labels": [], "entities": []}, {"text": "3. VBSIX: Our proposed training algorithm.", "labels": [], "entities": [{"text": "VBSIX", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.938892126083374}]}, {"text": "We also evaluated REINFORCE, where MonteCarlo sampling is used as search strategy).", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9922239780426025}]}, {"text": "We followed the implementation of, who performed variance reduction with a constant baseline and added -greedy exploration.", "labels": [], "entities": [{"text": "variance reduction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7712269127368927}]}, {"text": "We found that REINFORCE fails to discover any correct programs to bootstrap from.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.5358746647834778}]}, {"text": "reports test accuracy of VBSIX compared to the baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9930241107940674}, {"text": "VBSIX", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.931369423866272}]}, {"text": "First, VBSIX outperforms all baselines in all cases.", "labels": [], "entities": []}, {"text": "MML is the strongest baseline, but even with an increased beam (K = 64), VBSIX (K = 32) surpasses it by a large margin (more than 20 points on average).", "labels": [], "entities": [{"text": "MML", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9677406549453735}, {"text": "VBSIX", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.8807225823402405}]}, {"text": "On top of the improvement inaccuracy, in ALCHEMY and TAN-GRAM the standard deviation of VBSIX is lower than the other baselines across the 6 random seeds, showing the robustness of our model.", "labels": [], "entities": [{"text": "ALCHEMY", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9268835783004761}, {"text": "TAN-GRAM", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8895647525787354}, {"text": "VBSIX", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.7970901131629944}]}, {"text": "EXPERT-MML performs worse than MML in all cases.", "labels": [], "entities": [{"text": "EXPERT-MML", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7814401984214783}]}, {"text": "We hypothesize that using the denotation y as input to the expert policy \u03c0 expert \u03b8 results in many spurious programs, i.e., they are unrelated to the utterance meaning.", "labels": [], "entities": []}, {"text": "This is since the expert can learn to perform actions that take it to the target world state while ignoring the utterances completely.", "labels": [], "entities": []}, {"text": "Such programs will lead to bad generalization of \u03c0 \u03b8 . Using a critic at training time eliminates this problem, since its scores depend on \u03c0 \u03b8 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracy and standard deviation of VBSIX compared to MML baselines (top) and our training  methods (bottom). We evaluate the same model over the first 3 and 5 utterances in each domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9743289947509766}, {"text": "VBSIX", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9261246919631958}, {"text": "MML baselines", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8117091655731201}]}, {"text": " Table 4: Test accuracy comparison to prior work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9895821809768677}]}]}