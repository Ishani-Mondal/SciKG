{"title": [{"text": "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation", "labels": [], "entities": [{"text": "Overcoming Catastrophic Forgetting", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6915419598420461}, {"text": "Domain Adaptation of Neural Machine Translation", "start_pos": 42, "end_pos": 89, "type": "TASK", "confidence": 0.7693112045526505}]}], "abstractContent": [{"text": "Continued training is an effective method for domain adaptation in neural machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7840496897697449}, {"text": "neural machine translation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.6417945623397827}]}, {"text": "However, in-domain gains from adaptation come at the expense of general-domain performance.", "labels": [], "entities": []}, {"text": "In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge.", "labels": [], "entities": []}, {"text": "To mitigate it, we adapt Elastic Weight Consolidation (EWC)-a machine learning method for learning anew task without forgetting previous tasks.", "labels": [], "entities": []}, {"text": "Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art.", "labels": [], "entities": []}, {"text": "We also explore the full range of general-domain performance available when some in-domain degradation is acceptable .", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) performs poorly without large training corpora.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8405124942461649}]}, {"text": "Domain adaptation is required when there is sufficient data in the desired language pair but insufficient data in the desired domain (the topic, genre, style or level of formality).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7905963659286499}]}, {"text": "This work focuses on the supervised domain adaptation problem where a small in-domain parallel corpus is available for training.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6431053578853607}]}, {"text": "Continued training () (also called fine-tuning), where a model is first trained on general-domain data and then domain adapted by training on in-domain data, is a popular approach in this setting as it leads to empirical improvements in the targeted domain.", "labels": [], "entities": []}, {"text": "One downside of continued training is that the adapted model's ability to translate generaldomain sentences is severely degraded during adaptation).", "labels": [], "entities": [{"text": "translate generaldomain sentences", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.8085591793060303}]}, {"text": "We interpret this drop in general-domain performance as catastrophic forgetting of general-domain translation knowledge.", "labels": [], "entities": [{"text": "general-domain translation knowledge", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.6754780014355978}]}, {"text": "Degradation of general-domain performance maybe problematic when the domain adapted NMT system is used to translate text outside its target domain, which can happen if there is a mismatch between the data available for domain-specific training and the test data.", "labels": [], "entities": []}, {"text": "Poor performance may also concern end users of these MT systems who are expecting good performance on 'easy' generic sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9807984232902527}]}, {"text": "Elastic Weight Consolidation (EWC)) is a method for training neural networks to learn anew task without forgetting previously learned tasks.", "labels": [], "entities": [{"text": "Elastic Weight Consolidation (EWC))", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7532257040341696}]}, {"text": "We extend EWC to continued training in NMT (see \u00a73): Our first task is to translate general-domain sentences, and our second is to translate domainspecific sentences (without forgetting how to translate general-domain sentences).", "labels": [], "entities": [{"text": "translate general-domain sentences", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.8303488294283549}]}, {"text": "EWC works by adding a per-parameter regularizer, based on the Fisher Information matrix, while training on the second task.", "labels": [], "entities": [{"text": "EWC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9139630794525146}]}, {"text": "At a high level, the regularization term keeps parameters which are important to general-domain performance close to the initial general-domain model values during continued training, while allowing parameters less important to general-domain performance to adapt more aggressively to the in-domain data.", "labels": [], "entities": []}, {"text": "We show that when adapting general-domain models to the domain of patents, EWC can substantially improve the retention of general-domain performance (up to 18.1 BLEU) without degrading in-domain translation quality.", "labels": [], "entities": [{"text": "retention", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9653328061103821}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9918577075004578}]}, {"text": "Our proposed method outperforms the previous state-of-the-art method) at retaining general-domain performance while adapting to anew domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our general-domain training data is the concatenation of the parallel portions of the WMT17 news translation task ( and OpenSubtitles18 () corpora.", "labels": [], "entities": [{"text": "WMT17 news translation task", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.8522024154663086}]}, {"text": "For De\u2194En and Ru\u2194En, we use newstest2017 and the final 2500 lines of OpenSubtitles as our test set.", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9530742764472961}]}, {"text": "We use newstest2016 and the penultimate 2500 lines of OpenSubtitles as the development set.", "labels": [], "entities": []}, {"text": "For Zh\u2194En, we use the final and penultimate 4000 lines of the UN portion of the WMT data and the final and penultimate 2500 lines of OpenSubtitles as our test and development sets, respectively.", "labels": [], "entities": [{"text": "UN portion of the WMT data", "start_pos": 62, "end_pos": 88, "type": "DATASET", "confidence": 0.7683035433292389}, {"text": "OpenSubtitles", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9214025139808655}]}, {"text": "We use the World Intellectual Property Organization (WIPO) COPPA-V2 corpus as our in-domain dataset.", "labels": [], "entities": [{"text": "World Intellectual Property Organization (WIPO) COPPA-V2 corpus", "start_pos": 11, "end_pos": 74, "type": "DATASET", "confidence": 0.7599434389008416}]}, {"text": "The WIPO data consist of parallel sentences from international patent application abstracts.", "labels": [], "entities": [{"text": "WIPO data", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9152125120162964}]}, {"text": "WIPO De\u2194En data are large enough to train strong indomain systems , so we truncate to 100k lines to simulate a more interesting domain adaptation scenario.", "labels": [], "entities": [{"text": "WIPO De\u2194En data", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8594711780548095}, {"text": "domain adaptation", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7425619661808014}]}, {"text": "We reserve 3000 lines each for in-domain development and test sets.", "labels": [], "entities": []}, {"text": "We apply the Moses tok- enizer () and byte-pair encoding (BPE) ().", "labels": [], "entities": [{"text": "byte-pair encoding (BPE)", "start_pos": 38, "end_pos": 62, "type": "METRIC", "confidence": 0.8622624158859253}]}, {"text": "We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k.", "labels": [], "entities": []}, {"text": "BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets.", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.4292702078819275}]}, {"text": "Token counts for corpora are shown in.", "labels": [], "entities": []}, {"text": "We implemented 5 both EWC and MCL in Sockeye (.", "labels": [], "entities": [{"text": "EWC", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.648910403251648}, {"text": "Sockeye", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9727875590324402}]}, {"text": "To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences.", "labels": [], "entities": []}, {"text": "For efficiency, we compute gradients fora batch of sentences prior to squaring and accumulating them.", "labels": [], "entities": []}, {"text": "Fisher regularization is implemented as weight decay (towards\u02c6\u03b8 towards\u02c6 towards\u02c6\u03b8 G ) in Adam (.", "labels": [], "entities": []}, {"text": "Preliminary experiments in Ru\u2192En found no meaningful difference in general-domain or indomain performance when computing the diagonal of \u00af F on varying amounts of data ranging from 500k sentences to the full dataset.", "labels": [], "entities": []}, {"text": "We also tried computing the diagonal of \u00af F on held-out data, as github.com/thompsonb/sockeye_ewc there is some evidence that estimating Fisher on held out data reduces overfitting in natural gradient descent (.", "labels": [], "entities": []}, {"text": "However, we again found no meaningful differences.", "labels": [], "entities": []}, {"text": "All results presented herein estimate the the diagonal of \u00af F on 500k training data sentences, which took less than an hour on a GTX 1080 Ti GPU.", "labels": [], "entities": [{"text": "diagonal of \u00af F", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.701056495308876}, {"text": "GTX 1080 Ti GPU", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8333885073661804}]}, {"text": "We use a two-layer LSTM network with hidden unit size 512.", "labels": [], "entities": []}, {"text": "The general-domain models are trained with a learning rate of 3E-4.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9675105810165405}]}, {"text": "We use dropout (0.1) on both RNN inputs and states.", "labels": [], "entities": []}, {"text": "We use label smoothing (0.1) for all experiments except with MCL, because MCL explicitly regularizes the output distribution.", "labels": [], "entities": []}, {"text": "MCL uses an interpolation of the cross entropy between the output distribution of the model being trained and the general-domain models output distribution (scaled by \u03b1) and the standard training loss (scaled by 1 \u2212 \u03b1).", "labels": [], "entities": [{"text": "MCL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7975138425827026}]}, {"text": "For MCL, we do a grid search over learning rates (10 \u22124 , 10 \u22125 , 10 \u22126 ) and \u03b1 values of (0.1, 0.3, 0.5, 0.7, 0.9).", "labels": [], "entities": []}, {"text": "For EWC, we do a grid search over the same learning rates and weight decay values of (10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22125 ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: General-domain BLEU for: general-domain  model prior to adaptation (GD), standard continued  training (CT), and best performing MCL and EWC  models with no in-domain degradation compared to CT  (delta from CT). Best improvement over CT bolded.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9321043491363525}]}]}