{"title": [{"text": "Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations", "labels": [], "entities": [{"text": "Syntax-Enhanced Neural Machine Translation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7297974675893784}]}], "abstractContent": [{"text": "Syntax has been demonstrated highly effective in neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8070228000481924}]}, {"text": "Previous NMT models integrate syntax by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods , which may suffer from error propagation.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel method to integrate source-side syntax implicitly for NMT.", "labels": [], "entities": []}, {"text": "The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs).", "labels": [], "entities": []}, {"text": "Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models.", "labels": [], "entities": []}, {"text": "The method can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models.", "labels": [], "entities": []}, {"text": "We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9778248369693756}]}, {"text": "In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.8134569724400839}]}, {"text": "The widely used * Corresponding author.", "labels": [], "entities": []}, {"text": "sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.8116387724876404}]}, {"text": "Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence ().", "labels": [], "entities": []}, {"text": "Recently, inspired by the success of syntaxbased SMT ( , researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (.", "labels": [], "entities": [{"text": "syntaxbased SMT", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.4807722866535187}]}, {"text": "As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees.,  and  show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models.", "labels": [], "entities": []}, {"text": "Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs.", "labels": [], "entities": []}, {"text": "Even with deliberate batching method of , our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system.", "labels": [], "entities": [{"text": "gated recurrent unit (GRU)", "start_pos": 94, "end_pos": 120, "type": "METRIC", "confidence": 0.7364905973275503}]}, {"text": "To solve the problem, Tree-Linearization is a good alternative for syntax encoding.", "labels": [], "entities": [{"text": "syntax encoding", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7120718061923981}]}, {"text": "The main idea is to linearize syntax trees into sequential symbols, and then exploit the resulting sequences as inputs for NMT.", "labels": [], "entities": []}, {"text": "propose a depth-first method to traverse a constituent tree, converting it into a sequence of symbols mixed with sentential words and syntax labels.", "labels": [], "entities": []}, {"text": "Similarly, combine several strategies of tree traversing for dependency syntax integration.", "labels": [], "entities": [{"text": "dependency syntax integration", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7845238844553629}]}, {"text": "In this work, we present an implicit syntax encoding method for NMT, enhancing NMT models by syntax-aware word representations (SAWRs).", "labels": [], "entities": []}, {"text": "illustrates the basic idea, where trees are modeled indirectly by sequential vectors extracted from an encoder-decoder dependency parser.", "labels": [], "entities": []}, {"text": "On the one hand, the method avoids the structural heterogeneity and thus can be integrated efficiently, and on the other hand, it does not require discrete 1-best tree outputs, alleviating the error propagation problem induced from syntax parsers.", "labels": [], "entities": []}, {"text": "Concretely, the vector outputs are extracted from the encoding outputs of the encoder-decoder dependency parser.", "labels": [], "entities": []}, {"text": "As shown in, the encoding outputs, denoted as o = o 1 \u00b7 \u00b7 \u00b7 o 6 , are then integrated into Seq2Seq NMT models by directly concatenated with the source input word embeddings after a linear projection.", "labels": [], "entities": []}, {"text": "We start with a Seq2Seq baseline with attention mechanism () for study, following previous studies of the same research line, and then integrate source dependency syntax by SAWRs.", "labels": [], "entities": []}, {"text": "We conduct experiments on ChineseEnglish and English-Vietnamese translation tasks, respectively.", "labels": [], "entities": [{"text": "English-Vietnamese translation tasks", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6918532351652781}]}, {"text": "The results show that our method is very effective in source syntax integration.", "labels": [], "entities": [{"text": "source syntax integration", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7409069140752157}]}, {"text": "With source dependency syntax, the performances of Chinese-English and English-Vietnamese translation can be significantly boosted by 1.74 BLEU points and 0.80 BLEU points, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9992284774780273}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9990015625953674}]}, {"text": "We also compare the method with the representative Tree-RNN and Tree-Linearization approaches of syntax integration, finding that our method is able to achieve larger improvements than the two approaches for both tasks.", "labels": [], "entities": [{"text": "syntax integration", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7176008522510529}]}, {"text": "All the codes are released publicly available at https://github.com/zhangmeishan/SYN4NMT under Apache License 2.0.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Final results of Chinese-English translation. All syntax-integrated approaches are significantly better than  the baseline system (p < 0.05).", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6282549500465393}]}, {"text": " Table 3: The influence of fine-tuning parser parameters  in the SAWR system.", "labels": [], "entities": []}, {"text": " Table 4: Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization.", "labels": [], "entities": []}, {"text": " Table 5: Final results based on the transformer. Only the SAWR results are significantly better (p < 0.05).", "labels": [], "entities": [{"text": "SAWR", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.4581909775733948}]}]}