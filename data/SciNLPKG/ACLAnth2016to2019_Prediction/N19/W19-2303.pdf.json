{"title": [{"text": "How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature", "labels": [], "entities": [{"text": "Compare Summarizers", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.6604991257190704}, {"text": "Neural Summarization", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6631026118993759}]}], "abstractContent": [{"text": "Until recently, summarization evaluations compared systems that produce summaries of the same target length.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9934448003768921}]}, {"text": "Neural approaches to summarization however have done away with length requirements.", "labels": [], "entities": [{"text": "summarization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9927896857261658}, {"text": "length", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9561141729354858}]}, {"text": "Here we present detailed experiments demonstrating that summaries of different length produced by the same system have a clear non-linear pattern of quality as measured by ROUGE F1 scores: initially steeply improving with summary length, then starting to gradually decline.", "labels": [], "entities": [{"text": "ROUGE F1 scores", "start_pos": 172, "end_pos": 187, "type": "METRIC", "confidence": 0.8527960181236267}, {"text": "summary length", "start_pos": 222, "end_pos": 236, "type": "METRIC", "confidence": 0.7676714956760406}]}, {"text": "Neural models produce summaries of different length, possibly confounding improvements of summa-rization techniques with potentially spurious learning of optimal summary length.", "labels": [], "entities": []}, {"text": "We propose anew evaluation method where ROUGE scores are normalized by those of a random system producing summaries of the same length.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9563946425914764}]}, {"text": "We reanalyze a number of recently reported results and show that some negative results are in fact reports of system improvement once differences in length are taken into account.", "labels": [], "entities": []}, {"text": "Finally, we present a small-scale human evaluation showing a similar trend of perceived quality increase with summary length, calling for the need of similar normalization in reporting human scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.", "labels": [], "entities": [{"text": "text summarization of news", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8240260034799576}]}, {"text": "This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes) or influential work advocating for fixed summary length around 85-90 words).", "labels": [], "entities": [{"text": "DUC shared task", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.7923850417137146}, {"text": "summaries", "start_pos": 68, "end_pos": 77, "type": "TASK", "confidence": 0.9679216146469116}]}, {"text": "With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned.", "labels": [], "entities": [{"text": "fixing required summary length", "start_pos": 60, "end_pos": 90, "type": "METRIC", "confidence": 0.5321237295866013}]}, {"text": "There are some exceptions (), but starting with (, systems produce summaries of variable length.", "labels": [], "entities": []}, {"text": "This trend is not necessarily bad.", "labels": [], "entities": []}, {"text": "Prior work has shown that people prefer summaries of different length depending on the information they search for ( and that variable length summaries were more effective in task-based evaluations ().", "labels": [], "entities": []}, {"text": "There are, at the same time, reasons for concern.", "labels": [], "entities": []}, {"text": "The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression; for this task a meaningful evaluation should explicitly take output length into account).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7413179874420166}]}, {"text": "For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9723934531211853}, {"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9825899004936218}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8173446655273438}]}, {"text": "Best practices for using ROUGE call for truncating the summaries to the desired length () 2 . () suggested using ROUGE F1 instead of recall, with the following justification \"full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths.", "labels": [], "entities": [{"text": "ROUGE F1", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.8361595869064331}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.997671902179718}]}, {"text": "Full-length F1 solves this problem since it can penalize longer summaries.\".", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.960871696472168}, {"text": "penalize longer summaries.", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6305787563323975}]}, {"text": "The rest of the neural summarization literature adopted F1 evaluation without further discussion.", "labels": [], "entities": [{"text": "neural summarization", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.5256185680627823}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9582454562187195}]}, {"text": "In this paper we study how ROUGE F1 scores  change with summary length, finding that in the ranges of typical lengths for neural systems it in fact does not penalize longer summaries.", "labels": [], "entities": [{"text": "ROUGE F1 scores", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.8863539695739746}]}, {"text": "We propose an alternative evaluation that appropriately normalizes ROUGE scores and reinterpret several recent results to show that not taking into account differences in length may have favored misleading conclusions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9611703157424927}, {"text": "length", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9753609895706177}]}, {"text": "We also present a pilot analysis of summary length inhuman evaluation.", "labels": [], "entities": [{"text": "summary length inhuman evaluation", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.4735633656382561}]}], "datasetContent": [{"text": "We re-test 16 systems on the CNN/DailyMail test set: (1) Pointer-Generator () and its variants: a baseline sequence-to-sequence attentional model (baseline), a Pointer-Generator model with soft switch between generating from vocabulary and copying from input (pointer-gen) and the same Pointer-Generator with coverage loss (pointer-cov) for preventing repetitive generation.", "labels": [], "entities": [{"text": "CNN/DailyMail test set", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9364669919013977}, {"text": "coverage loss (pointer-cov)", "start_pos": 309, "end_pos": 336, "type": "METRIC", "confidence": 0.880459213256836}]}, {"text": "There are three other content-selection variants proposed in () which are also based on Pointer-Generator: (i) aligning ref-erence with source article (mask-hi, mask-lo) (ii) training tagger and summarizer at the same time (multitask), and (iii) a differentiable model with a soft mask predicted by selection probabilities.", "labels": [], "entities": []}, {"text": "(2) Abstractive system with bottom-up attention (bottom-up) () and the same model using Transformer (BU trans) (.", "labels": [], "entities": []}, {"text": "(3) Neural latent extractive model (latent ext) and the same model with compression over the extracted sentences (latent cmpr) (.", "labels": [], "entities": []}, {"text": "This setting is important to study, because compression naturally produces a shorter summary and a meaningful analysis of the effect is needed.", "labels": [], "entities": []}, {"text": "(4) TextRank system used in previous section, with maximum summary length set to 50 and 70.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7845547199249268}, {"text": "summary length", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.8174677491188049}]}, {"text": "(5) Lead-3 related systems: the first 3 sentences of each article (lead3); compressed first 3 sentences of each article which has length of corresponding pointer-gen (lead-pointer) and pointercov (lead-cov) output, similar to (3).", "labels": [], "entities": []}, {"text": "The compression model we used is a Pointer-Generator trained on 1160401 aligned sentence/reference pairs extracted from CNN/DailyMail training data and Annotated Gigaword (AGIGA) (.", "labels": [], "entities": [{"text": "CNN/DailyMail training data", "start_pos": 120, "end_pos": 147, "type": "DATASET", "confidence": 0.9067395329475403}]}, {"text": "We extract the pairs from CNN/DailyMail when every token from the summary sentence can be found in the article sentence.", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9082936843236288}]}, {"text": "The pairs are extracted from AGIGA when over 70% tokens of a lead sentence are also in the headline.", "labels": [], "entities": [{"text": "AGIGA", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.6466155052185059}]}, {"text": "The minimum and maximum decoding step are set to be equal so that the output lengths are fixed.", "labels": [], "entities": []}, {"text": "Specifically, let c i be the length of a summary produced by pointer-gen, l i be the length of lead 3 sentences for the same article and l (j) i be the length of j th sentence (j \u2264 3).", "labels": [], "entities": []}, {"text": "The j th lead sentence is forced to have output length of l (j) i c i /l i tokens.", "labels": [], "entities": []}, {"text": "The average number of tokens are not exactly the same since the size after scaling maybe off by at most 1 token.", "labels": [], "entities": []}, {"text": "The random scores are the average over n activations of random systems introduced in \u00a72 (n = 10 in our setting).", "labels": [], "entities": []}, {"text": "The instability of random systems can be mitigated by setting n to be large enough.", "labels": [], "entities": []}, {"text": "Besides, the average overlarge amounts of test articles can also weaken this issue since we focus on system-level comparison instead of input-level.", "labels": [], "entities": []}, {"text": "Given a system output length, we use linear interpolation of the two closest points to estimate the ROUGE score of a random system which has the same average output length.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.9846839606761932}]}, {"text": "5 shows the average length of summaries produced by each system, the system ROUGE-1 F1 score, the corresponding ROUGE-1 F1 score of a random system with the same average summary length, and the proposed normalized ROUGE-1 evaluation score.", "labels": [], "entities": [{"text": "ROUGE-1 F1 score", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.8870563904444376}, {"text": "ROUGE-1 F1 score", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.8288275798161825}]}, {"text": "The bottom of the table gives the sum of absolute system rank change with respect to the ordering by summary length and correlations between corresponding values with summary length.", "labels": [], "entities": []}, {"text": "All systems produce summaries in the 43-85 word range, where we already established that ROUGE F1 increases steeply with summary length.", "labels": [], "entities": [{"text": "ROUGE F1", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8825930953025818}]}, {"text": "Another important observation is that the scores of random systems follow exactly the ordering by length; here summary length alone is responsible for the over 5 ROUGE point improvement.", "labels": [], "entities": [{"text": "ROUGE point", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9699322879314423}]}, {"text": "Next to notice is that the normalization leads to about double the difference in rank change with respect to length than regular ROUGE F1.", "labels": [], "entities": [{"text": "rank change", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9144180417060852}, {"text": "ROUGE F1", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9471717774868011}]}, {"text": "Hence, these scores give information about summary quality that is less related to summary length.", "labels": [], "entities": []}, {"text": "Now we get to revisit some of the conclusions drawn solely from ROUGE scores, without taking summary length into account.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9242933392524719}]}, {"text": "Many of the neural abstractive systems produce outputs with scores worse than the lead3 baseline.", "labels": [], "entities": []}, {"text": "However this baseline results in the longest summaries.", "labels": [], "entities": []}, {"text": "Moreover, after normalization, it becomes clear that lead3 is in fact considerably worse than pointer-cov.", "labels": [], "entities": []}, {"text": "As presented in, the TextRank system with summary length of 70 has better ROUGE scores than the same system with summary length of 50.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9981300234794617}]}, {"text": "Once these are normalized, however, the system with shorter summaries appears to be more effective (6 points better in normalized score).", "labels": [], "entities": []}, {"text": "Finally, we compare the two pairs of extractive systems as well as their versions in which the extracted sentences are compressed.", "labels": [], "entities": []}, {"text": "The compressed summaries are about 40 words shorter for the systems in (3) and 30 words shorter in (5).", "labels": [], "entities": []}, {"text": "Plain ROUGE scores decidedly indicate that compression worsens system performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9868035912513733}]}, {"text": "When normalized however, latent cmpr emerges as the third most effective system, immediately follow the bottomup systems ().", "labels": [], "entities": [{"text": "latent cmpr", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.677104264497757}]}, {"text": "This is not the case for the simplistic compression variant in lead3, which produces shorter summaries but barely changes its rank in the normalized score ranking.", "labels": [], "entities": []}, {"text": "Finally, we compare the systems that reported outperforming the lead3 baseline.", "labels": [], "entities": []}, {"text": "The latent ext system results in summaries very similar in length to lead3.", "labels": [], "entities": [{"text": "length", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9535655975341797}]}, {"text": "Given previous analysis, one might think the ROUGE improvement is due to summary length.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9929859042167664}, {"text": "length", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.5970140695571899}]}, {"text": "However, the normalized score shows that this is not the case and that the latent ext is indeed better than lead3.", "labels": [], "entities": [{"text": "latent ext", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.8864525556564331}]}, {"text": "Even more impressive is the analysis of the bottom-up system, which has better ROUGE scores than lead even though it produces shorter summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.998139500617981}]}, {"text": "It keeps its first place position even after normalization.", "labels": [], "entities": []}, {"text": "Overall, the analyses we present provide compelling evidence for the importance of summary length on system evaluation.", "labels": [], "entities": []}, {"text": "Relying only on ROUGE would at times confound improvement in content selection with the learned ability to generate longer summaries. and summary length LE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9497441053390503}, {"text": "content selection", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7016776502132416}, {"text": "summary length LE", "start_pos": 138, "end_pos": 155, "type": "METRIC", "confidence": 0.7262050906817118}]}, {"text": "Each dimension is the same as in.", "labels": [], "entities": []}, {"text": "Entries with p-value smaller than 0.05 are marked with * .  We also conduct a pilot human evaluation experiment using the same data as in ().", "labels": [], "entities": []}, {"text": "The human evaluation data are 60 articles from the Newsroom test set and summaries generated by seven systems.", "labels": [], "entities": [{"text": "Newsroom test set", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9870672225952148}]}, {"text": "These are (1) extractive systems: first three sentences of the article (lead3), textrank with word limit of 50 (textrank) and the 'fragments' system (frag) representing the best performance an extractive system can achieve.", "labels": [], "entities": []}, {"text": "(2) an abstractive system (Rush et al., 2015) (abstractive) trained on Newsroom data and (3) systems with mixed strategies: Pointer-Generator trained: Average informativeness and verbosity rating for lead system with max length of 50, 70, 90 and 110.", "labels": [], "entities": [{"text": "Newsroom data", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9750377833843231}, {"text": "max length", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.9002773463726044}]}, {"text": "on CNN/DailyMail data set (ptr c), on subset of Newsroom training set (ptr s) and a subset of Newsroom training data (ptr n).", "labels": [], "entities": [{"text": "CNN/DailyMail data set", "start_pos": 3, "end_pos": 25, "type": "DATASET", "confidence": 0.9370111107826233}, {"text": "Newsroom training set", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9579527179400126}, {"text": "Newsroom training data", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.934282143910726}]}, {"text": "After examining the outputs of each system, the abstractive system was excluded because the model was not properly trained.", "labels": [], "entities": []}, {"text": "Human evaluation results for each system are shown in.", "labels": [], "entities": []}, {"text": "We ask annotators to rate six aspects of summary content quality informativeness (IN), relevance (RL), verbosity (VE), unnecessary content (UC), making people want to continue reading the original article after reading the summary (CN) and being a sufficient substitute for the original article (SR) and compute the correlation among these dimensions as well as with summary length.", "labels": [], "entities": [{"text": "relevance (RL)", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9515601694583893}, {"text": "verbosity (VE)", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.658444806933403}, {"text": "unnecessary content (UC)", "start_pos": 119, "end_pos": 143, "type": "METRIC", "confidence": 0.6684686422348023}]}, {"text": "Instead of rating in the range of 1 to 5 as in the original article, we ask the workers to rate in a range of 1 to 7, with higher value corresponds to summary is informative and relevant to the source article, not verbose, has no unnecessary content, much information to be attained after reading summary and can serve as a perfect surrogate to the article.", "labels": [], "entities": []}, {"text": "The correlation among six aspects and with summary length are shown in table 4.", "labels": [], "entities": [{"text": "summary length", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.6576284766197205}]}, {"text": "Some of the newly introduced questions, such as unnecessary content and verbosity, were intended to capture aspects of the summary which may favor shorter summaries.", "labels": [], "entities": []}, {"text": "Relevance is the score introduced in the original () study and measures to faithfulness of content, as neural systems tend to include summary content that is not supported by the original article being summarized.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9884646534919739}]}, {"text": "We find that in general people favor systems that produce longer summaries.", "labels": [], "entities": []}, {"text": "However, similar to our initial experiment with ROUGE, there is noway to know if the improvement is due simply to the longer length, in which more content can be presented, or in the content selection capabilities of the system.", "labels": [], "entities": []}, {"text": "The highest correlation between summary length and a human rating is that for informativeness, which in hindsight is completely intuitive because the longer the summary,  the more information it includes.", "labels": [], "entities": [{"text": "length", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.5221879482269287}]}, {"text": "The exact same informativeness definition is used for the Newsroom leaderboard ( . Clearly, a meaningful interpretation of the human scores will require normalization similar to the one we presented for ROUGE, with human ratings for random or lead summaries of different length, so the overall effectiveness of the system over these is measured in evaluation.", "labels": [], "entities": [{"text": "Newsroom leaderboard", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9742828607559204}, {"text": "ROUGE", "start_pos": 203, "end_pos": 208, "type": "DATASET", "confidence": 0.5391760468482971}]}, {"text": "To mirror the analysis of ROUGE scores, we conduct another experiment where we present the workers with lead system of max length 50, 70, 90 and 110 as well as the reference.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9825242161750793}, {"text": "max length 50", "start_pos": 119, "end_pos": 132, "type": "METRIC", "confidence": 0.9287580053011576}]}, {"text": "Complete sentences are extracted so that readability is maintained.", "labels": [], "entities": []}, {"text": "Each HIT is assigned to 3 workers and only contains one summary-reference pair.", "labels": [], "entities": []}, {"text": "The average length of these four systems are 38.0, 53.4, 75.1, 92.5 respectively.", "labels": [], "entities": []}, {"text": "Workers are told that they may assume the reference summary captures all key points of the article, then we ask them to rate the informativeness and verbosity question again.", "labels": [], "entities": []}, {"text": "Average ratings for each length can be seen in Table 5.", "labels": [], "entities": []}, {"text": "Much like ROUGE, human evaluation of informativeness is also confounded by summary length and requires normalization for meaningful evaluation.", "labels": [], "entities": []}, {"text": "We normalize the original human ratings for each system with the interpolated (IN) rating in table 5 and present it in table 6.", "labels": [], "entities": [{"text": "interpolated (IN) rating", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.916742479801178}]}, {"text": "We also evaluated how the verbosity score behaves when applied to summaries of that length.", "labels": [], "entities": []}, {"text": "We chose that because it has the lowest overall correlation with the informativeness and relevance evaluations introduced in prior work.", "labels": [], "entities": []}, {"text": "Its (and its related evaluation of unnecessary content) correlation with length is not significant but still appears high.", "labels": [], "entities": [{"text": "length", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9918828010559082}]}, {"text": "Better sense of the relationship can be obtained in future work when a larger number of system can be evaluated.", "labels": [], "entities": []}, {"text": "Unlike informativeness, verbosity human scores fluctuate with length, increasing and decreasing without clear pattern.", "labels": [], "entities": []}, {"text": "This suggests future human evaluations should involve more similar judgments likely to capture precision in content selection, which are currently missing in the field.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.997836172580719}, {"text": "content selection", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7487435042858124}]}], "tableCaptions": [{"text": " Table 1: System performance on the CNN/DailyMail  test set, including average summary length, system  ROUGE-1 F1 score, ROUGE-1 F1 for the random sys- tem with same average length. Systems are ordered  by length. Values in the last three columns are sub- scripted by the difference in rank when sorted by cor- responding item as compared to when sorted by length.  In the bottom of the table, we show the sum of ab- solute rank change, Spearman and Pearson correlation  between corresponding values and length.", "labels": [], "entities": [{"text": "CNN/DailyMail  test set", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.9470781087875366}, {"text": "average summary length", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.7794449726740519}, {"text": "ROUGE-1 F1 score", "start_pos": 103, "end_pos": 119, "type": "METRIC", "confidence": 0.8348240852355957}, {"text": "ROUGE-1 F1", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.8129485547542572}, {"text": "ab- solute rank change", "start_pos": 413, "end_pos": 435, "type": "METRIC", "confidence": 0.7477360248565674}, {"text": "Pearson correlation", "start_pos": 450, "end_pos": 469, "type": "METRIC", "confidence": 0.8879891335964203}]}, {"text": " Table 2: Prompts presented to Amazon Mechanical  Turk workers", "labels": [], "entities": [{"text": "Amazon Mechanical  Turk", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.9318263729413351}]}, {"text": " Table 4: Correlation among the six human rating di- mensions defined in", "labels": [], "entities": []}, {"text": " Table 3. Entries with  p-value smaller than 0.05 are marked with  *  .", "labels": [], "entities": []}, {"text": " Table 5: Average informativeness and verbosity rating  for lead system with max length of 50, 70, 90 and 110.", "labels": [], "entities": [{"text": "max length", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9121091663837433}]}, {"text": " Table 6: Human ratings normalized by interpolated in- formativeness rating in table 5.", "labels": [], "entities": []}, {"text": " Table 7: System performance on the CNN/DailyMail  test set, including average summary length, system  ROUGE-1 F1 score, ROUGE-1 F1 for the random sys- tem with same average length. Systems are ordered  by length. Values in the last three columns are sub- scripted by the difference in rank when sorted by cor- responding item as compared to when sorted by length.", "labels": [], "entities": [{"text": "CNN/DailyMail  test set", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.9500107407569885}, {"text": "average summary length", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.7983390887578329}, {"text": "ROUGE-1 F1 score", "start_pos": 103, "end_pos": 119, "type": "METRIC", "confidence": 0.8573346535364786}, {"text": "ROUGE-1", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9954413175582886}, {"text": "F1", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.5313512086868286}]}]}