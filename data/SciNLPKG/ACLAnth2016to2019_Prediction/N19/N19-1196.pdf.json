{"title": [{"text": "Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models", "labels": [], "entities": [{"text": "Generating Knowledge Graph Paths from Textual Definitions", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7661890132086617}]}], "abstractContent": [{"text": "We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem.", "labels": [], "entities": []}, {"text": "Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the target node following hypernym-hyponym relationships.", "labels": [], "entities": []}, {"text": "In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully in-terpretable in the context of the underlying on-tology, in an end-to-end manner.", "labels": [], "entities": []}, {"text": "We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text-to-entity mapping is the task of associating a text with a concept in a knowledge graph or an ontology (we use two terms, interchangeably).", "labels": [], "entities": [{"text": "Text-to-entity mapping", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6814569681882858}]}, {"text": "Recent works () use neural networks to project a text to a vector space where the entities of a KG are represented as continuous vectors.", "labels": [], "entities": []}, {"text": "Despite being successful, these models have two main disadvantages.", "labels": [], "entities": []}, {"text": "First, they rely on a predefined vector space which is used as a gold standard representation for the entities in a KG.", "labels": [], "entities": []}, {"text": "Therefore, the quality of these algorithms depends on how well the vector space is represented.", "labels": [], "entities": []}, {"text": "Second, these algorithms are not interpretable; hence, it is impossible to understand why a certain text was linked to a particular entity.", "labels": [], "entities": []}, {"text": "To address these issues we propose a novel technique which first represents an ontology concept as a sequence of its ancestors in the ontology (hypernyms) and then maps the corresponding textual description to this unique representation.", "labels": [], "entities": []}, {"text": "For example, given the textual description of the concept swift (\"small bird that resembles a swallow and is noted for its rapid flight\"), we map it to the hierarchical sequence of entities in a lexical ontology: animal \u2192 chordate \u2192 vertebrate \u2192 bird \u2192 apodiform bird.", "labels": [], "entities": []}, {"text": "This sequence of nodes constitutes a path.", "labels": [], "entities": []}, {"text": "Our model is based on a sequence-to-sequence neural network) coupled with an attention mechanism ().", "labels": [], "entities": []}, {"text": "Specifically, we use an LSTM) encoder to project the textual description into a vector space and an LSTM decoder to predict the sequence of entities that are relevant to this definition.", "labels": [], "entities": []}, {"text": "With this framework we do not need to rely on the pre-existing vector space of the entities, since the decoder explicitly learns topological dependencies between the entities of the ontology.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed model is more interpretable for two reasons.", "labels": [], "entities": []}, {"text": "First, instead of the closest points in a vector space, it outputs paths; therefore, we can trace all predictions the model makes.", "labels": [], "entities": []}, {"text": "Second, the attention mechanism allows to visualise which words in a textual description the model selects while predicting a specific concept in the path.", "labels": [], "entities": []}, {"text": "In this paper, we consider rooted tree graphs 2 only and leave the extension of the algorithm for more generic graphs to future work.", "labels": [], "entities": []}, {"text": "We evaluate the ability of our model in generating graph paths for previously unseen textual definitions on seven ontologies (Section 3).", "labels": [], "entities": []}, {"text": "We show that our technique either outperforms or performs on a par with a competitive multi-sense LSTM model () by better utilising external information in the form of word embeddings.", "labels": [], "entities": []}, {"text": "The code and resources for the paper can be found at https://github.com/VictorProkhorov/ Text2Path.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with seven graphs four of which are related to the bio-medical domain: Phenotype And Trait Ontology 3 (PATO), Human Disease Ontology (, Human Phenotype Ontology ( and Gene Ontology 4.", "labels": [], "entities": []}, {"text": "The other three graphs, i.e. WN animal.n.01 5 , WN plant.n.02 and WN entity.n.01 are subgraphs of the WordNet 3.0.", "labels": [], "entities": [{"text": "WordNet 3.0", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9138399064540863}]}, {"text": "We present the statistics of the graphs in.", "labels": [], "entities": []}, {"text": "All the ontologies we experimented with are represented as directed acyclic graphs (DAGs).", "labels": [], "entities": []}, {"text": "This creates an ambiguity for node path definitions since there are multiple pathways from a root concept to other concepts.", "labels": [], "entities": []}, {"text": "We have assumed that a single unambiguous pathway will reduce the complexity of the problem and leave the comparison with ambiguous pathways (which would inevitably involve a more complex model) to future work.", "labels": [], "entities": []}, {"text": "To convert a DAG to a tree: Statistics of the Graphs.", "labels": [], "entities": []}, {"text": "|V| is the number of nodes, depth is the path length from the root of a graph to anode, branch is the number of neighbours anode has (leaves were removed from the calculation).", "labels": [], "entities": []}, {"text": "The first value in the parentheses corresponds to the average and the second to the maximum value.", "labels": [], "entities": []}, {"text": "A.D stands for average number of decisions the model makes to infer a path, i.e A.D = average depth \u00d7 average branch.", "labels": [], "entities": [{"text": "A.D", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9902768135070801}, {"text": "A.D", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.991778552532196}]}, {"text": "we constrain each entity to have only one parent node.", "labels": [], "entities": []}, {"text": "The edges between the other parent nodes are removed.", "labels": [], "entities": []}, {"text": "We also experiment with two path representations.", "labels": [], "entities": []}, {"text": "Our first approach, text2nodes, uses the label of an entity (cf. Section 1) to represent a path.", "labels": [], "entities": []}, {"text": "This is not efficient since the decoder of the model needs to select between all of the entities in an ontology and also requires more parameters in the model.", "labels": [], "entities": []}, {"text": "Our second approach, text2edges, to reduce the number of symbols for the model to choose from, uses edges to represent the path.", "labels": [], "entities": []}, {"text": "To do this we create an artificial vocabulary of the size \u2206(G), where \u2206(G) corresponds to the maximum degree of anode.", "labels": [], "entities": []}, {"text": "Each edge in the graph is labeled using the artificial vocabulary.", "labels": [], "entities": []}, {"text": "For the example in Section 1, the path would be an- \u2192 apodiform bird where {a,b,c,d} is the artificial vocabulary.", "labels": [], "entities": []}, {"text": "In the resulting path we discard labels for the entities; therefore, the path reduces to:  To perform evaluation of the models described above we used Ancestor-F1 score (.", "labels": [], "entities": [{"text": "Ancestor-F1 score", "start_pos": 151, "end_pos": 168, "type": "METRIC", "confidence": 0.7852071821689606}]}, {"text": "This metric compares the ancestors (is \u2212 a model ) of the predicted node with the ancestors (is \u2212 a gold ) of the gold node in the taxonomy.", "labels": [], "entities": []}, {"text": "where P and R are precision and recall, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996942281723022}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9989830851554871}]}, {"text": "The Ancestor-F1 is then defined as:  To verify the reliability of our model on text-toentity mapping we did a set of experiments on the seven graphs (Section 3) where we map a textual definition of a concept to a path.", "labels": [], "entities": []}, {"text": "To conduct the experiments we randomly sampled 10% of leaves from the graph.", "labels": [], "entities": []}, {"text": "From this sample, 90% are used to evaluate the model and 10% are used to tune the model.", "labels": [], "entities": []}, {"text": "The remaining nodes in the graph are used for training.", "labels": [], "entities": []}, {"text": "We sample leaves for two reasons: (1) to predict a leaf, the model needs to make the maximum number of (correct) predictions and   to emphasize that this is disadvantageous for our model, since all the symbols in the path are predicted by it and in the case of the baselines only a single node is predicted.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Models that are in the last three rows of use pre-trained word embeddings () in the encoder.", "labels": [], "entities": []}, {"text": "MS-LSTM and our models that are above the last three rows use randomly initialised word vectors.", "labels": [], "entities": [{"text": "MS-LSTM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9752097725868225}]}, {"text": "We had four observations: (1) without pre-trained word embeddings in the encoder our model outperforms the best MS-LSTM \u03bb = 0.5 only on two of the seven graphs, (2) the text2edges * model outperforms all the other models including MS-LSTM * \u03bb=0.5 , (3) the text2edges model can better exploit pre-trained word embeddings than MS-LSTM, (4) our model performs better when the paths are represented using edges (rather than nodes).", "labels": [], "entities": []}, {"text": "We also found that there is a strong negative correlation (Spearman: \u22120.75, Pearson: \u22120.80) between A.D. and the Ancestor F1 score for the text2edges * model, meaning that with an increase in A.D. the Ancestor F1 score decreases.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9819658398628235}, {"text": "Pearson: \u22120.80)", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.9485320448875427}, {"text": "Ancestor F1 score", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.6895851890246073}, {"text": "Ancestor F1 score", "start_pos": 201, "end_pos": 218, "type": "METRIC", "confidence": 0.7047780950864156}]}], "tableCaptions": [{"text": " Table 1: Statistics of the Graphs. |V| is the number of  nodes, depth is the path length from the root of a graph  to a node, branch is the number of neighbours a node  has (leaves were removed from the calculation). The  first value in the parentheses corresponds to the average  and the second to the maximum value. A.D stands for  average number of decisions the model makes to infer  a path, i.e A.D = average depth \u00d7 average branch.", "labels": [], "entities": [{"text": "A.D", "start_pos": 319, "end_pos": 322, "type": "METRIC", "confidence": 0.9891093969345093}, {"text": "A.D", "start_pos": 401, "end_pos": 404, "type": "METRIC", "confidence": 0.9791694283485413}]}, {"text": " Table 2: Ancestor F1 results. Numbers in bold represent the best performing system on a graph. Models marked  with  *  make use of pre-trained word embedding in their encoder. Lambda (\u03bb) is defined in Section 3.1. We  use the same number of epochs, batch size and number of latent dimensions both for MS-LSTM and our models  (Appendix C).", "labels": [], "entities": []}, {"text": " Table 3: Statistics of nodes with multiple inheritance.  Mult.P % stands for percentage of nodes with more than  one parent node. AV.P stands for average number of  parents a node with multiple inheritance has.", "labels": [], "entities": [{"text": "Mult.P", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9976255297660828}, {"text": "AV.P", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9935234785079956}]}, {"text": " Table 4: Statistics of invalid sequences. Invalid % is the  percentage of invalid sequences and N total is the total  number of sequences that were tested.", "labels": [], "entities": [{"text": "Invalid", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9969645142555237}]}]}