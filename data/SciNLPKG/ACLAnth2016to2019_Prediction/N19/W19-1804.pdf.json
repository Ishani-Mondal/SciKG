{"title": [], "abstractContent": [{"text": "We revisit a particular visual grounding method: the \"Image Retrieval Using Scene Graphs\" (IRSG) system of Johnson et al.", "labels": [], "entities": [{"text": "Image Retrieval Using Scene Graphs\" (IRSG)", "start_pos": 54, "end_pos": 96, "type": "TASK", "confidence": 0.8133121397760179}]}, {"text": "Our experiments indicate that the system does not effectively use its learned object-relationship models.", "labels": [], "entities": []}, {"text": "We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it.", "labels": [], "entities": [{"text": "IRSG dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.955175518989563}, {"text": "Visual Relationship Dataset (VRD)", "start_pos": 69, "end_pos": 102, "type": "DATASET", "confidence": 0.7117466777563095}]}, {"text": "We find that these datasets exhibit biases that allow methods that ignore relationships to perform relatively well.", "labels": [], "entities": []}, {"text": "We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed.", "labels": [], "entities": [{"text": "IRSG dataset", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9499349296092987}]}, {"text": "Our studies contribute to a more general effort: that of better understanding what machine learning methods that combine language and vision actually learn and what popular datasets actually test.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual grounding is the general task of locating the components of a structured description in an image.", "labels": [], "entities": [{"text": "Visual grounding", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7024874091148376}]}, {"text": "In the visual-grounding literature, the structured description is often a natural-language phrase that has been parsed as a scene graph or as a subject-predicate-object triple.", "labels": [], "entities": []}, {"text": "As one example of a visual-grounding challenge, illustrates the \"Image Retrieval using Scene Graphs\" (IRSG) task.", "labels": [], "entities": [{"text": "Image Retrieval using Scene Graphs\" (IRSG) task", "start_pos": 65, "end_pos": 112, "type": "TASK", "confidence": 0.8181338608264923}]}, {"text": "Here the sentence \"A standing woman wearing dark sunglasses\" is converted to a scene-graph representation (right) with nodes corresponding to objects, attributes, and relationships.", "labels": [], "entities": []}, {"text": "Given a scene graph and an input image, the grounding task is to create bounding boxes corresponding to the specified objects, such that the located objects have the specified attributes and relationships.", "labels": [], "entities": []}, {"text": "A final energy score reflects the quality of the match between the scene graph and the located boxes (lower is better), and can be used to rank images in a retrieval task.", "labels": [], "entities": []}, {"text": "A second example of visual grounding, illustrated in, is the \"Referring Relationships\" (RR) task of.", "labels": [], "entities": [{"text": "Referring Relationships\" (RR) task", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7082605532237461}]}, {"text": "Here, a sentence (e.g., \"A horse following a person\") is represented as a subject-predicate-object triple (\"horse\", \"following\", \"person\").", "labels": [], "entities": []}, {"text": "Given a triple and an input image, the task is to create bounding boxes corresponding to the named subject and object, such that the located boxes fit the specified predicate.", "labels": [], "entities": []}, {"text": "Visual grounding tasks-at the intersection of vision and language-have become a popular area of research in machine learning, with the potential of improving automated image editing, captioning, retrieval, and question-answering, among other tasks.", "labels": [], "entities": [{"text": "image editing", "start_pos": 168, "end_pos": 181, "type": "TASK", "confidence": 0.7110166102647781}]}, {"text": "While deep neural networks have produced impressive progress in object detection, visualgrounding tasks remain highly challenging.", "labels": [], "entities": [{"text": "object detection", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.8376081883907318}]}, {"text": "On the language side, accurately transforming a natural language phrase to a structured description can be difficult.", "labels": [], "entities": []}, {"text": "On the vision side, the challenge is to learn-in away that can be generalized-visual features of objects and attributes as well as flexible models of spatial and other relationships, and then to apply these models to figure out which of a given object class (e.g., woman) is the one referred to, sometimes locating small objects and recog-  An example of the referring-relationship-grounding task of.", "labels": [], "entities": []}, {"text": "Right: A phrase broken into subject, predicate, and object categories.", "labels": [], "entities": []}, {"text": "Left: a candidate grounding of subject and object in a test image.", "labels": [], "entities": []}, {"text": "nizing hard-to-see attributes (e.g., dark vs. clear glasses).", "labels": [], "entities": []}, {"text": "To date, the performance of machine learning systems on visual-grounding tasks with real-world datasets has been relatively low compared to human performance.", "labels": [], "entities": []}, {"text": "In addition, some in the machine-vision community have questioned the effectiveness of popular datasets that have been developed to evaluate the performance of systems on visual grounding tasks like the ones illustrated in.", "labels": [], "entities": []}, {"text": "Recently showed that for the widely used dataset Google-Ref (, the task of grounding referring expressions has exploitable biases: for example, a system that predicts only object categories-ignoring relationships and attributes-still performs well on this task.", "labels": [], "entities": []}, {"text": "report related biases in visual question-answering datasets.", "labels": [], "entities": []}, {"text": "In this paper we re-examine the visual grounding approach of to determine how well this system is actually performing scene-graph grounding.", "labels": [], "entities": []}, {"text": "In particular, we compare this system with a simple baseline method to test if the original system is using information from object relationships, as claimed by.", "labels": [], "entities": []}, {"text": "In addition, we investigate possible biases and other problems with the dataset used by, aversion of which has also been used in many later studies.", "labels": [], "entities": [{"text": "aversion", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9930068254470825}]}, {"text": "We briefly survey related work in visual grounding, and discuss possible future studies in this area.", "labels": [], "entities": [{"text": "visual grounding", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7179819047451019}]}], "datasetContent": [], "tableCaptions": []}