{"title": [{"text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9839937686920166}, {"text": "Language Understanding", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7281696796417236}]}], "abstractContent": [{"text": "We introduce anew language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.997844934463501}]}, {"text": "Unlike recent language representation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8249421119689941}]}, {"text": "As a result , the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models fora wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8891775012016296}, {"text": "question answering", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.8426329791545868}, {"text": "language inference", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.6570728868246078}]}, {"text": "BERT is conceptually simple and empirically powerful.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8912606239318848}]}, {"text": "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "labels": [], "entities": [{"text": "GLUE score", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9056874215602875}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.8580521941184998}, {"text": "SQuAD v1.1 question answering Test F1", "start_pos": 218, "end_pos": 255, "type": "TASK", "confidence": 0.6083075304826101}, {"text": "F1", "start_pos": 317, "end_pos": 319, "type": "METRIC", "confidence": 0.7120439410209656}]}], "introductionContent": [{"text": "Language model pre-training has been shown to be effective for improving many natural language processing tasks).", "labels": [], "entities": []}, {"text": "These include sentence-level tasks such as natural language inference) and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 215, "end_pos": 239, "type": "TASK", "confidence": 0.6285439928372701}, {"text": "question answering", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.7944940328598022}]}, {"text": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.", "labels": [], "entities": []}, {"text": "The feature-based approach, such as ELMo (), uses task-specific architectures that include the pre-trained representations as additional features.", "labels": [], "entities": []}, {"text": "The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.", "labels": [], "entities": []}, {"text": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.", "labels": [], "entities": []}, {"text": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.", "labels": [], "entities": []}, {"text": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.", "labels": [], "entities": []}, {"text": "For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (.", "labels": [], "entities": [{"text": "OpenAI GPT", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.7957239151000977}]}, {"text": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.819812148809433}]}, {"text": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9966574907302856}]}, {"text": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7725324630737305}]}, {"text": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.", "labels": [], "entities": []}, {"text": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.", "labels": [], "entities": [{"text": "MLM", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9423627853393555}]}, {"text": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.", "labels": [], "entities": [{"text": "next sentence prediction", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6809596021970113}]}, {"text": "The contributions of our paper are as follows: \u2022 We demonstrate the importance of bidirectional pre-training for language representations.", "labels": [], "entities": []}, {"text": "Unlike, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.49676916003227234}]}, {"text": "This is also in contrast to, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.", "labels": [], "entities": []}, {"text": "\u2022 We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.", "labels": [], "entities": []}, {"text": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9204918742179871}]}, {"text": "\u2022 BERT advances the state of the art for eleven NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9937054514884949}]}, {"text": "The code and pre-trained models are available at https://github.com/ google-research/bert.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present BERT fine-tuning results on 11 NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9958471655845642}]}, {"text": "The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in: MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task (.", "labels": [], "entities": [{"text": "GLUE benchmark", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7557976841926575}, {"text": "MNLI Multi-Genre Natural Language Inference", "start_pos": 109, "end_pos": 152, "type": "TASK", "confidence": 0.6463664531707763}, {"text": "crowdsourced entailment classification task", "start_pos": 171, "end_pos": 214, "type": "TASK", "confidence": 0.7028777301311493}]}, {"text": "Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.", "labels": [], "entities": []}, {"text": "QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent (.", "labels": [], "entities": [{"text": "QQP Quora Question", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9149030049641927}]}, {"text": "SST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews BERT E E 1 E ...", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8253495891888937}, {"text": "single-sentence classification task consisting of sentences extracted from movie reviews", "start_pos": 50, "end_pos": 138, "type": "TASK", "confidence": 0.8143513381481171}, {"text": "BERT E E 1 E", "start_pos": 139, "end_pos": 151, "type": "METRIC", "confidence": 0.9306124210357666}]}, {"text": "[CLS] Tok 1 [CLS] Tok 1 ... with human annotations of their sentiment).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).  The number below each task denotes the number of training examples. The \"Average\" column is slightly different  than the official GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and  accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.88194340467453}, {"text": "WNLI set", "start_pos": 278, "end_pos": 286, "type": "DATASET", "confidence": 0.9515124261379242}, {"text": "BERT", "start_pos": 290, "end_pos": 294, "type": "METRIC", "confidence": 0.9854302406311035}, {"text": "OpenAI GPT", "start_pos": 299, "end_pos": 309, "type": "DATASET", "confidence": 0.9202834665775299}, {"text": "F1", "start_pos": 342, "end_pos": 344, "type": "METRIC", "confidence": 0.9977935552597046}, {"text": "accuracy", "start_pos": 434, "end_pos": 442, "type": "METRIC", "confidence": 0.9989442229270935}, {"text": "BERT", "start_pos": 512, "end_pos": 516, "type": "METRIC", "confidence": 0.9832618236541748}]}, {"text": " Table 2: SQuAD 1.1 results. The BERT ensemble  is 7x systems which use different pre-training check- points and fine-tuning seeds.", "labels": [], "entities": [{"text": "BERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9843270182609558}]}, {"text": " Table 3: SQuAD 2.0 results. We exclude entries that  use BERT as one of their components.", "labels": [], "entities": [{"text": "BERT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9855853915214539}]}, {"text": " Table 4: SWAG Dev and Test accuracies.  \u2020 Human per- formance is measured with 100 samples, as reported in  the SWAG paper.", "labels": [], "entities": [{"text": "SWAG Dev", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.641234815120697}, {"text": "SWAG paper", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.9306932985782623}]}, {"text": " Table 5: Ablation over the pre-training tasks using the  BERT BASE architecture. \"No NSP\" is trained without  the next sentence prediction task. \"LTR & No NSP\" is  trained as a left-to-right LM without the next sentence  prediction, like OpenAI GPT. \"+ BiLSTM\" adds a ran- domly initialized BiLSTM on top of the \"LTR + No  NSP\" model during fine-tuning.", "labels": [], "entities": [{"text": "BERT BASE", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8334947228431702}, {"text": "OpenAI GPT", "start_pos": 239, "end_pos": 249, "type": "DATASET", "confidence": 0.882297694683075}]}, {"text": " Table 6: Ablation over BERT model size. #L = the  number of layers; #H = hidden size; #A = number of at- tention heads. \"LM (ppl)\" is the masked LM perplexity  of held-out training data.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.957769513130188}, {"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8391045331954956}, {"text": "A", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.9503816366195679}]}, {"text": " Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev  set. The reported Dev and Test scores are averaged over  5 random restarts using those hyperparameters.", "labels": [], "entities": []}, {"text": " Table 8: Ablation over different masking strategies.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9093289971351624}]}, {"text": " Table 8. In the table,  MASK means that we replace the target token with  the [MASK] symbol for MLM; SAME means that  we keep the target token as is; RND means that  we replace the target token with another random  token.", "labels": [], "entities": [{"text": "MASK", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9735271334648132}, {"text": "SAME", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9851406812667847}, {"text": "RND", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9035137295722961}]}]}