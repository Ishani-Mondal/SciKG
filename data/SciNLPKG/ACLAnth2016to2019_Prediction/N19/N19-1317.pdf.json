{"title": [{"text": "Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification", "labels": [], "entities": [{"text": "Sentence Simplification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.9208105504512787}]}], "abstractContent": [{"text": "Sentence simplification is the task of rewriting texts so they are easier to understand.", "labels": [], "entities": [{"text": "Sentence simplification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9347797632217407}]}, {"text": "Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforcement learning and memory augmentation.", "labels": [], "entities": []}, {"text": "One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the original sentence, resulting in outputs that are relatively long and complex.", "labels": [], "entities": []}, {"text": "We aim to alleviate this issue through the use of two main techniques.", "labels": [], "entities": []}, {"text": "First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function during training.", "labels": [], "entities": []}, {"text": "Second, we generate a large set of diverse candidate simplifications attest time, and rerank these to promote fluency, adequacy, and simplicity.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9867777824401855}]}, {"text": "Here, we measure simplicity through a novel sentence complexity model.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9837130308151245}]}, {"text": "These extensions allow our models to perform competitively with state-of-the-art systems while generating simpler sentences.", "labels": [], "entities": []}, {"text": "We report standard automatic and human evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text simplification aims to reduce the complexity of texts and preserve their meaning, making their content more accessible to a broader audience.", "labels": [], "entities": [{"text": "Automatic text simplification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5966654022534689}]}, {"text": "This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring anew field.", "labels": [], "entities": []}, {"text": "Text simplification has gained wide interest in recent years due to its relevance for NLP tasks.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7854671478271484}, {"text": "NLP tasks", "start_pos": 86, "end_pos": 95, "type": "TASK", "confidence": 0.8981181085109711}]}, {"text": "Simplifying text during preprocessing can improve the performance of syntactic parsers) and Our code is available in our fork of Sockeye ( at https://github.com/rekriz11/sockeye-recipes.", "labels": [], "entities": []}, {"text": "semantic role labelers, and can improve the grammaticality (fluency) and meaning preservation (adequacy) of translation output.", "labels": [], "entities": [{"text": "semantic role labelers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6762251456578573}]}, {"text": "Most text simplification work has approached the task as a monolingual machine translation problem (.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7984713017940521}, {"text": "monolingual machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.7294978698094686}]}, {"text": "Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation ( and dialogue systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8059617578983307}]}, {"text": "One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification.", "labels": [], "entities": []}, {"text": "Several recent efforts have attempted to alleviate this problem using reinforcement learning) and memory augmentation (, but these systems often still produce outputs that are longer than the reference sentences.", "labels": [], "entities": []}, {"text": "To avoid this problem, we propose to extend the generic Seq2Seq framework at both training and inference time by encouraging the model to choose simpler content words, and by effectively choosing an output based on a large set of can-didate simplifications.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: \u2022 We propose a custom loss function to replace standard cross entropy probabilities during training, which takes into account the complexity of content words.", "labels": [], "entities": []}, {"text": "\u2022 We include a similarity penalty at inference time to generate more diverse simplifications, and we further cluster similar sentences together to remove highly similar candidates.", "labels": [], "entities": []}, {"text": "\u2022 We develop methods to rerank candidate simplifications to promote fluency, adequacy, and simplicity, helping the model choose the best option from a diverse set of sentences.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9948176741600037}]}, {"text": "An analysis of each individual components reveals that of the three contributions, reranking simplifications at post-decoding stage brings about the largest benefit for the simplification system.", "labels": [], "entities": []}, {"text": "We compare our model to several state-of-the-art systems in both an automatic and human evaluation settings, and show that the generated simple sentences are shorter and simpler, while remaining competitive with respect to fluency and adequacy.", "labels": [], "entities": []}, {"text": "We also include a detailed error analysis to explain where the model currently falls short and provide suggestions for addressing these issues.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work), we use SARI as our main automatic metric for evaluation (.", "labels": [], "entities": [{"text": "SARI", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9623725414276123}]}, {"text": "11 Specifically, SARI calculates how often a generated sentence correctly keeps, inserts, and deletes n-grams from the complex sentence, using the reference simple standard as the gold-standard, where 1 \u2264 n \u2264 4.", "labels": [], "entities": []}, {"text": "Note that we do not use  While SARI has been shown to correlate with human judgments on simplicity, it only weakly cor-: Average ratings of crowdsourced human judgments on fluency, adequacy and complexity.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9581997990608215}]}, {"text": "Ratings significantly different from S2S-All-FA are marked with * (p < 0.05); statistical significance tests were calculated using a student t-test.", "labels": [], "entities": []}, {"text": "We provide 95% confidence intervals for each rating in the appendix.", "labels": [], "entities": []}, {"text": "relates with judgments on fluency and adequacy ().", "labels": [], "entities": []}, {"text": "Furthermore, SARI only considers simplifications at the word level, while we believe that a simplification metric should also take into account sentence structure complexity.", "labels": [], "entities": [{"text": "SARI", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.4096490144729614}]}, {"text": "We plan to investigate this further in future work.", "labels": [], "entities": []}, {"text": "Due to the current perceived limitations of automatic metrics, we also choose to elicit human judgments on 200 randomly selected sentences to determine the relative overall quality of our simplifications.", "labels": [], "entities": []}, {"text": "For our first evaluation, we ask native English speakers on Amazon Mechanical Turk to evaluate the fluency, adequacy, and simplicity of sentences generated by our systems and the baselines, similar to.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.9635852575302124}, {"text": "simplicity", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9986571073532104}]}, {"text": "Each annotator rated these aspects on a 5-point Likert Scale.", "labels": [], "entities": [{"text": "Likert Scale", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9504810869693756}]}, {"text": "These results are found in.", "labels": [], "entities": []}, {"text": "As we can see, our best models substantially outperform the Hybrid and DMASS systems.", "labels": [], "entities": []}, {"text": "Note that DMASS performs the worst, potentially because the transformer model is a more complex model that requires more training data to work properly.", "labels": [], "entities": [{"text": "DMASS", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.713302731513977}]}, {"text": "Comparing to DRESS, our models generate simpler sentences, but DRESS better preserves the meaning of the original sentence.", "labels": [], "entities": []}, {"text": "To further investigate why this is the case, we know from that sentences generated by our model are overall shorter than other models, which also corresponds to higher TER scores.", "labels": [], "entities": [{"text": "TER scores", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9806041419506073}]}, {"text": "notes that on sentence compression, longer sentences are perceived by human annotators to preserve more meaning than shorter sentences, controlling for quality.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7146894484758377}]}, {"text": "Thus, the drop in human-judged adequacy maybe related to our sentences' relatively short lengths.", "labels": [], "entities": []}, {"text": "To test that this observation also holds true for simplicity, we took the candidates generated by our best model, and after reranking them as before, we selected three sets of sentences: \u2022 MATCH-Dress0: Highest ranked sentence with length closest to that of DRESS (DRESS-Len); average length is 14.10.", "labels": [], "entities": [{"text": "MATCH-Dress0", "start_pos": 189, "end_pos": 201, "type": "METRIC", "confidence": 0.9612172245979309}]}, {"text": "\u2022 MATCH-Dress+2: Highest ranked sentence with length closest to (DRESS-Len + 2); average length is 15.32.", "labels": [], "entities": [{"text": "MATCH-Dress+2", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9481030106544495}, {"text": "length", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9661674499511719}, {"text": "DRESS-Len + 2)", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.8556554913520813}]}, {"text": "\u2022 MATCH-Dress-2: Highest ranked sentence with length closest to (DRESS-Len -2); average length is 12.61.", "labels": [], "entities": [{"text": "MATCH-Dress-2", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9448159337043762}, {"text": "length", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.974109947681427}, {"text": "DRESS-Len -2)", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9449104815721512}]}, {"text": "The average fluency, adequacy, and simplicity from human judgments on these new sentences are shown in, along with those ranked highest by our best model (Original).", "labels": [], "entities": [{"text": "simplicity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.998512327671051}]}, {"text": "As expected, meaning preservation does substantially increase as we increase the average sentence length, while simplicity decreases.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8800316452980042}, {"text": "simplicity", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9984447360038757}]}, {"text": "Interestingly, fluency also decreases as sentence length increases; this is likely due to our higher-ranked sentences having greater fluency, as defined by language model perplexity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Pearson Correlation and Overall Mean  Squared Error (MSE) of the word-level complexity pre- diction model (LinReg). Comparison to length-based  and frequency-based baselines.", "labels": [], "entities": [{"text": "Pearson Correlation and Overall Mean  Squared Error (MSE)", "start_pos": 11, "end_pos": 68, "type": "METRIC", "confidence": 0.8230939775705337}]}, {"text": " Table 4: Comparison of our models to baselines and  state-of-the-art models using SARI. We also include  oracle SARI scores (Oracle), given a perfect reranker.  S2S-All-FA is significantly better than the DMASS and  Hybrid baselines using a student t-test (p < 0.05).", "labels": [], "entities": []}, {"text": " Table 5: Average sentence length, FKGL, TER score  compared to input, and number of insertions. We also  calculate average edit distance (Edit) between candi- date sentences for applicable models.", "labels": [], "entities": [{"text": "FKGL", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9694869518280029}, {"text": "TER score", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9880864322185516}, {"text": "edit distance (Edit)", "start_pos": 124, "end_pos": 144, "type": "METRIC", "confidence": 0.7615681409835815}]}, {"text": " Table 6: Average ratings of crowdsourced human judgments on fluency, adequacy and complexity. Ratings  significantly different from S2S-All-FA are marked with * (p < 0.05); statistical significance tests were calculated  using a student t-test. We provide 95% confidence intervals for each rating in the appendix.", "labels": [], "entities": []}]}