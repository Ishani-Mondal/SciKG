{"title": [{"text": "Text Generation with Exemplar-based Adaptive Decoding", "labels": [], "entities": [{"text": "Text Generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7224898040294647}]}], "abstractContent": [{"text": "We propose a novel conditioned text generation model.", "labels": [], "entities": [{"text": "text generation", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7079462110996246}]}, {"text": "It draws inspiration from traditional template-based text generation techniques , where the source provides the content (i.e., what to say), and the template influences how to say it.", "labels": [], "entities": [{"text": "template-based text generation", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.7345742980639139}]}, {"text": "Building on the successful encoder-decoder paradigm, it first encodes the content representation from the given input text; to produce the output, it retrieves exemplar text from the training data as \"soft templates,\" which are then used to construct an exemplar-specific decoder.", "labels": [], "entities": []}, {"text": "We evaluate the proposed model on abstractive text sum-marization and data-to-text generation.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7345539331436157}]}, {"text": "Empirical results show that this model achieves strong performance and outperforms comparable baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization, machine translation, and data-to-text generation.", "labels": [], "entities": [{"text": "Conditioned text generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6709504624207815}, {"text": "text summarization", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7463942170143127}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7898982763290405}, {"text": "data-to-text generation", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.7579853534698486}]}, {"text": "In its common neural sequence-tosequence formulation), an encoder-decoder architecture is used.", "labels": [], "entities": []}, {"text": "The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention () and copy mechanisms (.", "labels": [], "entities": []}, {"text": "This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (.", "labels": [], "entities": []}, {"text": "The encoder-decoder approach differs considerably from earlier template-based meth-\u21e4 Work done during internship at ods, where the source content is filled into the slots of a handcrafted template.", "labels": [], "entities": []}, {"text": "These solutions offer higher generation precision compared to neural approaches, but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large.", "labels": [], "entities": []}, {"text": "To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (, inter alia).", "labels": [], "entities": []}, {"text": "In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7868847012519836}, {"text": "exemplar extraction", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7121701538562775}]}, {"text": "We propose a different strategy for using exemplars.", "labels": [], "entities": []}, {"text": "For motivation, shows a sourcetarget pair together with its exemplar from the Gigaword dataset (.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9675011336803436}]}, {"text": "The target is a summary of the source sentence, and the exemplar is retrieved from the training set ( \u00a73.2).", "labels": [], "entities": []}, {"text": "There is word overlap between the exemplar and the desired output, which would be easily captured by an attention/copy mechanism (e.g. Norway and aid).", "labels": [], "entities": []}, {"text": "Despite this, ideally, the model should also exploit the structural and stylistic aspects to produce an output with a similar sentence structure, even if the words are different.", "labels": [], "entities": []}, {"text": "Indeed, in traditional templates, the source  is supposed to determine \"what to say,\" while the templates aim to address \"how to say it,\" reminiscent of the classical content selection and surface realization pipeline).", "labels": [], "entities": []}, {"text": "For instance, an ideal template for this example might look as follows: grants aid of to In the neural formulation, the \"how to say it\" aspect is primarily controlled by the decoder.", "labels": [], "entities": []}, {"text": "Inspired by the above intuition, we propose exemplar-based adaptive decoding, where a customized decoder is constructed for each exemplar.", "labels": [], "entities": []}, {"text": "This is achieved by letting the exemplars to directly influence decoder parameters through a reparameterization step ( \u00a73.1).", "labels": [], "entities": []}, {"text": "The adaptive decoder can be used as a drop-in replacement in the encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "It offers the potential to better incorporate the exemplars' structural and stylistic aspects into decoding, without excessive increase in the amount of parameters or computational overhead.", "labels": [], "entities": []}, {"text": "We empirically evaluate our approach on abstractive text summarization and data-to-text generation ( \u00a74), on which most of the recent efforts on exemplar-guided text generation have been studied.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6023716727892557}, {"text": "data-to-text generation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7612268924713135}, {"text": "exemplar-guided text generation", "start_pos": 145, "end_pos": 176, "type": "TASK", "confidence": 0.669339636961619}]}, {"text": "On three benchmark datasets, our approach outperforms comparable baselines, and achieves performance competitive with the state of the art.", "labels": [], "entities": []}, {"text": "The proposed method can be applicable in many other conditioned text generation tasks.", "labels": [], "entities": [{"text": "conditioned text generation", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6362972458203634}]}, {"text": "Our implementation is available at https://homes.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section empirically evaluates the proposed model on two sets of text generation tasks: abstractive summarization ( \u00a74.2) and data-to-text generation ( \u00a74.3).", "labels": [], "entities": [{"text": "text generation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7163474857807159}, {"text": "abstractive summarization", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.6990877687931061}, {"text": "data-to-text generation", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7158606350421906}]}, {"text": "Before heading into the experimental details, we first describe the architectures of the compared models in \u00a74.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of instances and average text lengths  for the datasets used in the experiments. The lengths  are averaged over training instances.", "labels": [], "entities": []}, {"text": " Table 3: NYT text summarization test performance  in ROUGE recall values. This is a smaller portion of  the original test data, after filtering out instances with  summaries shorter than 50 tokens ( \u00a74.2; Durrett et al.,  2016).  \u2020 denotes the models using retrieved exemplars,  and bold font indicates best performance.", "labels": [], "entities": [{"text": "NYT text summarization", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6760905186335245}]}, {"text": " Table 4: Data-to-text generation performance in  ROUGE-4 and BLEU on the Wikibio test set ( \u00a74.3).  \u2020  indicates the models using retrieved exemplars.", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7112121284008026}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9956963062286377}, {"text": "Wikibio test set", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9577232003211975}]}]}