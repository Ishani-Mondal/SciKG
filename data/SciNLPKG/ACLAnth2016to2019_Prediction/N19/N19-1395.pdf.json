{"title": [{"text": "Question Answering as an Automatic Evaluation Metric for News Article Summarization", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8179668486118317}, {"text": "Summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.8249183893203735}]}], "abstractContent": [{"text": "Recent work in the field of automatic sum-marization and headline generation focuses on maximizing ROUGE scores for various news datasets.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8261352777481079}, {"text": "ROUGE scores", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.9371601045131683}]}, {"text": "We present an alternative, extrin-sic, evaluation metric for this task, Answering Performance for Evaluation of Summaries.", "labels": [], "entities": [{"text": "Answering", "start_pos": 72, "end_pos": 81, "type": "TASK", "confidence": 0.9811191558837891}]}, {"text": "APES utilizes recent progress in the field of reading-comprehension to quantify the ability of a summary to answer a set of manually created questions regarding central entities in the source article.", "labels": [], "entities": [{"text": "APES", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7229317426681519}]}, {"text": "We first analyze the strength of this metric by comparing it to known manual evaluation metrics.", "labels": [], "entities": []}, {"text": "We then present an end-to-end neural abstractive model that maximizes APES, while increasing ROUGE scores to competitive results.", "labels": [], "entities": [{"text": "APES", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.8755095601081848}, {"text": "ROUGE", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9961071610450745}]}], "introductionContent": [{"text": "The task of automatic text summarization aims to produce a concise version of a source document while preserving its central information.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.5980047980944315}]}, {"text": "Current summarization models are divided into two approaches, extractive and abstractive.", "labels": [], "entities": [{"text": "summarization", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.9742621779441833}]}, {"text": "In extractive summarization, summaries are created by selecting a collection of key sentences from the source document (e.g.,;).", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6671279966831207}, {"text": "summaries", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9474894404411316}]}, {"text": "Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5686138868331909}]}, {"text": "Progress in sequence-to-sequence models) has led to recent success in abstractive summarization models.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.6054212749004364}]}, {"text": "Current models) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE) scores.", "labels": [], "entities": [{"text": "ROUGE) scores", "start_pos": 96, "end_pos": 109, "type": "METRIC", "confidence": 0.9325970013936361}]}, {"text": "ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova's Summary: bolton will offer new contracts to emile heskey, 37, eidur gudjohnsen, 36, and adam bogdan, 27.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7063169479370117}, {"text": "summaries evaluation", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.9663945436477661}]}, {"text": "heskey and gudjohnsen joined on short-term deals in december.", "labels": [], "entities": []}, {"text": "eidur gudjohnsen has scored five times in the championship . APES score: 0.33 Baseline Model Summary (Encoder / Decoder / Attention / Copy / Coverage): bolton will offer new contracts to emile heskey, 37, eidur gudjohnsen, 36, and goalkeeper adam bogdan, 27.", "labels": [], "entities": [{"text": "APES score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9534794390201569}, {"text": "Baseline Model Summary", "start_pos": 78, "end_pos": 100, "type": "METRIC", "confidence": 0.9412263433138529}, {"text": "bolton", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.9637429118156433}]}, {"text": "heskey and gudjohnsen joined on short-term deals in december, and have helped neil lennon 's side steer clear of relegation.", "labels": [], "entities": []}, {"text": "eidur gudjohnsen has scored five times in the championship, as well as once in the cup this season . APES score: 0.33 Our Model (APES optimization): bolton will offer new contracts to emile heskey, 37, eidur gudjohnsen, 36, and goalkeeper adam bogdan, 27.", "labels": [], "entities": [{"text": "APES score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9532743096351624}, {"text": "APES optimization)", "start_pos": 129, "end_pos": 147, "type": "METRIC", "confidence": 0.9000896414120992}, {"text": "bolton", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.9471989274024963}]}, {"text": "heskey joined on short-term deals in december, and have helped neil lennon 's side steer clear of relegation.", "labels": [], "entities": []}, {"text": "eidur gudjohnsen has scored five times in the championship, as well as once in the cup this season.", "labels": [], "entities": []}, {"text": "lennon has also fined midfielders barry bannan and neil danns two weeks wages this week.", "labels": [], "entities": [{"text": "lennon", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9057227969169617}, {"text": "bannan", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.7011523842811584}]}, {"text": "both players have apologised to lennon . APES score: 1.00 Questions from the CNN/Daily Mail Dataset: Q: goalkeeper also rewarded with new contract; A: adam bogdan Q: and neil danns both fined by club after drinking incident; A: barry bannan Q: barry bannan and both fined by club after drinking incident; A: neil danns While it has been shown that ROUGE is correlated to Pyramid, show that this summary level correlation decreases significantly when only a single reference is given.", "labels": [], "entities": [{"text": "APES score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9278002679347992}, {"text": "CNN/Daily Mail Dataset", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.844754409790039}, {"text": "ROUGE", "start_pos": 348, "end_pos": 353, "type": "METRIC", "confidence": 0.9961381554603577}, {"text": "Pyramid", "start_pos": 371, "end_pos": 378, "type": "DATASET", "confidence": 0.8584282398223877}, {"text": "summary level correlation", "start_pos": 395, "end_pos": 420, "type": "METRIC", "confidence": 0.8816159764925638}]}, {"text": "In contrast to the smaller manually curated DUC datasets used in the past, more recent large-scale summarization and headline generation datasets (CNN/Daily Mail (, Gigaword (, New York Times) provide only a single reference summary for each source document.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7625471353530884}, {"text": "headline generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.659715011715889}, {"text": "CNN/Daily Mail", "start_pos": 147, "end_pos": 161, "type": "DATASET", "confidence": 0.871580496430397}, {"text": "Gigaword (, New York Times)", "start_pos": 165, "end_pos": 192, "type": "DATASET", "confidence": 0.7747993128640311}]}, {"text": "In this work, we introduce anew automatic evaluation metric more suitable for such single reference news article datasets.", "labels": [], "entities": []}, {"text": "We define APES, Answering Performance for Evaluation of Summaries, anew metric for automatically evaluating summarization systems by querying summaries with a set of questions central to the input document (see).", "labels": [], "entities": [{"text": "APES", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7620697617530823}, {"text": "Answering Performance for Evaluation of Summaries", "start_pos": 16, "end_pos": 65, "type": "TASK", "confidence": 0.6189347604910532}, {"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.941737711429596}]}, {"text": "Reducing the task of summaries evaluation to an extrinsic task such as question answering is intuitively appealing.", "labels": [], "entities": [{"text": "summaries evaluation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9636990129947662}, {"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8734400868415833}]}, {"text": "This reduction, however, is effective only under specific settings: (1) Availability of questions focusing on central information and (2) availability of a reliable question answering (QA) model.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.7827977061271667}]}, {"text": "Concerning issue 1, questions focusing on salient entities can be available as part of the dataset: the headline generation dataset most used in recent years, the CNN/Daily Mail dataset, was constructed by creating questions about entities that appear in the reference summary.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 163, "end_pos": 185, "type": "DATASET", "confidence": 0.8987214684486389}]}, {"text": "Since the target summary contains salient information from the source document, we consider all entities appearing in the target summary as salient entities.", "labels": [], "entities": []}, {"text": "In other cases, salient questions can be generated in an automated manner, as we discuss below.", "labels": [], "entities": []}, {"text": "Concerning issue 2, we focus on a relatively easy type of questions: given source documents and associated questions, a QA system can be trained over fill-in-the-blank type questions as was shown in and . In their work,  achieve 'ceiling performance' for the QA task on the CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "QA task", "start_pos": 259, "end_pos": 266, "type": "TASK", "confidence": 0.8394362926483154}, {"text": "CNN/Daily Mail dataset", "start_pos": 274, "end_pos": 296, "type": "DATASET", "confidence": 0.9356998443603516}]}, {"text": "We empirically assess in our work whether this performance level (accuracy of 72.4 and 75.8 over CNN and Daily Mail respectively) makes our evaluation scheme feasible and well correlated with manual summary evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9984734654426575}, {"text": "Daily Mail", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.8325488567352295}]}, {"text": "Given the availability of salient questions and automatic QA systems, we propose APES as an evaluation metric for news article datasets, the most popular summarization genre in recent years.", "labels": [], "entities": [{"text": "APES", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.7742282748222351}]}, {"text": "To measure the APES metric of a candidate summary, we run a trained QA system with the summary as input alongside a set of questions associated with the source document.", "labels": [], "entities": [{"text": "APES metric", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9103650450706482}]}, {"text": "The APES metric fora summarization model is the percentage of questions that were answered correctly over the whole dataset, as depicted in.", "labels": [], "entities": [{"text": "APES", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.5799453854560852}]}, {"text": "We leave the task of extending this method to other genres for future work.", "labels": [], "entities": []}, {"text": "Our contributions in this work are: (1) We first present APES, anew extrinsic summarization evaluation metric; (2) We show APES strength through an analysis of its correlation with Pyramid and Responsiveness manual metrics; (3) we present anew abstractive model which maximizes APES by increasing attention scores of salient entities, while increasing ROUGE to competitive level.", "labels": [], "entities": [{"text": "APES", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.6692757606506348}, {"text": "summarization evaluation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.7915665209293365}, {"text": "APES", "start_pos": 278, "end_pos": 282, "type": "TASK", "confidence": 0.800715982913971}, {"text": "ROUGE", "start_pos": 352, "end_pos": 357, "type": "METRIC", "confidence": 0.9950405955314636}]}, {"text": "We make two software packages available online: (a) An evaluation library which receives the same input as ROUGE and produces both APES and ROUGE scores.", "labels": [], "entities": [{"text": "APES", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.7663149237632751}]}, {"text": "1 (b) Our PyTorch () based summarizer that optimizes APES scores together with trained models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic evaluation metrics of summarization methods can be categorized into either intrinsic or extrinsic metrics.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.980305016040802}]}, {"text": "Intrinsic metrics measure a summary's quality by measuring its similarity to a manually produced target gold summary or by inspecting properties of the summary.", "labels": [], "entities": []}, {"text": "Examples of such metrics include ROUGE), Basic Elements () and Pyramid (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9644477367401123}]}, {"text": "Alternatively, extrinsic metrics test the ability of a summary to support performing related tasks and compare the performance of humans or systems when completing a task that requires understanding the source document.", "labels": [], "entities": []}, {"text": "Such extrinsic tasks may include text categorization, infor-mation retrieval, question answering () or assessing the relevance of a document to a query (.", "labels": [], "entities": [{"text": "infor-mation retrieval", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7325750291347504}, {"text": "question answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8620609045028687}]}, {"text": "ROUGE, or \"Recall-Oriented Understudy for Gisting Evaluation\"), refers to a set of automatic intrinsic metrics for evaluating automatic summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9659455418586731}]}, {"text": "ROUGE-N scores a candidate summary by counting the number of N-gram overlaps between the automatic summary and the reference summaries.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9627187252044678}]}, {"text": "Other notable metrics from this family are ROUGE-L, where scores are given by the Longest Common Subsequence (LCS) between the suggested and reference documents, and ROUGE-SU4, which uses skip-bigram, a more flexible method for computing the overlap of bigrams.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9559676051139832}, {"text": "Longest Common Subsequence (LCS)", "start_pos": 82, "end_pos": 114, "type": "METRIC", "confidence": 0.675749272108078}]}, {"text": "The Pyramid method () is a manual evaluation metric that analyzes multiple human-made summaries into \"Summary Content Units\" (SCUs) and assigns importance weights to each SCU.", "labels": [], "entities": []}, {"text": "Different summaries are scored by assessing the extent to which they convey SCUs according to their respective weights.", "labels": [], "entities": []}, {"text": "Pyramid is most effective when multiple human-made summaries alongside manual intervention to detect SCUs in source and target documents.", "labels": [], "entities": []}, {"text": "The Basic Elements method (), an automated procedure for finding short fragments of content, has been suggested to automate a method related to Pyramid.", "labels": [], "entities": []}, {"text": "Like Pyramid, this method requires multiple human-made gold summaries, making this method expensive in time and cost.", "labels": [], "entities": []}, {"text": "Responsiveness (, another manual metric is a measure of overall quality combining both content selection, like Pyramid, and linguistic quality.", "labels": [], "entities": []}, {"text": "Both Pyramid and Responsiveness are the standard manual approaches for content evaluation of summaries.", "labels": [], "entities": [{"text": "content evaluation of summaries", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7227228730916977}]}, {"text": "Automated Pyramid evaluation has been attempted in the past.", "labels": [], "entities": [{"text": "Pyramid evaluation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.761928915977478}]}, {"text": "This task is complex because it requires (1) identifying SCUs in a text, which requires syntactic parsing and the extraction of key subtrees from the identified units, and (2) the clustering of these extracted textual elements into semantically similar SCUs.", "labels": [], "entities": []}, {"text": "These two operations are noisy, and the compounded performance summary evaluation is relying on noisy intermediary representation accordingly suffers.", "labels": [], "entities": []}, {"text": "Other relevant quantities for summaries quality assessment include: readability (or fluency), grammaticality, coherence and structure, focus, referential clarity, and non-redundancy.", "labels": [], "entities": [{"text": "summaries quality assessment", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8727070490519205}, {"text": "clarity", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.847228467464447}]}, {"text": "Although some automatic methods were suggested as summarization evaluation metrics, these metrics are commonly assessed manually, and, therefore, rarely reported as part of experiments.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.8961622416973114}]}, {"text": "Our proposed evaluation method, APES, attempts to capture the capability of a summary to enable readers to answer questions -similar to the manual task initially discussed in and recently reported in.", "labels": [], "entities": [{"text": "APES", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7247480750083923}]}, {"text": "Our contribution consists of automating this method and assessing the feasibility of the resulting approximation.", "labels": [], "entities": []}, {"text": "When questions are not intrinsically available, one requires to (1) automatically generate relevant questions; (2) use an appropriate automatic QA system.", "labels": [], "entities": []}, {"text": "Similarly to the method used in, we produce fill-in-the-blank questions in the following way: given a reference summary, we find all possible entities, (i.e., Name, Nationality, Organization, Geopolitical Entity or Facility) using an NER system and we create fill-in-the-blank type questions where the answers are these entities.", "labels": [], "entities": []}, {"text": "We provide code for this procedure and apply it on the AESOP datasets in our experiments . For the automatic QA system, we reused in our experiment the same QA system trained on CNN/Daily Mail for different News datasets (including AESOP).", "labels": [], "entities": [{"text": "AESOP datasets", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9346460700035095}, {"text": "CNN/Daily Mail", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.8764268755912781}, {"text": "News datasets", "start_pos": 207, "end_pos": 220, "type": "DATASET", "confidence": 0.8956392407417297}, {"text": "AESOP", "start_pos": 232, "end_pos": 237, "type": "DATASET", "confidence": 0.883950412273407}]}, {"text": "To enable reproducibility, the trained models used are available online.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson Correlation of ROUGE and APES against Pyramid and Responsiveness on summary level. Sta- tistically significant differences are marked with *.", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7557563781738281}, {"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.6126050353050232}, {"text": "APES", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.748744010925293}, {"text": "Pyramid", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9031401872634888}]}, {"text": " Table 2: Correlation matrix of ROUGE and APES.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.8974671959877014}, {"text": "APES", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.49698519706726074}]}, {"text": " Table 1.  In Input level, correlation is computed for each  summary against its manual score. In contrast,  system level reports the average score for a sum- marization system over the entire dataset.", "labels": [], "entities": [{"text": "correlation", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9972776770591736}]}, {"text": " Table 4: APES: Percent of questions answered correctly using by document. *Obtained from the model uploaded  to github.com/abisee/pointer-generator.", "labels": [], "entities": [{"text": "APES", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9654309153556824}, {"text": "Obtained", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9901648163795471}]}]}