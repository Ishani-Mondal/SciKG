{"title": [{"text": "Noisy Neural Language Modeling for Typing Prediction in BCI Communication", "labels": [], "entities": [{"text": "Noisy Neural Language Modeling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5299279093742371}, {"text": "Typing Prediction", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6993804723024368}]}], "abstractContent": [{"text": "Language models have broad adoption in predictive typing tasks.", "labels": [], "entities": [{"text": "predictive typing tasks", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8666706482569376}]}, {"text": "When the typing history contains numerous errors, as in open-vocabulary predictive typing with brain-computer interface (BCI) systems, we observe significant performance degradation in both n-gram and recurrent neural network language models trained on clean text.", "labels": [], "entities": []}, {"text": "In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.", "labels": [], "entities": [{"text": "ranking character predictions", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7736934622128805}]}, {"text": "We also propose an effective strategy for combining evidence from multiple ambiguous histories of BCI electroencephalogram measurements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Brain-computer interface (BCI) systems provide a means of language communication for people who have lost the ability to speak, write, or type, e.g., patients with amyotrophic lateral sclerosis (ALS) or locked-in syndrome (LIS).", "labels": [], "entities": []}, {"text": "These systems are designed to detect a user's intent from electroencephalogram (EEG) or other signals and to translate them into typing commands.", "labels": [], "entities": []}, {"text": "Recent studies have shown that incorporating language information into BCI systems can significantly improve both their typing speed and accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9989748001098633}]}, {"text": "Existing methods for optimizing BCI systems with language information either focus on improving the accuracy of symbol classifiers by adding priors from language models(), or on accelerating the typing speed by tying prediction (), word completion, or automatic error correction (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9963634610176086}, {"text": "word completion", "start_pos": 232, "end_pos": 247, "type": "TASK", "confidence": 0.7766465842723846}]}, {"text": "Instead of displaying a keyboard layout, these BCI systems present candidate characters sequentially, as in the Shannon game, and then measure users' reactions with EEG or other signals.", "labels": [], "entities": []}, {"text": "Predictive performance is thus measured using the mean reciprocal rank of the correct character or the recall of the correct character in the k candidates presented in a batch to the user.", "labels": [], "entities": [{"text": "Predictive", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9562649726867676}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9984034895896912}]}, {"text": "Most previous work on language modeling for BCI employs n-gram language models although the past decade has seen recurrent and other neural architectures surpass these models for many tasks.", "labels": [], "entities": []}, {"text": "Furthermore, most predictive typing methods, for BCI or other applications, depend on language models trained on clean text; however, BCI output often contains noise due to misclassification of EEG or other input signals.", "labels": [], "entities": [{"text": "predictive typing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9139507710933685}]}, {"text": "To the best of our knowledge, language models have rarely been evaluated in with such character-level noise.", "labels": [], "entities": []}, {"text": "Recurrent language models, however, could effectively utilize contexts of 200 tokens on average (.", "labels": [], "entities": []}, {"text": "Although this might be a disadvantage with noisy histories, we will see that it is no worse than n-gram models with clean training and much better with noisy training.", "labels": [], "entities": []}, {"text": "In addition, existing work mainly focuses on prediction given a single sequence of tokens in the history, but the signal classifier for BCI systems might not always correctly rank the users' intent as the top candidate.", "labels": [], "entities": []}, {"text": "proposed incorporating ambiguous history into decoding with a joint word-character finite-state model, but typing prediction could not be further improved.", "labels": [], "entities": [{"text": "typing prediction", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.8873641192913055}]}, {"text": "Although (Sperber et al., 2017) considered lattice decoding for neural models, the task of integrating multiple candidate histories during online prediction has not been studied.", "labels": [], "entities": []}, {"text": "To address these challenges, we propose to train a noise-tolerant neural language model for online predictive typing and to provide a richer understanding of the effect of the noise on recurrent neural network language models.", "labels": [], "entities": [{"text": "predictive typing", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7992137670516968}]}, {"text": "We aim to an-swer the following questions: (1) what effect noise in different regions of the history and in different sentence and word positions has on recurrent language model character predictions; (2) how to mitigate performance degradation in LM predictive accuracy with noisy histories; and (3) whether including ambiguous history could help to improve the performance of neural language models.", "labels": [], "entities": [{"text": "recurrent language model character predictions", "start_pos": 153, "end_pos": 199, "type": "TASK", "confidence": 0.5899107933044434}, {"text": "LM predictive", "start_pos": 248, "end_pos": 261, "type": "TASK", "confidence": 0.81331005692482}]}, {"text": "In this paper, we investigate these questions by training long short-term memory (LSTM: Hochreiter and Schmidhuber, 1997) models on synthetic noisy data generated from the New York Times (NYT) corpus and the SUBTLEXus corpus to cover both formal and colloquial language.", "labels": [], "entities": []}, {"text": "Experimental results show that injecting noise into the training data improves the generalizability of language models on a predictive typing task.", "labels": [], "entities": []}, {"text": "Moreover, a neural language model trained on noisy text outperforms n-gram language models trained on noisy or clean text.", "labels": [], "entities": []}, {"text": "In fact, some language models trained on clean text do substantially worse than a character unigram baseline when presented with text with only uniform stationary noise.", "labels": [], "entities": []}, {"text": "Taking multiple possible candidates into consideration at each time step further improves predictive performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first introduce the details of our experimental setup ( \u00a74.1).", "labels": [], "entities": []}, {"text": "Then we compare the performance of the LSTM and baseline models trained on clean and noisy text ( \u00a74.2).", "labels": [], "entities": []}, {"text": "Further discussion of the effect of errors on LSTM models follows in \u00a74.3 and \u00a74.4.", "labels": [], "entities": []}, {"text": "\u00a74.5 explores whether including multiple candidate histories could further improve predictive performance.", "labels": [], "entities": []}, {"text": "Datasets We evaluate our model on two datasets: the New York Times (NTY) corpus and SUBTLEXus (Brysbaert and New, 2009) corpus of subtitles from movies and television.", "labels": [], "entities": [{"text": "New York Times (NTY) corpus", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.8020605530057635}, {"text": "SUBTLEXus (Brysbaert and New, 2009) corpus", "start_pos": 84, "end_pos": 126, "type": "DATASET", "confidence": 0.6001602411270142}]}, {"text": "The NYT has relatively longer sentences, richer vocabulary, and more formal language; SUBTLEXus tends toward colloquial language and shorter sentences.", "labels": [], "entities": [{"text": "NYT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.864904522895813}]}, {"text": "To make a fair comparison between the two corpora, we randomly sample two subsets from them with equal numbers of characters.", "labels": [], "entities": []}, {"text": "Both corpora are split into sentences, 80% sentences are randomly sampled as training set while the rest are used as test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 sum- marizes the data.", "labels": [], "entities": []}]}