{"title": [{"text": "Content-based Dwell Time Prediction Model for News Articles", "labels": [], "entities": [{"text": "Dwell Time Prediction", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.5171005229155222}]}], "abstractContent": [{"text": "The article dwell time (i.e., expected time that users spend on an article) is among the most important factors showing the article engagement.", "labels": [], "entities": [{"text": "article dwell time", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.6831466257572174}]}, {"text": "It is of great interest to news agencies to predict the dwell time of an article before its release.", "labels": [], "entities": []}, {"text": "It allows online newspapers to make informed decisions and publish more engaging articles.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel content-based approach based on a deep neural network architecture for predicting article dwell times.", "labels": [], "entities": [{"text": "predicting article dwell", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8022176424662272}]}, {"text": "The proposed model extracts emotion, event and entity-based features from an article, learns interactions among them, and combines the interactions with the word-based features of the article to learn a model for predicting the dwell time.", "labels": [], "entities": []}, {"text": "We apply the proposed model to areal dataset from a national newspaper showing that the proposed model outper-forms other state-of-the-art baselines.", "labels": [], "entities": [{"text": "areal dataset from a national newspaper", "start_pos": 31, "end_pos": 70, "type": "DATASET", "confidence": 0.6869685351848602}]}], "introductionContent": [{"text": "For online newspapers, it is desirable to predict how user-engaging an article is before publishing it so that editors have an idea about the prosperity of the article.", "labels": [], "entities": []}, {"text": "This will help editors select more engaging articles to publish and also make smarter decisions to increase revenue (e.g., displaying more advertisements with an engaging article).", "labels": [], "entities": []}, {"text": "Most of the previous studies focus on predicting the page views (i.e., user clicks) as the sole indicator of user engagement and article success.", "labels": [], "entities": []}, {"text": "However, click-based engagement modeling can be quite noisy (e.g., when a user clicks on a wrong article) and may not show the actual user engagement or satisfaction ().", "labels": [], "entities": [{"text": "click-based engagement modeling", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.6642661889394125}]}, {"text": "Alternatively, it is shown that the time that a user spends on a page, known as the dwell time, is one of the most significant indicators of user engagement ().", "labels": [], "entities": []}, {"text": "Thus, we consider dwell time as an engagement measure and design an effective model to predict the dwell time of an article based on its content.", "labels": [], "entities": []}, {"text": "There are some studies on dwell time prediction.", "labels": [], "entities": [{"text": "dwell time prediction", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.5660708645979563}]}, {"text": "Most of them predict dwell time for webpages instead of news articles.) use regression trees to predict the Weibull distributions of webpage dwell time using keywords and page size.) predict web content dwell time using support vector regression based on the content length and topic category across different devices.", "labels": [], "entities": []}, {"text": ") use a regression model to estimate the Gamma distributions of page dwell time based on the topic of the page, its length and its readability level.", "labels": [], "entities": []}, {"text": "To our knowledge, none of the studies focuses on news articles nor investigates whether high-level features such as events, entities and emotions play an important role in the user engagement of an article measured by dwell time.", "labels": [], "entities": []}, {"text": "We believe such high level features are important factors for dwell time prediction.", "labels": [], "entities": [{"text": "dwell time prediction", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.5693117678165436}]}, {"text": "In this paper we focus on news articles and consider events, emotions as well as people and organizations as main contributors to the article dwell time.", "labels": [], "entities": []}, {"text": "Both low-level (e.g., word-based) and high level features (e.g., people) are used in our prediction model.", "labels": [], "entities": []}, {"text": "However, features such as people and organizations have a very high dimensionality resulting in sparse data representations.", "labels": [], "entities": []}, {"text": "In addition, interactions between such features matter.", "labels": [], "entities": []}, {"text": "For example, articles mentioning two celebrities (e.g., Prince Harry and Meghan Markle) maybe more engaging than articles mentioning only one of them.", "labels": [], "entities": []}, {"text": "To address such issues, we propose a model based on the wide and deep neural network architecture () which memorizes the low order interactions between the sparse features (e.g., people in articles), and at the same time generalizes word-based content through the deep component.", "labels": [], "entities": []}, {"text": "In order to learn the interactions between features, we adopt the factorization machine (, which extracts feature interactions automatically, as the wide component in the proposed model.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows.", "labels": [], "entities": []}, {"text": "First, we design a novel framework for predicting the dwell time of a news article based on its content.", "labels": [], "entities": []}, {"text": "Second, we propose an effective deep neural network model that combines the low-order interactions between high-level factors (i.e., events, emotions and entities) and wordbased abstract features for article dwell time prediction.", "labels": [], "entities": [{"text": "article dwell time prediction", "start_pos": 200, "end_pos": 229, "type": "TASK", "confidence": 0.6920026242733002}]}, {"text": "Third, we apply the proposed model to areal dataset from the Globe and Mail 1 and show the effectiveness of the proposed model and the usefulness of event, emotion and entity based features and their interactions for dwell time prediction.", "labels": [], "entities": [{"text": "areal dataset from the Globe and Mail 1", "start_pos": 38, "end_pos": 77, "type": "DATASET", "confidence": 0.8286956697702408}, {"text": "dwell time prediction", "start_pos": 217, "end_pos": 238, "type": "TASK", "confidence": 0.6109464963277181}]}], "datasetContent": [{"text": "All the experiments are conducted on areal dataset from the Globe and Mail dataset.", "labels": [], "entities": [{"text": "Globe and Mail dataset", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.942148968577385}]}, {"text": "The data collection platform in this company records a timestamp whenever an article page is requested.", "labels": [], "entities": []}, {"text": "The difference between two consecutive page click timestamps is used to calculate the articles dwell times.", "labels": [], "entities": []}, {"text": "As usual in web analytics the last article in a visit is ignored as we cannot estimate the dwell time for it.", "labels": [], "entities": []}, {"text": "Clickstream data is usually noisy.", "labels": [], "entities": []}, {"text": "Thus, as a cleaning step, the articles with less than 10 views and dwell time more than 30 minutes are removed resulting in 28502 articles published over period of 2014-01 to 2014-07.", "labels": [], "entities": []}, {"text": "Moreover, all the experiments in this section are based on the 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We set M and K in the proposed model to 100 and 10 respectively.", "labels": [], "entities": []}, {"text": "We used the code in) with default parameter setting for non-neural networks, and neural network models are implemented using Keras with tensorflow backend (Chollet et al., 2015).", "labels": [], "entities": []}, {"text": "We utilize the following metrics to evaluate the performance of different models.", "labels": [], "entities": []}, {"text": "Given the actual dwell time y i and predicted dwell tim\u00ea y i for article a i (i = 1, 2, . .", "labels": [], "entities": []}, {"text": "We calculate the Mean Square Error (MSE) as follows:  Moreover, we calculate the Relative Absolute Error (RAE) as: where \u00af y i = 1 N N i=1 y i . Note that RAE \u2208 [0, \u221e).", "labels": [], "entities": [{"text": "Mean Square Error (MSE)", "start_pos": 17, "end_pos": 40, "type": "METRIC", "confidence": 0.956448475519816}, {"text": "Relative Absolute Error (RAE)", "start_pos": 81, "end_pos": 110, "type": "METRIC", "confidence": 0.9641664226849874}, {"text": "RAE", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.8977721333503723}]}, {"text": "shows the MSEs and RAEs of different baseline approaches as well as the proposed model.", "labels": [], "entities": [{"text": "RAEs", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.958168625831604}]}, {"text": "As shown, the proposed model outperforms all the baselines.", "labels": [], "entities": []}, {"text": "For shallow (i.e., LRbased) and RF-based models we learn the features using LDA or Doc2Vec approaches and then train the model with Linear Regression (LR) and Random Forest (RF) respectively.", "labels": [], "entities": [{"text": "Random Forest (RF)", "start_pos": 159, "end_pos": 177, "type": "METRIC", "confidence": 0.8124383330345154}]}, {"text": "As shown, among such models RF+Doc2Vec performs the best.", "labels": [], "entities": [{"text": "RF+Doc2Vec", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.7389942407608032}]}, {"text": "Among the deep neural network based baselines, we observe that MLP performs better than the other two.", "labels": [], "entities": [{"text": "MLP", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.5135002732276917}]}, {"text": "One reason could be that our dataset is not very big (with 28502 articles) and as a result the complex models such as CNN and LSTM may overfit to the training data.", "labels": [], "entities": [{"text": "CNN", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.9474220871925354}]}, {"text": "To investigate the effect of learning feature interactions with factorization machines, we created another baseline that use MLP with both words and augmented features as input without using factorization machines (denoted as 'MLP + Flat Augmented features\" in).", "labels": [], "entities": []}, {"text": "We choose MLP because it is the best among the baselines.", "labels": [], "entities": [{"text": "MLP", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.3982056677341461}]}, {"text": "As can be seen, the naive approach of adding the augmented features to MLP without using factorization machines leads to poor results.", "labels": [], "entities": []}, {"text": "shows the effect of different types of augmented features on the performance of the proposed model.", "labels": [], "entities": []}, {"text": "As we observe, using all the augmented features in the proposed model results in the best performance.", "labels": [], "entities": []}, {"text": "shows the model performance in terms of the number of hidden vectors per feature dimension.", "labels": [], "entities": []}, {"text": "We increase the number of hidden vectors    (i.e., K) in the factorization machine component and calculate the errors accordingly.", "labels": [], "entities": []}, {"text": "As can be observed, the errors decrease significantly by increasing K from 1 to 5, then becomes stable.", "labels": [], "entities": [{"text": "errors", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9761914014816284}]}, {"text": "This suggests that a value between 5 to 10 would be a good choice for this parameter.", "labels": [], "entities": []}, {"text": "To seethe effect of different deep component architecture shapes on the error measures, we keep the number of nodes constant (i.e., 600), and change the number of nodes in the hidden layers.", "labels": [], "entities": []}, {"text": "shows the effect of selecting different architectures on the errors.", "labels": [], "entities": []}, {"text": "As can be seen, the 250-100-250 is the worst among all architecture and 300-200-100 is slightly better than the others.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of different methods.", "labels": [], "entities": []}, {"text": " Table 2: Effect of different augmented features.", "labels": [], "entities": []}]}