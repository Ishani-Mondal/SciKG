{"title": [{"text": "Modeling Document-level Causal Structures for Event Causal Relation Identification", "labels": [], "entities": [{"text": "Event Causal Relation Identification", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.7309567555785179}]}], "abstractContent": [{"text": "We aim to comprehensively identify all the event causal relations in a document, both within a sentence and across sentences, which is important for reconstructing pivotal event structures.", "labels": [], "entities": []}, {"text": "The challenges we identified are two: 1) event causal relations are sparse among all possible event pairs in a document , in addition, 2) few causal relations are explicitly stated.", "labels": [], "entities": []}, {"text": "Both challenges are especially true for identifying causal relations between events across sentences.", "labels": [], "entities": []}, {"text": "To address these challenges, we model rich aspects of document-level causal structures for achieving comprehensive causal relation identification.", "labels": [], "entities": [{"text": "comprehensive causal relation identification", "start_pos": 101, "end_pos": 145, "type": "TASK", "confidence": 0.6359650567173958}]}, {"text": "The causal structures include heavy involvements of document-level main events in causal relations as well as several types of fine-grained constraints that capture implications from certain sentential syntactic relations and discourse relations as well as interactions between event causal relations and event corefer-ence relations.", "labels": [], "entities": []}, {"text": "Our experimental results show that modeling the global and fine-grained aspects of causal structures using Integer Linear Programming (ILP) greatly improves the performance of causal relation identification, especially in identifying cross-sentence causal relations.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 176, "end_pos": 206, "type": "TASK", "confidence": 0.6543683310349783}]}], "introductionContent": [{"text": "Understanding causal relations between events in a document is an important step in text understanding and is beneficial to various NLP applications, such as information extraction, question answering and text summarization.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8528440892696381}, {"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.8438735008239746}, {"text": "question answering", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.9099273979663849}, {"text": "text summarization", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.746592104434967}]}, {"text": "Causal relations can occur between any two events in a document, both between events within a sentence and between events across sentences.", "labels": [], "entities": []}, {"text": "In this paper, we aim to identify all the event causal relations in a document.", "labels": [], "entities": []}, {"text": "The main challenges for achieving comprehensive causal relation identification are that event causal relations are sparse among all the event pairs in a document and few event causal relations are explicitly stated.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6929148137569427}]}, {"text": "The challenges are especially true for identifying cross-sentence event causal relations and most of them have no clear causal indicators.", "labels": [], "entities": [{"text": "cross-sentence event causal relations", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.7197505831718445}]}, {"text": "To address these challenges, we model rich aspects of document-level causal structures, i.e., structural distributions of causal relations within a document, for achieving comprehensive causal relation identification in news articles.", "labels": [], "entities": [{"text": "causal relation identification in news articles", "start_pos": 186, "end_pos": 233, "type": "TASK", "confidence": 0.7867119014263153}]}, {"text": "Our key observation for improving causal relation identification is that causal relations, especially cross-sentence causal relations, tend to involve one or two main events of a document.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8231426477432251}]}, {"text": "The main events are the focus of a story, which are usually mentioned in the title of an article and have repeated mentions throughout the document.", "labels": [], "entities": []}, {"text": "Intuitively, causal relations in a document are often used to explain why the main events happened as well as consequences of the main events.", "labels": [], "entities": []}, {"text": "For example, as shown in, killing is the main event.", "labels": [], "entities": [{"text": "killing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9651212692260742}]}, {"text": "The events crossfire, spraying, richocheted, struck are its preconditions, and accuse, trial are its consequences.", "labels": [], "entities": []}, {"text": "Indeed, many causal relations are related to the main event.", "labels": [], "entities": []}, {"text": "In addition to the global causal structures related to main events of a document, we model three types of fine-grained causal structures in order to accurately identify each individual causal relation.", "labels": [], "entities": []}, {"text": "First, specific sentential syntactic relations may evoke causal relations between event pairs.", "labels": [], "entities": []}, {"text": "For instance, adverbial clause modifier of a verb phrase explains its consequence, condition or purpose.", "labels": [], "entities": [{"text": "adverbial clause modifier of a verb phrase", "start_pos": 14, "end_pos": 56, "type": "TASK", "confidence": 0.7901369759014675}]}, {"text": "Second, we model implications of a discourse relation between two text units (e.g., the contingency discourse relation) towards causal relations between events in the two text units.", "labels": [], "entities": []}, {"text": "Third, we model interactions between event causal relations and event coreference relations.", "labels": [], "entities": [{"text": "event coreference relations", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7493923306465149}]}, {"text": "For example, coreferent event mentions should have the same causal relations; a causal relation and an identity relation should not co-exist between any two events.", "labels": [], "entities": []}, {"text": "We use Integer Linear Programming (ILP) to model these rich causal structures within a document by designing constraints and modifying the objective function to encourage causal relations akin to the observed causal structures and discourage the opposite.", "labels": [], "entities": []}, {"text": "Our experimental results on the dataset EventStoryLine show that modeling the global and fine-grained aspects of causal structures within a document greatly improves the performance of causal relation identification, especially in identifying crosssentence causal relations.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 185, "end_pos": 215, "type": "TASK", "confidence": 0.6649357477823893}]}], "datasetContent": [{"text": "There are 22 topics in the EventStoryLine corpus.", "labels": [], "entities": [{"text": "EventStoryLine corpus", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.780450314283371}]}, {"text": "We put them in order based on their topic IDs and use documents in the last two topics as the development set.", "labels": [], "entities": []}, {"text": "We trained the ILP system using the rest 20 topics and tuned parameters based on the system performance on the development set.", "labels": [], "entities": []}, {"text": "We report experimental results by conducting 5-fold cross validation on the rest 20 topics.", "labels": [], "entities": []}, {"text": "For event causal relation identification, we report precision, recall, and F1-score.", "labels": [], "entities": [{"text": "event causal relation identification", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7841270864009857}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993966817855835}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9995948672294617}, {"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9991462230682373}]}, {"text": "The weighting parameters for constraints, including km 1 , km 2 , kn 1 , kn 2 , k f , kt , kc and k s , were first pre-set to be a small number 0.1.", "labels": [], "entities": []}, {"text": "We then conducted grid search and searched for the best value for each parameter over the range from 0.1 to 0.5 with a step size of 0.1.", "labels": [], "entities": []}, {"text": "The best values for the parameters are 0.2, 0.1, 0.1, 0.5, 0.2, 0.1, 0.1, 0.2 respectively.", "labels": [], "entities": []}, {"text": "The first section of table 2 shows the performance of baseline models on intra-and cross-sentence causal relation identification.", "labels": [], "entities": [{"text": "cross-sentence causal relation identification", "start_pos": 83, "end_pos": 128, "type": "TASK", "confidence": 0.7568339928984642}]}, {"text": "The model OP labels each event mention pair as causal and suffers from low precisions 9 , especially on identifying cross-sentence causal relations.", "labels": [], "entities": [{"text": "precisions", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9780758023262024}]}, {"text": "The two dependency path based neural network model do not perform effectively on identifying causal relations.", "labels": [], "entities": []}, {"text": "The performance is especially poor on cross-sentence cases.", "labels": [], "entities": []}, {"text": "The model LR (Lexical) improved the precision of causal relation identification but suf-  fers from low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994451403617859}, {"text": "causal relation identification", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6602965195973715}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9971519708633423}]}, {"text": "In contrast, the model LR (Causal Potential) improved the recall but suffers from low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9996342658996582}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9975226521492004}]}, {"text": "The model LR (full) with rich lexical, semantic and syntactic features achieved the best trade-off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9992461204528809}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9943599104881287}]}, {"text": "+ Score Replacement significantly improves the recall and F1-score on identifying cross-sentence causal relations, which also slightly improves the recall of intra-sentence cases.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9997106194496155}, {"text": "F1-score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9985785484313965}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9978572726249695}]}, {"text": "But the precision of causal relation identification remains low, especially on cross-sentence cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9952548742294312}, {"text": "causal relation identification", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7849792639414469}]}, {"text": "The second section of table 2 shows the performance of our ILP model after gradually adding each type of constraints.", "labels": [], "entities": []}, {"text": "+Main Event Constraints shows the performance of the ILP system with constraints encouraging causal relations involving a main event.", "labels": [], "entities": []}, {"text": "By modeling this aspect of document-level causal structures, the precision of cross-sentence causal relation identification was clearly improved by around 6.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9995342493057251}, {"text": "cross-sentence causal relation identification", "start_pos": 78, "end_pos": 123, "type": "TASK", "confidence": 0.7843831330537796}]}, {"text": "With a small loss on recall, the F1-score was improved by 4.1%.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999609649181366}, {"text": "F1-score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9986907839775085}]}, {"text": "Modeling this document-level causal structure also improves both precision and recall on identifying intra-sentence causal relations, but with a relatively small margin.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.99955815076828}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9995900988578796}]}, {"text": "Compared to the local pairwise model + Score Replacement, the overall F1-score improvement from using global main event constraints is statistically significant with p<0.05.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9987140893936157}]}, {"text": "+Locality Constraints strengthens the effects of modeling main events and further improved the performance of both cross-and intra-sentence causal relation identification.", "labels": [], "entities": [{"text": "cross-and intra-sentence causal relation identification", "start_pos": 115, "end_pos": 170, "type": "TASK", "confidence": 0.7201032638549805}]}, {"text": "Next, adding sentential syntactic structure based constraints (+Syntactic Constraints) recovered additional intra-sentence causal relations and cross-sentence causal relations as well due to score replacement, and improved their recall by 4.4% and 2.8% respectively with little or no drop on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.9993064403533936}, {"text": "precision", "start_pos": 292, "end_pos": 301, "type": "METRIC", "confidence": 0.9986552000045776}]}, {"text": "Then, after adding discourse constraints (+Discourse Constraints), both precision and recall on intra-sentence causal relation identification were slightly improved while the performance on cross-sentence causal relation identification remains roughly the same, this is mainly due to the fact that few cross-sentence discourse relations were identified by the discourse parser we used.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9997945427894592}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.999445378780365}, {"text": "intra-sentence causal relation identification", "start_pos": 96, "end_pos": 141, "type": "TASK", "confidence": 0.7193461060523987}, {"text": "cross-sentence causal relation identification", "start_pos": 190, "end_pos": 235, "type": "TASK", "confidence": 0.6960639208555222}]}, {"text": "Finally, after adding conference constraints (+Coreference Constraints), the precision of cross-sentence causal relation identification was increased by 2.9%, with a small loss on recall, the F1-score was improved by 1.8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9996426105499268}, {"text": "cross-sentence causal relation identification", "start_pos": 90, "end_pos": 135, "type": "TASK", "confidence": 0.71672423183918}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9992682337760925}, {"text": "F1-score", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9995056390762329}]}, {"text": "Unsurprisingly, the overall performance on intrasentence causal relation identification was not affected much by coreference constraints since event coreference relations often involve events across sentences.", "labels": [], "entities": [{"text": "intrasentence causal relation identification", "start_pos": 43, "end_pos": 87, "type": "TASK", "confidence": 0.6773787066340446}]}, {"text": "Compared to the model considering global constraints only (the line + Locality Constraints), the overall F1-score improvement from using fine-grained causal structure constraints is statistically significant with p<0.01.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.999097466468811}]}, {"text": "To sum up, by modeling the global and finegrained aspects of causal structures, the performance of both intra-and cross-sentence causal re- lation identification was greatly improved by 3.9% and 7.5% in F1-score respectively.", "labels": [], "entities": [{"text": "cross-sentence causal re- lation identification", "start_pos": 114, "end_pos": 161, "type": "TASK", "confidence": 0.7165507674217224}, {"text": "F1-score", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9994244575500488}]}, {"text": "Compared to the local pairwise model + Score Replacement, the overall F1-score improvement from using both global main event constraints and finegrained causal structure constraints is statistically significant with p<0.002.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9978933930397034}]}, {"text": "shows performance comparisons of three models on documents with different lengths.", "labels": [], "entities": []}, {"text": "The first impression is that causal relation identification becomes harder when documents are longer.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.7599930961926779}]}, {"text": "If we look into the figure, the score replacement heuristic improves the performance of causal relation identification on medium-sized documents, but not on short (< 4 sentences) or long (> 10 sentences) documents.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6938337484995524}]}, {"text": "This may either due to little event coreference information for use in short documents or event coreference information becoming too noisy in long documents.", "labels": [], "entities": []}, {"text": "Compared to the mixed effects of the score replacement heuristic, the ILP system improved the performance of causal relation identification consistently in documents of any length, through modeling rich document-level causal structures.", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.7036208311716715}]}], "tableCaptions": [{"text": " Table 2: Performance of different models on causal relation identification", "labels": [], "entities": [{"text": "causal relation identification", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6941140194733938}]}]}