{"title": [{"text": "AudioCaps: Generating Captions for Audios in The Wild", "labels": [], "entities": [{"text": "Generating Captions", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.679464727640152}]}], "abstractContent": [{"text": "We explore the problem of audio caption-ing 1 : generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research.", "labels": [], "entities": [{"text": "audio caption-ing 1", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7825136184692383}]}, {"text": "We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset (Gemmeke et al., 2017).", "labels": [], "entities": [{"text": "AudioSet dataset", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.9582739472389221}]}, {"text": "Our thorough empirical studies not only show that our collected captions are indeed loyal to the audio inputs but also discover what forms of audio representation and captioning models are effective for audio captioning.", "labels": [], "entities": [{"text": "audio captioning", "start_pos": 203, "end_pos": 219, "type": "TASK", "confidence": 0.7193661332130432}]}, {"text": "From extensive experiments, we also propose two novel components that are integrable with any attention-based captioning model to help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.", "labels": [], "entities": []}], "introductionContent": [{"text": "Captioning, the task of translating a multimedia input source into natural language, has been substantially studied over the past few years.", "labels": [], "entities": [{"text": "Captioning", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9585437178611755}]}, {"text": "The vast majority of the journey has been through the visual senses ranging from static images to videos.", "labels": [], "entities": []}, {"text": "Yet, the exploration into the auditory sense has been circumscribed to human speech transcription (, leaving the basic natural form of sound in an uncharted territory of the captioning research.", "labels": [], "entities": [{"text": "human speech transcription", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7101420760154724}]}, {"text": "Recently, sound event detection has gained much attention such as DCASE challenges) along with the release of a large scale AudioSet dataset (.", "labels": [], "entities": [{"text": "sound event detection", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7291628122329712}, {"text": "AudioSet dataset", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.936455100774765}]}, {"text": "However, sound classification (e.g. predicting multiple labels fora given sound) and event detection (e.g. localizing the sound of interest in a clip) may not be sufficient fora full understanding of the sound.", "labels": [], "entities": [{"text": "sound classification", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7217832952737808}, {"text": "event detection", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7110339254140854}]}, {"text": "Instead, a natural sen-1 For a live demo and details, https://audiocaps.github.io [Audio Captioning] A muffled rumble with man and woman talking in the background while a siren blares in the distance.", "labels": [], "entities": [{"text": "Audio Captioning", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.725473165512085}]}, {"text": "tence offers a greater freedom to express a sound, because it allows to characterize objects along with their states, properties, actions and interactions.", "labels": [], "entities": []}, {"text": "For example, suppose that suddenly sirens are ringing in the downtown area.", "labels": [], "entities": []}, {"text": "As a natural reaction, people may notice the presence of an emergency vehicle, even though they are unable to see any flashing lights nor feel the rush of wind from a passing vehicle.", "labels": [], "entities": []}, {"text": "Instead of simply tagging this sound as ambulance or siren, it is more informative to describe which direction the sound is coming from or whether the source of the sound is moving closer or further away, as shown in.", "labels": [], "entities": []}, {"text": "To that end, we address the audio captioning problem for audios in the wild, which has not been studied yet, to the best of our knowledge.", "labels": [], "entities": [{"text": "audio captioning", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7325737625360489}]}, {"text": "This work focuses on one of the most important bases toward this research direction, contributing a large-scale dataset.", "labels": [], "entities": []}, {"text": "The overarching sources of in-the-wild sounds are grounded on the AudioSet (, so far the largest collection of sound events collected from Youtube videos.", "labels": [], "entities": []}, {"text": "We newly collect human-written sentences fora subset of AudioSet audio clips via crowdsourcing on Amazon Mechanical Turk (section 3).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9028749068578085}]}, {"text": "We also develop two simple yet effective techniques to generate captions through the joint use of multi-level pretrained features and better attention mechanism named aligned-semantic attention (section 4).", "labels": [], "entities": []}, {"text": "Lastly, we perform experiments contrasting between video-based captions and audio-focused captions by employing a variety of features and captioning models (section 5).", "labels": [], "entities": []}, {"text": "The contributions of this work are as follows.", "labels": [], "entities": []}, {"text": "1. To the best of our knowledge, this work is the first attempt to address the audio captioning task for sound in the wild.", "labels": [], "entities": [{"text": "audio captioning task", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8104218443234762}]}, {"text": "We contribute its first large-scale dataset named AudioCaps, which consists of 46K pairs of audio clips and text description.", "labels": [], "entities": []}, {"text": "2. We perform thorough empirical studies not only to show that our collected captions are indeed true to the audio inputs and but also to discover what forms of audio representations and captioning models are effective.", "labels": [], "entities": []}, {"text": "For example, we observe that the embeddings from large-scale pretrained VGGish (Hershey et al., 2017) are powerful in describing the audio input, and both temporal and semantic attention are helpful to enhance captioning performance.", "labels": [], "entities": []}, {"text": "3. From extensive experiments, we propose two simple yet effective technical components that further improve audio captioning performance: the top-down multi-scale encoder that enables the joint use of multi-level features and aligned semantic attention that advances the consistency between semantic attention and spatial/temporal attention.", "labels": [], "entities": [{"text": "audio captioning", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.72856804728508}]}], "datasetContent": [{"text": "Our AudioCaps dataset entails 46K audio caption pairs.", "labels": [], "entities": [{"text": "AudioCaps dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8894820511341095}]}, {"text": "The audio sources are rooted in AudioSet (, a large-scale audio event dataset, from which we draft the AudioCaps, as discussed below.", "labels": [], "entities": []}, {"text": "We present more details of data collection and statistics in the Appendix.", "labels": [], "entities": [{"text": "data collection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.6846186965703964}]}, {"text": "We perform several quantitative evaluations to provide more insights about our AudioCaps dataset.", "labels": [], "entities": [{"text": "AudioCaps dataset", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9597079455852509}]}, {"text": "Specifically, our experiments are designed to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "Are the collected captions indeed faithful to the audio inputs?", "labels": [], "entities": []}, {"text": "2. Which audio features are useful for audio captioning on our dataset?", "labels": [], "entities": [{"text": "audio captioning", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.748475968837738}]}, {"text": "3. What techniques can improve the performance of audio captioning?", "labels": [], "entities": [{"text": "audio captioning", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7459685504436493}]}, {"text": "We present further implementation details and more experimental results in the Appendix.", "labels": [], "entities": []}, {"text": "Some resulting audio-caption pairs can be found at https://audiocaps.github.io/supp.", "labels": [], "entities": []}, {"text": "Before presenting the results of our experiments on these three questions, we first explain the experimental setting and baseline models.", "labels": [], "entities": []}, {"text": "Audio captioning can be quantitatively evaluated by the language similarity between the predicted sentences and the groundtruths (GTs) such as BLEU (), CIDEr (, METEOR (Banerjee and), ROUGE-L () and SPICE (.", "labels": [], "entities": [{"text": "Audio captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.708430677652359}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9983378648757935}, {"text": "METEOR", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9281091690063477}, {"text": "ROUGE-L", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.9737175703048706}]}, {"text": "In all metrics, higher scores indicate better performance.", "labels": [], "entities": []}, {"text": "Audios are resampled to 16kHz, and stereo is converted into mono by averaging both channels.", "labels": [], "entities": []}, {"text": "We zero-pad clips that are shorter than 10 seconds and extract three levels of audio features.", "labels": [], "entities": []}, {"text": "For the low-level audio feature, the lengthy raw audios are average-pooled by the WaveNet encoder as in.", "labels": [], "entities": []}, {"text": "For the mid-level feature, mel-frequency cepstral coefficients (MFCC) ( are extracted using librosa (McFee et al., 2015) with a window size of 1024, an overlap of 360 and the number of frames at 240, and encoded further with a bi-directional LSTM followed by a gated convolutional encoder ().", "labels": [], "entities": []}, {"text": "Lastly, we use two high-level features: the 24th output layer of SoundNet 6 (Aytar et al., 2016) with a (10 \u00d7 1024) dimension and the final output embedding of VGGish) with a (10 \u00d7 128) dimension of (time \u00d7 embedding).", "labels": [], "entities": [{"text": "VGGish", "start_pos": 160, "end_pos": 166, "type": "DATASET", "confidence": 0.9231926202774048}]}, {"text": "To contrast with video captioning datasets, we also extract video features at the frame-level and at the sequence-level from YouTube clips.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7424065172672272}]}, {"text": "For frame features, we use VGG16 ( pretrained on the ILSVRC-2014 dataset.", "labels": [], "entities": [{"text": "VGG16", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8837871551513672}, {"text": "ILSVRC-2014 dataset", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.9909555912017822}]}, {"text": "For sequence features, we use C3D 8 () pretrained on the Sport1M dataset ().", "labels": [], "entities": [{"text": "Sport1M dataset", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9929496049880981}]}, {"text": "We extract subsequent frames with 50% overlap centered at each time step on the input clips for AudioSet videos, while proceeding with no overlap for MSR-VTT clips as in the original paper.", "labels": [], "entities": []}, {"text": "We sample videos at 25fps.", "labels": [], "entities": []}, {"text": "The full ontology of selected labels is outlined in. shows the number of clips per word label.", "labels": [], "entities": []}, {"text": "The original AudioSet has an extreme label bias.", "labels": [], "entities": []}, {"text": "For instance, a difference of 660,282 between the average of top 3 most common and average of top 3 most uncommon classes.", "labels": [], "entities": []}, {"text": "Whereas our dataset at the moment has a difference of 971.", "labels": [], "entities": []}, {"text": "Notice the label bias is significantly reduced in comparison to the original AudioSet.", "labels": [], "entities": [{"text": "AudioSet", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9447373151779175}]}, {"text": "We plan to reduce this further in the upcoming releases.", "labels": [], "entities": []}, {"text": "compares our audio captioning dataset with some representative benchmarks of video captioning: MSR-VTT () and LSMDC (  property of our dataset is that the portion of verbs in the vocabularies are larger than the others.", "labels": [], "entities": [{"text": "LSMDC", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9154714941978455}]}, {"text": "This may imply that the captions describe what is happening rather than what is in the content.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Some statistics of AudioCaps dataset. We also  show average and median (in parentheses) values. la- bels refer to the semantic attributes.", "labels": [], "entities": [{"text": "AudioCaps dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9470694661140442}]}, {"text": " Table 2: Captioning results of different methods on AudioCaps measured by language similarity metrics.", "labels": [], "entities": [{"text": "Captioning", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9767285585403442}]}, {"text": " Table 3: Upper-bound of aligned semantic attention by language similarity metrics.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of captioning results between  video-based and audio-based datasets. The first three  methods perform captioning using only audios while  the last method C3D-LSTM, only use videos. The  gaps empirically show how much AudioCaps is audio- oriented in contrast to MSR-VTT.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of AudioCaps with MSR-VTT (Xu et al., 2016), LSMDC (Rohrbach et al., 2017).", "labels": [], "entities": [{"text": "LSMDC", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.6519800424575806}]}]}