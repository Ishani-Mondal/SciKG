{"title": [{"text": "An Annotated Dataset of Literary Entities", "labels": [], "entities": [{"text": "Annotated Dataset of Literary Entities", "start_pos": 3, "end_pos": 41, "type": "DATASET", "confidence": 0.835385549068451}]}], "abstractContent": [{"text": "We present anew dataset comprised of 210,532 tokens evenly drawn from 100 different English-language literary texts annotated for ACE entity categories (person, location, geo-political entity, facility, organization, and vehicle).", "labels": [], "entities": []}, {"text": "These categories include non-named entities (such as \"the boy\", \"the kitchen\") and nested structure (such as [[the cook]'s sister]).", "labels": [], "entities": []}, {"text": "In contrast to existing datasets built primarily on news (focused on geo-political entities and organizations), literary texts offer strikingly different distributions of entity categories , with much stronger emphasis on people and description of settings.", "labels": [], "entities": []}, {"text": "We present empirical results demonstrating the performance of nested entity recognition models in this domain; training natively on in-domain literary data yields an improvement of over 20 absolute points in F-score (from 45.7 to 68.3), and mitigates a disparate impact in performance for male and female entities present in models trained on news data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 208, "end_pos": 215, "type": "METRIC", "confidence": 0.998241662979126}]}], "introductionContent": [{"text": "Computational literary analysis works at the intersection of natural language processing and literary studies, drawing on the structured representation of text to answer literary questions about character (, objects and place.", "labels": [], "entities": [{"text": "Computational literary analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6203603545824686}]}, {"text": "Much of this work relies on the ability to extract entities accurately, including work focused on modeling (.", "labels": [], "entities": []}, {"text": "And yet, with notable exceptions (, nearly all of this work tends to use NER models that have been trained on non-literary data, for the simple reason that labeled data exists for domains like news through standard datasets like ACE (), CoNLL and)-and even historical non-fiction)-but not for literary texts.", "labels": [], "entities": []}, {"text": "This is naturally problematic for several reasons: models trained on out-of-domain data surely degrade in performance when applied to a very different domain, and especially for NER, as has shown; and without indomain test data, it is difficult to directly estimate the severity of this degradation.", "labels": [], "entities": [{"text": "NER", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9218920469284058}]}, {"text": "At the same time, literary texts also demand slightly different representations of entities.", "labels": [], "entities": []}, {"text": "While classic NER models typically presume a flat entity structure, relevant characters and places (and other entities) in literature need not be flat, and need not be named: The cook's sister ate lunch contains two PER entities ( and).", "labels": [], "entities": []}, {"text": "We present in this work anew dataset of entity annotations fora wide sample of 210,532 tokens from 100 literary texts to help address these issues and help advance computational work on literature.", "labels": [], "entities": []}, {"text": "These annotations follow the guidelines set forth by the ACE 2005 entity tagging task) in labeling all nominal entities (named and common alike), including those with nested structure.", "labels": [], "entities": [{"text": "ACE 2005 entity tagging task", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8437941193580627}]}, {"text": "In evaluating the stylistic difference between the texts in ACE 2005 (primarily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention directly translates into substantially improved accuracies for those classes when models are trained on them.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8983425498008728}, {"text": "accuracies", "start_pos": 276, "end_pos": 286, "type": "METRIC", "confidence": 0.9746528267860413}]}, {"text": "The dataset is freely available for download under a Creative Commons ShareAlike 4.0 license at https://github.com/dbamman/ litbank.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance on nested entity recognition using the layered BiLSTM-CRF of", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7307090163230896}]}, {"text": " Table 3: Recall on literary test data by the gender of  PER entity for models trained on ACE and literary data.", "labels": [], "entities": []}, {"text": " Table 4: Recall on literary test data by the gender of  PER entity for models trained on ACE and literary data,  excluding all gold entities beginning with Mrs., Miss  and Mr.", "labels": [], "entities": [{"text": "PER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9885099530220032}]}]}