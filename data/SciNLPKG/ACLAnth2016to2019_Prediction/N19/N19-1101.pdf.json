{"title": [{"text": "Generating Token-Level Explanations for Natural Language Inference", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6788480480511984}]}], "abstractContent": [{"text": "The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8270857234795889}, {"text": "sentence pair classification", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.7131310999393463}]}, {"text": "While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences.", "labels": [], "entities": []}, {"text": "In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose.", "labels": [], "entities": []}, {"text": "We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task.", "labels": [], "entities": [{"text": "LIME", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9166982769966125}]}, {"text": "We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions.", "labels": [], "entities": [{"text": "Multiple Instance Learning (MIL)", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6651080548763275}]}, {"text": "The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI.", "labels": [], "entities": [{"text": "zero-shot single-sentence tagging", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6733964284261068}]}, {"text": "We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the en-tailment relation.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8603627979755402}]}, {"text": "We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9991962313652039}]}], "introductionContent": [{"text": "Large-scale datasets for Natural Language Inference (NLI) have enabled the development of many deep-learning models.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8307907780011495}]}, {"text": "The task is modeled as 3-way classification of the entailment relation between a pair of sentences.", "labels": [], "entities": []}, {"text": "Model performance is assessed through accuracy on a held-out test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9991047978401184}]}, {"text": "While state-of-the-art models achieve high accuracy, their complexity makes it difficult to interpret their behavior.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9970877766609192}]}, {"text": "Explaining the predictions made by classifiers has been of increasing concern (Doshi-Velez and", "labels": [], "entities": [{"text": "Explaining the predictions made", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8379479199647903}]}], "datasetContent": [{"text": "We evaluate the generated explanations through evaluation of token-level F 1 scores comparing them against tokens selected by humans to explain the entailment relation using the e-SNLI dataset ().", "labels": [], "entities": [{"text": "e-SNLI dataset", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.9082895517349243}]}, {"text": "The development split of the e-SNLI dataset is used for hyperparameter selection and we report results on the test split.", "labels": [], "entities": [{"text": "e-SNLI dataset", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8602072298526764}, {"text": "hyperparameter selection", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6415487974882126}]}, {"text": "Where multiple annotations are available fora sentence pair, the union of the annotations is taken.", "labels": [], "entities": []}, {"text": "We also report average runtime per sentence in seconds measured using 1 thread on an AWS c4.xlarge instance.", "labels": [], "entities": [{"text": "AWS c4.xlarge instance", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.9534387191136678}]}, {"text": "Implementation Details The model is implemented in AllenNLP (  and we optimized our model with Adagrad, selecting the models which attained high hypothesis F 1 without greatly affecting the accuracy of entailment task (approx 81% for the thresholded attention model).", "labels": [], "entities": [{"text": "AllenNLP", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9753931164741516}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9971349239349365}]}, {"text": "The cell state and hidden dimension was 200 for the LSTM sentence encoder.", "labels": [], "entities": []}, {"text": "The projection for attention, f attend , was a single layer 200 dimension feed forward network with ReLU activation.", "labels": [], "entities": [{"text": "f attend", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.7322477400302887}]}, {"text": "The final feed forward classifier, f cls , dimension was and ReLU activation over the first 2 layers.", "labels": [], "entities": [{"text": "dimension", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.983163058757782}, {"text": "ReLU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9874310493469238}]}, {"text": "For the comparison against black-box explanation mechanisms, we use the code made public by the authors of the respective works setting any hyperparameters to the default values or those suggested in the papers.", "labels": [], "entities": []}, {"text": "Results Our experimental results) indicate that the LIME black-box explanation technique over the model described in Section 2 provides token-level explanations that are more similar to human judgments than thresholding the attention distributions.", "labels": [], "entities": [{"text": "LIME black-box explanation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6998456219832102}]}, {"text": "We show that the addition of MIL regularizers for generating explanations using thresholded attention improved precision and recall hypothesis explanations.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9993689656257629}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9773736000061035}]}, {"text": "However, similar improvements were not realized for the premise sentence.", "labels": [], "entities": []}, {"text": "While the black-box methods generated better explanations than thresholded attention, they were 3 orders of magnitude slower.", "labels": [], "entities": []}, {"text": "Only LIME was able to generate good tokenlevel explanations for the premise.", "labels": [], "entities": [{"text": "LIME", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.7859718203544617}]}, {"text": "This is in contrast to the attention-based explanations of the premise (in the model that LIME was run on) which could not generate satisfactory explanations (see row 2 of).", "labels": [], "entities": []}, {"text": "This supports findings in recent works) that indicate that attention does not always correspond to other measures of feature importance.", "labels": [], "entities": []}, {"text": "We also found that the black-box model explanation methods behave differently given the same model under test: the premise explanation generated by the Anchors method was more inline with what the model attended to, reflected by the lower recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 239, "end_pos": 245, "type": "METRIC", "confidence": 0.9971093535423279}]}, {"text": "The fully supervised model had high precision yet (relatively) low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9990725517272949}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9990836381912231}]}, {"text": "We observed it has a bias towards predicting common words that often appear in highlights (e.g. 'man', 'woman', 'dog', 'people') for both premise and hypothesis sentences rather than highlighting keywords that would form an instance-specific explanation.", "labels": [], "entities": [{"text": "predicting common words that often appear in highlights (e.g. 'man', 'woman', 'dog', 'people') for both premise and hypothesis sentences", "start_pos": 34, "end_pos": 170, "type": "Description", "confidence": 0.6951987935650733}]}, {"text": "This behaviour is also more pronounced in the premise sentence highlights rather than the hypothesis.", "labels": [], "entities": []}, {"text": "We reason that this due to how the SNLI dataset was constructed: a premise sentence was used to generate 3 hypothesis sentences (entailed, contradicted and neutral).", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.7476396560668945}]}, {"text": "This is corroborated by a survey of 250 instances from the SNLI dataset, where we found that all or part of the subject noun phrase remained unchanged between the premise and hypothesis sentences 60% of the time.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.9231946468353271}]}, {"text": "While the supervised model correctly captured commonly occurring text patterns, as demonstrated by the high F 1 scores, this behaviour alone was not sufficient to identify tokens that correlated with the entailment label.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9634623527526855}]}, {"text": "We found that most of the commonly predicted tokens by our supervised model did not appear in lists of features highly correlated with the entailment label ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Token-level scores for human-selected explanations of NLI using the e-SNLI dataset. The select-all  baseline precision for the premise is 18.5% and 35.2% for the hypothesis.", "labels": [], "entities": [{"text": "e-SNLI dataset", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9370644390583038}, {"text": "select-all  baseline precision", "start_pos": 98, "end_pos": 128, "type": "METRIC", "confidence": 0.5823063751061758}]}]}