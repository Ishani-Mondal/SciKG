{"title": [{"text": "Integration of Knowledge Graph Embedding into Topic Modeling with Hierarchical Dirichlet Process", "labels": [], "entities": [{"text": "Topic Modeling", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.790398895740509}]}], "abstractContent": [{"text": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models.", "labels": [], "entities": []}, {"text": "In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparamet-ric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8012619316577911}]}, {"text": "Specifically , we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretabil-ity of topics.", "labels": [], "entities": []}, {"text": "An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs.", "labels": [], "entities": []}, {"text": "Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.", "labels": [], "entities": [{"text": "document classification", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.6050475239753723}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.8476164937019348}]}], "introductionContent": [{"text": "Topic models, such as Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) (, play significant roles in helping machines interpret text documents.", "labels": [], "entities": []}, {"text": "Topic models consider documents as a bag of words.", "labels": [], "entities": []}, {"text": "Given the word information, topic models formulate documents as mixtures of latent topics, where these topics are generated via the multinomial distributions over words.", "labels": [], "entities": []}, {"text": "Bayesian methods are utilized to extract topical structures from the document-word frequency representations of the text corpus.", "labels": [], "entities": []}, {"text": "Without supervision, however, it is found that the topics generated from these models are often not interpretable ().", "labels": [], "entities": []}, {"text": "In recent studies, incorporating knowledge of different forms as a supervision has become a powerful strategy for discovering meaningful topics (   Most conventional approaches take prior domain knowledge into account to improve the topic coherence (.", "labels": [], "entities": []}, {"text": "One commonly used domain knowledge is based on word correlations ().", "labels": [], "entities": []}, {"text": "For example, must-links and cannot-links among words are generated by domain experts to help topic modeling ( . Another useful form of knowledge for topic discoveries is based on word semantics).", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7681763768196106}]}, {"text": "In particular, word embedding (, in which bag of words are transformed into vector representations so that contexts are embedded into those word vectors, are used as semantic regularities to enhance topic models).", "labels": [], "entities": []}, {"text": "Knowledge graph (KG) embedding) learns a low-dimensional continuous vector space for entities and relations to preserve the inherent structure of KGs.", "labels": [], "entities": []}, {"text": "proposes KGE-LDA to incorporate embeddings of KGs into topic models to extract better topic representations for documents and shows promising performance.", "labels": [], "entities": []}, {"text": "However, KGE-LDA forces words and entities to have identical latent representations, which is a rather restrictive assumption that prevents the topic model from recovering correct underlying latent structures of the data, especially in scenarios where only partial KGs are available.", "labels": [], "entities": []}, {"text": "This paper develops topic modeling with knowledge graph embedding (TMKGE), a hierarchical Dirichlet process (HDP) based model to extract more coherent topics by taking advantage of the KG structure.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7322250008583069}]}, {"text": "Unlike KGE-LDA, the proposed TMKGE allows for more flexible sharing of information between words and entities, by using a multinomial distribution to model the words and a multivariate Gaussian mixture to model the entities.", "labels": [], "entities": []}, {"text": "With this approach, we introduce two proportional vectors, one for words and one for entities.", "labels": [], "entities": []}, {"text": "In contrast, KGE-LDA only uses one, shared by both words and entities.", "labels": [], "entities": []}, {"text": "Similar to HDP, TMKGE includes a collection of Dirichlet processes (DPs) at both corpus and document levels.", "labels": [], "entities": []}, {"text": "The atoms of corpus-level DP form the base measure for document levels DPs of words and entities.", "labels": [], "entities": []}, {"text": "Therefore, the atoms of corpus-level DP can represent word topics, entity mixture components, or both of them.", "labels": [], "entities": []}, {"text": "provides an overview of TMKGE, where two sources of inputs, bag of words and KG embedding, extracted from corpus and KGs respectively, are passed into TMKGE.", "labels": [], "entities": []}, {"text": "As a nonparametric model, TMKGE does not assume a fix number of topics or entity mixture components as constraints.", "labels": [], "entities": [{"text": "TMKGE", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.7666827440261841}]}, {"text": "Instead, it learns the number of topics and entity mixture components automatically from the data.", "labels": [], "entities": []}, {"text": "Furthermore, an efficient online variational inference algorithm is developed, based on Sethuraman's stick-breaking construction of HDP.", "labels": [], "entities": []}, {"text": "We in fact construct stick-breaking inference in a minibatch fashion (, to derive a more efficient and scalable coordinateaccent variational inference for TMKGE.", "labels": [], "entities": []}, {"text": "Summary of contributions: TMKGE is a Bayesian nonparametric model to extract more coherent topics by taking advantage of knowledge graph structures.", "labels": [], "entities": []}, {"text": "We introduce two proportional vectors for more flexible sharing of information between words and entities.", "labels": [], "entities": []}, {"text": "We derive an efficient and scalable parameter estimation algorithm via online variational inference.", "labels": [], "entities": []}, {"text": "Finally, we empirically demonstrate the effectiveness of TMKGE in topic discovering and document classification.", "labels": [], "entities": [{"text": "topic discovering", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8509416282176971}, {"text": "document classification", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7524220049381256}]}], "datasetContent": [{"text": "We evaluate TMKGE on two experimental tasks and compare its performance to those of LDA, HDP and KGE-LDA.", "labels": [], "entities": []}, {"text": "For LDA and HDP, we use the online variational inference implementations.", "labels": [], "entities": []}, {"text": "More precisely, we will evaluate our framework by the test whether it finds coherent and meaningful topics and the test whether it can achieve good performance in document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.7250778526067734}]}, {"text": "We run our experiments on three popular datasets; 20 Newsgroups, NIPS and the Ohsumed corpus.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9468883275985718}, {"text": "Ohsumed corpus", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9370310008525848}]}, {"text": "The 20 Newsgroups dataset contains 18,846 documents evenly categorized into 20 different categories.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6771727800369263}]}, {"text": "The NIPS dataset contains 1,740 papers from the NIPS conference.", "labels": [], "entities": [{"text": "NIPS dataset contains 1,740 papers from the NIPS conference", "start_pos": 4, "end_pos": 63, "type": "DATASET", "confidence": 0.7785597112443712}]}, {"text": "The Ohsumed corpus is from the MEDLINE database.", "labels": [], "entities": [{"text": "Ohsumed corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9164751470088959}, {"text": "MEDLINE database", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9506848156452179}]}, {"text": "We consider the 13,929 unique Cardiovascular diseases abstracts in the first 20,000 abstracts of the year 1996.", "labels": [], "entities": []}, {"text": "Each document in the set has one or more associated categories from the 23 disease categories.", "labels": [], "entities": []}, {"text": "The documents belonging to multiple categories are eliminated so that 7,400 documents belonging to only one category remain.", "labels": [], "entities": []}, {"text": "The datasets are tokenized with Stanford CoreNLP ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9193210005760193}]}, {"text": "After standard pre-processing (such as removing stop words), there are 20,881 distinct words in the 20 Newsgroups dataset, 14,482 distinct words in the NIPS dataset and 8,446 distinct words in the Ohsumed dataset.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.7657244006792704}, {"text": "NIPS dataset", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.9915516674518585}, {"text": "Ohsumed dataset", "start_pos": 197, "end_pos": 212, "type": "DATASET", "confidence": 0.9837568998336792}]}], "tableCaptions": [{"text": " Table 1: Topic Coherence of all models on three datasets with different number of top words. A higher PMI score  implies a more coherent topic. Improvements of TMKGE over other methods are significant.", "labels": [], "entities": [{"text": "PMI score", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9359060823917389}]}, {"text": " Table 2: Example topics learned from three datasets by TMKGE with K = 300 and T = 20, and KGE-LDA with  K = 30. The last row for each model is the topic coherence computed using the 4,776,093 Wikipedia documents  as reference. Some medical short words: pbl = Peripheral blood leucocyte, meh = Mean erythrocyte hemoglobin.", "labels": [], "entities": []}, {"text": " Table 3: Document classification accuracy a five-way  classification on the comp subject of 20 Newsgroups  dataset and on the top five most frequent labels of  ohsumed dataset (no labels for NIPS dataset).", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9103729724884033}, {"text": "20 Newsgroups  dataset", "start_pos": 93, "end_pos": 115, "type": "DATASET", "confidence": 0.6523854931195577}, {"text": "NIPS dataset", "start_pos": 192, "end_pos": 204, "type": "DATASET", "confidence": 0.9584029018878937}]}, {"text": " Table 4. TMKGE  achieves the best performance on all models.", "labels": [], "entities": [{"text": "TMKGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.5681207776069641}]}, {"text": " Table 4: Document classification: all subjects of 20  Newsgroups dataset for more complete comparisons.  Clearly shown is the best performances of TMKGE", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7906979024410248}, {"text": "20  Newsgroups dataset", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.7048346400260925}, {"text": "TMKGE", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.7522861361503601}]}]}