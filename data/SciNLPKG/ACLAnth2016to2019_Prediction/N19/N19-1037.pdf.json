{"title": [{"text": "HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition", "labels": [], "entities": [{"text": "HiGRU", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8728130459785461}, {"text": "Emotion Recognition", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6563450247049332}]}], "abstractContent": [{"text": "In this paper, we address three challenges in utterance-level emotion recognition in dialogue systems: (1) the same word can deliver different emotions in different contexts; (2) some emotions are rarely seen in general dialogues; (3) long-range contextual information is hard to be effectively captured.", "labels": [], "entities": [{"text": "utterance-level emotion recognition", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.645200788974762}]}, {"text": "We therefore propose a hierarchical Gated Recurrent Unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings.", "labels": [], "entities": []}, {"text": "Moreover, we promote the framework to two variants, Hi-GRU with individual features fusion (HiGRU-f) and HiGRU with self-attention and features fusion (HiGRU-sf), so that the word/utterance-level individual inputs and the long-range con-textual information can be sufficiently utilized.", "labels": [], "entities": []}, {"text": "Experiments on three dialogue emotion datasets, IEMOCAP, Friends, and Emo-tionPush demonstrate that our proposed Hi-GRU models attain at least 8.7%, 7.5%, 6.0% improvement over the state-of-the-art methods on each dataset, respectively.", "labels": [], "entities": [{"text": "IEMOCAP", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.6745564341545105}]}, {"text": "Particularly, by utilizing only the textual feature in IEMO-CAP, our HiGRU models gain at least 3.8% improvement over the state-of-the-art conversational memory network (CMN) with the tri-modal features of text, video, and audio.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotion recognition is a significant artificial intelligence research topic due to the promising potential of developing empathetic machines for people.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9396497309207916}]}, {"text": "Emotion is a universal phenomena across different cultures and mainly consists of six basic types: anger, disgust, fear, happiness, sadness, and surprise.", "labels": [], "entities": [{"text": "surprise", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.946904718875885}]}, {"text": "In this paper, we focus on textual dialogue systems because textual feature dominates the performance over audio and video features (Poria et al., 2015, 2017).", "labels": [], "entities": []}, {"text": "In utterance-level emotion recognition, an utterance is a unit of speech bounded by breathes or pauses and its goal is to tag each utterance in a dialogue with the indicated emotion.", "labels": [], "entities": [{"text": "utterance-level emotion recognition", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.681284228960673}]}, {"text": "In this task, we address three challenges: First, the same word can deliver different emotions in different contexts.", "labels": [], "entities": []}, {"text": "For example, in, the word \"okay\" can deliver three different emotions, anger, neutral, and joy, respectively.", "labels": [], "entities": []}, {"text": "Strong emotions like joy and anger maybe indicated by the symbols \"!\" or \"?\" along the word.", "labels": [], "entities": []}, {"text": "To identify a speaker's emotion precisely, we need to explore the dialogue context sufficiently.", "labels": [], "entities": []}, {"text": "Second, some emotions are rarely seen in general dialogues.", "labels": [], "entities": []}, {"text": "For example, people are usually calm and present a neutral emotion while only in some particular situations, they express strong emotions, like anger or fear.", "labels": [], "entities": []}, {"text": "Thus we need to be sensitive to the minority emotions while relieving the effect of the majority emotions.", "labels": [], "entities": []}, {"text": "Third, the long-range contextual information is hard to be effectively captured in an utterance/dialogue, especially when the length of an utterance/dialogue in the testing set is longer than those in the training set.", "labels": [], "entities": []}, {"text": "To tackle these challenges, we propose a hierarchical Gated Recurrent Unit (HiGRU) framework for the utterance-level emotion recognition in dialogue systems.", "labels": [], "entities": [{"text": "utterance-level emotion recognition", "start_pos": 101, "end_pos": 136, "type": "TASK", "confidence": 0.6622001131375631}]}, {"text": "More specifically, HiGRU is composed by two levels of bidirectional GRUs, a lower-level GRU to model the word sequences of each utterance to produce individual utterance embeddings, and an upper-level GRU to capture the sequential and contextual relationship of utterances.", "labels": [], "entities": [{"text": "HiGRU", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8283170461654663}]}, {"text": "We further promote the proposed Hi-GRU to two variants, HiGRU with individual features fusion (HiGRU-f), and HiGRU with selfattention and features fusion.", "labels": [], "entities": [{"text": "HiGRU", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.8694407939910889}, {"text": "HiGRU", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8872083425521851}]}, {"text": "In HiGRU-f, the individual inputs, i.e., the word embeddings in the lower-level GRU and the individual utterance embeddings in the upper-level GRU, are concatenated with the hidden states to generate the contextual word/utterance embeddings, respectively.", "labels": [], "entities": [{"text": "HiGRU-f", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8606322407722473}]}, {"text": "In HiGRU-sf, a self-attention layer is placed on the hidden states from the GRU to learn long-range contextual embeddings, which are concatenated with the original individual embeddings and the hidden states to generate the contextual word/utterance embeddings.", "labels": [], "entities": [{"text": "GRU", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8927417993545532}]}, {"text": "Finally, the contextual utterance embedding is sent to a fullyconnected (FC) layer to determine the corresponding emotion.", "labels": [], "entities": []}, {"text": "To alleviate the effect of data imbalance issue, we follow) to train our models by minimizing a weighted categorical cross-entropy.", "labels": [], "entities": []}, {"text": "We summarize our contributions as follows: \u2022 We propose a HiGRU framework to better learn both the individual utterance embeddings and the contextual information of utterances, so as to recognize the emotions more precisely.", "labels": [], "entities": []}, {"text": "\u2022 We propose two progressive HiGRU variants, HiGRU-f and HiGRU-sf, to sufficiently incorporate the individual word/utterance-level information and the long-range contextual information respectively.", "labels": [], "entities": [{"text": "HiGRU-f", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8647566437721252}, {"text": "HiGRU-sf", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.8800918459892273}]}, {"text": "\u2022 We conduct extensive experiments on three textual dialogue emotion datasets, IEMO-CAP, Friends, and EmotionPush.", "labels": [], "entities": [{"text": "IEMO-CAP", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.7613122463226318}]}, {"text": "The results demonstrate that our proposed HiGRU models achieve at least 8.7%, 7.5%, 6.0% improvement over state-of-the-art methods on each dataset, respectively.", "labels": [], "entities": []}, {"text": "Particularly, by utilizing only the textual feature in IEMO-CAP, our proposed HiGRU models gain at least 3.8% improvement over the existing best model, conversational memory network (CMN) with not only the text feature, but also the visual, and audio features.", "labels": [], "entities": [{"text": "IEMO-CAP", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.7736107707023621}]}], "datasetContent": [{"text": "We conduct systematical experiments to demonstrate the advantages of our proposed HiGRU models.", "labels": [], "entities": []}, {"text": "The experiments are carried out on three textual dialogue emotion datasets (see the statistics in Table 1): Data Preprocessing.", "labels": [], "entities": []}, {"text": "We preprocess the datasets by the following steps: (1) The utterances are split into tokens with each word being made into the lowercase; (2) All non-alphanumerics except \"?\" and \"!\" are removed because these two symbols usually exhibit strong emotions, such as surprise, joy and anger; (3) We build a dictionary based on the words and symbols extracted, and follow) to represent the tokens by the publicly available 300-dimensional word2vec 4 vectors trained on 100 billion words from Google News.", "labels": [], "entities": []}, {"text": "The tokens not included in the word2vec dictionary are initialized by randomly-generated vectors.", "labels": [], "entities": []}, {"text": "To conduct fair comparison, we adopt two metrics as (, the weighted accuracy (WA) and unweighted accuracy (UWA): where p c is the percentage of the class c in the testing set, and ac is the corresponding accuracy.", "labels": [], "entities": [{"text": "accuracy (WA)", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.8946678787469864}, {"text": "unweighted accuracy (UWA)", "start_pos": 86, "end_pos": 111, "type": "METRIC", "confidence": 0.8354199647903442}, {"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9962807297706604}]}, {"text": "Generally, recognizing strong emotions may provide more value than detecting the neutral emotion ().", "labels": [], "entities": []}, {"text": "Thus, in Friends and EmotionPush, UWA is a more favorite evaluation metric because WA is heavily compromised with the large proportion of the neutral emotion.", "labels": [], "entities": [{"text": "UWA", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8963532447814941}, {"text": "WA", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9846749305725098}]}], "tableCaptions": [{"text": " Table 1: Statistics of the textual dialogue datasets.", "labels": [], "entities": [{"text": "textual dialogue datasets", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.6530326704184214}]}, {"text": " Table 3: Experimental results on Friends and EmotionPush. In the Train column, F(E) denotes the model is trained  on only one training set, Friends or EmotionPush. F+E means the model is trained on the mixed training set while  validated and tested individually. The results of SA-BiLSTM and CNN-DCNN are from (Luo et al., 2018) and  (Khosla, 2018), respectively.", "labels": [], "entities": [{"text": "F(E)", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9493342787027359}, {"text": "CNN-DCNN", "start_pos": 293, "end_pos": 301, "type": "DATASET", "confidence": 0.9029093980789185}]}]}