{"title": [{"text": "Detection of Abusive Language: the Problem of Biased Datasets", "labels": [], "entities": [{"text": "Detection of Abusive Language", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9167666137218475}]}], "abstractContent": [{"text": "We discuss the impact of data bias on abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6429836650689443}]}, {"text": "We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced.", "labels": [], "entities": []}, {"text": "Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling.", "labels": [], "entities": []}, {"text": "Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.", "labels": [], "entities": []}, {"text": "Examples are (1)-(3).", "labels": [], "entities": []}, {"text": "In the literature, closely related terms include hate speech ( or cyber bullying (.", "labels": [], "entities": []}, {"text": "While there maybe nuanced differences in meaning, they are all compatible with the general definition above.", "labels": [], "entities": []}, {"text": "(1) stop editing this, you dumbass.", "labels": [], "entities": []}, {"text": "(2) Just want to slap the stupid out of these bimbos!!!", "labels": [], "entities": []}, {"text": "(3) Go lick a pig you arab muslim piece of scum.", "labels": [], "entities": []}, {"text": "Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing.", "labels": [], "entities": []}, {"text": "NLP methods are required to focus human review efforts towards the most relevant microposts.", "labels": [], "entities": []}, {"text": "In this paper, we examine the issue of data bias.", "labels": [], "entities": [{"text": "data bias", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.683327928185463}]}, {"text": "For the creation of manually annotated datasets, randomly sampling microposts from large social media platforms typically results in a too small proportion of abusive comments (.", "labels": [], "entities": []}, {"text": "Therefore, more focused sampling strategies have to be applied which Present affiliation: Leibniz ScienceCampus, Heidelberg/Mannheim, Germany 1 http://thelawdictionary.org/ cause biases in the resulting datasets.", "labels": [], "entities": []}, {"text": "We show what implications this has on classifiers trained on these datasets: Previous evaluations reported high classification performance on datasets with difficult cases of abusive language, e.g. implicit abuse ( \u00a72).", "labels": [], "entities": []}, {"text": "Contrarily, we find that the high classification scores are likely to be the result of modeling the bias in those datasets.", "labels": [], "entities": []}, {"text": "Although we will explicitly name shortcomings of existing individual datasets, our paper is not intended as a reproach of those who created them.", "labels": [], "entities": []}, {"text": "On the contrary, we acknowledge the great efforts the researchers have taken to provide these resources.", "labels": [], "entities": []}, {"text": "Without them, much existing research would not have been possible.", "labels": [], "entities": []}, {"text": "However, we also noticed alack of awareness of the special properties of those datasets among researchers using them.", "labels": [], "entities": []}, {"text": "As we will illustrate with specific examples, this may result in unforeseen results of particular classification approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the limited space of this paper, we restrict our discussion to frequently cited (publicly available) datasets and datasets from shared tasks.", "labels": [], "entities": []}, {"text": "Substantial interannotation agreement has also been reported with these datasets.", "labels": [], "entities": []}, {"text": "As we focus on the detection of abusive language in general, for those datasets containing more fine-grained class inventories describing subtypes of abusive language 2 , we conflate the categories to one general category.", "labels": [], "entities": []}, {"text": "As a result, there are always only two categories: abuse and noabuse.", "labels": [], "entities": []}, {"text": "This merging removes differences between the individual annotation schemes that would otherwise impede a meaningful comparison.", "labels": [], "entities": []}, {"text": "shows a brief summary of the different datasets.", "labels": [], "entities": []}, {"text": "Among the properties, we list the performance of a text classifier in the right-most column.", "labels": [], "entities": []}, {"text": "Since in previous work performance on the different datasets was reported on the basis of different types of classifiers and also varying evaluation metrics, we ran the same classifier on all datasets in order to ensure a meaningful comparison.", "labels": [], "entities": []}, {"text": "We chose FastText, which is an efficient supervised classifier known to produce stateof-the-art performance on many text classification tasks 3 (Joulin et al., 2017) and whose results are easy to reproduce.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8176577885945638}]}, {"text": "Performance is evaluated in a 10-fold crossvalidation setting using the macroaverage F1-score.", "labels": [], "entities": [{"text": "macroaverage", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.6930408477783203}, {"text": "F1-score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8014897108078003}]}, {"text": "also describes the way the datasets were sampled.", "labels": [], "entities": []}, {"text": "Not a single dataset has been produced by pure random sampling.", "labels": [], "entities": []}, {"text": "This would always result in tiny proportions of abusive language.", "labels": [], "entities": []}, {"text": "For example, estimate that on Twitter, there are only between 0.1% up to at most 3% abusive tweets.", "labels": [], "entities": []}, {"text": "What comes closest to random sampling is the procedure followed by, and the Kagglechallenge.", "labels": [], "entities": [{"text": "Kagglechallenge", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.835070013999939}]}, {"text": "They took a random sample and applied some heuristics in order to boost the proportion of abusive microposts.", "labels": [], "entities": []}, {"text": "For instance, in the Kaggle-challenge, further microposts from users were added who had been blocked due to being reported to post personal attacks.", "labels": [], "entities": [{"text": "Kaggle-challenge", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8199875354766846}]}, {"text": "The procedures applied by other researchers are more drastic because, as we show in \u00a74 and \u00a75, they affect more heavily the topic distribution of the dataset.", "labels": [], "entities": []}, {"text": "These approaches do not even start with a random sample.", "labels": [], "entities": []}, {"text": "The topic distribution is mostly determined by the creators of the dataset themselves.", "labels": [], "entities": []}, {"text": "For example, Waseem and Hovy (2016) extract tweets matching query words likely to co-occur with abusive content.", "labels": [], "entities": []}, {"text": "choose Facebook-pages covering topics that similarly coincide with abusive language.", "labels": [], "entities": []}, {"text": "The resulting datasets are far from representing a natural sample of the underlying social-media sites.", "labels": [], "entities": []}, {"text": "shows that datasets that apply biased sampling (Warner, Waseem, Kumar) contain a high degree of implicit abuse.", "labels": [], "entities": []}, {"text": "Boosted random sampling, which provides a more realistic cross section of microposts, on the other hand, captures a larger amount of explicit abuse.", "labels": [], "entities": []}, {"text": "Future work should explore whether this is due to the predominance of explicit abuse on social media or some other reason, for example, the fact that human annotators more readily detect explicit abuse.", "labels": [], "entities": []}, {"text": "Intuitively, one would expect that the lower the proportion of explicit abuse is on the set of abusive microposts of a dataset, the lower the F1-score becomes because implicit abuse is not conveyed by lexical cues that are easy to learn.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9994786381721497}]}, {"text": "Table 1 confirms this notion, yet Waseem is the notable exception.", "labels": [], "entities": []}, {"text": "We need to find an explanation for this deviation since Waseem is by far the most frequently used dataset for detecting abusive language ().", "labels": [], "entities": []}, {"text": "This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented.", "labels": [], "entities": []}, {"text": "A possible way to prevent classification scores from looking unreasonably well is by applying cross-domain classification, i.e. testing a classifier on a dataset different from the one it was trained on.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.7214477509260178}]}, {"text": "The specific biases we pointed out should be primarily restricted to individual datasets and not carryover to other ones.", "labels": [], "entities": []}, {"text": "This is illustrated by Table 5.", "labels": [], "entities": []}, {"text": "Compared to in-domain classification (Table 1), all classifiers perform worse.", "labels": [], "entities": []}, {"text": "So all classifiers seem to be affected by data bias to some degree.", "labels": [], "entities": []}, {"text": "Datasets with explicit abuse and less biased sampling (Kaggle, Founta, Razavi) still perform reasonably when trained among each other, i.e. they are not heavily affected, whereas datasets with implicit abuse and biased sampling (Warner, Waseem, Kumar) perform poorly.", "labels": [], "entities": []}, {"text": "This time this also includes Waseem which implies that the good performance in in-domain classification was indeed caused by data bias.", "labels": [], "entities": []}, {"text": "Of course, cross-domain classification may not always be practical, particularly if a specific subtype of language abuse is studied for which there is only one dataset available.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.8000591695308685}]}, {"text": "However, even then, simple methods such as computing the words that highly correlate with the different classes on that dataset, similar to what we did in, may already indicate that there are biases hidden in the dataset.", "labels": [], "entities": []}, {"text": "If only a very small amount of biased words is identified, then usually it suffices to manually debias the dataset.", "labels": [], "entities": []}, {"text": "By that, one understands sampling additional microposts containing the words manually detected to be biased ().", "labels": [], "entities": []}, {"text": "For example, in the case of the Waseem-dataset, randomly sampling additional tweets matching the words announcer, commentator, football or sport, would reduce the sexism bias we reported in this paper (simply because random tweets are unlikely to contain sexist remarks unlike the existing tweets from the Waseem-dataset).", "labels": [], "entities": [{"text": "Waseem-dataset", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.946749210357666}, {"text": "Waseem-dataset", "start_pos": 306, "end_pos": 320, "type": "DATASET", "confidence": 0.9429317116737366}]}, {"text": "5 In order to avoid author bias to interfere with classification, one could restrict the number of microposts per author.", "labels": [], "entities": []}, {"text": "This would result in a more balanced distribution of microposts per author.: Cross-domain classification (eval.: F1).", "labels": [], "entities": [{"text": "Cross-domain classification", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7514677345752716}, {"text": "F1", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9875199794769287}]}], "tableCaptions": [{"text": " Table 1: Properties of the different datasets. (  *  : proportion of explicitly abusive microposts among abusive micro- posts.  \u2020 : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation  about Kaggle can therefore be found in that publication.)", "labels": [], "entities": []}, {"text": " Table 2: Top 10 words having strongest correla- tion with abusive microposts according to PMI on  Founta (dataset representing almost random sample)  and Waseem (dataset produced by biased sampling).", "labels": [], "entities": [{"text": "PMI", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.7620294690132141}, {"text": "Founta", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9106632471084595}]}, {"text": " Table 3: Impact of removing specific words from clas- sifier trained and tested on Waseem.", "labels": [], "entities": []}, {"text": " Table 4: The 5 most sexist and racist authors on the  Waseem-dataset and the number of their microposts.", "labels": [], "entities": []}, {"text": " Table 5: Cross-domain classification (eval.: F1).", "labels": [], "entities": [{"text": "Cross-domain classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8019732534885406}, {"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9813833832740784}]}]}