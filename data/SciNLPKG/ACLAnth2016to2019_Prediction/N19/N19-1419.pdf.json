{"title": [{"text": "A Structural Probe for Finding Syntax in Word Representations", "labels": [], "entities": [{"text": "Finding Syntax in Word Representations", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.7778136372566223}]}], "abstractContent": [{"text": "Recent work has improved our ability to detect linguistic knowledge in word representations.", "labels": [], "entities": []}, {"text": "However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety.", "labels": [], "entities": []}, {"text": "In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space.", "labels": [], "entities": []}, {"text": "The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree.", "labels": [], "entities": []}, {"text": "Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9868370294570923}]}], "introductionContent": [{"text": "As pretrained deep models that build contextualized representations of language continue to provide gains on NLP benchmarks, understanding what they learn is increasingly important.", "labels": [], "entities": []}, {"text": "To this end, probing methods are designed to evaluate the extent to which representations of language encode particular knowledge of interest, like part-ofspeech (), morphology (Peters et al., 2018a), or sentence length ().", "labels": [], "entities": []}, {"text": "Such methods work by specifying a probe (Conneau et al.,), a supervised model for finding information in a representation.", "labels": [], "entities": []}, {"text": "Of particular interest, both for linguistics and for building better models, is whether deep models' representations encode syntax.", "labels": [], "entities": []}, {"text": "Despite recent work (), open questions remain as to whether deep contextual models encode entire parse trees in their word representations.", "labels": [], "entities": []}, {"text": "In this work, we propose a structural probe, a simple model which tests whether syntax trees are consistently embedded in a linear transformation of a neural network's word representation space.", "labels": [], "entities": []}, {"text": "Tree structure is embedded if the transformed space has the property that squared L2 distance between two words' vectors corresponds to the number of edges between the words in the parse tree.", "labels": [], "entities": []}, {"text": "To reconstruct edge directions, we hypothesize a linear transformation under which the squared L2 norm corresponds to the depth of the word in the parse tree.", "labels": [], "entities": []}, {"text": "Our probe uses supervision to find the transformations under which these properties are best approximated for each model.", "labels": [], "entities": []}, {"text": "If such transformations exist, they define inner products on the original space under which squared distances and norms encode syntax trees -even though the models being probed were never given trees as input or supervised to reconstruct them.", "labels": [], "entities": []}, {"text": "This is a structural property of the word representation space, akin to vector offsets encoding word analogies (.", "labels": [], "entities": []}, {"text": "Using our probe, we conduct a targeted case study, showing that ELMo () and BERT) representations embed parse trees with high consistency in contrast to baselines, and in a low-rank space.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.8960044980049133}, {"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9944435954093933}]}, {"text": "In summary, we contribute a simple structural probe for finding syntax in word representations ( \u00a72), and experiments providing insights into and examples of how a low-rank transformation recovers parse trees from ELMo and BERT representations ( \u00a73,4).", "labels": [], "entities": [{"text": "BERT", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9194730520248413}]}, {"text": "Finally, we discuss our probe and limitations in the context of recent work ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "Using our probe, we evaluate whether representations from ELMo and BERT, two popular English models pre-trained on language modeling-like objectives, embed parse trees according to our structural hypothesis.", "labels": [], "entities": [{"text": "BERT", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9914815425872803}]}, {"text": "Unless otherwise specified, we permit the linear transformation B to be potentially full-rank (i.e., B is square.)", "labels": [], "entities": []}, {"text": "Later, we explore what rank of transformation is actually necessary for encoding syntax ( \u00a7 4.1).", "labels": [], "entities": []}, {"text": "Representation models We use the 5.5B-word pre-trained ELMo weights for all ELMo representations, and both BERT-base (cased) and BERT-large (cased).", "labels": [], "entities": [{"text": "BERT-base", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9958991408348083}, {"text": "BERT-large", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9923725128173828}]}, {"text": "The representations we evaluate are denoted ELMOK, BERTBASEK, BERTLARGEK, where K indexes the hidden layer of the corresponding model.", "labels": [], "entities": [{"text": "ELMOK", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9971053004264832}, {"text": "BERTBASEK", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9980547428131104}, {"text": "BERTLARGEK", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9978988170623779}]}, {"text": "All ELMo and BERT-large layers are dimensionality 1024; BERT-base layers are dimensionality 768.", "labels": [], "entities": [{"text": "BERT-large", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.988612711429596}, {"text": "BERT-base", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9796867370605469}]}, {"text": "Data We probe models for their ability to capture the Stanford Dependencies formalism (), claiming that capturing most aspects of the formalism implies an understanding of English syntactic structure.", "labels": [], "entities": []}, {"text": "To this end, we obtain fixed word representations for sentences of the parsing train/dev/test splits of the Penn Treebank (, with no pre-processing.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9919489622116089}]}, {"text": "Baselines Our baselines should encode features useful for training a parser, but not be capable of parsing themselves, to provide points of comparison against ELMo and BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9697428345680237}]}, {"text": "They are as follows: LINEAR : The tree resulting from the assumption that English parse trees form a left-to-right chain.", "labels": [], "entities": [{"text": "LINEAR", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9985319375991821}]}, {"text": "A model that encodes the positions of words should be able to meet this baseline.", "labels": [], "entities": []}, {"text": "We evaluate models on how well the predicted distances between all pairs of words reconstruct gold parse trees and correlate with the parse trees' distance metrics.", "labels": [], "entities": []}, {"text": "To evaluate tree reconstruction, we take each test sentence's predicted parse tree distances and compute the minimum spanning tree.", "labels": [], "entities": [{"text": "tree reconstruction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7268738597631454}]}, {"text": "We evaluate the predicted tree on undirected  We evaluate models on their ability to recreate the order of words specified by their depth in the parse tree.", "labels": [], "entities": []}, {"text": "We report the Spearman correlation betwen the true depth ordering and the predicted ordering, averaging first between sentences of the same length, and then across sentence lengths 5-50, as the \"norm Spearman (NSpr.)\".", "labels": [], "entities": []}, {"text": "We also evaluate models' ability to identify the root of the sentence as the least deep, as the \"root%\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of structural probes on the PTB WSJ test", "labels": [], "entities": [{"text": "PTB WSJ test", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9515115221341451}]}]}