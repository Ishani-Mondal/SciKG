{"title": [{"text": "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models -Is Single-Corpus Evaluation Enough?", "labels": [], "entities": []}], "abstractContent": [{"text": "This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.7490886449813843}]}, {"text": "GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark.", "labels": [], "entities": [{"text": "CoNLL-2014 benchmark", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9441654086112976}]}, {"text": "However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics.", "labels": [], "entities": []}, {"text": "To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ).", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 198, "end_pos": 208, "type": "DATASET", "confidence": 0.8564180731773376}, {"text": "FCE", "start_pos": 210, "end_pos": 213, "type": "DATASET", "confidence": 0.8021395802497864}]}, {"text": "Evaluation results reveal that the models' rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of correcting various grammatical errors in a given text, which is typically written by non-native speakers.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8344055016835531}]}, {"text": "Previous studies focused on typical errors such as those in the use of articles (), prepositions, and noun numbers ().", "labels": [], "entities": []}, {"text": "Machine translation approaches are being presently applied for GEC.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7870466411113739}, {"text": "GEC", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.5737033486366272}]}, {"text": "In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (.", "labels": [], "entities": [{"text": "GEC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9226988554000854}]}, {"text": "However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus.", "labels": [], "entities": [{"text": "GEC", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9787830114364624}]}, {"text": "The CoNLL-2014 shared task dataset () has been recently used for such evaluation.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task dataset", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.8186792880296707}]}, {"text": "Single-corpus evaluation maybe insufficient in cases wherein a GEC model generally aims to robustly correct grammatical errors in any written text partly because the task difficulty varies depending on proficiency levels and essay topics.", "labels": [], "entities": []}, {"text": "Although a model outperforms a baseline in one corpus, the model in another corpus may perform better, leading to different conclusions from what we know.", "labels": [], "entities": []}, {"text": "This study explores the necessity of performing cross-corpora evaluation for GEC models.", "labels": [], "entities": []}, {"text": "The performance of four recent models, namely three neural machine translation (NMT)-based models (LSTM, CNN, and transformer) and a statistical machine translation (SMT)-based model is evaluated against six learner corpora), FCE), JF-LEG (,).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)-", "start_pos": 133, "end_pos": 171, "type": "TASK", "confidence": 0.7320351103941599}]}, {"text": "Evaluation results show that the models' rankings considerably vary depending on the corpus.", "labels": [], "entities": []}, {"text": "Empirical results reveal that models must be evaluated using multiple corpora from different perspectives.", "labels": [], "entities": []}, {"text": "The contributions of this study are as follows: \u2022 We first explore the necessity of performing cross-corpora evaluation for GEC models.", "labels": [], "entities": []}, {"text": "\u2022 We empirically show that the single-corpus evaluation maybe unreliable.", "labels": [], "entities": []}, {"text": "\u2022 Our source code is published for crosscorpora evaluation so that researchers in the community can adequately and easily evaluate their models based on multiple corpora.", "labels": [], "entities": [{"text": "crosscorpora evaluation", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8331339359283447}]}], "datasetContent": [{"text": "Cross-corpora evaluation is discussed herein using six corpora, namely, FCE, JFLEG, KJ, and ICNALE.", "labels": [], "entities": [{"text": "FCE", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.5426099896430969}, {"text": "JFLEG", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.6823750734329224}, {"text": "ICNALE", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.6869907975196838}]}, {"text": "The following conditions were considered when selecting corpora: \u2022 The corpus must be used at least once in the GEC community.", "labels": [], "entities": []}, {"text": "\u2022 Based on the hypothesis that writers' proficiency affects the error distribution of any given text, we add a corpus with relatively low proficiency (KJ) compared to CoNLL-2014.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 167, "end_pos": 177, "type": "DATASET", "confidence": 0.9537191987037659}]}, {"text": "We explicitly describe each learner corpus as follows:, the official dataset of CoNLL-2014 shared task, is a collection of essays written by students at the National University of Singapore and is commonly used as test data for the CoNLL-2014 benchmark.", "labels": [], "entities": [{"text": "CoNLL-2014 benchmark", "start_pos": 232, "end_pos": 252, "type": "DATASET", "confidence": 0.9297845661640167}]}, {"text": "This dataset contains only two essay topics., the official dataset of CoNLL-2013 shared tasks, is commonly used as the development data for the contains essays written by college and graduate students from ten Asian countries/regions (China, Hong Kong, Indonesia, Japan, Korea, Pakistan, the Philippines, Singapore, Taiwan, and Thailand).", "labels": [], "entities": []}, {"text": "The original ICNALE is not error annotated.", "labels": [], "entities": [{"text": "ICNALE", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.7426843643188477}]}, {"text": "Therefore, we sampled a total number of 1,736 sentences, which are manually annotated with grammatical errors based on KJ 's annotation scheme.", "labels": [], "entities": []}, {"text": "summarizes the properties of these corpora.", "labels": [], "entities": []}, {"text": "Let N and M denote the total number of source words and sentences in a corpus, respectively.", "labels": [], "entities": []}, {"text": "Word error rate (WER) is defined as follows: where X m denotes each source sentence, Y m denotes each corrected sentence, and d(X m , Y m ) denotes the edit distance between X m and Y musing dynamic programming.", "labels": [], "entities": [{"text": "Word error rate (WER)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.8186093469460806}]}, {"text": "The following conclusions are derived: (1) CoNLL-2014 has narrow coverage of topics, proficiency and L1s compared with other corporas such as JFLEG and FCE.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8184982538223267}, {"text": "JFLEG", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.8726903796195984}, {"text": "FCE", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.8564082384109497}]}, {"text": "(2) Several learner corpora are available for the evaluation of GEC models.", "labels": [], "entities": []}, {"text": "These corpora can help investigate the performance of GEC models under different conditions.", "labels": [], "entities": []}, {"text": "We use two public datasets, namely Lang-8 (Mizumoto et al., 2011) and NUCLE (, for training.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.7872241139411926}]}, {"text": "Our pre-processing and experimental setup is similar to that reported previously (.", "labels": [], "entities": []}, {"text": "In particular, a subset of NUCLE (5.4K) is utilized as the development data for selecting the model; the remaining subset (1.3M) is utilized as the training data.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.795302152633667}]}, {"text": "All the models are trained, tuned, and tested in the same way.", "labels": [], "entities": []}, {"text": "The models are tested on each test data shown in.", "labels": [], "entities": []}, {"text": "As an evaluation metric, we use F 0.5 score computed by applying the MaxMatch scorer) and GLEU (.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9760295351346334}, {"text": "MaxMatch scorer", "start_pos": 69, "end_pos": 84, "type": "METRIC", "confidence": 0.583460807800293}, {"text": "GLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9989581108093262}]}, {"text": "We determine the average F 0.5 and average GLEU scores of the four models, which are trained with different random initializations, following a previously reported approach).", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9751646518707275}, {"text": "GLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9945382475852966}]}, {"text": "shows the performance of each model sorted from best to worst based on their F 0.5 score, revealing that the performance substantially varies depending on the corpus.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9784649610519409}]}, {"text": "For example, the performance of the transformer ranges from the score of F 0.5 , which is as low as 36.20 on CoNLL-2013, to as high as 60.06 on JFLEG.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9818116128444672}, {"text": "CoNLL-2013", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.9506405591964722}, {"text": "JFLEG", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9627842307090759}]}, {"text": "Notably, their rankings also considerably vary.", "labels": [], "entities": []}, {"text": "Transformer performs best on CoNLL-2014.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9541898369789124}]}, {"text": "However, it exhibits thirdbest performance among FCE, KJ, and ICNALE; LSTM outperforms the other models by a large margin of up to 5.3 F 0.5 points.", "labels": [], "entities": [{"text": "FCE", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8664301037788391}, {"text": "ICNALE", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.6493607759475708}]}, {"text": "Some examples of the model outputs are presented in.", "labels": [], "entities": []}, {"text": "Some situations are successfully corrected using transformer, whereas it failed to perform in other situations.", "labels": [], "entities": []}, {"text": "The reason for difference in the model rankings cannot be generally stated because it is influenced by various factors such as the learner's proficiency, essay topic, and L1.", "labels": [], "entities": [{"text": "L1", "start_pos": 171, "end_pos": 173, "type": "METRIC", "confidence": 0.9506248235702515}]}, {"text": "The experimental results show, however, that discussions based on the performance on CoNLL-2014 may only hold under certain conditions.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.8944634199142456}]}, {"text": "shows the performance measured in GLEU having a similar trend.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.5846013426780701}]}, {"text": "However, their  rankings on FCE show different trends in and.", "labels": [], "entities": [{"text": "FCE", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9475229382514954}]}, {"text": "This is partly because F 0.5 and GLEU evaluate different perspectives of the models.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.941146582365036}, {"text": "GLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.996335506439209}]}, {"text": "Furthermore, evaluation data and metric must be appropriately set depending on the factors that need to be evaluated in the model.", "labels": [], "entities": []}, {"text": "Experimental results indicate that the benchmark single-corpus evaluation is not robust; however, more diverse corpora remain undetermined.", "labels": [], "entities": []}, {"text": "Both JFLEG and FCE can be diverse corpora because they contain examination scripts written by language learners from allover the world.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.7745534181594849}, {"text": "FCE", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.749408483505249}]}, {"text": "JFLEG is particularly designed to contain more diverse corpus for developing and evaluating GEC models (.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9090576171875}]}, {"text": "If a diverse single-corpus evaluation suffices, the rankings of the models will remain the same.", "labels": [], "entities": []}, {"text": "However, experimental results have shown that the model rankings on both JF-LEG and FCE are different).", "labels": [], "entities": [{"text": "JF-LEG", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.8933407664299011}, {"text": "FCE", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.7835578918457031}]}, {"text": "Thus, single-corpus evaluation is deemed weak regardless of its diversity.", "labels": [], "entities": []}, {"text": "This study discusses the importance of evaluating GEC models from various perspectives using multiple corpora.", "labels": [], "entities": [{"text": "GEC", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9053505659103394}]}, {"text": "Multi-perspective evaluation does not necessarily mean using multiple corpora.", "labels": [], "entities": []}, {"text": "Many aspects in a corpus can be used for analysis, such as the proficiency of the writers, essay topics, and the writer 's native language.", "labels": [], "entities": []}, {"text": "As a case study, we evaluate and analyze the models regarding the essay WER.", "labels": [], "entities": []}, {"text": "shows the performance (in precision, recall, and F 0.5 ) of all the models when WER is the lowest (7.64 % for ICNALE) and the highest (20.86 % for JFLEG).", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9996477365493774}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994708895683289}, {"text": "F 0.5", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9861145317554474}, {"text": "WER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9979315996170044}, {"text": "ICNALE", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.7863603830337524}]}, {"text": "Transformer and LSTM outperform all the other models in the highest and the lowest error-rated corpora, respectively.", "labels": [], "entities": []}, {"text": "Experimental results show that LSTM and transformer maybe more precision-oriented and recall-oriented, respectively.", "labels": [], "entities": [{"text": "precision-oriented", "start_pos": 63, "end_pos": 81, "type": "METRIC", "confidence": 0.9973986148834229}, {"text": "recall-oriented", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9925616383552551}]}, {"text": "Further, precisionoriented models have an advantage over recalloriented models when a given text contains several errors, and vice versa.", "labels": [], "entities": [{"text": "precisionoriented", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.998364269733429}, {"text": "recalloriented", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9944879412651062}]}, {"text": "This knowledge enables choosing a model based on the task that has to be completed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Properties of evaluation corpora. Yes/No indicates whether the corpus exhibits each property in terms of  multiple L1, multiple proficiency and public available.", "labels": [], "entities": []}, {"text": " Table 4: Performance in precision, recall, and F 0.5 of  all models on the corpora when the WER is lowest and  highest.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9997729659080505}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.999778687953949}, {"text": "F 0.5", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9893067181110382}, {"text": "WER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9441744089126587}]}]}