{"title": [], "abstractContent": [{"text": "Attention mechanisms have seen wide adoption in neural NLP models.", "labels": [], "entities": []}, {"text": "In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs.", "labels": [], "entities": []}, {"text": "However, it is unclear what relationship exists between attention weights and model outputs.", "labels": [], "entities": []}, {"text": "In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \"explanations\" for predictions.", "labels": [], "entities": []}, {"text": "We find that they largely do not.", "labels": [], "entities": []}, {"text": "For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions.", "labels": [], "entities": []}, {"text": "Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.", "labels": [], "entities": []}, {"text": "Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For binary text classification, we use: Stanford Sentiment Treebank (SST)).", "labels": [], "entities": [{"text": "binary text classification", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6647716462612152}, {"text": "Stanford Sentiment Treebank (SST))", "start_pos": 40, "end_pos": 74, "type": "DATASET", "confidence": 0.9040798644224802}]}, {"text": "10,662 sentences tagged with sentiment on a scale from 1 (most negative) to 5 (most positive).", "labels": [], "entities": []}, {"text": "We filter out neutral instances and dichotomize the remaining sentences into positive (4, 5) and negative: Dataset characteristics.", "labels": [], "entities": []}, {"text": "For train and test size, we list the cardinality for each class, where applicable: 0/1 for binary classification (top), and 0 / 1 / 2 for NLI (bottom).", "labels": [], "entities": []}, {"text": "Average length is in tokens.", "labels": [], "entities": [{"text": "length", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.8572726845741272}]}, {"text": "Test metrics are F1 score, accuracy, and micro-F1 for classification, QA, and NLI, respectively; all correspond to performance using a BiLSTM encoder.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9731769859790802}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9971598386764526}, {"text": "QA", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.940862238407135}]}, {"text": "We note that results using convolutional and average (i.e., non-recurrent) encoders are comparable for classification though markedly worse for QA tasks.", "labels": [], "entities": [{"text": "classification", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.9648841023445129}]}, {"text": "IMDB Large Movie Reviews Corpus (Maas et al., 2011).", "labels": [], "entities": [{"text": "IMDB Large Movie Reviews Corpus", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.9261578321456909}]}, {"text": "Binary sentiment classification dataset containing 50,000 polarized (positive or negative) movie reviews, split into half for training and testing.", "labels": [], "entities": [{"text": "Binary sentiment classification", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6317851146062216}]}, {"text": "Twitter Adverse Drug Reaction dataset.", "labels": [], "entities": [{"text": "Twitter Adverse Drug Reaction dataset", "start_pos": 0, "end_pos": 37, "type": "DATASET", "confidence": 0.92407705783844}]}, {"text": "A corpus of \u223c8000 tweets retrieved from Twitter, annotated by domain experts as mentioning adverse drug reactions.", "labels": [], "entities": []}, {"text": "20 Newsgroups (Hockey vs Baseball).", "labels": [], "entities": []}, {"text": "Collection of \u223c20,000 newsgroup correspondences, partitioned (nearly) evenly across 20 categories.", "labels": [], "entities": []}, {"text": "We extract instances belonging to baseball and hockey, which we designate as 0 and 1, respectively, to derive a binary classification task.", "labels": [], "entities": []}, {"text": "AG News Corpus (Business vs World).", "labels": [], "entities": [{"text": "AG News Corpus (Business vs World)", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.9418425261974335}]}, {"text": "5 496,835 news articles from 2000+ sources.", "labels": [], "entities": []}, {"text": "We follow () in filtering out all but the top 4 categories.", "labels": [], "entities": []}, {"text": "We consider the binary classification task of discriminating between world (0) and business (1) articles.", "labels": [], "entities": []}, {"text": "MIMIC ICD9 (Diabetes).", "labels": [], "entities": [{"text": "MIMIC ICD9", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8956365287303925}]}, {"text": "A subset of discharge summaries from the MIMIC III dataset of electronic health records.", "labels": [], "entities": [{"text": "MIMIC III dataset of electronic health records", "start_pos": 41, "end_pos": 87, "type": "DATASET", "confidence": 0.8979588406426566}]}, {"text": "The task is to recognize if a given summary has been labeled with the ICD9 code for diabetes (or not).", "labels": [], "entities": [{"text": "ICD9 code", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.6758579015731812}]}, {"text": "MIMIC ICD9 (Chronic vs Acute Anemia).", "labels": [], "entities": [{"text": "MIMIC ICD9", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.690494567155838}]}, {"text": "A subset of discharge summaries from MIMIC III dataset known to correspond to patients with anemia.", "labels": [], "entities": [{"text": "MIMIC III dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.8733023603757223}]}, {"text": "Here the task to distinguish the type of anemia for each report -acute (0) or chronic (1).", "labels": [], "entities": []}, {"text": "For Question Answering (QA): CNN News Articles (.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8313277423381805}, {"text": "CNN News Articles", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9270210266113281}]}, {"text": "A corpus of cloze-style questions created via auto-matic parsing of news articles from CNN.", "labels": [], "entities": [{"text": "auto-matic parsing of news articles from CNN", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.7119994248662677}]}, {"text": "Each instance comprises a paragraph-question-answer triplet, where the answer is one of the anonymized entities in the paragraph.", "labels": [], "entities": []}, {"text": "We consider the three tasks presented in the original bAbI dataset paper, training separate models for each.", "labels": [], "entities": [{"text": "bAbI dataset paper", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9440197348594666}]}, {"text": "These entail finding (i) a single supporting fact fora question and (ii) two or (iii) three supporting statements, chained together to compose a coherent line of reasoning.", "labels": [], "entities": []}, {"text": "Finally, for Natural Language Inference (NLI): The SNLI dataset).", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.7972604036331177}, {"text": "SNLI dataset", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.7351029515266418}]}, {"text": "570k human-written English sentence pairs manually labeled for balanced classification with the labels neutral, contradiction, and entailment, supporting the task of natural language inference (NLI).", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 166, "end_pos": 198, "type": "TASK", "confidence": 0.6511034270127615}]}, {"text": "In this work, we generate an attention distribution over premise words conditioned on the hidden representation induced for the hypothesis.", "labels": [], "entities": []}, {"text": "We restrict ourselves to comparatively simple instantiations of attention mechanisms, as described in the preceding section.", "labels": [], "entities": []}, {"text": "This means we do not consider recently proposed 'BiAttentive' architectures that attend to tokens in the respective inputs, conditioned on the other inputs (.", "labels": [], "entities": []}, {"text": "provides summary statistics for all datasets, as well as the observed test performances for additional context.", "labels": [], "entities": []}, {"text": "We run a battery of experiments that aim to examine empirical properties of learned attention weights and to interrogate their interpretability and transparency.", "labels": [], "entities": []}, {"text": "The key questions are: Do  learned attention weights agree with alternative, natural measures of feature importance?", "labels": [], "entities": []}, {"text": "And, Had we attended to different features, would the prediction have been different?", "labels": [], "entities": []}, {"text": "More specifically, in Section 4.1, we empirically analyze the correlation between gradientbased feature importance and learned attention weights, and between 'leave-one-out' (LOO) measures and the same.", "labels": [], "entities": []}, {"text": "In Section 4.2 we then consider counterfactual (to those observed) attention distributions.", "labels": [], "entities": []}, {"text": "Under the assumption that attention weights are explanatory, such counterfactual distributions maybe viewed as alternative potential explanations; if these do not correspondingly change model output, then the original attention weights do not provide unique explanation for predictions, i.e., attending to other features could have resulted in the same output.", "labels": [], "entities": []}, {"text": "To generate counterfactual attention distributions, we first consider randomly permuting observed attention weights and recording associated changes in model outputs (4.2.1).", "labels": [], "entities": []}, {"text": "We then propose explicitly searching for \"adversarial\" attention weights that maximally differ from the observed attention weights (which one might show in a heatmap and use to explain a model prediction), and yet yield an effectively equivalent prediction (4.2.2).", "labels": [], "entities": []}, {"text": "The latter strategy also provides a useful potential metric for the reliability of attention weights as explanations: we can report a measure quantifying how different attention weights can be fora given instance without changing the model output by more than some threshold . All results presented below are generated on test sets.", "labels": [], "entities": []}, {"text": "We present results for Additive attention below.", "labels": [], "entities": [{"text": "Additive attention", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9391233623027802}]}, {"text": "The results for Scaled Dot Product in its place are comparable.", "labels": [], "entities": []}, {"text": "We provide a web interface to interactively browse the (very large set of) plots for all datasets, model variants, and experiment types: https://successar.github.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset characteristics. For train and test size, we list the cardinality for each class, where applicable:  0/1 for binary classification (top), and 0 / 1 / 2 for NLI (bottom). Average length is in tokens. Test metrics are  F1 score, accuracy, and micro-F1 for classification, QA, and NLI, respectively; all correspond to performance  using a BiLSTM encoder. We note that results using convolutional and average (i.e., non-recurrent) encoders are  comparable for classification though markedly worse for QA tasks.", "labels": [], "entities": [{"text": "Average length", "start_pos": 188, "end_pos": 202, "type": "METRIC", "confidence": 0.935355544090271}, {"text": "F1 score", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.9831658899784088}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9889906644821167}]}, {"text": " Table 2: Mean and std. dev. of correlations between gradient/leave-one-out importance measures and attention  weights. Sig. Frac. columns report the fraction of instances for which this correlation is statistically significant;  note that this largely depends on input length, as correlation does tend to exist, just weakly. Encoders are denoted  parenthetically. These are representative results; exhaustive results for all encoders are available to browse online.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9969360828399658}, {"text": "Sig. Frac.", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.8580614775419235}]}]}