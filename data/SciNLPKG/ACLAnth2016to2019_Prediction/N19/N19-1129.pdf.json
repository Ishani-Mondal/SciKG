{"title": [{"text": "Does My Rebuttal Matter? Insights from a Major NLP Conference", "labels": [], "entities": []}], "abstractContent": [{"text": "Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP.", "labels": [], "entities": [{"text": "Peer review", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.813157469034195}, {"text": "ML and NLP", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.6881426175435384}]}, {"text": "However, only few studies have evaluated its properties empirically.", "labels": [], "entities": []}, {"text": "Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018.", "labels": [], "entities": [{"text": "ACL-2018", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9131331443786621}]}, {"text": "We quantitatively and qualitatively assess the corpus.", "labels": [], "entities": []}, {"text": "This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses.", "labels": [], "entities": []}, {"text": "We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses.", "labels": [], "entities": []}, {"text": "Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a re-viewer's final score is largely determined by her initial score and the distance to the other reviewers' initial scores.", "labels": [], "entities": []}, {"text": "In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research.", "labels": [], "entities": []}, {"text": "We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Peer review is a widely adopted quality control mechanism in which the value of scientific work is assessed by several reviewers with a similar level of competence.", "labels": [], "entities": [{"text": "Peer review", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7443778216838837}]}, {"text": "Although peer review has been at the core of the scientific process for at least 200 years (, it is also a subject of debate: for instance, it has been found that peer reviewing can hardly recognize prospectively well-cited papers or major flaws (.", "labels": [], "entities": []}, {"text": "Further, observed substantial disagreement between two sets of reviews on the same set of submissions for * Equal contribution.", "labels": [], "entities": [{"text": "Equal contribution", "start_pos": 108, "end_pos": 126, "type": "METRIC", "confidence": 0.9577324688434601}]}, {"text": "the prestigious Conference on Neural Information Processing Systems (NeurIPS) 2014.", "labels": [], "entities": []}, {"text": "The rebuttal phase plays an important role in peer reviewing especially in top-tier conferences in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "peer reviewing", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7505154609680176}, {"text": "Natural Language Processing (NLP)", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.672360748052597}]}, {"text": "It allows authors to provide responses to address the criticisms and questions raised in the reviews and to defend their work.", "labels": [], "entities": []}, {"text": "Although there is evidence that reviewers do update their evaluations after the rebuttal phase 1 , it remains unclear what causes them to do so, and especially, whether they react to the author responses per se, or rather adjust to the opinions of their co-reviewers.", "labels": [], "entities": []}, {"text": "In order to obtain further insights into the reviewing process, especially regarding the role of the rebuttal phase in peer reviewing, in this work we present and analyze a review corpus of the 56th Annual Meeting of the Association for Computational Linguistics.", "labels": [], "entities": []}, {"text": "Every reviewer/author was asked whether she consented to freely using her review/author-response for research purposes and publishing the data under an appropriate open-source license within at earliest 2 years from the acceptance deadline (see supplementary material for the original consent agreement).", "labels": [], "entities": []}, {"text": "85% reviewers and 31% authors have consented to sharing their data.", "labels": [], "entities": []}, {"text": "The corpus comprises over 4k reviews (including review texts and scores) and 1.2k author responses.", "labels": [], "entities": []}, {"text": "Uniquely, the corpus includes both before-and after-rebuttal reviews for both accepted and rejected papers, making it a highly valuable resource for the community to study the role of the rebuttal phase.", "labels": [], "entities": []}, {"text": "The corpus as well as our source code and annotations are publicly avail-able at https://github.com/UKPLab/ naacl2019-does-my-rebuttal-matter.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9791744351387024}]}, {"text": "First, in \u00a73, we assess the corpus both quantitatively (e.g., correlating Overall Score with aspect scores such as Originality and Readability) and qualitatively (e.g., identifying key terms that differentiate \"good\" from \"bad\" author responses, annotating paper weaknesses given by reviewers, and rating the quality of individual author responses).", "labels": [], "entities": []}, {"text": "Second, in \u00a74, we develop a model to predict whether a reviewer will increase/decrease/keep her initial scores after the rebuttal.", "labels": [], "entities": [{"text": "keep her initial scores", "start_pos": 87, "end_pos": 110, "type": "METRIC", "confidence": 0.9358443021774292}]}, {"text": "We do so in order to analyze and disentangle the sources of review updates during the rebuttal stage.", "labels": [], "entities": []}, {"text": "We find that factoring in the author responses only marginally (but statistically significantly) improves the classification performance, and the score update decision is largely determined by the scores of peer reviewers.", "labels": [], "entities": []}, {"text": "Third, in \u00a75, we discuss multiple types of biases in the score update process, some of which potentially undermine the 'crowd-wisdom' of peer reviewing.", "labels": [], "entities": [{"text": "score update process", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7441490292549133}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the ACL-2018 corpus. Some  reviewers submitted their reviews after the rebuttal  started, hence the size of the after-rebuttal reviews is  larger than that of the before-rebuttal reviews.", "labels": [], "entities": [{"text": "ACL-2018 corpus", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9310904741287231}]}, {"text": " Table 2: Numbers and lengths of different components  in each review (mean\u00b1standard deviation).", "labels": [], "entities": []}, {"text": " Table 3: Statistics of different types of reviews.", "labels": [], "entities": []}, {"text": " Table 4: Frequent weakness types identified in reviews.", "labels": [], "entities": []}, {"text": " Table 5: Statistics of author responses (mean\u00b1standard  deviation for Length).", "labels": [], "entities": [{"text": "Length", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9723709225654602}]}, {"text": " Table 7: Percentage of agreement for spec, plt and  cvc scores. \"User-Score\" means the agreement be- tween the aggregated (by majority voting) users' pref- erences and score-induced preferences.", "labels": [], "entities": []}, {"text": " Table 7. The  agreement between the users' aggregated prefer- ences and score-induced preferences is quite high  for all three types, confirming the validity of the  scores. Note that the agreement for cvc is lower  than the other two; the reason might be that it  is difficult even for humans to judge convincing- ness of arguments, particularly when evaluated on  the sentence level without surrounding context nor  the corresponding review. The distribution of the  spec, plt and cvc scores for iResps, dResps  and kResps is in the supplementary material.", "labels": [], "entities": []}, {"text": " Table 8: Macro F-1 scores.", "labels": [], "entities": [{"text": "F-1", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7320219874382019}]}, {"text": " Table 9: Feature weights in multinomial logistic regres- sion trained on Full.", "labels": [], "entities": [{"text": "Full", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9493827223777771}]}, {"text": " Table 10: Feature weights in multinomial logistic re- gression trained on BRD.", "labels": [], "entities": [{"text": "BRD", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.41304585337638855}]}, {"text": " Table 12: Macro F-1 scores on Full. All results are averaged over 5000 repeats of 10-fold cross validation.", "labels": [], "entities": [{"text": "F-1", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.717345118522644}]}, {"text": " Table 13: Macro F-1 scores on BRD. All results are averaged over 5000 repeats of 10-fold cross validation.", "labels": [], "entities": [{"text": "F-1", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7993294596672058}, {"text": "BRD", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.6637859344482422}]}]}