{"title": [{"text": "Relation Classification Using Segment-Level Attention-based CNN and Dependency-based RNN", "labels": [], "entities": [{"text": "Relation Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9279349446296692}, {"text": "CNN", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.7607380747795105}, {"text": "RNN", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.34167975187301636}]}], "abstractContent": [{"text": "Recently, relation classification has gained much success by exploiting deep neural networks.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.978223979473114}]}, {"text": "In this paper, we propose anew model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs).", "labels": [], "entities": []}, {"text": "While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of the related entities.", "labels": [], "entities": []}, {"text": "Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the state-of-the-art without using any external lexical features.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.6658379435539246}]}], "introductionContent": [{"text": "Relation classification (RC) is a fundamental task in Natural Language Processing (NLP) that aims to identify semantic relations between pairs of marked entities in given sentences (instances).", "labels": [], "entities": [{"text": "Relation classification (RC)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9471989035606384}, {"text": "identify semantic relations between pairs of marked entities in given sentences (instances)", "start_pos": 101, "end_pos": 192, "type": "TASK", "confidence": 0.6972985054765429}]}, {"text": "It has attracted much research effort as it plays a vital role in many NLP applications such as Information Extraction and Question Answering.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.8746445178985596}, {"text": "Question Answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8330694735050201}]}, {"text": "Traditional approaches) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains.", "labels": [], "entities": []}, {"text": "Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention.", "labels": [], "entities": []}, {"text": "However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences () and dependency trees.", "labels": [], "entities": []}, {"text": "While raw sequences can provide all the information of relation instances, they also add noise to the models from redundant information.", "labels": [], "entities": []}, {"text": "While dependency tree structures help the models focus on the concise information captured by the shortest dependency path (SDP) between two entities, they lose some supplementary context in the raw sequence.", "labels": [], "entities": []}, {"text": "It is clear that the raw sequence and SDP highly complement each other.", "labels": [], "entities": []}, {"text": "We, therefore, combine them to be more effective in determining the relation without losing any information.", "labels": [], "entities": []}, {"text": "While CNNs are able to learn short patterns (local features) (, RNNs have been effective in learning word sequence information (long-distance features) (.", "labels": [], "entities": []}, {"text": "In this paper, we present anew model combining both CNNs and RNNs, exploiting the information from both the raw sequence and the SDP.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: (a) We combine Entity Tag Feature (ETF) and Tree-based Position Feature (TPF) ( to improve the semantic information between the marked entities in the raw input sentences.", "labels": [], "entities": [{"text": "Entity Tag Feature (ETF)", "start_pos": 60, "end_pos": 84, "type": "METRIC", "confidence": 0.695175881187121}]}, {"text": "(b) We propose Segment-Level Attention-based Convolutional Neural Networks (SACNNs) which automatically pay special attention to the important text segments from the raw sentence for RC.", "labels": [], "entities": []}, {"text": "(c) We build Dependency-based Recurrent Neural Networks (DepRNNs) on the SDP to gain longdistance features.", "labels": [], "entities": []}, {"text": "Then, we combine the SACNN and the DepRNN to preserve the full relational information.", "labels": [], "entities": [{"text": "DepRNN", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9216135740280151}]}, {"text": "Our proposed model achieves new state-of-the-art results on SemEval-2010 Task 8, compared with other complex models.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.5591490566730499}]}], "datasetContent": [{"text": "We evaluate our model on the SemEval2010 Task 8 which contains 8, 000 training sentences and 2, 717 test sentences, with 19 relations (9 directed relations and an undirected Other class).", "labels": [], "entities": []}, {"text": "Therefore, the relation classification task is treated as a multi-class classification problem.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.9178750912348429}]}, {"text": "Following previous work, the official macro-averaged F1-score, which excludes the Other relation, is used for evaluation.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9360525608062744}]}, {"text": "We randomly held out 10% of the training set for validation.", "labels": [], "entities": []}, {"text": "The Stanford Parser is also used to convert sentences to dependency trees.", "labels": [], "entities": []}, {"text": "For word embeddings, we use the 300-dimensional embeddings of.", "labels": [], "entities": []}, {"text": "In this work, we do not focus on comparing the effectiveness of the different pre-  trained embedding sets.", "labels": [], "entities": []}, {"text": "The above pre-trained embedding set is selected since it embeds dependency context to provide valuable syntactic information.", "labels": [], "entities": []}, {"text": "Four tokens: e1S, e1E, e2S, e2E and out-of-vocabulary words are initialized by sampling from a uniform distribution.", "labels": [], "entities": []}, {"text": "TPF is 15-dimensional and initialized randomly.", "labels": [], "entities": [{"text": "TPF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6279851198196411}]}, {"text": "Thus, the representation of each word has a dimensionality of 330 in the raw sentence.", "labels": [], "entities": []}, {"text": "Hyper-parameters in our model are as follows: 100 filters for each window size and ReLU as the activation function for each CNN in SACNN.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.997140645980835}]}, {"text": "In DepRNN, the dimension of each token is 300, the tanh activation function is applied to the last two hidden states, the dimension of each hidden state vector is 150.", "labels": [], "entities": [{"text": "DepRNN", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.8996022939682007}]}, {"text": "Other parameters include: L2 regularization with a weight of 10 \u22124 , a mini-batch size of 64, a dropout rate at the final layer p = 0.5 before a softmax classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different features in SACNN.  WE, ETF, and TPF stand for Word Embedding, Entity  Tag Feature, and Tree-based Position Feature.", "labels": [], "entities": [{"text": "WE", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9373379945755005}, {"text": "TPF", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.961426317691803}]}]}