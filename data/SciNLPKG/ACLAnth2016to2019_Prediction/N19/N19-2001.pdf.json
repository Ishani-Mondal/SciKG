{"title": [{"text": "Enabling Real-time Neural IME with Incremental Vocabulary Selection", "labels": [], "entities": [{"text": "Neural IME", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.5944628417491913}]}], "abstractContent": [{"text": "Input method editor (IME) converts sequential alphabet key inputs to words in a target language.", "labels": [], "entities": []}, {"text": "It is an indispensable service for billions of Asian users.", "labels": [], "entities": []}, {"text": "Although the neural-based language model is extensively studied and shows promising results in sequence-to-sequence tasks, applying a neural-based language model to IME was not considered feasible due to high latency when converting words on user devices.", "labels": [], "entities": []}, {"text": "In this work, we articulate the bottleneck of neural IME decoding to be the heavy softmax computation over a large vocabulary.", "labels": [], "entities": []}, {"text": "We propose an approach that incrementally builds a subset vocabulary from the word lattice.", "labels": [], "entities": []}, {"text": "Our approach always computes the probability with a selected subset vocabulary.", "labels": [], "entities": []}, {"text": "When the selected vocabulary is updated, the stale probabilities in previous steps are fixed by recomputing the missing logits.", "labels": [], "entities": []}, {"text": "The experiments on Japanese IME benchmark shows an over 50x speedup for the softmax computations comparing to the base-line, reaching real-time speed even on commodity CPU without losing conversion accuracy 1.", "labels": [], "entities": [{"text": "Japanese IME benchmark", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.870708187421163}]}, {"text": "The approach is potentially applicable to other incremental sequence-to-sequence decoding tasks such as real-time continuous speech recognition.", "labels": [], "entities": [{"text": "real-time continuous speech recognition", "start_pos": 104, "end_pos": 143, "type": "TASK", "confidence": 0.6273341700434685}]}], "introductionContent": [{"text": "Input Method Editors (IME) run on every desktop and mobile devices that allows users to type the scripts in their language.", "labels": [], "entities": [{"text": "Input Method Editors (IME)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6976538002490997}]}, {"text": "Though Latin users can type directly without conversion as a second step, some common languages such as Chinese and Japanese require users to convert the keyboard input sequence as there are thousands of characters in these languages.", "labels": [], "entities": []}, {"text": "The conversion task of an IME takes a key sequence and converts it to a sequence of words in the target language.", "labels": [], "entities": []}, {"text": "In the ideal case, the conversion results shall fit the intention of users.", "labels": [], "entities": []}, {"text": "The accuracy of the conversion task directly affects the typing efficiency and user experiences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9987642765045166}]}, {"text": "The conversion task is a sequence decoding task similar to speech recognition, machine translation, and optical character recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7506052255630493}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7563779950141907}, {"text": "optical character recognition", "start_pos": 104, "end_pos": 133, "type": "TASK", "confidence": 0.6316754122575124}]}, {"text": "Conventionally, an n-gram language model toolkit is used to evaluate the path probability during decoding.", "labels": [], "entities": []}, {"text": "Due to the ability of leveraging context information without hitting data sparsity issue, neural language models as an alternative option have been extensively studied in the past (, which achieve state-of-the-art performance on many tasks).", "labels": [], "entities": []}, {"text": "With emerging dedicated hardware processing unit such as custom ASIC, neuralbased models are promising to be even more widely applied to user devices.", "labels": [], "entities": []}, {"text": "However, neural-based language models were not considered feasible for the IME conversion task.", "labels": [], "entities": [{"text": "IME conversion", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.9845001101493835}]}, {"text": "The main reason is that an IME has to run interactively on various user devices, whereas speech recognition and machine translation services are normally provided on servers.", "labels": [], "entities": [{"text": "speech recognition and machine translation", "start_pos": 89, "end_pos": 131, "type": "TASK", "confidence": 0.6473353981971741}]}, {"text": "Furthermore, the neural model has to meet following requirements in order to be adopted in practice: 1) low-latency incremental conversion; 2) word lattice post-editing.", "labels": [], "entities": []}, {"text": "First, the conversion task for IME is an incremental process that needs to return the best paths immediately when receiving each key input.", "labels": [], "entities": [{"text": "IME", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9502775073051453}]}, {"text": "Existing speed optimization methods) normally increase the speed of processing sequences in batch.", "labels": [], "entities": []}, {"text": "Some methods incorporate prefix tree (, which doesn't ensure the worse case latency still meets the real-time requirement.", "labels": [], "entities": []}, {"text": "Second, IME allows users to post-edit the converted results at word lattice by manually selecting candidates.", "labels": [], "entities": [{"text": "IME", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.733925998210907}]}, {"text": "It limits the choice of many end-toend neural network architectures () as they do not provide away for users to select partial conversion results.", "labels": [], "entities": []}, {"text": "In this work, we enhance a neural language model tailored for IMEs to meet real-time inference speed requirements on the conversion task.", "labels": [], "entities": []}, {"text": "Our baseline model is composed of a LSTMbased language model (Hochreiter and Schmidhuber, 1997) with a Viterbi decoder as shows.", "labels": [], "entities": []}, {"text": "We articulate the bottleneck of run-time speed as the heavy linear transformation in the softmax layer.", "labels": [], "entities": []}, {"text": "We propose an incremental vocabulary selection approach that builds a subset vocabulary Vt at each decoding step t.", "labels": [], "entities": []}, {"text": "By only computing softmax over Vt , the cost of softmax significantly drops since |V t | is usually a small number that is less than 1% of the original vocabulary size.", "labels": [], "entities": []}, {"text": "We evaluate the speedup comparing to other softmax optimizations on a Japanese benchmark.", "labels": [], "entities": [{"text": "Japanese benchmark", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.8457110524177551}]}, {"text": "The contributions of this work can be summarized as: 1.", "labels": [], "entities": []}, {"text": "We propose a novel incremental vocabulary selection approach, which significantly reduces the latency of lattice decoding in IME conversion task.", "labels": [], "entities": [{"text": "IME conversion task", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.939444363117218}]}, {"text": "2. We provide an extensive comparison among different approaches for speeding up the softmax computation in lattice decoding.", "labels": [], "entities": []}, {"text": "3. We demonstrate that with our proposed acceleration method helps the neural models to meet the requirement for real-world applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use BCCWJ (Balanced Corpus of Contemporary Written Japanese) corpus () for evaluating our model.", "labels": [], "entities": [{"text": "BCCWJ (Balanced Corpus of Contemporary Written Japanese) corpus", "start_pos": 7, "end_pos": 70, "type": "DATASET", "confidence": 0.7562813341617585}]}, {"text": "The corpus is well balanced with various sources of text representing contemporary written Japanese.", "labels": [], "entities": []}, {"text": "This corpus contains 5.8M sentences, which are segmented into 127M tokens.", "labels": [], "entities": []}, {"text": "In our experiments, all words are further segmented into short unit words.", "labels": [], "entities": []}, {"text": "Each word has a format of \"display/reading/POS\".", "labels": [], "entities": []}, {"text": "The reading and part-of-speech (POS) attributes are attached to indicate different usages of the same word.", "labels": [], "entities": []}, {"text": "Among the 611K unique words, we choose top 50K frequent ones which cover 97.3% of the token appearances as an appropriate vocabulary size for the IME task.", "labels": [], "entities": [{"text": "IME task", "start_pos": 146, "end_pos": 154, "type": "TASK", "confidence": 0.9247572720050812}]}, {"text": "The words in the vocabulary are ranked with frequency.", "labels": [], "entities": []}, {"text": "Most frequent words are at the top.", "labels": [], "entities": []}, {"text": "The BCCWJ dataset is split into training, valid and test set.", "labels": [], "entities": [{"text": "BCCWJ dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9750962853431702}]}, {"text": "The ratio is 70%, 20%, and 10% respectively.", "labels": [], "entities": []}, {"text": "We randomly sample 2000 sentences from the test set for evaluating the conversion accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9705157279968262}]}, {"text": "For input method task, we evaluate the conversion accuracy using a Viterbi decoder with abeam size of 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9455357789993286}]}, {"text": "In Japanese, there are often more than one correct conversion results.", "labels": [], "entities": []}, {"text": "For instance, a verb may have two acceptable styles, one in original Japanese Kana, the other in Chinese characters.", "labels": [], "entities": []}, {"text": "To better evaluate the model performance, we also report the top-10 conversion accuracy in addition to top-1 conversion accuracy.", "labels": [], "entities": [{"text": "conversion", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9034284949302673}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.5315423011779785}, {"text": "conversion accuracy", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.6887972056865692}]}, {"text": "We implement using TensorFlow . We use a batch size of 384.", "labels": [], "entities": []}, {"text": "A dropout with a drop rate of 0.9 is applied before the LSTM layer.", "labels": [], "entities": []}, {"text": "Adam is used with a fixed learning rate of 0.001.", "labels": [], "entities": []}, {"text": "The hyper-parameters are shared for all experiments.", "labels": [], "entities": []}, {"text": "A replica of the same model is written in numpy to work with a Viterbi decoder in python.", "labels": [], "entities": []}, {"text": "It uses the weights learned with TensorFlow model.", "labels": [], "entities": []}, {"text": "The inference performance is measure with numpy on a single Intel E5 CPU.", "labels": [], "entities": []}, {"text": "We also apply the underline BLAS library to accelerate matrix operation.", "labels": [], "entities": [{"text": "BLAS", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.7227057814598083}]}, {"text": "We first compared the neural model performance with a conventional n-gram model.", "labels": [], "entities": []}, {"text": "We evaluate the perplexity of the n-gram model with SRILM package).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.6015700697898865}]}, {"text": "We choose modified Kneser Ney ( as the smoothing algorithm when learning the ngram model.", "labels": [], "entities": []}, {"text": "No cut-offs or pruning is applied.", "labels": [], "entities": []}, {"text": "The learned language model is plugged into our input method pipeline for evaluating the sentence conversion accuracy.", "labels": [], "entities": [{"text": "sentence conversion", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7190162241458893}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8163054585456848}]}, {"text": "The prediction accuracy is not provided as the perplexity directly reflects it.", "labels": [], "entities": [{"text": "prediction", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6706535816192627}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8845869302749634}]}, {"text": "The LSTM baseline model has a standard architecture.", "labels": [], "entities": []}, {"text": "The embedding size is 256, and the hidden size is 256.", "labels": [], "entities": []}, {"text": "The size of LSTM cells is selected empirically on a validation dataset.", "labels": [], "entities": []}, {"text": "In practice, using a network size bigger than 256 cannot gain significant improvement over perplexity.", "labels": [], "entities": []}, {"text": "In all the following experiments, we bind the input embedding and output embedding according to.", "labels": [], "entities": []}, {"text": "The idea is proven to save space and almost loss-less.", "labels": [], "entities": []}, {"text": "We treat it as the baseline model in the following experiments.", "labels": [], "entities": []}, {"text": "As shown in, the LSTM baseline achieved significant improvement on perplexity, comparing to conventional n-gram based models.", "labels": [], "entities": [{"text": "LSTM baseline", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.7332931458950043}]}, {"text": "The top-1 and top-10 path conversion accuracy were increased by 5.6% and 8.65% respectively comparing to tri-gram KN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9881424307823181}]}, {"text": "In real products, bigram is often used for decoding while tri-gram is only used for re-ranking the best paths.", "labels": [], "entities": []}, {"text": "Please note that in this evaluation, we did not apply any pruning for n-gram.", "labels": [], "entities": []}, {"text": "In practice, the n-gram model takes over 1GB storage size.", "labels": [], "entities": []}, {"text": "In this section, we compare the inference speed of various methods for accelerating the computation.", "labels": [], "entities": []}, {"text": "We measure the execution speed only for the component that computes the language model probabilities.", "labels": [], "entities": []}, {"text": "For neural-based methods, the component includes the LSTM and softmax layers.", "labels": [], "entities": []}, {"text": "For the n-gram model, the computation of probability is only a lookup in the hash tables.", "labels": [], "entities": []}, {"text": "Other components such as lattice construction are not included as they heavily depend on implementation.", "labels": [], "entities": [{"text": "lattice construction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7480109632015228}]}, {"text": "We report the decoding time in each step receiving a key input, as the per-step latency is critical for the real-time user experience.", "labels": [], "entities": []}, {"text": "The computation cost for decoding the whole sequence is linear to the number of steps.", "labels": [], "entities": []}, {"text": "For comparison, we also report the computation time of the softmax alone.", "labels": [], "entities": []}, {"text": "As shows, our proposed incremental vocabulary selection (IVS) achieves an 84x speedup for softmax computation comparing to the LSTM baseline.", "labels": [], "entities": [{"text": "vocabulary selection (IVS", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6516613662242889}]}, {"text": "The lattice vocabulary in our experiments contains only a few hundred words, while the full vocabulary has 50k words.", "labels": [], "entities": []}, {"text": "IVS only takes 3 ms to handle anew coming key.", "labels": [], "entities": [{"text": "IVS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5563965439796448}]}, {"text": "Such a high processing speed meets the real-time latency requirement even on low-end devices.", "labels": [], "entities": []}, {"text": "In contrast, the non-incremental vocabulary selection is less efficient since it recalculates from the beginning of each step.", "labels": [], "entities": []}, {"text": "After solving the speed bottleneck of computing the softmax layer, the majority of the computational cost comes from the LSTM cells.", "labels": [], "entities": []}, {"text": "We  curacy gap between the baseline and softmax approximation.", "labels": [], "entities": []}, {"text": "In our experiments, the number of samples used in top sampling and uniform sampling methods is both 400.", "labels": [], "entities": []}, {"text": "If we are able to train the neural language model from scratch, then the self-normalization) approach can be applied as a softmax approximation.", "labels": [], "entities": []}, {"text": "When applying selfnormalization, the model is trained with additional normalization terms, which force the exponential logits to sum up to 1.", "labels": [], "entities": []}, {"text": "Therefore, it eliminates the necessity of performing vocabulary sampling.", "labels": [], "entities": [{"text": "vocabulary sampling", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.814949244260788}]}, {"text": "However, if one only has access to a pre-trained language model, then applying self-normalization is not an option.", "labels": [], "entities": []}, {"text": "Differentiated softmax (D-Softmax) and its variation (D-softmax * ) ( can also reduce the amount of softmax computation by over a half depending on the segmentation strategy.", "labels": [], "entities": []}, {"text": "However, since the vocabulary size is still large, the room for speedup is limited.", "labels": [], "entities": []}, {"text": "For character-based LSTM models, we use a hidden size of 1024 and embedding size of 512.", "labels": [], "entities": []}, {"text": "Since there is no direct mapping between a single Kana and a single Chinese character, we use the word lattice and evaluate the path probability by character-based LSTM.", "labels": [], "entities": []}, {"text": "It reduces the vocabulary size from 50K to 3717 in this experiment.", "labels": [], "entities": [{"text": "vocabulary size", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9120761454105377}]}, {"text": "However, the amount of vocabulary is still nontrivial.", "labels": [], "entities": []}, {"text": "Furthermore, the integration of characterbased models is not as efficient as word-based The large beam size is set to 50 in our experiments.", "labels": [], "entities": []}, {"text": "Consequently, we have to use a large beam size to improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9973713159561157}]}, {"text": "The cost to loop previous steps during fixing the vocabulary is not given here due to the implementation detail.", "labels": [], "entities": []}, {"text": "Ina real scenario, users do not type the full sentence in one effort.", "labels": [], "entities": []}, {"text": "Instead, they type and convert in fragments, where each fragment contains a few words.", "labels": [], "entities": []}, {"text": "It is trivial compared to softmax computation.", "labels": [], "entities": []}, {"text": "We confirmed that batching is critical for accelerating matrix operations on CPU.", "labels": [], "entities": []}, {"text": "The LSTM computation without batching is almost 7x slower with abeam size 10.", "labels": [], "entities": []}, {"text": "Therefore, we batch all the softmax computations when fixing the vocabulary in the second pass to achieve the best speed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of baseline LSTM  model with conventional n-gram model.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of different model acceleration approaches in terms of computation time and resultant accu- racy. The computation time is reported for each step.", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9167269468307495}]}]}