{"title": [], "abstractContent": [], "introductionContent": [{"text": "Welcome to the Third Workshop on Structured Prediction for NLP!", "labels": [], "entities": [{"text": "Structured Prediction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7504016757011414}]}, {"text": "Structured prediction has a strong tradition within the natural language processing (NLP) community, owing to the discrete, compositional nature of words and sentences, which leads to natural combinatorial representations such as trees, sequences, segments, or alignments, among others.", "labels": [], "entities": [{"text": "Structured prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8864179253578186}]}, {"text": "It is no surprise that structured output models have been successful and popular in NLP applications since their inception.", "labels": [], "entities": []}, {"text": "Many other NLP tasks, including, but not limited to: semantic parsing, slot filling, machine translation, or information extraction, are commonly modeled as structured problems, and accounting for said structure has often lead to performance gain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7554621696472168}, {"text": "slot filling", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.8193768560886383}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7923191785812378}, {"text": "information extraction", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7619306445121765}]}, {"text": "Of late, continuous representation learning via neural networks has been a significant complementary direction, leading to improvements in unsupervised and semi-supervised pre-training, transfer learning, domain adaptation, etc.", "labels": [], "entities": [{"text": "continuous representation learning", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.6593855023384094}, {"text": "transfer learning", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.933671236038208}, {"text": "domain adaptation", "start_pos": 205, "end_pos": 222, "type": "TASK", "confidence": 0.8002666234970093}]}, {"text": "Using word embeddings as features for structured models such as part-of-speech taggers count among the very first uses of continuous embeddings in NLP, and the symbiosis between the two approaches is an exciting research direction today.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7255816161632538}]}, {"text": "The five papers (as well as three additional non-archival papers) accepted for presentation in this edition of the workshop, after double-blind peer review, all explore this interplay between structure and neural data representations, from different, important points of view.", "labels": [], "entities": []}, {"text": "The program includes work on structure-informed representation learning, transfer learning, partial supervision, and parallelization of computation in structured computation graphs.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.952503889799118}]}, {"text": "Our program also includes six invited presentations from influential researchers.", "labels": [], "entities": []}, {"text": "Our warmest thanks go to the program committee -for their time and effort providing valuable feedback, to all submitting authors -for their thought-provoking work, and to the invited speakers -for doing us the honor of joining our program.", "labels": [], "entities": []}, {"text": "We are looking forward to seeing you in Minneapolis!", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}