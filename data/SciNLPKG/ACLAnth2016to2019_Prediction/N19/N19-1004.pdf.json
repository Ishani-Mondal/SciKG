{"title": [{"text": "Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state.", "labels": [], "entities": []}, {"text": "To do so, we employ experimental method-ologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind.", "labels": [], "entities": []}, {"text": "We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures.", "labels": [], "entities": []}, {"text": "These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states.", "labels": [], "entities": []}, {"text": "We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulor-dava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG.", "labels": [], "entities": []}, {"text": "We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signalling changes in syntactic state.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is now standard practice in NLP to derive sentence representations using neural sequence models of various kinds).", "labels": [], "entities": []}, {"text": "However, we do not yet have a firm understanding of the precise content of these representations, which poses problems for interpretability, accountability, and controllability of NLP systems.", "labels": [], "entities": []}, {"text": "More specifically, the success of neural sequence models has raised the question of whether and how these networks learn robust syntactic generalizations about natural language, which would enable robust performance even on data that differs from the peculiarities of the training set.", "labels": [], "entities": []}, {"text": "Here we build upon recent work studying neural language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind.", "labels": [], "entities": []}, {"text": "The basic idea is to examine language models' behavior on targeted sentences chosen to probe particular aspects of the learned representations.", "labels": [], "entities": []}, {"text": "This approach was introduced by, followed more recently by others (, who used an agreement prediction task to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets.", "labels": [], "entities": [{"text": "agreement prediction", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.6663034856319427}]}, {"text": "can grammatically continue with was but not with were.", "labels": [], "entities": []}, {"text": "This dependency turns out to be learnable from a language modeling objective (.", "labels": [], "entities": []}, {"text": "Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler-gap dependencies and negative results for anaphoric dependencies.", "labels": [], "entities": []}, {"text": "In this work, we consider syntactic representations of a different kind.", "labels": [], "entities": []}, {"text": "Previous studies have focused on relationships of dependency: one word licenses another word, which is tested by asking whether a language model favors one (grammatically licensed) form over another in a particular context.", "labels": [], "entities": []}, {"text": "Here we focus instead on whether neural language models show evidence for incremental syntactic state representations: whether behavior of neural language models reflects the kind of generalizations that would be captured using a stack-based incremental parse state in a symbolic grammar-based model.", "labels": [], "entities": []}, {"text": "For example, during the underlined portion of Example (1), an incremental language model should represent and maintain the knowledge that it is currently inside a subordinate clause, implying (among other things) that a full main clause must follow.", "labels": [], "entities": []}], "datasetContent": [{"text": "In each experiment presented below, we design a set of sentences such that the word-by-word surprisal values will show evidence for syntactic state representations.", "labels": [], "entities": []}, {"text": "The idea is that certain words will be surprising to a language model only if the model has a representation of a certain syntactic state going into the word.", "labels": [], "entities": []}, {"text": "We analyze wordby-word surprisal profiles for these sentences using regression analysis.", "labels": [], "entities": []}, {"text": "Except where otherwise noted, all statistics are derived from linear mixedeffects models () with sumcoded fixed-effect predictors and maximal random slope structure ().", "labels": [], "entities": []}, {"text": "This method lets us factor out by-item variation in surprisal and focus on the contrasts between conditions.", "labels": [], "entities": []}], "tableCaptions": []}