{"title": [{"text": "Probing the Need for Visual Context in Multimodal Machine Translation", "labels": [], "entities": [{"text": "Multimodal Machine Translation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6408516963322958}]}], "abstractContent": [{"text": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial.", "labels": [], "entities": [{"text": "multimodal machine translation (MMT)", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7774800409873327}]}, {"text": "We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context.", "labels": [], "entities": []}, {"text": "In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations.", "labels": [], "entities": []}, {"text": "In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context.", "labels": [], "entities": [{"text": "MMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9815769791603088}]}, {"text": "Our results show that under limited tex-tual context, models are capable of leveraging the visual input to generate better translations.", "labels": [], "entities": []}, {"text": "This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.", "labels": [], "entities": [{"text": "MMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9523544907569885}]}], "introductionContent": [{"text": "Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images.", "labels": [], "entities": [{"text": "Multimodal Machine Translation (MMT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8220145404338837}]}, {"text": "Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (, MMT has so far been studied using the Multi30K dataset ), a multilingual extension of Flickr30K () with translations of the English image descriptions into German, French and Czech (.", "labels": [], "entities": [{"text": "First Conference on Machine Translation (WMT16)", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.6747793182730675}, {"text": "Multi30K dataset", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.8792402446269989}, {"text": "Flickr30K", "start_pos": 185, "end_pos": 194, "type": "DATASET", "confidence": 0.8857380151748657}]}, {"text": "The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features) (ii) cross-modal interactions with spatially-unaware global features and (iii) the integration of regional features from object detection networks.", "labels": [], "entities": []}, {"text": "Nevertheless, the conclusion about the contribution of the visual modality is still unclear: consider their multimodal gains \"modest\" and attribute the largest gain to the usage of external parallel corpora.", "labels": [], "entities": []}, {"text": "observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart.", "labels": [], "entities": []}, {"text": "The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation ( . Ina similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images.", "labels": [], "entities": []}, {"text": "These empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations.", "labels": [], "entities": []}, {"text": "We conjecture that the most plausible reason for the linguistic dominance is that -at least in Multi30K -the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process.", "labels": [], "entities": []}, {"text": "To investigate this hypothesis, we introduce several input degradation regimes (Section 2) and revisit state-of-the-art MMT models (Section 3) to assess their behavior under degraded regimes.", "labels": [], "entities": []}, {"text": "We further probe the visual sensitivity by deliberately feeding features from unrelated images.", "labels": [], "entities": []}, {"text": "Our results show that MMT models successfully exploit the visual modality when the linguistic context is scarce, but indeed tend to be less sensitive to this modality when exposed to complete sentences.", "labels": [], "entities": [{"text": "MMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9585063457489014}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: The impact of incongruent decoding for pro- gressive masking: all METEOR differences are against  the DIRECT model. The blinded systems are both  trained and decoded using incongruent features.", "labels": [], "entities": [{"text": "pro- gressive masking", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6645750626921654}, {"text": "METEOR", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9725825190544128}]}]}