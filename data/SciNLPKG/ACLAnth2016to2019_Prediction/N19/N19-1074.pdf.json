{"title": [{"text": "Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization", "labels": [], "entities": [{"text": "Concept Mention Grouping", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.6558579206466675}, {"text": "Summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.7784443497657776}]}], "abstractContent": [{"text": "Concept map-based multi-document summa-rization has recently been proposed as a variant of the traditional summarization task with graph-structured summaries.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9053344130516052}]}, {"text": "As shown by previous work, the grouping of coreferent concept mentions across documents is a crucial subtask of it.", "labels": [], "entities": [{"text": "grouping of coreferent concept mentions across documents", "start_pos": 31, "end_pos": 87, "type": "TASK", "confidence": 0.8422461577824184}]}, {"text": "However, while the current state-of-the-art method suggested anew grouping method that was shown to improve the summary quality, its use of pairwise comparisons leads to polynomial runtime complexity that prohibits the application to large document collections.", "labels": [], "entities": []}, {"text": "In this paper, we propose two alternative grouping techniques based on locality sensitive hashing, approximate nearest neighbor search and a fast clustering algorithm.", "labels": [], "entities": []}, {"text": "They exhibit linear and log-linear run-time complexity, making them much more scalable.", "labels": [], "entities": []}, {"text": "We report experimental results that confirm the improved runtime behavior while also showing that the quality of the summary concept maps remains comparable.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concept maps are labeled graphs with nodes representing concepts and edges showing relationships between them (.", "labels": [], "entities": []}, {"text": "Following earlier work on the automatic extraction of concept maps from text (, concept maps have recently been promoted as an alternative representation for summaries).", "labels": [], "entities": [{"text": "automatic extraction of concept maps from text", "start_pos": 30, "end_pos": 76, "type": "TASK", "confidence": 0.8267847555024284}, {"text": "summaries", "start_pos": 158, "end_pos": 167, "type": "TASK", "confidence": 0.973816454410553}]}, {"text": "In the corresponding task, concept map-based multi-document summarization (CM-MDS), a set of documents has to be automatically summarized as a concept map that does not exceed a pre-defined size limit.", "labels": [], "entities": [{"text": "concept map-based multi-document summarization (CM-MDS)", "start_pos": 27, "end_pos": 82, "type": "TASK", "confidence": 0.6701408156326839}]}, {"text": "An important subtask of CM-MDS is concept mention grouping, in which all mentions that refer to a specific concept should be grouped together.", "labels": [], "entities": [{"text": "concept mention grouping", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6214593648910522}]}, {"text": "Without grouping, duplicates can appear in a summary concept map that make the map harder to understand and that waste valuable space.", "labels": [], "entities": []}, {"text": "To approach the mention grouping subtask, proposed to make pairwise coreference classifications between mentions and to induce a partitioning from those predictions.", "labels": [], "entities": []}, {"text": "Their experiments showed that this leads to better summary concept maps, establishing the current state-of-the-art for CM-MDS.", "labels": [], "entities": []}, {"text": "However, the computational costs of the approach are high, as it exhibits a O(n 4 ) worst-case time complexity.", "labels": [], "entities": [{"text": "O(n 4 ) worst-case time complexity", "start_pos": 76, "end_pos": 110, "type": "METRIC", "confidence": 0.6288484670221806}]}, {"text": "When the number of documents that should be summarized is large, applying that technique can quickly become impractical.", "labels": [], "entities": []}, {"text": "But exactly for those large document sets, a summary would be most helpful.", "labels": [], "entities": []}, {"text": "As the first contribution of this paper, we propose two faster grouping techniques.", "labels": [], "entities": []}, {"text": "First, we apply locality sensitive hashing (LSH)) to word embeddings in order to find similar mentions without making all pairwise comparisons.", "labels": [], "entities": []}, {"text": "That directly leads to a simple O(n) grouping method.", "labels": [], "entities": [{"text": "O(n) grouping", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.454691880941391}]}, {"text": "Second, we also propose a novel grouping technique that combines the hashing approach with a fast partitioning algorithm called Chinese Whispers (CW)).", "labels": [], "entities": []}, {"text": "It has O(n log n) time complexity and the advantage of being more transparently controllable.", "labels": [], "entities": [{"text": "O", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9324550628662109}]}, {"text": "Since the reduced complexity of the two proposed techniques is gained through approximations, the resulting grouping could of course be of lower quality.", "labels": [], "entities": []}, {"text": "As the second contribution of this paper, we therefore carryout end-to-end experiments in the context of CM-MDS to analyze this trade-off.", "labels": [], "entities": []}, {"text": "We compare both techniques against the state-of-the-art approach in automatic and manual evaluations.", "labels": [], "entities": []}, {"text": "For both, we observe orders of mag-nitude faster runtimes with only small reductions in summary quality.", "labels": [], "entities": []}, {"text": "In the future, the techniques could also be applied beyond CM-MDS to speedup other similarity-based partitioning problems in NLP and its applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed concept mention grouping techniques for the task of CM-MDS.", "labels": [], "entities": []}, {"text": "Data and Metrics We use the benchmark corpus introduced by, the only existing dataset with manually created reference summary concept maps.", "labels": [], "entities": []}, {"text": "It provides reference summaries for document sets of web pages on 30 different topics.", "labels": [], "entities": []}, {"text": "As metrics, we compute the ROUGE and METEOR variants proposed with the dataset and also perform a human evaluation following the protocol of.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9942179918289185}, {"text": "METEOR", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9664093255996704}]}, {"text": "Implementation As the reference, we use the state-of-the-art pipeline of.   and the combined approach (LSH-CW) by substituting them into that pipeline.", "labels": [], "entities": []}, {"text": "For a fair comparison, we use the same 300-dimensional word2vec embeddings ( for LSH that have also been used in the log-linear model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Concept mention grouping runtimes on average and for the smallest and largest set. Count is the number  of concepts after grouping the mentions given in the first row. Runtimes are measured on the same machine.", "labels": [], "entities": [{"text": "Count", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9972514510154724}]}, {"text": " Table 2: Evaluation results for summary concept maps.  Italics denote F1-scores that are significantly different  from Reference (exact permutation test, \u03b1 = 0.05).", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9986060261726379}]}]}