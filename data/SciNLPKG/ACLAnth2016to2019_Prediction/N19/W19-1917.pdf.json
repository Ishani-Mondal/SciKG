{"title": [{"text": "Attention Neural Model for Temporal Relation Extraction", "labels": [], "entities": [{"text": "Temporal Relation Extraction", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8539072275161743}]}], "abstractContent": [{"text": "Neural network models have shown promise in the temporal relation extraction task.", "labels": [], "entities": [{"text": "temporal relation extraction task", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.7139752879738808}]}, {"text": "In this paper, we present the attention based neural network model to extract the containment relations within sentences from clinical narratives.", "labels": [], "entities": []}, {"text": "The attention mechanism used on top of GRU model outperforms the existing state-of-the-art neural network models on THYME corpus in intra-sentence temporal relation extraction.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8952537477016449}, {"text": "intra-sentence temporal relation extraction", "start_pos": 132, "end_pos": 175, "type": "TASK", "confidence": 0.6304262578487396}]}], "introductionContent": [{"text": "A well-known challenge in leveraging electronic health records (EHRs) for research is to extract the information embedded in clinical texts.", "labels": [], "entities": []}, {"text": "The recent progress in Natural Language Processing (NLP) techniques has facilitated the use of information in text for various clinical applications ().", "labels": [], "entities": []}, {"text": "One important NLP task in the clinical domain is to extract temporal relations between events and time expressions from clinical text for various EHR-based applications, such as clinical decision support and predictive modeling.", "labels": [], "entities": [{"text": "predictive modeling", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.9669539332389832}]}, {"text": "Along with studies in modeling clinical temporal events using structured EHR data (, a series of temporal information extraction share tasks have been organized to encourage community efforts on the temporal relation extraction on unstructured clinical texts from EHR, such as i2b2 (Informatics for Integrating Biology and the Bedside) 2012 challenge ( and Clinical TempEval shared tasks (.", "labels": [], "entities": [{"text": "EHR data", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.8573935031890869}, {"text": "temporal relation extraction", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.738423764705658}]}, {"text": "While both corpora are based on de-identified clinical notes, the major differences between i2b2 and TempEval are the evaluation and temporal event modeling.", "labels": [], "entities": []}, {"text": "The i2b2 challenge evaluation enumerates all possible entity pairs from a clinical document into the evaluation, while the TempEval tasks leverage the concept of narrative containers which will enhance conventional temporal relations.", "labels": [], "entities": []}, {"text": "In this study, we focus on the containment information extraction in TempEval.", "labels": [], "entities": [{"text": "containment information extraction", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.9227352142333984}]}, {"text": "In addition to the feature-based machine learning approaches such as Support Vector Machines (SVM) and conditional random field from topperforming TempEval 2016 systems (, there are several machine learning systems proposed after the shared task.", "labels": [], "entities": []}, {"text": "used a structured learning method to predict temporal relations:  proposed an XML tag representation neural models such as Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM)) to mark the positions of the entities and achieved better performance compared to token position embeddings.", "labels": [], "entities": []}, {"text": "They also evaluated the contains relations solely on medical events.", "labels": [], "entities": []}, {"text": "experimented on different representations of XML tags proposed in ( , and the results indicated that the input representation is an importance factor for the performance of neural models.", "labels": [], "entities": []}, {"text": "A bidirectional LSTM (BiLSTM) approach has also been proposed in.", "labels": [], "entities": []}, {"text": "Their model utilized character embeddings to create a hierarchical LSTM model with corpus entities attributes as input into the embedding layer of their neural architecture.", "labels": [], "entities": []}, {"text": "Recent related works using self-training ( and human-like temporal reasoning via tree-based LSTM-RNN () also achieved good performance in various evaluation scenarios, but direct comparisons are challenging due to differences in evaluation.", "labels": [], "entities": []}, {"text": "Inspired by visual attention models for object recognition in computer vision (), attention mechanism has also been successfully applied in several NLP tasks such as machine translation, machine reading (, document classification () and relation extraction (, to obtain state-of-the-art performance.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7635579109191895}, {"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8100191354751587}, {"text": "machine reading", "start_pos": 187, "end_pos": 202, "type": "TASK", "confidence": 0.7434276342391968}, {"text": "document classification", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7629709243774414}, {"text": "relation extraction", "start_pos": 237, "end_pos": 256, "type": "TASK", "confidence": 0.8472408950328827}]}, {"text": "The attention layer oversees the entire sequence of recurrent neural network (RNN) units and is trained to pay more \"attention\" to salient units.", "labels": [], "entities": []}, {"text": "In this paper, we present an attention neural model to identify containment relations from clinical narratives with annotated medical events and temporal information.", "labels": [], "entities": []}, {"text": "The model achieves stateof-the-art performance in intra-sentence temporal relation extraction while using minimal entity features and external knowledge.", "labels": [], "entities": [{"text": "intra-sentence temporal relation extraction", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.713378980755806}]}], "datasetContent": [{"text": "The official evaluation scripts of TempEval 2 use the concept of \"narrative containers\" to validate the results.", "labels": [], "entities": []}, {"text": "Narrative containers is a set of events that contains multiple temporal relations.", "labels": [], "entities": [{"text": "Narrative containers", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8732362687587738}]}, {"text": "The official evaluation uses narrative container to evaluate the system performance, instead of evaluating directly from the relation classification results by instances.", "labels": [], "entities": []}, {"text": "The usage of closure is intended to reduce the penalty caused by extracting the implicit relations that can be inferred between events but are not included in the annotation.", "labels": [], "entities": []}, {"text": "Following the shared task of TempEval 2016 and recent related work on the THYME corpus, we focus on the extraction of temporal containment relations.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9199973344802856}, {"text": "temporal containment relations", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.660191277662913}]}, {"text": "This is because the prevalence of contains relations is much higher than other temporal relations.", "labels": [], "entities": [{"text": "prevalence", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9707006812095642}]}, {"text": "We ran our experiments on similar settings as).", "labels": [], "entities": []}, {"text": "The cross sentence relations are excluded in our evaluation.", "labels": [], "entities": []}, {"text": "The models are implemented in Keras with Tensorflow backend.", "labels": [], "entities": []}, {"text": "The experiments are done on a computing server with NVIDIA Tesla P40 GPU.", "labels": [], "entities": []}, {"text": "Each epoch of attention based LSTM took approximately 300 seconds while GRU will take approximately 250 seconds, due to fewer trainable parameters needed for each unit.", "labels": [], "entities": [{"text": "GRU", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7975717782974243}]}, {"text": "The 300-dimension word embeddings from Glove-6B 3 are selected as the input based on our preliminary experiments on trained embeddings from biomedical domain () as well as the THYME corpus.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 176, "end_pos": 188, "type": "DATASET", "confidence": 0.9371345639228821}]}, {"text": "The embedding of out-of-vocabulary words, including special XML tags, are determined by random sampling from unit distribution in [-0.1, 0.1].", "labels": [], "entities": []}, {"text": "The hyperparameters are selected based on the optimal combination from the development set when training on  the training set.", "labels": [], "entities": []}, {"text": "To avoid potential overfitting during the training phase, we apply dropout technique () with the dropout rate of 0.5.", "labels": [], "entities": []}, {"text": "Adam optimizer () is used with learning rate 0.001 to train the model and sparse categorical cross entropy as the loss function.", "labels": [], "entities": []}, {"text": "We apply early stopping during training to avoid overfitting by terminating the training process if there is no validation accuracy increase in consecutive 4 epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.7632958889007568}]}, {"text": "Then the training and development set are combined to train the model while tested on the testing set.", "labels": [], "entities": []}, {"text": "The batch size of training is 64, and the unit size for RNN units is set 128 based on hyperparameter tuning.", "labels": [], "entities": []}, {"text": "The evaluation results on Event-Time and Event-Event relation extraction in closureenhanced precision (P), recall (R) and F1-score are shown in.", "labels": [], "entities": [{"text": "Event-Event relation extraction", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.5856373906135559}, {"text": "closureenhanced precision (P)", "start_pos": 76, "end_pos": 105, "type": "METRIC", "confidence": 0.8624723315238952}, {"text": "recall (R)", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9644235670566559}, {"text": "F1-score", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9980838298797607}]}, {"text": "\"ATT-\" denotes our attention based RNN models.", "labels": [], "entities": [{"text": "ATT-\"", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9573066532611847}]}, {"text": "The results in are directly comparable with the work in ( , since the models of Event-Time Event-Event relations are trained separately.", "labels": [], "entities": []}, {"text": "The most significant improvement is from the EventTime relation extraction, where the ATT-GRU (0.750) outperforms the CNN model by 0.050.", "labels": [], "entities": [{"text": "EventTime relation extraction", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6067717373371124}, {"text": "ATT-GRU", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9194589257240295}]}, {"text": "In the Event-Event relations, ATT-GRU model outperforms the CNN model, but is not as good as the feature based SVM model in the THYME system (-0.05).", "labels": [], "entities": [{"text": "ATT-GRU", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8996705412864685}]}, {"text": "One potential reason for the performance gain is that the ATT models oversee all units from the RNN layer rather than focusing on the max pooling of local features as CNN.", "labels": [], "entities": []}, {"text": "When we combined both Event-Time and Event-Event relations together, shows the results for all temporal relations within each sentence.", "labels": [], "entities": []}, {"text": "Compared to other neural network models, our proposed ATT-GRU (0.690 F1) is favorably comparable to the BiLSTM model incorporating cTAKES outputs 4 (BiLSTM+cTAKES) and character embeddings (+0.007).", "labels": [], "entities": [{"text": "ATT-GRU", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.8705745339393616}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.8947350382804871}]}, {"text": "We only use the raw text and annotated entity types, while BiLSTM+cTAKES requires finer granularity of the UMLS 5 entity types and semantic types as inputs.", "labels": [], "entities": []}, {"text": "It is our future perspective to utilize character-level information and entity attributes as the input to further improve our system.", "labels": [], "entities": []}, {"text": "ATT-GRU performs better than LSTM in all the three evaluation scenarios.", "labels": [], "entities": [{"text": "ATT-GRU", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5568758845329285}]}, {"text": "One potential reason is that GRU has less trainable parameters compared to LSTM, thus it may converge better in a corpus with relatively limited positive relational instances.", "labels": [], "entities": []}, {"text": "One challenge for neural models in the temporal relation extraction task is class imbalance.", "labels": [], "entities": [{"text": "temporal relation extraction task", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.7117109447717667}]}, {"text": "The majority of the errors are caused by the confusion between negative (\"NA\") and positive (\"CON-TAINS\"+\"CONTAINED\") instances, while very few of the errors are from the confusion of \"CON-TAINS\" and \"CONTAINED\" relations.", "labels": [], "entities": []}, {"text": "The ratios between positive and negative relations of EventEvent, Event-Time and those combined are 1:3.4, 1:12.7 and 1:8.4, respectively.", "labels": [], "entities": []}, {"text": "The class weights are tuned in the feature-based THYME system to improve the balance of precision and recall, but there is no such effort on other neural models in both our work and ( ).", "labels": [], "entities": [{"text": "THYME", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.6999192833900452}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9993581175804138}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9986977577209473}]}, {"text": "analyzed the impact of different XML tags for the temporal entities as inputs.", "labels": [], "entities": []}, {"text": "The one-token tag representation for multi-word temporal repressions (e.g. replacing the Timex3 mention \"March 11, 2014\" by \"<date>\") shows improvements on the classification, which is also used in our study.", "labels": [], "entities": [{"text": "multi-word temporal repressions", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6203664243221283}, {"text": "Timex3 mention \"March 11, 2014", "start_pos": 89, "end_pos": 119, "type": "DATASET", "confidence": 0.8858873758997236}]}, {"text": "Compared to Lin's method, our model is a single neural model instead of a combined model of CNN and SVM for Event-Event and Event-Time relations, respectively.", "labels": [], "entities": []}, {"text": "Leeuwenberg and Moens ( used structured learning on all relations within token distance of 30.", "labels": [], "entities": []}, {"text": "The framework can also be extended to model inter-sentence relations by adding such relation instances into the training and testing, but fine-tuned down-sampling needs to be done to optimize its performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance comparison in intra-sentence containment relations on test set", "labels": [], "entities": []}]}