{"title": [], "abstractContent": [{"text": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \"died\" and \"dyed\").", "labels": [], "entities": []}, {"text": "Supervised text generation is inappropriate due to the lack of a large corpus of puns, and even if such a corpus existed, mimicry is at odds with generating novel content.", "labels": [], "entities": [{"text": "text generation", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7250518947839737}]}, {"text": "In this paper, we propose an unsuper-vised approach to pun generation using a corpus of unhumorous text and what we call the local-global surprisal principle: we posit that in a pun sentence, there is a strong association between the pun word (e.g., \"dyed\") and the distant context, as well as a strong association between the alternative word (e.g., \"died\") and the immediate context.", "labels": [], "entities": [{"text": "pun generation", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8657666146755219}]}, {"text": "This contrast creates surprise and thus humor.", "labels": [], "entities": []}, {"text": "We instantiate this principle for pun generation in two ways: (i) as a measure based on the ratio of probabilities under a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model.", "labels": [], "entities": [{"text": "pun generation", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8683142960071564}]}, {"text": "Human evaluation shows that our retrieve-and-edit approach generates puns successfully 31% of the time, tripling the success rate of a neural generation baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (, story generation, and social chatbots.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7149905562400818}, {"text": "poetry generation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7745918333530426}, {"text": "story generation", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7916928827762604}]}, {"text": "In this paper, we explore creative generation with a focus on puns.", "labels": [], "entities": []}, {"text": "We follow the definition of puns in Aarons (2017);: \"A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological: An illustration of a homophonic pun.", "labels": [], "entities": []}, {"text": "The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated.", "labels": [], "entities": []}, {"text": "The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence.", "labels": [], "entities": []}, {"text": "similarity to another sign, for an intended humorous or rhetorical effect.\"", "labels": [], "entities": []}, {"text": "We focus on atypical class of puns where the ambiguity comes from two (near) homophones.", "labels": [], "entities": []}, {"text": "Consider the example in Figure 1: \"Yesterday I accidentally swallowed some food coloring.", "labels": [], "entities": []}, {"text": "The doctor says I'm OK, but I feel like I've dyed (died) a little inside.\".", "labels": [], "entities": []}, {"text": "The pun word shown in the sentence (\"dyed\") indicates one interpretation: the person is colored inside by food coloring.", "labels": [], "entities": []}, {"text": "On the other hand, an alternative word (\"died\") is implied by the context for another interpretation: the person is sad due to the accident.", "labels": [], "entities": []}, {"text": "Current approaches to text generation require lots of training data, but there is no large corpus of puns.", "labels": [], "entities": [{"text": "text generation", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8517389893531799}]}, {"text": "Even such a corpus existed, learning the distribution of existing data and sampling from it is unlikely to lead to truly novel, creative sentences.", "labels": [], "entities": []}, {"text": "Creative composition requires deviating from the norm, whereas standard generation approaches seek to mimic the norm.", "labels": [], "entities": []}, {"text": "Recently, proposed an unsupervised approach that generates puns from a neural language model by jointly decoding conditioned on both the pun and the alternative words, thus injecting ambiguity to the output sentence.", "labels": [], "entities": []}, {"text": "However, showed that ambiguity alone is insufficient to bring humor; the two mean-ings must also be supported by distinct sets of words in the sentence.", "labels": [], "entities": []}, {"text": "Inspired by, we propose a general principle for puns which we call local-global surprisal principle.", "labels": [], "entities": []}, {"text": "Our key observation is that the strength for the interpretation of the pun and the alternative words flips as one reads the sentence.", "labels": [], "entities": [{"text": "interpretation of the pun", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7951658219099045}]}, {"text": "For example, in, \"died\" is favored by the immediate (local) context, whereas \"dyed\" is favored by the global context (i.e. \"...food coloring...\").", "labels": [], "entities": []}, {"text": "Our surprisal principle posits that the pun word is much more surprising in the local context than in the global context, while the opposite is true for the alternative word.", "labels": [], "entities": []}, {"text": "We instantiate our local-global surprisal principle in two ways.", "labels": [], "entities": []}, {"text": "First, we develop a quantitative metric for surprise based on the conditional probabilities of the pun word and the alternative word given local and global contexts under a neural language model.", "labels": [], "entities": []}, {"text": "However, we find that this metric is not sufficient for generation.", "labels": [], "entities": []}, {"text": "We then develop an unsupervised approach to generate puns based on a retrieve-and-edit framework (  given an unhumorous corpus ().", "labels": [], "entities": []}, {"text": "We call our system SURGEN (SURprisal-based pun GENeration).", "labels": [], "entities": []}, {"text": "We test our approach on 150 pun-alternative word pairs.", "labels": [], "entities": []}, {"text": "1 First, we show a strong correlation between our surprisal metric and funniness ratings from crowdworkers.", "labels": [], "entities": []}, {"text": "Second, human evaluation shows that our system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (, and results in higher funniness scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluate how well our surprisal principle predicts the funniness of sentences perceived by humans (Section 4.2), and then compare our pun generation system and its varia-2 Path similarity is a score between 0 and 1 that is inversely proportional to the shortest distance between two word senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 307, "end_pos": 314, "type": "DATASET", "confidence": 0.9628934264183044}]}, {"text": "3 Pronouns are mapped to the synset person.n.01.", "labels": [], "entities": []}, {"text": "tions with a simple retrieval baseline and a neural generation model () (Section 4.3).", "labels": [], "entities": []}, {"text": "We show that the local-global surprisal scores strongly correlate with human ratings of funniness, and all of our systems outperform the baselines based on human evaluation.", "labels": [], "entities": []}, {"text": "In particular, RETRIEVE+SWAP+TOPIC (henceforth SURGEN) achieves the highest success rate and average funniness score among all systems.", "labels": [], "entities": [{"text": "RETRIEVE+SWAP+TOPIC", "start_pos": 15, "end_pos": 34, "type": "METRIC", "confidence": 0.7785094738006592}, {"text": "funniness score", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.8792389929294586}]}, {"text": "We use the pun dataset from 2017 SemEval task7 ().", "labels": [], "entities": [{"text": "pun dataset from 2017 SemEval task7", "start_pos": 11, "end_pos": 46, "type": "DATASET", "confidence": 0.8714930216471354}]}, {"text": "The dataset contains 1099 human-written puns annotated with pun words and alternative words, from which we take 219 for development.", "labels": [], "entities": []}, {"text": "We use BookCorpus () as the generic corpus for retrieval and training various components of our system.", "labels": [], "entities": [{"text": "BookCorpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9396253824234009}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics and funniness ratings of SEMEVAL and KAO. Pun or alternative words are underlined  in the example sentence. Each worker's ratings are standardized to z-scores. There is clear separation among the  three types in terms of funniness, where pun > swap-pun > non-pun.", "labels": [], "entities": [{"text": "KAO", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.7279732823371887}]}, {"text": " Table 2: Spearman correlation between different metrics and human ratings of funniness. Statistically significant  correlations with p-value < 0.05 are bolded. Our surprisal principle successfully differentiates puns from non- puns and swap-puns. Distinctiveness is the only metric that correlates strongly with human ratings within puns.  However, no single metric works well across different types of sentences.", "labels": [], "entities": []}, {"text": " Table 3: Human evaluation results of all sys- tems.", "labels": [], "entities": []}, {"text": " Table 4: Pairwise comparison between SURGEN and  NEURALJOINTDECODER (NJD), and between SUR- GEN and human written puns. Win % (lose %) is  the percentage among the human-rated 150 sentences  where SURGEN achieves a higher (lower) average  score compared to the other method. The rest are ties.", "labels": [], "entities": [{"text": "Win % (lose %)", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.9322250843048095}]}, {"text": " Table 5: Examples of generated puns with average human ratings of funniness", "labels": [], "entities": []}]}