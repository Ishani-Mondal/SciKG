{"title": [{"text": "Modelling Instance-Level Annotator Reliability for Natural Language Labelling Tasks", "labels": [], "entities": [{"text": "Modelling Instance-Level Annotator Reliability", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6022252589464188}, {"text": "Natural Language Labelling Tasks", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7289147824048996}]}], "abstractContent": [{"text": "When constructing models that learn from noisy labels produced by multiple annotators, it is important to accurately estimate the reliability of annotators.", "labels": [], "entities": []}, {"text": "Annotators may provide labels of inconsistent quality due to their varying expertise and reliability in a domain.", "labels": [], "entities": []}, {"text": "Previous studies have mostly focused on estimating each annotator's overall reliability on the entire annotation task.", "labels": [], "entities": [{"text": "reliability", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9385955929756165}]}, {"text": "However, in practice, the reliability of an annotator may depend on each specific instance.", "labels": [], "entities": [{"text": "reliability", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9730264544487}]}, {"text": "Only a limited number of studies have investigated modelling per-instance reliability and these only considered binary labels.", "labels": [], "entities": []}, {"text": "In this paper, we propose an un-supervised model which can handle both binary and multi-class labels.", "labels": [], "entities": []}, {"text": "It can automatically estimate the per-instance reliability of each annotator and the correct label for each instance.", "labels": [], "entities": []}, {"text": "We specify our model as a proba-bilistic model which incorporates neural networks to model the dependency between latent variables and instances.", "labels": [], "entities": []}, {"text": "For evaluation, the proposed method is applied to both synthetic and real data, including two labelling tasks: text classification and textual entail-ment.", "labels": [], "entities": [{"text": "text classification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7738097012042999}]}, {"text": "Experimental results demonstrate our novel method cannot only accurately estimate the reliability of annotators across different instances , but also achieve superior performance in predicting the correct labels and detecting the least reliable annotators compared to state-of-the-art baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many natural language processing (NLP) applications, the performance of supervised machine learning models depends on the quality of the corpus used to train the model.", "labels": [], "entities": []}, {"text": "Traditionally, labels are collected from multiple annotators/experts who are assumed to provide reliable labels.", "labels": [], "entities": []}, {"text": "However, in reality, these experts may have varying levels of expertise depending on the domains, and thus may disagree on labelling in certain cases.", "labels": [], "entities": []}, {"text": "A rapid and costeffective alternative is to obtain labels through crowdsourcing (.", "labels": [], "entities": []}, {"text": "In crowdsourcing, each instance is presented to multiple expert or non-expert annotators for labelling.", "labels": [], "entities": []}, {"text": "However, labels collected in this manner could be noisy, since some annotators could produce a significant number of incorrect labels.", "labels": [], "entities": []}, {"text": "This maybe due to differing levels of expertise, lack of financial incentive and interest, as well as the tedious and repetitive nature of the annotation task ().", "labels": [], "entities": []}, {"text": "Thus, in order to ensure the accuracy of the labelling and the quality of the corpus, it is crucial to estimate the reliability of the annotators automatically without human intervention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9981901049613953}]}, {"text": "Previous studies have mostly focused on evaluating the annotators' overall reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9535753130912781}]}, {"text": "Measuring the reliability on a per-instance basis is however useful as we may expect certain annotators to have more expertise in one domain than another, and as a consequence certain annotation decisions will be more difficult than others.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.970482587814331}]}, {"text": "This resolves a potential issue of models that only assign an overall reliability to each annotator, where such a model would determine an annotator with expertise in a single domain to be unreliable for the model, even though the annotations are reliable within the annotator's domain of expertise.", "labels": [], "entities": []}, {"text": "Estimating per-instance reliability is also helpful for unreliable annotator detection and task allocation in crowdsourcing, where the cost of labelling data is reduced using proactive learn-ing strategies for pairing instances with the most cost-effective annotators (.", "labels": [], "entities": [{"text": "annotator detection", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7687924802303314}]}, {"text": "Although reliability estimation has been studied fora longtime, only a limited number of studies have examined how to model the reliability of each annotator on a perinstance basis.", "labels": [], "entities": [{"text": "reliability estimation", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8040262460708618}]}, {"text": "Additionally, these in turn have only considered binary labels (, and cannot be extended to multi-class classification in a straightforward manner.", "labels": [], "entities": []}, {"text": "In order to handle both binary and multi-class labels, our approach extends one of the most popular probabilistic models for label aggregation, proposed by.", "labels": [], "entities": []}, {"text": "One challenge of extending the model is the definition of the label and reliability probability distributions on a perinstance basis.", "labels": [], "entities": []}, {"text": "Our approach introduces a classifier which predicts the correct label of an instance, and a reliability estimator, providing the probability that an annotator will label a given instance correctly.", "labels": [], "entities": [{"text": "reliability estimator", "start_pos": 92, "end_pos": 113, "type": "METRIC", "confidence": 0.9533946514129639}]}, {"text": "The approach allows us to simultaneously estimate the per-instance reliability of the annotators and the correct labels, allowing the two processes to inform each other.", "labels": [], "entities": []}, {"text": "Another challenge is to select appropriate training methods to learn a model with high and stable performance.", "labels": [], "entities": []}, {"text": "We investigate training our model using the EM algorithm and cross entropy.", "labels": [], "entities": []}, {"text": "For evaluation, we apply our method to six datasets including both synthetic and real-world datasets (see Section 4.1).", "labels": [], "entities": []}, {"text": "In addition, we also investigate the effect on the performance when using different text representation methods and text classification models (see Section 4.2).", "labels": [], "entities": [{"text": "text classification", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7374075353145599}]}, {"text": "Our contributions are as follows: firstly, we propose a novel probabilistic model for the simultaneous estimation of per-instance annotator reliability and the correct labels for natural language labelling tasks.", "labels": [], "entities": []}, {"text": "Secondly, our work is the first to propose a model for modelling per-instance reliability for both binary and multi-class classification tasks.", "labels": [], "entities": [{"text": "multi-class classification tasks", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.7438450256983439}]}, {"text": "Thirdly, we show experimentally how our method can be applied to different domains and tasks by evaluating it on both synthetic and realworld datasets.", "labels": [], "entities": []}, {"text": "We demonstrate that our method is able to capture the reliability of each annotator on a per-instance basis, and that this in turn helps improve the performance when predicting the underlying label for each instance and detecting the least reliable annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model was implemented using the Chainer deep learning framework 3 (Tokui et al., 2015).", "labels": [], "entities": []}, {"text": "Classifier: As shown in, in each experiment the output of the classifier is generated by a feed-forward neural network (FNN).", "labels": [], "entities": []}, {"text": "Each FNN consists of an input layer, two hidden layers and a softmax output layer.", "labels": [], "entities": []}, {"text": "The number of hidden units in each layer is listed in the third column of the table.", "labels": [], "entities": []}, {"text": "The ReLU activation function) was applied after each hidden layer.", "labels": [], "entities": []}, {"text": "The output size of all the Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) layers in our experiments is 100.", "labels": [], "entities": []}, {"text": "For the 2-dimensional classification task, each instance is simply represented using its position in 2-dimensional space.", "labels": [], "entities": [{"text": "2-dimensional classification task", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.8220749696095785}]}, {"text": "For the text classification tasks, we investigated 3 methods of representing the sentences: bag-of-words (BoW) weighted by Term Frequency-Inverse Document Frequency (TFIDF), an average word embedding (Avg.) and the output at the last step of an LSTM layer (Embed.\u2192LSTM).", "labels": [], "entities": [{"text": "text classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7310909032821655}, {"text": "bag-of-words (BoW) weighted by Term Frequency-Inverse Document Frequency (TFIDF)", "start_pos": 92, "end_pos": 172, "type": "METRIC", "confidence": 0.7918535333413345}, {"text": "Avg.", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.8388068079948425}]}, {"text": "For the embedding we use word2vec embeddings pre-trained on Google News () for the question classification and RTE tasks, and a pre-trained embedding () trained on a combination of English Wikipedia, PubMed and PMC texts for the sentence classification task.", "labels": [], "entities": [{"text": "question classification", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8001174926757812}, {"text": "PubMed and PMC texts", "start_pos": 200, "end_pos": 220, "type": "DATASET", "confidence": 0.6220327317714691}, {"text": "sentence classification", "start_pos": 229, "end_pos": 252, "type": "TASK", "confidence": 0.7951559126377106}]}, {"text": "For the RTE task, we implemented two classifiers.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 8, "end_pos": 16, "type": "TASK", "confidence": 0.9088769555091858}]}, {"text": "For the first one, each instance (i.e. a sentence pair) was represented as a concatenation of the average word embedding for each sentence (Cat. Avg.).", "labels": [], "entities": []}, {"text": "We also implemented, which runs each sentence through an LSTM, concatenates the outputs, and then feeds the concatenated output to an FNN with tanh activations.", "labels": [], "entities": []}, {"text": "Reliability Estimator: We model the reliability estimator as an FNN.", "labels": [], "entities": [{"text": "Reliability Estimator", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6822333782911301}, {"text": "FNN", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.7992844581604004}]}, {"text": "Its structure is the same as the classifier, albeit with different sizes of the two hidden layers.", "labels": [], "entities": []}, {"text": "For the experiments listed in Table 2, the number of units of each hidden layer in the FNN are 5, 100, 25, 25, 50, and 100 respectively.", "labels": [], "entities": [{"text": "FNN", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8976151943206787}]}, {"text": "The input to the estimator is the concatenation of the instance xi (i.e. its original feature vector or the output of the last hidden layer of the classifier) and a one-hot vector representing the annotator identity.", "labels": [], "entities": []}, {"text": "Learning Settings: For every experiment we use the Adam () optimiser with a weight decay rate 0.001, a gradient clipping of 5.0, \u03b1 = 0.001, \u03b2 1 = 0.9 and \u03b2 2 = 0.999.", "labels": [], "entities": [{"text": "weight decay rate", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.8588083585103353}]}, {"text": "We pre-train the classifier and reliability estimator for 200 epochs, using both majority voting and the model proposed by.", "labels": [], "entities": []}, {"text": "The maximum number of outer iterations is set to 500 and 20 for EM training and cross entropy training respectively.", "labels": [], "entities": []}, {"text": "The number of inner iterations is 50 in both cases.", "labels": [], "entities": []}, {"text": "True Label Prediction and Reliability Estimation: After training, for each instance xi we take its underlying label to be the most probable label according to the posterior oft i (see Equation (4)).", "labels": [], "entities": [{"text": "Reliability Estimation", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7110763341188431}]}, {"text": "We compared our predicted labels to the following state-of-the-art baselines: Majority Voting (MV), DS is the state-of-the-art method that models per-instance reliability.", "labels": [], "entities": []}, {"text": "We take the reliability of annotator j on instance xi to be the posterior probability that r ij is 1 (see Equation (5)).", "labels": [], "entities": [{"text": "reliability", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9952380657196045}]}, {"text": "Based on the results of the full model in, we can conclude that per-instance reliability modelling is beneficial to the label prediction task, and using the average pre-trained embedding can result in slightly better performance.", "labels": [], "entities": [{"text": "label prediction task", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8364622990290324}]}, {"text": "It is worth noting that the method used to pre-train the model had a noticeable effect on its performance, with better F1 scores being obtained when using DS pretraining.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9795801639556885}]}, {"text": "In the following experiments we only consider models pre-trained using the DS algorithm.", "labels": [], "entities": []}, {"text": "In order to investigate whether our method can successfully capture per-instance annotator reliability, for each annotator, we counted the number of correctly labelled instances and calculated the Question Classification DESC ENTY ABBR HUM NUM LOC Accuracy 1 8 100  average reliability for each class among the top 100 instances with the highest per-instance reliability as shown in and 6 5 . The cells with grey background colour indicate which domain, or class, the annotator has expertise in.", "labels": [], "entities": [{"text": "DESC ENTY ABBR HUM NUM LOC Accuracy 1 8 100  average reliability", "start_pos": 221, "end_pos": 285, "type": "METRIC", "confidence": 0.8051980634530386}]}, {"text": "It can be seen that all annotators obtain high accuracy on these instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9987435936927795}]}, {"text": "In general our method also captured the varying expertise of each narrow annotator, estimating their reliability on instances belonging to the corresponding classes as particularly high.", "labels": [], "entities": [{"text": "reliability", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9760059118270874}]}, {"text": "For these experiments in, we also investigated the performance when using two different classification models.", "labels": [], "entities": []}, {"text": "As seen in this table, both of them outperformed all baselines significantly.", "labels": [], "entities": []}, {"text": "presents the label prediction performance on the RTE dataset.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.6875771433115005}, {"text": "RTE dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9503237903118134}]}, {"text": "As not every annotator has provided labels for every instance in this dataset, for both the EM and cross entropy training we simply omitted missing instance/annotator pairs when calculating the loss functions.", "labels": [], "entities": []}, {"text": "As seen in the table, most of the baselines obtained high performance as the textual entailment recognition task is easy for non-expert annotators.", "labels": [], "entities": [{"text": "textual entailment recognition task", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.7742071449756622}]}, {"text": "However, our full model still achieved better prediction performance We omit the results for the sentence classification task for lack of space, as we consider the results on the question classification dataset to be representative.   than all of the baseline methods.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.8232702016830444}]}, {"text": "We also investigated the effectiveness of our model for removing noisy labels.", "labels": [], "entities": []}, {"text": "We compare our model to the five best-performing baselines (DS, LFC, CUBAM, VI and EM-MV in).", "labels": [], "entities": [{"text": "VI", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9496011734008789}, {"text": "EM-MV", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.8547055721282959}]}, {"text": "Each of these models are trained on the RTE dataset, after which the least reliable annotation for each instance is removed.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9364330470561981}]}, {"text": "We use the per-instance reliability for our model, the global reliability score of each annotator for LFC, CUBAM and VI, and the per-category annotator reliability for DS and EM-MV as the measure of the reliability of each annotation.", "labels": [], "entities": [{"text": "global reliability score", "start_pos": 55, "end_pos": 79, "type": "METRIC", "confidence": 0.7881445487340292}]}, {"text": "For each of these models, we then retrain the models in using the denoised dataset; the difference in performance can be seen in Table 9.", "labels": [], "entities": []}, {"text": "We can see that using per-instance reliability results in the largest improvement, while only considering the annotators' overall reliability may   cause a reduction in performance.", "labels": [], "entities": []}, {"text": "In order to analyse the per-instance reliability of the human annotators, for each annotator we rank the instances according to the annotator's perinstance reliability.", "labels": [], "entities": []}, {"text": "We look at the top 15 and bottom 15 instances, then count how many of them were correctly labelled (Cor.", "labels": [], "entities": []}, {"text": "Labels) as well as the average reliability on these instances (Avg. Reliability).", "labels": [], "entities": [{"text": "reliability", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9725033640861511}, {"text": "Avg. Reliability)", "start_pos": 63, "end_pos": 80, "type": "METRIC", "confidence": 0.8016699353853861}]}, {"text": "shows the results of five annotators . It can be seen that each annotator has considerably different reliabilities across instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classes and per-class instance counts.", "labels": [], "entities": []}, {"text": " Table 2: Classifiers used in the experiments.", "labels": [], "entities": []}, {"text": " Table 3: F1 scores of predicted labels on the 2- dimensional datasets when using the output of the last  hidden layer of the classifier to represent an instance  for the reliability estimator.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9657059907913208}]}, {"text": " Table 4: F1 scores of predicted labels on the text clas- sification datasets.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.968483567237854}, {"text": "text clas- sification datasets", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.5397087574005127}]}, {"text": " Table 5: Number of correctly labelled examples for  each annotator (N: narrow expert, B: broad expert, R:  random annotator and A: adversarial annotator) among  the 100 instances with highest per-instance reliability  on the question classification dataset.", "labels": [], "entities": [{"text": "question classification", "start_pos": 226, "end_pos": 249, "type": "TASK", "confidence": 0.6839391589164734}]}, {"text": " Table 6: Average reliability of each annotator among  the 100 instances with the highest per-instance reliabil- ity on the question classification dataset.", "labels": [], "entities": [{"text": "reliability", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.815891683101654}]}, {"text": " Table 7: F1 scores on text classification tasks when  only the reliability differs between the annotators.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993470311164856}, {"text": "text classification tasks", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8758387764294943}, {"text": "reliability", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9762366414070129}]}, {"text": " Table 8: Performance of predicted labels on the RTE  dataset (Krippendorff's alpha = 0.0995).", "labels": [], "entities": [{"text": "RTE  dataset", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9227095544338226}, {"text": "Krippendorff's alpha = 0.0995", "start_pos": 63, "end_pos": 92, "type": "METRIC", "confidence": 0.7845747411251068}]}, {"text": " Table 9: F1 score improvements after removing the la- bel produced by the least reliable annotator by using the  estimated overall reliability (LFC, CUBAM, VI, DS,  EM-MV) and per-instance reliability (Ours).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9533109962940216}, {"text": "per-instance reliability (Ours)", "start_pos": 177, "end_pos": 208, "type": "METRIC", "confidence": 0.7604693114757538}]}, {"text": " Table 10: Number of correct labels and average re- liability for each annotator among the instances with  highest and lowest per-instance reliability on the RTE  dataset.", "labels": [], "entities": [{"text": "Number", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9733462929725647}, {"text": "re- liability", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9492815931638082}, {"text": "RTE  dataset", "start_pos": 158, "end_pos": 170, "type": "DATASET", "confidence": 0.9369793236255646}]}]}