{"title": [{"text": "A Submodular Feature-Aware Framework for Label Subset Selection in Extreme Classification Problems", "labels": [], "entities": [{"text": "Label Subset Selection in Extreme Classification", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.7299767881631851}]}], "abstractContent": [{"text": "Extreme classification is a classification task on an extremely large number of labels (tags).", "labels": [], "entities": [{"text": "Extreme classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.755133330821991}, {"text": "classification task", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8845455944538116}]}, {"text": "User generated labels for any type of online data can be sparing per individual user but intractably large among all users.", "labels": [], "entities": []}, {"text": "It would be useful to automatically select a smaller, standard set of labels to represent the whole label set.", "labels": [], "entities": []}, {"text": "We can then solve efficiently the problem of multi-label learning with an intractably large number of interdependent labels, such as automatic tagging of Wikipedia pages.", "labels": [], "entities": [{"text": "multi-label learning", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7591198086738586}, {"text": "automatic tagging of Wikipedia pages", "start_pos": 133, "end_pos": 169, "type": "TASK", "confidence": 0.7054051876068115}]}, {"text": "We propose a submodular maximization framework with linear cost to find informative labels which are most relevant to other labels yet least redundant with each other.", "labels": [], "entities": []}, {"text": "A simple prediction model can then be trained on this label subset.", "labels": [], "entities": []}, {"text": "Our framework includes both label-label and label-feature dependencies, which aims to find the labels with the most representation and prediction ability.", "labels": [], "entities": []}, {"text": "In addition, to avoid information loss, we extract and predict outlier labels with weak dependency on other labels.", "labels": [], "entities": []}, {"text": "We apply our model to four standard natural language data sets including Bibsonomy entries with users assigned tags, web pages with user assigned tags, legal texts with EUROVOC descriptors(A topic hierarchy with almost 4000 categories regarding different aspects of European law) and Wikipedia pages with tags from social bookmarking as well as news videos for automated label detection from a lexicon of semantic concepts.", "labels": [], "entities": [{"text": "label detection", "start_pos": 371, "end_pos": 386, "type": "TASK", "confidence": 0.7294725477695465}]}, {"text": "Experimental results show that our proposed approach improves label prediction quality, in terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive in the others, even with a simple linear prediction model.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7664161622524261}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9995296001434326}, {"text": "nDCG", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9276140928268433}]}, {"text": "An ablation study shows how different data sets benefit from different aspects of our model, with all aspects contributing substantially to at least one data set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-label learning has recently attracted attention in the research community due to an increase in applications such as semantic labeling of images and videos, bio-informatics, genetic functions, and music categorization.", "labels": [], "entities": [{"text": "Multi-label learning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8555809259414673}, {"text": "semantic labeling of images and videos", "start_pos": 123, "end_pos": 161, "type": "TASK", "confidence": 0.8342853784561157}]}, {"text": "In addition, multilabel learning can address machine learning problems in web data mining, including recommender systems, multimedia sharing websites, and ranking ().", "labels": [], "entities": []}, {"text": "An important application of extreme multi-label learning is automatic tagging and social tagging of large information collections such as Wikipedia or the Web.", "labels": [], "entities": [{"text": "automatic tagging", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.5826647281646729}]}, {"text": "A user can add their own keywords to a text, as if they were the keywords they would use to look for the article in a search engine.", "labels": [], "entities": []}, {"text": "Since tags use an open vocabulary, the number of tags is increasing continually in order to adjust to the needs of new information.", "labels": [], "entities": []}, {"text": "Moreover, different users can assign different tags to the same resource, resulting in a great diversity of tags for that resource.", "labels": [], "entities": []}, {"text": "The biggest challenge of extreme multi-label learning is the dimension of the output space.", "labels": [], "entities": []}, {"text": "As the number of output labels increases, the number of output states increases exponentially.", "labels": [], "entities": []}, {"text": "In order to overcome this exponential growth, it is necessary to use label dependencies to simplify the problem (.", "labels": [], "entities": []}, {"text": "We propose a submodular maximization approach with a linear cost to find an informative set of labels.", "labels": [], "entities": []}, {"text": "In contrast to the other similar approaches () which consider only labellabel dependencies, we also consider label-feature dependencies and outlier labels that are highly independent of other labels.", "labels": [], "entities": []}, {"text": "Solving the problem using the selected (smaller number of) labels leads to minimizing both representation and training error.", "labels": [], "entities": []}, {"text": "Representation ability is equivalent to the power of the selected subset to reconstruct the remaining labels, and prediction ability is equivalent to training accuracy leading to less error propagation from predicted label subset to the remaining labels during reconstruction.", "labels": [], "entities": [{"text": "Representation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9063888788223267}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.90778648853302}]}, {"text": "Submodular maximization approaches have proved very effective in many applications, such as finding the most influential nodes in social networks to maximize the spread of information (for applications such as advertising and marketing () and video and image collection summarization ().", "labels": [], "entities": [{"text": "Submodular maximization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7896513640880585}, {"text": "video and image collection summarization", "start_pos": 243, "end_pos": 283, "type": "TASK", "confidence": 0.7402847468852997}]}, {"text": "There are many effective algorithms such as) to make submodular optimization approaches much faster or do them in a distributed way to perform faster parallel processing for very large scale datasets.", "labels": [], "entities": [{"text": "submodular optimization", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7603917121887207}]}], "datasetContent": [{"text": "We used six different datasets in the experiments.", "labels": [], "entities": []}, {"text": "The \"Bibtex\" dataset is a text dataset extracted from the BibSonomy website ( Algorithm 3 Prediction Algorithm. Input: prediction samples X.", "labels": [], "entities": [{"text": "Bibtex\" dataset", "start_pos": 5, "end_pos": 20, "type": "DATASET", "confidence": 0.8434128761291504}]}, {"text": "1: Predict candidate label subset and outlier labels using regression model 10. 2: Use 11 to produce full set of labels from candidate subset and outlier labels.", "labels": [], "entities": []}, {"text": "Output: Full label set for input X. contains metadata for the bibtex items like the title of the paper, the authors, etc and extracts the features according to the term frequency.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9700095057487488}]}, {"text": "The \"Mediamill\" dataset is extracted from the Mediamill contest datasets, which include low-level multimedia features (visual and textual features) extracted from 85 hours of international news videos from the TRECVID 2005/2006 benchmark datasets () labeled using a lexicon of 101 semantic concepts, like commercials, nature, and baseball.", "labels": [], "entities": [{"text": "Mediamill\" dataset", "start_pos": 5, "end_pos": 23, "type": "DATASET", "confidence": 0.8533211946487427}, {"text": "Mediamill contest datasets", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.9696137110392252}, {"text": "TRECVID 2005/2006 benchmark datasets", "start_pos": 210, "end_pos": 246, "type": "DATASET", "confidence": 0.9452218612035116}]}, {"text": "The \"Eurlex\" dataset includes 19,348 legal documents from European nations, containing several different types of documents, including treaties, legislation, case-law and legislative proposals, classified according to the EUROVOC descriptor using 3993 different classes, and 5000 features extracted using common TF-IDF term weighting (.", "labels": [], "entities": [{"text": "Eurlex\" dataset", "start_pos": 5, "end_pos": 20, "type": "DATASET", "confidence": 0.7841121951738993}]}, {"text": "The \"Delicious\" dataset is a text dataset extracted from the del.icio.us social bookmarking site on the 1st of April 2007 and contains textual data of web pages along with their user defined tags (.", "labels": [], "entities": [{"text": "Delicious\" dataset is a text dataset extracted from the del.icio.us social bookmarking site on the 1st of April 2007", "start_pos": 5, "end_pos": 121, "type": "DATASET", "confidence": 0.7523581355810165}]}, {"text": "The content of web pages was represented using the Boolean bag-ofwords model.", "labels": [], "entities": [{"text": "Boolean", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9302237629890442}]}, {"text": "\"Wiki10-31K\" is a collection of social tags forgiven Wikipedia pages with TF-IDF features.", "labels": [], "entities": []}, {"text": "The statistics of these datasets are provided in.", "labels": [], "entities": []}, {"text": "For the small datasets, \"Bibtex\", \"Mediamill\", \"Delicious\", and \"Eurlex\", the reported results are the average of 10 different experiments for random partitions of each dataset.", "labels": [], "entities": []}, {"text": "For the larger dataset, \"Wiki10-31K\", we did one experiment with the training and testing partition reported in.", "labels": [], "entities": [{"text": "Wiki10-31K", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9551228284835815}]}, {"text": "For all experiments we chose a label subset size of 100, except for Mediamill where we chose 30 since 100 would represent all labels.", "labels": [], "entities": [{"text": "Mediamill", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.9791354537010193}]}, {"text": "Model tuning is done in two phases: first we tune \u03b1 for group sparsity (Equation 8), and \u03b3 for weighting of the submodular functions (Equation 6), then we tune for \u03bb 1 and \u03bb 2 , the regression parameters for mapping back to the original label set (Equation 9) with \u03b1 and \u03b3 fixed.", "labels": [], "entities": []}, {"text": "All parameters were chosen by measuring the precision of 10-fold cross validation and using a grid search over the values {0, 10 \u22123,...,+3 } for each dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9993795156478882}]}, {"text": "The proposed method was compared with several state-of-the-art methods with diverse approaches.", "labels": [], "entities": []}, {"text": "LEML (), CPLST (Chen and Lin, 2012), CS () and SLEEC () which are embedding based approaches with a low-rank or sparse assumption in the label space.", "labels": [], "entities": [{"text": "SLEEC", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.8806608319282532}]}, {"text": "ML-CSSP ( which solves the problem in the original label space which ignores the training error in the subset selection step.", "labels": [], "entities": [{"text": "ML-CSSP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8266093730926514}]}, {"text": "FastXML (, and PD-sparse () which do not use an embedding transformation and aim to solve the problem without using compression or sampling.", "labels": [], "entities": [{"text": "FastXML", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9003273248672485}]}, {"text": "We have used the reported results, if available, and otherwise tuned the parameters for the baseline algorithms by means of 10-fold cross validation.", "labels": [], "entities": []}, {"text": "shows the average and standard deviation of Precision@k for the four small-scale datasets, \"Bibtex\", \"Mediamill\", \"Delicious\", and \"Eurlex\", and the large-scale dataset \"Wiki10-31k\".", "labels": [], "entities": [{"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8767830729484558}, {"text": "Wiki10-31k", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.7915185689926147}]}, {"text": "For \"Wiki10-31k\", results are reported only for those baselines that were tractable.", "labels": [], "entities": []}, {"text": "The results for nDCG@k are included in supplementary Material,.", "labels": [], "entities": []}, {"text": "Since the SLEEC and FastXML methods are ensemble-based, using multiple nonlinear models, it is not fair to compare them with the single model methods such as our own.", "labels": [], "entities": []}, {"text": "These methods partition the sample space into smaller tractable clusters and obtain separate classifiers for each partition.", "labels": [], "entities": []}, {"text": "We compare our method with these in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Ensemble-based nonlinear models. Best in  bold and not significantly different to best in italics.", "labels": [], "entities": []}, {"text": " Table 4: Ablation Study. Bold indicates a difference of  \u2265 0.8%", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9928552508354187}]}]}