{"title": [{"text": "Density Matching for Bilingual Word Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages.", "labels": [], "entities": [{"text": "cross-lingual word embedding", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6118002037207285}]}, {"text": "In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow.", "labels": [], "entities": []}, {"text": "The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings.", "labels": [], "entities": []}, {"text": "We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robust-ness and generalization to mappings between difficult language pairs or word pairs.", "labels": [], "entities": []}, {"text": "On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in away conducive to use in computational models ().", "labels": [], "entities": []}, {"text": "These embeddings have been shown to bean effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones or unsupervised learning).", "labels": [], "entities": []}, {"text": "There are two major paradigms in the learning of cross-lingual word embeddings: \"online\" and \"offline\".", "labels": [], "entities": []}, {"text": "\"Online\" methods learn the crosslingual embeddings directly from parallel corpora (), optionally augmented with monolingual corpora (.", "labels": [], "entities": []}, {"text": "In contrast, \"offline\" approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (.", "labels": [], "entities": []}, {"text": "In this work, we focus on this latter offline approach.", "labels": [], "entities": []}, {"text": "The goal of bilingual embedding is to learn a shared embedding space where words possessing similar meanings are projected to nearby points.", "labels": [], "entities": []}, {"text": "Early work focused on supervised methods maximizes the similarity of the embeddings of words that exist in a manually-created dictionary, according to some similarity metric (.", "labels": [], "entities": []}, {"text": "In contrast, recently proposed unsupervised methods frame this problem as minimization of some form of distance between the whole set of discrete word vectors in the chosen vocabulary, e.g. Wasserstein distance or Jensen-Shannon divergence (.", "labels": [], "entities": []}, {"text": "While these methods have shown impressive results for some language pairs despite the lack of supervision, regarding the embedding space as a set of discrete points has some limitations.", "labels": [], "entities": []}, {"text": "First, expressing embeddings as a single point in the space doesn't take into account the inherent uncertainty involved in learning embeddings, which can cause embedding spaces to differ significantly between training runs.", "labels": [], "entities": []}, {"text": "Second, even in a fixed embedding space the points surrounding those of words that actually exist in the pre-trained vocabulary also often are coherent points in the embedding space.", "labels": [], "entities": []}, {"text": "In this work, we propose a method for density matching for bilingual word embedding (DeMa-BWE).", "labels": [], "entities": []}, {"text": "Instead of treating the embedding space as a collection of discrete points, we express it as a probability density function over the entire continuous space over word vectors.", "labels": [], "entities": []}, {"text": "We assume each vector in the monolingual embedding space is generated from a Gaussian mixture model with components centered at the pretrained word embeddings (, and our approach then learns a bilingual mapping that most effectively matches the two probability densities of the two monolingual embedding spaces.", "labels": [], "entities": []}, {"text": "To learn in this paradigm, instead of using the pre-trained word embeddings as fixed training samples, at every training step we obtain samples from the Gaussian mixture space.", "labels": [], "entities": []}, {"text": "Thus, our method is exploring the entire embedding space instead of only the specific points assigned for observed words.", "labels": [], "entities": []}, {"text": "To calculate the density of the transformed samples, we use volume-preserving invertible transformations over the target word embeddings, which make it possible to perform density matching in a principled and efficient way ().", "labels": [], "entities": []}, {"text": "We also have three additional ingredients in the model that proved useful in stabilizing training: (1) a back-translation loss to allow the model to learn the mapping jointly in both directions, (2) an identical-word-matching loss that provides weak supervision by encouraging the model to have words with identical spellings be mapped to a similar place in the space, and (3) frequency-matching based Gaussian mixture weights that accounts for the approximate frequencies of aligned words.", "labels": [], "entities": []}, {"text": "Empirical results are strong; our method is able to effectively learn bilingual embeddings that achieve competitive or superior results on the MUSE dataset () over state-of-the-art published results on bilingual word translation and cross-lingual word similarity tasks.", "labels": [], "entities": [{"text": "MUSE dataset", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9512169063091278}, {"text": "bilingual word translation", "start_pos": 202, "end_pos": 228, "type": "TASK", "confidence": 0.6514335870742798}, {"text": "cross-lingual word similarity tasks", "start_pos": 233, "end_pos": 268, "type": "TASK", "confidence": 0.6570900306105614}]}, {"text": "The results are particularly encouraging on etymologically distant or morphologically rich languages, as our model is able to explore the integration over the embedding space by treating the space as a continuous one.", "labels": [], "entities": []}, {"text": "Moreover, unlike previous unsupervised methods that are usually sensitive to initialization or require sophisticated optimization procedures, our method is robust and requires no special initialization.", "labels": [], "entities": [{"text": "initialization", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9585820436477661}]}], "datasetContent": [{"text": "We evaluate our approach extensively on the bilingual lexicon induction (BLI) task, which measures the word translation accuracy in comparison to a gold standard.", "labels": [], "entities": [{"text": "bilingual lexicon induction (BLI) task", "start_pos": 44, "end_pos": 82, "type": "METRIC", "confidence": 0.5973184789930072}, {"text": "word translation", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.6928427070379257}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.7591731548309326}]}, {"text": "We report results on the widely used MUSE dataset (, which consists of FastText monolingual embeddings pretrained on Wikipedia (, and dictionaries for many language pairs divided into train and test sets.", "labels": [], "entities": [{"text": "MUSE dataset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8793954849243164}]}, {"text": "We follow the evaluation setups of (.", "labels": [], "entities": []}, {"text": "We evaluate DeMa-BWE by inducing lexicons between English and different languages including related languages, e.g. Spanish; etymologically distant languages, e.g. Japanese; and morphologically rich languages, e.g. Finnish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision@1 for the MUSE BLI task compared with previous work. All the baseline results employ CSLS  as the retrieval metric except for Sinkhorn  *  which uses cosine similarity. R represents refinement. Bold and italic  indicate the best unsupervised and overall numbers respectively. ('en' is English, 'es' is Spanish, 'de' is German,  'fr' is French, 'ru' is Russian, 'zh' is traditional Chinese, 'ja' is Japanese.)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.96601402759552}, {"text": "MUSE BLI task", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7025421559810638}]}, {"text": " Table 2: BLI Precision (@1) for morphologically complex languages. id+Procrustes (R)  *  is the result reported  in (S\u00f8gaard et al., 2018). 5k+Procrustes (R) uses the training dictionary with 5k unique query words. ('et' is  Estonian, 'fi' is Finnish, 'el' is Greek, 'hu' is Hungarian, 'pl' is Persian, 'tr' is Turkish.)", "labels": [], "entities": [{"text": "BLI Precision", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7804030179977417}]}, {"text": " Table 3: Pearson rank correlation (\u00d7100) on cross- lingual word similarity task. Bold indicates the best  unsupervised numbers.", "labels": [], "entities": [{"text": "Pearson rank correlation", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.9284754792849222}, {"text": "cross- lingual word similarity task", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.6917528311411539}]}, {"text": " Table 4: Ablation study on different components of  DeMa-BME.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9363782405853271}, {"text": "DeMa-BME", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9529406428337097}]}]}