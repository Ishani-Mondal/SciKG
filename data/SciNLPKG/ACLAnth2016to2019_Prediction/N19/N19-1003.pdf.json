{"title": [], "abstractContent": [{"text": "Self-training is a semi-supervised learning approach for utilizing unlabeled data to create better learners.", "labels": [], "entities": []}, {"text": "The efficacy of self-training algorithms depends on their data sampling techniques.", "labels": [], "entities": []}, {"text": "The majority of current sampling techniques are based on predetermined policies which may not effectively explore the data space or improve model generalizability.", "labels": [], "entities": []}, {"text": "In this work, we tackle the above challenges by introducing anew data sampling technique based on spaced repetition that dynamically samples informative and diverse unlabeled instances with respect to individual learner and instance characteristics.", "labels": [], "entities": []}, {"text": "The proposed model is specifically effective in the context of neu-ral models which can suffer from overfitting and high-variance gradients when trained with small amount of labeled data.", "labels": [], "entities": []}, {"text": "Our model outperforms current semi-supervised learning approaches developed for neural networks on publicly-available datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is often expensive or time-consuming to obtain labeled data for Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "In addition, manually-labeled datasets may not contain enough samples for downstream data analysis or novelty detection (.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.7869089245796204}]}, {"text": "To tackle these issues, semi-supervised learning) has become an important topic when one has access to small amount of labeled data and large amount of unlabeled data.", "labels": [], "entities": []}, {"text": "Self-training is a type of semi-supervised learning in which a downstream learner (e.g. a classifier) is first trained with labeled data, then the trained model is applied to unlabeled data to generate more labeled instances.", "labels": [], "entities": []}, {"text": "A select sample of these instances together with their pseudo (predicted) labels are added to the labeled data and the learner is re-trained using the new labeled dataset.", "labels": [], "entities": []}, {"text": "This process repeats until there is no more unlabeled data left or no improvement is observed in model performance on validation data (;.", "labels": [], "entities": []}, {"text": "Conventional self-training methods often rely on prediction confidence of their learners to sample unlabeled data.", "labels": [], "entities": []}, {"text": "Typically the most confident unlabeled instances are selected.", "labels": [], "entities": []}, {"text": "This strategy often causes only those unlabeled instances that match well with the current model being selected during self-training, therefore, the model may fail to best generalize to complete sample space (;.", "labels": [], "entities": []}, {"text": "Ideally, a self-training algorithm should explore the space thoroughly for better generalization and higher performance.", "labels": [], "entities": []}, {"text": "Recently developed an effective data sampling technique for \"co-training\") methods which require two distinct views of data.", "labels": [], "entities": []}, {"text": "Although effective, this model can't be readily applied to some text datasets due to the two distinct view requirement.", "labels": [], "entities": []}, {"text": "In the context of neural networks, pretraining is an effective semi-supervised approach in which layers of a network are first pretrained by learning to reconstruct their inputs, and then network parameters are optimized by supervised fine-tuning on a target task (.", "labels": [], "entities": []}, {"text": "While pretraining has been effective in neural language modeling and document classification, it has an inherent limitation: the same neural model or parts thereof must be used in both pretraining and fine-tuning steps.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6379881898562113}, {"text": "document classification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7735459506511688}]}, {"text": "This poses a major limitation on the design choices as some pretraining tasks may need to exploit several data types (e.g., speech and text), or might require deeper network architectures.", "labels": [], "entities": []}, {"text": "The above challenges and intuitions inspire our work on developing a novel approach for neural self-training.", "labels": [], "entities": []}, {"text": "The core part of our approach is a data sampling policy which is inspired by findings in cognitive psychology about spaced repetition; the phenomenon in which a learner (often a human) can learn efficiently and effectively by accurately scheduling reviews of learning materials.", "labels": [], "entities": []}, {"text": "In contrast to previous self-training approaches, our spaced repetitionbased data sampling policy is not predetermined, explores the entire data space, and dynamically selects unlabeled instances with respect to the \"strength\" of a downstream learner on a target task, and \"easiness\" of unlabeled instances.", "labels": [], "entities": []}, {"text": "In addition, our model relaxes the \"same model\" constraint of pretraining-based approaches by naturally decoupling pretraining and fine-tuning models through spaced repetition.", "labels": [], "entities": []}, {"text": "The contributions of this paper are (a): we propose an effective formulation of spaced repetition for self-training methods; to the best of our knowledge, this is the first work that investigates spaced repetition for semi-supervised learning, (b): our approach dynamically samples data, is not limited to predetermined sampling strategies, and naturally decouples pretraining and fine-tuning models, and (c): it outperforms current state-of-the-art baselines on large-scale datasets.", "labels": [], "entities": []}, {"text": "Our best model outperforms standard and current state-of-the-art semi-supervised learning methods by 6.5 and 4.1 points improvement in macro-F1 on sentiment classification task, and 3.6 and 2.2 points on churn classification task.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 147, "end_pos": 176, "type": "TASK", "confidence": 0.8843574126561483}, {"text": "churn classification task", "start_pos": 204, "end_pos": 229, "type": "TASK", "confidence": 0.8521109819412231}]}, {"text": "Further analyses show that the performance gain is due to our model's ability in sampling diverse and informative unlabeled instances (those that are different from training data and can improve model generalizability).", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare different self-training approaches in two settings where learners (neural networks) have low or high performance on original labeled data.", "labels": [], "entities": []}, {"text": "This consideration helps investigating sensitivity of different self-training algorithms to the initial performance of learners.", "labels": [], "entities": []}, {"text": "As datasets, we use movie reviews from IMDb and short microblog posts from Twitter.", "labels": [], "entities": []}, {"text": "These datasets and their corresponding tasks are described below and their statistics are provided in.", "labels": [], "entities": []}, {"text": "In terms of preprocessing, we change all texts to lowercase, and remove stop words, user names, and URLs from texts in these datasets: IMDb: The IMDb dataset was developed by for sentiment classification where systems should classify the polarity of a given movie review as positive or negative.", "labels": [], "entities": [{"text": "IMDb dataset", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.8615873157978058}, {"text": "sentiment classification", "start_pos": 179, "end_pos": 203, "type": "TASK", "confidence": 0.9416882395744324}]}, {"text": "The dataset contains 50K labeled movie reviews.", "labels": [], "entities": []}, {"text": "For the purpose of our experiments, we randomly sample 1K, 1K, and 48K instances from this data (with balanced distribution over classes) and treat them as labeled (training), validation, and test data respectively.", "labels": [], "entities": []}, {"text": "We create five such datasets for robustness against different seeding or data partitions.", "labels": [], "entities": []}, {"text": "This dataset also provides 50K unlabeled reviews.", "labels": [], "entities": []}, {"text": "Churn: This dataset contains more than 5K tweets about three telecommunication brands and was developed by Amiri and Daum\u00e9 III (2015) 2 for the task of churn prediction 3 where systems should predict if atwitter post indicates user intention about leaving a brand -classifying tweets as churny or non-churny with respect to brands.", "labels": [], "entities": [{"text": "churn prediction", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7985607087612152}]}, {"text": "We replace all target brand names with the keyword BRAND and other non-target brands with BRAND-OTHER for the purpose of our experiments.", "labels": [], "entities": [{"text": "BRAND", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9462761282920837}, {"text": "BRAND-OTHER", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9876083731651306}]}, {"text": "Similar to IMDb, we create five datasets for experiments.", "labels": [], "entities": []}, {"text": "We also crawl an additional 100K tweets about the target brands and treat them as unlabeled data.", "labels": [], "entities": []}, {"text": "We evaluate models in terms of macro-F1 score, i.e. the mean of F1-scores across classes.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9735231995582581}]}], "tableCaptions": [{"text": " Table 4: Macro-F1 performance of models across  datasets; Note that Standard ST (SST) samples only  1.4K and 0 instances from IMDb and Churn datasets  respectively; sampling more data decreases SST's per- formance down to 66.94 and 57.04 perhaps due to in- effective exploring of data space. Our model achieves  its best performance on IMDb and Churn datasets with  n = 5 and n = 7 Leitner queues respectively.", "labels": [], "entities": []}, {"text": " Table 5: Macro-F1 performance of diverse queues  across datasets. Compare these results with those ob- tained by designated queues in", "labels": [], "entities": []}]}