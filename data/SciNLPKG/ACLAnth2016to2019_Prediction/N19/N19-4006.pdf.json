{"title": [{"text": "Train, Sort, Explain: Learning to Diagnose Translation Models", "labels": [], "entities": [{"text": "Learning to Diagnose Translation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7299595326185226}]}], "abstractContent": [{"text": "Evaluating translation models is a trade-off between effort and detail.", "labels": [], "entities": [{"text": "Evaluating translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6597169637680054}, {"text": "detail", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9552679061889648}]}, {"text": "On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9978919625282288}]}, {"text": "To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts.", "labels": [], "entities": []}, {"text": "Inspired by adver-sarial settings, we train a neural text classi-fier to distinguish human from machine translations.", "labels": [], "entities": [{"text": "distinguish human from machine translations", "start_pos": 73, "end_pos": 116, "type": "TASK", "confidence": 0.6445307493209839}]}, {"text": "A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods.", "labels": [], "entities": []}, {"text": "Our proof-of-concept implementation , DiaMaT, is open source.", "labels": [], "entities": []}, {"text": "Applied to a dataset translated by a state-of-the-art neu-ral Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8975318074226379}]}], "introductionContent": [{"text": "A multi-dimensional diagnostic evaluation of performance or quality often turns out to be more helpful for system improvement than just considering a one-dimensional utilitarian metric, such as BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.996931791305542}]}, {"text": "This is exemplified by, for instance, the pioneering work of.", "labels": [], "entities": []}, {"text": "The authors introduced the attention mechanism responding to the findings of who reported that neural translation quality degraded with sentence length.", "labels": [], "entities": []}, {"text": "The attention mechanism was later picked up by for their attention-only Transformer model, which still is state of the art in machine translation (MT) (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.8407171726226806}]}, {"text": "Furthermore, while MT output approaches human translation quality and the claims for \"human parity\" () increase, multi-dimensional diagnostic evaluations can be useful to spot the thin line between the machine and the human.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9879872798919678}]}, {"text": "Diagnostic (linguistic) evaluations require human-expert feedback, which, however, is very time-consuming to collect.", "labels": [], "entities": [{"text": "Diagnostic (linguistic) evaluations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7501699805259705}]}, {"text": "For this reason, there is a need for tools that mitigate the effort, such as the ones developed by; Popovi\u00b4cPopovi\u00b4c (2011);;.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel approach for developing evaluation tools.", "labels": [], "entities": []}, {"text": "Contrary to the above tools that employ string comparison methods such as BLEU, implementations of the new approach derive annotations based on a neural model of explainability.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.959487795829773}]}, {"text": "This allows both capturing of semantics as well as focusing on the particular tendencies of MT errors.", "labels": [], "entities": [{"text": "capturing of semantics", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8489579359690348}, {"text": "MT errors", "start_pos": 92, "end_pos": 101, "type": "TASK", "confidence": 0.8936806619167328}]}, {"text": "Using neural methods for the evaluation and juxtaposition of translations has already been done by.", "labels": [], "entities": []}, {"text": "Their method, however, can only be applied to attentionbased models and their translations.", "labels": [], "entities": []}, {"text": "In contrast, our approach generalizes to arbitrary machine and even human translations.", "labels": [], "entities": []}, {"text": "After first discussing the abstract approach in the next section, we present a concrete open-source implementation, \"DiaMaT\" (from Diagnose Machine Translations).", "labels": [], "entities": [{"text": "Diagnose Machine Translations)", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6946789920330048}]}], "datasetContent": [{"text": "We tested DiaMaT on a corpus translated by an NMT Transformer engine ( conforming to the WMT14 data setup).", "labels": [], "entities": [{"text": "WMT14 data setup", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.9678670366605123}]}, {"text": "The NMT model was optimized on the testset of WMT13 and an ensemble of 5 best models was used.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9640797972679138}]}, {"text": "It was trained using, including Byte Pair Encoding (Sennrich et al., 2015) but no back-translation, achieving 32.68 BLEU on the test-set of WMT14.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9973324537277222}, {"text": "WMT14", "start_pos": 140, "end_pos": 145, "type": "DATASET", "confidence": 0.9587249159812927}]}, {"text": "Next, we trained the CNN text classifier sketched in for which we randomly drew 1M training samples (human references and machine translations alongside their sources) from the WMT18 training data (, excluding the WMT14 training data.", "labels": [], "entities": [{"text": "WMT18 training data", "start_pos": 177, "end_pos": 196, "type": "DATASET", "confidence": 0.9460643927256266}, {"text": "WMT14 training data", "start_pos": 214, "end_pos": 233, "type": "DATASET", "confidence": 0.9457889596621195}]}, {"text": "The validation set consisted of 100k randomly drawn samples from the same set and we drew another 100k samples randomly for training the explainability method of choice, PatternAttribution, which learns explanations from data.", "labels": [], "entities": []}, {"text": "All texts were embedded using pre-trained fastText word vectors (.", "labels": [], "entities": []}, {"text": "We evaluated the classifier on around 20k samples drawn from the official test sets, excluding WMT13.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9793692231178284}]}, {"text": "On this test set, the classifier achieved an accuracy of 75%, which is remarkable, considering the ongoing discussion about human parity ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9996254444122314}]}, {"text": "We also used this test set for steps 2 and 3.", "labels": [], "entities": []}, {"text": "Thus, neither the translation model, nor the text classifier, nor the explainability method encountered this split during training.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9655390381813049}]}, {"text": "For step 2, the machine translation was always passed to the right input layer and contributions to the right output neuron were retrieved with PatternAttribution.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6998466402292252}]}, {"text": "We then sorted the inputs by the softmax activation of the machine neuron, which moved inputs for which the classifier is certain that it has identified the machine correctly to the top.", "labels": [], "entities": []}], "tableCaptions": []}