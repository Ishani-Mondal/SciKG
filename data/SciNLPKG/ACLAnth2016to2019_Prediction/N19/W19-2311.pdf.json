{"title": [{"text": "Jointly Measuring Diversity and Quality in Text Generation Models", "labels": [], "entities": [{"text": "Text Generation", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7588223218917847}]}], "abstractContent": [{"text": "Text generation is an important Natural Language Processing task with various applications.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8012312948703766}]}, {"text": "Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings.", "labels": [], "entities": [{"text": "text generation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7834056913852692}]}, {"text": "The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9942604303359985}]}, {"text": "For example, repeatedly generation of only one high quality sentence would result in a high BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9847224950790405}]}, {"text": "On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts.", "labels": [], "entities": []}, {"text": "In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned gen-erative model and the real data distribution.", "labels": [], "entities": []}, {"text": "For this purpose, we first introduce a metric that approximates this distance using n-gram based measures.", "labels": [], "entities": []}, {"text": "Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced.", "labels": [], "entities": [{"text": "BERT", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9783270955085754}]}, {"text": "Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions.", "labels": [], "entities": []}, {"text": "Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generative models and especially Generative Adversarial Networks (GANs) have been received much attention in the last few years.", "labels": [], "entities": []}, {"text": "However, the evaluation of generated samples by these models is challenging.", "labels": [], "entities": []}, {"text": "Although some studies have recently focused on introducing measures like Inception * These authors contributed equally to this work.", "labels": [], "entities": [{"text": "Inception", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.7734171748161316}]}, {"text": "Score and Fr\u00e9chet Inception Distance (FID) to compare results of different GAN models for image generation, there is not a study to propose proper metrics for evaluation of text generation models.", "labels": [], "entities": [{"text": "Score", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9752305150032043}, {"text": "Fr\u00e9chet Inception Distance (FID)", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.9548583825429281}, {"text": "image generation", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7341283857822418}]}, {"text": "In the last few years, many GAN-based text generation models () have been proposed.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.8845806121826172}]}, {"text": "However, measuring the performance of these models in the corresponding papers is not comprehensive.", "labels": [], "entities": []}, {"text": "GANs suffer from the mode collapse problem () and the GAN-based text generation models may just produce a highly limited set of sentences and therefore just considering the quality of these generated sentences for comparison is not comprehensive.", "labels": [], "entities": [{"text": "GANs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8282228708267212}, {"text": "GAN-based text generation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7021934986114502}]}, {"text": "On the other hand, there are measures like Self-BLEU ( ) for evaluating the diversity of generated sentences, but they cannot consider the quality of samples at all.", "labels": [], "entities": []}, {"text": "Besides, designing an experiment of evaluating diversity by humans is not straightforward and thus it's necessary to have a jointly quality-diversity measuring metric.", "labels": [], "entities": []}, {"text": "In this paper, we intend to propose metrics sensitive to both quality and diversity simultaneously, assigning low scores not only to models generating low-quality samples but also to the ones with low-diversity samples (including the mode collapsed models).", "labels": [], "entities": []}, {"text": "To this end, we first propose the MS-Jaccard as an n-gram based measure that considers the quality and diversity of generated samples simultaneously.", "labels": [], "entities": []}, {"text": "It attempts to find the similarity of the set of generated samples by a model and the set of real (or test) samples.", "labels": [], "entities": []}, {"text": "Then, a featurebased measure is proposed to compare the real data distribution and the generative model distribution in the feature space.", "labels": [], "entities": []}, {"text": "Indeed, by borrowing the idea of FID () that is a popular feature-based evaluation metric in im-age generation tasks and advent of a recent highly deep model named BERT as a reference feature extractor for natural language texts, a metric is proposed for evaluation of natural language generation.", "labels": [], "entities": [{"text": "FID", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9925486445426941}, {"text": "BERT", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9829766750335693}, {"text": "natural language generation", "start_pos": 269, "end_pos": 296, "type": "TASK", "confidence": 0.6399403512477875}]}, {"text": "Finally, appropriate divergences between the oracle distribution and the (learned) model distribution is introduced for when the probabilistic oracle is considered as synthetic data distribution (and thus the target distribution is available for evaluation).", "labels": [], "entities": []}], "datasetContent": [{"text": "In Oracle-NLL evaluation introduced in (, the measured distance is Kullback-Leibler (KL) divergence of the generative model and the oracle which ignores the variety of generated sentences.", "labels": [], "entities": []}, {"text": "On the other hand, the inverse KL (that is relevant to the likelihood of real data in the text generation model) cannot guarantee the quality of generated samples by the model.", "labels": [], "entities": []}, {"text": "We propose measuring the distance of the probabilistic oracle distribution P (that generates real data) and the probabilistic generative model Q by asymmetric distance as an evaluation metric.", "labels": [], "entities": []}, {"text": "A wide range of distances can be utilized for this purpose.", "labels": [], "entities": []}, {"text": "One symmetric distance is Bhattacharyya that can be estimated by the Monte-Carlo as below:  In this section, we first conduct some experiments to evaluate text generation models using the existing and the proposed measures.", "labels": [], "entities": [{"text": "Bhattacharyya", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9778807163238525}, {"text": "text generation", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.7735438346862793}]}, {"text": "Then, we discuss about the appropriateness of the proposed metrics.", "labels": [], "entities": []}, {"text": "We evaluate the models on COCO image captions (), EMNLP2017 WMT News (, and IMDB (Maas et al., 2011) as the popular datasets for text generation.", "labels": [], "entities": [{"text": "COCO image captions", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7025377750396729}, {"text": "EMNLP2017 WMT News", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.7896526356538137}, {"text": "IMDB", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7154303193092346}, {"text": "text generation", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7901221811771393}]}, {"text": "In addition to these datasets, similar to (, we also consider a synthetic oracle produced by a probabilistic text generator that is a random initialized LSTM as a synthetic dataset.", "labels": [], "entities": []}, {"text": "The description of the datasets is as follows: \u2022 COCO Captions (): It is a collection of image captions containing around 600,000 captions.", "labels": [], "entities": [{"text": "COCO Captions", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.6447348594665527}]}, {"text": "Sentences having between 5 and 25 words are selected (resulting in 524,225 sentences) where 5,328 is the vocab size of the resulted dataset.", "labels": [], "entities": []}, {"text": "Among the resulted dataset, 40,000 samples are used for training, 20,000 samples for validation, and 20,000 for test.", "labels": [], "entities": []}, {"text": "\u2022 EMNLP2017 WMT News (: It is a collection of news texts for the machine translations task . Among aversion of this dataset for English corpus containing 500,000 sentences, sentences having more than 3 words with less than 150 frequency (these words are replaced with UNK) were dropped and sentences that have between 20 and 40 words selected.", "labels": [], "entities": [{"text": "EMNLP2017 WMT News", "start_pos": 2, "end_pos": 20, "type": "DATASET", "confidence": 0.7919496695200602}, {"text": "machine translations", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7725611627101898}]}, {"text": "The vocab size of the resulted dataset is 6,148.", "labels": [], "entities": []}, {"text": "Among this dataset, 40,000 samples are used for training, 20,000 samples for validation, and 20,000 for test.", "labels": [], "entities": []}, {"text": "\u2022 IMDB Movie Reviews (Maas et al., 2011): It is a collection of IMDB movie reviews for the sentiment analysis task, containing 25,000 labeled and 50,000 unlabeled ones.", "labels": [], "entities": [{"text": "IMDB Movie Reviews (Maas et al., 2011)", "start_pos": 2, "end_pos": 40, "type": "DATASET", "confidence": 0.8011997103691101}, {"text": "sentiment analysis task", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.95319002866745}]}, {"text": "We have selected the first two sentences of each review and replace words with less that 50 times frequency with UNK and keep sentences from length 5 to 40 with less than 5 UNKs.", "labels": [], "entities": []}, {"text": "The final dataset is subsampled to have 20,000 sentences for training data, 10,000 for validation, and 10,000 for test data leading to vocab size of 5,810.", "labels": [], "entities": []}, {"text": "\u2022 The models were trained on the similar dataset existing in their released code but collected from the original sites reported in corresponding reference papers.", "labels": [], "entities": []}, {"text": "In order to have a fair comparison, all settings of the models (e.g., same hidden) were kept the same as the Texygen framework.", "labels": [], "entities": [{"text": "Texygen framework", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.9675839841365814}]}, {"text": "Since setting a fixed number of epochs for terminating training of different methods does not seem such reasonable and resulting in unfair scores, we targeted multiple training termination criteria.", "labels": [], "entities": []}, {"text": "In the realworld datasets training, the training termination of the GANs were based on obtaining the best BLEU4 on validation data in addition to setting a max number of iterations for all the models.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9988691210746765}]}, {"text": "Besides, the training termination of MLE is based the NLL on the validation data while also setting a max number of iterations as above.", "labels": [], "entities": [{"text": "MLE", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7583880424499512}]}, {"text": "In the oracle training mode, the termination were done based on both Oracle-NLL on the validation set and again on a max number of iterations for all models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of models (using different measures) on COCO Captions dataset. MSJ, BL, and SBL denote  MS-Jaccard, BLEU, and Self-BLEU respectively.", "labels": [], "entities": [{"text": "COCO Captions dataset", "start_pos": 62, "end_pos": 83, "type": "DATASET", "confidence": 0.8343158960342407}, {"text": "BL", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9905205965042114}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9967527985572815}]}, {"text": " Table 2: Performance of models (using different measures) on EMNLP2017 WMT News dataset. MSJ, BL, and  SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.", "labels": [], "entities": [{"text": "EMNLP2017 WMT News dataset", "start_pos": 62, "end_pos": 88, "type": "DATASET", "confidence": 0.8729504197835922}, {"text": "BL", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9833533763885498}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9962918758392334}]}, {"text": " Table 3: Performance of models (using different measures) on IMDB Movie Reviews dataset. MSJ, BL, and SBL  denote MS-Jaccard, BLEU, and Self-BLEU respectively.", "labels": [], "entities": [{"text": "IMDB Movie Reviews dataset", "start_pos": 62, "end_pos": 88, "type": "DATASET", "confidence": 0.9588239639997482}, {"text": "BL", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9912651181221008}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9972112774848938}]}, {"text": " Table 4: Performance of models (using different mea- sures) on Oracle dataset.", "labels": [], "entities": [{"text": "Oracle dataset", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.8417519927024841}]}]}