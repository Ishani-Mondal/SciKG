{"title": [{"text": "A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction", "labels": [], "entities": [{"text": "Shortest", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9626865386962891}, {"text": "Relation Extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.939972847700119}]}], "abstractContent": [{"text": "To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence.", "labels": [], "entities": []}, {"text": "Each approach suffers from its own disadvantage of either missing or redundant information.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel model that combines the advantages of these two approaches.", "labels": [], "entities": []}, {"text": "This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP).", "labels": [], "entities": []}, {"text": "To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP.", "labels": [], "entities": []}, {"text": "Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8207572996616364}]}, {"text": "The data and source code are available at https: //github.com/catcd/RbSP.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most fundamental tasks in natural language processing, as well as in information extraction, is Relation Extraction (RE), i.e., determining the semantic relation between pairs of named entities or nominals in a sentence or a paragraph.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6835703651110331}, {"text": "information extraction", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8100214898586273}, {"text": "Relation Extraction (RE)", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8416790127754211}, {"text": "determining the semantic relation between pairs of named entities or nominals in a sentence or a paragraph", "start_pos": 139, "end_pos": 245, "type": "TASK", "confidence": 0.7926884924664217}]}, {"text": "Take the following sentences from the SemEval-2010 task 8 dataset () as examples: (i) We put the soured e1 in the butter e2 and started stirring it.", "labels": [], "entities": [{"text": "SemEval-2010 task 8 dataset", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.6825990676879883}]}, {"text": "(ii) The agitating [students] e1 also put up a [barricade] e2 on the Dhaka-Mymensingh highway.", "labels": [], "entities": [{"text": "Dhaka-Mymensingh highway", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.921183168888092}]}, {"text": "Here the nominals 'cream' and 'churn' in sentence (i) are of relation * * Corresponding author Entity-Destination(e1,e2) while nominals 'students' and 'barricade' in sentence (ii) are of relations Product-Producer(e2,e1).", "labels": [], "entities": []}, {"text": "The research history of RE has witnessed the development as well as the competition of a variety of RE methodologies.", "labels": [], "entities": [{"text": "RE", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9621571898460388}, {"text": "RE", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9754042029380798}]}, {"text": "All of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations.", "labels": [], "entities": []}, {"text": "Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE ().", "labels": [], "entities": []}, {"text": "By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (.", "labels": [], "entities": []}, {"text": "However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.).", "labels": [], "entities": []}, {"text": "Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP ( or applying a graph convolution over pruned dependency trees (.", "labels": [], "entities": [{"text": "SDP", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9652908444404602}]}, {"text": "Another approach to extract the relation between two entities is using whole sentence in which both are mentioned.", "labels": [], "entities": []}, {"text": "This approach seems to be slightly weaker than using the SDP since not all words in a sentence contribute equally to classify relations and this leads to unexpected noises (.", "labels": [], "entities": []}, {"text": "However, the emergence and development of attention mechanism () has re-vitalized this approach.", "labels": [], "entities": []}, {"text": "For RE, the attention mechanism is capable of picking out the relevant words concerning target entities/relations, and then we can find critical words which determine primary useful se-mantic information (.", "labels": [], "entities": [{"text": "RE", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.971388041973114}]}, {"text": "We therefore need to determine the object of attention, i.e., nominals themselves, their entity types or relation label.", "labels": [], "entities": []}, {"text": "However, conventional attention mechanism on sequence of words cannot make use of structural information on dependency tree.", "labels": [], "entities": []}, {"text": "Moreover, it is hard for machines to learn the attention weights from along sequence of input text.", "labels": [], "entities": []}, {"text": "In this work we propose an enhanced representation for relations that combines the advantages of the above approaches.", "labels": [], "entities": []}, {"text": "Basically, we focus on condensed semantic and syntactic information on the SDP.", "labels": [], "entities": []}, {"text": "Compensating for the limitations of the SDP may still lead to missing information so we enhance this with syntactic information from the full dependency parse tree.", "labels": [], "entities": []}, {"text": "Our idea is based on fundamental notion that the syntactic structure of a sentence consists of binary asymmetrical relations between words (.", "labels": [], "entities": []}, {"text": "Since these dependency relations hold between ahead word (parent, predicate) and a dependent word (children, argument), we try to use all child nodes of a word in the dependency tree to augment its information.", "labels": [], "entities": []}, {"text": "Depending on a specific set of relations, it will turnout that not all children are useful to enhance the parent node; we select relevant children by applying several attention mechanisms with kernel filters.", "labels": [], "entities": []}, {"text": "This new representation of relation is named Richer-but-Smarter SDP (RbSP).", "labels": [], "entities": []}, {"text": "Recently, deep neural networks (DNNs) have been effectively used to learn robust syntactic and semantic representations behind complex structures.", "labels": [], "entities": []}, {"text": "Thus, we propose a novel DNN framework which combines Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) () with a multi-attention layer.", "labels": [], "entities": []}, {"text": "Our work has three main contributions: \u2022 We proposed a novel representation of relation based on attentive augmented SDP that overcomes the disadvantages of traditional SDP.", "labels": [], "entities": []}, {"text": "\u2022 We improved the attention mechanism with kernel filters to capture the features from context vectors.", "labels": [], "entities": []}, {"text": "\u2022 We proposed an advanced DNN architecture that utilizes the proposed Richer-but-Smarter Shortest Dependency Path (RbSP) and other types of linguistic and architectural features.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model was evaluated on SemEval-2010 Task 8 dataset, which contains 10, 717 annotated relation classification examples and is separated into two subsets: 8, 000 instances for training and 2, 717 for testing.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 27, "end_pos": 54, "type": "DATASET", "confidence": 0.6871837824583054}, {"text": "relation classification", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.7351184189319611}]}, {"text": "We randomly split 10 percents of the training data for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.9654903411865234}]}, {"text": "There are 9 directed relations and one undirected Other class.", "labels": [], "entities": []}, {"text": "We conduct the training-testing process 20 times and calculate the averaged results.", "labels": [], "entities": []}, {"text": "For evaluation, the predicted labels were compared to the golden annotated data using standard precision (P), recall (R), and F1 score metrics.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9002581536769867}, {"text": "recall (R)", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9659701287746429}, {"text": "F1 score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9813590347766876}]}, {"text": "summarizes the performance of our model and comparative models.", "labels": [], "entities": []}, {"text": "For a fair comparison with other researches, we implemented a baseline model, in which we remove all the proposed augmented information (multi-layer attention with kernel filters and LSTM on original sentence).", "labels": [], "entities": []}, {"text": "This baseline model is similar to the model of with some technical improvements and additional information sources.", "labels": [], "entities": []}, {"text": "It yields higher F1 than competitors which are based on SDP without any data augmentation methods.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9997469782829285}]}, {"text": "This result is also comparative when is placed next to the result of basic Attention-CNN model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The comparison of our RbSP model with other comparative models on SemEval-2010 task 8 dataset.", "labels": [], "entities": [{"text": "SemEval-2010 task 8 dataset", "start_pos": 76, "end_pos": 103, "type": "DATASET", "confidence": 0.6808185130357742}]}, {"text": " Table 2: The examples of error from RbSP and Baseline models. The predicted labels are from the best runs.   \u2020 SIDs are sentence IDs in the testing dataset.  *  Abbreviation of relations:", "labels": [], "entities": [{"text": "Abbreviation", "start_pos": 162, "end_pos": 174, "type": "METRIC", "confidence": 0.9770934581756592}]}]}