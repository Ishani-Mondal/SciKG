{"title": [{"text": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "labels": [], "entities": [{"text": "BERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9864321351051331}, {"text": "Aspect-Based Sentiment Analysis", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7691521247227987}, {"text": "Constructing Auxiliary Sentence", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.8061641653378805}]}], "abstractContent": [{"text": "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA).", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8270791520675024}, {"text": "sentiment analysis (SA)", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.9005016565322876}]}, {"text": "In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.8169992566108704}]}, {"text": "We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5798407196998596}, {"text": "SentiHood and SemEval-2014 Task 4 datasets", "start_pos": 89, "end_pos": 131, "type": "DATASET", "confidence": 0.6327473372220993}]}, {"text": "The source codes are available at https://github.com/ HSLCY/ABSA-BERT-pair.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis (SA) is an important task in natural language processing.", "labels": [], "entities": [{"text": "Sentiment analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9357358336448669}, {"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6355022291342417}]}, {"text": "It solves the computational processing of opinions, emotions, and subjectivity -sentiment is collected, analyzed and summarized.", "labels": [], "entities": []}, {"text": "It has received much attention not only in academia but also in industry, providing real-time feedback through online reviews on websites such as Amazon, which can take advantage of customers' opinions on specific products or services.", "labels": [], "entities": []}, {"text": "The underlying assumption of this task is that the entire text has an overall polarity.", "labels": [], "entities": []}, {"text": "However, the users' comments may contain different aspects, such as: \"This book is a hardcover version, but the price is a bit high.\"", "labels": [], "entities": []}, {"text": "The polarity in 'appearance' is positive, and the polarity regarding 'price' is negative.", "labels": [], "entities": []}, {"text": "Aspect-based sentiment analysis (ABSA) () aims to identify fine-grained polarity towards a specific aspect.", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8142330547173818}]}, {"text": "This task allows users to evaluate aggregated sentiments for each aspect of a given product or service and gain a more granular understanding of their quality.", "labels": [], "entities": []}, {"text": "Both SA and ABSA are sentence-level or document-level tasks, but one comment may refer to more than one object, and sentence-level tasks cannot handle sentences with multiple targets.", "labels": [], "entities": []}, {"text": "Therefore, introduce the task of targeted aspect-based sentiment analysis (TABSA), which aims to identify fine-grained opinion polarity towards a specific aspect associated with a given target.", "labels": [], "entities": [{"text": "targeted aspect-based sentiment analysis (TABSA)", "start_pos": 33, "end_pos": 81, "type": "TASK", "confidence": 0.6452203861304692}]}, {"text": "The task can be divided into two steps: (1) the first step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target.", "labels": [], "entities": []}, {"text": "The earliest work on (T)ABSA relied heavily on feature engineering (, and subsequent neural network-based methods) achieved higher accuracy.", "labels": [], "entities": [{"text": "T)ABSA", "start_pos": 22, "end_pos": 28, "type": "TASK", "confidence": 0.5378067096074423}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9976929426193237}]}, {"text": "Recently, incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model.", "labels": [], "entities": []}, {"text": "optimize the memory network and apply it to their model to better capture linguistic structure.", "labels": [], "entities": []}, {"text": "More recently, the pre-trained language models, such as ELMo (, OpenAI GPT (, and BERT (), have shown their effectiveness to alleviate the effort of feature engineering.", "labels": [], "entities": [{"text": "OpenAI GPT", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.9015951752662659}, {"text": "BERT", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9958565831184387}]}, {"text": "Especially, BERT has achieved excellent results in QA and NLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9711713194847107}, {"text": "QA", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.5674775242805481}]}, {"text": "However, there is not much improvement in (T)ABSA task with the direct use of the pretrained BERT model (see).", "labels": [], "entities": [{"text": "T)ABSA task", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.7911665290594101}, {"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9502273797988892}]}, {"text": "We think this is due to the inappropriate use of the pre-trained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9848766326904297}]}, {"text": "Since the input representation of BERT can represent both a single text sentence and a pair of text sentences, we can convert (T)ABSA into a sentence-pair classification task and fine-tune the pre-trained BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8050675392150879}, {"text": "T)ABSA", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.8822208841641744}, {"text": "sentence-pair classification task", "start_pos": 141, "end_pos": 174, "type": "TASK", "confidence": 0.7704044381777445}, {"text": "BERT", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9546157121658325}]}, {"text": "In this paper, we investigate several methods of constructing an auxiliary sentence and transform (T)ABSA into a sentence-pair classification task.", "labels": [], "entities": [{"text": "sentence-pair classification task", "start_pos": 113, "end_pos": 146, "type": "TASK", "confidence": 0.7760719656944275}]}, {"text": "We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on (T)ABSA task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9617612957954407}]}, {"text": "We also conduct a comparative experiment to verify that the classification based on a sentence-pair is better than the single-sentence classification with fine-tuned BERT, which means that the improvement is not only from BERT but also from our method.", "labels": [], "entities": [{"text": "BERT", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9923908114433289}, {"text": "BERT", "start_pos": 222, "end_pos": 226, "type": "METRIC", "confidence": 0.9813443422317505}]}, {"text": "In particular, our contribution is two-fold: 1.", "labels": [], "entities": []}, {"text": "We propose anew solution of (T)ABSA by converting it to a sentence-pair classification task.", "labels": [], "entities": [{"text": "T)ABSA", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.6103349427382151}, {"text": "sentence-pair classification task", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.7850251098473867}]}, {"text": "2. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9618384838104248}, {"text": "SentiHood and SemEval-2014 Task 4 datasets", "start_pos": 87, "end_pos": 129, "type": "DATASET", "confidence": 0.7171197235584259}]}], "datasetContent": [{"text": "We evaluate our method on the SentiHood (Saeidi et al., 2016) dataset 2 , which consists of 5,215 sentences, 3,862 of which contain a single target, and the remainder multiple targets.", "labels": [], "entities": [{"text": "SentiHood (Saeidi et al., 2016) dataset 2", "start_pos": 30, "end_pos": 71, "type": "DATASET", "confidence": 0.6593581527471543}]}, {"text": "Each sentence contains a list of target-aspect pairs {t, a} with the sentiment polarity y.", "labels": [], "entities": []}, {"text": "Ultimately, given a sentence sand the target tin the sentence, we need to: (1) detect the mention of an aspect a for the target t; (2) determine the positive or negative sentiment polarity y for detected target-aspect pairs.", "labels": [], "entities": []}, {"text": "We also evaluate our method on SemEval-2014 Task 4 (Pontiki et al., 2014) dataset 3 for aspectbased sentiment analysis.", "labels": [], "entities": [{"text": "SemEval-2014 Task 4 (Pontiki et al., 2014) dataset 3", "start_pos": 31, "end_pos": 83, "type": "DATASET", "confidence": 0.5928253581126531}, {"text": "aspectbased sentiment analysis", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6768182019392649}]}, {"text": "The only difference from the SentiHood is that the target-aspect pairs {t, a} become only aspects a.", "labels": [], "entities": []}, {"text": "This setting allows us to jointly evaluate subtask 3 (Aspect Category Detection) and subtask 4 (Aspect Category Polarity).", "labels": [], "entities": [{"text": "Aspect Category Detection", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.5452781120936075}]}], "tableCaptions": [{"text": " Table 3: Performance on SentiHood dataset. We boldface the score with the best performance across all models.  We use the results reported in Saeidi et al. (2016), Ma et al. (2018) and Liu et al. (2018). \"-\" means not reported.", "labels": [], "entities": [{"text": "SentiHood dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7240965664386749}]}, {"text": " Table 4: Test set results for Semeval-2014 task 4 Sub- task 3: Aspect Category Detection. We use the results  reported in XRCE (Brun et al., 2014) and NRC-Canada  (Kiritchenko et al., 2014).", "labels": [], "entities": [{"text": "Aspect Category Detection", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7146363457043966}, {"text": "XRCE", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.8814495205879211}, {"text": "NRC-Canada", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.9194036722183228}]}, {"text": " Table 5: Test set accuracy (%) for Semeval-2014 task  4 Subtask 4: Aspect Category Polarity. We use the  results reported in XRCE (Brun et al., 2014), NRC- Canada (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8739061951637268}, {"text": "XRCE", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.892568826675415}, {"text": "NRC- Canada", "start_pos": 152, "end_pos": 163, "type": "DATASET", "confidence": 0.832786500453949}]}]}