{"title": [{"text": "Identifying and Reducing Gender Bias in Word-Level Language Models", "labels": [], "entities": [{"text": "Identifying and Reducing Gender Bias", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8262920022010803}]}], "abstractContent": [{"text": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data.", "labels": [], "entities": []}, {"text": "For example , doctor cooccurs more frequently with male pronouns than female pronouns.", "labels": [], "entities": [{"text": "doctor cooccurs", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.6655074656009674}]}, {"text": "In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus ; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias.", "labels": [], "entities": []}, {"text": "We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases.", "labels": [], "entities": []}, {"text": "We replicate this study on three training corpora-Penn Treebank, WikiText-2, and CNN/Daily Mail-resulting in similar conclusions .", "labels": [], "entities": [{"text": "training corpora-Penn Treebank", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.7775116761525472}, {"text": "WikiText-2", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.8549501299858093}, {"text": "CNN/Daily Mail-resulting", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.8879574537277222}]}], "introductionContent": [{"text": "Dealing with discriminatory bias in training data is a major issue concerning the mainstream implementation of machine learning.", "labels": [], "entities": []}, {"text": "Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes, or distort the truth.", "labels": [], "entities": []}, {"text": "Automated systems that depend on these models can take problematic actions based on biased profiling of individuals.", "labels": [], "entities": []}, {"text": "The National Institute for Standards and Technology (NIST) evaluated several facial recognition algorithms and found that they are systematically biased based on gender.", "labels": [], "entities": [{"text": "facial recognition", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8044561743736267}]}, {"text": "Algorithms performed worse on faces labeled as female than those labeled as male.", "labels": [], "entities": []}, {"text": "Models automating resume screening have also proved to have a heavy gender bias favoring male candidates.", "labels": [], "entities": [{"text": "resume screening", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8990140557289124}]}, {"text": "Such data and algorithmic biases have become a growing concern.", "labels": [], "entities": []}, {"text": "Evaluation and mitigation of biases in data and models that use the data has been a growing field of research in recent years.", "labels": [], "entities": []}, {"text": "One natural language understanding task vulnerable to gender bias is language modeling.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.687872846921285}, {"text": "language modeling", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7939108610153198}]}, {"text": "The task of language modeling has a number of practical applications, such as word prediction used in onscreen keyboards.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7355831265449524}, {"text": "word prediction", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.8007837533950806}]}, {"text": "If possible, we would like to identify the bias in the data used to train these models and reduce its effect on model behavior.", "labels": [], "entities": []}, {"text": "Towards this pursuit, we aim to evaluate the effect of gender bias on word-level language models that are trained on a text corpus.", "labels": [], "entities": []}, {"text": "Our contributions in this work include: (i) an analysis of the gender bias exhibited by publicly available datasets used in building state-of-the-art language models; (ii) an analysis of the effect of this bias on recurrent neural networks (RNNs) based word-level language models; (iii) a method for reducing bias learned in these models; and (iv) an analysis of the results of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "PTB Penn Treebank comprises of articles ranging from scientific abstracts, computer manuals, etc.", "labels": [], "entities": [{"text": "PTB Penn Treebank", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8855755925178528}]}, {"text": "In our experiments, we observe that PTB has a higher count of male words than female words.", "labels": [], "entities": [{"text": "PTB", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.39826372265815735}]}, {"text": "Following prior language modeling work, we use the Penn Treebank dataset (PTB; preprocessed by.", "labels": [], "entities": [{"text": "Penn Treebank dataset", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.9950266679128011}]}, {"text": "WikiText-2 WikiText-2 is twice the size of the PTB and is sourced from curated Wikipedia articles.", "labels": [], "entities": [{"text": "WikiText-2 WikiText-2", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7616804540157318}, {"text": "PTB", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9621272087097168}]}, {"text": "It is more diverse and therefore has a more balanced ratio of female to male gender words than PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8513152599334717}]}, {"text": "We use preprocessed WikiText-2 (Wikitext-2;.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for Penn Treebank and generated text for different \u03bb values", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9886818528175354}]}, {"text": " Table 2: Experimental results for WikiText-2 and generated text for different \u03bb values", "labels": [], "entities": []}, {"text": " Table 3: Experimental results for CNN/Daily Mail and generated text for different \u03bb values", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9135059267282486}]}]}