{"title": [{"text": "iComposer: An Automatic Songwriting System for Chinese Popular Music", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce iComposer, an interactive web-based songwriting system designed to assist human creators by greatly simplifying music production.", "labels": [], "entities": []}, {"text": "iComposer automatically creates melodies to accompany any given text.", "labels": [], "entities": [{"text": "iComposer", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.933838427066803}]}, {"text": "It also enables users to generate a set of lyrics given arbitrary melodies.", "labels": [], "entities": []}, {"text": "iCom-poser is based on three sequence-to-sequence models, which are used to predict melody, rhythm, and lyrics, respectively.", "labels": [], "entities": [{"text": "iCom-poser", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9025812149047852}]}, {"text": "Songs generated by iComposer are compared with human-composed and randomly-generated ones in a subjective test, the experimental results of which demonstrate the capability of the proposed system to write pleasing melodies and meaningful lyrics at a level similar to that of humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Music is a universal language.", "labels": [], "entities": []}, {"text": "There is no human society that does not, at some point, have a musical heritage and tradition.", "labels": [], "entities": []}, {"text": "Over the past few years, many computer science researchers have worked on music information retrieval, focusing on genre recognition, symbolic melodic similarity, melody extraction, mood classification, etc.", "labels": [], "entities": [{"text": "music information retrieval", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6874295274416605}, {"text": "genre recognition", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7385680079460144}, {"text": "melody extraction", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.7195023149251938}, {"text": "mood classification", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7505622506141663}]}, {"text": "With respect to computational creativity, tasks such as automatic music composition or lyrics generation have been discussed for decades.", "labels": [], "entities": [{"text": "automatic music composition", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6810221870740255}, {"text": "lyrics generation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7168105393648148}]}, {"text": "However, a relatively new topic-the relationship between lyrics and melody-remains somewhat mysterious.", "labels": [], "entities": []}, {"text": "It is difficult to explain how music and spoken words fit together and why the pairing of these two creates an emotionally compelling experience; we believe this merits a deeper investigation.", "labels": [], "entities": []}, {"text": "Inspired by work on text-to-image synthesis and image caption generation, we propose iComposer, a simple and effective bi-directional songwriting system that automatically generates melody from text and vice versa.", "labels": [], "entities": [{"text": "text-to-image synthesis", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7288916707038879}, {"text": "image caption generation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7981497248013815}]}, {"text": "We believe that iComposer has value in capturing relationships between lyrics and melody.", "labels": [], "entities": []}, {"text": "Moreover, iComposer makes it possible for more people, both professional and amateur musicians, to enjoy and benefit from this creative activity.", "labels": [], "entities": []}, {"text": "An LSTM (long short-term memory) based model is especially suitable for this task as it is structured to use historical information to predict the next value in the sequence.", "labels": [], "entities": []}, {"text": "Our architecture consists of three subnetworks, each of which is a sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "These three subnetworks generate the pitch, duration, and lyrics for each note, and jointly learn the structure of Chinese popular music.", "labels": [], "entities": [{"text": "duration", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9490930438041687}]}, {"text": "use textbased LSTM for automatic music composition, Potash, Romanov, and Rumshisky (2015) demonstrate the effectiveness of LSTM on rap lyrics generation, and propose an RNN-based lyrics language model conditioned on melodies of Japanese songs.", "labels": [], "entities": [{"text": "automatic music composition", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7069863875706991}, {"text": "rap lyrics generation", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.66690727074941}]}, {"text": "propose a sequence-to-sequence neural network model that composes melody from lyrics.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, iComposer is the first songwriting system that utilizes a sequence-to-sequence model in generating both melody and Chinese lyrics that match each other perfectly.", "labels": [], "entities": []}, {"text": "In addition to self-evaluation, we designed an experiment to evaluate the quality of the generated songs.", "labels": [], "entities": []}, {"text": "Thirty subjects were asked to subjectively rate the aesthetic value of 10 selections, a mixture of original songs and computer-generated songs.", "labels": [], "entities": []}, {"text": "The experimental results indicate that iComposer composes compelling songs that are quite similar to those composed by humans, and are much better than those generated by the baseline method.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purpose of this study, music sheets with vocal lines and their accompanying lyrics are necessary.", "labels": [], "entities": []}, {"text": "Moreover, files with a single instrument corresponding to the melody are favored for better performance in the LSTM model.", "labels": [], "entities": []}, {"text": "However, as far as we know there is no such dataset available, and most state-of-the-art melody extraction tools are still unsatisfactory.", "labels": [], "entities": [{"text": "melody extraction", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7363371104001999}]}, {"text": "We collected 1000 Chinese popular music pieces in MIDI format and extracted feature information using pretty midi, a Python module for creating, manipulating, and analyzing MIDI files (.", "labels": [], "entities": []}, {"text": "Musical notes are represented by MIDI note numbers from 0 to 127, where 60 is defined as middle C.", "labels": [], "entities": []}, {"text": "Note durations are represented by numbers in the range from 0.1 to 3.0 seconds.", "labels": [], "entities": [{"text": "durations", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9596292972564697}]}, {"text": "We then recruited 20 people with at least five years of experience playing instruments to isolate the main melodies from polyphonic music and align the lyrics to their corresponding notes, as shown in.", "labels": [], "entities": []}, {"text": "The dataset consists of approximately 300,000 character-note pairs, of which we partitioned 80% as the training set and used the rest for testing.", "labels": [], "entities": []}, {"text": "Note that the correspondence between notes and characters is not always one-toone.", "labels": [], "entities": []}, {"text": "A single sung character can be composed of multiple notes (i.e. one-to-many alignment).", "labels": [], "entities": []}, {"text": "In this section, we illustrate the behavior of the proposed system with an analysis of some generated lyrics and melodies.", "labels": [], "entities": []}, {"text": "After this, we provide and discuss details about the subjective test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Subjective Test Results", "labels": [], "entities": [{"text": "Subjective Test", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8777764439582825}]}]}