{"title": [{"text": "Reinforcement Learning based Curriculum Optimization for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7372398376464844}]}], "abstractContent": [{"text": "We consider the problem of making efficient use of heterogeneous training data in neu-ral machine translation (NMT).", "labels": [], "entities": [{"text": "neu-ral machine translation (NMT)", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.7414990862210592}]}, {"text": "Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training.", "labels": [], "entities": []}, {"text": "Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes.", "labels": [], "entities": []}, {"text": "Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically , jointly with the NMT system, in the course of a single training run.", "labels": [], "entities": []}, {"text": "We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively.", "labels": [], "entities": [{"text": "Paracrawl", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9803071618080139}, {"text": "WMT English-to-French datasets", "start_pos": 71, "end_pos": 101, "type": "DATASET", "confidence": 0.8714965581893921}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9981037378311157}]}, {"text": "Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation training data is typically heterogeneous: it may vary in characteristics such as domain, translation quality, and degree of difficulty.", "labels": [], "entities": [{"text": "Machine Translation training", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8959833979606628}]}, {"text": "Many approaches have been proposed to cope with heterogeneity, such as filtering () or down-weighting ( examples that are likely to be noisy or out of domain.", "labels": [], "entities": []}, {"text": "A powerful technique is to control the curriculum-the order in which examples are presented to the system-as is done in fine-tuning, where training occurs first on general data, and then on more valuable in-domain data.", "labels": [], "entities": []}, {"text": "Curriculum based approaches generalize data filtering and weighting by allowing examples to be visited multiple times Assuming integer weights. or not at all; and they additionally potentially enable steering the training trajectory toward a better global optimum than might be attainable with a static attribute-weighting scheme.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7270674407482147}]}, {"text": "Devising a good curriculum is a challenging task that is typically carried out manually using prior knowledge of the data and its attributes.", "labels": [], "entities": []}, {"text": "Although powerful heuristics like fine-tuning are helpful, setting hyper-parameters to specify a curriculum is usually a matter of extensive trial and error.", "labels": [], "entities": []}, {"text": "Automating this process with meta-learning is thus an attractive proposition.", "labels": [], "entities": []}, {"text": "However, it comes with many potential pitfalls such as failing to match a human-designed curriculum, or significantly increasing training time.", "labels": [], "entities": []}, {"text": "In this paper we present an initial study on meta-learning an NMT curriculum.", "labels": [], "entities": []}, {"text": "Starting from scratch, we attempt to match the performance of a successful non-trivial reference curriculum proposed by , in which training gradually focuses on increasingly cleaner data, as measured by an external scoring function.", "labels": [], "entities": []}, {"text": "Inspired by, we use a reinforcement-learning (RL) approach involving a learned agent whose task is to choose a corpus bin, representing a given noise level, at each NMT training step.", "labels": [], "entities": []}, {"text": "A challenging aspect of this task is that choosing only the cleanest bin is sub-optimal; the reference curriculum uses all the data in the early stages of training, and only gradually anneals toward the cleanest.", "labels": [], "entities": []}, {"text": "Furthermore, we impose the condition that the agent must learn its curriculum in the course of a single NMT training run.", "labels": [], "entities": []}, {"text": "We demonstrate that our RL agent can learn a curriculum that works as well as the reference, obtaining a similar quality improvement over a random-curriculum baseline.", "labels": [], "entities": []}, {"text": "Interestingly, it does so using a different strategy from the reference.", "labels": [], "entities": []}, {"text": "This result opens the door to learning more sophisticated curricula that exploit multiple data at- tributes and work with arbitrary corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our NMT model is similar to RNMT+, but with only four layers in both encoder and decoder.", "labels": [], "entities": []}, {"text": "Rewards (dev-set loglikelihood) are provided approximately every 10 training steps by an asynchronous process.", "labels": [], "entities": [{"text": "Rewards", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9676703214645386}]}, {"text": "We use the DQN agent implementation in Dopamine, 2 which includes an experience replay buffer to remove temporal correlations from the observations, among other DQN best practices.", "labels": [], "entities": [{"text": "Dopamine, 2", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.893700897693634}]}, {"text": "Due to the sparse and asynchronous nature of our rewards, we store observation, action transitions in a temporary buffer until anew reward arrives.", "labels": [], "entities": []}, {"text": "At this point, transitions are moved from the temporary buffer to the DQN agent's replay buffer.", "labels": [], "entities": []}, {"text": "The RL agent is trained after each NMT training step by sampling an RL mini-batch from the replay buffer.", "labels": [], "entities": [{"text": "NMT training step", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7952428857485453}]}, {"text": "Our RL hyper-parameter settings are listed in the appendix.", "labels": [], "entities": []}, {"text": "Following , we use the Paracrawl and WMT English-French corpora for our experiments.", "labels": [], "entities": [{"text": "Paracrawl", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.9561153650283813}, {"text": "WMT English-French corpora", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.8988816142082214}]}, {"text": "These contain 290M and 36M training sentences respectively.", "labels": [], "entities": []}, {"text": "WMT is relatively clean, while a large majority of Paracrawl sentence pairs contain noise.", "labels": [], "entities": [{"text": "WMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7510679364204407}, {"text": "Paracrawl sentence pairs", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.8413622975349426}]}, {"text": "We process both corpora with BPE, using a vocabulary size of 32k.", "labels": [], "entities": [{"text": "BPE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.8042349219322205}]}, {"text": "Both corpora are split into 6 equal-sized bins according to their noise level, as provided by CDS score.", "labels": [], "entities": [{"text": "CDS score", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.7081606388092041}]}, {"text": "In both settings, the WMT newstest 2010-2011 corpus is used as trusted data for CDS scores, which are computed using the models and procedure described in . For the prototype batch used to generate observations, we extracted the 32 sentences whose CDS scores are closest to the mean in each bin, giving a total of 192 sentences.", "labels": [], "entities": [{"text": "WMT newstest 2010-2011 corpus", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.9568953663110733}]}, {"text": "We use WMT 2012-2013 for development and WMT 2014 for test, and report tokenized, naturally-cased BLEU scores from the test checkpoint closest to the highest-BLEU dev checkpoint.", "labels": [], "entities": [{"text": "WMT 2012-2013", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8929444551467896}, {"text": "WMT", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7176573276519775}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9905440807342529}]}, {"text": "To combat variance caused by sampling different batches per bin (which produces somewhat different results even when bins are visited in fixed order), all models were run twice with different random seeds, and the model with the best score on the dev set was chosen.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores on Paracrawl and WMT En-Fr  datasets with uniform, heuristic and learned curricula.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986792206764221}, {"text": "Paracrawl and WMT En-Fr  datasets", "start_pos": 25, "end_pos": 58, "type": "DATASET", "confidence": 0.7319153666496276}]}]}