{"title": [{"text": "A Crowdsourced Frame Disambiguation Corpus with Ambiguity", "labels": [], "entities": [{"text": "Frame Disambiguation Corpus", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.692806472380956}, {"text": "Ambiguity", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9551740884780884}]}], "abstractContent": [{"text": "We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus.", "labels": [], "entities": [{"text": "FrameNet semantic frame disambiguation", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.6349772587418556}, {"text": "Wikipedia corpus", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.8755239546298981}]}, {"text": "The annotations were collected using a novel crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement.", "labels": [], "entities": []}, {"text": "In contrast to the typical approach of attributing the best single frame to each word, we provide a list of frames with disagreement-based scores that express the confidence with which each frame applies to the word.", "labels": [], "entities": []}, {"text": "This is based on the idea that inter-annotator disagreement is at least partly caused by ambiguity that is inherent to the text and frames.", "labels": [], "entities": []}, {"text": "We have found many examples where the semantics of individual frames overlap sufficiently to make them acceptable alternatives for interpreting a sentence.", "labels": [], "entities": []}, {"text": "We have argued that ignoring this ambiguity creates an overly arbitrary target for training and evaluating natural language processing systems-if humans cannot agree, why would we expect the correct answer from a machine to be any different?", "labels": [], "entities": []}, {"text": "To process this data we also utilized an expanded lemma-set provided by the Framester system, which merges FN with WordNet to enhance coverage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.910691499710083}]}, {"text": "Our dataset includes annotations of 1,000 sentence-word pairs whose lemmas are not part of FN.", "labels": [], "entities": [{"text": "FN", "start_pos": 91, "end_pos": 93, "type": "DATASET", "confidence": 0.9074591994285583}]}, {"text": "Finally we present metrics for evaluating frame disam-biguation systems that account for ambiguity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Crowdsourcing has been a popular method to collect corpora fora variety of natural language processing tasks (, although one of its downsides is the crowd's lack of domain knowledge that is helpful in solving some tasks.", "labels": [], "entities": []}, {"text": "Semantic frame disambiguation is an example of a complex natural language processing task that is usually performed by linguistic experts, subjected to strict annotation guidelines and quality control.", "labels": [], "entities": [{"text": "Semantic frame disambiguation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7453791300455729}]}, {"text": "The theory of frame semantics (J Fillmore, 1982) defines a frame as an abstract representation of a word sense, describing a type of entity, relation, or event, together with the associated roles implied by the frame.", "labels": [], "entities": [{"text": "frame semantics (J Fillmore, 1982)", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.5735464096069336}]}, {"text": "The FrameNet (FN) corpus () is a collection of semantic frames, together with a corpus of documents annotated with these frames.", "labels": [], "entities": [{"text": "FrameNet (FN) corpus", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7108052253723145}]}, {"text": "Similarly to word-sense disambiguation, frame disambiguation is the task of obtaining the correct frame for each word, since many words have multiple possible meanings.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.752341091632843}, {"text": "frame disambiguation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6646715700626373}]}, {"text": "Using domain experts for frame disambiguation is expensive and time consuming, resulting in small corpora for this task that do not scale well for modern machine learning methods -FN version 1.7, the latest one at the time of writing, contains only about 10,000 sentences annotated with frames.", "labels": [], "entities": [{"text": "frame disambiguation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7290655374526978}, {"text": "FN version 1.7", "start_pos": 180, "end_pos": 194, "type": "DATASET", "confidence": 0.8957657814025879}]}, {"text": "Furthermore, only using one expert to perform the annotation makes it difficult to capture any diversity of perspectives.", "labels": [], "entities": []}, {"text": "There have been a number of small-scale attempts at using crowdsourcing for frame disambiguation in sentences, showing that the crowd has comparable performance to the FN domain experts (, and that the crowd can be used to correct wrong examples that have been collected automatically (.", "labels": [], "entities": [{"text": "frame disambiguation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7015870213508606}]}, {"text": "Crowd performance can be improved by combining frame role identification with disambiguation (, or by asking crowd workers to give each other feedback and then letting them change their answer (.", "labels": [], "entities": [{"text": "frame role identification", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6742236812909445}]}, {"text": "Crowdsourcing has also been useful to identify the ambiguity in frame disambiguation.", "labels": [], "entities": []}, {"text": "Previously, we have shown () that while the crowd and FN expert mostly agree over frame disambiguation, disagreement cases are often caused by ambiguity, such as vague or overlapping frame definitions, or incomplete information in the sentence.", "labels": [], "entities": []}, {"text": "Because of these issues with the input data, the approach of selecting one single correct frame for every word, and ignoring alternative interpretations, often results in arbitrary, incomplete ground truth corpora.", "labels": [], "entities": []}, {"text": "In order to aggregate annotated data while preserving disagreement, we use the CrowdTruth method 1, which encourages using multiple crowd annotators to perform the same work, and processes the disagreement between them to signal low quality workers, sentences, and frames.", "labels": [], "entities": []}, {"text": "This paper presents a crowdsourced FN frame disambiguation corpus of 5,042 sentence-word pairs (which has since grown to over 9,000 since the submission of this paper).", "labels": [], "entities": [{"text": "FN frame disambiguation", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6760693987210592}]}, {"text": "More than 1,000 of these are lexical units (LUs) not part of FN.", "labels": [], "entities": [{"text": "FN", "start_pos": 61, "end_pos": 63, "type": "DATASET", "confidence": 0.9452772736549377}]}, {"text": "To our knowledge, it is the largest corpus of this type outside of FN.", "labels": [], "entities": [{"text": "FN", "start_pos": 67, "end_pos": 69, "type": "DATASET", "confidence": 0.911777675151825}]}, {"text": "In addition, we applied the CrowdTruth method, in which each sentence and lexical item is accompanied by a list of multiple frames with scores that express the confidence with which each frame applies to the word.", "labels": [], "entities": []}, {"text": "This allows us to demonstrate that ambiguity is a prominent feature of frame disambiguation, with many cases where more than one possible frame can apply to the same word.", "labels": [], "entities": [{"text": "frame disambiguation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7000163942575455}]}, {"text": "Finally, we present an evaluation of several frame disambiguation models using evaluation metrics that leverage the multiple answers and their confidence scores, and show that even a model that always predicts the top crowd answer will not always have the best performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Instead of traditional evaluation metrics that require binary labels, we propose an evaluation methodology that is able to consider multiple candidate frames for each sentence and their quality scores.", "labels": [], "entities": []}, {"text": "We use Kendall's \u03c4 list ranking coefficient ( and cosine similarity to calculate the distance between the list of frames produced by the crowd labeled with the F SS, and the frames predicted by the baselines in each sentence.", "labels": [], "entities": []}, {"text": "Whereas Kendall's \u03c4 only accounts for the ranking of the F SS for each frame, cosine similarity uses the actual F SS values in the calculation of the similarity.", "labels": [], "entities": [{"text": "F SS", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9318458437919617}]}, {"text": "Both metrics compute a score per sentence (Kendall's \u03c4 \u2208 [\u22121, 1], and cosine similarity \u2208 [0, 1]).", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.7844724357128143}]}, {"text": "Using these metrics, we produce two aggregate statistics over our test corpus: (1) the area-under-curve (AU C) for each metric, normalized by the corpus size, and (2) the SQSweighted average of each metric (w \u2212 avg), which also accounts for the ambiguity of the sentence as expressed by the SQS.", "labels": [], "entities": [{"text": "area-under-curve (AU C)", "start_pos": 87, "end_pos": 110, "type": "METRIC", "confidence": 0.8650157213211059}]}, {"text": "We evaluate on two versions of the corpus: (1) the restricted set (R-SET) of 4,000 sentences with LUs from the FN corpus, and (2) the full set (F-SET) of 5,042 sentences.", "labels": [], "entities": [{"text": "FN corpus", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.9661113917827606}, {"text": "F-SET", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.8174448013305664}]}, {"text": "The results ( & show that OS+ performs best out of all the models, even taking into account sentences with LUs not in FN for which OS+ cannot disambiguate.", "labels": [], "entities": []}, {"text": "FS performs the worst out of all models on R-SET, because it cannot find newly added frames from the latest FN release, but improves on the F-SET (FS can find candidate frames for LUs not in FN).", "labels": [], "entities": [{"text": "FS", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5487134456634521}, {"text": "F-SET", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.48414990305900574}]}, {"text": "The scores on the F-SET were lower for all baselines, suggesting that sentences with LUs not in FN are more difficult to classify -this could be because FN is missing frames that can express the full meaning of these LUs.", "labels": [], "entities": [{"text": "F-SET", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.7566323280334473}]}, {"text": "TC has a good performance, but is far from being unbeatable -when measuring Kendall's \u03c4 over the R-SET, OS+ performs better than TC.", "labels": [], "entities": [{"text": "TC", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7952451109886169}, {"text": "Kendall's \u03c4", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.7328800956408182}]}], "tableCaptions": [{"text": " Table 2: Aggregated evaluation results.", "labels": [], "entities": [{"text": "Aggregated", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9087666869163513}]}]}