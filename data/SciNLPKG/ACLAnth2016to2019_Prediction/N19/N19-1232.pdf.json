{"title": [{"text": "Doc2hash: Learning Discrete Latent Variables for Document Retrieval", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.743336021900177}]}], "abstractContent": [{"text": "Learning to hash via generative models has become a powerful paradigm for fast similarity search in documents retrieval.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6794233322143555}, {"text": "documents retrieval", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6787927597761154}]}, {"text": "To get binary representation (i.e., hash codes), the discrete distribution prior (i.e., Bernoulli Distribution) is applied to train the variational autoencoder (VAE).", "labels": [], "entities": [{"text": "variational autoencoder (VAE", "start_pos": 136, "end_pos": 164, "type": "METRIC", "confidence": 0.6399499326944351}]}, {"text": "However, the discrete stochastic layer is usually incompatible with the backpropaga-tion in the training stage and thus causes a gradient flow problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method, Doc2hash, that solves the gradient flow problem of the discrete stochastic layer by using continuous relaxation on priors, and trains the generative model in an end-to-end manner to generate hash codes.", "labels": [], "entities": []}, {"text": "In qualitative and quantitative experiments, we show the proposed model outperforms other state of the art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "A popular theme for deep learning is that of representation learning, whose goal is to use existing data to learn a compact and meaningful representation when building classifiers or other predictors.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.9163395166397095}]}, {"text": "Learning continuous representation has been achieving a great success in various NLP tasks, including text classification, word and sentence representation (), yet learning discrete representation is potentially more suitable for tasks we are interested in such as learning to hash.", "labels": [], "entities": [{"text": "text classification", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.778807669878006}, {"text": "word and sentence representation", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.6301479041576385}]}, {"text": "Hashing serves as a fast solution for similarity search also called approximate nearest neighbor search.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7917774021625519}, {"text": "approximate nearest neighbor search", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.5811902061104774}]}, {"text": "In document retrieval, semantic hashing is the strategy that turn the document into binary codes (hash codes) which capture semantic information.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7166948467493057}]}, {"text": "One can use a query (a document) to retrieve similar documents by calculating the hamming distances between their hash codes.", "labels": [], "entities": []}, {"text": "Given the fast calculation process and the efficient storage property (one modern PC can execute millions of hamming distance computations in just a few milliseconds).", "labels": [], "entities": []}, {"text": "This semantic hashing strategy is very attractive.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of modeling latent variables via generative models in solving various NLP problems (van den, Some approaches obtain binary codes of documents from a generative perspective via parameterizing models using neural networks and stochastic optimization using gradient-based techniques:) designed a two-stage training procedures to generate hash codes with variational autoencoder: (i) it first infers continuous representations of text through VAE with isotropic Gaussian distribution prior (ii) Obtain hash codes via binarizing the continuous representation of texts.", "labels": [], "entities": [{"text": "VAE", "start_pos": 470, "end_pos": 473, "type": "DATASET", "confidence": 0.8597458600997925}]}, {"text": "Since the model parameters are not learned in an end-to-end manner, the two-stage training strategy may result in a suboptimal local optima.", "labels": [], "entities": []}, {"text": "() replaced the Gaussian distribution prior with Bernoulli distribution prior so that the stochastic layer of the VAE can directly produce binary codes in the latent space and train hash codes in an end-to-end manner.", "labels": [], "entities": [{"text": "VAE", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8487429022789001}]}, {"text": "Unfortunately, the Bernoulli stochastic layer is non-differentiable.", "labels": [], "entities": []}, {"text": "Although the StraightThrough (ST) estimator is adapted to propagate gradients where it skips the gradient of the stochastic layer during backpropagation (.", "labels": [], "entities": []}, {"text": "The ST estimator is still a biased estimator which introduces high-variance biased gradients of objectives during the training.", "labels": [], "entities": []}, {"text": "In this paper, we propose a generative model with a categorical distribution prior for semantic hashing which learns binary codes of documents in an end-to-end manner.", "labels": [], "entities": []}, {"text": "To train the generative model, instead of using the ST estimator, we use the Gumbel-Softmax trick to overcome the inability by applying the re-parameterization trick to discrete latent variables.", "labels": [], "entities": []}, {"text": "There are two advantages: 1) a nice parameterization fora discrete (or categorical) distribution is given in terms of the Gumbel distribution (the Gumbel trick); and 2) although the corresponding function is noncontinuous, it can be made continuous by applying using a continuous approximation that depends on a temperature parameter.", "labels": [], "entities": []}, {"text": "Therefore, it produces low-variance biased gradients of the stochastic layer in the backpropagation.", "labels": [], "entities": []}, {"text": "Our experiment shows our model achieves the state of the art performance on three standard datasets for semantic hashing.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate Doc2hash over various number of bits on the three datasets.", "labels": [], "entities": [{"text": "Doc2hash", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.8667313456535339}]}, {"text": "In the unsupervised hashing, Doc2hash outperforms NASH-DN, the current state of the art model for semantic hashing, and other methods.", "labels": [], "entities": []}, {"text": "The shows the result of TMC dataset.", "labels": [], "entities": [{"text": "TMC dataset", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.8484554886817932}]}, {"text": "Doc2hash shows its ability to assign the similar data (with the same label) to the hash codes of which hamming distances are small.", "labels": [], "entities": [{"text": "Doc2hash", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9718353748321533}]}, {"text": "The same trend and superiority of Doc2hash are also observed in both Reuters and RCV1 datasets as shown in Tables 2 and 3.", "labels": [], "entities": [{"text": "Doc2hash", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9390238523483276}, {"text": "Reuters and RCV1 datasets", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.727851539850235}]}, {"text": "Note that we do not compare with NASH-DN in RCV1 because NASH doesn't report any results on RCV1 dataset and doesn't release any code to reproduce their result.", "labels": [], "entities": [{"text": "NASH-DN", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9519861936569214}, {"text": "NASH", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9556907415390015}, {"text": "RCV1 dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.963538408279419}]}, {"text": "We compare the reminding methods with Doc2hash.", "labels": [], "entities": [{"text": "reminding", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9229814410209656}, {"text": "Doc2hash", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.935549795627594}]}, {"text": "shows our approach improves the precision with significant margin compared with other models.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996399879455566}]}, {"text": "We note that the retrieval results tend to drop when we set the length of hash codes to be 64 or larger, which also happens for some baselines.", "labels": [], "entities": []}, {"text": "It is probably because of over-fitting.", "labels": [], "entities": []}, {"text": "Compared with other methods, the proposed method is robust when the codes length increase.", "labels": [], "entities": []}, {"text": "Our approach performs better than other methods in 64 and 128 bits setting, suggesting that Doc2hash can effectively generate hash codes to documents even with limited training data.", "labels": [], "entities": []}, {"text": "We also evaluate Doc2hash in the supervised hashing setting with the same datasets.", "labels": [], "entities": [{"text": "Doc2hash", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.911533534526825}]}, {"text": "We make use of the label/tag information during training.", "labels": [], "entities": []}, {"text": "As shown in Tables 1, 2 and 3, Doc2hash yields better results than the other baseline models in different bits length.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision of the top 100 retrieved documents  on TMC dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9859253764152527}, {"text": "TMC dataset", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9447929859161377}]}, {"text": " Table 2: Precision of the top 100 retrieved documents  on Reuters dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9845996499061584}, {"text": "Reuters dataset", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9717549085617065}]}]}