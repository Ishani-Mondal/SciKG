{"title": [{"text": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations", "labels": [], "entities": [{"text": "WiC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8982683420181274}, {"text": "Word-in-Context Dataset", "start_pos": 9, "end_pos": 32, "type": "DATASET", "confidence": 0.6434659063816071}, {"text": "Evaluating Context-Sensitive Meaning Representations", "start_pos": 37, "end_pos": 89, "type": "TASK", "confidence": 0.7032115682959557}]}], "abstractContent": [{"text": "By design, word embeddings are unable to model the dynamic nature of words' semantics , i.e., the property of words to correspond to potentially different meanings.", "labels": [], "entities": []}, {"text": "To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed.", "labels": [], "entities": []}, {"text": "However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words.", "labels": [], "entities": []}, {"text": "In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contex-tual Word Similarity, and highlight its shortcomings.", "labels": [], "entities": [{"text": "Stanford Contex-tual Word Similarity", "start_pos": 140, "end_pos": 176, "type": "TASK", "confidence": 0.6582512259483337}]}, {"text": "To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations.", "labels": [], "entities": [{"text": "Word in Context dataset", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.5971622243523598}]}, {"text": "WiC is released in https://pilehvar.github.io/wic/.", "labels": [], "entities": [{"text": "WiC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9043961763381958}]}], "introductionContent": [{"text": "One of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears.", "labels": [], "entities": []}, {"text": "Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words 1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context.", "labels": [], "entities": []}, {"text": "To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings, which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language.", "labels": [], "entities": []}, {"text": "word embeddings (, which instead compute a single dynamic embedding fora given word which can adapt itself to arbitrary contexts for the word.", "labels": [], "entities": []}, {"text": "Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation.", "labels": [], "entities": []}, {"text": "Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carryout impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline).", "labels": [], "entities": []}, {"text": "Despite providing a suitable means of verifying the effectiveness of the embeddings, the downstream evaluation cannot replace generic evaluations as it is difficult to isolate the impact of embeddings from many other factors involved, including the algorithmic configuration and parameter setting of the system.", "labels": [], "entities": []}, {"text": "To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics.", "labels": [], "entities": [{"text": "Stanford Contextual Word Similarity (SCWS) dataset", "start_pos": 22, "end_pos": 72, "type": "DATASET", "confidence": 0.7416316904127598}]}, {"text": "In Section 4 we will explain the limitations of this dataset for the evaluation of recent work in the literature.", "labels": [], "entities": []}, {"text": "In this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings.", "labels": [], "entities": []}, {"text": "WiC provides multiple interesting characteristics: (1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification dataset, in which, unlike SCWS, identical words are paired with each other (in different con-F There's a lot of trash on the bed of the river -I keep a glass of water next to my bed when I sleep F Justify the margins -The end justifies the means T Air pollution -Open a window and let in some air T The expanded window will give us time to catch the thieves -You have a two-hour window of clear weather to finish working on the lawn: Sample positive (T) and negative (F) pairs from the WiC dataset (target word in italics).", "labels": [], "entities": [{"text": "WiC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9071706533432007}, {"text": "contextualized word and sense representation", "start_pos": 123, "end_pos": 167, "type": "TASK", "confidence": 0.6490947067737579}, {"text": "word sense disambiguation", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.669712652762731}, {"text": "WiC dataset", "start_pos": 729, "end_pos": 740, "type": "DATASET", "confidence": 0.9074508249759674}]}, {"text": "texts); hence, a context-insensitive word embedding model would perform similarly to a random baseline; and (3) it is constructed using high quality annotations curated by experts.", "labels": [], "entities": []}, {"text": "2 WiC: the Word-in-Context dataset We frame the task as binary classification.", "labels": [], "entities": [{"text": "Word-in-Context dataset", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.829150915145874}]}, {"text": "Each instance in WiC has a target word w, either a verb or a noun, for which two contexts, c 1 and c 2 , are provided.", "labels": [], "entities": []}, {"text": "Each of these contexts triggers a specific meaning of w.", "labels": [], "entities": []}, {"text": "The task is to identify if the occurrences of win c 1 and c 2 correspond to the same meaning or not.", "labels": [], "entities": []}, {"text": "lists some examples from the dataset.", "labels": [], "entities": []}, {"text": "In what follows in this section, we describe the construction procedure of the dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with recent multi-prototype and contextualized word embedding techniques.", "labels": [], "entities": []}, {"text": "Evaluation of other embedding models as well as word sense disambiguation systems is left for future work.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.648899088303248}]}, {"text": "We also report results for two baseline models which view the task as context (sentence) similarity.", "labels": [], "entities": []}, {"text": "The BoW system views the sentence as a bag of words and computes a simple embedding as average of its words.", "labels": [], "entities": [{"text": "BoW", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9436105489730835}]}, {"text": "Multi-prototype models DeConf* 52.4 \u00b1 0.8 62.1 SW2V* 54.1 \u00b1 0.5 59.1 JBT 54.1 \u00b1 0.6 54.5 Sentence-level baselines BoW 54.2 \u00b1 1.3 61.0 Sentence LSTM 53.1 \u00b1 0.9: Accuracy % performance of different models on the WiC dataset.", "labels": [], "entities": [{"text": "BoW", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8507580757141113}, {"text": "Accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9894936084747314}, {"text": "WiC dataset", "start_pos": 210, "end_pos": 221, "type": "DATASET", "confidence": 0.8858910799026489}]}, {"text": "The estimated (human-level) performance is 80.0 (cf. Section 2.2) and a random baseline would perform at 50.0.", "labels": [], "entities": []}, {"text": "Systems marked with * make use of external lexical resources.", "labels": [], "entities": []}, {"text": "on the Google News corpus.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.8654077649116516}]}, {"text": "Sentence LSTM is another baseline, which differently from the other models, does not obtain explicit encoded representations of the target word or sentence.", "labels": [], "entities": [{"text": "Sentence LSTM", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7982028126716614}]}, {"text": "The system has two LSTM layers with 50 units, one for each context side, which concatenates the outputs and passes that to a feedforward layer with 64 neurons, followed by a dropout layer at rate 0.5, and a final one-neuron output layer of sigmoid activation.", "labels": [], "entities": []}, {"text": "We used two simple binary classifiers in our experiments on top of all comparison systems (except for the LSTM baseline).", "labels": [], "entities": [{"text": "LSTM baseline", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.7528686225414276}]}, {"text": "MLP: a simple dense network with 100 hidden neurons (ReLU activation), and one output neuron (sigmoid activation), tuned on the development set (batch size: 32; optimizer: Adam; loss: binary crossentropy).", "labels": [], "entities": [{"text": "MLP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7595388889312744}]}, {"text": "Given the stochasticity of the network optimizer, we report average results for five runs (\u00b1 standard deviation).", "labels": [], "entities": []}, {"text": "Threshold: a simple threshold-based classifier based on the cosine distance of the two input vectors, tuned with step size 0.02 on the development set.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.987179160118103}]}, {"text": "shows the results on WiC.", "labels": [], "entities": [{"text": "WiC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.9624463319778442}]}, {"text": "In general, the dataset proves to be very difficult for all the techniques, with the best model, i.e., BERT large , providing around 14% absolute improvement over a random baseline.", "labels": [], "entities": [{"text": "BERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9987576007843018}]}, {"text": "Among the two classifiers, the simple threshold-based strategy, which computes the cosine distance between the two encodings, proves to be more efficient than the MLP network which might not be suitable for this setting with small amount of training data.", "labels": [], "entities": []}, {"text": "The 16.2% absolute accuracy difference between human-level upperbound and state-of-the-art performance suggests, however, a challenging dataset and encourages future research in context-sensitive word embeddings to leverage WiC in their evaluations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9310913681983948}]}], "tableCaptions": [{"text": " Table 2: Statistics of different splits of WiC.", "labels": [], "entities": [{"text": "WiC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8497152924537659}]}]}