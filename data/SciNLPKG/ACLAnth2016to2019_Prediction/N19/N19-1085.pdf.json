{"title": [{"text": "Improving Event Coreference Resolution by Learning Argument Compatibility from Unlabeled Data", "labels": [], "entities": [{"text": "Improving Event Coreference Resolution", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9239818453788757}]}], "abstractContent": [{"text": "Argument compatibility is a linguistic condition that is frequently incorporated into modern event coreference resolution systems.", "labels": [], "entities": [{"text": "Argument compatibility", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.744621604681015}, {"text": "event coreference resolution", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.7971882621447245}]}, {"text": "If two event mentions have incompatible arguments in any of the argument roles, they cannot be coreferent.", "labels": [], "entities": []}, {"text": "On the other hand, if these mentions have compatible arguments, then this maybe used as information toward deciding their coreferent status.", "labels": [], "entities": []}, {"text": "One of the key challenges in leveraging argument compatibility lies in the paucity of labeled data.", "labels": [], "entities": [{"text": "leveraging argument compatibility", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7094961007436117}]}, {"text": "In this work, we propose a transfer learning framework for event coreference resolution that utilizes a large amount of unlabeled data to learn the argument compatibility between two event mentions.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.8226331273714701}]}, {"text": "In addition, we adopt an interactive inference network based model to better capture the (in)compatible relations between the context words of two event mentions.", "labels": [], "entities": []}, {"text": "Our experiments on the KBP 2017 English dataset confirm the effectiveness of our model in learning argument compatibility, which in turn improves the performance of the overall event coreference model.", "labels": [], "entities": [{"text": "KBP 2017 English dataset", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.9714790135622025}, {"text": "learning argument compatibility", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.7527234355608622}]}], "introductionContent": [{"text": "Events are essential building blocks of all kinds of natural language text.", "labels": [], "entities": []}, {"text": "An event can be described several times from different aspects in the same document, resulting in multiple surface forms of event mentions.", "labels": [], "entities": []}, {"text": "The goal of event coreference resolution is to identify event mentions that correspond to the same real-world event.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.8360832532246908}]}, {"text": "This task is critical for natural language processing applications that require deep text understanding, such as storyline extraction/generation, text summarization, question answering, and information extraction.", "labels": [], "entities": [{"text": "storyline extraction/generation", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.8816483914852142}, {"text": "text summarization", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7346632480621338}, {"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.9116320013999939}, {"text": "information extraction", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.8950087130069733}]}, {"text": "shows a document consisting of three events described by six different event mentions.", "labels": [], "entities": []}, {"text": "Among these event mentions, m 1 , m 2 and m 4 are coreferent, since they all correspond to the event of the KMT party electing anew party chief.", "labels": [], "entities": [{"text": "KMT party electing anew party chief", "start_pos": 108, "end_pos": 143, "type": "TASK", "confidence": 0.719558447599411}]}, {"text": "Similarly, m 3 and m 5 are also coreferent, while m 6 is not coreferent with any other event mentions.", "labels": [], "entities": []}, {"text": "An event mention consists of a trigger and zero or more arguments.", "labels": [], "entities": []}, {"text": "The trigger of an event mention is the word/phrase that is considered the most representative of the event, such as the word meeting form 3 or the word elected form . Triggers of coreferent event mentions must be related, that is, they should describe the same type of events.", "labels": [], "entities": []}, {"text": "For example, m 1 and m 3 cannot be coreferent, since their trigger words -elect and meeting -are not related.", "labels": [], "entities": []}, {"text": "Arguments are the participants of an event, each having its role.", "labels": [], "entities": []}, {"text": "For example, KMT is the AGENTargument and new party chief is the PATIENTargument of m 1 . Argument compatibility is an important linguistic condition for determining the coreferent status between two event mentions.", "labels": [], "entities": [{"text": "AGENTargument", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9649410843849182}, {"text": "PATIENTargument", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.988222062587738}]}, {"text": "Two arguments are incompatible if they do not correspond to the same real-world entity when they are expressed in the same level of specificity; Despite its importance, incorporating argument compatibility into event coreference systems is challenging due to the lack of sufficient labeled data.", "labels": [], "entities": []}, {"text": "Many existing works have relied on implementing argument extractors as upstream components and designing argument features that capture argument compatibility in event coreference resolvers.", "labels": [], "entities": [{"text": "event coreference resolvers", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.7123906910419464}]}, {"text": "However, the error introduced in each of the steps propagates through these resolvers and hinders their performance considerably.", "labels": [], "entities": []}, {"text": "In light of the aforementioned challenge, we propose a framework for transferring argument (in)compatibility knowledge to the event coreference resolution system, specifically by adopting the interactive inference network ( as our model structure.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.7210957507292429}]}, {"text": "The idea is as follows.", "labels": [], "entities": []}, {"text": "First, we train a network to determine whether the corresponding arguments of an event mention pair are compatible on automatically labeled training instances collected from a large unlabeled news corpus.", "labels": [], "entities": []}, {"text": "Second, to transfer the knowledge of argument (in)compatibility to an event coreference resolver, we employ the network (pre)trained in the previous step as a starting point and train it to determine whether two event mentions are coreferent on manually labeled event coreference corpora.", "labels": [], "entities": [{"text": "event coreference resolver", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6594821512699127}]}, {"text": "Third, we iteratively repeat the above two steps, where we use the learned coreference model to relabel the argument compatibility instances, retrain the network to determine argument compatibility, and use the resulting pretrained network to learn an event coreference resolver.", "labels": [], "entities": [{"text": "event coreference resolver", "start_pos": 252, "end_pos": 278, "type": "TASK", "confidence": 0.635045975446701}]}, {"text": "In essence, we mutually bootstrap the argument (in)compatibility determination task and the event coreference resolution task.", "labels": [], "entities": [{"text": "compatibility determination task", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7608165641625723}, {"text": "event coreference resolution", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7953485449155172}]}, {"text": "First, we utilize and leverage the argument (in)compatibility knowledge acquired from a large unlabeled corpus for event coreference resolution.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.8254564007123312}]}, {"text": "Second, we employ the interactive inference network as our model structure to iteratively learn argument compatibility and event coreference resolution.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.7582510014375051}]}, {"text": "Initially proposed for the task of natural language inference, the interactive inference network is suitable for capturing the semantic relations between word pairs.", "labels": [], "entities": []}, {"text": "Experimental results on the KBP coreference dataset show that this network architecture is also suitable for capturing the argument compatibility between event mentions.", "labels": [], "entities": [{"text": "KBP coreference dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.8732969562212626}]}, {"text": "Third, our model achieves state-of-the-art results on the KBP 2017 English dataset (, which confirms the effectiveness of our method.", "labels": [], "entities": [{"text": "KBP 2017 English dataset", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.9610821753740311}]}], "datasetContent": [{"text": "We follow the standard evaluation setup adopted in the official evaluation of the KBP event nugget detection and coreference task.", "labels": [], "entities": [{"text": "KBP event nugget detection", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.6218406558036804}, {"text": "coreference task", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.8488654792308807}]}, {"text": "This evaluation setup is based on four distinct scoring measures -MUC ( , B 3 (Bagga and Baldwin, 1998), CEAF e () and BLANC (Recasens and Hovy, 2011) -and the unweighted average of their F-scores (AVG-F).", "labels": [], "entities": [{"text": "MUC", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.8625689148902893}, {"text": "CEAF e", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8524413406848907}, {"text": "BLANC", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.994379460811615}, {"text": "F-scores", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9962691068649292}, {"text": "AVG-F)", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9642655551433563}]}, {"text": "We use AVG-F as the main evaluation measure when comparing system performances.", "labels": [], "entities": [{"text": "AVG-F", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9931718111038208}]}], "tableCaptions": [{"text": " Table 2: Examples of related triggers.", "labels": [], "entities": []}, {"text": " Table 3: Event coreference resolution results of our proposed system, compared with the biLSTM baseline model  and the current state-of-the-art system.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.825228234132131}, {"text": "biLSTM baseline model", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.8633061846097311}]}]}