{"title": [{"text": "Using Similarity Measures to Select Pretraining Data for NER", "labels": [], "entities": [{"text": "Similarity Measures", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.9669747352600098}, {"text": "NER", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9732705354690552}]}], "abstractContent": [{"text": "Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "However, the measure and impact of similarity between pre-training data and target task data are left to intuition.", "labels": [], "entities": [{"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9536837339401245}]}, {"text": "We propose three cost-effective measures to quantify different aspects of similarity between source pretraining and target task data.", "labels": [], "entities": []}, {"text": "We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.7879892041285833}]}, {"text": "Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pre-training data is dissimilar.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern neural architectures for NLP are highly effective when provided a large amount of labelled training data (.", "labels": [], "entities": []}, {"text": "However, a large labelled data set is not always readily accessible due to the high cost of expertise needed for labelling or even due to legal barriers.", "labels": [], "entities": []}, {"text": "Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (.", "labels": [], "entities": []}, {"text": "Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.9117149710655212}]}, {"text": "This produces dramatic improvements over a range of NLP tasks where appropriate unlabelled data is available.", "labels": [], "entities": []}, {"text": "However, there is still alack of systematic study on how to select appropriate data to pretrain word vectors or LMs.", "labels": [], "entities": []}, {"text": "We observe a range of heuristic strategies in the literature: (1) collecting a large amount of generic data, e.g., web crawl; (2) selecting data from a similar field (the subject matter of the content being discussed), e.g., biology (); and, (3) selecting data from a similar tenor (the participants in the discourse, their relationships to each other, and their purposes), e.g., Twitter, or online forums (.", "labels": [], "entities": []}, {"text": "In all these settings, the decision is based on heuristics and varies according to the individual's experience.", "labels": [], "entities": []}, {"text": "We also conducted a pilot study that suggests that the practitioner's intuition is to prioritise field over tenor (see Section 3).", "labels": [], "entities": []}, {"text": "Our overarching goal is to develop a costeffective approach that, given a NER data set, nominates the most suitable source data to pretrain word vectors or LMs from several options.", "labels": [], "entities": [{"text": "NER data set", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.8060552080472311}]}, {"text": "Our approach builds on the hypothesis that the more similar the source data is to the target data, the better the pretrained models are, all other aspects (such as source data size) being equal.", "labels": [], "entities": []}, {"text": "We propose using target vocabulary covered rate and language model perplexity to select pretraining data.", "labels": [], "entities": []}, {"text": "We also introduce anew measure based on the change from word vectors pretrained on source data to word vectors initialized from source data and then trained on target data.", "labels": [], "entities": []}, {"text": "Experiments leverage 30 data pairs from five source and six target NER data sets, each selected to provide a range of fields (i.e., biology, computer science, medications, local business) and tenors (i.e., encyclopedia articles, journal articles, experimental protocols, online reviews).", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as below: \u2022 We propose methods to quantitatively measure different aspects of similarity between source and target data sets and find that these measures are predictive of the impact of pretraining data on final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9816777110099792}]}, {"text": "To the best of our knowledge, this is the first systematic study to investigate LMs pretrained on various data sources.", "labels": [], "entities": []}, {"text": "1 \u2022 We find that it is important to consider tenor as well as field when selecting pretraining data, contrary to human intuitions.", "labels": [], "entities": []}, {"text": "\u2022 We show that models pretrained on a modest amount of similar data outperform pretrained models that take weeks to train over very large generic data.", "labels": [], "entities": []}], "datasetContent": [{"text": "To investigate the impact of source data on pretrained word vectors and LMs, we pretrain word vectors and LMs on different sources separately, then observe how the effectiveness of these pretrained models varies in different NER data sets.", "labels": [], "entities": []}, {"text": "We use the BiLSTM-CRF model, a state-of-theart model for sequence tagging tasks, as a supervised model for the target NER task.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7587783833344778}]}, {"text": "We follow the architecture proposed in (, except that we use two BiLSTM-layers and employ a CNN network to learn character-level representations (.", "labels": [], "entities": []}, {"text": "Micro average F 1 score is used to evaluate the performance of the tagger (.", "labels": [], "entities": [{"text": "Micro average F 1 score", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7899672627449036}]}, {"text": "Word vectors are pretrained using word2vec with its default hyper-parameter setting ( . In different experiments, we only replace the word embedding weights initialized byword vectors pretrained on different source data, then make these weights trained jointly with other model parameters.", "labels": [], "entities": []}, {"text": "The baseline is denoted as None in, where word embedding weights are randomly initialized.", "labels": [], "entities": []}, {"text": "LMs are pretrained using the architecture proposed by with hyperparameters in ().", "labels": [], "entities": []}, {"text": "The supervised model used for NER is the same BiLSTM-CRF model mentioned above, and we follow the approach proposed by to incorporate the pretrained LMs.", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9716511368751526}]}, {"text": "Note that these pretrained LMs are character-based.", "labels": [], "entities": []}, {"text": "Therefore, words in the target data set are first converted into a sequence of characters, and then fed into the LMs.", "labels": [], "entities": []}, {"text": "The contextualized representation of each word is generated using the outputs of all layers of the pretrained LMs, then injected to the input of the second BiL-STM layer of the supervised model.", "labels": [], "entities": []}, {"text": "Using proposed similarity measures, we first quantify the similarity between all source-target pairs (Section 7.1), then investigate how these measures can be used to predict the usefulness of pretraining data (Section 7.2).", "labels": [], "entities": []}, {"text": "Finally, we take the source data size into consideration, and observe its impact on the effectiveness of pretrained model on both similar and dissimilar source-target settings (Section 7.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens.", "labels": [], "entities": [{"text": "NER data sets", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.8516190052032471}]}, {"text": " Table 3: Similarity between source and target data sets (left), and the effectiveness of word vectors and LMs  pretrained using different sources for NER (right). Lower PPL or WVV values indicate higher similarity between  source and target, while higher TVC and TVcC values indicate higher similarity. None rows refer to the models  that word embedding weights are randomly initialized with no pretrained LMs. \u2206 shows absolute improvement.  We repeat every NER experiment 5 times, and report mean and standard deviation of test F 1 scores.", "labels": [], "entities": [{"text": "standard deviation of test F 1 scores", "start_pos": 503, "end_pos": 540, "type": "METRIC", "confidence": 0.7319380555834089}]}, {"text": " Table 4: Correlation coefficients between similarity  measures and the effectiveness of pretrained models.", "labels": [], "entities": []}, {"text": " Table 5: Comparison between our best performance  pretrained models and the publicly available ones,  which are pretrained on much larger corpora.", "labels": [], "entities": []}, {"text": " Table 6: Impact of hyper-parameter setting on the  effectiveness of pretrained word vectors. 'Opt' is  hyper-parameter setting proposed in (Chiu et al., 2016),  whereas 'Def' is the default setting in word2vec.", "labels": [], "entities": []}]}