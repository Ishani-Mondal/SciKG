{"title": [], "abstractContent": [{"text": "Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs).", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8514043092727661}]}, {"text": "To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora.", "labels": [], "entities": []}, {"text": "The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available fora very narrow selection of languages and domains.", "labels": [], "entities": []}, {"text": "In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions.", "labels": [], "entities": [{"text": "extraction of opinion target expressions", "start_pos": 141, "end_pos": 181, "type": "TASK", "confidence": 0.8137696981430054}]}, {"text": "We leverage multilingual word embed-dings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction.", "labels": [], "entities": [{"text": "OTE extraction", "start_pos": 175, "end_pos": 189, "type": "TASK", "confidence": 0.9609473943710327}]}, {"text": "Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language.", "labels": [], "entities": []}, {"text": "Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data.", "labels": [], "entities": []}, {"text": "Furthermore, we can increase this performance up to 87% of a base-line model trained on target language data by performing cross-lingual learning from multiple source languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been an increasing interest in developing sentiment analysis models that predict sentiment at a more fine-grained level than at the level of a complete document.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9056258201599121}]}, {"text": "A paradigm coined as Aspect-based Sentiment Analysis (ABSA) addresses this need by defining the sentiment expressed in a text relative to an opinion target (also called aspect).", "labels": [], "entities": [{"text": "Aspect-based Sentiment Analysis (ABSA)", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.7465779085954031}]}, {"text": "Consider the following example from a restaurant review: \" Moules were excellent , lobster ravioli was VERY salty ! \" In this example, there are two sentiment statements, one positive and one negative.", "labels": [], "entities": [{"text": "VERY", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9977086782455444}]}, {"text": "The positive one is indicated by the word \"excellent\" and is expressed towards the opinion target \"Moules\".", "labels": [], "entities": [{"text": "Moules", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.7723364233970642}]}, {"text": "The second, negative sentiment, is indicated by the word \"salty\" and is expressed towards the \"lobster ravioli\".", "labels": [], "entities": []}, {"text": "A key task within this fine-grained sentiment analysis consists of identifying so called opinion target expressions (OTE).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7768484354019165}]}, {"text": "To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora.", "labels": [], "entities": []}, {"text": "In this paper, we are concerned with how to transfer classifiers trained on one domain to another domain.", "labels": [], "entities": []}, {"text": "In particular, we focus on the transfer of models across languages to alleviate the need for multilingual training data.", "labels": [], "entities": []}, {"text": "We propose a model that is capable of accurate zero-shot cross-lingual OTE extraction, thus reducing the reliance on annotated data for every language.", "labels": [], "entities": [{"text": "OTE extraction", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.8646588027477264}]}, {"text": "Similar to, our model leverages multilingual word embeddings () that share a common vector space across various languages.", "labels": [], "entities": []}, {"text": "The shared space allows us to transfer a model trained on source language data to predict OTEs in a target language for which no (i.e. zero-shot setting) or only small amounts of data are available, thus allowing to apply our model to under-resourced languages.", "labels": [], "entities": []}, {"text": "Our main contributions can be summarized as follows: \u2022 We present the first approach for zero-shot cross-lingual opinion target extraction and achieve up to 87% of the performance of a monolingual baseline.", "labels": [], "entities": [{"text": "zero-shot cross-lingual opinion target extraction", "start_pos": 89, "end_pos": 138, "type": "TASK", "confidence": 0.6535825729370117}]}, {"text": "\u2022 We investigate the benefit of using multi-ple source languages for cross-lingual learning and show that we can improve by 6 to 8 points in F 1 -Score compared to a model trained on a single source language.", "labels": [], "entities": [{"text": "F 1 -Score", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9784954488277435}]}, {"text": "\u2022 We investigate the benefit of augmenting the zero-shot approach with additional data points from the target language.", "labels": [], "entities": []}, {"text": "We observe that we can save hundreds of annotated data points by employing a cross-lingual approach.", "labels": [], "entities": []}, {"text": "\u2022 We compare two methods for obtaining cross-lingual word embeddings on the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we investigate the proposed zeroshot cross-lingual approach and evaluate it on the widely used dataset of Task 5 of the SemEval 2016 workshop.", "labels": [], "entities": [{"text": "SemEval 2016 workshop", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.6476859847704569}]}, {"text": "With our evaluation, we answer the following research questions: RQ1: To what degree is the model capable of performing OTE extraction for unseen languages?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.6969075202941895}, {"text": "OTE extraction for unseen languages", "start_pos": 120, "end_pos": 155, "type": "TASK", "confidence": 0.8748176693916321}]}, {"text": "RQ2: Is there a benefit in training on more than one source language?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8490054607391357}]}, {"text": "RQ3: What improvements can be expected when a small amount of samples for the target language are available?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8961568474769592}]}, {"text": "RQ4: How big is the impact of the used alignment method on the OTE extraction performance?", "labels": [], "entities": [{"text": "RQ4", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.911250650882721}, {"text": "OTE extraction", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.9485600888729095}]}, {"text": "Before we answer these questions, we give a brief overview over the used datasets and resources.", "labels": [], "entities": []}, {"text": "As part of Task 5 of the SemEval 2016 workshop (), a collection of datasets for aspect-based sentiment analysis on various languages and domains was published.", "labels": [], "entities": [{"text": "SemEval 2016 workshop", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.888279378414154}, {"text": "aspect-based sentiment analysis", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6508439580599467}]}, {"text": "Due to its relatively large number of samples and high coverage of languages and domains, the datasets are commonly used to evaluate ABSA approaches.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 133, "end_pos": 137, "type": "TASK", "confidence": 0.9924249649047852}]}, {"text": "To answer our research questions, we make use of a selection of the available datasets.", "labels": [], "entities": []}, {"text": "We evaluate our cross-lingual approach on the available datasets for the restaurant domain for the 5 languages Dutch (nl), English (en), Russian (ru), Spanish (es) and Turkish (tr) . gives a brief overview of the used datasets.", "labels": [], "entities": []}, {"text": "In all our experiments, we report F 1 -scores for the extracted opinion target expressions computed on exact matches of the character spans as in the original SemEval task (.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.980737030506134}]}, {"text": "As described in Section 2.2, our model relies on pretrained multilingual embeddings.", "labels": [], "entities": []}, {"text": "For both SVD-aligned and ADV-aligned, we use the embeddings as provided by the original authors.", "labels": [], "entities": []}, {"text": "However, we restrict our vocabulary to the most frequent 50,000 words per language to reduce memory consumption.", "labels": [], "entities": []}, {"text": "For all experiments, we fix our model architecture to 5 convolution layers with each having a kernel size of 3, a dimensionality of 300 units and a ReLU activation function.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9125977158546448}]}, {"text": "The penultimate feed-forward layer has 300 dimensions and a ReLU activation, as well.", "labels": [], "entities": [{"text": "ReLU activation", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.9119815826416016}]}, {"text": "We apply dropout () on the word embedding layer with a rate of 0.3 and between all other layers with 0.5.", "labels": [], "entities": []}, {"text": "The word embeddings and the penultimate layer are L1-regularized).", "labels": [], "entities": []}, {"text": "The network's parameters are optimized using the stochastic optimization technique Adam (.", "labels": [], "entities": []}, {"text": "We optimize the number of training epochs for each model using early stopping () but do not tune other hyperparameters of our models.", "labels": [], "entities": []}, {"text": "We always pick 20% of our available training data for the validation process.", "labels": [], "entities": [{"text": "validation", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.9848015904426575}]}, {"text": "For the zero-shot scenario, this entails that we optimize the number of epochs on the source language and not on the target language to simulate true zero-shot learning.", "labels": [], "entities": []}, {"text": "As appearing in the respective embedding files.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the SemEval 2016 ABSA dataset  for the restaurant domain.", "labels": [], "entities": [{"text": "SemEval 2016 ABSA dataset", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.6850648298859596}]}, {"text": " Table 2: Zero-shot results for cross-lingual learning  from multiple source languages to a target language.", "labels": [], "entities": []}, {"text": " Table 3: Overview of the current state-of-the-art for  opinion target extraction for 5 languages. Our model is  trained on the combined training data of all languages  and evaluated on the respective test datasets. The row  marked with * is the baseline provided by the work- shop organizers. To our knowledge, no better model is  published for Russian and Turkish.", "labels": [], "entities": [{"text": "opinion target extraction", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.682168702284495}]}]}