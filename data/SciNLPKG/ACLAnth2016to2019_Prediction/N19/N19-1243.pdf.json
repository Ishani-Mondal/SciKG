{"title": [{"text": "Old is Gold: Linguistic Driven Approach for Entity and Relation Linking of Short Text", "labels": [], "entities": [{"text": "Old", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9277915954589844}, {"text": "Entity and Relation Linking of Short Text", "start_pos": 44, "end_pos": 85, "type": "TASK", "confidence": 0.7955925038882664}]}], "abstractContent": [{"text": "Short texts challenge NLP tasks such as named entity recognition, disambiguation, linking and relation inference because they do not provide sufficient context or are partially mal-formed (e.g. wrt.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.5994002024332682}, {"text": "relation inference", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7637952864170074}]}, {"text": "capitalization, long tail entities , implicit relations).", "labels": [], "entities": []}, {"text": "In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph.", "labels": [], "entities": []}, {"text": "Falcon overcomes the challenges of short text using a lightweight linguistic approach relying on a background knowledge graph.", "labels": [], "entities": []}, {"text": "Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of English morphology (e.g. compounding, headword identification) and utilizes an extended knowledge graph created by merging entities and relations from various knowledge sources.", "labels": [], "entities": [{"text": "joint entity and relation linking of a short text", "start_pos": 16, "end_pos": 65, "type": "TASK", "confidence": 0.7917091449101766}, {"text": "headword identification", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.8060242831707001}]}, {"text": "It uses the context of entities for finding relations and does not require training data.", "labels": [], "entities": []}, {"text": "Our empirical study using several standard benchmarks and datasets show that Falcon significantly outper-forms state-of-the-art entity and relation linking for short text query inventories.", "labels": [], "entities": [{"text": "relation linking", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.706298440694809}]}], "introductionContent": [{"text": "Entity Linking (EL) task annotates surface forms in the text with the corresponding reference mentions in knowledge bases such as Wikipedia.", "labels": [], "entities": [{"text": "Entity Linking (EL) task annotates surface forms", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7992052137851715}]}, {"text": "It involves the two sub-tasks, i.e. Named Entity Recognition and Disambiguation (NER and NED) tasks.", "labels": [], "entities": [{"text": "Named Entity Recognition and Disambiguation", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.7271989822387696}]}, {"text": "The state of the art contains considerable research body for EL from text to its Wikipedia mention First three authors have equal contribution.", "labels": [], "entities": [{"text": "EL", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.6266906261444092}]}, {"text": "With the emergence of Knowledge Graphs (KGs) which represent data in a higher structured and semantic format such as DBpedia (, and Wikidata) that utilize Wikipedia as familiar knowledge source, retrieval-based applications such as question answering (QA) systems or keyword-based semantic search systems are empowered to provide more cognitive capabilities.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 232, "end_pos": 255, "type": "TASK", "confidence": 0.8503377079963684}]}, {"text": "Entity linking is a crucial component fora variety of applications built on knowledge graphs.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7678126394748688}]}, {"text": "For instance, an ideal NED tool on DBpedia recognizes the entities embedded in the question 'Who wrote the book The Pillars of The Earth?' and links them to the corresponding DBpedia entity (e.g. 'Pillars of The Earth' to dbr:The_Pillars_of_the_Earth) . Another important NLP task is relation linking; it is about linking surface forms in text representing a relation to equivalent relations (predicates) of a KG.", "labels": [], "entities": [{"text": "relation linking", "start_pos": 284, "end_pos": 300, "type": "TASK", "confidence": 0.8354250490665436}]}, {"text": "In our example question, an ideal relation linking (RL) tool links 'wrote' to dbo:author 2 . There are existing approaches which address EL and RL tasks either jointly or independently ().", "labels": [], "entities": []}, {"text": "However, they mostly fail in case of short text (e.g. question or key words based query) because the short text does not provide sufficient context which is essential for the disambiguation process.", "labels": [], "entities": []}, {"text": "More importantly, a short text is often malformed meaning the text is incomplete, inexpressive, or implicit which is the case, particularly for relations in short sentences.", "labels": [], "entities": []}, {"text": "In this paper, we contribute to proposing a novel approach for jointly linking entities and relations within a short text into the entities and relations of DBpedia KG.", "labels": [], "entities": [{"text": "DBpedia KG", "start_pos": 157, "end_pos": 167, "type": "DATASET", "confidence": 0.9542420208454132}]}, {"text": "This approach is robust to the challenges of short text, and moreover, it is efficient.", "labels": [], "entities": []}, {"text": "Existing approaches and systems for NER, NED, EL, and RL resort to machine learning and deep learning approaches that require a large training data).", "labels": [], "entities": []}, {"text": "These approaches achieve high performance on data similar to seen data.", "labels": [], "entities": []}, {"text": "For instance, Singh et al.", "labels": [], "entities": []}, {"text": "(2018c) evaluated 20 NED tools for question answering over the DBpedia KG including TagMe, DBpedia Spotlight (Mendes et al., 2011),, and several APIs released by industry including Ambiverse (Ambiverse, 2018), TextRazor.", "labels": [], "entities": [{"text": "question answering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7619556784629822}, {"text": "DBpedia KG", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.9412178695201874}, {"text": "Ambiverse (Ambiverse, 2018)", "start_pos": 181, "end_pos": 208, "type": "DATASET", "confidence": 0.7690958976745605}, {"text": "TextRazor", "start_pos": 210, "end_pos": 219, "type": "DATASET", "confidence": 0.8931955695152283}]}, {"text": "Among all, TagMe reports the highest F-score (0.67) over the complex question answering dataset LC-QuAD (TagMe is one of the top performing tools with an F-score of 0.91 on the generic WikiDisamb30 dataset).", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.99918133020401}, {"text": "question answering dataset LC-QuAD", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.6388329342007637}, {"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9899573922157288}, {"text": "WikiDisamb30 dataset", "start_pos": 185, "end_pos": 205, "type": "DATASET", "confidence": 0.8975124657154083}]}, {"text": "Please be noted that TagMe was explicitly released for short text.", "labels": [], "entities": [{"text": "TagMe", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.917759358882904}]}, {"text": "However, when the input text is from a domain different from the training domain, its performance significantly falls down.", "labels": [], "entities": []}, {"text": "Regarding the performance of various RL approaches such as ReMatch (), SIBKB ( ) is still low concerning accuracy and run-time even if they are purposefully developed fora particular domain or task.", "labels": [], "entities": [{"text": "RL", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9668329358100891}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9991881251335144}]}, {"text": "This deficiency is due to disregarding the context of the entities (.", "labels": [], "entities": []}, {"text": "Therefore, when aiming for annotating entities and relations of short text, it is important to develop an approach which a) is agnostic of the requirement of large training data and b) jointly links entities and relations to its KG equivalence.", "labels": [], "entities": []}, {"text": "We target the problem of joint entity and relation linking within short text using the DBpedia KG as background knowledge.", "labels": [], "entities": [{"text": "joint entity and relation linking within short text", "start_pos": 25, "end_pos": 76, "type": "TASK", "confidence": 0.7120334468781948}, {"text": "DBpedia KG", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9702059626579285}]}, {"text": "We propose a novel approach that resorts to several fundamental principles of English morphology such as compounding, righthand rule for headword identification and utilizes an extended knowledge graph created by merging entities and relations from various knowledge sources.", "labels": [], "entities": [{"text": "headword identification", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.90159872174263}]}, {"text": "The approach focuses on capturing semantics underlying the input text by using the context of entities for finding relations and does not require any training data.", "labels": [], "entities": []}, {"text": "Albeit simple, to the best of our knowledge, the combination of strategies and optimization of our approach is unique.", "labels": [], "entities": []}, {"text": "Our evaluations show that it leads to substantial gains in recall, precision, and F-score on various benchmarks and domains.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9996161460876465}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9995043277740479}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9992477893829346}]}, {"text": "Falcon is available as an open Web API 3 , and its source code is released to ensure reproducibility.", "labels": [], "entities": []}, {"text": "Another open source contribution is an extended knowledge graph which we built by merging information from several sources, e.g. DBpedia, Wikidata, Oxford dictionary, and Wordnet.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9381818175315857}, {"text": "Oxford dictionary", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.9680583477020264}, {"text": "Wordnet", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9741194248199463}]}, {"text": "These contributions are in our public Github . The paper is structured as follows: the next sec-tion motivates our work by illustrating several limitations of state of the art over short text.", "labels": [], "entities": []}, {"text": "Section 3 detailed our approach and we present evaluation results in Section 4.", "labels": [], "entities": []}, {"text": "We describe related literature in Section 5 and Section 6 concludes our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a local laptop machine, with eight cores and 16GB RAM running Ubuntu 18.04 for implementation.", "labels": [], "entities": []}, {"text": "Falcon is deployed as public API on a server with 723GB RAM, 96 cores (Intel(R) Xeon(R) Platinum 8160 CPU with 2.10GHz) running Ubuntu 18.04.", "labels": [], "entities": []}, {"text": "This API is used for calculating all the results.", "labels": [], "entities": []}, {"text": "The EL systems have been evaluated on different settings in literature, therefore to provide a fair evaluation we utilize Gerbil (, which is a benchmarking framework for EL systems and integrated Falcon API into the Gerbil architecture.", "labels": [], "entities": []}, {"text": "We report macro precision (P), macro recall (R), and macro F-score 7 in the tables.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.9154874682426453}, {"text": "recall (R)", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9625836759805679}, {"text": "F-score 7", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9163494110107422}]}, {"text": "Falcon average run time is 1.9 seconds per question.", "labels": [], "entities": []}, {"text": "Gerbil does not benchmark RL systems; therefore, RL https://github.com/dice-group/gerbil/ wiki/Precision,-Recall-and-F1-measure systems are benchmarked using Frankenstein platform ().", "labels": [], "entities": [{"text": "Precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9214383363723755}]}, {"text": "Our code, Extended KG, and data is in Github.", "labels": [], "entities": []}, {"text": "We employ two distinct datasets: 1) the LC-QuAD () dataset comprises 5,000 complex questions for DBpedia (80 percent questions are with more than one entity and relation) where average question length is 12.29 words.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.8948823809623718}]}, {"text": "2) QALD-7 () is the most popular benchmarking dataset for QA over DBpedia comprising 215 questions.", "labels": [], "entities": []}, {"text": "In QALD, the average question length is 7.41 words and over 50% of the questions include a single entity and relation.", "labels": [], "entities": []}, {"text": "For our linguistic based approach, we randomly selected 100 questions each from SimpleQuestions dataset () and complex questions 9 for the formation of rules.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 80, "end_pos": 103, "type": "DATASET", "confidence": 0.9063577353954315}]}, {"text": "Performance Evaluation summarizes Falcon's performance compared to state-of-the-art systems integrated in Gerbil.", "labels": [], "entities": []}, {"text": "For the QALD and LC-QuAD datasets, Falcon significantly outperforms the baseline.", "labels": [], "entities": [{"text": "QALD", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8104763627052307}, {"text": "LC-QuAD datasets", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.7881805002689362}]}, {"text": "Similar observations are made for relation linking, where the performance of Falcon is approximately twice as high as the next best competitor on all datasets (cf.).", "labels": [], "entities": [{"text": "relation linking", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.9721968472003937}]}, {"text": "Success cases of Falcon: Falcon overcomes several major issues of short text such as capitalization of surface forms, derived word forms of relation labels and successfully handles long tail entities.", "labels": [], "entities": []}, {"text": "For entity linking, we achieve slightly better performance on LC-QuAD than QALD.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8295613825321198}]}, {"text": "This is due to the fact that LC-QuAD questions mostly contain more than one entity and relation and thus provide more context to understand the short text.", "labels": [], "entities": []}, {"text": "Also, major failure cases of state-of-the-art EL systems over these datasets are due to the short length and limitation to exploit the context.", "labels": [], "entities": []}, {"text": "For example the question 'Give me the count of all people who ascended a peak in California.'", "labels": [], "entities": []}, {"text": "(dbr:California is correct entity), TagMe provides two entities: dbr:California (for surface form California) and dbr:Give_In_to_Me (for \"Give me\" For example, in question 'How many writers worked on the album Main Course?', the expected entity is dbr:Main_Course.", "labels": [], "entities": [{"text": "TagMe", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9173483848571777}, {"text": "Main_Course", "start_pos": 252, "end_pos": 263, "type": "DATASET", "confidence": 0.8373434543609619}]}, {"text": "However, Falcon returns dbr:Critters_2:_The_Main_Course.", "labels": [], "entities": [{"text": "Critters_2:_The_Main_Course", "start_pos": 28, "end_pos": 55, "type": "DATASET", "confidence": 0.6809900734159682}]}, {"text": "This is caused by compounding and the resulting token for this question was 'album Main Course'.", "labels": [], "entities": [{"text": "album", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.7242862582206726}, {"text": "Main", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.924971878528595}]}, {"text": "For the same question Falcon correctly links the relations.", "labels": [], "entities": []}, {"text": "We further analyzed failure cases of Falcon for RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.7762249708175659}]}, {"text": "We found that more than half of the questions which were unanswered have implicit relations.", "labels": [], "entities": []}, {"text": "For example, for the question 'In what city is the Heineken brewery?' with the two relations dbo:locationCity and dbo:manufacturer, Falcon returns dbo:city as relation.", "labels": [], "entities": []}, {"text": "There are few types of questions ('Count all the scientologists.')", "labels": [], "entities": []}, {"text": "for which Falcon fails both for EL and RL tasks.", "labels": [], "entities": [{"text": "EL", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.7934268116950989}]}, {"text": "This question is relatively short and requires reasoning to provide correct entities and relations (dbr:Scientology and dbo:religion).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the Falcon Framework com- pared to various entity linking tools.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the Falcon Framework com- pared to various Relation Linking tools.", "labels": [], "entities": [{"text": "Relation Linking", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.9251076281070709}]}, {"text": " Table 2.  Failure cases of Falcon: There are few EL  cases where Falcon fails.", "labels": [], "entities": []}]}