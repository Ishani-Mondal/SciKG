{"title": [{"text": "A Modeling Study of the Effects of Surprisal and Entropy in Perceptual Decision Making of an Adaptive Agent", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9193313717842102}, {"text": "Perceptual Decision Making of an Adaptive Agent", "start_pos": 60, "end_pos": 107, "type": "TASK", "confidence": 0.7218997648784092}]}], "abstractContent": [{"text": "Processing difficulty in online language comprehension has been explained in terms of sur-prisal and entropy reduction.", "labels": [], "entities": []}, {"text": "Although both hypotheses have been supported by experimental data, we do not fully understand their relative contributions on processing difficulty.", "labels": [], "entities": []}, {"text": "To develop a better understanding, we propose a mechanistic model of perceptual decision making that interacts with a simulated task environment with temporal dynamics.", "labels": [], "entities": []}, {"text": "The proposed model collects noisy bottom-up evidence over multiple timesteps, integrates it with its top-down expectation, and makes perceptual decisions, producing processing time data directly without relying on any linking hypothesis.", "labels": [], "entities": []}, {"text": "Temporal dynamics in the task environment was determined by a simple finite-state grammar, which was designed to create the situations where the surprisal and en-tropy reduction hypotheses predict different patterns.", "labels": [], "entities": []}, {"text": "After the model was trained to maximize rewards, the model developed an adap-tive policy and both surprisal and entropy effects were observed especially in a measure reflecting earlier processing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past decades, computational models of sentence comprehension have improved our understanding of processing difficulty arising in online language comprehension.", "labels": [], "entities": []}, {"text": "It has been discovered that information-theoretic complexity metrics can predict processing difficulty (for review, see.", "labels": [], "entities": []}, {"text": "The surprisal hypothesis proposes processing difficulty of a word wk in a context w 1:k\u22121 is proportional to its surprisal, \u2212 log p(w k |w 1:k\u22121 ).", "labels": [], "entities": []}, {"text": "proved that surprisal is equivalent to Kullback-Leibler divergence between the probability distributions over parse trees T before and after observing the word wk , D KL (P (T |w 1:k )P (T |w 1:k\u22121 )).", "labels": [], "entities": [{"text": "surprisal", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9869895577430725}]}, {"text": "On the other hand, the entropy reduction hypothesis claims that processing difficulty is proportional to a non-negative amount of entropy reduced after observing a word wk : max(H(S|w 1:k\u22121 ) \u2212 H(S|w 1:k ), 0) where S is a random variable of sentences.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9381090402603149}]}, {"text": "It is not clear why the language processing system works insensitive to negative entropy changes.", "labels": [], "entities": []}, {"text": "Both hypotheses have been supported by experimental data (for surprisal, see; for entropy reduction, see.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8646145164966583}]}, {"text": "Some behavioral studies reported both effects of surprisal and entropy reduction and in such cases, the surprisal effect was much stronger than the entropy reduction effect.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7678832411766052}]}, {"text": "However, we do not have comprehensive understanding of their relative contribution to processing load.", "labels": [], "entities": []}, {"text": "Empirically, the estimation of surprisal and entropy values requires a language model, the quality of which depends on many factors (e.g., the corpus size, the model type) (c.f., argued the effect of surprisal was robust when the measures were estimated using a wide range of language models with different qualities).", "labels": [], "entities": []}, {"text": "Also surprisal and entropy values tend to be highly correlated in natural languages, which makes it difficult to tease apart their relative roles in online language processing.", "labels": [], "entities": []}, {"text": "To avoid these empirical problems, we introduce a simple experimental paradigm, which combines two well-established paradigms: saccade target selection) and artificial language paradigm (), both of which have been used to answer related questions.", "labels": [], "entities": []}, {"text": "In the artificial language paradigm, we design a language such that it has some distributional properties of interest.", "labels": [], "entities": []}, {"text": "For example, we can design a grammar in which the surprisal and the entropy reduction hypotheses make different predictions.", "labels": [], "entities": []}, {"text": "For example, used a simple finite-state grammar to create such situation and discussed alternative accounts of processing difficulty.", "labels": [], "entities": []}, {"text": "In the present study, we used a variant of their grammar (see.", "labels": [], "entities": []}, {"text": "Due to the simplicity of the grammar, entropy and entropy reduction measures are perfectly correlated.", "labels": [], "entities": []}, {"text": "When we discuss the effect of those measures, we will refer to it as the entropy effect but we are neutral in whether it should be interpreted as the effect of entropy or the effect of entropy reduction; we reserve the question for future work.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 185, "end_pos": 202, "type": "TASK", "confidence": 0.6675755083560944}]}, {"text": "Figure 1: Model architecture.", "labels": [], "entities": []}, {"text": "The model consists of two modules: perception module and decision making module.", "labels": [], "entities": [{"text": "decision making", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.8442399501800537}]}, {"text": "Equipped with a perfect language model, the perception module (implemented as a Hidden Markov Model) integrates noisy inputs from environment with its top-down expectation.", "labels": [], "entities": []}, {"text": "The decision making module (implemented as a neural network with the Actor Critic architecture) makes an action based on the output of the perception module.", "labels": [], "entities": [{"text": "decision making", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7942469418048859}]}, {"text": "To develop a better understanding, we propose a mechanistic model of perceptual decision making and investigate its behavior in a simulated task environment with temporal dynamics, focusing on the effects of surprisal and/or entropy.", "labels": [], "entities": []}, {"text": "presents the architecture of the model and how it interacts with the task environment.", "labels": [], "entities": []}, {"text": "It consists of two components: the perception module at the bottom collects noisy bottom-up evidence from the task environment and updates its state (expressed in [posterior] probability distributions).", "labels": [], "entities": []}, {"text": "The decision making module at the top monitors the state of the perception module and makes an action (i.e., decision), which will update the state of the task environment.", "labels": [], "entities": [{"text": "decision making", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7462869882583618}]}, {"text": "The design of the perception module was inspired by that investigated a related research question.", "labels": [], "entities": []}, {"text": "Unlike their model, we used reinforcement learning to let the agent develop an optimal policy.", "labels": [], "entities": []}, {"text": "The main contribution of the present study is that we propose a full cognitive architecture that performs perceptual decision making, which we argue shares a core computational problem of uncertainty management with online language comprehension tasks (e.g., self-paced reading) and investigate the optimal behavior by exploring an unrestricted decision policy space.", "labels": [], "entities": [{"text": "decision making", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7350639998912811}, {"text": "uncertainty management", "start_pos": 188, "end_pos": 210, "type": "TASK", "confidence": 0.7571298778057098}]}, {"text": "In the following sections, we will present each component in in detail.", "labels": [], "entities": []}, {"text": "In Discussion, we conclude.", "labels": [], "entities": []}, {"text": "The events occurring at two sample trials are shown.", "labels": [], "entities": []}, {"text": "The agent is asked to \"look at\" the target (color dot) as quickly and accurately as possible.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Distributional information for unique  context-target combinations. p targ represents the  unigram target probability.", "labels": [], "entities": []}, {"text": " Table 4: Different module settings. \u03b7 noise determines  the amount of memory noise while 1/\u03ba determines the  amount of input noise. We fixed \u03b1 (false negative rate)  and \u03b2 (false positive rate) to 0.05 in this study.", "labels": [], "entities": []}]}