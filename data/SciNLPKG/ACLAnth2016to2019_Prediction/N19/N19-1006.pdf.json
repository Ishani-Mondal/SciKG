{"title": [{"text": "Pre-training on High-Resource Speech Recognition Improves Low-Resource Speech-to-Text Translation", "labels": [], "entities": [{"text": "High-Resource Speech Recognition Improves Low-Resource Speech-to-Text Translation", "start_pos": 16, "end_pos": 97, "type": "TASK", "confidence": 0.6828790434769222}]}], "abstractContent": [{"text": "We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST.", "labels": [], "entities": [{"text": "direct speech-to-text translation (ST)", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.8257027367750803}, {"text": "automatic speech recognition (ASR) task", "start_pos": 163, "end_pos": 202, "type": "TASK", "confidence": 0.8205158965928214}]}, {"text": "We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish-English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available.", "labels": [], "entities": [{"text": "ST", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9172045588493347}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9985785484313965}]}, {"text": "Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement , despite the fact that the shared language in these tasks is the target language text, not the source language audio.", "labels": [], "entities": []}, {"text": "Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST.", "labels": [], "entities": [{"text": "ASR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9309926629066467}]}, {"text": "Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.", "labels": [], "entities": [{"text": "Mboshi-French", "start_pos": 154, "end_pos": 167, "type": "DATASET", "confidence": 0.49215802550315857}, {"text": "ST", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.692768931388855}, {"text": "BLEU", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9986425042152405}]}], "introductionContent": [{"text": "Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered; or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language.", "labels": [], "entities": [{"text": "Speech-to-text Translation (ST)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8624371230602265}]}, {"text": "Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8571542382240296}, {"text": "machine translation (MT)", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.8217596113681793}, {"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.9232825040817261}]}, {"text": "These resources are often unavailable for low-resource languages, but for our potential applications, there maybe some source language audio paired with target language text translations.", "labels": [], "entities": []}, {"text": "In these scenarios, end-to-end ST is appealing.", "labels": [], "entities": [{"text": "ST", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.5815028548240662}]}, {"text": "Recently, showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9859504103660583}]}, {"text": "But this result required over 150 hours of translated audio for training, still a substantial resource requirement.", "labels": [], "entities": []}, {"text": "By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9835551381111145}]}, {"text": "Other low-resource systems have similarly low accuracies (.", "labels": [], "entities": []}, {"text": "To improve end-to-end ST in low-resource settings, we can try to leverage other data resources.", "labels": [], "entities": [{"text": "ST", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8929717540740967}]}, {"text": "For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (.", "labels": [], "entities": [{"text": "ST", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.8926303386688232}]}, {"text": "But source language transcriptions are unlikely to be available in our scenarios of interest.", "labels": [], "entities": []}, {"text": "Could we improve low-resource ST by leveraging data from a high-resource language?", "labels": [], "entities": []}, {"text": "For ASR, training a single model on multiple languages can be effective for all of them ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9900025129318237}]}, {"text": "For MT, transfer learning has been very effective: pretraining a model fora high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9939653277397156}, {"text": "transfer learning", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.959674209356308}]}, {"text": "Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model's parameters for low-resource ST.", "labels": [], "entities": [{"text": "ASR task", "start_pos": 219, "end_pos": 227, "type": "TASK", "confidence": 0.9087261855602264}]}, {"text": "We first test our approach using Spanish as the source language and English as the target.", "labels": [], "entities": []}, {"text": "After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only 10.8 for an ST model without ASR pre-training.", "labels": [], "entities": [{"text": "ASR", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9291422963142395}, {"text": "BLEU score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9844876229763031}]}, {"text": "Analyzing this result, we discover that the main benefit of pre-training arises from the transfer of the encoder parameters, which model the input acoustic signal.", "labels": [], "entities": []}, {"text": "In fact, this effect is so strong that we also obtain improvements by pre-training on a language that differs from both the source and the target: pre-training on French and fine-tuning on SpanishEnglish.", "labels": [], "entities": []}, {"text": "We hypothesize that pre-training the encoder parameters, even on a different language, allows the model to better learn about linguistically meaningful phonetic variation while normalizing over acoustic variability such as speaker and channel differences.", "labels": [], "entities": []}, {"text": "We conclude that the acousticphonetic learning problem, rather than translation itself, is one of the main difficulties in low-resource ST.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9585565328598022}]}, {"text": "A final set of experiments confirm that ASR pretraining also helps on another language pair where the input is truly low-resource: Mboshi-French.", "labels": [], "entities": [{"text": "ASR pretraining", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.9332241117954254}]}], "datasetContent": [{"text": "We report BLEU () for all our models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9711697697639465}]}, {"text": "In low-resource settings, BLEU scores tend to below, difficult to interpret, and poorly correlated with model performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9985259175300598}]}, {"text": "This is because BLEU requires exact four-gram matches only, but low four-gram accuracy may obscure a high unigram accuracy and inexact translations that partially capture the semantics of an utterance, and these can still be very useful in situations like language documentation and crisis response.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9884188771247864}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9536389708518982}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8542619347572327}, {"text": "language documentation", "start_pos": 256, "end_pos": 278, "type": "TASK", "confidence": 0.7215763032436371}, {"text": "crisis response", "start_pos": 283, "end_pos": 298, "type": "TASK", "confidence": 0.7004523277282715}]}, {"text": "Therefore, we also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9777795076370239}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9980177879333496}]}, {"text": "To compute these scores, we use METEOR (Lavie and Agarwal, 2007) with default settings for English and French.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9766573905944824}]}, {"text": "For example, METEOR assigns \"eat\" a recall of 1 against reference \"eat\" and a recall of 0.8 against reference \"feed\", which it considers a synonym match.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.776210367679596}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9973347783088684}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9962859153747559}]}, {"text": "We also include evaluation scores fora naive baseline model that predicts the K most frequent words of the training set as a bag of words for each test utterance.", "labels": [], "entities": []}, {"text": "We set K to be the value at which precision/recall are most similar, which is always between 5 and 20 words.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9988721013069153}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9628522992134094}]}, {"text": "This provides an empirical lower bound on precision and recall, since we would expect any usable model to outperform a system that does not even depend on the input utterance.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9994303584098816}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9992042183876038}]}, {"text": "We do not compute BLEU for these baselines, since they do not predict sequences, only bags of words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9983805418014526}]}, {"text": "We denote each ASR model by L-Nh, where L is a language code and N is the size of the training set in hours.", "labels": [], "entities": [{"text": "ASR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.973206639289856}]}, {"text": "For example, en300h denotes an English ASR model trained on 300 hours of data.", "labels": [], "entities": []}, {"text": "Training ASR models for state-of-the-art performance requires substantial hyper-parameter tuning and long training times.", "labels": [], "entities": [{"text": "ASR", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9796261191368103}]}, {"text": "Since our goal is simply to see whether pre-training is useful, we stopped pretraining our models after around 30 epochs (3 days) to focus on transfer experiments.", "labels": [], "entities": []}, {"text": "As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain 16% WER on Switchboard train-dev, and 22.7% WER on the French Globalphone dev set.", "labels": [], "entities": [{"text": "ASR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.977812647819519}, {"text": "WER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.995241641998291}, {"text": "WER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9936894178390503}, {"text": "French Globalphone dev set", "start_pos": 160, "end_pos": 186, "type": "DATASET", "confidence": 0.9359286576509476}]}, {"text": "We believe that better ASR pre-training may produce better ST results, but we leave this for future work.", "labels": [], "entities": [{"text": "ASR pre-training", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.8995660245418549}, {"text": "ST", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.8798252940177917}]}], "tableCaptions": [{"text": " Table 1: Word Error Rate (WER, in %) for the ASR  models used as pretraining, computed on Switchboard  train-dev for English and Globalphone dev for French.", "labels": [], "entities": [{"text": "Word Error Rate (WER", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8644566178321839}, {"text": "Switchboard  train-dev", "start_pos": 91, "end_pos": 113, "type": "DATASET", "confidence": 0.8438066840171814}]}, {"text": " Table 4: Fisher dev set BLEU scores for sp-en-20h.  baseline: model without transfer learning. Last two  columns: Using encoder parameters from French ASR  (+fr-20h), and English ASR (+en-20h).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9888122081756592}]}, {"text": " Table 5: Mboshi-to-French translation scores, with and  without ASR pre-training. Pr. is the precision, and  Rec. the recall score. fr-top-8w and fr-top-10w are  naive baselines that, respectively, predict the 8 or 10  most frequent training words. For en + fr, we use en- coder parameters from en-300h and attention+decoder  parameters from fr-20h", "labels": [], "entities": [{"text": "Mboshi-to-French translation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5941824316978455}, {"text": "Pr.", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9591431021690369}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995567202568054}, {"text": "Rec", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9349249005317688}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9987888932228088}]}]}