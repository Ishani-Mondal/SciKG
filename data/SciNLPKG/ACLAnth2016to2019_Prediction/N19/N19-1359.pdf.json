{"title": [{"text": "Information Aggregation for Multi-Head Attention with Routing-by-Agreement", "labels": [], "entities": [{"text": "Multi-Head Attention", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8040617406368256}, {"text": "Routing-by-Agreement", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.48388704657554626}]}], "abstractContent": [{"text": "Multi-head attention is appealing for its ability to jointly extract different types of information from multiple representation sub-spaces.", "labels": [], "entities": []}, {"text": "Concerning the information aggre-gation, a common practice is to use a con-catenation followed by a linear transformation, which may not fully exploit the expressiveness of multi-head attention.", "labels": [], "entities": []}, {"text": "In this work, we propose to improve the information aggregation for multi-head attention with a more powerful routing-by-agreement algorithm.", "labels": [], "entities": []}, {"text": "Specifically, the routing algorithm iteratively updates the proportion of how much apart (i.e. the distinct information learned from a specific subspace) should be assigned to a whole (i.e. the final output representation), based on the agreement between parts and wholes.", "labels": [], "entities": []}, {"text": "Experimental results on linguistic probing tasks and machine translation tasks prove the superiority of the advanced information aggregation over the standard linear transformation.", "labels": [], "entities": [{"text": "linguistic probing tasks", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7735321919123331}, {"text": "machine translation tasks", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8268246650695801}]}], "introductionContent": [{"text": "Attention model becomes a standard component of the deep learning networks, contributing to impressive results in machine translation, image captioning (, speech recognition (), among many other applications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.829748272895813}, {"text": "image captioning", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7438458800315857}, {"text": "speech recognition", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7623061537742615}]}, {"text": "Its superiority lies in the ability of modeling the dependencies between representations without regard to their distance.", "labels": [], "entities": []}, {"text": "Recently, the performance of attention is further improved by multi-head mechanism (, which parallelly performs attention functions on different representation subspaces of the input sequence.", "labels": [], "entities": []}, {"text": "Consequently, different attention heads are able to capture distinct linguistic properties of * Zhaopeng Tu is the corresponding author of the paper.", "labels": [], "entities": []}, {"text": "This work was mainly conducted when Jian Li, Baosong Yang, and Zi-Yi Dou were interning at Tencent AI Lab.", "labels": [], "entities": []}, {"text": "the input, which are embedded in different subspaces (.", "labels": [], "entities": []}, {"text": "Subsequently, a linear transformation is generally employed to aggregate the partial representations extracted by different attention heads (.", "labels": [], "entities": []}, {"text": "Most existing work focus on extracting informative or distinct partial-representations from different subspaces (e.g., while few studies have paid attention to the aggregation of the extracted partialrepresentations.", "labels": [], "entities": []}, {"text": "Arguably, information extraction and aggregation are both crucial for multi-head attention to generate an informative representation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.824795663356781}]}, {"text": "Recent studies in multimodal learning show that a straightforward linear transformation for fusing features in different sets of representations usually limits the extent of abstraction (.", "labels": [], "entities": []}, {"text": "A natural question arises: whether the straightforward linear transformation is expressive enough to fully capture the rich information distributed in the extracted partial-representations?", "labels": [], "entities": []}, {"text": "In this work, we provide the first answer to this question.", "labels": [], "entities": []}, {"text": "We propose to empirically validate the importance of information aggregation in multihead attention, by comparing the performance of the standard linear function and advanced aggregation functions on various tasks.", "labels": [], "entities": [{"text": "information aggregation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7430095970630646}, {"text": "multihead attention", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8531086146831512}]}, {"text": "Specifically, we cast information aggregation as the assigningparts-to-wholes problem, and investigate the effectiveness of the routing-byagreement algorithm -an appealing alternative to solving this problem ().", "labels": [], "entities": []}, {"text": "The routing algorithm iteratively updates the proportion of how much apart should be assigned to a whole, based on the agreement between parts and wholes.", "labels": [], "entities": [{"text": "routing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9683647751808167}]}, {"text": "We leverage the routing algorithm to aggregate the information distributed in the extracted partial-representations.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the aggregated representations on both linguistic probing tasks as well as machine translation tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7481201887130737}]}, {"text": "The probing tasks ( consists of 10 classification problems to study what linguistic properties are captured by input representations.", "labels": [], "entities": []}, {"text": "Probing analysis show that our approach indeed produces more informative representation, which embeds more syntactic and semantic information.", "labels": [], "entities": []}, {"text": "For translation tasks, we validate our approach on top of the advanced TRANSFORMER model () on both WMT14 English\u21d2German and WMT17 Chinese\u21d2English data.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9793376922607422}, {"text": "TRANSFORMER", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.8876716494560242}, {"text": "WMT14 English\u21d2German", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.8933415710926056}, {"text": "WMT17 Chinese\u21d2English data", "start_pos": 125, "end_pos": 151, "type": "DATASET", "confidence": 0.9188833832740784}]}, {"text": "Experimental results show that our approach consistently improves translation performance across languages while keeps the computational efficiency.", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9666133522987366}]}, {"text": "The main contributions of this paper can be summarized as follows: \u2022 To our best knowledge, this is the first work to demonstrate the necessity and effectiveness of advanced information aggregation for multi-head attention.", "labels": [], "entities": []}, {"text": "\u2022 Our work is among the few studies (cf. () which prove that the idea of capsule networks can have promising applications on natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.698829174041748}]}], "datasetContent": [{"text": "In this section, we evaluate the performance of our proposed models on both linguistic probing tasks and machine translation tasks.", "labels": [], "entities": [{"text": "linguistic probing tasks", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.7787620226542155}, {"text": "machine translation tasks", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.8212775588035583}]}], "tableCaptions": [{"text": " Table 1: Classification accuracies on 10 probing tasks of evaluating the linguistic properties (\"Surface\", \"Syntec- tic\", and \"Semantic\") learned by sentence encoder. \"BASE\" denotes the standard linear transformation, \"SIMPLE\"  is the simple routing algorithm, and \"EM\" is the EM routing algorithm.", "labels": [], "entities": [{"text": "BASE", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9958894848823547}, {"text": "EM routing", "start_pos": 278, "end_pos": 288, "type": "TASK", "confidence": 0.7361588180065155}]}, {"text": " Table 2: Effect of information aggregation on different attention components, i.e., encoder self-attention (\"Enc- Self\"), encoder-decoder attention (\"Enc-Dec\"), and decoder self-attention (\"Dec-Self\"). \"# Para.\" denotes the  number of parameters, and \"Train\" and \"Decode\" respectively denote the training speed (steps/second) and decod- ing speed (sentences/second).", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of different layers in the encoder,  which are implemented as multi-head self-attention  with the EM routing based information aggregation.  \"1\" denotes the bottom layer, and \"6\" the top layer.", "labels": [], "entities": []}, {"text": " Table 4: Comparing with existing NMT systems on WMT14 English\u21d2German (\"En\u21d2De\") and WMT17  Chinese\u21d2English (\"Zh\u21d2En\") tasks. \"\u2191 / \u21d1\": significantly better than the baseline counterpart (p < 0.05/0.01).", "labels": [], "entities": [{"text": "WMT14", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9466068148612976}, {"text": "WMT17", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9481695890426636}]}]}