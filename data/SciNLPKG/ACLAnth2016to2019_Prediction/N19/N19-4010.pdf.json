{"title": [{"text": "FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8102920055389404}, {"text": "NLP", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.7228624224662781}]}], "abstractContent": [{"text": "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.5264641642570496}, {"text": "text classification", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7746235430240631}]}, {"text": "The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings.", "labels": [], "entities": []}, {"text": "This effectively hides all embedding-specific engineering complexity and allows researchers to \"mix and match\" various embeddings with little effort.", "labels": [], "entities": []}, {"text": "The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick setup of experiments.", "labels": [], "entities": [{"text": "data fetching", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7916267812252045}]}, {"text": "Finally , FLAIR also ships with a \"model zoo\" of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7157025337219238}]}, {"text": "This paper gives an overview of the framework and its functionality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Classic pre-trained word embeddings have been shown to be of great use for downstream NLP tasks, both due to their ability to assist learning and generalization with information learned from unlabeled data, as well as the relative ease of including them into any learning approach ().", "labels": [], "entities": []}, {"text": "Many recently proposed approaches go beyond the initial \"one word, one embedding\" paradigm to better model additional features such as subword structures) and meaning ambiguity ().", "labels": [], "entities": []}, {"text": "Though shown to be extremely powerful, such embeddings have the drawback that they cannot be used to simply initialize the embedding layer of a neural network and thus require specific reworkings of the overall model architecture.", "labels": [], "entities": []}, {"text": "A common example is that many current approaches combine classic word embeddings with character-level features trained on task data.", "labels": [], "entities": []}, {"text": "To accomplish this, they use a hierarchical learning architecture in which the output states of a character-level CNN or RNN are concatenated with the output of the embedding layer.", "labels": [], "entities": []}, {"text": "While modern deep learning frameworks such as PYTORCH () make the construction of such architectures relatively straightforward, architectural changes are nevertheless required for something that is fundamentally just another method for embedding words.", "labels": [], "entities": []}, {"text": "Similarly, recent works-including our own-have proposed methods that produce different embeddings for the same word depending on its contextual usage).", "labels": [], "entities": []}, {"text": "The string \"Washington\" for instance would be embedded differently depending on whether the context indicates this string to be a last name or a location.", "labels": [], "entities": []}, {"text": "While shown to be highly powerful, especially in combination with classic word embeddings, such methods require an architecture in which the output states of a trained language model (LM) are concatenated with the output of the embedding layer, thus adding architectural complexity.", "labels": [], "entities": []}, {"text": "These examples illustrate that word embeddings typically cannot simply be mixed and matched with minimal effort, but rather require specific reworkings of the model architecture.", "labels": [], "entities": []}, {"text": "Proposed solution: FLAIR framework.", "labels": [], "entities": [{"text": "FLAIR framework", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.711067870259285}]}, {"text": "With this paper, we present anew framework designed to address this problem.", "labels": [], "entities": []}, {"text": "The principal design goal is to abstract away from specific engineering challenges that different types of word embeddings raise.", "labels": [], "entities": []}, {"text": "We created a simple, unified interface for all word embeddings as well as arbitrary combinations of embeddings.", "labels": [], "entities": []}, {"text": "This interface, we argue, allows researchers to build a single model architecture that can then make use of any type of word embedding with no additional engineering effort.", "labels": [], "entities": []}, {"text": "To further simplify the process of setting up and executing experiments, FLAIR includes convenience methods for downloading standard NLP research datasets and reading them into data structures for the framework.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.6562504172325134}, {"text": "NLP research datasets", "start_pos": 133, "end_pos": 154, "type": "DATASET", "confidence": 0.6486942569414774}]}, {"text": "It also includes model training and hyperparameter selection routines to facilitate typical training and testing workflows.", "labels": [], "entities": []}, {"text": "In addition, FLAIR also ships with a growing list of pre-trained models allowing users to apply already trained models to their text.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6324256658554077}]}, {"text": "This paper gives an overview of the framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "To facilitate setting up experiments, we include convenience methods to download publicly available benchmark datasets fora variety of NLP tasks and read them into data structures for training.", "labels": [], "entities": []}, {"text": "For instance, to download the universal dependency  Internally, the data fetcher checks if the requested dataset is already present on local disk and if not, downloads it.", "labels": [], "entities": []}, {"text": "The dataset is then read into an object of type TaggedCorpus which defines training, testing and development splits.", "labels": [], "entities": []}, {"text": "gives an overview of all datasets that are currently downloadable.", "labels": [], "entities": []}, {"text": "Other datasets, such as the CoNLL-03 datasets for English and German, require licences and thus cannot be automatically downloaded.", "labels": [], "entities": [{"text": "CoNLL-03 datasets", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9718324542045593}]}], "tableCaptions": []}