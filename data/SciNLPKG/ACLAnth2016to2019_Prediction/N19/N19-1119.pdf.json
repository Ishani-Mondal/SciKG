{"title": [{"text": "Competence-based Curriculum Learning for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6754954953988394}]}], "abstractContent": [{"text": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes.", "labels": [], "entities": []}, {"text": "This is undesirable as it requires extensive hy-perparameter tuning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance.", "labels": [], "entities": [{"text": "NMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.8980841636657715}]}, {"text": "Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model.", "labels": [], "entities": []}, {"text": "Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines.", "labels": [], "entities": []}, {"text": "We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9991342425346375}, {"text": "BLEU", "start_pos": 257, "end_pos": 261, "type": "METRIC", "confidence": 0.9982940554618835}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT;;) now represents the state-of-the-art adapted inmost machine translation systems (, largely due to its ability to benefit from end-to-end training on massive amounts of data.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.702668410539627}]}, {"text": "In particular, recentlyintroduced self-attentional Transformer architectures ( are rapidly becoming the de-facto standard in NMT, having demonstrated both superior performance and training speed compared to previous architectures using recurrent neural networks (RNNs;).", "labels": [], "entities": []}, {"text": "However, large scale NMT systems are often hard to train, requiring complicated heuristics which can be both timeconsuming and expensive to tune.", "labels": [], "entities": []}, {"text": "This is especially true for Transformers which, when carefully tuned, have been shown to consistently outperform RNNs (, but on the other hand, also rely on a number of heuristics such as specialized learning rates and large-batch training.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to tackle this problem by proposing a curriculum learning framework for training NMT systems that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance.", "labels": [], "entities": []}, {"text": "It allows us to train both RNNs and, perhaps more importantly, Transformers, with relative ease.", "labels": [], "entities": []}, {"text": "Our proposed method is based on the idea of teaching algorithms in a similar manner as humans, from easy concepts to more difficult ones.", "labels": [], "entities": []}, {"text": "This idea can be traced back to the work of and.", "labels": [], "entities": []}, {"text": "The main motivation is that training algorithms can perform better if training data is presented in a specific order, starting from easy examples and moving onto more difficult ones, as the learner becomes more competent.", "labels": [], "entities": []}, {"text": "In the case of machine learning, it can also bethought of as a means to avoid getting stuck in bad local optima early on in training.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7940938174724579}]}, {"text": "An overview of the proposed framework is shown in.", "labels": [], "entities": []}, {"text": "Notably, we are not the first to examine curriculum learning for NMT, although other related works have met with mixed success.", "labels": [], "entities": [{"text": "NMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7101126909255981}]}, {"text": "explore impact of several curriculum heuristics on training a translation system fora single epoch, presenting the training examples in an easy-to-hard order based on sentence length and vocabulary frequency.", "labels": [], "entities": []}, {"text": "However, their strategy introduces all training samples during the first epoch, and how this affects learning in following epochs is not clear, with official evaluation results) indicating that final performance may indeed be hurt with this strategy.", "labels": [], "entities": []}, {"text": "Contemporaneously to our work, further propose to split the training samples into a predefined number of bins (5, in their case), based on various difficulty metrics.", "labels": [], "entities": []}, {"text": "A manually designed curriculum schedule then specifies the bins from which the model samples training examples.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that benefits of curriculum learning are highly sensitive to several hyperparameters (e.g., learning rate, number of iterations spent in each phase, etc.), and largely provide benefits in convergence speed as opposed to final model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9654240012168884}]}, {"text": "In contrast to these previous approaches, we define a continuous curriculum learning method (instead of a discretized regime) with only one tunable hyperparameter (the duration of curriculum learning).", "labels": [], "entities": []}, {"text": "Furthermore, as opposed to previous work which only focuses on RNNs, we also experiment with Transformers, which are notoriously hard to train.", "labels": [], "entities": []}, {"text": "Finally, unlike any of the work described above, we show that our curriculum approach helps not only in terms of convergence speed, but also in terms of the learned model performance.", "labels": [], "entities": []}, {"text": "In summary, our method has the following desirable features: 1.", "labels": [], "entities": []}, {"text": "Abstract: It is a novel, generic, and extensible formulation of curriculum learning.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9486123323440552}]}, {"text": "A number of previous heuristic-based approaches, such as that of, can be formulated as special cases of our framework.", "labels": [], "entities": []}, {"text": "2. Simple: It can be applied to existing NMT systems with only a small modification to their training data pipelines.", "labels": [], "entities": [{"text": "Simple", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9951024055480957}]}, {"text": "3. Automatic: It does not require any tuning other than picking the value of a single parameter, which is the length of the curriculum (i.e., for how many steps to use curriculum learning, before easing into normal training).", "labels": [], "entities": [{"text": "Automatic", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9752535223960876}]}, {"text": "4. Efficient: It reduces training time by up to 70%, whereas contemporaneous work of reports reductions of up to 46%.", "labels": [], "entities": [{"text": "Efficient", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9988717436790466}, {"text": "training time", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8221962451934814}]}, {"text": "5. Improved Performance: It improves the performance of the learned models by up to 2.2 BLEU points, where the best setting reported by achieves gains of up 1.55 BLEU after careful tuning.", "labels": [], "entities": [{"text": "Improved Performance", "start_pos": 3, "end_pos": 23, "type": "METRIC", "confidence": 0.8334654271602631}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9988282322883606}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9986376166343689}]}, {"text": "In the next section, we introduce our proposed curriculum learning framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we use three of the most commonly used datasets in NMT, that range from a small benchmark dataset to a large-scale dataset with millions of sentences.", "labels": [], "entities": []}, {"text": "Statistics about the datasets are shown in \"k\" stands for \"thousand\" and \"m\" stands for \"million\".", "labels": [], "entities": []}, {"text": "(due to GPU memory constraints).", "labels": [], "entities": []}, {"text": "During inference, we employ beam search with abeam size of 10 and the length normalization scheme of.", "labels": [], "entities": []}, {"text": "We set the initial competence c 0 to 0.01, in all experiments.", "labels": [], "entities": [{"text": "initial competence c 0", "start_pos": 11, "end_pos": 33, "type": "METRIC", "confidence": 0.8199628740549088}]}, {"text": "This means that all models start training using the 1% easiest training examples.", "labels": [], "entities": []}, {"text": "The curriculum length T is effectively the only hyperparameter that we need to set for our curriculum methods.", "labels": [], "entities": [{"text": "curriculum length T", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.566578596830368}]}, {"text": "In each experiment, we set T in the following manner: we train the baseline model without using any curriculum and we compute the number of training steps it takes to reach approximately 90% of its final BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.9797727465629578}]}, {"text": "We then set T to this value.", "labels": [], "entities": []}, {"text": "This results in T being set to 5,000 for the RNN experiments on the IWSLT datasets, and 20,000 for the corresponding Transformer experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9988044500350952}, {"text": "RNN", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7990714311599731}, {"text": "IWSLT datasets", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.9801223874092102}]}, {"text": "For WMT, we set T to 20,000 and 50,000 for RNNs and Transformers, respectively.", "labels": [], "entities": [{"text": "WMT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8564932346343994}, {"text": "T", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9993368983268738}]}, {"text": "Furthermore, we use the following notation and abbreviations when presenting our results: -Plain: Trained without using any curriculum.", "labels": [], "entities": [{"text": "Plain", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9668887853622437}]}, {"text": "occurring words, while ignoring words that appear less than 5 times in the whole corpus.", "labels": [], "entities": []}, {"text": "For the IWSLT-16 and WMT-16 experiments we use a bytepair encoding (BPE) vocabulary) trained using 32,000 merge operations, similar to the original Transformer paper by.", "labels": [], "entities": [{"text": "IWSLT-16", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8669793009757996}, {"text": "WMT-16", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.7436054348945618}, {"text": "bytepair encoding (BPE) vocabulary", "start_pos": 49, "end_pos": 83, "type": "METRIC", "confidence": 0.7419000267982483}]}, {"text": "We present a summary of our results in and we also show complete learning curves for all methods in.", "labels": [], "entities": []}, {"text": "The evaluation metrics we use are the test set BLEU score and the time it takes for the models using curriculum learning to obtain the BLEU score that the baseline models attain at convergence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9773159027099609}, {"text": "BLEU score", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.976448655128479}]}, {"text": "We observe that Transformers consistently benefit from our curriculum learning approach, achieving gains of up to 2 BLEU, and reductions in training time of up to 70%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9994826316833496}]}, {"text": "RNNs also benefit, but to a lesser extent.", "labels": [], "entities": []}, {"text": "This is consistent with our motivation for this paper, which: Summary of experimental results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9516321420669556}]}, {"text": "For each method and dataset, we present the test set BLEU score of the best model based on validation set performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9961705803871155}]}, {"text": "We also show the relative time required to obtain the BLEU score of the best performing baseline model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9784115552902222}]}, {"text": "For example, if an RNN gets to 26.27 BLEU in 10, 000 steps and the SL curriculum gets to the same BLEU in 3, 000 steps, then the plain model gets a score of 1.0 and the SL curriculum receives a score of 3, 000/10, 000 = 0.3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9948133230209351}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9925406575202942}]}, {"text": "\"Plain\" stands for the model trained without a curriculum and, for Transformers, \"Plain*\" stands for the model trained using the learning rate schedule shown in Equation 9.", "labels": [], "entities": []}, {"text": "stems from the observation that training RNNs is easier and more robust than training Transformers.", "labels": [], "entities": []}, {"text": "Furthermore, the square root competence model consistently outperforms the linear model, which fits well with our intuition and motivation for introducing it.", "labels": [], "entities": []}, {"text": "Regarding the difficulty heuristics, sentence length and sentence rarity both result in similar performance.", "labels": [], "entities": []}, {"text": "We also observe that, for the two small datasets, RNNs converge faster than Transformers in terms of both the number of training iterations and the overall training time.", "labels": [], "entities": []}, {"text": "This is contrary to other results in the machine translation community (e.g.,), but could be explained by the fact that we are not using any learning rate schedule for training Transformers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7782509326934814}]}, {"text": "However, they never manage to outperform Transformers in terms of test BLEU score of the final model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9742176830768585}]}, {"text": "Furthermore, to the best of our knowledge, for IWSLT-15 we achieve state-of-the-art performance.", "labels": [], "entities": [{"text": "IWSLT-15", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8425374627113342}]}, {"text": "The highest previously reported result was 29.03 BLEU (, in a multi-lingual setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9843777418136597}]}, {"text": "Using our curriculum learning approach we are able to achieve a BLEU score of 29.81 for this dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9856209456920624}]}, {"text": "Overall, we have shown that our curriculum learning approach consistently outperforms models trained without any curriculum, in both limited data settings and large-scale settings.", "labels": [], "entities": []}, {"text": "In all of our IWSLT experiments so far, we use the default AMSGrad learning rate of 0.001 and intentionally avoid using any learning rate schedules.", "labels": [], "entities": [{"text": "AMSGrad learning rate", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.5981440941492716}]}, {"text": "However, Transformers are not generally trained without a learning rate schedule, due to their instability.", "labels": [], "entities": []}, {"text": "Such schedules typically use a warm-up phase, which means that the learning rate starts at a very low value and keeps increasing until the end of the warm-up period, after which a decay rate is typically used.", "labels": [], "entities": []}, {"text": "In order to show that our curriculum learning approach can act as a principled alternative to such highly tuned learning rate schedules, we now present the results we obtain when training our Transformers using the following learning rate schedule: where t is the current training step, d embedding is the word embeddings size, and T warmup is the number of warmup steps and is set to 10,000 in these experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 332, "end_pos": 333, "type": "METRIC", "confidence": 0.9834859371185303}]}, {"text": "This schedule was proposed in the original Transformer paper (, and was tuned for the WMT dataset.", "labels": [], "entities": [{"text": "Transformer paper", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7370944023132324}, {"text": "WMT dataset", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.9663861989974976}]}, {"text": "The results obtained when using this learning rate schedule are also shown in table 2, under the name \"Plain*\".", "labels": [], "entities": []}, {"text": "In both cases, our curriculum learning approach obtains a better model in about 70% less training time.", "labels": [], "entities": []}, {"text": "This is very important, especially when applying Transformers in new datasets, because such learning rate heuristics often require careful tuning.", "labels": [], "entities": []}, {"text": "This tuning can be both very expensive and time consuming, often resulting in very complex mathematical expressions, with no clear motivation or intuitive explanation.", "labels": [], "entities": []}, {"text": "Our curriculum learning approach achieves better results, in significantly less time, while only requiring one parameter (the length of the curriculum).", "labels": [], "entities": []}, {"text": "Note that even without using any learning rate schedule, our curriculum methods were able to achieve performance comparable to the \"Plain*\" in about twice as many training steps.", "labels": [], "entities": []}, {"text": "\"Plain\" was notable to achieve a BLEU score above 2.00 even after fives times as many training steps, at which point we stopped these experiments.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9858841300010681}]}, {"text": "We are releasing an implementation of our proposed method and experiments built on top of the machine translation library released by , using TensorFlow Scala , and is available at https://github.com/ eaplatanios/symphony-mt.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.6796505898237228}]}, {"text": "Furthermore, all experiments can be run on a machine with a single Nvidia V100 GPU, and 24 GBs of system memory.", "labels": [], "entities": [{"text": "Nvidia V100 GPU", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.8935369253158569}]}, {"text": "Our most expensive experiments -the ones using Transformers on the WMT-16 dataset -take about 2 days to complete, which would cost about $125 on a cloud computing service such as Google Cloud or Amazon Web Services, thus making our results reproducible, even by independent researchers.", "labels": [], "entities": [{"text": "WMT-16 dataset", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.985860288143158}]}], "tableCaptions": [{"text": " Table 1: Number of parallel sentences in each dataset.", "labels": [], "entities": []}, {"text": " Table 2: Summary of experimental results. For each method and dataset, we present the test set BLEU score of  the best model based on validation set performance. We also show the relative time required to obtain the BLEU  score of the best performing baseline model. For example, if an RNN gets to 26.27 BLEU in 10, 000 steps and  the SL curriculum gets to the same BLEU in 3, 000 steps, then the plain model gets a score of 1.0 and the SL  curriculum receives a score of 3, 000/10, 000 = 0.3. \"Plain\" stands for the model trained without a curriculum  and, for Transformers, \"Plain*\" stands for the model trained using the learning rate schedule shown in Equation 9.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9967735409736633}, {"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9987309575080872}, {"text": "BLEU", "start_pos": 305, "end_pos": 309, "type": "METRIC", "confidence": 0.9883003234863281}, {"text": "BLEU", "start_pos": 367, "end_pos": 371, "type": "METRIC", "confidence": 0.9840705394744873}]}]}