{"title": [{"text": "Improving Neural Machine Translation with Neural Syntactic Distance", "labels": [], "entities": [{"text": "Improving Neural Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8735519647598267}, {"text": "Neural Syntactic Distance", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.655162384112676}]}], "abstractContent": [{"text": "The explicit use of syntactic information has been proved useful for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.8016485075155894}]}, {"text": "However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient.", "labels": [], "entities": []}, {"text": "Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence.", "labels": [], "entities": [{"text": "Neural syntactic distance (NSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.835938553015391}]}, {"text": "NSD has been used for constituent parsing, but not in machine translation.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7805275619029999}, {"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7165680676698685}]}, {"text": "We propose five strategies to improve NMT with NSD.", "labels": [], "entities": [{"text": "NMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9667356610298157}]}, {"text": "Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En-Ja), +1.3 (Ja-En), +1.2 (En-Ch), and +1.0 (Ch-En) BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.9920700192451477}]}], "introductionContent": [{"text": "In recent years, neural machine translation (NMT) has been developing rapidly and has become the de facto approach for machine translation.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.8294597367445627}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8253239691257477}]}, {"text": "To improve the performance of the conventional NMT models, one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model.", "labels": [], "entities": []}, {"text": "Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures (, and (2) those that use linear-structured NNs to represent linearized syntax structures (.", "labels": [], "entities": []}, {"text": "For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in- * Corresponding author efficient.", "labels": [], "entities": []}, {"text": "In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem.", "labels": [], "entities": []}, {"text": "Although using a shorter sequence may improve the efficiency, some syntactic information is lost.", "labels": [], "entities": []}, {"text": "We propose a method of using syntactic information in NMT that overcomes the disadvantages of both methods.", "labels": [], "entities": []}, {"text": "The basis of our method is the neural syntactic distance (NSD), a recently proposed concept used for constituent parsing.", "labels": [], "entities": [{"text": "neural syntactic distance (NSD)", "start_pos": 31, "end_pos": 62, "type": "METRIC", "confidence": 0.7035811344782511}, {"text": "constituent parsing", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7417309880256653}]}, {"text": "NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information.", "labels": [], "entities": [{"text": "NSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6543128490447998}]}, {"text": "However, there are no previous studies that use NSD in NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9150826930999756}]}, {"text": "Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically.", "labels": [], "entities": []}, {"text": "The strategies are summarized below.", "labels": [], "entities": []}, {"text": "\u2022 Extend NSD to dependency trees, which is inspired by the dependency language model).", "labels": [], "entities": []}, {"text": "\u2022 Use NSDs as input sequences 1 , where an NSD is regarded as a linguistic input feature).", "labels": [], "entities": []}, {"text": "\u2022 Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning).", "labels": [], "entities": []}, {"text": "\u2022 Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer ().", "labels": [], "entities": [{"text": "positional encoding (PE)", "start_pos": 13, "end_pos": 37, "type": "METRIC", "confidence": 0.7953903317451477}]}, {"text": "\u2022 Add a loss function for NSD to achieve distance-aware training).", "labels": [], "entities": []}], "datasetContent": [{"text": "The first five rows of compare the results of using different NSDs.", "labels": [], "entities": []}, {"text": "When NSD was used at the source side (En-Ja/En-Ch), all kinds of NSDs improved translation performance.", "labels": [], "entities": []}, {"text": "This indicates that NSD can be regarded as a useful linguistic feature to improve NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9156187772750854}]}, {"text": "In contrast, when NSD was used at the target side (Ja-En/Ch-En), d Sand d G hurt the performance.", "labels": [], "entities": []}, {"text": "This is because the values of d Sand d G are volatile.", "labels": [], "entities": []}, {"text": "A tiny change of syntactic structure often causes a big change of d Sand d G . Since the model has to predict the NSD during decoding, once there is one error, the subsequent predictions will be heavily influenced.", "labels": [], "entities": []}, {"text": "The use of d Rand d D remedies this problem.", "labels": [], "entities": []}, {"text": "Furthermore, the effects of d Sand d G are similar, because they are equivalent in nature (refer to Eq. 2).", "labels": [], "entities": []}, {"text": "Rows 5 to 8 of evaluate the use of dependency NSD (d D ) as syntactic PE.", "labels": [], "entities": []}, {"text": "Note that for all the experiments, we used not only the syntactic PE but the conventional PE.", "labels": [], "entities": [{"text": "PE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9326040148735046}]}, {"text": "Experiment results show that this strategy is indeed useful.", "labels": [], "entities": []}, {"text": "When the dominators of Eqs.", "labels": [], "entities": []}, {"text": "15 and 16, \u03bb SP E , were set to 10 4 , there was no improvement.", "labels": [], "entities": [{"text": "SP E", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9228741526603699}]}, {"text": "When they were set to 40, the improvement was remarkable.", "labels": [], "entities": []}, {"text": "This indicates that our design of syntactic PE is reasonable.", "labels": [], "entities": []}, {"text": "NSD as input/output and source/target sequences.", "labels": [], "entities": [{"text": "NSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7986855506896973}]}, {"text": "Rows 8 to 12 of  bf , which increases the model capacity.", "labels": [], "entities": []}, {"text": "Second, performance improved for using NSDs both as input and output sequences, and combining both obtained further improvement.", "labels": [], "entities": []}, {"text": "Third, NSDs improved the performance both on the source and the target sides.", "labels": [], "entities": []}, {"text": "All these results indicate the robustness of NSDs.", "labels": [], "entities": []}, {"text": "The last three rows compare the different effects of the items in the loss function.", "labels": [], "entities": []}, {"text": "When only L NM T are used, the performance is extremely poor.", "labels": [], "entities": []}, {"text": "This is within expectations, because with only L NM T , weights related to NSDs are kept to the initial values and were not updated, and hence detrimental to learning.", "labels": [], "entities": []}, {"text": "Adding Lent dist improves the results significantly, but the improvement is lower than that of L dist . This is because training with Lent dist treats different values of NSDs equally, while L dist penalizes larger differences between the predicted NSD and the golden NSD more severely.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of strategies. I/O: use NSDs as the input or output sequences. Functions f", "labels": [], "entities": []}]}