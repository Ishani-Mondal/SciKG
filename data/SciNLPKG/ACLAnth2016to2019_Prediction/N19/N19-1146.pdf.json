{"title": [{"text": "Detecting Cognitive Impairments by Agreeing on Interpretations of Linguistic Features", "labels": [], "entities": [{"text": "Detecting Cognitive Impairments", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8961603840192159}]}], "abstractContent": [{"text": "Linguistic features have shown promising applications for detecting various cognitive impairments.", "labels": [], "entities": []}, {"text": "To improve detection accuracies, increasing the amount of data or the number of linguistic features have been two applicable approaches.", "labels": [], "entities": []}, {"text": "However, acquiring additional clinical data can be expensive, and hand-crafting features is burdensome.", "labels": [], "entities": []}, {"text": "In this paper, we take a third approach, proposing Consensus Networks (CNs), a framework to classify after reaching agreements between modalities.", "labels": [], "entities": []}, {"text": "We divide linguistic features into non-overlapping subsets according to their modalities, and let neural networks learn low-dimensional representations that agree with each other.", "labels": [], "entities": []}, {"text": "These representations are passed into a classifier network.", "labels": [], "entities": []}, {"text": "All neural networks are optimized iteratively.", "labels": [], "entities": []}, {"text": "In this paper, we also present two methods that improve the performance of CNs.", "labels": [], "entities": []}, {"text": "We then present ablation studies to illustrate the effectiveness of modality division.", "labels": [], "entities": [{"text": "modality division", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7663894593715668}]}, {"text": "To understand further what happens in CNs, we visualize the representations during training.", "labels": [], "entities": []}, {"text": "Overall, using all of the 413 linguistic features, our models significantly outperform traditional classifiers, which are used by the state-of-the-art papers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Alzheimer's disease (AD) and its usual precursor, mild cognitive impairment (MCI), are prevalent neurodegerative conditions that inhibit cognitive abilities.", "labels": [], "entities": [{"text": "Alzheimer's disease (AD)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6448675692081451}]}, {"text": "Cognitive impairments are traditionally diagnosed only with standard clinical tests like MoCA () and the ReyAuditory Verbal learning Test, but hiring clinicians to administer these tests and analyze their results is costly.", "labels": [], "entities": [{"text": "Cognitive impairments", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8613304495811462}, {"text": "MoCA", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8188310265541077}, {"text": "ReyAuditory Verbal learning", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.5556458036104838}]}, {"text": "Fortunately, many cognitive impairments can be observable in daily life, because they impact one's language abilities.", "labels": [], "entities": []}, {"text": "For example, cognitively impaired people tend to use more pronouns instead of nouns, and pause more often between sentences in narrative speech).", "labels": [], "entities": []}, {"text": "This insight makes automatic detection possible.", "labels": [], "entities": [{"text": "automatic detection", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.514555037021637}]}, {"text": "Machine learning classifiers can detect cognitive impairments given descriptive linguistic features.", "labels": [], "entities": []}, {"text": "In recent work, linguistic features including pronoun-noun-ratios, pauses, and soon, are used to train classifiers to detect cognitive diseases in various tasks.", "labels": [], "entities": [{"text": "soon", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9595802426338196}]}, {"text": "For example, achieved up to 82% accuracy on DementiaBank 1 , the largest publicly available dataset on detecting cognitive impairments from speech, and achieved up to 86% accuracy on a corpus of 500 subjects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9984540939331055}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9978316426277161}]}, {"text": "estimated Mini-Mental State Estimation scores (MMSEs), describing the cognitive status and characterizing the extent of cognitive impairment.", "labels": [], "entities": [{"text": "Mini-Mental State Estimation scores (MMSEs)", "start_pos": 10, "end_pos": 53, "type": "METRIC", "confidence": 0.7174533903598785}]}, {"text": "To improve the accuracy of automated assessment using engineered linguistic features, there are usually two approaches: incorporating more clinical data or calculating more features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9962079524993896}]}, {"text": "Taking the first approach, incorporated normative data from Talk2Me 2 and the Wisconsin Longitudinal Study () in addition to DementiaBank, which increased AD:control accuracy up to 93%, and moderateAD:mildAD:control three-way classification accuracy to 70%.", "labels": [], "entities": [{"text": "Wisconsin Longitudinal Study", "start_pos": 78, "end_pos": 106, "type": "DATASET", "confidence": 0.9269674221674601}, {"text": "AD:control accuracy", "start_pos": 155, "end_pos": 174, "type": "METRIC", "confidence": 0.7990293353796005}, {"text": "mildAD:control three-way classification accuracy", "start_pos": 201, "end_pos": 249, "type": "METRIC", "confidence": 0.7365131775538126}]}, {"text": "Taking the second approach, used 12 features derived from vector space models and reached a .80 F-score on DementiaBank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9996365308761597}, {"text": "DementiaBank", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9108595252037048}]}, {"text": "calculated features depicting characteristics of cooccurrence graphs of narrative transcripts (e.g., the degree of each vertex in the graph).", "labels": [], "entities": []}, {"text": "Their classifiers reached 65% accuracy on DementiaBank (MCI versus a subset of Control).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994588494300842}]}, {"text": "There are limitations in either of the two approaches.", "labels": [], "entities": []}, {"text": "On one hand, acquiring additional clinical data can be expensive.", "labels": [], "entities": []}, {"text": "Moreover, the additional data should be similar enough to existing training data to be helpful.", "labels": [], "entities": []}, {"text": "On the other hand, crafting new features requires creativity and collaboration with subject matter experts, and the implementation can be time consuming.", "labels": [], "entities": []}, {"text": "Neither of these approaches is satisfactory.", "labels": [], "entities": []}, {"text": "These limitations motivate us to take a third, novel approach.", "labels": [], "entities": []}, {"text": "Instead of using new data or computing new features, we use the existing linguistic features.", "labels": [], "entities": []}, {"text": "If the speaker is cognitively impaired, and their language ability is affected, features from each of the acoustic, syntactic, and semantic modalities should reflect such change (.", "labels": [], "entities": []}, {"text": "We therefore need to distill the common information revealed by features from multiple, mainly distinct, modalities.", "labels": [], "entities": []}, {"text": "To utilize information common across different modalities, and let classifiers look at each modality and supervise each other.", "labels": [], "entities": []}, {"text": "These examples illustrated the effectiveness of multi-view learning in utilizing common information among different observations, but their algorithms fail to train useful classifiers for cognitive impairments in our datasets.", "labels": [], "entities": []}, {"text": "Without explicit supervision, selfsupervised models almost always converge to a state producing the same predictions for all people, giving trivial classifiers.", "labels": [], "entities": []}, {"text": "Instead of aligning the predictions from modalities, we let the representations of the modalities agree.", "labels": [], "entities": []}, {"text": "Generative adversarial networks (GANs) provide an approach.", "labels": [], "entities": []}, {"text": "In GANs, a \"discriminator\" network is trained to tell whether a vector is drawn from the real world or produced synthetically by a \"generator\" neural network, while the generator is trained to synthesize images as close to real data as possible.", "labels": [], "entities": [{"text": "GANs", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.9486470818519592}]}, {"text": "We borrow this setting, and encourage the neural networks interpreting different modalities to produce representations of modalities as similar to each other as possible.", "labels": [], "entities": []}, {"text": "This leads to our classifier framework, consensus networks (CNs).", "labels": [], "entities": []}, {"text": "Consensus networks constitute a framework using adversarial training to utilize common information among modalities for classification.", "labels": [], "entities": []}, {"text": "In this framework, several neural networks (\"ePhysicians\") are juxtaposed, each learning the representation of a partition of linguistic features for each transcript.", "labels": [], "entities": []}, {"text": "Being trained towards producing agreed representations, we show they are increasingly able to capture common information contained across disparate subsets of linguistic features.", "labels": [], "entities": []}, {"text": "We empirically add two extensions to CN that improve the classification accuracies, called the \"noise modality\" and \"cooperative optimization\" respectively, as explained below.", "labels": [], "entities": []}, {"text": "To illustrate the effectiveness of the consensus mechanisms, we present two ablation studies.", "labels": [], "entities": []}, {"text": "First, we compare neural networks built with consensus (CN) and those without (MLP).", "labels": [], "entities": []}, {"text": "On partial or complete modalities, CN outperforms MLP significantly.", "labels": [], "entities": [{"text": "MLP", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.6808665990829468}]}, {"text": "Second, we compare CNs built with linguistic features divided into random subsets.", "labels": [], "entities": []}, {"text": "Division according to their natural modalities train better consensus networks.", "labels": [], "entities": []}, {"text": "We also visualize the representations during training procedure, and show that when the representations agree, their distributions appear symmetric.", "labels": [], "entities": []}, {"text": "Overall, taking all 413 linguistic features, our models significantly outperform traditional classifiers (e.g., support vector machines, quadratic discriminant analysis, random forest, Gaussian process), which are used by the state-of-the-art.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use DementiaBank, the largest publicly available dataset for detecting cognitive impairments.", "labels": [], "entities": []}, {"text": "It includes verbal descriptions (and associated transcripts) of the Cookie Theft picture description task from the Boston Diagnostic Aphasia Examination.", "labels": [], "entities": [{"text": "Cookie Theft picture description task", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.8289644241333007}, {"text": "Boston Diagnostic Aphasia Examination", "start_pos": 115, "end_pos": 152, "type": "DATASET", "confidence": 0.9728383123874664}]}, {"text": "The version we have access to contains 240 speech samples labeled Control (from 98 people), 234 with AD (from 148 people), and 43 with MCI (from 19 people) . All participants were older than 44 years.", "labels": [], "entities": [{"text": "Control", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.8191035389900208}, {"text": "AD", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9953598380088806}]}, {"text": "We first show the effectiveness of our two improvements to the model.", "labels": [], "entities": []}, {"text": "Next, we do two ablation studies on the arrangements of modalities.", "labels": [], "entities": []}, {"text": "Then, we evaluate our model against several traditional supervised learning classifiers used by stateof-the-art works.", "labels": [], "entities": []}, {"text": "To understand the model further, we also visualize the principal components of the representation vectors throughout several runs.", "labels": [], "entities": []}, {"text": "With the previous sets of experiments, we have a best working architecture.", "labels": [], "entities": []}, {"text": "We now evaluate it against traditional classifiers, which are used by We test several traditional supervised learning benchmark algorithms here: support vector machine (SVM), quadratic discriminant analysis (QDA), random forest (RF), and Gaussian process (GP).", "labels": [], "entities": []}, {"text": "For completeness, multiple layer perceptrons (MLPs) containing all features as inputs are also mentioned in.", "labels": [], "entities": []}, {"text": "On the binary classification task (healthy control vs. dementia), our model does better than them all.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5. On the binary clas- sification task (healthy control vs. dementia), our  model does better than them all.", "labels": [], "entities": []}]}