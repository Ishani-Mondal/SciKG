{"title": [{"text": "Subword Encoding in Lattice LSTM for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Subword Encoding", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8970520198345184}, {"text": "Chinese Word Segmentation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.53472371896108}]}], "abstractContent": [{"text": "We investigate subword information for Chi-nese word segmentation, by integrating sub word embeddings trained using byte-pair encoding into a Lattice LSTM (LaLSTM) network over a character sequence.", "labels": [], "entities": [{"text": "Chi-nese word segmentation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6739168067773184}]}, {"text": "Experiments on standard benchmark show that subword information brings significant gains over strong character-based segmentation models.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first research on the effectiveness of subwords on neural word seg-mentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Chinese word segmentation (CWS) is a traditional NLP task, the features for which have been a central research topic.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7372126579284668}]}, {"text": "Statistical methods consider characters (), subwords (), and words as input features.", "labels": [], "entities": []}, {"text": "Among these, both characters () and words () have also shown useful in recent neural models.", "labels": [], "entities": []}, {"text": "However, how to utilize the subword features in neural networks has not been investigated yet.", "labels": [], "entities": []}, {"text": "In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm and the lattice LSTM structure ( . The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation.", "labels": [], "entities": [{"text": "subword-based neural word segmentor", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.6282185614109039}]}, {"text": "In particular, our baseline is a BiLSTM-CRF segmentor) and we replace LSTM with lattice LSTM using subwords to encode character composition information.", "labels": [], "entities": []}, {"text": "Our code 1 is based on NCRF++ ( ).", "labels": [], "entities": [{"text": "NCRF++", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9387603402137756}]}, {"text": "Compared with character-based neural segmentors, our model can utilize abundant character combination (subword) information, which is effective to disambiguate characters.", "labels": [], "entities": [{"text": "character-based neural segmentors", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.669619063536326}]}, {"text": "For example, in, the subword \" (Academy)\" ensures that the character \"\" means \"Academy(noun)\" rather than \"study(verb)\".", "labels": [], "entities": []}, {"text": "Compared with the word-based neural models (, ambiguous subwords in a context can provide additional information for disambiguation.", "labels": [], "entities": []}, {"text": "For instance, the subword \"(Academy of Sciences)\" and \" (Academy)\" can be useful in determining the correct segmentation, which is \"/(Academy of Sciences/)\".", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to use subwords in a neural network segmentor.", "labels": [], "entities": []}, {"text": "We investigate the contributions of subword lexicons and their pretrained embeddings through controlled experiments.", "labels": [], "entities": []}, {"text": "Results on four benchmarks show that the proposed model can give comparable results with state-of-the-art models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the CTB6 development dataset to investigate the contribution of character bigram information and the subword information.", "labels": [], "entities": [{"text": "CTB6 development dataset", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.96205073595047}]}, {"text": "shows the iteration curve of F-scores against different numbers of training iterations with different character representations.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9513083100318909}]}, {"text": "\" Bigram\" represents the model using both character unigram and bigram information (embedding concatenation).", "labels": [], "entities": []}, {"text": "Character bigram information can improve the baseline significantly.", "labels": [], "entities": []}, {"text": "When the \"LaLSTM+Subword\" structure is added, the model performance is further improved.", "labels": [], "entities": []}, {"text": "This shows that subword information has a great ability to disambiguate the characters.", "labels": [], "entities": []}, {"text": "Zhang and  observed that character bigram information has a negative effect in lattice LSTM on Chinese NER task, while we find a different result on Chinese word segmentation where character bigram information gives significant improvements in the lattice LSTM.", "labels": [], "entities": [{"text": "NER task", "start_pos": 103, "end_pos": 111, "type": "TASK", "confidence": 0.6920144855976105}, {"text": "Chinese word segmentation", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.5912027259667715}]}, {"text": "This is likely because character bigrams are informative but ambiguous.", "labels": [], "entities": []}, {"text": "They can provide more useful character disambiguation evidence in segmentation than in NER where lattice LSTM works well in disambiguating characters.", "labels": [], "entities": [{"text": "character disambiguation", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7006100118160248}, {"text": "segmentation", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.9697983264923096}, {"text": "NER", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.6931634545326233}]}, {"text": "shows the main results and the recent stateof-the-art neural CWS models.", "labels": [], "entities": []}, {"text": "integrated both discrete features and neural features in a transition-based framework.", "labels": [], "entities": []}, {"text": "proposed the dependency-based gated recursive neural network to utilize long distance dependencies.", "labels": [], "entities": []}, {"text": "\u2020 utilized pretrained character representations from multitasks.", "labels": [], "entities": []}, {"text": "We examine their non-pretrained model performance for fair comparison.", "labels": [], "entities": []}, {"text": "built a bidirectional LSTM model with carefully hyperparameter selection.", "labels": [], "entities": []}, {"text": "These methods are orthogonal to and can be integrated into our lattice structure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Main results (F1).", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7600090503692627}, {"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9815828204154968}]}, {"text": " Table 3: Lexicon and embeddings on CTB6.", "labels": [], "entities": [{"text": "CTB6", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9828172922134399}]}]}