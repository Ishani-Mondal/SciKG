{"title": [], "abstractContent": [{"text": "Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data.", "labels": [], "entities": []}, {"text": "In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification.", "labels": [], "entities": []}, {"text": "To reduce model complexity , we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node.", "labels": [], "entities": []}, {"text": "Thus, complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency.", "labels": [], "entities": []}, {"text": "The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the fully-connected attention-based models, like, become popular in natural language processing (NLP) applications, notably machine translation () and language modeling (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8192912042140961}, {"text": "language modeling", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7339320778846741}]}, {"text": "Some recent work also suggest that Transformer can bean alternative to recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in many NLP tasks, such as GPT (), BERT, Transformer-XL ( and Universal Transformer (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9272702932357788}]}, {"text": "More specifically, there are two limitations of the Transformer.", "labels": [], "entities": []}, {"text": "First, the computation and mem- ory overhead of the Transformer are quadratic to the sequence length.", "labels": [], "entities": [{"text": "mem- ory overhead", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.9772414416074753}]}, {"text": "This is especially problematic with long sentences.", "labels": [], "entities": []}, {"text": "Transformer-XL () provides a solution which achieves the acceleration and performance improvement, but it is specifically designed for the language modeling task.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.7771271765232086}]}, {"text": "Second, studies indicate that Transformer would fail on many tasks if the training data is limited, unless it is pre-trained on a large corpus.", "labels": [], "entities": []}, {"text": "(. A key observation is that Transformer does not exploit prior knowledge well.", "labels": [], "entities": []}, {"text": "For example, the local compositionality is already a robust inductive bias for modeling the text sequence.", "labels": [], "entities": []}, {"text": "However, the Transformer learns this bias from scratch, along with non-local compositionality, thereby increasing the learning cost.", "labels": [], "entities": []}, {"text": "The key insight is then whether leveraging strong prior knowledge can help to \"lighten up\" the architecture.", "labels": [], "entities": []}, {"text": "To address the above limitation, we proposed anew lightweight model named \"StarTransformer\".", "labels": [], "entities": [{"text": "StarTransformer", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9243412017822266}]}, {"text": "The core idea is to sparsify the architecture by moving the fully-connected topology into a star-shaped structure.", "labels": [], "entities": []}, {"text": "Star-Transformer has two kinds of connections.", "labels": [], "entities": []}, {"text": "The radical connections preserve the non-local communication and remove the redundancy in fully-connected network.", "labels": [], "entities": []}, {"text": "The ring connections embody the local-compositionality prior, which has the same role as in CNNs/RNNs.", "labels": [], "entities": []}, {"text": "The direct outcome of our design is the improvement of both efficiency and learning cost: the computation cost is reduced from quadratic to linear as a function of input sequence length.", "labels": [], "entities": []}, {"text": "An inherent advantage is that the ring connections can effectively reduce the burden of the unbias learning of local and non-local compositionality and improve the generalization ability of the model.", "labels": [], "entities": []}, {"text": "What remains to be tested is whether one shared relay node is capable of capturing the long-range dependencies.", "labels": [], "entities": []}, {"text": "We evaluate the Star-Transformer on three NLP tasks including Text Classification, Natural Language Inference, and Sequence Labelling.", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7709280848503113}, {"text": "Sequence Labelling", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.8923027813434601}]}, {"text": "Experimental results show that Star-Transformer outperforms the standard Transformer consistently and has less computation complexity.", "labels": [], "entities": []}, {"text": "An additional analysis on a simulation task indicates that StarTransformer preserve the ability to handle with long-range dependencies which is a crucial feature of the standard Transformer.", "labels": [], "entities": [{"text": "StarTransformer", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8938656449317932}]}, {"text": "In this paper, we claim three contributions as the following and our code is available on Github 1 : \u2022 Compared to the standard Transformer, StarTransformer has a lightweight structure but with an approximate ability to model the long-range dependencies.", "labels": [], "entities": []}, {"text": "It reduces the number of connections from n 2 to 2n, where n is the sequence length.", "labels": [], "entities": []}, {"text": "\u2022 The Star-Transformer divides the labor of semantic compositions between the radical and the ring connections.", "labels": [], "entities": []}, {"text": "The radical connections focus on the non-local compositions and the ring connections focus on the local composition.", "labels": [], "entities": []}, {"text": "Therefore, Star-Transformer works for modestly sized datasets and does not rely on heavy pre-training.", "labels": [], "entities": []}, {"text": "\u2022 We design a simulation task \"Masked Summation\" to probe the ability dealing with long-range dependencies.", "labels": [], "entities": []}, {"text": "In this task, we verify that both Transformer and StarTransformer are good at handling long-range dependencies compared to the LSTM and BiLSTM.", "labels": [], "entities": [{"text": "StarTransformer", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8991503715515137}]}], "datasetContent": [{"text": "We evaluate Star-Transformer on one simulation task to probe its behavior when challenged with long-range dependency problem, and three real tasks (Text Classification, Natural Language Inference, and Sequence Labelling).", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7357426583766937}, {"text": "Sequence Labelling", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.8485080301761627}]}, {"text": "All experiments are ran on a NVIDIA Titan X card.", "labels": [], "entities": [{"text": "NVIDIA Titan X card", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.8774948567152023}]}, {"text": "Datasets used in this paper are listed in the Tab-1.", "labels": [], "entities": [{"text": "Tab-1", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9511711001396179}]}, {"text": "We use the Adam () as our optimizer.", "labels": [], "entities": []}, {"text": "On the real task, we set the embedding size to 300 and initialized with GloVe ().", "labels": [], "entities": [{"text": "GloVe", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9717144966125488}]}, {"text": "And the symbol \"Ours + Char\" means an additional character-level pre-trained embedding JMT () is used.", "labels": [], "entities": []}, {"text": "Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT.", "labels": [], "entities": [{"text": "JMT", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.924135684967041}]}, {"text": "We also fix the embedding layer of the Star-Transformer in all experiments.", "labels": [], "entities": []}, {"text": "Since semi-or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo ( and BERT, we exclude these models in the comparison and focus on the relevant architectures.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.5533497333526611}, {"text": "BERT", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9929351210594177}]}], "tableCaptions": [{"text": " Table 1: An overall of datasets and its hyper-parameters, \"H DIM, #head, head DIM\" indicates the dimension of  hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16  \u2020  consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test.", "labels": [], "entities": []}, {"text": " Table 2: Test Accuracy on SST dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9783661365509033}, {"text": "SST dataset", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8884485363960266}]}, {"text": " Table 3: Test Accuracy over MTL-16 datasets. \"Test Time\" means millisecond per batch on the test set (batch size  is 128). \"Len.\" means the average sequence length on the test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9518126845359802}, {"text": "MTL-16 datasets", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7689864933490753}, {"text": "Test Time\"", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.831628660360972}, {"text": "Len.", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9466014504432678}]}, {"text": " Table 4: Test Accuracy on SNLI dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9735041260719299}, {"text": "SNLI dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7745680212974548}]}, {"text": " Table 5: Results on sequence labeling tasks. We list the \"Advanced Techniques\" except widely-used pre-trained  embeddings (GloVe, Word2Vec, JMT) in columns. The \"Char\" indicates character-level features, it also includes  the Capitalization Features, Suffix Feature, Lexicon Features, etc. The \"CRF\" means an additional Conditional  Random Field (CRF) layer.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7428511679172516}, {"text": "Word2Vec", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.9044069647789001}]}, {"text": " Table 6: Test Accuracy on SNLI dataset, CoNLL2003  NER task and the Masked Summation n = 200, k =  10, d = 10.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9115498065948486}, {"text": "SNLI dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.8363664448261261}, {"text": "CoNLL2003  NER task", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.7718597253163656}]}]}