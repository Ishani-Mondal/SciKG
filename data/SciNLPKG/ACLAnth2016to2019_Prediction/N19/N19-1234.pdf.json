{"title": [{"text": "Latent Code and Text-based Generative Adversarial Networks for Soft-text Generation", "labels": [], "entities": [{"text": "Soft-text Generation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7173390686511993}]}], "abstractContent": [{"text": "Text generation with generative adversarial networks (GANs) can be divided into the text-based and code-based categories according to the type of signals used for discrimination.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7844022512435913}]}, {"text": "In this work, we introduce a novel text-based approach called Soft-GAN to effectively exploit GAN setup for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7722405791282654}]}, {"text": "We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which we will refer to as soft-text.", "labels": [], "entities": []}, {"text": "This soft representation will be used in GAN discrimination to synthesize similar soft-texts.", "labels": [], "entities": [{"text": "GAN discrimination", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9834139943122864}]}, {"text": "We also propose hybrid latent code and text-based GAN (LATEXT-GAN) approaches with one or more discriminators, in which a combination of the latent code and the soft-text is used for GAN discriminations.", "labels": [], "entities": [{"text": "GAN discriminations", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.9850920140743256}]}, {"text": "We perform a number of subjective and objective experiments on two well-known datasets (SNLI and Image COCO) to validate our techniques.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.7815356254577637}]}, {"text": "We discuss the results using several evaluation metrics and show that the proposed techniques outperform the traditional GAN-based text-generation methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text generation is an active research area and has many real-world applications, including, but not limited to, machine translation (), AI chat bots (, image captioning ( , question answering and information retrieval ( . Recurrent neural network language models (RNNLMs) () is the most popular approach for text generation which rely on maximum likelihood estimation (MLE) solutions such as teacher forcing) (i.e. the model is trained to predict the next word given all the previous predicted words); however, it is well-known in the literature that MLE is a simplistic objective for this complex NLP task (.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.804658442735672}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8177124261856079}, {"text": "image captioning", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7252665907144547}, {"text": "question answering", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.783673882484436}, {"text": "information retrieval", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.6992288380861282}, {"text": "text generation", "start_pos": 308, "end_pos": 323, "type": "TASK", "confidence": 0.7271215617656708}]}, {"text": "It is reported that MLEbased methods suffer from exposure bias, which means that at training time the model is exposed to gold data only, but attest time it observes its own predictions.", "labels": [], "entities": []}, {"text": "Hence, wrong predictions quickly accumulate and result in poor text generation quality.", "labels": [], "entities": []}, {"text": "However, generative adversarial networks (GANs) () which are based on an adversarial loss function suffers less from the mentioned problems of the MLE solutions.", "labels": [], "entities": []}, {"text": "The great success of GANs in image generation framework) motivated researchers to apply its framework to NLP applications as well.", "labels": [], "entities": [{"text": "image generation framework", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.8142978350321451}]}, {"text": "GANs have been extensively used recently in various NLP applications such as machine translation (), dialogue models (, question answering (, and natural language generation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8263156712055206}, {"text": "question answering", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.8993842303752899}, {"text": "natural language generation", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.6799355943997701}]}, {"text": "However, applying GAN in NLP is challenging due to the discrete nature of the text.", "labels": [], "entities": []}, {"text": "Consequently, back-propagation would not be feasible for discrete outputs and it is not straightforward to pass the gradients through the discrete output words of the generator.", "labels": [], "entities": []}, {"text": "Traditional methods for GAN-based text generation can be categorized according to the type of the signal used for discrimination into two categories: text-based and code-based techniques.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.9075260361035665}]}, {"text": "Code-based methods such as adversarial autoencoder (AAE) () and adversarially regularized AE (ARAE) () derive a latent space representation of the text using an AE and attempt to learn data manifold of that latent space () instead of mod-eling text directly.", "labels": [], "entities": [{"text": "AE (ARAE)", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.8854954540729523}]}, {"text": "Text-based solutions, such as reinforcement learning (RL) based methods or approaches based on continuous approximation of discrete sampling, focus on generating text directly from the generator.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.668670266866684}]}, {"text": "RL-based methods treat the distribution of GAN generator as a stochastic policy and hold the discriminator responsible for providing proper reward for the generator's actions.", "labels": [], "entities": []}, {"text": "However, the RL-based methods often need pre-training and are computationally more expensive compared to the methods of the other two categories.", "labels": [], "entities": []}, {"text": "In the continuous approximation approach for generating text with GANs, the goal is to find a continuous approximation of the discrete sampling by using the Gumbel Softmax technique or approximating the non-differentiable argmax operator with a continuous function (.", "labels": [], "entities": []}, {"text": "In this paper, we introduce Soft-GAN as anew solution for the main bottleneck of using GAN for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7863423526287079}]}, {"text": "Our solution is based on an AE to derive a soft representation of the real text (i.e. soft-text).", "labels": [], "entities": [{"text": "AE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9960049986839294}]}, {"text": "This soft-text is fed to the GAN discriminator instead of the conventional one-hot representation used in ().", "labels": [], "entities": []}, {"text": "Furthermore, we propose hybrid latent code and text-based GAN (LATEXT-GAN) approaches and show that how we can improve code-based and text-based text generation techniques by considering both signals in the GAN framework.", "labels": [], "entities": [{"text": "text-based text generation", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.6581707696119944}]}, {"text": "We summarize the main contributions of this paper as: \u2022 We introduce anew text-based solution Soft-GAN using the above soft-text discrimination.", "labels": [], "entities": []}, {"text": "We also demonstrate the rationale behind this approach.", "labels": [], "entities": []}, {"text": "\u2022 We introduce LATEXT-GAN approaches for GAN-based text generation using both latent code and soft-text discrimination.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8826723297437032}]}, {"text": "To the best of our knowledge, this is the first time where a GAN-based text generation framework uses both code and text-based discrimination.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.8372278809547424}]}, {"text": "\u2022 We evaluate our methods using subjective and objective evaluation metrics.", "labels": [], "entities": []}, {"text": "We show that our proposed approaches outperform the conventional GAN-based text generation techniques that do not need pre-training.", "labels": [], "entities": [{"text": "GAN-based text generation", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.8308277328809103}]}], "datasetContent": [{"text": "We do our experiments on two different datasets: the Stanford Natural Language Inference (SNLI) corpus 1 , which contains 714,667 sentences for training and 13323 sentences for testing, and the Image COCO 2 dataset's image caption annotations, where we sample 3 10,000 sentences as training set and another 10,000 as test set ().", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus 1", "start_pos": 53, "end_pos": 104, "type": "DATASET", "confidence": 0.7730124062962003}, {"text": "Image COCO 2 dataset", "start_pos": 194, "end_pos": 214, "type": "DATASET", "confidence": 0.859238862991333}]}, {"text": "For the SNLI dataset, we use a vocabulary size of 10000 words and use the maximum sentence length of size 15.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.7808542847633362}]}, {"text": "For the COCO dataset, we use a vocabulary size of 5000 and perform experiments using the maximum sentence length of sizes 15 and 20.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.932468831539154}]}, {"text": "We train a simple AE using one layer with 512 LSTM cells (Hochreiter and Schmidhuber, 1997) for both the encoder and the decoder.", "labels": [], "entities": []}, {"text": "For decoding, the output from the previous time step is used as the input to the next time step.", "labels": [], "entities": []}, {"text": "We use the hidden code c from the last time step of the encoder and applied as an additional input at each time step of decoding.", "labels": [], "entities": []}, {"text": "We normalize the code and then added an exponentially decaying noise before decoding.", "labels": [], "entities": []}, {"text": "The greedy search approach is applied to get the best output.", "labels": [], "entities": []}, {"text": "We train the autoencoder using Adam) optimizer with learning rate = 0.001, \u03b2 1 = 0.9, and \u03b2 2 = 0.999.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.960919976234436}]}, {"text": "We use CNN-based generator and discriminator with residual blocks (.", "labels": [], "entities": []}, {"text": "The tanh function is applied on the output of the ARAE generator (.", "labels": [], "entities": [{"text": "ARAE generator", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8410511314868927}]}, {"text": "We train the generator and the discriminator using Adam optimizer with learning rate = 0.0001, \u03b2 1 = 0.5, and \u03b2 2 = 0.9.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.9624159634113312}]}, {"text": "We do not apply any kind of attention mechanisms () and pre-training () in our experiments.", "labels": [], "entities": []}, {"text": "We use the WGAN-GP (Gulrajani et al., 2017) approach with 5 discriminator updates for every generator update and a gradient penalty co-efficient of \u03bb=10 unlike a setup in ().", "labels": [], "entities": [{"text": "WGAN-GP", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9305055737495422}]}, {"text": "For the AAE-based experiments, we normalize the data drawn from a prior distribution z.", "labels": [], "entities": [{"text": "AAE-based", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.6049138307571411}]}, {"text": "We train the models for 200000 iterations wherein each iteration we sample a random batch and train the networks of the models.", "labels": [], "entities": []}, {"text": "We use the frequently used BLEU metric) to evaluate the word similarity between sentences and the perplexity to evaluate our techniques.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9794949293136597}]}, {"text": "We calculate BLEU-n scores for n-grams without a brevity penalty (.", "labels": [], "entities": [{"text": "BLEU-n scores", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9741836488246918}]}, {"text": "The results with the best BLEU-n scores in the synthesized generated texts are reported.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9991414546966553}]}, {"text": "To calculate the BLEU-n scores, we generate ten batches of sentences as candidate texts, i.e. 640 sentences and use the entire test set as reference texts.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9981884360313416}]}, {"text": "As the GAN-based models usually suffer from mode collapse (i.e., generating same samples over and over again), evaluating models by only BLEU metric is not appropriate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9984276294708252}]}, {"text": "So, we also calculate recently proposed self-BLEU scores for the COCO dataset using maximum sentence length of size 20 and 10k synthetic sentences to evaluate the diversity (.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9587382078170776}]}, {"text": "Using one synthetic sentence as hypothesis and others as reference, the BLEU is calculated for every synthetic sentence, and define the average BLEU score as the self-BLEU (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9989783763885498}, {"text": "BLEU score", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9806647002696991}]}, {"text": "A higher self-BLEU score describe less diversity.", "labels": [], "entities": []}, {"text": "For the perplexity evaluations, we generate 100k and 10k sentences for the SNLI and the COCO datasets respectively using the models of the last iteration.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8979181051254272}, {"text": "COCO datasets", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9499184191226959}]}, {"text": "The BLEU score results for the n-grams of the synthesized texts are depicted in with maximum sentence length of 15 for the SNLI and the COCO datasets respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9816161096096039}, {"text": "SNLI", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.8882545232772827}, {"text": "COCO datasets", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.9615871906280518}]}, {"text": "We also report experimental results with a longer maximum sentence length of 20 using the COCO dataset to differentiate the effectiveness of code and textbased solutions (in).", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9641328454017639}]}, {"text": "Furthermore, we report the BLEU and self-BLEU score results of our proposed approaches in and 6 respectively for the COCO dataset to compare with the results of the existing approaches reported in (.: BLEU-n (B-n) scores results using SNLI dataset and 640 synthetic sentences From tables 2, 3, and 4, we can see that our proposed approaches outperform the standalone code (AAE or ARAE) and text-based (IWGAN) solutions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9985270500183105}, {"text": "COCO dataset", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.9731230437755585}, {"text": "BLEU-n (B-n) scores", "start_pos": 201, "end_pos": 220, "type": "METRIC", "confidence": 0.9096532464027405}, {"text": "SNLI dataset", "start_pos": 235, "end_pos": 247, "type": "DATASET", "confidence": 0.949044793844223}]}, {"text": "For the maximum sentence length of size 15 experiments, the LATEXT-GAN I is better than LATEXT-GAN II and III for shorter length text (e.g., 2,3-grams).", "labels": [], "entities": [{"text": "LATEXT-GAN I", "start_pos": 60, "end_pos": 72, "type": "METRIC", "confidence": 0.7532066106796265}]}, {"text": "The performance of the LATEXT-GAN II and III degrades with increasing maximum sentence length to 20.", "labels": [], "entities": []}, {"text": "This is be-: BLEU-n (B-n) scores results for COCO dataset with maximum sentence length of size 15 and 640 synthetic sentences: BLEU-n (B-n) scores results for COCO dataset with maximum sentence length of size 20 and over 640 synthetic sentences cause for longer sequence length experiments, the hidden code of the last time step might not be able to keep all the information from the earlier time steps.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9986567497253418}, {"text": "COCO dataset", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9391158521175385}, {"text": "BLEU-n", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9985212683677673}, {"text": "COCO dataset", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.9021439254283905}]}, {"text": "On the other hand, the LATEXT-GAN I and the Soft-GAN improve their performance with increasing maximum sentence length to 20.", "labels": [], "entities": []}, {"text": "This might be because of the encoder enhances its representation better to the prior distribution, z from which the text is generated.", "labels": [], "entities": []}, {"text": "Furthermore, the Soft-GAN outperforms all the proposed LATEXT GAN approaches.", "labels": [], "entities": []}, {"text": "We also compare our proposed approaches with, some RLbased approaches (SeqGAN ( ,,) and MLE approach described in a benchmark platform ( where they apply pre-training before applying adversarial training.", "labels": [], "entities": [{"text": "MLE", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.7828086018562317}]}, {"text": "We evaluate the BLEU and Self-BLEU score results on 10k synthetic sentences using the maximum sentence length of size 20 for the COCO dataset with a vocabulary of size 5000 as in ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9987738728523254}, {"text": "COCO dataset", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.9598216414451599}]}, {"text": "The BLEU and the self-BLEU score results are reported in and 6 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9984883069992065}]}, {"text": "From, it can be noted that our proposed approaches show comparable results to the RL-based solutions for the BLEU score results.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9564543962478638}]}, {"text": "We can also see that our proposed LATEXT-GAN III approach gives lower self-BLEU scores in.", "labels": [], "entities": []}, {"text": "From the above experimental results, we can note that LATEXT-GAN III can generate real-like and more diverse sentences compare to some approaches reported in ( and our other proposed approaches.: BLEU-n (B-n) scores results for COCO dataset using sequence length 20 and 10k generated sentences  The forward and reverse perplexities of the LMs trained with maximum sentence length of 15 and 20 using the SNLI and the COCO datasets respectively are described in.", "labels": [], "entities": [{"text": "BLEU-n (B-n) scores", "start_pos": 196, "end_pos": 215, "type": "METRIC", "confidence": 0.9053375959396363}, {"text": "COCO dataset", "start_pos": 228, "end_pos": 240, "type": "DATASET", "confidence": 0.8573046028614044}, {"text": "SNLI", "start_pos": 403, "end_pos": 407, "type": "DATASET", "confidence": 0.9284225702285767}, {"text": "COCO datasets", "start_pos": 416, "end_pos": 429, "type": "DATASET", "confidence": 0.9610314667224884}]}, {"text": "The forward perplexities (F-PPL) are calculated by training an RNN language model () on real training data and evaluated on the synthetic samples.", "labels": [], "entities": [{"text": "F-PPL", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9382525086402893}]}, {"text": "This measure describe the fluency of the synthetic samples.", "labels": [], "entities": []}, {"text": "We also calculate the reverse perplexities (R-PPL) by training an RNNLM on the synthetic samples and evaluated on the real test data.", "labels": [], "entities": [{"text": "R-PPL", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9273984432220459}]}, {"text": "We can easily compare the performance of the LMs by using the forward perplexities while it is not possible by using the reverse perplexities as the models are trained using the synthetic samples with different vocabulary sizes.", "labels": [], "entities": []}, {"text": "The perplexities of the LMs using real data are 16.01 and 67.05 for the SNLI and the COCO datasets respectively reported in F-PPL column.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9072138071060181}, {"text": "COCO datasets", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9603162109851837}, {"text": "F-PPL", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.873162031173706}]}, {"text": "From the tables, we can note the models with lower forward perplexities (higher fluency) for the synthetic samples tend to have higher reverse perplexities except the AAE-based models () and/or the IWGAN.", "labels": [], "entities": [{"text": "AAE-based", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.7182687520980835}, {"text": "IWGAN", "start_pos": 198, "end_pos": 203, "type": "DATASET", "confidence": 0.8954347372055054}]}, {"text": "The forward perplexity for the IWGAN is the worst which means that the synthetic sentences of the IWGAN model are not fluent or real-like sentences.", "labels": [], "entities": [{"text": "IWGAN", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.907049298286438}]}, {"text": "For the SNLI dataset, we can note that the LATEXT-GAN II and III approaches can generate more fluent sentences than the other approaches.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.7564355731010437}]}, {"text": "For the COCO dataset, it can be seen that the forward perplexity of the LATEXT-GAN I (51.39) is far lower than the real data (67.05) which means the model suffers from mode-collapse.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9389934539794922}, {"text": "forward perplexity", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.9485110938549042}, {"text": "LATEXT-GAN I", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.7140829563140869}]}, {"text": "The Soft-GAN, the LATEXT-GAN II and III approaches suffer less from the mode-collapse.: Forward (F) and Reverse (R) perplexity (PPL) results for the SNLI and COCO datasets using synthetic sentences of maximum length 15 and 20 respectively.", "labels": [], "entities": [{"text": "Forward (F)", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.955618143081665}, {"text": "Reverse (R) perplexity (PPL)", "start_pos": 104, "end_pos": 132, "type": "METRIC", "confidence": 0.950572744011879}, {"text": "SNLI and COCO datasets", "start_pos": 149, "end_pos": 171, "type": "DATASET", "confidence": 0.6708722859621048}]}, {"text": "The subjective judgments of the synthetic sentences of the models trained using the COCO dataset with maximum sentence length of size 20 is reported in.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9641834795475006}]}, {"text": "We used 20 different random synthetic sentences generated by using the last iteration of each model and gave them to a group of 5 people.", "labels": [], "entities": []}, {"text": "We asked them to rate the sentences based on a 5-point Likert scale according to their fluency.", "labels": [], "entities": []}, {"text": "The raters are asked to score 1 which corresponds to gibberish, 3 corresponds to understandable but ungrammatical, and 5 correspond to naturally constructed and understandable sentences (.", "labels": [], "entities": []}, {"text": "From, we can note that the proposed LATEXT-GAN III approach get the higher rate compare to the other approaches.", "labels": [], "entities": [{"text": "LATEXT-GAN III", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.4604755640029907}]}, {"text": "From all the above different evaluations, we can note that the synthetic sentences by using the LATEXT-GAN II and III approaches are more balanced in diversity and fluency compare to the other approaches.", "labels": [], "entities": []}, {"text": "We also depicted some examples of the synthetic sentences for the COCO and the SNLI datasets in and 10 respectively.", "labels": [], "entities": [{"text": "COCO", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9343011379241943}, {"text": "SNLI datasets", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9466005265712738}]}], "tableCaptions": [{"text": " Table 2: BLEU-n (B-n) scores results using SNLI  dataset and 640 synthetic sentences", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9986573457717896}, {"text": "SNLI  dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9293659031391144}]}, {"text": " Table 3: BLEU-n (B-n) scores results for COCO  dataset with maximum sentence length of size 15 and  640 synthetic sentences", "labels": [], "entities": [{"text": "BLEU-n (B-n) scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8630381941795349}, {"text": "COCO  dataset", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8899320662021637}]}, {"text": " Table 4: BLEU-n (B-n) scores results for COCO  dataset with maximum sentence length of size 20 and  over 640 synthetic sentences", "labels": [], "entities": [{"text": "BLEU-n (B-n) scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8605733156204224}, {"text": "COCO  dataset", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8831815421581268}]}, {"text": " Table 5: BLEU-n (B-n) scores results for COCO dataset  using sequence length 20 and 10k generated sentences", "labels": [], "entities": [{"text": "BLEU-n (B-n) scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8447574734687805}, {"text": "COCO dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.910641223192215}]}, {"text": " Table 6: Self-BLEU scores results for COCO dataset  using sequence length 20 and 10k generated sentences", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9234389066696167}]}, {"text": " Table 7: Forward (F) and Reverse (R) perplexity (PPL)  results for the SNLI and COCO datasets using synthetic  sentences of maximum length 15 and 20 respectively.", "labels": [], "entities": [{"text": "Forward (F)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9392741471529007}, {"text": "Reverse (R) perplexity (PPL)", "start_pos": 26, "end_pos": 54, "type": "METRIC", "confidence": 0.9502649083733559}, {"text": "SNLI and COCO datasets", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.7113996595144272}]}, {"text": " Table 8: Human Evaluation on the synthetic sentences", "labels": [], "entities": []}]}