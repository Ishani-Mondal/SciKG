{"title": [{"text": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "labels": [], "entities": [{"text": "Predicting Annotation Difficulty", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8841595649719238}, {"text": "Improve Task Routing", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7936095595359802}, {"text": "Biomedical Information Extraction", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.642192413409551}]}], "abstractContent": [{"text": "Modern NLP systems require high-quality annotated data.", "labels": [], "entities": []}, {"text": "In specialized domains, expert annotations maybe prohibitively expensive.", "labels": [], "entities": []}, {"text": "An alternative is to rely on crowdsourcing to reduce costs at the risk of introducing noise.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that directly model-ing instance difficulty can be used to improve model performance, and to route instances to appropriate annotators.", "labels": [], "entities": []}, {"text": "Our difficulty prediction model combines two learned representations: a 'universal' encoder trained on out-of-domain data, and a task-specific encoder.", "labels": [], "entities": [{"text": "difficulty prediction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7069466859102249}]}, {"text": "Experiments on a complex biomedical information extraction task using expert and lay anno-tators show that: (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance; (ii) using difficulty scores to weight instances during training provides further, consistent gains; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing.", "labels": [], "entities": [{"text": "biomedical information extraction task", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7144925147294998}, {"text": "task routing", "start_pos": 425, "end_pos": 437, "type": "TASK", "confidence": 0.7169764935970306}]}, {"text": "Our experiments confirm the expectation that for specialized tasks expert annotations are higher quality than crowd labels, and hence preferable to obtain if practical.", "labels": [], "entities": []}, {"text": "Moreover , augmenting small amounts of expert data with a larger set of lay annotations leads to further improvements in model performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Assembling training corpora of annotated natural language examples in specialized domains such as biomedicine poses considerable challenges.", "labels": [], "entities": []}, {"text": "Experts with the requisite domain knowledge to perform high-quality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations.", "labels": [], "entities": []}, {"text": "A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 109, "end_pos": 139, "type": "DATASET", "confidence": 0.8109557131926218}]}, {"text": "However, crowd workers in general are likely to provide noisy annotations), an issue exacerbated by the technical nature of specialized content.", "labels": [], "entities": []}, {"text": "Some of this noise may reflect worker quality and can be modeled, but for some instances laypeople may simply lack the domain knowledge to provide useful annotation.", "labels": [], "entities": []}, {"text": "In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature.", "labels": [], "entities": [{"text": "EBM-NLP corpus", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.81428062915802}]}, {"text": "We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.8198904097080231}]}, {"text": "We then obtain expert annotations for the abstracts predicted to be most difficult, as well as fora similar number of randomly selected abstracts.", "labels": [], "entities": []}, {"text": "The annotation of highly specialized data and the use of lay and expert annotators allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty?", "labels": [], "entities": []}, {"text": "We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling.", "labels": [], "entities": []}, {"text": "We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Further, such predictions can be used during training to improve information extraction models.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8400943875312805}]}, {"text": "Are there systematic differences between expert and lay annotations?", "labels": [], "entities": []}, {"text": "We observe decidedly lower agreement between lay workers as compared to domain experts.", "labels": [], "entities": [{"text": "agreement", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9703506827354431}]}, {"text": "Lay annotations have high precision but low recall with respect to expert annota-tions in the new data that we collected.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9989890456199646}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9994502663612366}]}, {"text": "More generally, we expect lay annotations to be lower quality, which may translate to lower precision, recall, or both, compared to expert annotations.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9992268085479736}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.998982846736908}]}, {"text": "Can one rely solely on lay annotations?", "labels": [], "entities": []}, {"text": "Reasonable models can be trained using lay annotations alone, but similar performance can be achieved using markedly less expert data.", "labels": [], "entities": []}, {"text": "This suggests that the optimal ratio of expert to crowd annotations for specialized tasks will depend on the cost and availability of domain experts.", "labels": [], "entities": []}, {"text": "Expert annotations are preferable whenever its collection is practical.", "labels": [], "entities": []}, {"text": "But in real-world settings, a combination of expert and lay annotations is better than using lay data alone.", "labels": [], "entities": []}, {"text": "Does it matter what data is annotated by experts?", "labels": [], "entities": []}, {"text": "We demonstrate that a system trained on combined data achieves better predictive performance when experts annotate difficult examples rather than instances selected at i.i.d. random.", "labels": [], "entities": []}, {"text": "Our contributions in this work are summarized as follows.", "labels": [], "entities": []}, {"text": "We define a task difficulty prediction task and show how this is related to, but distinct from, inter-worker agreement.", "labels": [], "entities": [{"text": "task difficulty prediction task", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6841685473918915}]}, {"text": "We introduce anew model for difficulty prediction combining learned representations induced via a pre-trained 'universal' sentence encoder, and a sentence encoder learned from scratch for this task.", "labels": [], "entities": [{"text": "difficulty prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7156296372413635}]}, {"text": "We show that predicting annotation difficulty can be used to improve the task routing and model performance fora biomedical information extraction task.", "labels": [], "entities": [{"text": "predicting annotation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.825854480266571}, {"text": "task routing", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.7247761785984039}, {"text": "biomedical information extraction task", "start_pos": 113, "end_pos": 151, "type": "TASK", "confidence": 0.7117434740066528}]}, {"text": "Our results open up anew direction for ensuring corpus quality.", "labels": [], "entities": []}, {"text": "We believe that item difficulty prediction will likely be useful in other, nonspecialized tasks as well, and that the most effective data collection in specialized domains requires research addressing the fundamental questions we examine here.", "labels": [], "entities": [{"text": "item difficulty prediction", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.5923571586608887}]}], "datasetContent": [{"text": "We trained models for each label type separately.", "labels": [], "entities": []}, {"text": "Word embeddings were initialized to 300d GloVe vectors () trained on common crawl data; 2 these are fine-tuned during training.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer) with learning rate and decay set to 0.001 and 0.99, respectively.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9263719320297241}, {"text": "decay", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.7956520915031433}]}, {"text": "We used batch sizes of 16.", "labels": [], "entities": []}, {"text": "We used the large version of the universal sentence encoder 3 with a transformer ().", "labels": [], "entities": []}, {"text": "We did not update the pretrained sentence encoder parameters during training.", "labels": [], "entities": []}, {"text": "All hyperparamaters for all models (including hidden layers, hidden sizes, and dropout) were tuned using Vizier () via 10-fold cross validation on the training set maximizing for F1.", "labels": [], "entities": [{"text": "Vizier", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8961048722267151}, {"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.8004153370857239}]}, {"text": "As a baseline, we also trained a linear SupportVector Regression) model on ngram features (n ranges from 1 to 3).", "labels": [], "entities": []}, {"text": "5 4 This performs random search over the number of hidden layers (1-5), hidden sizes (128-1024), and dropout (0.1-0.5).", "labels": [], "entities": []}, {"text": "We perform gridsearch over the hyperparameter C.", "labels": [], "entities": []}, {"text": "For all models, correlations for Intervention and Outcomes are higher than for Population, which is expected given the difficulty distributions in.", "labels": [], "entities": [{"text": "Intervention", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9342839121818542}]}, {"text": "In these, the sentences are more uniformly distributed, with a fair number of difficult and easier sentences.", "labels": [], "entities": []}, {"text": "By contrast, in Population there area greater number of easy sentences and considerably fewer difficult sentences, which makes the difficulty ranking task particularly challenging.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average inter-worker agreement.", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation coefficients of sentence  difficulty predictions.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8791148960590363}, {"text": "sentence  difficulty predictions", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7513304054737091}]}, {"text": " Table 4: Medical IE performance by re-weighting sentences according to predicted agreement or difficulty scores.", "labels": [], "entities": [{"text": "Medical IE", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.4954947382211685}]}, {"text": " Table 5: Interventions IE model performance trained  crowd or expert. The first four models are trained with  a subset of 1k abstracts and the base model is trained  with all 5k abstracts.", "labels": [], "entities": [{"text": "Interventions IE", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6944700479507446}]}, {"text": " Table 6: Interventions IE model performance trained by  mixing annotations from experts and crowd workers.  [D]: Difficult-Expert; [R]: Random-Expert; [Other]:  the rest of the abstracts with crowd annotation only.", "labels": [], "entities": [{"text": "Interventions IE", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7143118977546692}]}]}