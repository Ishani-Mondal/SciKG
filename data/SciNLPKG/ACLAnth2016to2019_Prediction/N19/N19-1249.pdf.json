{"title": [{"text": "SC-LSTM: Learning Task-Specific Representations in Multi-Task Learning for Sequence Labeling", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.89574134349823}]}], "abstractContent": [{"text": "Multi-task learning (MTL) has been studied recently for sequence labeling.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8567913174629211}, {"text": "sequence labeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.6999870538711548}]}, {"text": "Typically, auxiliary tasks are selected specifically in order to improve the performance of a target task.", "labels": [], "entities": []}, {"text": "Jointly learning multiple tasks in away that benefit all of them simultaneously can increase the utility of MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9687257409095764}]}, {"text": "In order to do so, we propose anew LSTM cell which contains both shared parameters that can learn from all tasks, and task-specific parameters that can learn task specific information.", "labels": [], "entities": []}, {"text": "We name it a Shared-Cell Long-Short Term Memory (SC-LSTM).", "labels": [], "entities": []}, {"text": "Experimental results on three sequence labeling benchmarks (named-entity recognition, text chunking, and part-of-speech tagging) demonstrate the effectiveness of our SC-LSTM cell.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7087917774915695}, {"text": "text chunking", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.7811591625213623}, {"text": "part-of-speech tagging", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7148286402225494}]}], "introductionContent": [{"text": "As one of the fundamental tasks in NLP, sequence labeling has been studied for years.", "labels": [], "entities": [{"text": "NLP", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9415273070335388}, {"text": "sequence labeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6729978024959564}]}, {"text": "Before the blooming of neural network methods, handcrafted features were widely used in traditional approaches like CRFs, HMMs, and maximum entropy classifiers (.", "labels": [], "entities": []}, {"text": "However, applying them to different tasks or domains is hard.", "labels": [], "entities": []}, {"text": "Recently, instead of using handcrafted features, end-to-end neural network based systems have been developed for sequence labeling tasks, such as LSTM-CNN (, LSTM-CRF (, and LSTM-CNN-CRF ().", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7405872742335001}]}, {"text": "These models utilize LSTM to encode the global information of a sentence into a word-level representation of its tokens, which avoids manual feature engineering.", "labels": [], "entities": []}, {"text": "Moreover, by incorporating a character-level representation of tokens, these models further improve.", "labels": [], "entities": []}, {"text": "In many such studies, though, neural network models are trained toward a single task in a supervised way by making use of relatively small annotated training material.", "labels": [], "entities": []}, {"text": "Jointly learning multiple tasks can reduce the risk of over-fitting to one task, and many attempts have been made at doing so for sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7578421831130981}]}, {"text": "Results so far are not conclusive.", "labels": [], "entities": []}, {"text": "Some works have reported negative results overall.", "labels": [], "entities": []}, {"text": "For instance in their pioneering work, observed that training their model on NER, POS tagging and chunking altogether led to slight decrease in performance compared to a similar model trained on each task separately.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.7284692227840424}]}, {"text": "study chunking and CCG super tagging, coupled with an additional POS tagging task.", "labels": [], "entities": [{"text": "chunking", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.9606530666351318}, {"text": "CCG super tagging", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.636542002360026}, {"text": "POS tagging task", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7757171392440796}]}, {"text": "They do report gains on both target tasks over single task models, but results varied depending where the additional task was taken care of in their architecture.", "labels": [], "entities": []}, {"text": "The authors actually reported a failure to leverage other labelling tasks, and concluded that combined tasks should be sufficiently similar to the target one, for significant gains to be observed.", "labels": [], "entities": []}, {"text": "Similarly, Alonso and Plank (2017) achieved significant improvements for only 1 out of 5 tasks considered.", "labels": [], "entities": []}, {"text": "Also of interest is the work of ( where the authors investigate the classical shared encoder-based MTL framework) on 11 sequence labeling datasets including POS, NER, and chunking.", "labels": [], "entities": []}, {"text": "They report that chunking is beneficial to NER, while POS tagging can be harmful.", "labels": [], "entities": [{"text": "chunking", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.9773231744766235}, {"text": "NER", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9292975068092346}, {"text": "POS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7831078469753265}]}, {"text": "We present in Section 2 the two major approaches proposed for multi-task learning and discuss their limitations.", "labels": [], "entities": []}, {"text": "We describe our approach in Section 3, and present our experimental settings and results in Section 4 and 5 respectively.", "labels": [], "entities": []}, {"text": "We further analyze our approach in Section 6, discuss related works in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we start by comparing MTL approaches based on LSTM and SC-LSTM cells.", "labels": [], "entities": [{"text": "MTL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9790738821029663}]}, {"text": "We then report the performance of variants of our approach we implemented and compare it to stateof-the-art models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main characteristics of the datasets used.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameters used for training the SC- LSTM-LM-CNN model.", "labels": [], "entities": []}, {"text": " Table 3: Results of models being trained in STL or  MTL mode. For all MTL models, we report the best  performance via a small grid search over combinations  of the hidden size", "labels": [], "entities": []}, {"text": " Table 4: F1-score on the CoNLL03 NER dataset. Mod- els with \u2663 use both train and dev splits for training.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988669157028198}, {"text": "CoNLL03 NER dataset", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.8964835206667582}]}, {"text": " Table 5: F1-score on the CoNLL00 chunking dataset.  Configurations with a : sign are approaches we reim- plemented.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989404082298279}, {"text": "CoNLL00 chunking dataset", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.9058911800384521}]}, {"text": " Table 6. The only study we found that  reports results on the UD v1.3 benchmark we used  here is (", "labels": [], "entities": [{"text": "UD v1.3 benchmark", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.704058309396108}]}]}