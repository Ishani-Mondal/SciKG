{"title": [{"text": "An Encoding Strategy Based Word-Character LSTM for Chinese NER", "labels": [], "entities": [{"text": "NER", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.6280434727668762}]}], "abstractContent": [{"text": "A recently proposed lattice model has demonstrated that words in character sequence can provide rich word boundary information for character-based Chinese NER model.", "labels": [], "entities": []}, {"text": "In this model, word information is integrated into a shortcut path between the start and the end characters of the word.", "labels": [], "entities": []}, {"text": "However, the existence of shortcut path may cause the model to degenerate into a partial word-based model, which will suffer from word segmentation errors.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.6858739405870438}]}, {"text": "Furthermore, the lattice model cannot be trained in batches due to its DAG structure.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel word-character LSTM(WC-LSTM) model to add word information into the start or the end character of the word, alleviating the influence of word segmentation errors while obtaining the word boundary information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.6702654659748077}]}, {"text": "Four different strategies are explored in our model to encode word information into a fixed-sized representation for efficient batch training.", "labels": [], "entities": []}, {"text": "Experiments on benchmark datasets show that our proposed model outperforms other state-of-the-arts models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Name Entity Recognition(NER) is a basic task of many NLP systems including Information Retrieval (, Relationship Extraction (, Question Answering ().", "labels": [], "entities": [{"text": "Name Entity Recognition(NER)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8183831224838892}, {"text": "Information Retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7680169939994812}, {"text": "Relationship Extraction", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7162958085536957}, {"text": "Question Answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7741494476795197}]}, {"text": "The main task of NER is to identify named entities such as person, location, organization, etc.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9842925667762756}]}, {"text": "Various methods have been proposed to tackle this problem, including Hidden Markov Models(HMMs) (, Maximum Entropy Models(ME) (, Support Vector Machines(SVM) and Conditional Random Fields(CRF)).", "labels": [], "entities": []}, {"text": "With the development of deep learning, neural net-: An example of the lattice model degenerates into a partial word-based model.", "labels": [], "entities": []}, {"text": "Due to the shortcut path \"\u6c5f(River)\" \u2192 \"\u6c5f \u6c34(River Water)\" \u2192 \"\u6c34(Water)\", the model incorrectly predicts that \"\u6c5f(River)\" and \"\u6c34(Water)\" belong to the same entity.", "labels": [], "entities": []}, {"text": "Red labels(without underline) denote predicted labels, and blue labels(with underline) denote gold labels.", "labels": [], "entities": []}, {"text": "works) have been introduced to NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.8232923150062561}]}, {"text": "To avoid the segmentation errors, most of neural Chinese NER models are character-based.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9615893959999084}]}, {"text": "Although character-based method has achieved good performance, it does not exploit word information in character sequence.", "labels": [], "entities": []}, {"text": "Entity boundaries usually coincide with some word boundaries, which suggests that words in character sequence can provide rich boundary information for character-based model.", "labels": [], "entities": []}, {"text": "To integrate words information into character-based model,  propose a lattice-structured LSTM model to encode a sequence of input characters as well as all potential words that match a lexicon.", "labels": [], "entities": []}, {"text": "Their model is an extension of characterbased LSTM-CRF model and uses extra \"shortcut paths\" to link the memory cell between the start and the end characters of a word for utilizing word information.", "labels": [], "entities": []}, {"text": "And the gated recurrent unit is used to control the contribution of shortcut paths and path between adjacent characters.", "labels": [], "entities": []}, {"text": "However, as the study of (  shown, the gate mechanism fails to choose the right path sometimes.", "labels": [], "entities": []}, {"text": "As shown in, wrong choices may cause lattice model to degenerate into a partial word-based model, which suffers from word segmentation errors.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.6953332424163818}]}, {"text": "In addition, due to the variable length of words, the length of the whole path is not fixed.", "labels": [], "entities": []}, {"text": "Besides, each character is bounded with a variable-sized candidate word sets, which means the amount of incoming and outcoming paths is not fixed either.", "labels": [], "entities": []}, {"text": "In this case, lattice LSTM model is deprived of the power of batch training, and hence it is highly inefficient.", "labels": [], "entities": []}, {"text": "To address the above problems, we propose a novel word-character LSTM(WC-LSTM) to integrate word information into character-based model.", "labels": [], "entities": []}, {"text": "To prevent our model from degenerating into a partial word-based model, we assign word information to a single character and ensure that there are no shortcut paths between characters.", "labels": [], "entities": []}, {"text": "Specifically, word information is assigned to its end character and start character in forward WC-LSTM and backward WC-LSTM respectively.", "labels": [], "entities": []}, {"text": "We introduce four strategies to extract fixed-sized useful information from different words, which ensures that our proposed model can perform batch training without losing word information.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our architecture on four widely used datasets.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed model outperforms other state-of-the-art models on the four datasets.", "labels": [], "entities": []}, {"text": "Our contributions of this paper can be concluded as follows: \u2022 We propose a novel word-character LSTM(WC-LSTM) to incorporate word information into character-based model.", "labels": [], "entities": []}, {"text": "\u2022 We explore four different strategies to encode word information into a fixed-sized vector, which enables our proposed model to be trained in batches and adapted to various application scenarios.", "labels": [], "entities": []}, {"text": "\u2022 Our proposed model outperforms other models and achieves new state-of-the-art over four Chinese NER datasets.", "labels": [], "entities": [{"text": "Chinese NER datasets", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.7751651803652445}]}, {"text": "We release the source code for further research 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "shows the experimental results on OntoNote 4 dataset.", "labels": [], "entities": [{"text": "OntoNote 4 dataset", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.8760051727294922}]}, {"text": "The \"Input\" column shows the representation of input sentence, where \"Gold seg\" means a sequence of words with goldstandard segmentation, and \"No seg\" means a sequence of character without any segmentation.", "labels": [], "entities": []}, {"text": "The first block in are the results of wordbased models (.", "labels": [], "entities": []}, {"text": "By using gold-standard segmentation and external labeled data, all of them achieve good performance.", "labels": [], "entities": []}, {"text": "But the only resource used in our model are pretrained character and word embeddings.", "labels": [], "entities": []}, {"text": "The first two rows in the second block show the performance of the lattice model and characterbased model.", "labels": [], "entities": []}, {"text": "The character baseline denotes the original character-based BiLSTM-CRF model.", "labels": [], "entities": []}, {"text": "propose a lattice LSTM to exploit word information in character sequence, giving the F1 score of 73.88%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9862724542617798}]}, {"text": "Compared with the character baseline, lattice model gains 8.92% improvement in F1 score, which shows the importance of word information in character sequence.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9855733811855316}]}, {"text": "In the last four rows, we list the results of our proposed model.", "labels": [], "entities": []}, {"text": "The results show that all of our models outperform other character-based models, and the one with self-attention strategy achieves the best result.", "labels": [], "entities": []}, {"text": "Without gold-standard segmentation and external labeled data, our model gives competitive results to the word-based models on this dataset.", "labels": [], "entities": []}, {"text": "Compared with the character baseline, our model with self-attention obtains 9.48% improvement in F1 score, which proves the effectiveness of our way to integrating word information.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9886374771595001}]}, {"text": "Compared with lattice model, all of our models achieve better results, which shows that our   approach to integrating word information is more reasonable than lattice model.", "labels": [], "entities": []}, {"text": "shows the results on MSRA dataset. and use the statistical model with rich hand-crafted features.", "labels": [], "entities": [{"text": "MSRA dataset.", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9695553183555603}]}, {"text": "exploit radical features in Chinese character.", "labels": [], "entities": []}, {"text": "joint train Chinese NER task with Chinese word segmentation, in which adversarial learning and selfattention mechanism are applied for better performance.", "labels": [], "entities": [{"text": "NER task", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8420794308185577}, {"text": "Chinese word segmentation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6075398524602255}]}, {"text": "We can observe that our proposed models outperformance the above models and the one with average strategy achieves new state-of-the-art performance.", "labels": [], "entities": []}, {"text": "shows the results 4 on Weibo dataset.", "labels": [], "entities": [{"text": "Weibo dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9955985844135284}]}, {"text": "The \"NE\", \"NM\" and \"Overall\" columns denote F1-score for named entities, nominal entities(excluding named entities) and both respectively.", "labels": [], "entities": [{"text": "NE", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9869656562805176}, {"text": "F1-score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9994550347328186}]}, {"text": "We can see that WC-LSTM model with longest word first strategy achieves new state-ofthe-art performance.", "labels": [], "entities": []}, {"text": "Multi-task learning) and semi-supervised learning  are the most common methods The results of (: Time per epoch of models for Weibo NER task due to the small amount of training data.", "labels": [], "entities": [{"text": "Weibo NER task", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.8857274850209554}]}, {"text": "All of the above models require additional cross-domain or semi-supervised data.", "labels": [], "entities": []}, {"text": "Compared with those models, our model does not need additional labeled data; we only exploit pretrained character and word embeddings.", "labels": [], "entities": []}, {"text": "shows the results on Chinese Resume dataset.", "labels": [], "entities": [{"text": "Chinese Resume dataset", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.9235530694325765}]}, {"text": "Consistent with the previous results, our models outperform lattice model . The above experimental results strongly verify that our method to utilize word information is more effective than the lattice model.", "labels": [], "entities": []}, {"text": "Our proposed model has achieved state-of-theart results on various domains such as news, social media, and Chinese resume.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets", "labels": [], "entities": []}, {"text": " Table 1.  Implementation Details. We utilize the char- acter and word embeddings used in (", "labels": [], "entities": [{"text": "Implementation", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.737589418888092}]}, {"text": " Table 2: Results on OntoNotes", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.5251433849334717}]}, {"text": " Table 3: Results on MSRA", "labels": [], "entities": [{"text": "MSRA", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.48902010917663574}]}, {"text": " Table 4: Results on Weibo NER", "labels": [], "entities": [{"text": "Weibo NER", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9453895092010498}]}, {"text": " Table 5: Results on Chinese Resume", "labels": [], "entities": [{"text": "Chinese Resume", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7113470733165741}]}, {"text": " Table 6: Time per epoch of models", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9518320560455322}]}, {"text": " Table 9: Comparison F1 scores between our proposed  model with and without pretrained word embeddings.  Where \"init\" and \"pretrain\" denote without and with  pretrained embeddings respectively. \"+\" denotes the  boost value to baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9949567914009094}]}]}