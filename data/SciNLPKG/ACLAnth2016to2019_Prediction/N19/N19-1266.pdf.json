{"title": [{"text": "The World in My Mind: Visual Dialog with Adversarial Multi-modal Feature Encoding", "labels": [], "entities": []}], "abstractContent": [{"text": "Visual Dialog is a multi-modal task that requires a model to participate in a multi-turn human dialog grounded on an image, and generate correct, human-like responses.", "labels": [], "entities": [{"text": "Visual Dialog", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6717367023229599}]}, {"text": "In this paper, we propose a novel Adversarial Multi-modal Feature Encoding (AMFE) framework for effective and robust auxiliary training of visual dialog systems.", "labels": [], "entities": []}, {"text": "AMFE can force the language-encoding part of a model to generate hidden states in a distribution closely related to the distribution of real-world images, resulting in language features containing general knowledge from both modalities by nature, which can help generate both more correct and more general responses with reasonably low time cost.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7275404930114746}]}, {"text": "Experimental results show that AMFE can steadily bring performance gains to different models on different scales of data.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.681990921497345}]}, {"text": "Our method outperforms both the supervised learning baselines and other fine-tuning methods, achieving state-of-the-art results on most met-rics of VisDial v0.5/v0.9 generative tasks.", "labels": [], "entities": [{"text": "VisDial v0.5/v0.9 generative tasks", "start_pos": 148, "end_pos": 182, "type": "TASK", "confidence": 0.5014433910449346}]}], "introductionContent": [{"text": "In recent years, there has been a rising attention in Artificial Intelligence on how to train a model to understand visual inputs from the physical world, and communicate them with human language.", "labels": [], "entities": []}, {"text": "Typical problems include Visual Question Answering (VQA) () and Image Captioning (.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7305347522099813}, {"text": "Image Captioning", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7322994768619537}]}, {"text": "These tasks require a model to read an image and generate a proper response, such as answering a question grounded on the image, or generating a sentence to describe the image.", "labels": [], "entities": []}, {"text": "As a more difficult extension, Visual Dialog) is a cluster of tasks featuring two agents conducting a multi-turn dialog grounded on an image.", "labels": [], "entities": [{"text": "Visual Dialog)", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.5934132238229116}]}, {"text": "A model is usually trained * Corresponding Author to predict every single response of one agent in the two, based on the image and dialog history.", "labels": [], "entities": []}, {"text": "There are also some different task settings such as directly training two agents to complete a goaldriven cooperative task such as Guessing Game (.", "labels": [], "entities": [{"text": "Guessing Game", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.8879427015781403}]}, {"text": "Tasks involving both the physical world (visual images) and abstract world (languages) share a core issue: how to establish connections between these two worlds, and is there a framework to leverage these connections for learning?", "labels": [], "entities": []}, {"text": "Temporarily, the majority of answers are learning end-to-end models with multi-modal feature fusion (.", "labels": [], "entities": []}, {"text": "These methods usually merge the visual and language features into rich representations containing information from both sides.", "labels": [], "entities": []}, {"text": "Some cross-modal attention methods () formulate the visual-language connections explicitly by parameterizing the attention weights to learn whether there is high correlation within certain pairs of language and visual feature vectors.", "labels": [], "entities": []}, {"text": "However, in all these works, the merged representations or attention weights are only learned from pairwise (one image, one sentence) co-occurrence, and serve for the optimization of a loss function only related to the final ground-truth response.", "labels": [], "entities": []}, {"text": "In fact, the features from both sides are not truly connected in an aspect of general distributions, but only merged into anew vector for each training/testing sample.", "labels": [], "entities": []}, {"text": "We suppose that this is not good enough fora model to distill knowledge from both of the two worlds because the language/visual vectors do not contain knowledge from the other modality in the bottom level before they are merged.", "labels": [], "entities": []}, {"text": "In this paper, we discuss another possibility.", "labels": [], "entities": []}, {"text": "We want to establish an unsupervised framework of multi-modal encoding, which directly generates an \"image feature distribution\" from a language distribution, or vice versa.", "labels": [], "entities": []}, {"text": "For example, when a neural network based model receives a natural language sentence x as input, it encodes x into a sequence of high-dimensional continuous vectors.", "labels": [], "entities": []}, {"text": "All these language vectors can be projected into another latent space to have anew distribution pl . We train the language encoder to let the new distribution pl be the same as, or very close to, the distribution p v of all image features observed and encoded in the task data.", "labels": [], "entities": []}, {"text": "Since we can partly recover a real-world image distribution from the language vectors achieved in this way, these language vectors intrinsically contain both language semantics and real-world image properties.", "labels": [], "entities": []}, {"text": "This is a higherlevel connection between the two worlds.", "labels": [], "entities": []}, {"text": "In order to train a model to generate samples subject to a certain distribution p v from an original distribution pl , Generative Adversarial Networks (GANs) have been proved very effective.", "labels": [], "entities": []}, {"text": "used adversarial training on the vectors produced by sentence encoders for different languages in unsupervised machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7584100663661957}]}, {"text": "However, different languages in their task are in single modality and share encoder structures, making the same method not directly usable and extendable for multi-modal tasks with largely different prior distributions and complex encoder structures with attention.", "labels": [], "entities": []}, {"text": "In our work, we propose Adversarial Multi-modal Feature Encoding (AMFE), a novel GAN-based training schedule with an attention-based sampleselecting method, which can successfully force the multi-modal vectors to have closely related distributions, benefitting the performances of various visual dialog systems.", "labels": [], "entities": []}, {"text": "We test our method on the VisDial () benchmark (one example is shown in Table 1).", "labels": [], "entities": []}, {"text": "A normal sample of VisDial contains an image and 10 turns of question-answering dialog from two people grounded on the image.", "labels": [], "entities": [{"text": "VisDial", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8794814348220825}]}, {"text": "A series of models have been proposed to solve the task, including memory and attention based models (), reinforcement learning (), knowledge transfer techniques () and GAN (.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7364166975021362}, {"text": "GAN", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.7120437622070312}]}, {"text": "designed a complex attention model and applied GAN in a traditional way to force the generated tokens to mimic realworld language (language vs. language), making their model only trainable through sequence Caption: A dog with goggles is in a motorcycle sidecar A(1): can you tell what kind of dog this is B can you tell if motorcycle is moving or still B(2): it's parked A(3): is dog's tongue lolling out B(3): not really sampling and reinforcement learning.", "labels": [], "entities": []}, {"text": "Our work, on the other hand, applies a directly differentiable GAN on continuous vectors as a multi-modal feature encoding method (language vs. image).", "labels": [], "entities": []}, {"text": "Our contributions include: \u2022 We propose AMFE: a novel Adversarial Multi-modal Feature Encoding framework to benefit visual dialog models.", "labels": [], "entities": []}, {"text": "The core idea is to force features from different modalities to have closely related distributions.", "labels": [], "entities": []}, {"text": "\u2022 We develop efficient AMFE implementations, including a novel attention-based sample selecting method, for various commonlyused visual dialog models.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results show that AMFE brings robust performance gains to different visual dialog models.", "labels": [], "entities": []}, {"text": "We achieve state-of-the-arts on most metrics of VisDial v0.5/v0.9 generative tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "VisDial is a visual dialog dataset based on MS COCO () images.", "labels": [], "entities": [{"text": "VisDial", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8962115049362183}, {"text": "MS COCO () images", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8538782447576523}]}, {"text": "There are 10 turns of human-posed question-answering dialogs on each image, with the questioner kept not seeing the image during the data collection process.", "labels": [], "entities": []}, {"text": "For generative models, a model must give the probability of generating each candidate answer without seeing other candidates, and the rank of the ground-truth answer in the 100 candidates is used to compute different evaluation metrics; for discriminative models, the model can read and encode all the candidate answers and directly assign scores on them.", "labels": [], "entities": []}, {"text": "According to the nature of GANs Algorithm 1 AMFE Training Procedure.", "labels": [], "entities": [{"text": "GANs Algorithm 1 AMFE Training Procedure", "start_pos": 27, "end_pos": 67, "type": "DATASET", "confidence": 0.5948878477017084}]}, {"text": "Require: \u03b1 the learning rate; c the clipping parameter; M the batch size; w0 the initial D-Bot parameters; \u03b80 the initial A-Bot parameters; dialog samples.", "labels": [], "entities": []}, {"text": "Pre-train A-Bot with Lsu (Eq. 8).", "labels": [], "entities": [{"text": "A-Bot", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9078134298324585}]}, {"text": "while \u03b8 has not converged do Sample M turns of (q, H, I, a) dialog samples.", "labels": [], "entities": []}, {"text": "Forward A-Bot and select h l by attention weights.", "labels": [], "entities": [{"text": "A-Bot", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9770336747169495}]}, {"text": "Compute Compute Lsu with ground-truth answers using (Eq. 8).", "labels": [], "entities": []}, {"text": "Update \u03b8 to minimize LG = Lsu + \u03bbL adv . Switch to D-Bot training.", "labels": [], "entities": [{"text": "LG", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9581518769264221}]}, {"text": "Select image vectors hv by attention weights.", "labels": [], "entities": []}, {"text": "Re-generate h l samples using the updated A-Bot..", "labels": [], "entities": [{"text": "A-Bot.", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9500684142112732}]}, {"text": "end while and similarities to real-world application scenarios, we use the generative setting for our model: it is equipped with a sequential decoder instead of a scoring module.", "labels": [], "entities": []}, {"text": "For fair and sufficient comparison, we evaluate our model on both VisDial v0.5 and VisDial v0.9.", "labels": [], "entities": [{"text": "VisDial v0.9", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8344582319259644}]}, {"text": "VisDial v0.5 has 68k COCO images, fora total of 680k QA-pairs.", "labels": [], "entities": [{"text": "VisDial", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.934496283531189}]}, {"text": "Following ( and (), we use 50,729 images for training, 7,663 for validation and 9,628 for testing.", "labels": [], "entities": []}, {"text": "Visdial v0.9 has 123,287 images.", "labels": [], "entities": [{"text": "Visdial v0.9", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.803329586982727}]}, {"text": "There are different splitting of train/valid/test in previous work.", "labels": [], "entities": []}, {"text": "We follow () to use 82k for training, 1k for validation and 40k for testing.", "labels": [], "entities": []}, {"text": "We compare our results to several existing models on the VisDial dataset, including: \u2022 Answer Prior (): directly encoding answer candidates with an LSTM and scoring by a linear model that captures the frequency of answers in the training set.", "labels": [], "entities": [{"text": "VisDial dataset", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.946603924036026}]}, {"text": "\u2022 NN-QI (): a k-Nearest Neighborhood method considering only the question and the image.", "labels": [], "entities": []}, {"text": "Unlike generative methods, both Answer Prior and NN-QI need to know the answer candidates.", "labels": [], "entities": []}, {"text": "\u2022 LF-QIH-G (): a Late Fusion encoder that encodes the question, image and history separately.", "labels": [], "entities": []}, {"text": "The encoded features are concatenated and linearly transformed to a joint representation.", "labels": [], "entities": []}, {"text": "The answer is produced by a generative decoder.", "labels": [], "entities": []}, {"text": "\u2022 HRE (): the HRE model introduced in Section 3.3.", "labels": [], "entities": [{"text": "HRE", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.8613317608833313}]}, {"text": "\u2022 HREA-QIH-G (): a modified HRE A-Bot with attention to dialog history.", "labels": [], "entities": [{"text": "HREA-QIH-G", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.7651276588439941}]}, {"text": "\u2022 MN-QIH-G (): a Memory Network encoder that stores each piece of dialog history embeddings in an explicit memory.", "labels": [], "entities": [{"text": "MN-QIH-G", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.7152183651924133}]}, {"text": "These embeddings can be attended and fused while generating the answer.", "labels": [], "entities": []}, {"text": "\u2022 HCIAE (): the HCIAE model introduced in Section 3.3.", "labels": [], "entities": []}, {"text": "\u2022 CoAtt (: this is a previous state-of-the-art model with a more complex co-attention encoder; the decoder is enhanced by adversarial reinforcement learning for better answer generation.", "labels": [], "entities": [{"text": "CoAtt", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9243196845054626}, {"text": "answer generation", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.8733904957771301}]}, {"text": "On VisDial v0.5, two previous top models area Memory Network based model (MN-QIH-G) by) and a multi-loss training on HRE encoder (Frozen-Q-Multi) based on goaldriven reinforcement learning ().", "labels": [], "entities": []}, {"text": "We start from the same HRE hyper-parameters and checkpoint as (), but continue with our AMFE instead of reinforcement learning.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.515041172504425}]}, {"text": "shows the results on all the five evaluation metrics on VisDial v0.5.", "labels": [], "entities": []}, {"text": "Results in the first 4 rows are copied from ().", "labels": [], "entities": []}, {"text": "AMFE achieves better performances than the supervised training of A-Bot model (HRE-MLE), especially significant on R@5, R@10 and mean rank, indicating that the adversarial feature encoding results in \"generally better\" dialogs.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6043702363967896}, {"text": "R@5", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9466307560602824}, {"text": "mean rank", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9506500363349915}]}, {"text": "It also outperforms the another HRE-like model with history attentions (HREA-QIH-G).", "labels": [], "entities": []}, {"text": "While used for multiloss training, AMFE is significantly better than Frozen-Q-Multi, setting anew state-of-the-art on all metrics.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.5381702780723572}]}, {"text": "We point out that in Frozen-Q-Multi (), the goal-driven reinforcement leaning reward is computed pair-wise (considering how much can the questioner rebuild the image from the answerer's words), but the reward computed with a single image is not good enough to evaluate the dialog actions.", "labels": [], "entities": []}, {"text": "This is because language is much more abstract than image, and failure to recover an image does not necessarily mean that the dialog is actually bad.", "labels": [], "entities": []}, {"text": "Our method could avoid this issue because adversarial training is based on general distributions.", "labels": [], "entities": []}, {"text": "On VisDial v0.9, we observe that using AMFE on HCIAE can also boost the performances.", "labels": [], "entities": []}, {"text": "Comparing HCIAE-G-MLE and HCIAE-AMFE, we can observe the same advantage over supervised training as on HRE, indicating that our method works for different dataset scales and A-Bot struc-  We explain the efficiency of AMFE in two aspects.", "labels": [], "entities": [{"text": "A-Bot", "start_pos": 174, "end_pos": 179, "type": "METRIC", "confidence": 0.9895819425582886}]}, {"text": "Firstly, AMFE is an adversarial training procedure forcing the language to be encoded into a distribution closely connected to the images.", "labels": [], "entities": []}, {"text": "With attention-based sample selection, the most informative samples from both modalities are able to transfer knowledge.", "labels": [], "entities": []}, {"text": "Secondly, like Batch Normalization, AMFE contributes to bring better numerical properties to the intermediate tensors in a network, especially on their means and variances, which could potentially benefit model performance.", "labels": [], "entities": [{"text": "Batch Normalization", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7777192890644073}]}, {"text": "The above results show that AMFE is especially strong at more \"general\" metrics such as R@5 and mean rank.", "labels": [], "entities": [{"text": "AMFE", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.5654312372207642}, {"text": "R@5", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9503046870231628}, {"text": "mean rank", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9426475763320923}]}, {"text": "To confirm that adversarial training on hidden states can help much to generate responses that are more natural, we randomly select 100 dialog samples from both VisDial v0.5 and v0.9 dataset, and ask two human subjects to vote for the responses generated by two groups of models: HRE-MLE vs. HRE-AMFE on v0.5, and HCIAE-G-MLE vs. HCIAE-AMFE on v0.9, both with beam-size 5.", "labels": [], "entities": []}, {"text": "Model names are hidden while voting.", "labels": [], "entities": []}, {"text": "We ask the human subjects to consider two metrics separately: (1) the fluency of the generated answer sentences and (2) the correctness of the answers compared to the ground-truths.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: VisDial v0.5 evaluation results. The five met- rics are mean reciprocal rank, recall of the ground-truth  answer in the top-1/5/10 ranked candidates (higher is  better), and the mean rank of the ground-truth answer  (lower is better).", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.8952181339263916}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9984470009803772}]}, {"text": " Table 3: VisDial v0.9 evaluation results. The five met- rics are mean reciprocal rank, recall of the ground-truth  answer in the top-1/5/10 ranked candidates, (higher is  better) and the mean rank of the ground-truth answer  (lower is better).", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.8875553409258524}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9980900883674622}]}, {"text": " Table 4: VisDial v0.9 ablation results training with  HCIAE-G-MLE and AMFE fine-tuning. AMFE- standard uses \u03bb = 1 with AbS selection.", "labels": [], "entities": [{"text": "HCIAE-G-MLE", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.8674458265304565}, {"text": "AMFE", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.6767420768737793}, {"text": "AMFE- standard", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8023316860198975}, {"text": "AbS", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9854140281677246}]}, {"text": " Table 6: Human voting result on 100 samples from Vis- Dial v0.5 and v0.9.", "labels": [], "entities": [{"text": "Vis- Dial v0.5", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8816802203655243}]}, {"text": " Table 5. In the  first example, the model trained with AMFE gen- erates a right vs. wrong answer in the 8-th turn,  and a grammatically better response in the 5-th  turn, compared to supervised pre-training. In the", "labels": [], "entities": []}]}