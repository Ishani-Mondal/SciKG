{"title": [{"text": "Personalized Neural Embeddings for Collaborative Filtering with Text", "labels": [], "entities": []}], "abstractContent": [{"text": "Collaborative filtering (CF) is a core technique for recommender systems.", "labels": [], "entities": [{"text": "Collaborative filtering (CF)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8409218907356262}]}, {"text": "Traditional CF approaches exploit user-item relations (e.g., clicks, likes, and views) only and hence they suffer from the data sparsity issue.", "labels": [], "entities": []}, {"text": "Items are usually associated with unstructured text such as article abstracts and product reviews.", "labels": [], "entities": []}, {"text": "We develop a Personalized Neural Embedding (PNE) framework to exploit both interactions and words seamlessly.", "labels": [], "entities": []}, {"text": "We learn such em-beddings of users, items, and words jointly, and predict user preferences on items based on these learned representations.", "labels": [], "entities": []}, {"text": "PNE estimates the probability that a user will like an item by two terms-behavior factors and semantic factors.", "labels": [], "entities": []}, {"text": "On two real-world datasets, PNE shows better performance than four state-of-the-art baselines in terms of three metrics.", "labels": [], "entities": [{"text": "PNE", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.5504840016365051}]}, {"text": "We also show that PNE learns meaningful word embed-dings by visualization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recommender systems are widely used in ecommerce platforms, such as to help consumers buy products at Amazon, watch videos on Youtube, and read articles on Google News.", "labels": [], "entities": []}, {"text": "They are useful to alleviate the information overload and improve user satisfaction.", "labels": [], "entities": []}, {"text": "Given history records of consumers such as the product transactions and movie watching, collaborative filtering (CF) is among the most effective approaches based on the simple intuition that if users rated items similarly in the past then they are likely to rate items similarly in the future ().", "labels": [], "entities": [{"text": "movie watching", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7344425320625305}, {"text": "collaborative filtering (CF)", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.7013722777366638}]}, {"text": "History records include both implicit (e.g., purchase and clicks) and explicit (e.g., likes/dislikes and ratings) feedback which can be represented as a user-item interaction matrix.", "labels": [], "entities": []}, {"text": "Typically, observed user-item interactions are incomplete with a large portion remaining not recorded.", "labels": [], "entities": []}, {"text": "The goal of recommendation is to predict user preferences on these missing interactions.", "labels": [], "entities": []}, {"text": "This setting requires to complete the partial observed rating matrix.", "labels": [], "entities": []}, {"text": "Matrix Factorization (MF) techniques which can learn latent factors for users and items are the main cornerstone for CF.", "labels": [], "entities": [{"text": "CF", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9477478265762329}]}, {"text": "It is effective and flexible to integrate with additional data sources (.", "labels": [], "entities": []}, {"text": "Recently, neural networks like Multilayer Perceptron (MLP) are used to learn an interaction function from data with the power of learning highly nonlinear relationships between users and items ().", "labels": [], "entities": []}, {"text": "MF and neural CF exploit user-item behavior interactions only and hence they both suffer from the data sparsity and cold-start issues.", "labels": [], "entities": []}, {"text": "Items are usually associated with unstructured text, like news articles and product reviews.", "labels": [], "entities": []}, {"text": "These additional sources are essential for recommendation beyond user-item interactions since they contain independent and diverse information.", "labels": [], "entities": []}, {"text": "Hence, they provide an opportunity to alleviate the data sparsity issue ().", "labels": [], "entities": []}, {"text": "For application domains like recommending research papers and news articles, the unstructured text associated with the item is its text content ().", "labels": [], "entities": []}, {"text": "For some domains like recommending products, the unstructured text associated with the item is its user reviews which justify the rating behavior.", "labels": [], "entities": []}, {"text": "These methods adopt topic modelling techniques and neural networks to exploit the item content leading to performance improvement.", "labels": [], "entities": []}, {"text": "A typical way of exploiting text content is to firstly extract a feature vector for each document by averaging word embeddings in the document, and then to learn a text factor corresponding to this feature vector (.", "labels": [], "entities": []}, {"text": "These embed-dings are pre-trained from a large corpus such as Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.9615614414215088}]}, {"text": "This approach separates the extraction of text feature from the learning of user-item interaction.", "labels": [], "entities": []}, {"text": "These two processes cannot benefit from each other and errors in the previous step maybe propagate to the successive steps.", "labels": [], "entities": []}, {"text": "Another way is to learn a topic vector using topic modelling () by aligning behavior factors and topic factors with a link function such as softmax and offset.", "labels": [], "entities": [{"text": "offset", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.960540771484375}]}, {"text": "Recently, neural networks are used to learn a representation from the text using autoencoders (, recurrent networks (, and convolutional networks (.", "labels": [], "entities": []}, {"text": "These methods treat different words in the document as equal importance and do not match word semantics with the specific user.", "labels": [], "entities": []}, {"text": "Instead, we achieve to learn a personalized word embedding with the guidance of user-item interactions.", "labels": [], "entities": []}, {"text": "That is, the importance of words is learned to match user preferences.", "labels": [], "entities": []}, {"text": "The attention mechanism can be used to learn these importance weights.", "labels": [], "entities": []}, {"text": "Memory Networks (MemNet) have been used in recommendation to model item content (, capture user neighborhood (, and learn latent relationships ().", "labels": [], "entities": []}, {"text": "We follow this thread to adapt a MemNet to match word semantics with user preferences.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural framework to exploit relational interactions and text content seamlessly.", "labels": [], "entities": []}, {"text": "The proposed Personalized Neural Embedding (PNE) model fuses semantic representations learnt from unstructured text with behavior representations learnt from user-item interactions jointly for effective estimation of user preferences on items.", "labels": [], "entities": []}, {"text": "PNE estimates the preference probability by two kinds of factors.", "labels": [], "entities": []}, {"text": "The behavior factor is to capture the personalized preference of a user to an item learned from behavior interactions.", "labels": [], "entities": []}, {"text": "The semantic factor is to capture the high-level representation attentively extracted from the unstructured text by matching word semantics with user preferences.", "labels": [], "entities": []}, {"text": "To model the behavior factor, we adopt a neural CF approach, which learns the user-item nonlinear interaction relationships using a neural network (CFNet).", "labels": [], "entities": []}, {"text": "To model the semantic factor, we adopt a memory network to match word semantics with the specific user via the attention mechanism inherent in the memory module (MemNet), determining which words are highly relevant to the user preferences.", "labels": [], "entities": []}, {"text": "PNE integrates relational interactions with unstructured text by bridging neural CF and memory networks.", "labels": [], "entities": []}, {"text": "PNE can also learn meaningful word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate PNE on two datasets with five baselines in terms of three metrics.", "labels": [], "entities": [{"text": "PNE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7943712472915649}]}, {"text": "We evaluate on two real-world datasets.", "labels": [], "entities": []}, {"text": "The public Amazon products and a company Cheetah Mobile news () (see).", "labels": [], "entities": [{"text": "Cheetah Mobile news", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8016434113184611}]}, {"text": "We preprocess the data following the strategy in ().", "labels": [], "entities": []}, {"text": "The size of word vocabulary is 8,000.", "labels": [], "entities": []}, {"text": "We adopt leave-one-out evaluation () and use three ranking metrics: hit ratio (HR), normalized discounted cumulative gain (ND-CG), and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "hit ratio (HR)", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.9618221282958984}, {"text": "normalized discounted cumulative gain (ND-CG)", "start_pos": 84, "end_pos": 129, "type": "METRIC", "confidence": 0.7888420479638236}, {"text": "mean reciprocal rank (MRR)", "start_pos": 135, "end_pos": 161, "type": "METRIC", "confidence": 0.9238158464431763}]}, {"text": "We compare with five baselines (see).", "labels": [], "entities": []}, {"text": "\u2022 BPR () is a latent factor model based on matrix factorization.", "labels": [], "entities": [{"text": "BPR", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.8754268288612366}]}, {"text": "\u2022 HFT ( adopts topic distributions to learn latent factors from text reviews.", "labels": [], "entities": [{"text": "HFT", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.6630328297615051}]}, {"text": "\u2022 TBPR (: Results (\u00d7100) on Cheetah dataset.", "labels": [], "entities": [{"text": "TBPR", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.981562614440918}, {"text": "Cheetah dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9434866905212402}]}, {"text": "Best baseline marked with asterisk and best result in boldfaced.", "labels": [], "entities": []}, {"text": "\u2022 MLP () is a neural CF approach.", "labels": [], "entities": []}, {"text": "Note that, CFNet of PNE is an MLP with only one hidden layer.", "labels": [], "entities": [{"text": "CFNet of PNE", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.7728079954783121}]}, {"text": "\u2022 LCMR () is a deep model for CF with unstructured text.", "labels": [], "entities": []}, {"text": "Note that, MemNet of PNE is the same with the local MemNet of LCMR with only one-hop hidden layer.", "labels": [], "entities": [{"text": "LCMR", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.8041223883628845}]}, {"text": "Our method is implemented by TensorFlow (.", "labels": [], "entities": []}, {"text": "Parameters are randomly initialized from Gaussian with optimizer Adam.", "labels": [], "entities": []}, {"text": "Learning rate is 0.001, batch size is 128, the ratio of negative sampling is 1.", "labels": [], "entities": [{"text": "Learning rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9518067836761475}]}], "tableCaptions": [{"text": " Table 1: Datasets and statistics.", "labels": [], "entities": []}, {"text": " Table 3: Results (\u00d7100) on Amazon dataset. Best base- line marked with asterisk and best result in boldfaced.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.967255562543869}]}, {"text": " Table 4: Results (\u00d7100) on Cheetah dataset. Best base- line marked with asterisk and best result in boldfaced.", "labels": [], "entities": [{"text": "Cheetah dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.964929461479187}]}]}