{"title": [{"text": "Correlation Coefficients and Semantic Textual Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "A large body of research into semantic tex-tual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks.", "labels": [], "entities": [{"text": "semantic tex-tual similarity", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.5931339661280314}]}, {"text": "By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases.", "labels": [], "entities": []}, {"text": "In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 62, "end_pos": 79, "type": "METRIC", "confidence": 0.7346157431602478}, {"text": "Pearson correlation coefficient", "start_pos": 113, "end_pos": 144, "type": "METRIC", "confidence": 0.8592966794967651}]}, {"text": "We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.7527792155742645}, {"text": "cosine similarity)", "start_pos": 69, "end_pos": 87, "type": "METRIC", "confidence": 0.746452271938324}]}, {"text": "Importantly, we show that Pearson correlation is appropriate for some word vectors but not others.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.8781269192695618}]}, {"text": "When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance.", "labels": [], "entities": []}, {"text": "We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks.", "labels": [], "entities": []}, {"text": "On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual embeddings are immensely popular because they help us reason about the abstract and fuzzy notion of semantic similarity in purely geometric terms.", "labels": [], "entities": []}, {"text": "Distributed representations of words in particular () have had a massive impact on machine learning (ML), natural language processing (NLP), and information retrieval (IR).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.8111435174942017}, {"text": "information retrieval (IR)", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.8579637169837951}]}, {"text": "Recently, much effort has also been directed towards learning representations for larger pieces of text, with methods ranging from clever compositions of word embeddings) to sophisticated neural architectures (.", "labels": [], "entities": []}, {"text": "Comparatively, there is little research into similarity measures for textual embeddings.", "labels": [], "entities": []}, {"text": "Despite some investigations into alternatives, cosine similarity has persistently remained the default and unquestioned choice across the field.", "labels": [], "entities": []}, {"text": "This is partly because cosine similarity is very convenient and easy to understand.", "labels": [], "entities": []}, {"text": "Sometimes, however, we have to resist what is convenient and instead use what is appropriate.", "labels": [], "entities": []}, {"text": "The core idea behind our work is to treat each word or sentence embedding as a sample of (e.g. 300) observations from some scalar random variable.", "labels": [], "entities": []}, {"text": "Hence, no matter how mysterious word vectors appear to be, just like any samples, they become subject to the full power of traditional statistical analysis.", "labels": [], "entities": []}, {"text": "We first show that in practice, the widely used cosine similarity is nothing but the Pearson correlation coefficient computed from the paired sample.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.7193067371845245}, {"text": "Pearson correlation coefficient", "start_pos": 85, "end_pos": 116, "type": "METRIC", "confidence": 0.9514112869898478}]}, {"text": "However, Pearson's r is extremely sensitive to even slight departures from normality, where a single outlier can conceal the underlying association.", "labels": [], "entities": []}, {"text": "For example, we find that Pearson's r (and thus cosine similarity) is acceptable for word2vec and fastText but not for GloVe embeddings.", "labels": [], "entities": []}, {"text": "Perhaps surprisingly, when we average word vectors to represent sentences, cosine similarity remains acceptable for word2vec, but not for fastText any longer.", "labels": [], "entities": []}, {"text": "We show that this seemingly counterintuitive behaviour can be predicted by elementary univariate statistics, something that is already well known to researchers and practitioners alike.", "labels": [], "entities": []}, {"text": "Furthermore, when there are clear indications against cosine similarity, we propose to repurpose rank-based correlation coefficients, such as Spearman's \u03c1 and Kendall's \u03c4 , as similarity measures between textual embeddings.", "labels": [], "entities": []}, {"text": "We support this proposition by a series of experiments on word-and sentence-level semantic textual similarity (STS) tasks.", "labels": [], "entities": [{"text": "word-and sentence-level semantic textual similarity (STS) tasks", "start_pos": 58, "end_pos": 121, "type": "TASK", "confidence": 0.6975928445657095}]}, {"text": "Our results confirm that rankbased correlation coefficients are much more effective when the majority of vectors break the assumptions of normality.", "labels": [], "entities": []}, {"text": "Moreover, we show how even the simplest sentence embeddings (such as averaged word vectors) compared by rank correlation easily rival recent deep representations compared by cosine similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "To empirically validate the utility of the statistical framework presented in Section 3, we run a set of evaluations on word-and sentence-level STS tasks.", "labels": [], "entities": [{"text": "word-and sentence-level STS tasks", "start_pos": 120, "end_pos": 153, "type": "TASK", "confidence": 0.6209416538476944}]}, {"text": "In all experiments we rely on the following publicly available word embeddings: GloVe (Pennington et al., 2014) trained on Common Crawl (840B tokens), fastText ( ) trained on Common Crawl (600B tokens), and word2vec (Mikolov et al., 2013b,c) trained on Google News.", "labels": [], "entities": []}, {"text": "All the source code for our experiments is available on GitHub 1 ; in the case of the sentence-level tasks we rely also on the SentEval toolkit (: Spearman's \u03c1 on word similarity tasks for combinations of word vectors and the following similarity metrics: cosine similarity (COS), Pearson's r (PRS), Spearman's \u03c1 (SPR), and Kendall \u03c4 (KEN).", "labels": [], "entities": [{"text": "cosine similarity (COS)", "start_pos": 256, "end_pos": 279, "type": "METRIC", "confidence": 0.8859112381935119}, {"text": "Pearson's r (PRS)", "start_pos": 281, "end_pos": 298, "type": "METRIC", "confidence": 0.8372045556704203}, {"text": "Kendall \u03c4 (KEN)", "start_pos": 324, "end_pos": 339, "type": "METRIC", "confidence": 0.9256704449653625}]}, {"text": "N indicates the proportion of sentence vectors in a task for which the null hypothesis of normality in a ShapiroWilk test was not rejected at \u03b1 = 0.05.", "labels": [], "entities": []}, {"text": "The V column indicates the type of the best performing method: a rank-based correlation coefficient (R), a non-rankbased correlation or measure (N), or a tie (=).", "labels": [], "entities": [{"text": "rank-based correlation coefficient (R)", "start_pos": 65, "end_pos": 103, "type": "METRIC", "confidence": 0.8387167751789093}, {"text": "correlation or measure (N)", "start_pos": 121, "end_pos": 147, "type": "METRIC", "confidence": 0.8082767327626547}]}, {"text": "The winners in V were determined by comparing the top rankbased method for that vector/task combination with the top non-rank-based method.", "labels": [], "entities": []}, {"text": "Winners were assigned only when the difference was statistically significant as determined by 95% BCa confidence intervals..", "labels": [], "entities": [{"text": "BCa confidence intervals.", "start_pos": 98, "end_pos": 123, "type": "METRIC", "confidence": 0.9619419177373251}]}, {"text": "These datasets contain pairs of words and a human-annotated similarity score for each pair.", "labels": [], "entities": []}, {"text": "The success metric for the experiments is the Spearman correlation between the human-annotated similarity scores and the scores generated by the algorithm.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 46, "end_pos": 66, "type": "METRIC", "confidence": 0.8279780745506287}]}, {"text": "To avoid any confusion whatsoever, note that here Spearman correlation serves as an evaluation criterion; this is completely unrelated to using Spearman correlation as a similarity measure between word embeddings as proposed in Section 3.", "labels": [], "entities": []}, {"text": "Bias-corrected and accelerated bootstrap 95% confidence intervals were used to determine statistical significance.", "labels": [], "entities": [{"text": "Bias-corrected", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.975636899471283}, {"text": "bootstrap 95% confidence intervals", "start_pos": 31, "end_pos": 65, "type": "METRIC", "confidence": 0.8027247309684753}]}, {"text": "We report the results for different combinations of word vectors and similarity measures in.", "labels": [], "entities": []}, {"text": "The main takeaways from these experiments are the following: \u2022 There is no significant difference between the results obtained with cosine similarity and Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 154, "end_pos": 173, "type": "METRIC", "confidence": 0.9416374862194061}]}, {"text": "This is because empirically, the means across dimensions of these word vectors are approximately zero, in which case cosine similarity and Pearson correlation are approximately the same.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 139, "end_pos": 158, "type": "METRIC", "confidence": 0.920231282711029}]}, {"text": "\u2022 Rank correlation coefficients tend to perform on par or better than cosine and Pearson on tasks and word vectors where there is a high proportion of non-normally distributed word vectors (over 90%).", "labels": [], "entities": [{"text": "Rank correlation", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.7499643564224243}, {"text": "Pearson", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9662367105484009}]}, {"text": "This makes sense because it is precisely in the non-normal cases where Pearson correlation fails.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.5910245478153229}]}, {"text": "\u2022 When word vectors seem mostly normal, our analysis does not tell us definitively whether cosine similarity or rank correlation should perform better, and indeed we see that cosine and Pearson perform on par or better than Spearman and Kendall.", "labels": [], "entities": []}, {"text": "In the second set of experiments, we use the datasets from the sentence-level Semantic Textual Similarity shared task series).", "labels": [], "entities": [{"text": "Semantic Textual Similarity shared task", "start_pos": 78, "end_pos": 117, "type": "TASK", "confidence": 0.7826322078704834}]}, {"text": "The success metric for these experiments is the Pearson correlation between the humanannotated sentence similarity scores and the scores generated by the algorithm.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.9785578846931458}]}, {"text": "Again, this use of Pearson correlation as an evaluation criterion is completely unrelated to its use as a similarity measure between sentence embeddings.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.6272519826889038}]}, {"text": "Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pearson correlations reported in our experiments involving this task have been re-calculated accordingly.", "labels": [], "entities": [{"text": "STS13 SMT", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.607770174741745}, {"text": "Pearson correlations", "start_pos": 93, "end_pos": 113, "type": "METRIC", "confidence": 0.9675638675689697}]}, {"text": "For these experiments we use averaged word vectors as a sentence representation for various: Mean Pearson correlation on STS tasks for methods using combinations of word vectors and similarity metrics.", "labels": [], "entities": [{"text": "Mean Pearson correlation", "start_pos": 93, "end_pos": 117, "type": "METRIC", "confidence": 0.887855589389801}]}, {"text": "All methods use averaged word vectors to represent sentences.", "labels": [], "entities": []}, {"text": "The similarity measures are: cosine similarity (COS), Pearson's r (PRS), Spearman's \u03c1 (SPR), Kendall \u03c4 (KEN) and APSynP (APS).", "labels": [], "entities": [{"text": "cosine similarity (COS)", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.9080825448036194}, {"text": "Pearson's r (PRS)", "start_pos": 54, "end_pos": 71, "type": "METRIC", "confidence": 0.9236241579055786}, {"text": "Spearman's \u03c1 (SPR)", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.7562736968199412}, {"text": "Kendall \u03c4 (KEN)", "start_pos": 93, "end_pos": 108, "type": "METRIC", "confidence": 0.9392719030380249}, {"text": "APSynP (APS)", "start_pos": 113, "end_pos": 125, "type": "METRIC", "confidence": 0.9005234241485596}]}, {"text": "N indicates the proportion of sentence vectors in a task for which the null hypothesis of normality in a ShapiroWilk test was not rejected at \u03b1 = 0.05 types of word vector, with similarity computed by the different correlation coefficients as well as cosine similarity and APSynP (.", "labels": [], "entities": [{"text": "APSynP", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.9871319532394409}]}, {"text": "We report these results in, and the full significance analysis for each subtask in.", "labels": [], "entities": [{"text": "significance analysis", "start_pos": 41, "end_pos": 62, "type": "METRIC", "confidence": 0.965684562921524}]}, {"text": "Our observations for the sentence-level experiments are as follows: \u2022 The conclusions from the word-level tasks continue to hold and are even more pronounced: in particular, cosine and Pearson are essentially equivalent, and the increase in performance of rank-based correlation coefficients over cosine similarity on non-normal sentence vectors is quite dramatic.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9348497986793518}]}, {"text": "\u2022 Averaged word vectors compared with rank correlation easily rival modern deep representations compared with cosine similarity.", "labels": [], "entities": []}, {"text": "Finally, the fraction of non-normal word vectors used in sentence-level tasks is consistent with the results reported for the word-level tasks in.", "labels": [], "entities": []}, {"text": "However, we observe the following curious phenomenon for fastText.", "labels": [], "entities": []}, {"text": "While there is no evidence against normality for the majority of fastText vectors, perhaps surprisingly, when we average them to represent sentences, such sentence embeddings are almost entirely non-normal).", "labels": [], "entities": []}, {"text": "Empirically we observe that many high-frequency words or stopwords have prominently non-normal fastText vectors.", "labels": [], "entities": []}, {"text": "Although stopwords constitute only a small fraction of the entire vocabulary, they are very likely to occur in any given sentence, thus rendering most sentence embeddings non-normal as well.", "labels": [], "entities": []}, {"text": "While it's tempting to invoke the Central Limit Theorem (at least for longer sentences), under our formalism, averaging word vectors corresponds to averaging scalar random variables used to represent words, which are neither independent nor identically distributed.", "labels": [], "entities": []}, {"text": "In other words, there are no easy guarantees of normality for such sentence vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean Pearson correlation on STS tasks for  methods using combinations of word vectors and sim- ilarity metrics. All methods use averaged word vec- tors to represent sentences. The similarity measures  are: cosine similarity (COS), Pearson's r (PRS), Spear- man's \u03c1 (SPR), Kendall \u03c4 (KEN) and APSynP (APS).  N indicates the proportion of sentence vectors in a task  for which the null hypothesis of normality in a Shapiro- Wilk test was not rejected at \u03b1 = 0.05", "labels": [], "entities": [{"text": "cosine similarity (COS)", "start_pos": 216, "end_pos": 239, "type": "METRIC", "confidence": 0.9327982187271118}, {"text": "Pearson's r (PRS)", "start_pos": 241, "end_pos": 258, "type": "METRIC", "confidence": 0.9318658312161764}, {"text": "Spear- man's \u03c1 (SPR)", "start_pos": 260, "end_pos": 280, "type": "METRIC", "confidence": 0.6419915594160557}, {"text": "Kendall \u03c4 (KEN)", "start_pos": 282, "end_pos": 297, "type": "METRIC", "confidence": 0.9357022643089294}, {"text": "APSynP (APS)", "start_pos": 302, "end_pos": 314, "type": "METRIC", "confidence": 0.9212310612201691}]}, {"text": " Table 3: Mean Pearson correlation on STS tasks for  a variety of methods in the literature compared to av- eraged fastText vectors with different similarity met- rics: cosine similarity (COS), Spearman's \u03c1 (SPR), and  Kendall \u03c4 (KEN). Values in bold indicate best results  per task. Previous results are taken from Perone et al.  (2018) (only two significant figures provided) and Sub- ramanian et al. (2018).  \u2020 indicates the only STS13 re- sult (to our knowledge) that includes the SMT subtask.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 15, "end_pos": 34, "type": "METRIC", "confidence": 0.8766141831874847}, {"text": "cosine similarity (COS)", "start_pos": 169, "end_pos": 192, "type": "METRIC", "confidence": 0.8695293426513672}, {"text": "Spearman's \u03c1 (SPR)", "start_pos": 194, "end_pos": 212, "type": "METRIC", "confidence": 0.7560784767071406}, {"text": "Kendall \u03c4 (KEN)", "start_pos": 219, "end_pos": 234, "type": "METRIC", "confidence": 0.9617044687271118}, {"text": "SMT", "start_pos": 485, "end_pos": 488, "type": "TASK", "confidence": 0.9731312990188599}]}, {"text": " Table 4: Pearson correlations between human sentence similarity score and a generated score. Generated scores  were produced via measuring Spearman correlation (SPR), as explained in Section 3, and cosine similarity (COS)  between averaged word vectors. Values in bold represent the best result for a subtask given a set of word vectors,  based on a 95% BCa confidence interval", "labels": [], "entities": [{"text": "Spearman correlation (SPR)", "start_pos": 140, "end_pos": 166, "type": "METRIC", "confidence": 0.9134091854095459}, {"text": "cosine similarity (COS)", "start_pos": 199, "end_pos": 222, "type": "METRIC", "confidence": 0.9123086571693421}, {"text": "BCa confidence interval", "start_pos": 355, "end_pos": 378, "type": "METRIC", "confidence": 0.9565393129984537}]}]}