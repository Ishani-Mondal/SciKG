{"title": [], "abstractContent": [{"text": "Fine-grained Entity typing (FGET) is the task of assigning a fine-grained type from a hierarchy to entity mentions in the text.", "labels": [], "entities": [{"text": "Fine-grained Entity typing (FGET)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7158622940381368}]}, {"text": "As the tax-onomy of types evolves continuously, it is desirable for an entity typing system to be able to recognize novel types without additional training.", "labels": [], "entities": []}, {"text": "This work proposes a zero-shot entity typing approach that utilizes the type description available from Wikipedia to build a distributed semantic representation of the types.", "labels": [], "entities": []}, {"text": "During training, our system learns to align the entity mentions and their corresponding type representations on the known types.", "labels": [], "entities": []}, {"text": "At test time, any new type can be incorporated into the system given its Wikipedia descriptions.", "labels": [], "entities": []}, {"text": "We evaluate our approach on FIGER, a public benchmark entity tying dataset.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.8523823022842407}]}, {"text": "Because the existing test set of FIGER covers only a small portion of the fine-grained types, we create anew test set by manually annotating a portion of the noisy training data.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.482728511095047}]}, {"text": "Our experiments demonstrate the effectiveness of the proposed method in recognizing novel types that are not present in the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity Typing assigns a semantic type (e.g., person, location, organization) to an entity mention in text based on the local context.", "labels": [], "entities": [{"text": "Entity Typing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7886632680892944}]}, {"text": "It is useful for enhancing a variety of Natural Language Processing(NLP) tasks such as question answering, relation extraction (, and entity linking ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.880689799785614}, {"text": "relation extraction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.835474044084549}, {"text": "entity linking", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.7415835857391357}]}, {"text": "Traditional Named Entity Typing systems consider a small set of coarse types (e.g., person, location, organization);).", "labels": [], "entities": []}, {"text": "Recent studies address larger sets of fine-grained types organized in type hierarchies (e.g., person/artist, person/author) (.", "labels": [], "entities": []}, {"text": "Fine-Grained Entity Typing (FGET) is usually approached as a multi-label classification task where an entity mention can be assigned multiple types that usually constitute a path in the hierarchy (.", "labels": [], "entities": [{"text": "Fine-Grained Entity Typing (FGET)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7742265909910202}, {"text": "multi-label classification task", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.7873523632685343}]}, {"text": "In real-world scenarios, there is a need to deal with ever-growing type taxonomies.", "labels": [], "entities": []}, {"text": "New types emerge, and existing types are refined into finer sub-categories.", "labels": [], "entities": []}, {"text": "Traditional methods for entity typing assume that the training data contains all possible types, thus require new annotation effort for each new type that emerges.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7819503247737885}]}, {"text": "Zero-shot learning (ZSL), a special kind of transfer learning, allows for new types to be incorporated at the prediction stage without the need for additional annotation and retraining.", "labels": [], "entities": [{"text": "Zero-shot learning (ZSL)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.68009113073349}]}, {"text": "The main idea behind ZSL is to learn a shared semantic space for representing both the seen and unseen types, which allows the knowledge about how examples link to the seen types to be transferred to unseen types.", "labels": [], "entities": []}, {"text": "For fine-grained entity types, we observe that their associated Wikipedia pages often provide a rich description of the types.", "labels": [], "entities": []}, {"text": "To capture this, we propose a Description-based Zero-shot Entity Typing (DZET) approach that utilizes the Wikipedia description of each type (e.g., see https://en.wikipedia.org/wiki/ Artist for description of the type person/artist) to generate a representation of that type.", "labels": [], "entities": [{"text": "Description-based Zero-shot Entity Typing (DZET)", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.7331265338829586}]}, {"text": "We learn to project the entity-mention representations and the type representations into a shared semantic space, such that the mention is closer to the correct type(s) than the incorrect types.", "labels": [], "entities": []}, {"text": "The mid-level type representation derived from the Wikipedia page along with the learned projection function allows the system to recognize new types requiring zero training examples.", "labels": [], "entities": []}, {"text": "We investigate different approaches for constructing the type representation based on Wikipedia descriptions.", "labels": [], "entities": []}, {"text": "Note that the descriptions can be quite long, often containing many different parts that are useful for recognizing different entity mentions.", "labels": [], "entities": []}, {"text": "This motivates us to generate a bag of representations for each type and apply average pooling to aggregate the results.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our methods on FIGER, a benchmark dataset for the FNET task, in which types are organized in 2-levels hierarchy.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.5709170699119568}]}, {"text": "In this work, We focus on testing our method's capability in recognizing unseen fine-grained types ( Level-2 types in this dataset).", "labels": [], "entities": []}, {"text": "As the current test set of FIGER contains examples from only a few level-2 types, we created anew test data that covers most of the level-2 types by manually annotating a portion of the noisy training data.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.7063277363777161}]}, {"text": "Below we summarize our main contributions.", "labels": [], "entities": []}, {"text": "\u2022 We proposed a description-based zero-shot fine-grained entity typing framework that uses Wikipedia descriptions to represent and detect novel types unseen in training.", "labels": [], "entities": []}, {"text": "\u2022 We created anew test set for fine-grained entity typing that provides much better coverage of the level-2 (fine-grained) types compared to the original FIGER test data.", "labels": [], "entities": [{"text": "FIGER test data", "start_pos": 154, "end_pos": 169, "type": "DATASET", "confidence": 0.9162388443946838}]}, {"text": "\u2022 We provided experimental evidence of the effectiveness of our approach in comparison with established baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use FIGER, a publicly available fine-grained entity typing benchmark dataset in which types are organized into a 2-level hierarchy.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.818246066570282}]}, {"text": "The training data consists of sentences sampled from Wikipedia articles and automatically annotated via distant supervision ().", "labels": [], "entities": []}, {"text": "The test data consisting of manually annotated sentences sampled from news reports.", "labels": [], "entities": []}, {"text": "To evaluate our capability to recognize fine-grained types in zero-shot setting, we assume all second-level types are unseen during training, i.e., we remove all level-2 types from the train and dev data but keep them in the test data.", "labels": [], "entities": []}, {"text": "We observe that the FIGER test set covers only a small number of second-level types.", "labels": [], "entities": [{"text": "FIGER test set", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.9022713899612427}]}, {"text": "This renders it insufficient for testing under the evaluation setting we adopt.", "labels": [], "entities": []}, {"text": "Moreover, the training data is noisy since it is automatically annotated by distant supervision.", "labels": [], "entities": []}, {"text": "As a result, we cannot just use part of it for testing.", "labels": [], "entities": []}, {"text": "To overcome this limitation, We manually annotated anew test set from the noisy training data.", "labels": [], "entities": []}, {"text": "We first divide the train set into clean and noisy as suggested in.", "labels": [], "entities": []}, {"text": "Clean examples are those whose types fall on a single path (not necessarily ending with a leaf) in \u03a8.", "labels": [], "entities": []}, {"text": "For instance, the mention with labels person, person/author, and person/doctor is considered as noisy example because the labels form two paths.", "labels": [], "entities": []}, {"text": "We then manually verify the correctness of up to 20 examples from the clean training data for every level-2 type.", "labels": [], "entities": []}, {"text": "These examples are removed from training and added to the test set.", "labels": [], "entities": []}, {"text": "We ignore the types with no clean examples.", "labels": [], "entities": []}, {"text": "The statistics of the new and original datasets are reported in.", "labels": [], "entities": []}, {"text": "We consider two baselines that employ the same neural architecture but use different type representations.", "labels": [], "entities": []}, {"text": "The Label embd baseline use the average of the embedding of the words in the type label as the type representation.", "labels": [], "entities": [{"text": "Label embd baseline", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8645753860473633}]}, {"text": "ProtoLE baseline uses the prototypes-based label embedding learned by , where each type is represented by the set of the most representative entity mentions.", "labels": [], "entities": [{"text": "ProtoLE baseline", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8918616771697998}]}, {"text": "The type embedding is the average of all mentions in the corresponding prototype.", "labels": [], "entities": []}, {"text": "Following prior works in FGET, we report Accuracy (Strict-F 1), loose Macro-averaged F 1 (F 1 ma ) and loose Microaveraged F 1 (F 1 mi ) (.", "labels": [], "entities": [{"text": "FGET", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9484710693359375}, {"text": "Accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9987000226974487}, {"text": "loose Macro-averaged F 1 (F 1 ma )", "start_pos": 64, "end_pos": 98, "type": "METRIC", "confidence": 0.7779273092746735}, {"text": "Microaveraged F 1 (F 1 mi )", "start_pos": 109, "end_pos": 136, "type": "METRIC", "confidence": 0.7397268526256084}]}, {"text": "The training and hyperparameter tuning details are described in the Appendices.", "labels": [], "entities": []}, {"text": "presents the results on FIGER, evaluated on all types (Overall), the seen types (Level-1) and the unseen types (Level-2) respectively.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.7700789570808411}]}, {"text": "From the results, we can see that our description based methods have a particularly strong advantage over baselines on level-2 types.", "labels": [], "entities": []}, {"text": "This is consistent with our expectation because Wikipedia descriptions tend to be highly informative for fine-grained types, but less so for coarser types.", "labels": [], "entities": []}, {"text": "Among the average encoders, we found that weighting the word embedding by the word tfidf produces better results than treating the words equivalently.", "labels": [], "entities": []}, {"text": "As expected, using LSTM based multi-representation adds a noticeable benefit to our system as it produces the best performance among all tested methods, achieving the best performance for level-2 types and outperforming oth-   ers by a large margin while maintaining a highly competitive performance for level-1 types.", "labels": [], "entities": []}, {"text": "The effect of description quality.", "labels": [], "entities": []}, {"text": "analyzes the relationship between the length of the Wikipedia description as one criterion of the description quality and the performance of Multi-rep method.", "labels": [], "entities": []}, {"text": "In particular, we group the types based on the length of their Wikipedia descriptions and provide the five-number summary box plot of the F-scores for each group.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9798491597175598}]}, {"text": "It can be readily observed that the performance is low when the description of the type's Wikipedia page is too short (< 1000 words) or too long ( > 4000 words).", "labels": [], "entities": []}, {"text": "Short descriptions are less informative and carry less shared semantics with the type's mentions.", "labels": [], "entities": []}, {"text": "On the other hand, overly long descriptions could also be confusing as it might share a significant number of common words with the descriptions of other types.", "labels": [], "entities": []}, {"text": "A closer look into the results unveils some exceptions.", "labels": [], "entities": []}, {"text": "For example, the F-score on the type '/education/educational-degree' is 0.7742 even it has along description (6845 words).", "labels": [], "entities": [{"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.998497724533081}]}, {"text": "The description of this type is indeed very informative and includes a comprehensive list of the educational degrees awarded all around the world.", "labels": [], "entities": []}, {"text": "The length of the description is not the only factor that affects the performance of DZET methods.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9823471307754517}]}, {"text": "One factor is the performance on the Level-1 types.", "labels": [], "entities": []}, {"text": "Since the inference is performed by following the type hierarchy, if an incorrect type is inferred at level-1, there is no hope to get the correct level-2 type.", "labels": [], "entities": []}, {"text": "Another factor is the amount of overlapping between the descriptions of the related types.", "labels": [], "entities": []}, {"text": "For instance, Multi-rep produces zero F-score on the types'/event/protest' and '/location/province' because they share a lot of common words with the types '/event/attack' and '/location/county' respectively, which negatively affects the ability of Multi-rep to distinguish between the related types.", "labels": [], "entities": [{"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9992644190788269}]}, {"text": "Both '/event/protest' and '/location/province' have a description length between 2000 and 3000 words.", "labels": [], "entities": []}, {"text": "To mitigate the effect of the contents overlapping between the highly related type, We plan to apply mention-sensitive attention mechanisms for future work to aggregate the scores in Multi-rep instead of max-pooling.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of FIGER dataset.", "labels": [], "entities": [{"text": "FIGER dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8254744410514832}]}, {"text": " Table 2: Level-1 , Level-2 and overall performance of the models on FIGER dataset.", "labels": [], "entities": [{"text": "FIGER dataset", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9398219585418701}]}]}