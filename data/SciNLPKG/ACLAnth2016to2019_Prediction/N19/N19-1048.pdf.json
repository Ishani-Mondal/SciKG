{"title": [{"text": "Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts", "labels": [], "entities": [{"text": "Attentive Mimicking", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8013282418251038}]}], "abstractContent": [{"text": "Learning high-quality embeddings for rare words is a hard problem because of sparse context information.", "labels": [], "entities": []}, {"text": "Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embed-dings of frequent words from their surface form and then used to compute embeddings for rare words.", "labels": [], "entities": [{"text": "Mimicking", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9524195790290833}]}, {"text": "In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word's surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding.", "labels": [], "entities": []}, {"text": "In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words.", "labels": [], "entities": []}, {"text": "Thus, compared to previous work, attentive mimicking improves embeddings fora much larger part of the vocabulary, including the medium-frequency range.", "labels": [], "entities": [{"text": "attentive mimicking", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.46443094313144684}]}], "introductionContent": [{"text": "Word embeddings have led to large performance gains in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7406549155712128}]}, {"text": "However, embedding methods generally need many observations of a word to learn a good representation for it.", "labels": [], "entities": []}, {"text": "One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning.", "labels": [], "entities": []}, {"text": "This can either be done directly (, or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to fine-tune embeddings () or to completely recompute them.", "labels": [], "entities": []}, {"text": "The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings.", "labels": [], "entities": []}, {"text": "However, these methods only work if a word's meaning can at least partially be predicted from its form.", "labels": [], "entities": []}, {"text": "A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations.", "labels": [], "entities": []}, {"text": "While most contemporary approaches exclusively use context information for this task (e.g., recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance.", "labels": [], "entities": []}, {"text": "The problem we address in this paper is that often, only few of a word's contexts provide valuable information about its meaning.", "labels": [], "entities": []}, {"text": "Nonetheless, the current state of the art treats all contexts the same.", "labels": [], "entities": []}, {"text": "We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn -by way of self-attention -to pick a subset of especially informative and reliable contexts.", "labels": [], "entities": [{"text": "mimicking", "start_pos": 96, "end_pos": 105, "type": "TASK", "confidence": 0.960118293762207}]}, {"text": "This mechanism is based on the observation that in many cases, reliable contexts fora given word tend to resemble each other.", "labels": [], "entities": []}, {"text": "We call our proposed architecture attentive mimicking (AM).", "labels": [], "entities": [{"text": "architecture attentive mimicking (AM)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.7366294662157694}]}, {"text": "Our contributions are as follows: (i) We introduce the attentive mimicking model.", "labels": [], "entities": []}, {"text": "It produces high-quality embeddings for rare and mediumfrequency words by attending to the most informative contexts.", "labels": [], "entities": []}, {"text": "(ii) We propose a novel evaluation method based on VecMap ( that allows us to easily evaluate the embedding quality of low-and medium-frequency words.", "labels": [], "entities": []}, {"text": "(iii) We show that attentive mimicking improves word embeddings on various datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we follow the setup of and use the Westbury Wikipedia Corpus (WWC)) for training of all embedding models.", "labels": [], "entities": [{"text": "Westbury Wikipedia Corpus (WWC))", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.9768005609512329}]}, {"text": "To obtain training instances (w, C) for both FCM and AM, we sample words and contexts from the WWC based on their frequency, using only words that occur at least 100 times.", "labels": [], "entities": [{"text": "WWC", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9691413044929504}]}, {"text": "We always train FCM and AM on skipgram embeddings () obtained using Gensim.", "labels": [], "entities": [{"text": "FCM", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.7333868741989136}, {"text": "AM", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9572952389717102}]}, {"text": "Our experimental setup differs from that of Schick and Sch\u00fctze (2019) in two respects: (i) Instead of using a fixed number of contexts for C, we randomly sample between 1 and 64 contexts and (ii) we fix the number of training epochs to 5.", "labels": [], "entities": []}, {"text": "The rationale behind our first modification is that we want our model to produce high-quality embeddings both when we only have a few contexts available and when there is a large number of contexts to pick from.", "labels": [], "entities": []}, {"text": "We fix the number of epochs simply because our evaluation tasks come without development sets on which it maybe optimized.", "labels": [], "entities": []}, {"text": "To evaluate our model, we apply a novel, intrinsic evaluation method that compares embedding spaces by transforming them into a common space ( \u00a74.1).", "labels": [], "entities": []}, {"text": "We also test our model on three word-level downstream tasks ( \u00a74.2, \u00a74.3, \u00a74.4) to demonstrate its versatile applicability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average cosine similarities for the VecMap  evaluation, scaled by a factor of 100.  \u2020: Downsampled  words were included in the training set.", "labels": [], "entities": []}, {"text": " Table 2: Spearman's \u03c1 for various approaches on  SemEval2015 Task 10E", "labels": [], "entities": [{"text": "SemEval2015 Task 10E", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7668430209159851}]}, {"text": " Table 3: Results on the Name Typing dataset for various word frequencies f . The model that uses a linear combi- nation of AM embeddings with skipgram is denoted AM+skip.", "labels": [], "entities": [{"text": "Name Typing", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.828368753194809}]}, {"text": " Table 4: Spearman's \u03c1 for the Chimeras task given 2, 4  and 6 context sentences for the made-up word", "labels": [], "entities": [{"text": "Chimeras task", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9108585715293884}]}]}