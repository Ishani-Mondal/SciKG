{"title": [{"text": "A Simple Joint Model for Improved Contextual Neural Lemmatization", "labels": [], "entities": [{"text": "Improved Contextual Neural Lemmatization", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.7030085027217865}]}], "abstractContent": [{"text": "English verbs have multiple forms.", "labels": [], "entities": []}, {"text": "For instance , talk may also appear as talks, talked or talking, depending on the context.", "labels": [], "entities": []}, {"text": "The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma.", "labels": [], "entities": []}, {"text": "We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.6962603032588959}]}, {"text": "Our paper describes the model in addition to training and decoding procedures.", "labels": [], "entities": []}, {"text": "Error analysis indicates that joint morphological tagging and lemma-tization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.", "labels": [], "entities": [{"text": "joint morphological tagging", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6288917164007822}]}, {"text": "Code and pre-trained models are available at https://sigmorphon.github.io/ sharedtasks/2019/task2/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemmatization is a core NLP task that involves a string-to-string transduction from an inflected word form to its citation form, known as the lemma.", "labels": [], "entities": []}, {"text": "More concretely, consider the English sentence: The bulls are running in Pamplona.", "labels": [], "entities": []}, {"text": "A lemmatizer will seek to map each word to a form you may find in a dictionary-for instance, mapping running to run.", "labels": [], "entities": []}, {"text": "This linguistic normalization is important in several downstream NLP applications, especially for highly inflected languages.", "labels": [], "entities": [{"text": "linguistic normalization", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.7420933246612549}]}, {"text": "Lemmatization has previously been shown to improve recall for information retrieval), to aid machine translation ( and is a core part of modern parsing systems ().", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9245167970657349}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.99817955493927}, {"text": "information retrieval", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.7314205169677734}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.798314094543457}]}, {"text": "However, the task is quite nuanced as the proper choice of the lemma is context dependent.", "labels": [], "entities": []}, {"text": "Listing order is random.: Our structured neural model shown as a hybrid (directed-undirected) graphical model ().", "labels": [], "entities": []}, {"text": "Notionally, thew i denote inflected word forms, them i denote morphological tags and the i denote lemmata.", "labels": [], "entities": []}, {"text": "instance, in the sentence A running of the bulls took place in Pamplona, the word running is its own lemma, since, here, running is a noun rather than an inflected verb.", "labels": [], "entities": []}, {"text": "Several counter-examples exist to this trend, as discussed in depth in.", "labels": [], "entities": []}, {"text": "Thus, a good lemmatizer must make use of some representation of each word's sentential context.", "labels": [], "entities": []}, {"text": "The research question in this work is, then, how do we design a lemmatization model that best extracts the morpho-syntax from the sentential context?", "labels": [], "entities": []}, {"text": "Recent work) has presented a system that directly summarizes the sentential context using a recurrent neural network to decide how to lemmatize.", "labels": [], "entities": []}, {"text": "As Bergmanis and Goldwater (2018)'s system currently achieves state-of-the-art results, it must implicitly learn a contextual representation that encodes the necessary morpho-syntax, as such knowledge is requisite for the task.", "labels": [], "entities": []}, {"text": "We contend, however, that rather than expecting the network to implicitly learn some no-Inflection: \u0432\u0435\u0441\u044c \u0441\u0447\u0430\u0441\u0442\u043b\u0438\u0432\u044b\u0439 \u0441\u0435\u043c\u044c\u044f \u043f\u043e\u0445\u043e\u0436 \u0434\u0440\u0443\u0433 \u043d\u0430 \u0434\u0440\u0443\u0433\u0430\u2026 Lemma:: Example of a morphologically tagged (in purple) and lemmatized (in red) sentence in Russian using the annotation scheme provided in the UD dataset.", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 287, "end_pos": 297, "type": "DATASET", "confidence": 0.912959635257721}]}, {"text": "The translation is given below (in blue).", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9249354600906372}]}, {"text": "tion of morpho-syntax, it is better to explicitly train a joint model to morphologically disambiguate and lemmatize.", "labels": [], "entities": []}, {"text": "Indeed, to this end, we introduce a joint model for the introduction of morphology into a neural lemmatizer.", "labels": [], "entities": [{"text": "introduction of morphology", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7902417580286661}]}, {"text": "A key feature of our model is its simplicity: Our contribution is to show how to stitch existing models together into a joint model, explaining how to train and decode the model.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9920925498008728}]}, {"text": "However, despite the model's simplicity, it still achieves a significant improvement over the state of the art on our target task: lemmatization.", "labels": [], "entities": []}, {"text": "Experimentally, our contributions are threefold.", "labels": [], "entities": []}, {"text": "First, we show that our joint model achieves stateof-the-art results, outperforming (on average) all competing approaches on a 20-language subset of the Universal Dependencies (UD) corpora.", "labels": [], "entities": []}, {"text": "Second, by providing the joint model with gold morphological tags, we demonstrate that we are far from achieving the upper bound on performance-improvements on morphological tagging could lead to substantially better lemmatization.", "labels": [], "entities": []}, {"text": "Finally, we provide a detailed error analysis indicating when and why morphological analysis helps lemmatization.", "labels": [], "entities": []}, {"text": "We offer two tangible recommendations: one is better off using a joint model (i) for languages with fewer training data available and (ii) languages that have richer morphology.", "labels": [], "entities": []}, {"text": "Our system and pre-trained models on all languages in the latest version of the UD corpora 12 are released at https://sigmorphon.github.", "labels": [], "entities": [{"text": "UD corpora 12", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9064285556475321}]}, {"text": "io/sharedtasks/2019/task2/.", "labels": [], "entities": []}, {"text": "1 We compare to previously published numbers on nonrecent versions of UD, but the models we release are trained on the current version (2.3).", "labels": [], "entities": [{"text": "UD", "start_pos": 70, "end_pos": 72, "type": "DATASET", "confidence": 0.6848348379135132}]}, {"text": "Instead of UD schema for morphological attributes, we use the UniMorph schema) instead.", "labels": [], "entities": []}, {"text": "Note the mapping from UD schema to UniMorph schema is not one-to-one mapping).", "labels": [], "entities": []}], "datasetContent": [{"text": "To enable a fair comparison with, we use the Universal Dependencies Treebanks () for all our experiments.", "labels": [], "entities": [{"text": "Universal Dependencies Treebanks", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.6849309007326762}]}, {"text": "Following previous work, we use v2.0 of the treebanks for all languages, except Dutch, for which v2.1 was used due to inconsistencies in v2.0.", "labels": [], "entities": []}, {"text": "The standard splits are used for all treebanks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Here we present the number of tokens in each  of the UD treebanks we use as well as the number of  morphological tags. Note, we take the number of tags  as a proxy for the morphological complexity of the lan- guage. Finally, we present numbers on validation set  from our method with greedy decoding and from the  strongest baseline (Lematus) as well as the difference.  Correlations between the first two columns and the dif- ferences are shown in", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.7757632732391357}]}, {"text": " Table 1. In addition to the perfor-", "labels": [], "entities": []}]}