{"title": [{"text": "Language and Dialect Identification of Cuneiform Texts", "labels": [], "entities": [{"text": "Language and Dialect Identification of Cuneiform Texts", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7348939946719578}]}], "abstractContent": [{"text": "This article introduces a corpus of cuneiform texts from which the dataset for the use of the Cuneiform Language Identification (CLI) 2019 shared task was derived as well as some preliminary language identification experiments conducted using that corpus.", "labels": [], "entities": [{"text": "Cuneiform Language Identification (CLI)", "start_pos": 94, "end_pos": 133, "type": "TASK", "confidence": 0.7661695679028829}, {"text": "language identification", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.7553444802761078}]}, {"text": "We also describe the CLI dataset and how it was derived from the corpus.", "labels": [], "entities": [{"text": "CLI dataset", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.8824456334114075}]}, {"text": "In addition, we provide some baseline language identification results using the CLI dataset.", "labels": [], "entities": [{"text": "language identification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6994661241769791}, {"text": "CLI dataset", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.9470554292201996}]}, {"text": "To the best of our knowledge, the experiments detailed here represent the first time that automatic language identification methods have been used on cuneiform data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7098814398050308}]}], "introductionContent": [{"text": "We have compiled a corpus of cuneiform texts intended to be used in language identification experiments.", "labels": [], "entities": [{"text": "language identification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.743978351354599}]}, {"text": "As the basis for our corpus, we used the Open Richly Annotated Cuneiform Corpus (Oracc).", "labels": [], "entities": [{"text": "Open Richly Annotated Cuneiform Corpus (Oracc)", "start_pos": 41, "end_pos": 87, "type": "DATASET", "confidence": 0.7069462053477764}]}, {"text": "In Oracc, the texts are stored in transliterated form.", "labels": [], "entities": []}, {"text": "We created a tool, Nuolenna, which can transform the transliterations back to the cuneiform script.", "labels": [], "entities": []}, {"text": "Selecting all monolingual lines from Oracc and transforming the transliterations into cuneiform, we created anew corpus for Sumerian and six Akkadian dialects.", "labels": [], "entities": []}, {"text": "This corpus was used in the initial experiments where the possibility of language identification in cuneiform texts was verified.", "labels": [], "entities": [{"text": "language identification", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7481320202350616}]}, {"text": "In this paper, we report some of the results from the initial experiments.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first time that automatic language identification methods have been used on cuneiform data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.712671548128128}]}, {"text": "The methods we use for language identification utilize mainly character n-grams and their observed probabilities in text.", "labels": [], "entities": [{"text": "language identification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7910058200359344}]}, {"text": "[http://oracc.museum.upenn.edu] For the use of the Cuneiform Language Identification (CLI) 2019 shared task 2 , we extracted a dataset from the corpus.", "labels": [], "entities": [{"text": "Cuneiform Language Identification (CLI) 2019 shared task", "start_pos": 51, "end_pos": 107, "type": "TASK", "confidence": 0.7911046677165561}]}, {"text": "The dataset is divided into training, development, and test portions to be used in the CLI shared task which is part of the third VarDial Evaluation Campaign.", "labels": [], "entities": [{"text": "VarDial Evaluation Campaign", "start_pos": 130, "end_pos": 157, "type": "DATASET", "confidence": 0.7041863203048706}]}, {"text": "We implemented four baseline language identifiers and evaluated their performance using the CLI dataset.", "labels": [], "entities": [{"text": "CLI dataset", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.943303108215332}]}, {"text": "The results of the evaluation are presented here.", "labels": [], "entities": []}], "datasetContent": [{"text": "To find out to what extent identifying the language of cuneiform text is possible, we performed initial language identification experiments using a stateof-the-art language identification method called HeLI (Jauhiainen et al., 2016).", "labels": [], "entities": [{"text": "identifying the language of cuneiform text", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.8255132138729095}, {"text": "initial language identification", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.6837303241093954}]}, {"text": "The HeLI method has recently fared well in VarDial shared tasks for Swiss-German dialect and Indo-Aryan language identification (Jauhiainen et al., 2018a,b).", "labels": [], "entities": [{"text": "VarDial shared tasks", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.705525279045105}, {"text": "Indo-Aryan language identification", "start_pos": 93, "end_pos": 127, "type": "TASK", "confidence": 0.5826818545659384}]}, {"text": "The experiments were conducted on individual lines as well as texts spanning several lines.", "labels": [], "entities": []}, {"text": "In Oracc, the transliterated words are separated by whitespaces, which is not the casein the original documents.", "labels": [], "entities": []}, {"text": "In order to mimic the original documents, we removed all the whitespaces from each line of cuneiform text.", "labels": [], "entities": []}, {"text": "We also ignored any completely broken signs, which were marked with an 'x'.", "labels": [], "entities": []}, {"text": "The individual words in Oracc are tagged with language or dialect information, and sometimes a single line includes words in different languages or dialects.", "labels": [], "entities": []}, {"text": "As we set out to do language identification on monolingual texts, we used all those lines which had words in only one language, leaving out multilingual lines.", "labels": [], "entities": [{"text": "language identification", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7036757469177246}]}, {"text": "The language tagging in Oracc is not always precise, and therefore some lines in our dataset might still include several languages.", "labels": [], "entities": [{"text": "language tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7249915599822998}]}, {"text": "In the preliminary experiments, we experimented with the language identification of both monolingual lines and monolingual texts spanning several lines with the information about line breaks retained.", "labels": [], "entities": [{"text": "language identification", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6855113804340363}]}, {"text": "Mostly, each text had the lines of one original document, but if the document was multilingual, it was divided into different texts according to the languages attested.", "labels": [], "entities": []}, {"text": "We left out the Akkadian varieties of Old and Middle Assyrian as the number of lines available for those dialects was less than 1,000.", "labels": [], "entities": []}, {"text": "We had datasets in the Sumerian language as well as the Akkadian varieties of Old Babylonian, Middle Babylonian peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian, and NeoAssyrian.", "labels": [], "entities": []}, {"text": "The statistics of the corpus used in the preliminary experiments are shown in.", "labels": [], "entities": []}, {"text": "We were interested in experimenting in both indomain and out-of-domain test settings as well as with language identification on two different levels: individual lines and texts.", "labels": [], "entities": [{"text": "language identification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.7214592397212982}]}, {"text": "In supervised machine learning, the testing data is in-domain if it is similar to the training data.", "labels": [], "entities": []}, {"text": "For example, if sentences are from texts that belong to the same genre or collection they are considered more in-domain than if they are not.", "labels": [], "entities": []}, {"text": "An even stronger in-domain case is if the sentences are from the same text.", "labels": [], "entities": []}, {"text": "Classification of test data which is in-domain with the training data is usually much easier than when it is out-of-domain.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9527350068092346}]}, {"text": "The texts in the Oracc export were in the order 11 of \"projects,\" which are collections of texts that have some common theme.", "labels": [], "entities": [{"text": "Oracc export", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.784557968378067}]}, {"text": "The texts in different projects can be considered to be more out-of-domain with each other than those from the same project.", "labels": [], "entities": []}, {"text": "The projects from which the texts were extracted are listed in.", "labels": [], "entities": []}, {"text": "From this corpus, we generated four different test settings.", "labels": [], "entities": []}, {"text": "For the out-of-domain experiments, we divided the corpus so that we used the first half of the corpus for training and the second half was divided between development and testing.", "labels": [], "entities": []}, {"text": "For the in-domain experiments, we divided the corpus into parts of 20 lines or texts and took the 10 first lines or texts from each part for training, the next 5 for development, and the last 5 for testing.", "labels": [], "entities": []}, {"text": "We, thus, ended up with four different datasets, 12 two for lines and two for texts.", "labels": [], "entities": []}, {"text": "Each of the datasets had 50% of the material for training, 25% for development, and 25% for testing.", "labels": [], "entities": []}, {"text": "The HeLI method is a supervised-learning language identification method where the language models are created from a correctly tagged training corpus.", "labels": [], "entities": [{"text": "supervised-learning language identification", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.737417737642924}]}, {"text": "The language models consist of words and sign (character) n-grams.", "labels": [], "entities": []}, {"text": "When n-grams are extracted from a corpus, the number of unique n- The projects were in the alphabetical order by their abbreviations.", "labels": [], "entities": []}, {"text": "See grams is higher the longer the n-grams are.", "labels": [], "entities": []}, {"text": "The actual number of occurrences of the longer n-grams is lower than the shorter n-grams.", "labels": [], "entities": []}, {"text": "The exact optimal value for n depends on, among many other variables, the size of the training corpus, the length of the text to be identified, and the repertoire of the languages considered.", "labels": [], "entities": []}, {"text": "Sometimes the longer n-grams could carry important information even though they are seldom found in the text to be identified.", "labels": [], "entities": []}, {"text": "The basic idea in the HeLI method is to score individual words using the longest length n-grams possible.", "labels": [], "entities": []}, {"text": "For each individual language, the words are scored first, after which the whole text gets the average score of the individual words.", "labels": [], "entities": []}, {"text": "In the case of cuneiform text, as it is not divided into words, we use just sign n-grams and consider a line of text as a word as far as the HeLI method is concerned.", "labels": [], "entities": []}, {"text": "The individual words, or in this case lines, are scored by taking the average score of the found n-grams.", "labels": [], "entities": []}, {"text": "Using the notation introduced by, the individual n-grams f found from the line to be tested, get a score R as in Equation 1: where c(C g , f ) is the count of the feature fin the training corpus C g of the language g and l CF g is the total number of occurrences of all the n-grams of the same length in the training corpus.", "labels": [], "entities": [{"text": "Equation", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9147971868515015}]}, {"text": "As smoothing, in case the count of a feature is zero in some languages, this version of the HeLI method uses a score R HeLI (g, f ) for the count of one multiplied by a penalty multiplier.", "labels": [], "entities": [{"text": "HeLI", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.7712950706481934}]}, {"text": "Using the development sets, we optimized the sign n-gram range and the penalty multiplier for each setting individually.", "labels": [], "entities": []}, {"text": "The results of these experiments are presented in.", "labels": [], "entities": []}, {"text": "As the performance measure, we use the F1-score which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991833567619324}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9994913339614868}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9965294003486633}]}, {"text": "The results clearly show how the task of identifying a single line is much harder than that of a complete text.", "labels": [], "entities": [{"text": "identifying a single line", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8144363611936569}]}, {"text": "The task of out-of-domain identification is also clearly more difficult than that of in-domain, as was expected.", "labels": [], "entities": [{"text": "out-of-domain identification", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6851012110710144}]}, {"text": "Quite many of the misclassified lines were very short; many consisted only of one sign and were truly ambiguous and often present in different dialects and even languages.", "labels": [], "entities": []}, {"text": "Nevertheless, it was still possible to attain reasonably good language identification results.", "labels": [], "entities": [{"text": "language identification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.6958762258291245}]}, {"text": "The hardest test setting was  where the language of individual out-of-domain lines was to be identified.", "labels": [], "entities": []}, {"text": "To us, this seemed to be the most interesting and relevant setting to be used in the CLI shared task, especially if we leave out the extremely short and possibly ambiguous lines.", "labels": [], "entities": [{"text": "CLI shared task", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.6583582560221354}]}, {"text": "For the CLI task, we created a separate, especially tailored dataset.", "labels": [], "entities": []}, {"text": "The participants were given texts for training and development and separate texts were given for testing at the end of the campaign.", "labels": [], "entities": []}, {"text": "The training set was exactly the same as the one we used in the preliminary experiments and the number of lines in the training portion for each language or dialect is shown in.", "labels": [], "entities": []}, {"text": "For the CLI development and test sets, we performed some further operations.", "labels": [], "entities": []}, {"text": "The original datasets included duplicate lines, so we first removed all duplicates.", "labels": [], "entities": []}, {"text": "Then we filtered out all lines shorter than three characters.", "labels": [], "entities": []}, {"text": "After these operations, the smallest sets were those of Old Babylonian including 668 lines in the development set and 985 lines in the test set.", "labels": [], "entities": []}, {"text": "As we wanted to make the development and the test sets equal in size between languages and dialects, we randomly selected the same number of lines from the other languages.", "labels": [], "entities": []}, {"text": "Thus, in the CLI task, the development sets for each language consisted of 668 lines and the test sets of 985 lines.", "labels": [], "entities": []}, {"text": "We used four of the methods described in the survey by to implement baseline language identifiers for the CLI task.", "labels": [], "entities": []}, {"text": "As features, we used sign n-grams of different lengths.", "labels": [], "entities": []}, {"text": "The first method is called simple scoring.", "labels": [], "entities": [{"text": "simple scoring", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6812371611595154}]}, {"text": "In simple scoring, all the n-grams generated from the line to be identified M are compared to the language models and for each n-gram found in a language model dom(O(C g )), the score R of the language g is increased by one.", "labels": [], "entities": []}, {"text": "The language gaining the highest score is selected as the predicted language.", "labels": [], "entities": []}, {"text": "formulate the method as in Equation 2: where l M F is the number of individual features in the line M and f i is its ith feature.", "labels": [], "entities": []}, {"text": "The second method is the sum of relative frequencies where relative frequencies are added to the score of the language.", "labels": [], "entities": []}, {"text": "formulate the method as in Equation 3: where c(C g , f i ) is the count of the feature f i in the training corpus.", "labels": [], "entities": []}, {"text": "The third method is the product of relative frequencies where the relative frequencies are multiplied together.", "labels": [], "entities": []}, {"text": "formulate the method as in Equation 4: The actual implementation adds together negative logarithms of the relative frequencies, which produces results with the same ordering.", "labels": [], "entities": []}, {"text": "As a smoothing value, we used the negative logarithm of a comparably small relative frequency.", "labels": [], "entities": []}, {"text": "The actual value was optimized using the development set.", "labels": [], "entities": []}, {"text": "The product of relative frequencies differs from the HeLI method (Equation 1) in that it always uses the full range of available feature types (different length n-grams), as opposed to the HeLI  method, which uses only the longest length ngrams applicable.", "labels": [], "entities": []}, {"text": "The fourth method is a majority-voting-based ensemble of the three previous methods.", "labels": [], "entities": []}, {"text": "The parameters and the best possible language models are determined by training the identifier using the training set and evaluating its performance on the development set.", "labels": [], "entities": []}, {"text": "Once the best parameters are decided, the texts in the development set can also be added to the training set for the final evaluation against the test set.", "labels": [], "entities": []}, {"text": "We used the macro F1-score as the measure for language identification performance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9169211387634277}, {"text": "language identification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8180178999900818}]}, {"text": "For each of the methods, we evaluated all possible sign n-gram ranges from 1 to 15 using the development set.", "labels": [], "entities": []}, {"text": "shows the results for all the methods using parameters optimized with the development set.", "labels": [], "entities": []}, {"text": "In the voting ensemble, we used the best parameters for the methods from the individual experiments, and in case of a tie, the result from the product of relative frequencies was used.", "labels": [], "entities": []}, {"text": "The product of relative frequencies method is clearly superior to the other two basic methods with an F1-score of 0.7206 using 2.0 as the smoothing value and sign n-grams from one to four.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9994458556175232}]}, {"text": "Adding the prediction information from the other two methods in the form of a voting ensemble also fails to improve the result.", "labels": [], "entities": []}, {"text": "The F1-score achieved when using the HeLI method does not reach the one from the product of relative frequencies method either.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993521571159363}, {"text": "HeLI", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.4860045611858368}]}, {"text": "The F1-score gained by the HeLI method is clearly higher than the score attained in the preliminary experiments, which was as expected, as we had filtered out some of the most difficult cases. is a confusion matrix displaying the exact number of identifications.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994158744812012}, {"text": "HeLI", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.4763333797454834}]}, {"text": "The diagonal values represent correct identifications.", "labels": [], "entities": []}, {"text": "Standard Babylonian and Neo-Babylonian were the most difficult varieties to distinguish, mostly being erroneously identified as each other.", "labels": [], "entities": []}, {"text": "Late Babylonian was the easiest to identify, with a recall of over 96%.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9931842684745789}]}], "tableCaptions": [{"text": " Table 1: Number of texts, lines, and signs for each language or variety in the corpus.", "labels": [], "entities": []}, {"text": " Table 3: The F1-scores attained by the HeLI method  in the preliminary experiments.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9991222023963928}, {"text": "HeLI", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.683831512928009}]}, {"text": " Table 4.  For the CLI development and test sets, we per- formed some further operations. The original  datasets included duplicate lines, so we first re- moved all duplicates. Then we filtered out all lines  shorter than three characters. After these opera- tions, the smallest sets were those of Old Baby- lonian including 668 lines in the development set  and 985 lines in the test set. As we wanted to  make the development and the test sets equal in  size between languages and dialects, we randomly  selected the same number of lines from the other  languages. Thus, in the CLI task, the development  sets for each language consisted of 668 lines and the test sets of 985 lines.", "labels": [], "entities": [{"text": "Old Baby- lonian", "start_pos": 298, "end_pos": 314, "type": "DATASET", "confidence": 0.9412226378917694}]}, {"text": " Table 5: The macro F1-scores attained by the baseline  methods with the CLI dataset.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9019604325294495}, {"text": "CLI dataset", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.958827942609787}]}, {"text": " Table 6: Confusion matrix for the product of relative  frequencies method. The rows indicate the actual lan- guages and the columns indicate predicted languages.  Correct identifications are emphasized.", "labels": [], "entities": []}]}