{"title": [], "abstractContent": [{"text": "Unsupervised domain adaptation (UDA) is the task of modifying a statistical model trained on labeled data from a source domain to achieve better performance on data from a target domain , with access to only unlabeled data in the target domain.", "labels": [], "entities": [{"text": "Unsupervised domain adaptation (UDA)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7831390152374903}]}, {"text": "Existing state-of-the-art UDA approaches use neural networks to learn representations that can predict the values of subset of important features called \"pivot features.\"", "labels": [], "entities": []}, {"text": "In this work, we show that it is possible to improve on these methods by jointly training the representation learner with the task learner, and examine the importance of existing pivot selection methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised domain adaptation (UDA) is the task of modifying a statistical model trained on labeled data from a source domain to achieve better performance on data from a target domain, without access to any labeled data in the target domain.", "labels": [], "entities": [{"text": "Unsupervised domain adaptation (UDA)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7895333021879196}]}, {"text": "Supervised domain adaptation methods can obtain excellent performance from a small number of labeled examples in the target domain, but UDA is attractive in cases where annotation requires specialized expertise or the number of meaningfully different sub-domains is large (e.g., both are true for clinical NLP).", "labels": [], "entities": []}, {"text": "Structural correspondence learning) (SCL) is one widely-used method for UDA in natural language processing.", "labels": [], "entities": [{"text": "Structural correspondence learning) (SCL)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8939758794648307}, {"text": "UDA", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.977901041507721}]}, {"text": "The key idea in SCL is that a subset of features, believed to be predictive across domains, are selected as pivot features.", "labels": [], "entities": [{"text": "SCL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9747101664543152}]}, {"text": "For each selected pivot feature, SCL creates an auxiliary classification task of predicting the value of that feature in an instance, given the values of all the non-pivot features for that instance.", "labels": [], "entities": []}, {"text": "The auxiliary classifiers therefore learn important cross-domain information about the structure of the feature space, which the SCL algorithm uses to create an augmented representation that aligns features from different domains (further details in Section 2).", "labels": [], "entities": []}, {"text": "Meanwhile, recent advances in neural network learning have shown that training regimens that jointly consider evidence from multiple sources can improve performance -both multi-task learning and fine tuning.", "labels": [], "entities": []}, {"text": "However, existing SCL-based methods treat the representation learning and task learning as separate tasks, so the parameters of the representation learning machinery are fixed before training for the downstream task.", "labels": [], "entities": []}, {"text": "Jointly learning the representation-and task-specific parameters can potentially allow a learning algorithm to find representations that are better suited for the task.", "labels": [], "entities": []}, {"text": "In this work, we describe anew UDA algorithm that is trained to jointly maximize two objectives: the primary supervised task in the source domain, and a pivot feature reconstruction task that can be trained on unlabeled data.", "labels": [], "entities": [{"text": "pivot feature reconstruction", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.6749394734700521}]}, {"text": "We also explore the importance of pivot feature selection to this algorithm, in experiments that quantitatively and qualitatively examine the quality of existing pivot selection methods.", "labels": [], "entities": [{"text": "pivot feature selection", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6629236737887064}]}, {"text": "We find that our joint neural approach to SCL improves unsupervised domain adaptation substantially on a standard sentiment classification task.", "labels": [], "entities": [{"text": "SCL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9788316488265991}, {"text": "domain adaptation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7130455076694489}, {"text": "sentiment classification task", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.8808673222859701}]}, {"text": "Our results also show that while existing pivot selection methods perform well, they are below an oracle-provided ceiling for many source-target pairs for the sentiment classification task we examine.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 159, "end_pos": 188, "type": "TASK", "confidence": 0.9302136301994324}]}], "datasetContent": [{"text": "We follow the standard setup for the Amazon sentiment task, splitting each source dataset into 1600 training and 400 validation instances, and evaluating on the entire labeled target dataset for each pair.", "labels": [], "entities": [{"text": "Amazon sentiment task", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6615880032380422}]}, {"text": "We compare against two baselines: First, the reported results of Ziser and Reichart, and second, our replication of their results using their code.", "labels": [], "entities": []}, {"text": "Our replication changed their code by replacing the stochastic gradient descent optimizer with Adam (, and increasing the training batch size from 1 to 50.", "labels": [], "entities": []}, {"text": "These changes were made to speed training runs during development; we found they produced better-thanreported results and include these superior results as an even stronger baseline.", "labels": [], "entities": []}, {"text": "We report results of two configurations of our joint learner.", "labels": [], "entities": []}, {"text": "The first configuration (Joint MI ) uses the MI between source labels and features to select 100 pivot features.", "labels": [], "entities": []}, {"text": "The second configuration (Joint Oracle ) is an oracle-informed system where we use the MI between target labels and features to select pivot features, but only use source labels while training the network.", "labels": [], "entities": []}, {"text": "Both the AE-SCL R model and our Joint MI model were run for 10 iterations to minimize differences due to random initialization and to calculate significance statistics.", "labels": [], "entities": [{"text": "AE-SCL R", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.6439444124698639}]}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "First, we note that our replication of AE-SCL im-  proves upon their reported results in 8 of 12 pairs, often by substantial margins, and is only worse in one pair (Kitchen\u2192Books).", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9717296957969666}]}, {"text": "Our Joint MI method is superior to the reported AE-SCL results in all pairs, 1.7 points (absolute) on average, and significantly better than the AE-SCL R in 9 of 12 pairs, using Welch's one-tailed t-test.", "labels": [], "entities": [{"text": "Joint MI", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.4108281135559082}, {"text": "AE-SCL", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9874246716499329}, {"text": "AE-SCL R", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.958440363407135}]}, {"text": "This is, to our knowledge, the best result on this task using a feature-based approach (i.e., excluding systems that use embeddings).", "labels": [], "entities": []}, {"text": "Despite constraining our system to adapting feature-based models, this result is competitive with the best-known result using a pure neural approach with embeddings as input, as report an average accuracy of 0.804.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9974669218063354}]}, {"text": "The Joint Oracle configuration shows that, despite the large gains of joint training, there is still significant improvement available with better pivot selection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of results. B=Books, D=Dvd,  E=Electronics,K=Kitchen. AE-SCL=Reported results  from Ziser and Reichart (2017). AE-SCL R =Replicated  results from the same. Joint M I =results from the joint  model in Section 3.1. Joint O =results from the joint  model using oracle MI pivot selection. Bold indicates  a significant difference (p < 0.05) between AE-SCL R  and Joint M I using Welch's one-tailed t-test.", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9923468828201294}, {"text": "AE-SCL R", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9611104130744934}]}]}