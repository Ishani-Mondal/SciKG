{"title": [{"text": "Recursive Subtree Composition in LSTM-Based Dependency Parsing", "labels": [], "entities": [{"text": "Recursive Subtree Composition", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7596096197764078}, {"text": "LSTM-Based Dependency Parsing", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.5920616288979849}]}], "abstractContent": [{"text": "The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing.", "labels": [], "entities": [{"text": "tree structure modelling", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6383878489335378}, {"text": "neural dependency parsing", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6044794817765554}]}, {"text": "We investigate the impact of adding a tree layer on top of a sequential model by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM.", "labels": [], "entities": []}, {"text": "Composition seems superfluous with such a model, suggesting that BiLSTMs capture information about sub-trees.", "labels": [], "entities": []}, {"text": "We perform model ablations to tease out the conditions under which composition helps.", "labels": [], "entities": []}, {"text": "When ablating the backward LSTM, performance drops and composition does not recover much of the gap.", "labels": [], "entities": []}, {"text": "When ablating the forward LSTM, performance drops less dramatically and composition recovers a substantial part of the gap, indicating that a forward LSTM and composition capture similar information.", "labels": [], "entities": []}, {"text": "We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers.", "labels": [], "entities": []}, {"text": "To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM.", "labels": [], "entities": []}, {"text": "We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recursive neural networks allow us to construct vector representations of trees or subtrees.", "labels": [], "entities": []}, {"text": "They have been used for constituency parsing by and and for dependency parsing by and , among others.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8467925190925598}, {"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9151333868503571}]}, {"text": "In particular,  showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 120, "end_pos": 155, "type": "TASK", "confidence": 0.6179790794849396}]}, {"text": "These results were further strengthened in who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by.", "labels": [], "entities": [{"text": "Recurrent Neural Network Grammar (RNNG) generative parser", "start_pos": 114, "end_pos": 171, "type": "TASK", "confidence": 0.6954425805144839}]}, {"text": "Ina parallel development, showed that using BiLSTMs for feature extraction can lead to high parsing accuracy even with fairly simple parsing architectures, and using BiLSTMs for feature extraction has therefore become very popular in dependency parsing.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7437666356563568}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9516022801399231}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.870716392993927}, {"text": "feature extraction", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.726616695523262}, {"text": "dependency parsing", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.8340250551700592}]}, {"text": "It is used in the state-of-the-art parser of, was used in 8 of the 10 highest performing systems of the 2017 CoNLL shared task () and 10 out of the 10 highest performing systems of the 2018 CoNLL shared task (.", "labels": [], "entities": [{"text": "CoNLL shared task", "start_pos": 190, "end_pos": 207, "type": "DATASET", "confidence": 0.8079301317532858}]}, {"text": "This raises the question of whether features extracted with BiLSTMs in themselves capture information about subtrees, thus making recursive composition superfluous.", "labels": [], "entities": []}, {"text": "Some support for this hypothesis comes from the results of which indicate that LSTMs can capture hierarchical information: they can be trained to predict long-distance number agreement in English.", "labels": [], "entities": []}, {"text": "Those results were extended to more constructions and three additional languages by.", "labels": [], "entities": []}, {"text": "However, have also shown that although sequential LSTMs can learn syntactic information, a recursive neural network which explicitly models hierarchy (the RNNG model from ) is better at this: it performs better on the number agreement task from.", "labels": [], "entities": []}, {"text": "To further explore this question in the context of dependency parsing, we investigate the use of recursive composition (henceforth referred to as composition) in a parser with an architecture like the one in.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7946445643901825}]}, {"text": "This allows us to explore variations of features and isolate the conditions under which composi-tion is helpful.", "labels": [], "entities": []}, {"text": "We hypothesise that the use of a BiLSTM for feature extraction makes it possible to capture information about subtrees and therefore makes the use of subtree composition superfluous.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7504723370075226}]}, {"text": "We hypothesise that composition becomes useful when part of the BiLSTM is ablated, the forward or the backward LSTM.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.841562032699585}]}, {"text": "We further hypothesise that composition is most useful when the parser has no access to information about the function of words in the context of the sentence given by POS tags.", "labels": [], "entities": []}, {"text": "When using POS tags, the tagger has indeed had access to the full sentence.", "labels": [], "entities": []}, {"text": "We additionally look at what happens when we ablate character vectors which have been shown to capture information which is partially overlapping with information from POS tags.", "labels": [], "entities": []}, {"text": "We experiment with a wider variety of languages than  in order to explore whether the usefulness of different model variants vary depending on language type.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: UAS ensemble (full) and ablated experiments.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4013284742832184}]}]}