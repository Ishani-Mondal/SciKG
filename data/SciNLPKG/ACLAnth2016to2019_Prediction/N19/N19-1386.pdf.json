{"title": [{"text": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training", "labels": [], "entities": [{"text": "Word Translation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7034948617219925}]}], "abstractContent": [{"text": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space.", "labels": [], "entities": []}, {"text": "However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs.", "labels": [], "entities": []}, {"text": "In this work, we revisit ad-versarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results.", "labels": [], "entities": [{"text": "word translation", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.720775842666626}]}, {"text": "Our method includes regu-larization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator.", "labels": [], "entities": [{"text": "input reconstruction", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7605399191379547}]}, {"text": "Extensive experimen-tations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning cross-lingual word embeddings has been shown to bean effective way to transfer knowledge from one language to another for many key linguistic tasks including machine translation, named entity recognition, part-of-speech tagging, and parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8140218555927277}, {"text": "named entity recognition", "start_pos": 188, "end_pos": 212, "type": "TASK", "confidence": 0.633532295624415}, {"text": "part-of-speech tagging", "start_pos": 214, "end_pos": 236, "type": "TASK", "confidence": 0.7448762357234955}, {"text": "parsing", "start_pos": 242, "end_pos": 249, "type": "TASK", "confidence": 0.9633697867393494}]}, {"text": "While earlier efforts solved the associated word alignment problem using large parallel corpora (, broader applicability demands methods to relax this requirement since acquiring a large corpus of parallel data is not feasible inmost scenarios.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7750857472419739}]}, {"text": "Recent methods instead use embeddings learned from monolingual data, and learn a linear mapping from one language to another with the underlying assumption that two embedding spaces exhibit similar geometric structures (i.e., approximately isomorphic).", "labels": [], "entities": []}, {"text": "This allows the model to learn effective cross-lingual representations without expensive supervision (.", "labels": [], "entities": []}, {"text": "Given monolingual word embeddings of two languages, show that a linear mapping can be learned from a seed dictionary of 5000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors.", "labels": [], "entities": []}, {"text": "Subsequent works ( propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function.", "labels": [], "entities": []}, {"text": "While these methods assume some supervision in the form of a seed dictionary, recently fully unsupervised methods have shown competitive results.", "labels": [], "entities": []}, {"text": "first reported encouraging results with adversarial training.", "labels": [], "entities": []}, {"text": "improved this approach with post-mapping refinements, showing impressive results for several language pairs.", "labels": [], "entities": []}, {"text": "Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (.", "labels": [], "entities": []}, {"text": "Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently ().", "labels": [], "entities": []}, {"text": "In particular, show that the adversarial methods of and fail for many language pairs.", "labels": [], "entities": []}, {"text": "In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.", "labels": [], "entities": []}, {"text": "Our main idea is to learn the cross-lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space.", "labels": [], "entities": []}, {"text": "We accomplish this by proposing a novel adversarial autoencoder framework (, where adversarial mapping is done at the (latent) code space as opposed to the original embedding space.", "labels": [], "entities": []}, {"text": "This gives the model the flexibility to automatically induce the required geometric structures in its latent code space that could potentially yield better mappings.", "labels": [], "entities": []}, {"text": "recently find that the isomorphic assumption made by most existing methods does not hold in general even for two closely related languages like English and German.", "labels": [], "entities": []}, {"text": "In their words \"approaches based on this assumption have important limitations\".", "labels": [], "entities": []}, {"text": "By mapping the latent vectors through adversarial training, our approach therefore departs from the isomorphic assumption.", "labels": [], "entities": []}, {"text": "In our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator.", "labels": [], "entities": []}, {"text": "This forces the discriminator to improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation.", "labels": [], "entities": []}, {"text": "To guide the mapping, we include two additional constraints.", "labels": [], "entities": []}, {"text": "Our first constraint enforces cycle consistency so that code vectors after being translated from one language to another, and then translated back to their source space remain close to the original vectors.", "labels": [], "entities": []}, {"text": "The second constraint ensures reconstruction of the original input word embeddings from the back-translated codes.", "labels": [], "entities": []}, {"text": "This grounding step forces the model to retain word semantics during the mapping process.", "labels": [], "entities": []}, {"text": "We conduct a series of experiments with six different language pairs (in both directions) comprising European, non-European, and low-resource languages from two different datasets.", "labels": [], "entities": []}, {"text": "Our results show that our model is more robust and yields significant gains over for all translation tasks in all evaluation measures.", "labels": [], "entities": []}, {"text": "Our method also gives better initial mapping compared to other existing methods).", "labels": [], "entities": []}, {"text": "We also perform an extensive ablation study to understand the contribution of different components of our model.", "labels": [], "entities": []}, {"text": "The study reveals that cycle consistency contributes the most, while adversarial training of the target encoder and post-cycle reconstruction also have significant effect.", "labels": [], "entities": []}, {"text": "We have released our source code at https://ntunlpsg.github.io/ project/unsup-word-translation/ The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "After discussing related work in Section 2, we present our unsupervised word translation approach with adversarial autoencoder in Section 3.", "labels": [], "entities": [{"text": "word translation", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.716520369052887}]}, {"text": "We describe our experimental setup in Section 4, and present our results with in-depth analysis in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we summarize our findings with possible future directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the tradition, we evaluate our model on word translation (a.k.a. bilingual lexicon induction) task, which measures the accuracy of the predicted dictionary to a gold standard dictionary.", "labels": [], "entities": [{"text": "word translation", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7651570737361908}, {"text": "bilingual lexicon induction)", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.751781091094017}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9982137680053711}]}, {"text": "We evaluate our model on two different datasets.", "labels": [], "entities": []}, {"text": "The first one is from, which consists of FastText monolingual embeddings of (d =) 300 dimensions () trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs.", "labels": [], "entities": []}, {"text": "3 To show the generality of different methods, we consider European, non-European and low-resource languages.", "labels": [], "entities": []}, {"text": "In particular, we evaluate on English (En) from/to Spanish (Es), German (De), Italian (It), Arabic (Ar), Malay (Ms), and Hebrew (He).", "labels": [], "entities": []}, {"text": "We also evaluate on the more challenging dataset of and its subsequent extension by.", "labels": [], "entities": []}, {"text": "We will refer to this dataset as Dinu-Artexe dataset.", "labels": [], "entities": [{"text": "Dinu-Artexe dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.9644695222377777}]}, {"text": "From this dataset, we choose to experiment on English from/to Italian and Spanish.", "labels": [], "entities": []}, {"text": "English and Italian embeddings were trained on WacKy corpora using CBOW (, while the Spanish embeddings were trained on WMT News Crawl.", "labels": [], "entities": [{"text": "WacKy corpora", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9806202352046967}, {"text": "CBOW", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.6878407597541809}, {"text": "WMT News Crawl", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.9698085983594259}]}, {"text": "The CBOW vectors are also of 300 dimensions.", "labels": [], "entities": [{"text": "CBOW vectors", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8107743263244629}]}], "tableCaptions": [{"text": " Table 1: Word translation accuracy (P@1) on European  languages on the dataset of Conneau et al. (2018) using  fastText embeddings (trained on Wikipedia). '-' indi- cates the authors did not report the number.", "labels": [], "entities": [{"text": "Word translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6053010076284409}, {"text": "accuracy (P@1)", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.7943821996450424}]}, {"text": " Table 2: Word translation accuracy (P@1) on non- European and low-resource languages on the dataset of  Conneau et al. (2018) using fastText embeddings. **  indicates the model failed to converge.", "labels": [], "entities": [{"text": "Word translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6155756264925003}, {"text": "accuracy (P@1)", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.8260213534037272}]}, {"text": " Table 3: Word translation accuracy (P@1) on the  English-Italian and English-Spanish language pairs of  Dinu-Artetxe dataset (Dinu et al., 2015; Artetxe et al.,  2017). All methods use CBOW embeddings. ** in- dicates the model failed to converge; '-' indicates the  authors did not report the number.", "labels": [], "entities": [{"text": "Word translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.586922213435173}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8537087440490723}, {"text": "P@1)", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9244699329137802}, {"text": "Dinu-Artetxe dataset", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.7614058554172516}]}, {"text": " Table 4: Conneau et al. (2018) refinement applied to  the initial mappings of Artetxe et al. (2018b). ** indi- cates the model failed to converge.", "labels": [], "entities": []}, {"text": " Table 5: Ablation study of our adversarial autoencoder model on the dataset of", "labels": [], "entities": []}]}