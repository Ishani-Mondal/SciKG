{"title": [{"text": "COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge", "labels": [], "entities": [{"text": "Question Answering Challenge Targeting Commonsense Knowledge", "start_pos": 17, "end_pos": 77, "type": "TASK", "confidence": 0.8869141737620035}]}], "abstractContent": [{"text": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context.", "labels": [], "entities": []}, {"text": "Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background.", "labels": [], "entities": []}, {"text": "To investigate question answering with prior knowledge, we present COMMONSENSEQA: a challenging new dataset for commonsense question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8114628493785858}, {"text": "commonsense question answering", "start_pos": 112, "end_pos": 142, "type": "TASK", "confidence": 0.7913006742795309}]}, {"text": "To capture commonsense beyond associations, we extract from CON-CEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept.", "labels": [], "entities": []}, {"text": "Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts.", "labels": [], "entities": []}, {"text": "This encourages workers to create questions with complex semantics that often require prior knowledge.", "labels": [], "entities": []}, {"text": "We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines.", "labels": [], "entities": []}, {"text": "Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance , which is 89%.", "labels": [], "entities": [{"text": "BERT-large", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9974097609519958}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995805621147156}]}], "introductionContent": [{"text": "When humans answer questions, they capitalize on their commonsense and background knowledge about spatial relations, causes and effects, scientific facts and social conventions.", "labels": [], "entities": []}, {"text": "For instance, given the question \"Where was Simon when he heard the lawn mower?\", one can infer that the lawnmower is close to Simon, and that it is probably outdoors and situated at street level.", "labels": [], "entities": [{"text": "Simon", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.8906412720680237}]}, {"text": "This type of knowledge seems trivial for humans, but is still out of the reach of current natural language understanding (NLU) systems.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 90, "end_pos": 126, "type": "TASK", "confidence": 0.8143118520577749}]}, {"text": "* The authors contributed equally and three target concepts (dashed) are sampled from CONCEPT-NET (b) Crowd-workers generate three questions, each having one of the target concepts for its answer (), while the other two targets are not ().", "labels": [], "entities": [{"text": "CONCEPT-NET", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.9162473082542419}]}, {"text": "Then, for each question, workers choose an additional distractor from CONCEPTNET (in italics), and author one themselves (in bold).", "labels": [], "entities": [{"text": "CONCEPTNET", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8174690008163452}]}, {"text": "Work on Question Answering (QA) has mostly focused on answering factoid questions, where the answer can be found in a given context with little need for commonsense knowledge ().", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.9043893218040466}]}, {"text": "Small benchmarks such as the Winograd Scheme Challenge (Levesque, 2011) and COPA), targeted commonsense more directly, but have been difficult to collect at scale. has been quickly realized that models trained on large amounts of unlabeled data capture well this type of information and performance on SWAG is already at human level.", "labels": [], "entities": [{"text": "Winograd Scheme Challenge (Levesque, 2011)", "start_pos": 29, "end_pos": 71, "type": "DATASET", "confidence": 0.8709111884236336}, {"text": "COPA", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.5284121632575989}]}, {"text": "VCR () is another very recent attempt that focuses on the visual aspects of commonsense.", "labels": [], "entities": [{"text": "VCR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8215599060058594}]}, {"text": "Such new attempts highlight the breadth of commonsense phenomena, and make it evident that research on commonsense has only scratched the surface.", "labels": [], "entities": []}, {"text": "Thus, there is need for datasets and models that will further our understanding of what is captured by current NLU models, and what are the main lacunae.", "labels": [], "entities": []}, {"text": "In this work, we present COMMONSENSEQA, anew dataset focusing on commonsense question answering, based on knowledge encoded in CONCEPTNET ().", "labels": [], "entities": [{"text": "commonsense question answering", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.7951589028040568}]}, {"text": "We propose a method for generating commonsense questions at scale by asking crowd workers to author questions that describe the relation between concepts from CONCEPTNET (.", "labels": [], "entities": []}, {"text": "A crowd worker observes a source concept ('River' in) and three target concepts (') that are all related by the same CONCEPT-NET relation (AtLocation).", "labels": [], "entities": []}, {"text": "The worker then authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not.", "labels": [], "entities": []}, {"text": "This primes the workers to add commonsense knowledge to the question, that separates the target concept from the distractors.", "labels": [], "entities": []}, {"text": "Finally, for each question, the worker chooses one additional distractor from CONCEPTNET, and authors another distractor manually.", "labels": [], "entities": []}, {"text": "Thus, in total, five candidate answers accompany each question.", "labels": [], "entities": []}, {"text": "Because questions are generated freely by workers, they often require background knowledge that is trivial to humans but is seldom explicitly reported on the web due to reporting bias).", "labels": [], "entities": []}, {"text": "Thus, questions in COMMONSENSEQA have a different nature compared to prior QA benchmarks, where questions are authored given an input text.", "labels": [], "entities": []}, {"text": "Using our method, we collected 12,247 commonsense questions.", "labels": [], "entities": []}, {"text": "We present an analysis that illustrates the uniqueness of the gathered questions compared to prior work, and the types of commonsense skills that are required for tackling it.", "labels": [], "entities": []}, {"text": "We extensively evaluate models on COMMON-SENSEQA, experimenting with pre-trained models, fine-tuned models, and reading comprehension (RC) models that utilize web snippets extracted from Google search on top of the question itself.", "labels": [], "entities": []}, {"text": "We find that fine-tuning BERT-LARGE () on COMMONSENSEQA obtains the best performance, reaching an accuracy of 55.9%.", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9965567588806152}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.999419093132019}]}, {"text": "This is substantially lower than human performance, which is 88.9%.", "labels": [], "entities": []}, {"text": "To summarize, our contributions are:", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to develop a method for generating questions that can be easily answered by humans without context, and require commonsense knowledge.", "labels": [], "entities": []}, {"text": "We generate multiple-choice questions in a process that comprises the following steps.", "labels": [], "entities": []}, {"text": "1. We extract subgraphs from CONCEPTNET,  each with one source concept and three target concepts.", "labels": [], "entities": []}, {"text": "2. We ask crowdsourcing workers to author three questions per subgraph (one per target concept), to add two additional distractors per question, and to verify questions' quality.", "labels": [], "entities": []}, {"text": "3. We add textual context to each question by querying a search engine and retrieving web snippets.", "labels": [], "entities": []}, {"text": "The entire data generation process is summarized in.", "labels": [], "entities": [{"text": "data generation", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.767052948474884}]}, {"text": "We now elaborate on each of the steps: where the nodes C represent natural language concepts, and edges R represent commonsense relations.", "labels": [], "entities": []}, {"text": "Triplets (c 1 , r, c 2 ) carry commonsense knowledge such as '(gambler, CapableOf, lose money)'.", "labels": [], "entities": []}, {"text": "CONCEPTNET contains 32 million triplets.", "labels": [], "entities": [{"text": "CONCEPTNET", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9347771406173706}]}, {"text": "To select a subset of triplets for crowdsourcing we take the following steps: 1.", "labels": [], "entities": []}, {"text": "We filter triplets with general relations (e.g., RelatedTo) or relations that are already well-explored in NLP (e.g., IsA).", "labels": [], "entities": []}, {"text": "In total we use 22 relations.", "labels": [], "entities": []}, {"text": "2. We filter triplets where one of the concepts is more than four words or not in English.", "labels": [], "entities": []}, {"text": "3. We filter triplets where the edit distance between c 1 and c 2 is too low.", "labels": [], "entities": []}, {"text": "This results in a set of 236,208 triplets (q, r, a), where we call the first concept the question concept and the second concept the answer concept.", "labels": [], "entities": []}, {"text": "We aim to generate questions that contain the question concept and where the answer is the answer concept.", "labels": [], "entities": []}, {"text": "To create multiple-choice questions we need to choose distractors for each question.", "labels": [], "entities": []}, {"text": "Sampling distractors at random from CONCEPT-NET is a bad solution, as such distractors are easy to eliminate using simple surface clues.", "labels": [], "entities": [{"text": "Sampling distractors at random", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9073111712932587}]}, {"text": "To remedy this, we propose to create question sets: for each question concept q and relation r we group three different triplets {(q, r, a 1 ), (q, r, a 2 ), (q, r, a 3 )} (see).", "labels": [], "entities": []}, {"text": "This generates three answer concepts that are semantically similar and have a similar relation to the question concept q.", "labels": [], "entities": []}, {"text": "This primes crowd workers to formulate questions that require background knowledge about the concepts in order to answer the question.", "labels": [], "entities": []}, {"text": "The above procedure generates approximately 130,000 triplets (43,000 question sets), for which we can potentially generate questions.", "labels": [], "entities": []}, {"text": "Crowdsourcing questions We used Amazon Mechanical Turk (AMT) workers to generate and validate commonsense questions.", "labels": [], "entities": []}, {"text": "AMT workers saw, for every question set, the question concept and three answer concepts.", "labels": [], "entities": [{"text": "AMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7727050185203552}]}, {"text": "They were asked to formulate three questions, where all questions contain the question concept.", "labels": [], "entities": []}, {"text": "Each question should have as an answer one of the answer concepts, but not the other two.", "labels": [], "entities": []}, {"text": "To discourage workers from providing simple surface clues for the answer, they were instructed to avoid using words that have a strong relation to the answer concept, for example, not to use the word 'open' when the answer is 'door'.", "labels": [], "entities": []}, {"text": "Formulating questions for our task is nontrivial.", "labels": [], "entities": []}, {"text": "Thus, we only accept annotators for which at least 75% of the questions they formulate pass the verification process described below.", "labels": [], "entities": []}, {"text": "Adding additional distractors To make the task more difficult, we ask crowd-workers to add two additional incorrect answers to each formulated question.", "labels": [], "entities": []}, {"text": "One distractor is selected from a set of answer concepts with the same relation to the question concept in CONCEPTNET, in red).", "labels": [], "entities": [{"text": "CONCEPTNET", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.8668999075889587}]}, {"text": "The second distractor is formulated manually by the workers themselves, in purple).", "labels": [], "entities": []}, {"text": "Workers were encouraged to formulate a distractor that would seem plausible or related to the question but easy for humans to dismiss as incorrect.", "labels": [], "entities": []}, {"text": "In total, each formulated question is accompanied with five candidate answers, including one  CONCEPTNET concepts and relations COM-MONSENSEQA builds on CONCEPTNET, which contains concepts such as dog, house, or rowboat, connected by relations such as Causes, CapableOf, or Antonym.", "labels": [], "entities": []}, {"text": "The top-5 question concepts in COMMONSENSEQA are 'Person' (3.1%), 'People' (2.0%), 'Human' (0.7%), 'Water' (0.5%) and 'Cat' (0.5%).", "labels": [], "entities": [{"text": "COMMONSENSEQA", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.7591431736946106}]}, {"text": "In addition, we present the main relations along with the percentage of questions generated from them in  Experimental Setup We split the data into a training/development/test set with an 80/10/10 split.", "labels": [], "entities": []}, {"text": "We perform two types of splits: (a) random split -where questions are split uniformly at random, and (b) question concept split -where each of the three sets have disjoint question concepts.", "labels": [], "entities": [{"text": "question concept split", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.5773054858048757}]}, {"text": "We empirically find (see below) that a random split is harder for models that learn from COMMONSENSEQA, because the same question concept appears in the training set and development/test set with different answer concepts, and The original weights and code released by Google maybe found here: https://github.com/google-research/bert networks that memorize might fail in such a scenario.", "labels": [], "entities": []}, {"text": "Since the random split is harder, we consider it the primary split of COMMONSENSEQA.", "labels": [], "entities": [{"text": "COMMONSENSEQA", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.855861485004425}]}, {"text": "We evaluate all models on the test set using accuracy (proportion of examples for which prediction is correct), and tune hyper-parameters for all trained models on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992827773094177}]}, {"text": "To understand the difficulty of the task, we add a SANITY mode, where we replace the hard distractors (that share a relation with the question concept and one formulated by a worker) with random CONCEPT-NET distractors.", "labels": [], "entities": [{"text": "SANITY", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.8821836113929749}]}, {"text": "We expect a reasonable baseline to perform much better in this mode.", "labels": [], "entities": []}, {"text": "For pre-trained word embeddings we consider 300d GloVe embeddings () and 300d Numberbatch CONCEPTNET node embeddings (, which are kept fixed at training time.", "labels": [], "entities": [{"text": "Numberbatch CONCEPTNET node embeddings", "start_pos": 78, "end_pos": 116, "type": "DATASET", "confidence": 0.8147067129611969}]}, {"text": "We also combine ESIM with 1024d ELMo contextual representations, which are also fixed during training.", "labels": [], "entities": []}, {"text": "Human Evaluation To test human accuracy, we created a separate task for which we did not use a qualification test, nor used AMT master workers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9949507713317871}]}, {"text": "We sampled 100 random questions and for each question gathered answers from five workers that were not involved in question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7117553353309631}]}, {"text": "Humans obtain 88.9% accuracy, taking a majority vote for each question.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9812544584274292}]}, {"text": "Results presents test set results for all models and setups.", "labels": [], "entities": []}, {"text": "The best baselines are BERT-LARGE and GPT with an accuracy of 55.9% and 45.5%, respectively, on the random split (63.6% and 55.5%, respectively, on the question concept split).", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9991870522499084}, {"text": "GPT", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8357027769088745}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992924928665161}]}, {"text": "This is well below human accuracy, demonstrating that the benchmark is much easier for humans.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9927219748497009}]}, {"text": "Nevertheless, this result is much higher than random (20%), showing the ability of language models to store large amounts of information related to commonsense knowledge.", "labels": [], "entities": []}, {"text": "The top part of describes untrained models.", "labels": [], "entities": []}, {"text": "We observe that performance is higher than random, but still quite low.", "labels": [], "entities": []}, {"text": "The middle part describes models that were trained on COMMON-SENSEQA, where BERT-LARGE obtains best performance, as mentioned above.", "labels": [], "entities": [{"text": "COMMON-SENSEQA", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.7210776805877686}, {"text": "BERT-LARGE", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9982835054397583}]}, {"text": "ESIM models follow BERT-LARGE and GPT, and obtain much lower performance.", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9971887469291687}, {"text": "GPT", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7309810519218445}]}, {"text": "We note that ELMo representations did not improve performance compared to GloVe embeddings, possibly because we were un-   able to improve performance by back-propagating into the representations themselves (as we do in BERT-LARGE and GPT).", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 220, "end_pos": 230, "type": "METRIC", "confidence": 0.9370301961898804}, {"text": "GPT", "start_pos": 235, "end_pos": 238, "type": "DATASET", "confidence": 0.9134395718574524}]}, {"text": "The bottom part shows results for BIDAF++ that uses web snippets as context.", "labels": [], "entities": [{"text": "BIDAF++", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.730156272649765}]}, {"text": "We observe that using snippets does not lead to high performance, hinting that they do not carry a lot of useful information.", "labels": [], "entities": []}, {"text": "Performance on the random split is five points lower than the question concept split on average across all trained models.", "labels": [], "entities": []}, {"text": "We hypothesize that this is because having questions in the development/test set that share a question concept with the training set, but have a different answer, creates difficulty for networks that memorize the relation between a question concept and an answer.", "labels": [], "entities": []}, {"text": "Lastly, all SANITY models that were trained on COMMONSENSEQA achieve very high performance (92% for BERT-LARGE), showing that selecting difficult distractors is crucial.", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9950436353683472}]}, {"text": "Baseline analysis To understand the performance of BERT-LARGE, we analyzed 100 examples from the development set).", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.932820737361908}]}, {"text": "We labeled examples with categories (possibly more than one per example) and then computed the average accuracy of the model for each category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.995550274848938}]}, {"text": "We found that the model does well (77.7% accuracy) on examples where surface clues hint to the correct answer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9983565211296082}]}, {"text": "Examples that involve negation or understanding antonyms have lower accuracy (42.8%), similarly to examples that require factoid knowledge (38.4%).", "labels": [], "entities": [{"text": "negation", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.965523898601532}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.998955488204956}]}, {"text": "Accuracy is particularly low in questions where the correct answer has finer granularity compared to one of the distractors (35.4%), and in cases where the correct answer needs to meet a conjunction of conditions, and the distractor meets only one of them (23.8%).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9958567023277283}]}, {"text": "Learning Curves To extrapolate how current models might perform with more data, we evaluated BERT-large on the development set, training with varying amounts of data.", "labels": [], "entities": [{"text": "BERT-large", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9972570538520813}]}, {"text": "The resulting learning curves are plotted in.", "labels": [], "entities": []}, {"text": "For each training set size, hyper-parameters were identical to section 5, except the number of epochs was varied to keep the number of mini-batches during training constant.", "labels": [], "entities": [{"text": "hyper-parameters", "start_pos": 28, "end_pos": 44, "type": "METRIC", "confidence": 0.9757329225540161}]}, {"text": "To deal with learning instabilities, each data point is the best of 3 runs.", "labels": [], "entities": []}, {"text": "We observe that the accuracy of BERT-LARGE is expected to be roughly 75% assuming 100k examples, still sub- stantially lower than human performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997403025627136}, {"text": "BERT-LARGE", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9946730732917786}]}], "tableCaptions": [{"text": " Table 1: Key statistics for COMMONSENSEQA", "labels": [], "entities": [{"text": "COMMONSENSEQA", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.46266287565231323}]}, {"text": " Table 5: Test set accuracy for all models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.973442554473877}]}, {"text": " Table 6: BERT-LARGE baseline analysis. For each category we provide two examples, the correct answer, one  distractor, model accuracy and frequency in the dataset. The predicted answer is in bold.", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9817520380020142}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9809963703155518}]}]}