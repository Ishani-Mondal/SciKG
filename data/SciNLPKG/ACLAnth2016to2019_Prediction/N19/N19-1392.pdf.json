{"title": [{"text": "Polyglot Contextual Representations Improve Crosslingual Transfer", "labels": [], "entities": [{"text": "Contextual Representations Improve Crosslingual Transfer", "start_pos": 9, "end_pos": 65, "type": "TASK", "confidence": 0.7692157208919526}]}], "abstractContent": [{"text": "We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages.", "labels": [], "entities": []}, {"text": "Our method combines the advantages of contex-tual word representations with those of multilingual representation learning.", "labels": [], "entities": []}, {"text": "We produce language models from dissimilar language pairs (English/Arabic and English/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and non-contextual variants.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7835009694099426}, {"text": "semantic role labeling", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.6506357987721761}, {"text": "named entity recognition", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.6155863106250763}]}, {"text": "Our results provide further evidence for the benefits of polyglot learning , in which representations are shared across multiple languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "State-of-the-art methods for crosslingual transfer make use of multilingual word embeddings, and much research has explored methods that align vector spaces for words in different languages).", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.8017663657665253}]}, {"text": "On the other hand, contextual word representations (CWR) extracted from language models (LMs) have advanced the state of the art beyond what was achieved with word type representations on many monolingual NLP tasks ( . Thus, the question arises: can contextual word representations benefit from multilinguality?", "labels": [], "entities": []}, {"text": "We introduce a method to produce multilingual CWR by training a single \"polyglot\" language model on text in multiple languages.", "labels": [], "entities": []}, {"text": "As our work is a multilingual extension of ELMo (Peters et al., 2018), we call it Rosita (after a bilingual character from Sesame Street).", "labels": [], "entities": []}, {"text": "Our hypothesis is that, although each language is unique, different languages manifest similar characteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single model with data from multiple languages.", "labels": [], "entities": []}, {"text": "Previous work has shown this to be true to some degree in the context of syntactic dependency parsing ( , semantic role labeling (, named entity recognition (, and language modeling for phonetic sequences and for speech recognition (.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.733319083849589}, {"text": "semantic role labeling", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6549559533596039}, {"text": "named entity recognition", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.6595944762229919}, {"text": "speech recognition", "start_pos": 213, "end_pos": 231, "type": "TASK", "confidence": 0.7537981271743774}]}, {"text": "Recently, de showed that parameter sharing between languages can improve performance in dependency parsing, but the effect is variable, depending on the language pair and the parameter sharing strategy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8480010032653809}]}, {"text": "Other recent work also reported that concatenating data from different languages can hurt performance in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8303475081920624}]}, {"text": "These mixed results suggest that while crosslingual transfer in neural network models is a promising direction, the best blend of polyglot and language-specific elements may depend on the task and architecture.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7229649275541306}]}, {"text": "However, we find overall contextual representations from polyglot language models succeed in a range of settings, even where multilingual word type embeddings do not, and area useful technique for crosslingual transfer.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 197, "end_pos": 218, "type": "TASK", "confidence": 0.856896311044693}]}, {"text": "We explore crosslingual transfer between highly dissimilar languages (English\u2192Chinese and English\u2192Arabic) for three core tasks: Universal Dependency (UD) parsing, semantic role labeling (SRL), and named entity recognition (NER).", "labels": [], "entities": [{"text": "Universal Dependency (UD) parsing", "start_pos": 128, "end_pos": 161, "type": "TASK", "confidence": 0.6128720094760259}, {"text": "semantic role labeling (SRL)", "start_pos": 163, "end_pos": 191, "type": "TASK", "confidence": 0.7684390395879745}, {"text": "named entity recognition (NER)", "start_pos": 197, "end_pos": 227, "type": "TASK", "confidence": 0.7641409983237585}]}, {"text": "We provide some of the first work using polyglot LMs to produce contextual representations, 1 and the first analysis comparing them to monolingual LMs for this purpose.", "labels": [], "entities": []}, {"text": "We also introduce an LM variant which takes multilingual word embedding input as well as character input, and explore its applicability for producing contextual word representations.", "labels": [], "entities": []}, {"text": "Our experiments focus on comparisons in three dimensions: monolingual vs. polyglot representations, contextual vs. word type embeddings, and, within the contextual representation paradigm, purely character-based language models vs. ones that include word-level input.", "labels": [], "entities": []}, {"text": "Previous work has shown that contextual representations offer a significant advantage over traditional word embeddings (word type representations).", "labels": [], "entities": []}, {"text": "In this work, we show that, on these tasks, polyglot character-based language models can provide benefits on top of those offered by contextualization.", "labels": [], "entities": []}, {"text": "Specifically, even when crosslingual transfer with word type embeddings hurts target language performance relative to monolingual models, polyglot contextual representations can improve target language performance relative to monolingual versions, suggesting that polyglot language models tie dissimilar languages in an effective way.", "labels": [], "entities": []}, {"text": "In this paper, we use the following terms: crosslingual transfer and polyglot learning.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7957432270050049}]}, {"text": "While crosslingual transfer is often used in situations where target data are absent or scarce, we use it broadly to mean any method which uses one or more source languages to help process another target language.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7545648217201233}]}, {"text": "We also draw a sharp distinction between multilingual and polyglot models.", "labels": [], "entities": []}, {"text": "Multilingual learning can happen independently for different languages, but a polyglot solution provides a single model for multiple languages, e.g., by parameter sharing between languages in networks during training.", "labels": [], "entities": []}], "datasetContent": [{"text": "All of our task models (UD, SRL, and NER) are implemented in AllenNLP, version 0.7.2 ( . We generally follow the default hyperparameters and training schemes provided in the AllenNLP library regardless of language.", "labels": [], "entities": [{"text": "AllenNLP", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9732828140258789}, {"text": "AllenNLP library", "start_pos": 174, "end_pos": 190, "type": "DATASET", "confidence": 0.9755253791809082}]}, {"text": "See appendix for the complete list of our hyperparameters.", "labels": [], "entities": []}, {"text": "For each task, we experiment with five types of word representations: in addition to the three language model types (MONOCHAR, ROSI-TACHAR, and ROSITAWORD) described above, we show results for the task models trained with monolingual and polyglot non-contextual word embeddings.", "labels": [], "entities": [{"text": "MONOCHAR", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.7463957071304321}, {"text": "ROSI-TACHAR", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9088106751441956}, {"text": "ROSITAWORD", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.948868989944458}]}, {"text": "After pretraining, the word representations are fine-tuned to the specific task during task training.", "labels": [], "entities": []}, {"text": "In non-contextual cases, we fine-tune by updating word embeddings directly, while in contextual cases, we only update coefficients fora linear combination of the internal representation layers for efficiency ( . In order to properly evaluate our models' generalization ability, we ensure that sentences in the test data are excluded from the data used to train the language models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LAS for UD parsing, F 1 for SRL, and F 1 for NER, with different input representations. For UD, each  number is an average over five runs with different initialization, with standard deviation. SRL/NER results are from  one run. The \"task lang.\" column indicates whether the UD/SRL/NER model was trained on annotated text in the  target language alone, or a blend of English and the target language data. ROSITAWORD LMs use as word-level  input the same multilingual word vectors as fastText models. The best prior result for Ontonotes Chinese NER is  in Shen et al. (2018); the others are from Pradhan et al. (2013).", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9790495038032532}, {"text": "UD parsing", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.8956786692142487}, {"text": "SRL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9490770101547241}, {"text": "Ontonotes Chinese NER", "start_pos": 536, "end_pos": 557, "type": "TASK", "confidence": 0.6312816739082336}]}, {"text": " Table 2: LAS (F 1 ) comparison to the winning systems  for each language in the CoNLL 2018 shared task for  UD. We use predicted POS and the segmentation of the  winning system for that language. The ROSITACHAR  LM variant was selected based on development perfor- mance in the gold-segmentation condition.", "labels": [], "entities": [{"text": "LAS (F 1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8636602163314819}, {"text": "CoNLL 2018 shared task for  UD", "start_pos": 81, "end_pos": 111, "type": "DATASET", "confidence": 0.7963228722413381}, {"text": "POS", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9814227819442749}]}, {"text": " Table 3: Language Model Hyperparameters.", "labels": [], "entities": []}, {"text": " Table 4: UD Parsing Hyperparameters.", "labels": [], "entities": [{"text": "UD Parsing Hyperparameters", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7872114380200704}]}]}