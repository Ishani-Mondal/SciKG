{"title": [{"text": "GraphIE: A Graph-Based Framework for Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7111967504024506}]}], "abstractContent": [{"text": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8488696575164795}]}, {"text": "Non-local and non-sequential context is, however, a valuable source of information to improve predictions.", "labels": [], "entities": []}, {"text": "In this paper, we introduce GraphIE, a framework that operates over a graph representing abroad set of dependencies between textual units (i.e. words or sentences).", "labels": [], "entities": []}, {"text": "The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions.", "labels": [], "entities": []}, {"text": "Evaluation on three different tasks-namely textual, social media and visual information extraction-shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.", "labels": [], "entities": [{"text": "information extraction-shows", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.8349729776382446}, {"text": "sequence tagging", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.633533775806427}]}], "introductionContent": [{"text": "Most modern Information Extraction (IE) systems are implemented as sequential taggers.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8482865512371063}]}, {"text": "While such models effectively capture relations in the local context, they have limited capability of exploiting non-local and non-sequential dependencies.", "labels": [], "entities": []}, {"text": "In many applications, however, such dependencies can greatly reduce tagging ambiguity, thereby improving overall extraction performance.", "labels": [], "entities": []}, {"text": "For instance, when extracting entities from a document, various types of non-local contextual information such as co-references and identical mentions may provide valuable cues.", "labels": [], "entities": []}, {"text": "See for example, in which the non-local relations are crucial to discriminate the entity type of the second mention of Washington (i.e. PERSON or LOCATION).", "labels": [], "entities": []}, {"text": "Most of the prior work looking at the non-local dependencies incorporates them by constraining the output space in a structured prediction framework (.", "labels": [], "entities": []}, {"text": "Such approaches, however, mostly overlook the richer set of structural relations in the input space.", "labels": [], "entities": []}, {"text": "With reference to the example in, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns).", "labels": [], "entities": []}, {"text": "In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it . Designing effective features is however challenging, arbitrary and time consuming, especially when the underlying structure is complex.", "labels": [], "entities": []}, {"text": "Moreover, these approaches have limited capacity of capturing node interactions informed by the graph structure.", "labels": [], "entities": []}, {"text": "In this paper, we propose GraphIE, a framework that improves predictions by automatically learning the interactions between local and non-local dependencies in the input space.", "labels": [], "entities": []}, {"text": "Our approach integrates a graph module with the encoder-decoder architecture for sequence tagging.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.6860571354627609}]}, {"text": "The algorithm operates over a graph, where nodes correspond to textual units (i.e. words or sentences) and edges describe their relations.", "labels": [], "entities": []}, {"text": "At the core of our model, a recurrent neural network sequentially encodes local contextual representations and then the graph module iteratively propagates information between neighboring nodes using graph convolutions (.", "labels": [], "entities": []}, {"text": "The learned representations are finally projected back to a recurrent decoder to support tagging at the word level.", "labels": [], "entities": []}, {"text": "We evaluate GraphIE on three IE tasks, namely textual, social media, and visual () information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7736573219299316}]}, {"text": "For each task, we provide in input a simple task-specific graph, which defines the data structure without access to any major processing or external resources.", "labels": [], "entities": []}, {"text": "Our model is expected to learn from the relevant dependencies to identify and extract the appropriate information.", "labels": [], "entities": []}, {"text": "Experimental results on multiple benchmark datasets show that GraphIE consistently outperforms a strong and commonly adopted sequential model (SeqIE, i.e. a bidirectional long-short term memory (BiLSTM) followed by a conditional random fields (CRF) module).", "labels": [], "entities": []}, {"text": "Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction ().", "labels": [], "entities": [{"text": "IE task", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.8197460174560547}, {"text": "CONLL03 dataset", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9790662229061127}, {"text": "chemical entity extraction", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.645184338092804}]}, {"text": "In the social media IE task, GraphIE improves over SeqIE by 3.7% in extracting the EDU-CATION attribute from twitter users.", "labels": [], "entities": [{"text": "IE task", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.8285336494445801}]}, {"text": "In visual IE, finally, we outperform the baseline by 1.2%.", "labels": [], "entities": [{"text": "IE", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.6267280578613281}]}], "datasetContent": [{"text": "We evaluate the model on three tasks, including two traditional IE tasks, namely textual information extraction and social media information extraction, and an under-explored task -visual information extraction.", "labels": [], "entities": [{"text": "IE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9554265737533569}, {"text": "textual information extraction", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.6131069362163544}, {"text": "social media information extraction", "start_pos": 116, "end_pos": 151, "type": "TASK", "confidence": 0.6078384146094322}, {"text": "information extraction", "start_pos": 188, "end_pos": 210, "type": "TASK", "confidence": 0.8285056948661804}]}, {"text": "For each of these tasks, we created a simple task-specific graph topology, designed to easily capture the underlying structure of the input data without any major processing.", "labels": [], "entities": []}, {"text": "Table 1 summarizes the three tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the CONLL03 and the CHEMD- NER datasets (Task 1).", "labels": [], "entities": [{"text": "CONLL03", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9638237357139587}, {"text": "CHEMD- NER datasets", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.871488556265831}]}, {"text": " Table 3: Statistics of the EDUCATION and JOB datasets  (Task 2).", "labels": [], "entities": [{"text": "JOB datasets", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8826706409454346}]}, {"text": " Table 4: NER accuracy on the CONLL03 and the  CHEMDNER datasets (Task 1). Scores for our methods  are the average of 5 runs. * indicates statistical signifi- cance of the improvement over SeqIE (p < 0.01).", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8904027342796326}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9896630644798279}, {"text": "CONLL03", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9810118675231934}, {"text": "CHEMDNER datasets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9346903860569}]}, {"text": " Table 5: Extraction accuracy on the EDUCATION and JOB datasets (Task 2). Dictionary is a naive method which  creates a dictionary of entities from the training set and extracts their mentions during testing time. Scores are the  average of 5 runs. * indicates the improvement over SeqIE is statistically significant (Welch's t-test, p < 0.01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9058372974395752}, {"text": "JOB datasets", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.867730051279068}]}, {"text": " Table 6: Extraction accuracy on the AECR dataset  (Task 3). Scores are the average of 5 runs. P. is the  abbreviation for Patient, and R. for Reporter.  \u2020 indi- cates statistical significance of the improvement over  SeqIE (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9625042676925659}, {"text": "AECR dataset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9705361127853394}, {"text": "indi- cates statistical significance", "start_pos": 156, "end_pos": 192, "type": "METRIC", "confidence": 0.8481586456298829}]}]}