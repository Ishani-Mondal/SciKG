{"title": [{"text": "Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7010962665081024}]}], "abstractContent": [{"text": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form.", "labels": [], "entities": []}, {"text": "Using context can help, both for unseen and ambiguous words.", "labels": [], "entities": []}, {"text": "Yet most context-sensitive approaches require full lemma-annotated sentences for training, which maybe scarce or unavailable in low-resource languages.", "labels": [], "entities": []}, {"text": "In addition (as shown here), in a low-resource setting, a lemmatizer can learn more from n labeled examples of distinct words (types) than from n (contigu-ous) labeled tokens, since the latter contain far fewer distinct types.", "labels": [], "entities": []}, {"text": "To combine the efficiency of type-based learning with the benefits of context, we propose away to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from Wikipedia that provide sentence contexts for the unambiguous UniMorph examples.", "labels": [], "entities": []}, {"text": "Despite these being unambiguous examples , the model successfully generalizes from them, leading to improved results (both overall , and especially on unseen words) in comparison to a baseline that does not use context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many lemmatizers work on isolated wordforms.", "labels": [], "entities": []}, {"text": "Lemmatizing in context can improve accuracy on ambiguous and unseen words), but most systems for contextsensitive lemmatization must train on complete sentences labeled with POS and/or morphological tags as well as lemmas, and have only been tested with 20k-300k training tokens ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9930900931358337}]}, {"text": "1 The smallest of these corpora contains 20k tokens of Bengali annotated only with lemmas, which reported took around two person months to create.", "labels": [], "entities": []}, {"text": "Intuitively, though, sentence-annotated data is inefficient for training a lemmatizer, especially in low-resource settings.", "labels": [], "entities": []}, {"text": "Training on (say) 1000 word types will provide far more information about a language's morphology than training on 1000 contiguous tokens, where fewer types are represented.", "labels": [], "entities": []}, {"text": "As noted above, sentence data can help with ambiguous and unseen words, but we show here that when data is scarce, this effect is small relative to the benefit of seeing more word types.", "labels": [], "entities": []}, {"text": "Motivated by this result, we propose a training data augmentation method that combines the efficiency of type-based learning and the expressive power of a context-sensitive model.", "labels": [], "entities": []}, {"text": "We use, a state-of-theart lemmatizer that learns from lemma-annotated words in their N -character contexts.", "labels": [], "entities": []}, {"text": "No predictions about surrounding words are used, so fully annotated training sentences are not needed.", "labels": [], "entities": []}, {"text": "We exploit this fact by combining two sources of training data: 1k lemma-annotated types (with contexts) from the Universal Dependency Treebank (UDT) v2.2 4 (, plus examples obtained by finding unambiguous word-lemma pairs in inflection tables from the Universal Morphology (UM) project and collecting sentence contexts for them from Wikipedia.", "labels": [], "entities": [{"text": "Universal Dependency Treebank (UDT) v2.2", "start_pos": 114, "end_pos": 154, "type": "DATASET", "confidence": 0.8137615748814174}]}, {"text": "Although these examples are noisy and biased, we show that they improve lemmatization accuracy in experiments on 10 languages, and that the use of context helps, both overall and especially on unseen words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9830291867256165}]}, {"text": "inspired by the re-inflection model of, which won the 2016 SIGMOR-PHON shared task (.", "labels": [], "entities": [{"text": "SIGMOR-PHON shared task", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.5900471210479736}]}, {"text": "It is built using the Nematus machine translation toolkit, which uses the architecture of: a 2-layer bidirectional GRU encoder and a 2-layer decoder with a conditional GRU) in the first layer and a GRU in the second layer.", "labels": [], "entities": [{"text": "Nematus machine translation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7266793847084045}]}, {"text": "Lematus takes as input a character sequence representing the wordform in its N -character context, and outputs the characters of the lemma.", "labels": [], "entities": []}, {"text": "Special input symbols are used to represent the left and right boundary of the target wordform (<lc>, <rc>) and other word boundaries (<s>).", "labels": [], "entities": []}, {"text": "For example, if N = 15, the system trained on Latvian would be expected to produce the characters of the lemma cel\u00b8\u0161cel\u00b8\u0161 (meaning road) given input such as:", "labels": [], "entities": []}], "datasetContent": [{"text": "Baselines and Training Parameters We use four baselines: (1) Lemming 8 () is a context-sensitive system that uses log-linear models to jointly tag and lemmatize the data, and is trained on sentences annotated with both lemmas and POS tags.", "labels": [], "entities": []}, {"text": "The hard monotonic attention model (HMAM) 9) is a neural sequence-tosequence model with a hard attention mechanism that advances through the sequence monotonically.", "labels": [], "entities": []}, {"text": "It is trained on word-lemma pairs (without context) Recent efforts to unify the two resources have mostly focused on validating dataset schema (, leaving conflicts in word lemmas unresolved.", "labels": [], "entities": []}, {"text": "We estimated (by counting types that are unambiguous in each dataset but have different lemmas across them) that annotation inconsistencies affect up to 1% of types in the languages we used.", "labels": [], "entities": []}, {"text": "8 http://cistern.cis.lmu.de/lemming 9 https://github.com/ZurichNLP/ coling2018-neural-transition-basedmorphology with character-level alignments learned in a preprocessing step using an alignment model, and it has proved to be competitive in low resource scenarios.", "labels": [], "entities": [{"text": "ZurichNLP", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9226414561271667}]}, {"text": "(3) Our naive Baseline outputs the most frequent lemma (or one lemma at random from the options that are equally frequent) for words observed in training.", "labels": [], "entities": []}, {"text": "For unseen words it outputs the wordform itself.", "labels": [], "entities": []}, {"text": "(4) We also try a baseline data augmentation approach (AE Aug Baseline) inspired by and, who showed that adding training examples where the network simply learns to auto-encode corpus words can improve morphological inflection results in low-resource settings.", "labels": [], "entities": [{"text": "AE Aug Baseline", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9334947466850281}]}, {"text": "The AE Aug Baseline is a variant of Lematus 0-ch which augments the UDT lemmatization examples by auto-encoding the inflected forms of the UM examples (i.e., it just treats them as corpus words).", "labels": [], "entities": [{"text": "AE Aug Baseline", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8125757575035095}]}, {"text": "Comparing AE Aug Baseline to Lematus 0-ch augmented with UM lemma-inflection examples tells us whether using the UM lemma information helps more than simply auto-encoding more inflected examples.", "labels": [], "entities": [{"text": "AE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8908932209014893}]}, {"text": "To train the models we use the default settings for Lemming and the suggested lemmatization parameters for HMAM.", "labels": [], "entities": [{"text": "HMAM", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.8672763109207153}]}, {"text": "We mainly follow the hyperparameters used by for Lematus; details are in Appendix B. Languages and Training Data We conduct preliminary experiments on five development languages: Estonian, Finnish, Latvian, Polish, and Russian.", "labels": [], "entities": []}, {"text": "In our final experiments we also add Bulgarian, Czech, Romanian, Swedish and Turkish.", "labels": [], "entities": []}, {"text": "We vary the amount and type of training data (types vs. tokens, UDT only, UM only, or UDT plus up to 10k UM examples), as described in Section 4.", "labels": [], "entities": []}, {"text": "To obtain N UM-based training examples, we select the first N unambiguous UM types (with their sentence contexts) from shuffled Wikipedia sentences.", "labels": [], "entities": []}, {"text": "For experiments with j > 1 examples per type, we first find all UM types with at least j sentence contexts in Wikipedia and then choose the N distinct types and their j contexts uniformly at random.", "labels": [], "entities": []}, {"text": "Evaluation To evaluate models' ability to lemmatize wordforms in their sentence context we follow and use the full UDT development and test sets.", "labels": [], "entities": [{"text": "UDT development and test sets", "start_pos": 115, "end_pos": 144, "type": "DATASET", "confidence": 0.7498453617095947}]}, {"text": "Unlike Bergmanis and Goldwater (2018) who reported token level lemmatization exact match accuracy, we report type-level micro averaged lemmatization ex-  act match accuracy.", "labels": [], "entities": [{"text": "token level lemmatization exact match accuracy", "start_pos": 51, "end_pos": 97, "type": "METRIC", "confidence": 0.5131189773480097}, {"text": "type-level micro averaged lemmatization ex-  act match accuracy", "start_pos": 109, "end_pos": 172, "type": "METRIC", "confidence": 0.5219963325394524}]}, {"text": "This measure better reflects improvements on unseen words, which tend to be rare but are more important (since a most-frequentlemma baseline does very well on seen words, as shown by).", "labels": [], "entities": []}, {"text": "We separately report performance on unseen and ambiguous tokens.", "labels": [], "entities": []}, {"text": "For a fair comparison across scenarios with different training sets, we count as unseen only words that are not ambiguous and are absent from all training sets/scenarios introduced in Section 4.", "labels": [], "entities": []}, {"text": "Due to the small training sets, between 70-90% of dev set types are classed as unseen in each language.", "labels": [], "entities": []}, {"text": "We define a type as ambiguous if the empirical entropy over its lemmas is greater than 0.1 in the full original UDT training splits.", "labels": [], "entities": [{"text": "UDT training splits", "start_pos": 112, "end_pos": 131, "type": "DATASET", "confidence": 0.8555009563763937}]}, {"text": "According to this measure, only 1.2-5.3% of dev set types are classed as ambiguous in each language.", "labels": [], "entities": []}, {"text": "Significance Testing All systems are trained and tested on ten languages.", "labels": [], "entities": []}, {"text": "To test for statistically significant differences between the results of two systems we use a Monte Carlo method: for each set of results (i.e. a set of 10 numerical values) we generate 10000 random samples, where each sample swaps the results of the two systems for each language with a probability of 0.5.", "labels": [], "entities": []}, {"text": "We then obtain a p-value as the proportion of samples for which the difference on average was at least as large as the difference observed in our experiments.", "labels": [], "entities": []}, {"text": "1k tokens vs. first 1k distinct types of the UDT training sets.", "labels": [], "entities": [{"text": "UDT training sets", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.8122634092966715}]}, {"text": "shows that if only 1k examples are available, using types is clearly better for all systems.", "labels": [], "entities": []}, {"text": "Although Lematus does relatively poorly on the token data, it benefits the most from switching to types, putting it on par with HMAM and suggesting is it likely to benefit more from additional type data.", "labels": [], "entities": []}, {"text": "Lemming requires token-based data, but does worse than HMAM (a context-free method) in the token-based setting, and we also see no benefit from context in comparing Lematus 20-ch vs Lematus 0-ch.", "labels": [], "entities": []}, {"text": "So overall, in this very low-resource scenario with no data augmentation, context does not appear to help.", "labels": [], "entities": []}, {"text": "Using UM + Wikipedia Only We now try training only on UM + Wikipedia examples, rather than examples from UDT.", "labels": [], "entities": [{"text": "UM + Wikipedia", "start_pos": 6, "end_pos": 20, "type": "DATASET", "confidence": 0.8791396617889404}, {"text": "UM + Wikipedia examples", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.8870608955621719}, {"text": "UDT", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.9505876302719116}]}, {"text": "We use 1k, 2k or 5k unambiguous types from UM with a single example context from Wikipedia for each.", "labels": [], "entities": []}, {"text": "With 5k types we also try adding more example contexts (2, 3, or 5 examples for each type).", "labels": [], "entities": []}, {"text": "presents the results (for unseen words only).", "labels": [], "entities": []}, {"text": "As with the UDT experiments, there is little difference between Lematus 20-ch and Lematus 0-ch in the smallest data setting.", "labels": [], "entities": []}, {"text": "However, when the number of training types increases to 5k, the benefits of context begin to show, with Lematus 20-ch yielding a 1.6% statistically significant (p < 0.001) improvement over Lematus 0-ch.", "labels": [], "entities": []}, {"text": "The results for increasing the number of examples per type are numerically higher than the one-example case, but the differences are not statistically significant.", "labels": [], "entities": []}, {"text": "It is worth noting that the accuracy even with 5k UM types is considerably lower than the accuracy of the model trained on only 1k UDT types (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996371269226074}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994263648986816}]}, {"text": "We believe this discrepancy is due to the issues of biased/incomplete data noted above.", "labels": [], "entities": []}, {"text": "For example, we analyzed the Latvian data and found that the available tables for nouns, verbs, and adjectives give rise to 78 paradigm slots.", "labels": [], "entities": [{"text": "Latvian data", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7615084946155548}]}, {"text": "The 17 POS tags in UDT give rise to about 10 times as many paradigm slots, although only 448 are present in the unseen words of the dev set.", "labels": [], "entities": [{"text": "UDT", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.818692147731781}]}, {"text": "Of these, 197 are represented amongst the 1k UDT training types, whereas only 25 are included in the 1k UM training types.", "labels": [], "entities": []}, {"text": "As a result, about 72% of the unseen types of dev set have no representative of their paradigm slot in 1k types of UM, whereas this figure is only 17% for the 1k types of UDT.", "labels": [], "entities": []}, {"text": "Data Augmentation Although UM + Wikipedia examples alone are not sufficient to train a good lemmatizer, they might improve a low-resource baseline trained on UDT data.", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6940181702375412}, {"text": "UM + Wikipedia", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.626703292131424}, {"text": "UDT data", "start_pos": 158, "end_pos": 166, "type": "DATASET", "confidence": 0.8832294046878815}]}, {"text": "To see, we augmented the 1k UDT types with 1k, 5k or 10k UM types with contexts from Wikipedia.", "labels": [], "entities": []}, {"text": "summarizes the results, showing that despite the lower quality of the UM + Wikipedia examples, using them improves results of all systems, and more so with more examples.", "labels": [], "entities": [{"text": "UM + Wikipedia examples", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.9241808205842972}]}, {"text": "Improvements are especially strong for unseen types, which constitute more than 70% of types in the dev set.", "labels": [], "entities": []}, {"text": "Furthermore, the benefit of the additional UM examples is above and beyond the effect of auto-encoding (AE Aug Baseline) for all systems in all data scenarios.", "labels": [], "entities": [{"text": "AE Aug Baseline)", "start_pos": 104, "end_pos": 120, "type": "METRIC", "confidence": 0.9663788378238678}]}, {"text": "Considering the two context-free models, HMAM does better on the un-augmented 1k UDT data, but (as predicted by our results above) it benefits less from data augmentation than does Lematus 0-ch, so with added data they are statistically equivalent (p = 0.07 on the test set with 10k UM).", "labels": [], "entities": []}, {"text": "More importantly, Lematus 20-ch begins to outperform the context-free models with as few as 1k UM + Wikipedia examples, and the difference increases with more examples, eventually reaching over 4% better on the test set than the next best model (Lematus 0-ch) when 10k UM + Wikipedia examples are used (p < 0.001) This indicates that the system can learn useful contextual cues even from unambiguous training examples.", "labels": [], "entities": []}, {"text": "Finally, gives a breakdown of Lematus 20-ch dev set accuracy for individual languages, showing that data augmentation helps consistently, although results suggest diminishing returns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8641676902770996}]}, {"text": "Data Augmentation in Medium Resource Setting To examine the extent to which augmented data can help in the medium resource setting of 10k continuous tokens of UDT used in previous work, we follow Bergmanis and Goldwater (2018) and train Lematus 20-ch models for all ten languages using the first 10k tokens of UDT and compare them with models trained on 10k tokens of UDT augmented with 10k UM types.", "labels": [], "entities": [{"text": "Data Augmentation in Medium Resource Setting", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6806263774633408}]}, {"text": "To provide a better comparison of our results, we report both the type and the token level development set accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9150078296661377}]}, {"text": "First  of all, shows that training on 10k continuous tokens of UDT yields a token level accuracy that is about 8% higher than when using the 1k types of UDT augmented with 10k UM types-the best-performing data augmentation systems (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9607003927230835}]}, {"text": "Again, we believe this performance gap is due to the issues with the biased/incomplete data noted above.", "labels": [], "entities": []}, {"text": "For example, we analyzed errors that were unique to the model trained on the Latvian augmented data and found that 41% of the errors were due to wrongly lemmatized words other than nouns, verbs, and adjectives-the three POSs with available inflection tables in UM.", "labels": [], "entities": [{"text": "Latvian augmented data", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.7599497238794962}]}, {"text": "For instance, improperly lemmatized pronouns amounted to 14% of the errors on the Latvian dev set.", "labels": [], "entities": [{"text": "Latvian dev set", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8062166571617126}]}, {"text": "also shows that UM examples with Wikipedia contexts benefit lemmatization not only in the low but also the medium resource setting, yielding statistically significant type and token level accuracy gains over models trained on 10k UDT continuous tokens alone (for both Unseen and All p < 0.001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.832131028175354}]}], "tableCaptions": [{"text": " Table 2: Average type level lemmatization exact match  accuracy on five development languages in type and to- ken based training data scenarios. Colour-scale is com- puted over the whole Ambig. column and over all but  Baseline rows for the other columns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9302191138267517}, {"text": "Colour-scale", "start_pos": 146, "end_pos": 158, "type": "METRIC", "confidence": 0.9711748361587524}, {"text": "Ambig. column", "start_pos": 188, "end_pos": 201, "type": "DATASET", "confidence": 0.967512051264445}]}, {"text": " Table 3: Average lemmatization accuracy for all 10  languages, trained on 1k UDT types (No aug.), or 1k  UDT plus 1k, 5k, or 10k UM types with contexts from  Wikipedia. The numerically highest scores in each  data setting are bold;  *  ,  \u2020 , and  \u2021 indicate statistically  significant improvements over HMAM (Makarov and  Clematide, 2018b), Lematus 0-ch and 20-ch, respec- tively (all p < 0.05; see text for details). Colour-scale  is computed over the whole Ambig. column and over  all but Baseline rows for the other columns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9928033947944641}, {"text": "Ambig. column", "start_pos": 461, "end_pos": 474, "type": "DATASET", "confidence": 0.977354109287262}]}, {"text": " Table 4: Lematus 20-ch average lemmatization type  and token accuracy for all 10 languages, trained on 1k  UDT types, 1k UDT augmented with 10k UM types,  10k UDT continuous tokens, or 10k UDT continuous  tokens augmented with 10k UM types. Unless speci- fied otherwise data consists of distinct types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9824140667915344}]}, {"text": " Table 5: Individual type level lemmatization ac- curacy for all 10 languages on development set,  trained on 1k UDT types (no augmentation) with  contexts from Wikipedia. The numerically highest  scores for each language are bold. For the summary  of results see", "labels": [], "entities": []}, {"text": " Table 6: Individual type level lemmatization ac- curacy for all 10 languages on development set,  trained on 1k UDT types plus 1k UM types with  contexts from Wikipedia. The numerically highest  scores for each language are bold. For the summary  of results see Table 3.", "labels": [], "entities": []}, {"text": " Table 7: Individual type level lemmatization ac- curacy for all 10 languages on development set,  trained on 1k UDT types plus 5k UM types with  contexts from Wikipedia. The numerically highest  scores for each language are bold. For the summary  of results see Table 3.", "labels": [], "entities": []}, {"text": " Table 8: Individual type level lemmatization ac- curacy for all 10 languages on development set,  trained on 1k UDT types plus 10k UM types with  contexts from Wikipedia. The numerically highest  scores for each language are bold. For the summary  of results see Table 3.", "labels": [], "entities": []}, {"text": " Table 9: Individual type and token level lemmatization accuracy for all 10 languages on development set for  Lematus 20-ch models trained on 10k UDT tokens and 10k UDT tokens plus 10k UM types with contexts from  Wikipedia. The numerically highest scores for each language are bold. For the summary of results see", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9919946193695068}]}]}