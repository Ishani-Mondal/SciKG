{"title": [{"text": "Learning Relational Representations by Analogy using Hierarchical Siamese Networks", "labels": [], "entities": [{"text": "Learning Relational Representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.780407577753067}]}], "abstractContent": [{"text": "We address relation extraction as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8665507435798645}]}, {"text": "In our assumption, if two pairs of entities belong to the same relation , then those two pairs are analogous.", "labels": [], "entities": []}, {"text": "Following this idea, we collect a large set of analogous pairs by matching triples in knowledge bases with web-scale corpora through distant supervision.", "labels": [], "entities": []}, {"text": "We leverage this dataset to train a hierarchical siamese network in order to learn entity-entity embeddings which encode rela-tional information through the different linguistic paraphrasing expressing the same relation.", "labels": [], "entities": []}, {"text": "We evaluate our model in a one-shot learning task by showing a promising generalization capability in order to classify unseen relation types, which makes this approach suitable to perform automatic knowledge base population with minimal supervision.", "labels": [], "entities": []}, {"text": "Moreover , the model can be used to generate pre-trained embeddings which provide a valuable signal when integrated into an existing neural-based model by outperforming the state-of-the-art methods on a downstream relation extraction task.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 214, "end_pos": 238, "type": "TASK", "confidence": 0.7796967228253683}]}], "introductionContent": [{"text": "The task of identifying semantic relationships between entities in unstructured textual corpora, namely Relation Extraction (RE), is often a prerequisite for many other natural language understanding tasks, e.g. automatic knowledge base population, question answering, etc.", "labels": [], "entities": [{"text": "identifying semantic relationships between entities in unstructured textual corpora", "start_pos": 12, "end_pos": 95, "type": "TASK", "confidence": 0.737940271695455}, {"text": "Relation Extraction (RE)", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.7650062084197998}, {"text": "natural language understanding", "start_pos": 169, "end_pos": 199, "type": "TASK", "confidence": 0.7327099839846293}, {"text": "question answering", "start_pos": 249, "end_pos": 267, "type": "TASK", "confidence": 0.8963145911693573}]}, {"text": "RE is commonly addressed as a classification task (), where a model is trained to classify relation mentions in text among a predefined set of relation types.", "labels": [], "entities": [{"text": "RE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9705190658569336}]}, {"text": "For instance, given the sentence \"Robert Plant is the singer of the band Led Zeppelin\", an effective RE system might extract the triple memberOf(ROBERT PLANT, LED ZEP-PELIN), where memberOf is a relation label expressed by the linguistic context \"is the singer of the band\".", "labels": [], "entities": [{"text": "RE", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.740965723991394}, {"text": "ROBERT PLANT", "start_pos": 145, "end_pos": 157, "type": "METRIC", "confidence": 0.9449333250522614}, {"text": "LED ZEP-PELIN)", "start_pos": 159, "end_pos": 173, "type": "METRIC", "confidence": 0.8901790380477905}]}, {"text": "Since a given relation can be expressed using different textual patterns surrounding entities, the state-of-the-art RE models which follow this approach need a considerable amount of examples for each relation to reach satisfactory performance.", "labels": [], "entities": [{"text": "RE", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.8763269186019897}]}, {"text": "Distant supervision () instead uses training examples from a knowledge base, guaranteeing a large amount of (popular) relation examples without human intervention, which can be used effectively by neural networks (.", "labels": [], "entities": []}, {"text": "However, even with this technique, approaching RE as a classification task presents several limitations: (1) distant supervision models are not accurate in extracting relations with a long-tailed distribution, because they typically have a small set of instances in knowledge bases; (2) inmost domains, relation types are very specific and only a few examples of each relation are available; (3) these models cannot be applied to recognize new relation types not observed during training.", "labels": [], "entities": [{"text": "RE", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9909374117851257}]}, {"text": "In this paper, we address RE from a different perspective by reducing it to an analogy problem.", "labels": [], "entities": [{"text": "RE", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9807929992675781}]}, {"text": "Our assumption states that if two pairs of entities, (A, B) and (C, D), have at least one relation in common r, then those two pairs are analogous.", "labels": [], "entities": []}, {"text": "Viceversa, solving proportional analogies, such as A : B = C : D, consists of identifying the implicit relations shared between two pairs of entities.", "labels": [], "entities": []}, {"text": "For example, ROME:ITALY=PARIS:FRANCE is a valid analogy because capitalOf is a relation in common.", "labels": [], "entities": [{"text": "ROME:ITALY", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.6971688270568848}, {"text": "PARIS", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8841027021408081}, {"text": "FRANCE", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.8620703220367432}]}, {"text": "Based on this idea, we propose an end-to-end neural model able to measure the degree of analogical similarity between two entity pairs, instead of predicting a confidence score for each relation type.", "labels": [], "entities": []}, {"text": "An entity pair is represented through its mentions in a textual corpus, sequences of sentences where entities in the pair co-occur.", "labels": [], "entities": []}, {"text": "If a mention represents a specific relation type, then this relationship is expressed by the linguistic context surrounding the two entities.", "labels": [], "entities": []}, {"text": "E.g., \"Rome is the capital of Italy\" or \"The capital of France is Paris\" referring to the example above.", "labels": [], "entities": []}, {"text": "Thus, given two analogous entity pairs represented by their textual mentions sets as input, the model is trained to minimize the difference between the representations of relations having the same linguistic patterns.", "labels": [], "entities": []}, {"text": "In other words, the model learns the different paraphrases expressing the same relation.", "labels": [], "entities": []}, {"text": "In our research hypothesis, a model trained in such way is able to recognize analogies between unseen entity pairs belonging to new unseen relation types by: (1) generalizing over the sequence of words in the mentions; (2) projecting the sequence of words in the mentions into a vector space representing relational semantics.", "labels": [], "entities": []}, {"text": "This approach poses several research questions: (RQ1) How to collect and organize a dataset for training?", "labels": [], "entities": []}, {"text": "(RQ2) What kinds of models are effective for this task?", "labels": [], "entities": []}, {"text": "(RQ3) How should the model be evaluated?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.8231558203697205}]}, {"text": "Knowledge bases, such as Wikidata or DBpedia, consist of large relational data sources organized in the form of triples, predicate(SUBJECT, OBJECT).", "labels": [], "entities": []}, {"text": "We exploit this information to build a reliable set of analogous facts used as ground truth.", "labels": [], "entities": []}, {"text": "Then, we adopt distant supervision to retrieve relation mentions in web-scale textual corpora by matching the subject-object entities which co-occur in the same sentences (.", "labels": [], "entities": []}, {"text": "Through this technique we can train our model on millions of analogy examples without human supervision.", "labels": [], "entities": []}, {"text": "Since our goal is to train a model able to compute the relational similarity given two sets of textual mentions, we use siamese networks to learn discriminative features between those two instances ().", "labels": [], "entities": []}, {"text": "This kind of neural network has been used in both computer vision () and natural language processing) in order to map two similar instances close in a feature space.", "labels": [], "entities": []}, {"text": "However, in our setting each instance consists of a set of mentions, therefore it is inherently a multi-instance learning task . We propose a hierarchical siamese network Due to the weak supervision, the whole set of mentions with an attention mechanism at both word level ( and at the set level () in order to select the textual mention which better describes the relation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first application of a siamese network by pairing sets of instances, so it can be considered a novelty of this work.", "labels": [], "entities": []}, {"text": "We evaluate the generalization capability of our model in recognizing unseen relation types through an one-shot relational classification task introduced in this paper.", "labels": [], "entities": [{"text": "relational classification", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.7038335502147675}]}, {"text": "We train the parameters of the model on a subset of most frequent relations of one of three different distantly supervised datasets used in our experiments.", "labels": [], "entities": []}, {"text": "Then, we evaluate it on the long-tailed relations of each dataset.", "labels": [], "entities": []}, {"text": "During the test phase, only a single example for each unseen relation is provided.", "labels": [], "entities": []}, {"text": "This example is not used to update the parameters of the model as in a classification task, but rather to produce the vector representation of the relation itself.", "labels": [], "entities": []}, {"text": "Entity pairs having mention sets close to this representation are more likely to be analogous.", "labels": [], "entities": []}, {"text": "The experiments show promising results of our approach on this task, compared with the recent deep models commonly used for encoding textual representations (.", "labels": [], "entities": []}, {"text": "However, when the number of the unseen relation types increases, the performance of our model become far from the results obtained in the one-shot image classification (, opening an interesting challenge for future work.", "labels": [], "entities": [{"text": "one-shot image classification", "start_pos": 138, "end_pos": 167, "type": "TASK", "confidence": 0.6373849908510844}]}, {"text": "Finally, our model shows a transfer capability in other tasks through the use of its pre-trained vectors.", "labels": [], "entities": []}, {"text": "Indeed, a branch of the hierarchical siamese network can be used to generate entity-entity representations given sets of mentions as input, that we call analogy embeddings.", "labels": [], "entities": []}, {"text": "In our experiments, we integrate those representations into an existing end-to-end model based on convolutional networks (, outperfoming the state-of-the-art systems on two shared datasets commonly used for distantly supervised relation extraction.", "labels": [], "entities": [{"text": "distantly supervised relation extraction", "start_pos": 207, "end_pos": 247, "type": "TASK", "confidence": 0.6667764559388161}]}], "datasetContent": [{"text": "Once the analogy model is trained, it has two different capabilities.", "labels": [], "entities": []}, {"text": "First, the whole HSN architecture can be used as a binary classifier in order to infer if two entity pairs, expressed by their mentions sets, are analogous.", "labels": [], "entities": []}, {"text": "Second, we can use its subnetwork before the merge layer as a feature extractor to generate entity-entity vectors given sets of sentences as input which can be used as pretrained analogy embeddings in other tasks.", "labels": [], "entities": []}, {"text": "We evaluate our model on the one-shot relational classification and distantly supervised relation extraction benchmarks.", "labels": [], "entities": [{"text": "relational classification", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7761633396148682}, {"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.6991529017686844}]}, {"text": "In the entire experimental protocol we exploit three different datasets (see the supplemental material for details).", "labels": [], "entities": []}, {"text": "T-REX () is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples, having 685 unique relations.", "labels": [], "entities": []}, {"text": "NYT-FB () is a standard benchmark for distantly supervised relation extraction.", "labels": [], "entities": [{"text": "NYT-FB", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8688679337501526}, {"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7090739756822586}]}, {"text": "The text of New York Times was processed with a named entity recognizer and the identified entities linked by name to Freebase.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8158387740453085}, {"text": "Freebase", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9664948582649231}]}, {"text": "CC-DBP () is a webscale KB population benchmark.", "labels": [], "entities": []}, {"text": "It combines the text of Common Crawl with the entity-relationentity triples from 298 frequent relations in DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9441275000572205}]}, {"text": "Mentions of DBpedia entities are located in text by matching the preferred label.", "labels": [], "entities": []}, {"text": "This task is similar to NYT-FB, but it has a much larger number of relations, triples and textual contexts.", "labels": [], "entities": [{"text": "NYT-FB", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.5384643077850342}]}, {"text": "The statistics of the three datasets are summarized in.", "labels": [], "entities": []}, {"text": "Aside from the difference in size and KB adopted, it worth noting also the difference in terms of corpus style of these datasets.", "labels": [], "entities": [{"text": "KB", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.8242297172546387}]}, {"text": "For instance, T-REX has well-written textual mentions, because the sentences are extracted from Wikipedia.", "labels": [], "entities": []}, {"text": "Conversely, CC-DBP and NYT-FB contain dirtier sentences which mean a high probability of incurring wrong labeling.", "labels": [], "entities": [{"text": "NYT-FB", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9098657965660095}]}], "tableCaptions": [{"text": " Table 1: Statistics of the distantly supervised datasets.", "labels": [], "entities": []}, {"text": " Table 3: AUC and F1 results on CC-DBP.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7902087569236755}, {"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9973239898681641}]}]}