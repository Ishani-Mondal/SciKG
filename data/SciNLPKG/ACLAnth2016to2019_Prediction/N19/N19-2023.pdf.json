{"title": [{"text": "Cross-lingual Transfer Learning for Japanese Named Entity Recognition", "labels": [], "entities": [{"text": "Cross-lingual Transfer Learning", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7998414635658264}, {"text": "Japanese Named Entity Recognition", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6060863137245178}]}], "abstractContent": [{"text": "This work explores cross-lingual transfer learning (TL) for named entity recognition, focusing on bootstrapping Japanese from En-glish.", "labels": [], "entities": [{"text": "cross-lingual transfer learning (TL)", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.8425294011831284}, {"text": "named entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.6297640899817148}]}, {"text": "A deep neural network model is adopted and the best combination of weights to transfer is extensively investigated.", "labels": [], "entities": []}, {"text": "Moreover, a novel approach is presented that overcomes linguistic differences between this language pair by romanizing a portion of the Japanese input.", "labels": [], "entities": []}, {"text": "Experiments are conducted on external datasets, as well as internal large-scale real-world ones.", "labels": [], "entities": []}, {"text": "Gains with TL are achieved for all evaluated cases.", "labels": [], "entities": [{"text": "Gains", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9903571009635925}, {"text": "TL", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.995834469795227}]}, {"text": "Finally, the influence on TL of the target dataset size and of the target tagset distribution is further investigated.", "labels": [], "entities": [{"text": "TL", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9886205792427063}]}], "introductionContent": [{"text": "Due to the growing interest in voice-controlled devices, such as Amazon Alexa-enabled devices or Google Home, porting these devices to new languages quickly and cheaply has become an important goal.", "labels": [], "entities": []}, {"text": "One of the main components of such a device is a model for Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7751537213722864}]}, {"text": "Typically, NER models are trained on large amounts of annotated training data.", "labels": [], "entities": [{"text": "NER", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9806010723114014}]}, {"text": "However, collecting and annotating the required data to bootstrap a large-scale NER model for an industry application with reasonable performance is time-consuming, costly, and it doesn't scale to a growing number of new languages.", "labels": [], "entities": []}, {"text": "Aiming to reduce the time and costs needed for bootstrapping an NER model fora new language, we leverage existing resources.", "labels": [], "entities": []}, {"text": "In particular, we The author Andrew Johnson conducted the work for this paper during an internship at Amazon, Aachen, Germany.", "labels": [], "entities": []}, {"text": "explore cross-lingual transfer learning, in which weights from a trained model in the source language are transferred to a model in the target language.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.8467679421106974}]}, {"text": "Transfer learning (TL) has been shown previously to improve performance for target models (.", "labels": [], "entities": [{"text": "Transfer learning (TL)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9157908916473388}]}, {"text": "However, work related to crosslingual transfer learning for NER has mainly focused on rather similar languages, e.g. transferring from English to German or Spanish.", "labels": [], "entities": []}, {"text": "In contrast, we focus on transferring between dissimilar languages, i.e. from English to Japanese.", "labels": [], "entities": []}, {"text": "We present experimental results on external, i.e. publicly available, corpora, as well as on internally gathered large-scale real-world datasets.", "labels": [], "entities": []}, {"text": "First, a deep neural network model is developed for NER, and we extensively explore which combinations of weights are most useful for transferring information from English to Japanese.", "labels": [], "entities": [{"text": "NER", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8742091059684753}]}, {"text": "Furthermore, aiming to overcome the linguistic and orthographic dissimilarity between English and Japanese, we propose to romanize the Japanese input, i.e. convert the Japanese text into the Latin alphabet.", "labels": [], "entities": []}, {"text": "This results in a common character embedding space between the two languages, and intuitively should allow for more efficient transfer learning at the character level.", "labels": [], "entities": []}, {"text": "Gains with TL are achieved on all evaluated target datasets, even large-scale industrial ones.", "labels": [], "entities": [{"text": "Gains", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9762192368507385}, {"text": "TL", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9412353038787842}]}, {"text": "Moreover, the effect of TL on the target dataset size and of the target tagset distribution is investigated.", "labels": [], "entities": []}, {"text": "Finally, we show that similar gains are achieved when applying the proposed approach from English to German, indicating the possibility to generalize it both to European and nonEuropean target languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, the datasets as well as the details of the developed NER model are presented.", "labels": [], "entities": []}, {"text": "For our experiments we make use of datasets in three languages.", "labels": [], "entities": []}, {"text": "First, an English dataset is used to train the source NER model.", "labels": [], "entities": []}, {"text": "Then, a target language dataset, which is smaller in size than the source dataset, is used to build a target NER model.", "labels": [], "entities": []}, {"text": "This serves as the target baseline.", "labels": [], "entities": []}, {"text": "The weights transferred from the source model are used to initialize this target model, which is then trained with the available target data, resulting in anew target model.", "labels": [], "entities": []}, {"text": "As mentioned before, the focus of this paper is TL between dissimilar languages, and thus the main experiments use a Japanese dataset as the target corpus.", "labels": [], "entities": [{"text": "TL between dissimilar languages", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8132262527942657}]}, {"text": "However, for the sake  of comparison, we also conducted some experiments using a German target dataset, thus transferring between more similar languages, i.e. both belonging to the indo-European family, and evaluating the generalization power of the adopted approach.", "labels": [], "entities": [{"text": "German target dataset", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.6526692807674408}]}, {"text": "We evaluate our approach both on external and internal datasets.", "labels": [], "entities": []}, {"text": "External datasets are composed of company data and are mainly used for comparing our monolingual models to the state-of-the-art, while internal datasets are composed of publicly available data and are used to explore potential data reductions in a real-world large-scale industry setting.", "labels": [], "entities": []}, {"text": "Segmentation and romanization of Japanese text are performed with the open source Japanese text analyzer MeCab 2 .  Applying the best configuration established previously, i.e. transfer \"Char+Dense\" layers and use of MOM, the results before and after TL on the full JP datasets are presented in In addition, important relative gains are achieved by TL in the small external datasets, making our method particularly suited for bootstrapping anew language with very limited available annotated data.", "labels": [], "entities": [{"text": "JP datasets", "start_pos": 266, "end_pos": 277, "type": "DATASET", "confidence": 0.7497404217720032}]}, {"text": "Another interesting outcome is that we still see gains in the large internal datasets (i.e. up to 1M training utterances in the internal \"Large\" set).", "labels": [], "entities": []}, {"text": "This will be investigated further in the next section (Section 5.4).", "labels": [], "entities": []}, {"text": "Results on DE internal datasets are presented forsake of comparison and show the same trends as JP internal datasets, thus revealing the generalization of our approach for cross-lingual TL both to European and non-European target languages.", "labels": [], "entities": [{"text": "DE internal datasets", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.7868154247601827}, {"text": "JP internal datasets", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.6519333819548289}]}, {"text": "To further investigate how the size of the target datasets influences the performance of TL, we conducted experiments on different sizes of the internal data.", "labels": [], "entities": [{"text": "TL", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9787998199462891}]}, {"text": "This was done by training on subsets of the original \"Large\" JP internal training set, with sizes varying from 10k to 1M utterances.", "labels": [], "entities": [{"text": "Large\" JP internal training set", "start_pos": 54, "end_pos": 85, "type": "DATASET", "confidence": 0.7190900345643362}]}, {"text": "Note that the source English training data is still used in full each time.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "As expected, larger gains are observed for smaller splits.", "labels": [], "entities": []}, {"text": "However, TL still produces statistically significant gains for all split sizes.", "labels": [], "entities": [{"text": "TL", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.5184681415557861}]}, {"text": "Note also that training, for example, on 500k utterances with TL is better than training on 1M utterances without TL, indicating the possibility of reducing data requirements with TL even in large-scale industrial systems.", "labels": [], "entities": []}, {"text": "A further analysis of the results on the internal datasets showed that the frequency of a tag class in the target training data correlated the most with TL gain.", "labels": [], "entities": [{"text": "TL gain", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.9858239591121674}]}, {"text": "This is visualized in fora subset of the JP \"Small\" dataset.", "labels": [], "entities": [{"text": "JP \"Small\" dataset", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9473792314529419}]}, {"text": "An arrow is used for each tag class with the tail of the arrow indicating the F1 score without TL and the point indicating the F1 score of that same class with TL.", "labels": [], "entities": [{"text": "F1 score without TL", "start_pos": 78, "end_pos": 97, "type": "METRIC", "confidence": 0.829044908285141}, {"text": "F1 score", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9870523810386658}, {"text": "TL", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.8337832093238831}]}, {"text": "Thus, classes with gains point upward (blue arrows), while those that performed worse point downwards (red arrows).", "labels": [], "entities": []}, {"text": "Classes that showed no change are indicated as circles.", "labels": [], "entities": []}, {"text": "These mostly cluster along the bottom as classes that have an F1 score of zero before and after TL.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9914810955524445}, {"text": "TL", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9137448668479919}]}, {"text": "The tags are arranged along the x axis based on their frequency in the target training dataset.", "labels": [], "entities": []}, {"text": "gain less by TL, probably because they already perform well.", "labels": [], "entities": [{"text": "TL", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9987549781799316}]}, {"text": "Tag classes generally begin to show gains from TL only after they pass a certain minimum frequency threshold in the target dataset, which appears to be around 100.", "labels": [], "entities": [{"text": "TL", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9625136852264404}]}, {"text": "This maybe the reason why we have TL gains even with large target datasets.", "labels": [], "entities": [{"text": "TL", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.7445619106292725}]}, {"text": "As infrequent tag classes are observed more and more in larger splits, they begin to cross this threshold and gain from TL.", "labels": [], "entities": [{"text": "TL", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9871211647987366}]}, {"text": "Real-world data generally have long-tailed distributions, thus even very large target datasets are likely to have tag classes with few data which can benefit from TL.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of utterances per external dataset", "labels": [], "entities": []}, {"text": " Table 4: Absolute gains on JP datasets by transferring  different layer combinations", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.896682858467102}]}, {"text": " Table 5: Romanization of Japanese -Effect on TL", "labels": [], "entities": [{"text": "Effect", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.7550352215766907}, {"text": "TL", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.4341818690299988}]}, {"text": " Table 6: Results with TL over full JP and DE datasets", "labels": [], "entities": [{"text": "TL", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9359631538391113}, {"text": "DE datasets", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8608971238136292}]}]}