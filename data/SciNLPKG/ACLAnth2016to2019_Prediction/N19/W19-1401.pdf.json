{"title": [{"text": "A Report on the Third VarDial Evaluation Campaign", "labels": [], "entities": [{"text": "VarDial Evaluation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6059974133968353}]}], "abstractContent": [{"text": "In this paper, we present the findings of the Third", "labels": [], "entities": [{"text": "Third", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6847902536392212}]}], "introductionContent": [{"text": "The series of workshops on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial) has reached its sixth edition in 2019, evidencing the interest of the CL/NLP community in this topic.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial)", "start_pos": 27, "end_pos": 116, "type": "TASK", "confidence": 0.6870082039386034}]}, {"text": "The third VarDial Evaluation Campaign 1 featuring five shared tasks, described in detail in this report, has been organized as part of VarDial 2019 co-located with the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).", "labels": [], "entities": []}, {"text": "It follows two editions of the campaign organized in 2017 with four tasks ( and in 2018 with five tasks (.", "labels": [], "entities": []}, {"text": "Since its first edition, shared tasks have been organized as part of the VarDial, most notably the Discriminating between Similar Languages organized from 2014 to).", "labels": [], "entities": [{"text": "VarDial", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8260552287101746}]}, {"text": "The shared tasks organized at VarDial helped providing evaluation benchmarks and public datasets (e.g. () for different tasks such as dialect identification, morphosyntactic tagging, and crosslingual dependency parsing.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7337593287229538}, {"text": "morphosyntactic tagging", "start_pos": 158, "end_pos": 181, "type": "TASK", "confidence": 0.6562738120555878}, {"text": "crosslingual dependency parsing", "start_pos": 187, "end_pos": 218, "type": "TASK", "confidence": 0.7207664251327515}]}, {"text": "Similar languages such as Bulgarian and Macedonian, and Czech and Slovak, along with varieties and dialects of Arabic, German, Hindi, Portuguese, and Spanish have been included in the competitions organized within the scope of VarDial.", "labels": [], "entities": [{"text": "VarDial", "start_pos": 227, "end_pos": 234, "type": "DATASET", "confidence": 0.8637490272521973}]}, {"text": "In this paper, we present the results and main findings of the third VarDial Evaluation Campaign.", "labels": [], "entities": [{"text": "VarDial Evaluation Campaign", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.7129926880200704}]}, {"text": "The five tasks organized this year were: German Dialect Identification (GDI) presented in Section 4, Cross-lingual Morphological Analysis (CMA) presented in Section 5, Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT) presented in Section 6, Moldavian vs. Romanian Cross-dialect Topic identification (MRC) presented in Section 7, and finally, Cuneiform Language Identification (CLI) presented in Section 8.", "labels": [], "entities": [{"text": "German Dialect Identification (GDI)", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6976740310589472}, {"text": "Cross-lingual Morphological Analysis (CMA)", "start_pos": 101, "end_pos": 143, "type": "TASK", "confidence": 0.7469179332256317}, {"text": "Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT)", "start_pos": 168, "end_pos": 246, "type": "TASK", "confidence": 0.5713306441903114}, {"text": "Moldavian vs. Romanian Cross-dialect Topic identification (MRC)", "start_pos": 271, "end_pos": 334, "type": "TASK", "confidence": 0.7059994604852464}, {"text": "Cuneiform Language Identification (CLI)", "start_pos": 372, "end_pos": 411, "type": "TASK", "confidence": 0.7682928442955017}]}, {"text": "In, we include references to the 14 system description papers written by the participants of the campaign and published in the VarDial workshop proceedings.", "labels": [], "entities": [{"text": "VarDial workshop proceedings", "start_pos": 127, "end_pos": 155, "type": "DATASET", "confidence": 0.8605845967928568}]}], "datasetContent": [{"text": "The dataset used in the CLI shared task, as well as its creation, is described in detail by.", "labels": [], "entities": []}, {"text": "The dataset was created using openly available transliterations originating from the Open Richly Annotated Cuneiform Corpus (Oracc).", "labels": [], "entities": [{"text": "Open Richly Annotated Cuneiform Corpus (Oracc)", "start_pos": 85, "end_pos": 131, "type": "DATASET", "confidence": 0.6821464970707893}]}, {"text": "In Oracc, the texts, originally written using the cuneiform script, are mostly stored in transliterated form.", "labels": [], "entities": []}, {"text": "A special conversion program was used to transform these transliterated texts to Unicode cuneiform encoding.", "labels": [], "entities": []}, {"text": "The data consists of texts originally appearing in one line of cuneiform writing.", "labels": [], "entities": []}, {"text": "Word boundaries were not marked in the original script, but in the transliterations the word boundaries were marked.", "labels": [], "entities": []}, {"text": "In order to produce more realistic cuneiform writing, the word boundaries were again removed in the conversion to Unicode cuneiform.", "labels": [], "entities": []}, {"text": "Each line, thus, may consist of one or more words.", "labels": [], "entities": []}, {"text": "The sizes of the training sets for each language varied, and the exact number of lines in each can be seen in.", "labels": [], "entities": []}, {"text": "In addition to the training set, the participants were provided with 668 lines of development data for each language.", "labels": [], "entities": []}, {"text": "The test set had 985 lines for each language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The teams that participated in the Third VarDial Evaluation Campaign.", "labels": [], "entities": [{"text": "VarDial Evaluation Campaign", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.47260022163391113}]}, {"text": " Table 2: Results and rankings of GDI participants. The table also specifies the data formats and techniques used  by the participants.", "labels": [], "entities": []}, {"text": " Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.", "labels": [], "entities": [{"text": "CMA task", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.8412838578224182}, {"text": "F-scores", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9802277684211731}, {"text": "BASELINE-I", "start_pos": 381, "end_pos": 391, "type": "METRIC", "confidence": 0.9813242554664612}, {"text": "BASELINE-II", "start_pos": 424, "end_pos": 435, "type": "METRIC", "confidence": 0.9781326055526733}]}, {"text": " Table 4: The macro F1-scores for DMT-Traditional and DMT-Simplified shared task alongside with the summary  of methods and features used by the teams.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.7344842553138733}, {"text": "DMT-Traditional", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8193578720092773}]}, {"text": " Table 5: The number of samples (#samples) and the  number of tokens (#tokens) contained in the training,  development (public validation plus test sets) and pri- vate test sets included in the MOROCO dataset.", "labels": [], "entities": [{"text": "MOROCO dataset", "start_pos": 194, "end_pos": 208, "type": "DATASET", "confidence": 0.9216212034225464}]}, {"text": " Table 6: Results on MRC subtask 1 (binary classifica- tion by dialect).", "labels": [], "entities": [{"text": "MRC subtask", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.6156556308269501}]}, {"text": " Table 7: Results on MRC subtask 2 (multi-class cat- egorization by topic of Romanian text samples using  Moldavian text samples for training).", "labels": [], "entities": [{"text": "MRC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8926582932472229}]}, {"text": " Table 8: Results on MRC subtask 2 (multi-class cat- egorization by topic of Moldavian text samples using  Romanian text samples for training).", "labels": [], "entities": [{"text": "MRC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8951361179351807}]}]}