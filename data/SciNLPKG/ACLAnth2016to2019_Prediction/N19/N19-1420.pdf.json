{"title": [{"text": "CNM: An Interpretable Complex-valued Network for Matching", "labels": [], "entities": [{"text": "Matching", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.8918525576591492}]}], "abstractContent": [{"text": "This paper seeks to model human language by the mathematical framework of quantum physics.", "labels": [], "entities": []}, {"text": "With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems.", "labels": [], "entities": []}, {"text": "A complex-valued network is built to implement this framework for semantic matching.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7698346674442291}]}, {"text": "With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings.", "labels": [], "entities": []}, {"text": "The proposed complex-valued network for matching (CNM) 1 achieves comparable performances to strong CNN and RNN base-lines on two benchmarking question answering (QA) datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a growing concern on the interpretability of neural networks.", "labels": [], "entities": [{"text": "interpretability of neural networks", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.8213469684123993}]}, {"text": "Along with the increasing power of neural networks comes the challenge of interpreting the numerical representation of network components into human-understandable language.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9656841158866882}]}, {"text": "points out two important factors fora model to be interpretable, namely post-hoc interpretability and transparency.", "labels": [], "entities": [{"text": "transparency", "start_pos": 102, "end_pos": 114, "type": "METRIC", "confidence": 0.9616952538490295}]}, {"text": "The former refers to explanations of why a model works after it is executed, while the latter concerns self-explainability of components through some mechanisms in the designing phase of the model.", "labels": [], "entities": []}, {"text": "We seek inspirations from quantum physics to build transparent and post-hoc interpretable networks for modeling human language.", "labels": [], "entities": []}, {"text": "The emerging research field of cognition suggests that there exist quantum-like phenomena inhuman cognition (), especially language understanding (.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7200293838977814}]}, {"text": "Intuitively, a * Equal Contribution \u2020 Corresponding Author 1 https://github.com/wabyking/qnn.git sentence can be treated as a physical system with multiple words (like particles), and these words are usually polysemous (superposed) and correlated (entangled) with each other.", "labels": [], "entities": []}, {"text": "Motivated by these existing works, we aim to investigate the following Research Question (RQ).", "labels": [], "entities": [{"text": "Research Question (RQ)", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.5177613496780396}]}, {"text": "RQ1: Is it possible to model human language with the mathematical framework of quantum physics?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8766870498657227}]}, {"text": "Towards this question, we build a novel quantum-theoretic framework for modeling language, in an attempt to capture the quantumness in the cognitive aspect of human language.", "labels": [], "entities": []}, {"text": "The framework models different linguistic units as quantum states with the adoption of quantum probability (QP), which is the mathematical framework of quantum physics that models uncertainly on a uniform Semantic Hilbert Space (SHS).", "labels": [], "entities": []}, {"text": "Complex values are crucial in the mathematical framework of characterizing quantum physics.", "labels": [], "entities": [{"text": "characterizing quantum physics", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.8864135146141052}]}, {"text": "In order to preserve physical properties, the linguistic units have to be represented as complex vectors or matrices.", "labels": [], "entities": []}, {"text": "This naturally gives rise to another research question: RQ2: Can we benefit from the complex-valued representation of human language in areal natural language processing (NLP) scenario?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.6288540959358215}]}, {"text": "To this end, we formulate a linguistic unit as a complex-valued vector, and link its length and direction to different physical meanings: the length represents the relative weight of the word while the direction is viewed as a superposition state.", "labels": [], "entities": []}, {"text": "The superposition state is further represented in an amplitude-phase manner, with amplitudes corresponding to the lexical meaning and phases implicitly reflecting the higher-level semantic aspects such as polarity, ambiguity or emotion.", "labels": [], "entities": []}, {"text": "In order to evaluate the above framework, we implement it as a complex-valued network (CNM) for semantic matching.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7485814988613129}]}, {"text": "The network is applied to the question answering task, which is the most typical matching task that aims at selecting the best answer fora question from a pool of candidates.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8425357540448507}]}, {"text": "In order to facilitate local matching with ngrams of a sentence pair, we design a local matching scheme in CNM.", "labels": [], "entities": []}, {"text": "Most of State-of-the-art QA models are mainly based on Convolution Neural Network (CNN), Recurrent Neural Network (RNN) and many variants thereof (.", "labels": [], "entities": []}, {"text": "However, with opaque structures of convolutional kernels and recurrent cells, these models are hard to understand for humans.", "labels": [], "entities": []}, {"text": "We argue that our model is advantageous in terms of interpretability.", "labels": [], "entities": []}, {"text": "Our proposed CNM is transparent in that it is designed in alignment with quantum physics.", "labels": [], "entities": []}, {"text": "Experiments on benchmarking QA datasets show that CNM has comparable performance to strong CNN and RNN baselines, whilst admitting post-hoc interpretations to human-understandable language.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.6701783090829849}]}, {"text": "We therefore answer RQ1 by claiming that it is possible to model human language with the proposed quantum-theoretical framework in this paper.", "labels": [], "entities": [{"text": "RQ1", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.47759371995925903}]}, {"text": "Furthermore, an ablation study shows that the complex-valued word embedding performs better than its real counterpart, which allows us to answer RQ2 by claiming that we benefit from the complex-valued representation of natural language on the QA task.", "labels": [], "entities": []}], "datasetContent": [{"text": "2 and 3 show the experiment results on TREC QA and WikiQA respectively, where bold values are the best performances out of all models.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.6330781877040863}, {"text": "WikiQA", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9252736568450928}]}, {"text": "Our model achieves 3 best performances out of the 4 metrics on TREC QA and WikiQA, and performs slightly worse than the best-performed models on the remaining metric.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9093698561191559}, {"text": "WikiQA", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.818806529045105}]}, {"text": "This illustrates the effectiveness of our proposed model from a general perspective.", "labels": [], "entities": []}, {"text": "Specifically, CNM outperforms most CNN and LSTM-based models, which have more complicated structures and a relatively larger parameters scale.", "labels": [], "entities": []}, {"text": "Also, CNM performs better than existing quantum-inspired QA models, QLM and NNQLM on both datasets, which means that the quantum theoretical framework gives rise to better performs model.", "labels": [], "entities": []}, {"text": "Moreover, a significant improvement over NNQLM-1 is observed on these two datasets, supporting our claim that the trace inner product is not an effective distance metric of two density matrices.: Ablation Test.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.994297206401825}]}, {"text": "The values in parenthesis are the performance differences between the model and CNM.", "labels": [], "entities": [{"text": "CNM", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9169726371765137}]}], "tableCaptions": [{"text": " Table 1: Dataset Statistics. For each cell, the values de- note the number of questions and question-answer pairs  respectively.", "labels": [], "entities": []}, {"text": " Table 2: Experiment Results on TREC QA Dataset.  The best performed values are in bold.", "labels": [], "entities": [{"text": "TREC QA Dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8785258332888285}]}, {"text": " Table 3: Experiment Results on WikiQA Dataset.The  best performed values for each dataset are in bold.", "labels": [], "entities": [{"text": "WikiQA Dataset.The", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9658720195293427}]}, {"text": " Table 4: Ablation Test. The values in parenthesis are the  performance differences between the model and CNM.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9760885238647461}, {"text": "CNM", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.9371599555015564}]}]}