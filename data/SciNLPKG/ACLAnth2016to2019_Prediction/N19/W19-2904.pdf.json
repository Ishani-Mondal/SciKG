{"title": [{"text": "Surprisal and Interference Effects of Case Markers in Hindi Word Order", "labels": [], "entities": [{"text": "Case Markers in Hindi Word Order", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.5898673435052236}]}], "abstractContent": [{"text": "Based on the Production-Distribution-Comprehension (PDC) account of language processing, we formulate two distinct hypotheses about case marking, word order choices and processing in Hindi.", "labels": [], "entities": [{"text": "case marking", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.7117873281240463}]}, {"text": "Our first hypothesis is that Hindi tends to optimize for processing efficiency at both lexical and syntactic levels.", "labels": [], "entities": []}, {"text": "We quantify the role of case markers in this process.", "labels": [], "entities": []}, {"text": "For the task of predicting the reference sentence occurring in a corpus (amidst meaning-equivalent grammatical variants) using a machine learning model, surprisal estimates from an artificial version of the language (i.e., Hindi without any case markers) result in lower prediction accuracy compared to natural Hindi.", "labels": [], "entities": [{"text": "predicting the reference sentence occurring in a corpus", "start_pos": 16, "end_pos": 71, "type": "TASK", "confidence": 0.871634416282177}, {"text": "accuracy", "start_pos": 282, "end_pos": 290, "type": "METRIC", "confidence": 0.9631033539772034}]}, {"text": "Our second hypothesis is that Hindi tends to minimize interference due to case markers while ordering preverbal constituents.", "labels": [], "entities": []}, {"text": "We show that Hindi tends to avoid placing next to each other constituents whose heads are marked by identical case inflections.", "labels": [], "entities": []}, {"text": "Our findings adhere to PDC assumptions and we discuss their implications for language production, learning and universals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language universals encode distributional regularities across languages of the world.", "labels": [], "entities": []}, {"text": "This study is motivated by the well known correlation between case marking and increased word order flexibility, often expressed as an implicational universal . The origin of such universals has been the topic of a long-standing debate in linguistics and cognitive science.", "labels": [], "entities": [{"text": "case marking", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.7175646424293518}]}, {"text": "As the cited work expounds, one view is that language universals emerged due to constraints specific to the system of language and not related to the other cognitive faculties).", "labels": [], "entities": []}, {"text": "Another view is that languages evolved overtime as a consequence of cognitive mechanisms and pressures linked with language use.", "labels": [], "entities": []}, {"text": "Thus, cognitive biases related to processing), learnability and communicative efficiency (Jaeger and Tily, 2011) have been proposed as underlying the systematic similarities and divergences between natural languages.", "labels": [], "entities": []}, {"text": "The Production-Distribution-Comprehension (PDC) account of language processing proposed by is an integrated theory of language production and comprehension that seeks to connect language production with typology and comprehension.", "labels": [], "entities": []}, {"text": "It is broadly in the spirit of the second view regarding linguistic universals described above and posits production difficulty as the sole factor influencing linguistic form.", "labels": [], "entities": []}, {"text": "Hence it is an unconventional approach, contrasting radically with alternative accounts of language use in which language forms are shaped by constraints on language acquisition processes or considerations of facilitating language comprehension for the listeners.", "labels": [], "entities": []}, {"text": "Based on PDC assumptions, we formulated two hypotheses linking processing efficiency, case marking, and word order choices at the level of individual speakers (as opposed to the population level) in Hindi, a language having predominantly SOV word order.", "labels": [], "entities": [{"text": "case marking", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.7370520234107971}]}, {"text": "Hindi has a rich system of case markers along with a relatively flexible word order) and thus adheres to the implicational universal stated at the outset.", "labels": [], "entities": []}, {"text": "The PDC principle Easy First stipulates that more accessible words are ordered before less accessible words.", "labels": [], "entities": []}, {"text": "Accessibility of a word is influenced by its ease of retrievability from memory.", "labels": [], "entities": []}, {"text": "Inspired by the stated PDC principle, our first hypothesis is that Hindi tends to optimize for processing efficiency at both lexical and syntactic levels.", "labels": [], "entities": []}, {"text": "We investigate the role of case markers in this process by comparing the processing efficiency of natural Hindi and an artificial version of Hindi without case markers.", "labels": [], "entities": []}, {"text": "Based on the PDC principle of Reduce Interference, our second hypothesis is that Hindi orders constituents such that phonological interference caused by case marker repetition is minimized.", "labels": [], "entities": [{"text": "Reduce Interference", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8738287687301636}]}, {"text": "Interference is the idea that entities with similar properties (like form, meaning, animacy, concreteness and so forth) cause processing difficulties when they occur in proximity.", "labels": [], "entities": [{"text": "Interference", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9541022181510925}]}, {"text": "A long line of research attests the role of interference in both production) and comprehension).", "labels": [], "entities": []}, {"text": "In order to test the stated hypotheses, we deploy a machine learning model to predict the reference sentence occurring in the Hindi-Urdu TreeBank (HUTB) corpus) of written text 2 (Example 1a below), amidst a set of artificially created grammatical variants expressing the same proposition (Examples 1b-1c).", "labels": [], "entities": [{"text": "Hindi-Urdu TreeBank (HUTB) corpus", "start_pos": 126, "end_pos": 159, "type": "DATASET", "confidence": 0.8611439814170202}]}, {"text": "Case markers are shown in bold for illustration purposes.", "labels": [], "entities": []}, {"text": "The variants above have two adjacent komarked constituents, potentially causing interference during production.", "labels": [], "entities": []}, {"text": "So the PDC account would not prefer these sentences on account of production difficulty and instead prefer the reference sentence above.", "labels": [], "entities": [{"text": "PDC account", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.920813798904419}]}, {"text": "The possibility that speakers chose the reference sentence above so that it would facilitate comprehension for listeners (compared to variant sentences which might be harder to interpret) is not considered by the PDC account.", "labels": [], "entities": [{"text": "PDC account", "start_pos": 213, "end_pos": 224, "type": "DATASET", "confidence": 0.9176244139671326}]}, {"text": "We quantified processing efficiency using surprisal, originally proposed as a measure of language comprehension difficulty by Surprisal Theory.", "labels": [], "entities": []}, {"text": "Consequently, we introduced surprisal estimated from n-gram and dependency parsing models into a logistic regression model for the task of predicting the reference sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7196556925773621}]}, {"text": "Our choice of surprisal is inspired by, who point out that the desiderata for PDC to become a theory of powerful empirical import is that it should make quantitative and localized predictions about incremental processing difficulty at each word.", "labels": [], "entities": []}, {"text": "They highlight the fact that such a theory already exists, viz.", "labels": [], "entities": []}, {"text": "the Surprisal Theory of language comprehension mentioned above.", "labels": [], "entities": [{"text": "Surprisal Theory of language comprehension", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.8135668277740479}]}, {"text": "A perusal of the literature on information density in language production suggests that surprisal is a reasonable choice to model production difficulty as well.", "labels": [], "entities": []}, {"text": "Information density and surprisal are mathematically equivalent and both quantify the contextual predictability of a linguistic unit.", "labels": [], "entities": []}, {"text": "But surprisal is based on different theoretical assumptions about resource allocation in comprehension.", "labels": [], "entities": []}, {"text": "Recent research has demonstrated that reduction phenomena at both lexical, verb contraction) and syntactic, thatcomplementizer choice) levels exhibit the drive to minimize variation in information density across the linguistic signal.", "labels": [], "entities": []}, {"text": "Moreover, instances of the same word which have greater predictability tend to bespoken faster and with less emphasis on acoustic details ().", "labels": [], "entities": []}, {"text": "The work cited above uses lexical frequencies or n-gram models over words to estimate contextual predictability.", "labels": [], "entities": []}, {"text": "More recently, showed that syntactic surprisal estimated from a top-down incremental parser is positively correlated with the duration of words in spontaneous speech, even in the presence of controls including word frequencies and trigram lexical surprisal estimates.", "labels": [], "entities": []}, {"text": "Crucial to our study, words which are predictable in context have been interpreted to be more accessible in recent research).", "labels": [], "entities": []}, {"text": "The results of our experiments show that reference sentences tend to minimize both trigram and dependency parser surprisal in comparison to their variants.", "labels": [], "entities": []}, {"text": "Further, we show that the prediction accuracies of surprisal estimates derived from an artificially created version of Hindi without any case markers are significantly worse than the corresponding surprisal estimates based on natural Hindi.", "labels": [], "entities": []}, {"text": "This experiment demonstrates the crucial contribution of case markers towards the predictive ability of surprisal and confirms our first hypothesis.", "labels": [], "entities": [{"text": "predictive", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.9528033137321472}]}, {"text": "Subsequently, we demonstrate that Hindi tends to avoid placing together constituents whose heads are marked by the same case marker.", "labels": [], "entities": []}, {"text": "Moreover, incorporating predictors based on adjacent case marker sequences in a statistical model significantly improves model prediction accuracy over an extremely competitive baseline provided by n-gram and dependency parser surprisal.", "labels": [], "entities": [{"text": "model prediction", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.6586606353521347}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.8791384100914001}, {"text": "dependency parser", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.682719275355339}]}, {"text": "Phonological interference is a plausible explanation for this phenomenon and lends credence to our second hypothesis.", "labels": [], "entities": []}, {"text": "The Hindi sentence comprehension literature provides only limited support for interference involving case marker sequences.", "labels": [], "entities": []}, {"text": "Hence, it is plausible that this effect is a factor confined to the production system and not related to considerations of language comprehension.", "labels": [], "entities": []}, {"text": "Further research using spoken corpora and spontaneous production experiments need to be performed in order to validate the psychological reality of our findings.", "labels": [], "entities": []}, {"text": "Given that symbols used in the Hindi orthography have a direct correspondence with the sounds of the language), we expect speech to behave similarly.", "labels": [], "entities": []}, {"text": "Our main contribution is that we broaden the typological base of the PDC account of language processing, leveraging its connection with the well established surprisal theory of language comprehension.", "labels": [], "entities": [{"text": "PDC account of language processing", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.5972146272659302}]}, {"text": "state that surprisal would enable PDC to be implemented computationally, thus facilitating hypothesis testing on a wide range of linguistic phenomena crosslinguistically.", "labels": [], "entities": [{"text": "PDC", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9604538679122925}]}, {"text": "To this end, we setup a computational framework consisting of standard tools and techniques from the field of Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.8127166926860809}]}, {"text": "Methodologically, the task of referent sentence prediction is a relatively novel way of studying word order and is inspired from the surface realization component of NLG.", "labels": [], "entities": [{"text": "referent sentence prediction", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8061735431353251}]}, {"text": "Recently, using a similar setup, showed the impact of dependency length on English word order choices.", "labels": [], "entities": []}, {"text": "In this paper, Section 2 provides necessary background and Section 3 provides details of our data sets and models.", "labels": [], "entities": []}, {"text": "Section 4 presents our experiments and their results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summa- rizes the conclusions of our study and discusses the implications of our results for language production and learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe three experiments to test our hypotheses on the transformed version of our data set consisting of 175801 data points using a logistic regression model.", "labels": [], "entities": [{"text": "data set consisting of 175801 data points", "start_pos": 104, "end_pos": 145, "type": "DATASET", "confidence": 0.7013166802270072}]}, {"text": "The goal is to predict \"1\" and \"0\" labels (as described in the previous section) using a set of cognitively motivated features.", "labels": [], "entities": []}, {"text": "We calculated lexical and dependency parser surprisal feature values over entire sentences by summing the log probabilities of the surprisal values of individual words.", "labels": [], "entities": [{"text": "dependency parser surprisal feature", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.7864043936133385}]}, {"text": "We carried out 27-fold cross-validation; for each run, a model trained on 26 folds (consisting of 1 fold for hyperparameter tuning) was used to generate predictions about the remaining fold (100 training iterations using lbfgs solver in python scikit-learn toolkit-v0.16.1).", "labels": [], "entities": []}, {"text": "Here, we test the hypothesis that word order choices in language are optimized for processing efficiency by incorporating trigram and dependency parser surprisal as predictors in a logistic re- gression model.", "labels": [], "entities": []}, {"text": "A negative regression coefficient for these predictors would imply that corpus sentences have lower surprisal than variants.", "labels": [], "entities": []}, {"text": "For the entire corpus, indicates this trend, where the mean trigram surprisal per sentence of the corpus of reference sentences is lower than the corresponding value of all their variants ( in the Appendix depicts the same trend for syntactic surprisal).", "labels": [], "entities": [{"text": "trigram surprisal", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.8889380693435669}]}, {"text": "For the task of predicting HUTB reference sentences, both our surprisal measures have a negative regression coefficient, individually as well as in combination (first three rows of).", "labels": [], "entities": [{"text": "predicting HUTB reference sentences", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.7946415096521378}]}, {"text": "This confirms our hypothesis that word order choices optimize for processing efficiency.", "labels": [], "entities": []}, {"text": "Given our interpretation of low surprisal as denoting ease of accessibility, our first experiment shows that Hindi  The reference sentence (Example 2a with trigram surprisal of 45.60 hartleys) has the ergativeaccusative (ne-ko) ordering of case-marked nouns compared to the variant (Example 2b with higher trigram surprisal 47.12) having the opposite ordering of nouns.", "labels": [], "entities": []}, {"text": "Overall, 6% of a total of 175 HUTB sentences having ergative and accusative case markers exhibit a non-canonical accusativeergative order ().", "labels": [], "entities": []}, {"text": "In both sentences above, the case markers in questions are separated by a single word and hence form part of a single trigram.", "labels": [], "entities": []}, {"text": "Thus trigram surprisal is able to model the dominant order successfully while dispreferring the opposite order seen in Example 2b.", "labels": [], "entities": []}, {"text": "Moreover, dependency parser surprisal has much lower classification accuracy compared to trigram surprisal and has a very negligible impact on performance on top of trigram surprisal.", "labels": [], "entities": [{"text": "dependency parser surprisal", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8163205981254578}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.8633792400360107}]}, {"text": "Thus, surprisal estimates from an incremental dependency parser are not effective in modelling constituent order choices.", "labels": [], "entities": []}, {"text": "This is slightly unexpected as showed that surprisal estimates derived via the dependency parser deployed in our work accounts for per-word reading times for the Potsdam-Allahabad corpus over and above bigram frequencies.", "labels": [], "entities": [{"text": "Potsdam-Allahabad corpus", "start_pos": 162, "end_pos": 186, "type": "DATASET", "confidence": 0.8621802926063538}]}, {"text": "Using a similar setup, showed that for the task of predicting English syntactic choice alternations, PCFG surprisal performed significantly better than n-gram model surprisal and the impact of dependency length is over and above both the aforementioned surprisal predictors.", "labels": [], "entities": [{"text": "predicting English syntactic choice alternations", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.8804993748664856}]}, {"text": "We are in the process of creating a constituency structure treebank for Hindi and plan to experiment with surprisal derived from a constituent-structure parser very soon.", "labels": [], "entities": []}, {"text": "In recently completed work, show that for Hindi, dependency length exhibits a weak effect over and above surprisal for predicting corpus sentences amidst artificial variants.", "labels": [], "entities": [{"text": "predicting corpus sentences", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.8624712824821472}]}, {"text": "Finally, we examined 1022 reference-variant pairs in our dataset where none of our features was able to predict the reference sentence correctly.", "labels": [], "entities": []}, {"text": "We isolated cases involving other factors like given-new orders (30% cases), focus or topic considerations (marked by hi or to markers constituting 10% of cases) and null subjects (7.5%).", "labels": [], "entities": []}, {"text": "Such discourse considerations are not encoded in our surprisal estimates (confined to single sentences) and further research can incorporate information about sentences from the preceding context into surprisal estimates.", "labels": [], "entities": []}, {"text": "Note that when considering the relationship between communicative efficiency and word order choices, there is a potential 'levels' problem.", "labels": [], "entities": []}, {"text": "At the level of evolutionary timescales and entire populations, one might expect the grammar or distributional properties of the language to be adapted for efficiency.", "labels": [], "entities": []}, {"text": "But at the level of an individual speaker's production choices, certain measures of efficiency will in turn depend on the extant distribution of linguistic forms.", "labels": [], "entities": []}, {"text": "So there is a potential circularity in trying to assess the validity of such measures.", "labels": [], "entities": []}, {"text": "Here we seek to model only the lower of these levels, i.e., individual choices over a human lifetime.", "labels": [], "entities": []}, {"text": "Hence, all the non-corpus variants we consider are grammatical.", "labels": [], "entities": []}, {"text": "We assume that the grammar of the language is held fixed, and within the set of possible word order variants of a sentence licensed by that extant grammar, seek to model why speakers may have a greater propensity to produce some variants over others.", "labels": [], "entities": []}, {"text": "In the light of the PDC principle of Minimize Interference, we investigate whether interference   between NPs whose heads are marked by the same case marker influence preverbal constituent ordering choices in Hindi.", "labels": [], "entities": [{"text": "Minimize Interference", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9744400978088379}]}, {"text": "Since PDC seeks to link production and comprehension, our experiments are also motivated by prior work on case marker interference in sentence comprehension in SOV languages like Japanese (), Korean () and Hindi.", "labels": [], "entities": []}, {"text": "Our work is directly related to the experiments on identical case marking described in Chapter 3 of Vasishth (2003).", "labels": [], "entities": [{"text": "case marking", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.7092847228050232}, {"text": "Vasishth (2003)", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.4886351674795151}]}, {"text": "In the case of Hindi center-embeddings, this work examined whether NPs having nominal heads marked by identical case markers induce similarity-based interference effects at the subsequent verb as predicted by the Retrieval Interference Theory).", "labels": [], "entities": []}, {"text": "The study shows limited support for interference emanating from phonologically similar case markers.", "labels": [], "entities": []}, {"text": "In order to investigate interference caused by case markers in syntactic choice, we designed features based on case markers and incorporated them into our logistic regression model.", "labels": [], "entities": []}, {"text": "For each dependency tree, we introduced two types of features associated with preverbal constituents of the root verb.", "labels": [], "entities": []}, {"text": "1. Case-sequence features: Counts of case marker sequences associated with the heads of a pair of adjacent constituents.", "labels": [], "entities": []}, {"text": "We also introduced generic case-sequence features same-seq and diffseq to model the overall trend.", "labels": [], "entities": []}, {"text": "For each tree, these features denote the total number of identical and different case markers sequences associated with pairs of adjacent constituents.", "labels": [], "entities": []}, {"text": "the fact that languages often use adverbial elements or other non-case marked arguments to separate case marked constituents.", "labels": [], "entities": []}, {"text": "illustrates our case features based on the dependency tree in corresponding to Example 1a.", "labels": [], "entities": []}, {"text": "In isolation, the case-sequence and casedistance features exhibit accuracies around 70% (second column of).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9930873513221741}]}, {"text": "The case sequence and distance features together induce a significant accuracy increase of 1.5% (McNemar's two-tailed significance p < 0.001) over a baseline model consisting of lexical and dependency parser surprisal as features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9990189075469971}, {"text": "McNemar's two-tailed significance p", "start_pos": 97, "end_pos": 132, "type": "METRIC", "confidence": 0.7122465431690216}]}, {"text": "Though this might be a small increase when considered in isolation, we would like to note that our baseline model is extremely competitive (90.16% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9986722469329834}]}, {"text": "Even dependency parser surprisal did not confer considerable performance gains over and above trigram surprisal as discussed earlier.", "labels": [], "entities": [{"text": "dependency parser surprisal", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.8553136189778646}]}, {"text": "So in this context, the contribution of case features is noteworthy.", "labels": [], "entities": []}, {"text": "Subsequently, we examined the learned weights of our case sequence features) in our best model containing surprisal and all the case marker features.", "labels": [], "entities": []}, {"text": "A negative weight is associated with four of the seven identical case marker sequences as well as the same-seq feature encoding the overall pattern across all case markers.", "labels": [], "entities": []}, {"text": "These negative weights lend support to our hypothesis that Hindi shows a dispreference for placing together constituents whose heads are marked using the same case inflection.", "labels": [], "entities": []}, {"text": "Interference due to repetition of phonologically identical case markers maybe a plausible explanation for this phenomenon.", "labels": [], "entities": []}, {"text": "However, three other case marker sequences have a positive weight and hence indicate a tendency towards adjacency.", "labels": [], "entities": []}, {"text": "These three case markers are much lower in frequency in the HUTB compared to the other four and might not represent the dominant tendency.", "labels": [], "entities": [{"text": "frequency", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9798000454902649}]}, {"text": "However, future inquiries need to explore the role of case-based facilita-tion.", "labels": [], "entities": []}, {"text": "Since our features are not sensitive to clause boundaries, conclusive evidence for phonological interference will emerge only after controlling for clause boundaries.", "labels": [], "entities": []}, {"text": "The best model (baseline + case marker features) picked the reference sentence (Example 1a) while the baseline model erroneously selected the artificially generated variants (Examples 1b and 1c).", "labels": [], "entities": []}, {"text": "The reference sentence has two komarked constituents separated by intervening constituents.", "labels": [], "entities": []}, {"text": "In contrast, the variant sentences have two adjacent ko-marked constituents, potentially causing interference.", "labels": [], "entities": []}, {"text": "These examples also highlight the ambiguous nature of the ko-marker in denoting several functions in Hindi.", "labels": [], "entities": []}, {"text": "As noted by, ko marks both accusative and dative case on objects (company in the cited examples) as well as dative subjects.", "labels": [], "entities": []}, {"text": "In addition, it also occurs on spatial and temporal adjuncts (as in.", "labels": [], "entities": []}, {"text": "In these examples, since ko marks both dative case and temporality, interference might be purely phonological in nature and not related to the actual grammatical function being marked.", "labels": [], "entities": []}, {"text": "Further, we calculated the ranking accuracy of our main models, i.e., the percentage of times a model ranked the reference sentence compared to all its variants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9890251755714417}]}, {"text": "(column 3) indicates that introducing case marker features into the baseline model induced significant ranking accuracy gains (McNemar's two-tailed significance p < 0.001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9705408811569214}, {"text": "McNemar's two-tailed significance p", "start_pos": 127, "end_pos": 162, "type": "METRIC", "confidence": 0.7198323249816895}]}, {"text": "So our best model ranked Example 1a as the best sentence among all the other variants.", "labels": [], "entities": []}, {"text": "Our classification and ranking results suggest that the PDC Reduce Interference principle of production ease is a valid constraint in constituent ordering.", "labels": [], "entities": [{"text": "PDC Reduce Interference", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.6408916711807251}]}, {"text": "In Hindi sentence comprehension, Vasishth (2003) explored the idea of Positional similarity (), whereby the position of otherwise syntactically indiscriminable NPs in the structure contribute to interference at the subsequent verb.", "labels": [], "entities": [{"text": "Positional similarity", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8791604936122894}]}, {"text": "So he compared reading times at the innermost verb in the sequences of constituents with heads marked by ne-se-ko-ko and ne-ko-se-ko inflections.", "labels": [], "entities": []}, {"text": "However, there was no significant difference in reading times between these sequences, thus offering no support for positional similarity during comprehension.", "labels": [], "entities": []}, {"text": "This is the experimental condition which is most closely linked to our work.", "labels": [], "entities": []}, {"text": "Interpreted in conjunction with our findings, case marker interference in Hindi appears to be a constraint on production rather than comprehension.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Values of case", "labels": [], "entities": []}, {"text": " Table 5: Pairwise classification and ranking accuracy (***", "labels": [], "entities": [{"text": "Pairwise classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8798255324363708}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9881540536880493}]}]}