{"title": [{"text": "A Dynamic Speaker Model for Conversational Interactions", "labels": [], "entities": []}], "abstractContent": [{"text": "Individual differences in speakers are reflected in their language use as well as in their interests and opinions.", "labels": [], "entities": []}, {"text": "Characterizing these differences can be useful in human-computer interaction , as well as analysis of human-human conversations.", "labels": [], "entities": []}, {"text": "In this work, we introduce a neural model for learning a dynamically updated speaker embedding in a conversational context.", "labels": [], "entities": []}, {"text": "Initial model training is unsuper-vised, using context-sensitive language generation as an objective, with the context being the conversation history.", "labels": [], "entities": [{"text": "context-sensitive language generation", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.6480075716972351}]}, {"text": "Further fine-tuning can leverage task-dependent supervised training.", "labels": [], "entities": []}, {"text": "The learned neural representation of speakers is shown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations.", "labels": [], "entities": [{"text": "content ranking", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6943175345659256}, {"text": "dialog act prediction", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6430661479632059}]}], "introductionContent": [{"text": "Representing language in context is key to improving natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8018127977848053}]}, {"text": "There area variety of useful contexts, including word history, related documents, author/speaker information, social context, knowledge graphs, visual or situational grounding, etc.", "labels": [], "entities": []}, {"text": "This paper addresses the problem of modeling the speaker.", "labels": [], "entities": []}, {"text": "Accounting for author/speaker variations has been shown to be useful in many NLP tasks, including language understanding, language generation (, human-computer dialog policy (, query completion (, comment recommendation () and more.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7845101654529572}, {"text": "language generation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7641106247901917}, {"text": "query completion", "start_pos": 177, "end_pos": 193, "type": "TASK", "confidence": 0.6795647293329239}]}, {"text": "In this work, we specifically focus on dialogs, including both human-computer (socialbot) and human-human conversations.", "labels": [], "entities": []}, {"text": "While many studies rely only on discrete metadata and/or demographic information, such information is not always available.", "labels": [], "entities": []}, {"text": "Thus, it is of interest to learn about the speaker from the language directly, as it relates to the person's interests and speaking style.", "labels": [], "entities": []}, {"text": "Motivated by the success of unsupervised contextualized representation learning for words and documents, our approach is to use unsupervised learning with a neural model of a speaker's dialog history.", "labels": [], "entities": []}, {"text": "The model uses latent speaker mode vectors for representing a speaker turn as in ( , which provides a framework for analysis of what the model learns about speaking style.", "labels": [], "entities": []}, {"text": "Further, the model is structured to allow a dynamic update of the speaker vector at each turn in a dialog, in order to capture changes overtime and improve the speaker representation with added data.", "labels": [], "entities": []}, {"text": "The speaker embeddings can be used as context in conversational language understanding tasks, e.g., as an additional input in dialog policy prediction in human-computer dialogs or in understanding dialog acts in human-human dialogs.", "labels": [], "entities": [{"text": "conversational language understanding tasks", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.7140870913863182}, {"text": "dialog policy prediction", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6713785330454508}]}, {"text": "In the supervised training of such tasks, the speaker model can be fine-tuned.", "labels": [], "entities": []}, {"text": "This work makes two primary contributions.", "labels": [], "entities": []}, {"text": "First, we propose a neural model for learning dynamically updated speaker embeddings in conversational interactions.", "labels": [], "entities": []}, {"text": "The model training is unsupervised, relying on only the speaker's conversation history rather than meta information (e.g., age, gender) or audio signals which may not be available in a privacy-sensitive situation.", "labels": [], "entities": []}, {"text": "The model also has a learnable component for analyzing the latent modes of the speaker, which can be helpful for aligning the learned characteristics of a speaker with the human-interpretable factors.", "labels": [], "entities": []}, {"text": "Second, we use the learned dynamic speaker embed-", "labels": [], "entities": []}], "datasetContent": [{"text": "As described in \u00a73.2, we use the first 5 user turns to derive the user embedding vector fora conversation.", "labels": [], "entities": []}, {"text": "We compare our dynamic speaker model with three other unsupervised models.", "labels": [], "entities": []}, {"text": "DynamicSpeakerModel: For the proposed dynamic speaker model, we concatenate the speaker state vector ht and the local speaker mode vector  Here, we use all information accumulated before the system turn of suggesting the topic to build the corresponding user embedding vector.", "labels": [], "entities": []}, {"text": "Since the UtteranceLDA is not as effective based on static embedding experiments, we only consider extending UtteranceAE and TopicDecisionEncoder models for comparison here.", "labels": [], "entities": []}, {"text": "In our experiments, we compare three settings for using the dynamic speaker model.", "labels": [], "entities": []}, {"text": "In the pre-train setting, the dynamic speaker model is trained on the SwDA data without the dialog act labels.", "labels": [], "entities": [{"text": "SwDA data", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.8289781212806702}]}, {"text": "We then freeze the model when training the tagging LSTM.", "labels": [], "entities": [{"text": "tagging LSTM", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.7783856093883514}]}, {"text": "In contrast, in the pretrain + fine-tune setting, the dynamic speaker model is fine-tuned together with the tagging LSTM.", "labels": [], "entities": []}, {"text": "Finally, in the pre-train w/Fisher + fine-tune setting, the dynamic speaker model is pre-trained on the combination of SwDA and Fisher datasets, and then fine-tuned together with the tagging LSTM on the SwDA dataset.", "labels": [], "entities": [{"text": "Fisher datasets", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.7059581428766251}, {"text": "SwDA dataset", "start_pos": 203, "end_pos": 215, "type": "DATASET", "confidence": 0.9434692561626434}]}, {"text": "For all three settings, we use the same vocabulary V of size 21K which combines all tokens from the SwDA training set and those appearing at least 5 time in the Fisher corpus.", "labels": [], "entities": [{"text": "SwDA training set", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.7302126785119375}, {"text": "Fisher corpus", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.7769520580768585}]}, {"text": "We compare our results to best published results.", "labels": [], "entities": []}, {"text": "In), a convolutional neural network (CNN) is used to encode utterances.", "labels": [], "entities": []}, {"text": "A recurrent neural network (RNN) is then applied on top of the CNN to encode both utterances and speaker label information for predicting the dialog acts.", "labels": [], "entities": []}, {"text": "propose a discourse-aware RNN LM by treating", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.7244775295257568}]}], "tableCaptions": [{"text": " Table 1: Data statistics of the topic decision dataset.", "labels": [], "entities": []}, {"text": " Table 2: Test set results (in %) for topic decision pre- dictions using static user embeddings.", "labels": [], "entities": []}, {"text": " Table 3: Test set results (in %) for topic decision pre- dictions using dynamic user embeddings.  *  : The im- provement of DynamicSpeakerModel over both Top- icDecisionLSTM and UtteranceAE + LSTM is statis- tically significant based on both t-test and McNemar's  test (p < .001).", "labels": [], "entities": [{"text": "McNemar's  test", "start_pos": 254, "end_pos": 269, "type": "DATASET", "confidence": 0.7562683820724487}]}, {"text": " Table 5: Test set accuracy for SwDA dialog act clas- sification.  *  : The improvement of pre-train w/ Fisher  + fine-tune is statistically significant over pre-train +  fine-tune based on McNemar's test (p < .001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9917628765106201}, {"text": "SwDA dialog act clas- sification", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7889818648497263}]}]}