{"title": [{"text": "Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a multi-task learning framework to learn a joint Machine Reading Comprehension (MRC) model that can be applied to a wide range of MRC tasks in different domains.", "labels": [], "entities": [{"text": "Machine Reading Comprehension (MRC)", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.6590565741062164}]}, {"text": "Inspired by recent ideas of data selection in machine translation, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss.", "labels": [], "entities": [{"text": "data selection", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.6972785294055939}, {"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6668481379747391}]}, {"text": "Empirical study shows that our approach can be applied to many existing MRC models.", "labels": [], "entities": [{"text": "MRC", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9555642604827881}]}, {"text": "Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets.", "labels": [], "entities": [{"text": "MRC benchmark datasets", "start_pos": 142, "end_pos": 164, "type": "DATASET", "confidence": 0.7578035493691763}]}, {"text": "We release our code at https://github.com/ xycforgithub/MultiTask-MRC.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Reading Comprehension (MRC) has gained growing interest in the research community ().", "labels": [], "entities": [{"text": "Machine Reading Comprehension (MRC)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8815646866957346}]}, {"text": "In an MRC task, the machine reads a text passage and a question, and generates (or selects) an answer based on the passage.", "labels": [], "entities": [{"text": "MRC task", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.9356053471565247}]}, {"text": "This requires the machine to possess strong comprehension, inference and reasoning capabilities.", "labels": [], "entities": []}, {"text": "Over the past few years, there has been much progress in building end-toend neural network models () for MRC.", "labels": [], "entities": [{"text": "MRC", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9781765341758728}]}, {"text": "However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (,b) with around 10M parameters).", "labels": [], "entities": [{"text": "MRC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9397985339164734}, {"text": "MS MARCO", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.6431078761816025}]}, {"text": "To prevent over-fitting, recently there have been some studies on using pre-trained word embeddings () and contextual embeddings in the MRC model training, as well as back-translation approaches () for data augmentation.", "labels": [], "entities": [{"text": "MRC", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.7972970604896545}]}, {"text": "Multi-task learning) is a widely studied area in machine learning, aiming at better model generalization by combining training datasets from multiple tasks.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8072234094142914}]}, {"text": "In this work, we explore a multi-task learning (MTL) framework to enable the training of one universal model across different MRC tasks for better generalization.", "labels": [], "entities": [{"text": "MRC tasks", "start_pos": 126, "end_pos": 135, "type": "TASK", "confidence": 0.9089343547821045}]}, {"text": "Intuitively, this multi-task MRC model can be viewed as an implicit data augmentation technique, which can improve generalization on the target task by leveraging training data from auxiliary tasks.", "labels": [], "entities": [{"text": "MRC", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8261458277702332}]}, {"text": "We observe that merely adding more tasks cannot provide much improvement on the target task.", "labels": [], "entities": []}, {"text": "Thus, we propose two MTL training algorithms to improve the performance.", "labels": [], "entities": [{"text": "MTL training", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8855497539043427}]}, {"text": "The first method simply adopts a sampling scheme, which randomly selects training data from the auxiliary tasks controlled by a ratio hyperparameter; The second algorithm incorporates recent ideas of data selection in machine translation (van der.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 218, "end_pos": 237, "type": "TASK", "confidence": 0.7617445290088654}]}, {"text": "It learns the sample weights from the auxiliary tasks automatically through language models.", "labels": [], "entities": []}, {"text": "Prior to this work, many studies have used upstream datasets to augment the performance of MRC models, including word embedding), language models (ELMo) () and machine translation (.", "labels": [], "entities": [{"text": "MRC", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9379039406776428}, {"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.8264918923377991}]}, {"text": "These methods aim to obtain a robust semantic encoding of both passages and questions.", "labels": [], "entities": []}, {"text": "Our MTL method is orthogonal to these methods: rather than enriching semantic embedding with external knowledge, we leverage existing MRC datasets across different domains, which help make the whole comprehension process more robust and universal.", "labels": [], "entities": [{"text": "MTL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9513756036758423}]}, {"text": "Our experiments show that MTL can bring further performance boost when combined with contextual representations from pre-trained language models, e.g.,).", "labels": [], "entities": [{"text": "MTL", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9410081505775452}]}, {"text": "To the best of our knowledge, this is the first work that systematically explores multi-task learning for MRC.", "labels": [], "entities": [{"text": "MRC", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9797008037567139}]}, {"text": "In previous methods that use language models and word embedding, the external embedding/language models are pre-trained separately and remain fixed during the training of the MRC model.", "labels": [], "entities": []}, {"text": "Our model, on the other hand, can be trained with more flexibility on various MRC tasks.", "labels": [], "entities": [{"text": "MRC tasks", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.9303581118583679}]}, {"text": "MTL is also faster and easier to train than embedding/LM methods: our approach requires no pre-trained models, whereas back translation and ELMo both rely on large models that would need days to train on multiple.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8052036166191101}, {"text": "back translation", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.6891742795705795}]}, {"text": "We validate our MTL framework with two state-of-the-art models on four datasets from different domains.", "labels": [], "entities": []}, {"text": "Experiments show that our methods lead to a significant performance gain over single-task baselines on SQuAD (, and WhoDid-What (, while achieving state-of-the-art performance on the latter two.", "labels": [], "entities": [{"text": "WhoDid-What", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.7718736529350281}]}, {"text": "For example, on NewsQA (, our model surpassed human performance by 13.4 (46.5 vs 59.9) and 3.2 (72.6 vs 69.4) absolute points in terms of exact match and F1.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9884527325630188}, {"text": "exact", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9903884530067444}, {"text": "match", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.5090025663375854}, {"text": "F1", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9988528490066528}]}, {"text": "The contribution of this work is three-fold.", "labels": [], "entities": []}, {"text": "First, we apply multi-task learning to the MRC task, which brings significant improvements over single-task baselines.", "labels": [], "entities": [{"text": "MRC task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.9096985161304474}]}, {"text": "Second, the performance gain from MTL can be easily combined with existing methods to obtain further performance gain.", "labels": [], "entities": [{"text": "MTL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8395960927009583}]}, {"text": "Third, the proposed sampling and re-weighting scheme can further improve the multi-task learning performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "SQuAD on SQuAD includes only one or two words inmost cases).", "labels": [], "entities": []}, {"text": "We instead just use the length of answers as a signal for scores.", "labels": [], "entities": []}, {"text": "Let l k a be the length of A k , the cross-entropy answer score is defined as: where freq C is the frequency of answer lengths in task C \u2208 {1, k}.", "labels": [], "entities": [{"text": "freq C", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9812629818916321}]}, {"text": "The cross entropy scores are then normalized overall samples in task C to create a comparable metric across all auxiliary tasks: for C \u2208 {1, 2, ..., K}.", "labels": [], "entities": []}, {"text": "For C \u2208 {2, ..., K}, the maximum and minimum are taken overall samples in task k.", "labels": [], "entities": []}, {"text": "For C = 1 (target task), they are taken overall available samples.", "labels": [], "entities": []}, {"text": "Intuitively, H C,Q and H C,A represents the similarity of text Q, A to task C; a low H C,Q (resp. H C,A ) means that Q k (resp.", "labels": [], "entities": []}, {"text": "A k ) is easy to predict and similar to C, and vice versa.", "labels": [], "entities": []}, {"text": "We would like samples that are most similar from data in the target domain (low H 1 ), and most different (informative) from data in the auxiliary task (high H k ).", "labels": [], "entities": []}, {"text": "We thus compute the following cross-entropy difference for each external data: fork \u2208 {2, ..., K}.", "labels": [], "entities": []}, {"text": "Note that a low CED score indicates high importance.", "labels": [], "entities": [{"text": "CED score", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9844274818897247}, {"text": "importance", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9628965258598328}]}, {"text": "Finally, we transform the scores to weights by taking negative, and normalize between [0, 1]: Here the maximum and minimum are taken overall available samples and task.", "labels": [], "entities": []}, {"text": "Our training algorithm is the same as Algorithm 1, but for minibatch b we instead use the loss in step 6.", "labels": [], "entities": []}, {"text": "We define CED (Q 1 , A 1 ) \u2261 1 for all target samples (P 1 , Q 1 , A 1 ).", "labels": [], "entities": [{"text": "CED (Q 1 , A 1 )", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.819081611931324}]}, {"text": "Our experiments are designed to answer the following questions on multi-task learning for MRC: 1.", "labels": [], "entities": [{"text": "MRC", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9616469740867615}]}, {"text": "Can we improve the performance of existing MRC systems using multi-task learning?", "labels": [], "entities": [{"text": "MRC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9753271341323853}]}, {"text": "2. How does multi-task learning affect the performance if we combine it with other external data?", "labels": [], "entities": []}, {"text": "3. How does the learning algorithm change the performance of multi-task MRC?", "labels": [], "entities": [{"text": "MRC", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.7631576657295227}]}, {"text": "4. How does our method compare with existing MTL methods?", "labels": [], "entities": []}, {"text": "We first present our experiment details and results for MT-SAN.", "labels": [], "entities": [{"text": "MT-SAN", "start_pos": 56, "end_pos": 62, "type": "TASK", "confidence": 0.7746477127075195}]}, {"text": "Then, we provide a comprehensive study on the effectiveness of various MTL algorithms in Section 5.4.", "labels": [], "entities": [{"text": "MTL", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9613761901855469}]}, {"text": "At last, we provide some additional results on combining MTL with DrQA ( to show the flexibility of our approach 1 .  We conducted experiments on SQuAD: Performance of our method to train SAN in multi-task setting, competing published results, leaderboard results and human performance, on SQuAD dataset (single model).", "labels": [], "entities": [{"text": "DrQA", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9142639636993408}, {"text": "SQuAD dataset", "start_pos": 290, "end_pos": 303, "type": "DATASET", "confidence": 0.9245751202106476}]}, {"text": "Note that BERT uses a much larger language model, and is not directly comparable with our results.", "labels": [], "entities": [{"text": "BERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9604275822639465}]}, {"text": "We expect our test performance is roughly similar or a bit higher than our dev performance, as is the case with other competing models.", "labels": [], "entities": []}, {"text": "We mostly focus on span-based datasets for MT-SAN, namely SQuAD, NewsQA, and MS MARCO.", "labels": [], "entities": [{"text": "MT-SAN", "start_pos": 43, "end_pos": 49, "type": "TASK", "confidence": 0.7652175426483154}, {"text": "SQuAD", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8782829642295837}, {"text": "NewsQA", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.921035885810852}, {"text": "MS MARCO", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.80926713347435}]}, {"text": "We convert MS MARCO into an answer-span dataset to be consistent with SQuAD and NewsQA, following ().", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7679175734519958}, {"text": "SQuAD", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8944117426872253}, {"text": "NewsQA", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9459621906280518}]}, {"text": "For each question, we search for the best span using ROUGE-L score in all passage texts and use the span to train our model.", "labels": [], "entities": [{"text": "ROUGE-L score", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9774421453475952}]}, {"text": "We exclude questions with maximal ROUGE-L scoreless than 0.5 during training.", "labels": [], "entities": [{"text": "ROUGE-L scoreless", "start_pos": 34, "end_pos": 51, "type": "METRIC", "confidence": 0.9256460070610046}]}, {"text": "For evaluation, we use our model to find a span in all passages.", "labels": [], "entities": []}, {"text": "The prediction score is multiplied with the ranking score, trained following's method to determine the final answer.", "labels": [], "entities": []}, {"text": "We train our networks using algorithms in Section 4, using SQuAD as the target task.", "labels": [], "entities": []}, {"text": "For experiments with two datasets, we use Algorithm 2; for experiments with three datasets we find the re-weighting mechanism in Section 4.2 to have a better performance (a detailed comparison will be presented in Section 5.4).", "labels": [], "entities": [{"text": "Algorithm", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9503995180130005}]}, {"text": "For generating sample weights, we build a LSTM language model on questions following the implementation of with the same hyperparameters.", "labels": [], "entities": []}, {"text": "We only keep the 10,000 most frequent words, and replace the other words with a special out-of-vocabulary token.", "labels": [], "entities": []}, {"text": "Parameters of MT-SAN are mostly the same as in the original paper ().", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9679132699966431}, {"text": "MT-SAN", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.5743876099586487}]}, {"text": "We utilize spaCy 2 to tokenize the text and generate part-ofspeech and named entity labels.", "labels": [], "entities": []}, {"text": "We use a 2-layer BiLSTM with 125 hidden units as the BiLSTM throughout the model.", "labels": [], "entities": []}, {"text": "During training, we drop the activation of each neuron with 0.3 probability.", "labels": [], "entities": []}, {"text": "For optimization, we use Adamax () with a batch size of 32 and a learning rate of 0.002.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9599380195140839}]}, {"text": "For prediction, we compute an exponential moving average (EMA, of model parameters with a decay rate of 0.995 and use it to compute the model performance.", "labels": [], "entities": [{"text": "prediction", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9705365896224976}, {"text": "exponential moving average (EMA,", "start_pos": 30, "end_pos": 62, "type": "METRIC", "confidence": 0.8097727497418722}]}, {"text": "For experiments with ELMo, we use the model implemented by AllenNLP 3 . We truncate passage to contain at most 1000 tokens during training and eliminate those data with answers located after the 1000th token.", "labels": [], "entities": [{"text": "AllenNLP 3", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9583867788314819}]}, {"text": "The training converges in around 50 epochs for models without ELMo (similar to the single-task SAN); For models with ELMo, the convergence is much faster (around 30 epochs).", "labels": [], "entities": []}, {"text": "To demonstrate the flexibility of our approach, we also adapt DrQA () into our MTL framework.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9371584057807922}]}, {"text": "We only test DrQA using the basic Algorithm 2, since our goal is mainly to test the MTL framework.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9512052536010742}, {"text": "MTL framework", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.7903203070163727}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets. Some numbers come from (Sugawara et al., 2017).", "labels": [], "entities": []}, {"text": " Table 2. By using  MTL on SQuAD and NewsQA, we can improve  the exact-match (EM) and F1 score by (2%, 1.5%),  respectively, both with and without ELMo. The  similar gain indicates that our method is orthogo- nal to ELMo. Note that our single-model perfor- mance is slightly higher than the original SAN, by  incorporating EMA and highway networks. By in- corporating with multi-task learning, it further im- proves the performance. The performance gain by  adding MARCO is relatively smaller, with 1% in  EM and 0.5% in F1. We conjecture that MARCO  is less helpful due to its differences in both the  question and answer style. For example, ques- tions in MS MARCO are real web search queries,  which are short and may have typos or abbrevia- tions; while questions in SQuAD and NewsQA are  more formal and well written.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9448677897453308}, {"text": "exact-match", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9585850834846497}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9982280135154724}]}, {"text": " Table 3. The performance gain with multi- task learning is even larger on NewsQA, with over  2% in both EM and F1. Experiments with and  without ELMo give similar results. What is worth  noting is that our approach not only achieves new  state-of-art results with a large margin but also sur- passes human performance on NewsQA.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.9586271643638611}, {"text": "F1", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9612954258918762}, {"text": "NewsQA", "start_pos": 322, "end_pos": 328, "type": "DATASET", "confidence": 0.9644463658332825}]}, {"text": " Table 3: Performance of our method to train SAN  in multi-task setting, with published results and hu- man performance on NewsQA dataset. All SAN results  are from our models. \"S+N\" means jointly training on  SQuAD and NewsQA References: 1 : implemented by  Trischler et al. (2017). 2 :Weissenborn et al.(2017). 3 :  Kundu and Ng(2018).", "labels": [], "entities": [{"text": "NewsQA dataset", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9952514171600342}, {"text": "NewsQA", "start_pos": 220, "end_pos": 226, "type": "DATASET", "confidence": 0.8859375715255737}]}, {"text": " Table 4: Performance of our method to train SAN in  multi-task setting, competing published results and hu- man performance, on MS MARCO dataset. The scores  stand for (BLEU-1, ROUGE-L) respectively. All SAN  results are our results. \"3 dataset\" means we train using  SQuAD+NewsQA+MARCO. References: 1 : (Weis- senborn et al., 2017).", "labels": [], "entities": [{"text": "MS MARCO dataset", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.8888393044471741}, {"text": "BLEU-1", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9988019466400146}, {"text": "ROUGE-L", "start_pos": 178, "end_pos": 185, "type": "METRIC", "confidence": 0.9844479560852051}]}, {"text": " Table 5: Performance of MT-SAN on SQuAD Dev and  WDW test set. Accuracy is used to evaluate WDW.  \"S+W\" means jointly training on SQuAD and WDW.", "labels": [], "entities": [{"text": "MT-SAN", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.7359145879745483}, {"text": "WDW test set", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9283355474472046}, {"text": "Accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9921354055404663}, {"text": "WDW", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.7638970017433167}, {"text": "WDW", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.8842411041259766}]}, {"text": " Table 6: Comparison of methods to use external data.  BT stands for back translation (Yu et al., 2018).", "labels": [], "entities": [{"text": "BT", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9187278151512146}]}, {"text": " Table 8: Scores for examples from NewsQA and MS MARCO and average scores for specific groups of samples.  CED is as in", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9859967827796936}, {"text": "MS MARCO", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.7575110793113708}]}, {"text": " Table 9: Single model performance of our method to train DrQA on multi-task setting, as well as state-of-the- art (SOTA) results and human performance. SQuAD and NewsQA performance are measured by (EM, F1), and  WDW by accuracy percentage. All results are on development set unless otherwise noted. Published SOTA results  come from (Wang et al., 2018a; Hu et al., 2018; Kundu and Ng, 2018; Yang et al., 2016) respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.8217604756355286}, {"text": "WDW", "start_pos": 213, "end_pos": 216, "type": "METRIC", "confidence": 0.9626511931419373}, {"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9982808828353882}]}]}