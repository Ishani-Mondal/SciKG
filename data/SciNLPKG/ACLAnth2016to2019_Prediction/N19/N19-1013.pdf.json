{"title": [{"text": "Wifi keeps dropping on 5Ghz network", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete.", "labels": [], "entities": []}, {"text": "We propose that modeling hypothetical answers (to clarification questions) as latent variables can guide our approach into generating more useful clarification questions.", "labels": [], "entities": []}, {"text": "We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question.", "labels": [], "entities": []}, {"text": "We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.", "labels": [], "entities": []}], "introductionContent": [{"text": "A goal of natural language processing is to develop techniques that enable machines to process naturally occurring language.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6951583027839661}]}, {"text": "However, not all language is clear and, as humans, we may not always understand each other; in cases of gaps or mismatches in knowledge, we tend to ask questions (.", "labels": [], "entities": []}, {"text": "In this work, we focus on the task of automatically generating clarification questions: questions that ask for information that is missing from a given linguistic context.", "labels": [], "entities": []}, {"text": "Our clarification question generation model builds on the sequence-tosequence approach that has proven effective for several language generation tasks.", "labels": [], "entities": [{"text": "clarification question generation", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6508247256278992}, {"text": "language generation tasks", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.7814390162626902}]}, {"text": "Unfortunately, training a sequenceto-sequence model directly on (context, question) pairs yields questions that are highly generic 1 , corroborating a common finding in dialog systems ().", "labels": [], "entities": []}, {"text": "Our goal is to be able to generate clarification questions that are useful and specific.", "labels": [], "entities": []}, {"text": "To achieve this, we begin with a recent observation of, who consider the task of question reranking: a good clarification question is the one whose answer has a high utility, which they define as the likelihood that this question would lead to an answer that will make the context more complete ( \u00a72.3).", "labels": [], "entities": [{"text": "question reranking", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.770449161529541}]}, {"text": "Inspired by this, we construct a model that first generates a question given a context, and then generates a hypothetical answer to that question.", "labels": [], "entities": []}, {"text": "Given this (context, question, answer) triple, we train a utility calculator to estimate the usefulness of this question.", "labels": [], "entities": []}, {"text": "We then show that this utility calculator can be generalized using ideas for generative adversarial networks () for text (, wherein the utility calculator plays the role of the \"discriminator\" and the question generator is the \"generator\" ( \u00a72.2), which we train using the MIXER algorithm (.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two datasets: Amazon product descriptions () and Stack Exchange posts ().", "labels": [], "entities": []}, {"text": "Our two main contributions are: 1.", "labels": [], "entities": []}, {"text": "An adversarial training approach for generating clarification questions that models the utility of updating a context with an answer to the clarification question.", "labels": [], "entities": []}, {"text": "2. An empirical evaluation using both automatic metrics and human judgments to show that our adversarially trained model generates questions that are more useful and specific to the context than all the baseline models.", "labels": [], "entities": []}, {"text": "Product T-fal Nonstick Cookware Set, title 18 pieces, Red Product Easy non-stick 18pc set includes every description piece for your everyday meals.", "labels": [], "entities": [{"text": "Product T-fal Nonstick Cookware Set", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.7458849430084229}]}, {"text": "Exceptionally durable dishwasher safe cookware for easy cleanup.", "labels": [], "entities": []}, {"text": "Oven safe up to 350.F/177.C Question Are they induction compatible?", "labels": [], "entities": []}, {"text": "Answer They are aluminium so the answer is NO.", "labels": [], "entities": [{"text": "NO", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9975464940071106}]}, {"text": "Figure 1: Sample product description from Amazon paired with a clarification question and answer.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9264062643051147}]}], "datasetContent": [{"text": "We base our experimental design on the following research questions: 1.", "labels": [], "entities": []}, {"text": "Do generation models outperform simpler retrieval baselines?", "labels": [], "entities": []}, {"text": "2. Does optimizing the UTILITY reward improve over maximum likelihood training?", "labels": [], "entities": [{"text": "UTILITY reward", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8117903172969818}]}, {"text": "3. Does using adversarial training improve over optimizing the pretrained UTILITY?", "labels": [], "entities": [{"text": "UTILITY", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.8186172246932983}]}, {"text": "4. How do the models perform when evaluated for nuances such as specificity & usefulness?", "labels": [], "entities": []}, {"text": "We evaluate our model on two datasets.", "labels": [], "entities": []}, {"text": "In this dataset, context is a product description on amazon.com combined with the product title, question is a clarification question asked to the product and answer is the seller's (or other users') reply to the question.", "labels": [], "entities": []}, {"text": "To obtain these data triples, we combine the Amazon question-answering dataset) with the Amazon reviews dataset  We evaluate initially with automated evaluation metrics, and then more substantially with crowdsourced human judgments.", "labels": [], "entities": [{"text": "Amazon question-answering dataset", "start_pos": 45, "end_pos": 78, "type": "DATASET", "confidence": 0.840803861618042}, {"text": "Amazon reviews dataset", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.9210831721623739}]}, {"text": "In this section, we describe the details of our experimental setup.", "labels": [], "entities": []}, {"text": "We tokenize and lowercase all inputs (context, question and answers).", "labels": [], "entities": []}, {"text": "We set the max length of context to be 100, question to be 20 and answer to be 20.", "labels": [], "entities": []}, {"text": "We find that increasing the length of contexts (to 150 or 200) of question/ answer (to 40) yields similar results according to automatic metrics with increased experimentation time.", "labels": [], "entities": []}, {"text": "Our sequence-to-sequence model (Section 2.1) operates on word embeddings which are pretrained on in domain data using Glove).", "labels": [], "entities": []}, {"text": "As frequently used in previous work on neural network modeling, we use an embeddings of size 200 and a vocabulary with cutoff frequency set to 10.", "labels": [], "entities": [{"text": "neural network modeling", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.729387124379476}]}, {"text": "During train time, we use teacher forcing.", "labels": [], "entities": []}, {"text": "During test time, we use beam search decoding with beam size 5.", "labels": [], "entities": []}, {"text": "We use a hidden layer of size two for both the encoder and decoder recurrent neural network models with size of hidden unit set to 100.", "labels": [], "entities": []}, {"text": "We use a dropout of 0.5 and learning ratio of 0.0001.", "labels": [], "entities": [{"text": "learning ratio", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.980722188949585}]}, {"text": "In the MIXER model, we start with \u2206 = T and decrease it by 2 for every epoch (we found decreasing \u2206 to 0 is ineffective for our task, hence we stop at 2).", "labels": [], "entities": []}, {"text": "First half of shows the system generated questions for three product descriptions in the Amazon dataset.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.9731273353099823}]}, {"text": "In the first example, the product is a shower curtain.", "labels": [], "entities": []}, {"text": "The Reference question is specific and highly useful.", "labels": [], "entities": [{"text": "Reference", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.8478991389274597}]}, {"text": "Lucene, on the other hand, picks a moderately specific (\"how to clean it?\") but useful question.", "labels": [], "entities": []}, {"text": "MLE model generates a generic but useful \"is it waterproof?\".", "labels": [], "entities": [{"text": "MLE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6590688824653625}]}, {"text": "Max-Utility generates comparatively a much longer question but in doing so loses out on relevance.", "labels": [], "entities": [{"text": "relevance", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9466964602470398}]}, {"text": "This behavior of generating two unrelated sentences is observed quite a few times in both Max-Utility and GAN-Utility models.", "labels": [], "entities": []}, {"text": "This suggests that these models, in trying to be very specific, end up losing out on relevance.", "labels": [], "entities": []}, {"text": "In the same example, GAN-Utility also generates a fairly long question which, although awkwardly phrase, is quite specific and useful.", "labels": [], "entities": []}, {"text": "In the second example, the product is a Duvet Cover Set.", "labels": [], "entities": []}, {"text": "Both Reference and Lucene questions here are examples of questions that are pretty much useful only to the person asking the question.", "labels": [], "entities": []}, {"text": "We find many such questions in both Reference and Lucene outputs which is the main reason for the comparatively lower usefulness scores for their outputs.", "labels": [], "entities": [{"text": "Lucene outputs", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.7253111898899078}]}, {"text": "All three of our models generate irrelevant questions since the product description explicitly says that the set is full size.", "labels": [], "entities": []}, {"text": "In the last example, the product is a set of mopping clothes.", "labels": [], "entities": []}, {"text": "Reference question is quite specific but has low usefulness.", "labels": [], "entities": []}, {"text": "Lucene picks an irrelevant question.", "labels": [], "entities": []}, {"text": "MLE and Max-Utility generate highly specific and useful questions.", "labels": [], "entities": [{"text": "MLE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.401763379573822}]}, {"text": "GAN-Utility generates an ungrammatical question by repeating the last word many times.", "labels": [], "entities": []}, {"text": "We observe this behavior quite a few times in the outputs of both Max-Utility and GAN-Utility models suggesting that our sequence-to-sequence models are not very good at maintaining long range dependencies.", "labels": [], "entities": []}, {"text": "Second half of shows the system generated questions for three posts from the Stack Exchange dataset.", "labels": [], "entities": [{"text": "Stack Exchange dataset", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8968113660812378}]}, {"text": "The first example is of a post where someone describes their issue of not being able to recover from their boot.", "labels": [], "entities": []}, {"text": "Reference and Lucene questions are useful.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.6366274356842041}]}, {"text": "MLE generates a generic question that is not very useful.", "labels": [], "entities": [{"text": "MLE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7236120700836182}]}, {"text": "Max-Utility generates a useful question but has slight ungrammaticality in it.", "labels": [], "entities": []}, {"text": "GAN-Utility, on the other hand, generates a specific and an useful question.", "labels": [], "entities": [{"text": "GAN-Utility", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7353673577308655}]}, {"text": "In the second example, again Reference and Lucene questions are useful.", "labels": [], "entities": [{"text": "Reference", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.7021700143814087}]}, {"text": "MLE generates a generic question.", "labels": [], "entities": []}, {"text": "Max-Utility and GAN-Utility both generate fairly specific question but contain unknown tokens.", "labels": [], "entities": []}, {"text": "The Stack Exchange dataset contains several technical terms leading to along tail in the vocabulary.", "labels": [], "entities": [{"text": "Stack Exchange dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7554280757904053}]}, {"text": "Owing to this, we find that both Max-Utility and GAN-Utility models generate many instances of questions with unknown tokens.", "labels": [], "entities": []}, {"text": "In the third example, the Reference question is very generic.", "labels": [], "entities": [{"text": "Reference", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.8679530024528503}]}, {"text": "Lucene asks a relevant question.", "labels": [], "entities": []}, {"text": "MLE again generates a generic question.", "labels": [], "entities": [{"text": "MLE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7528882622718811}]}, {"text": "Both Max-Utility and GAN-Utility generate specific and relevant questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement on the five criteria  used in human-based evaluation.", "labels": [], "entities": []}, {"text": " Table 2: DIVERSITY as measured by the proportion of unique trigrams in model outputs. Bigrams and unigrams  follow similar trends. BLEU and METEOR scores using up to 10 references for the Amazon dataset and up to  six references for the StackExchange dataset. Numbers in bold are the highest among the models. All results for  Amazon are on the entire test set whereas for StackExchange they are on the 500 instances of the test set that have  multiple references.", "labels": [], "entities": [{"text": "DIVERSITY", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9710407257080078}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9991118311882019}, {"text": "METEOR", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9970101118087769}, {"text": "Amazon dataset", "start_pos": 189, "end_pos": 203, "type": "DATASET", "confidence": 0.9648310542106628}, {"text": "StackExchange dataset", "start_pos": 238, "end_pos": 259, "type": "DATASET", "confidence": 0.9062663316726685}]}, {"text": " Table 3: Results of human judgments on model generated questions on 300 sample Home & Kitchen product  descriptions. Numeric range corresponds to the options described in  \u00a73.3. The difference between the bold and the  non-bold numbers is statistically significant with p <0.05. Reference is excluded in the significance calculation.", "labels": [], "entities": []}, {"text": " Table 4: Example outputs from each of the systems for two product descriptions along with the usefulness and the  specificity score given by human annotators.", "labels": [], "entities": []}]}