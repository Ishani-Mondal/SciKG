{"title": [{"text": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9040868878364563}, {"text": "Multi-Round Reasoning in Visual Dialog", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.6345102727413178}]}], "abstractContent": [{"text": "Visual Dialog is a multimodal task of answering a sequence of questions grounded in an image (using the conversation history as context).", "labels": [], "entities": [{"text": "Visual Dialog", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6106716990470886}, {"text": "answering a sequence of questions grounded in an image", "start_pos": 38, "end_pos": 92, "type": "TASK", "confidence": 0.6654328041606479}]}, {"text": "It entails challenges in vision, language, reasoning, and grounding.", "labels": [], "entities": []}, {"text": "However , studying these subtasks in isolation on large, real datasets is infeasible as it requires prohibitively-expensive complete annotation of the 'state' of all images and dialogs.", "labels": [], "entities": []}, {"text": "We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning in visual dialog.", "labels": [], "entities": []}, {"text": "Specifically, we construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset.", "labels": [], "entities": [{"text": "CLEVR dataset", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.9425053894519806}]}, {"text": "This combination results in a dataset where all aspects of the visual dialog are fully annotated.", "labels": [], "entities": []}, {"text": "In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images , totaling to 4.25M question-answer pairs.", "labels": [], "entities": []}, {"text": "We use CLEVR-Dialog to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance).", "labels": [], "entities": []}, {"text": "This is the first analysis of its kind for visual dialog models that was not possible without this dataset.", "labels": [], "entities": []}, {"text": "We hope the findings from CLEVR-Dialog will help inform the development of future models for visual dialog.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8923216462135315}]}, {"text": "Our code and dataset are publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The focus of this work is on intelligent systems that can see (perceive their surroundings through vision), talk (hold a visually grounded dialog), and reason (store entities in memory as a dialog progresses, refer back to them as appropriate, count, compare, etc.).", "labels": [], "entities": []}, {"text": "Recent works have begun studying such systems under the umbrella of Visual Dialog (, where https://github.com/satwikkottur/clevr-dialog an agent must answer a sequence of questions grounded in an image.", "labels": [], "entities": [{"text": "Visual Dialog", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.6741721034049988}]}, {"text": "As seen in, this entails challenges in -vision (e.g., identifying objects and their attributes in the image), language/reasoning (e.g., keeping track of and referencing previous conversation via memory), and grounding (e.g., grounding textual entities in the image).", "labels": [], "entities": []}, {"text": "In order to train and evaluate agents for Visual Dialog, collected a large dataset of human-human dialog on real images collected between pairs of workers on Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Visual Dialog", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.6962709724903107}, {"text": "Amazon Mechanical Turk (AMT)", "start_pos": 158, "end_pos": 186, "type": "DATASET", "confidence": 0.942346453666687}]}, {"text": "While such large-scale realistic datasets enable new lines of research, it is difficult to study the different challenges (vision, language, reasoning, grounding) in isolation or to breakdown the performance of systems over different challenges to identify bottlenecks, because that would require prohibitively-expensive complete annotation of the 'state' of all images and dialogs (all entities, coreferences, etc.).", "labels": [], "entities": []}, {"text": "In this work, we draw inspiration from, and develop a large diagnostic dataset-CLEVR-Dialog-for studying and benchmarking multi-round reasoning in visuallygrounded dialog.", "labels": [], "entities": []}, {"text": "Each CLEVR image is synthetically rendered by a particular scene graph) and thus, is by construction exhaustively annotated.", "labels": [], "entities": []}, {"text": "We construct a dialog grammar that is grounded in these scene graphs.", "labels": [], "entities": []}, {"text": "Specifically, similar to, we view dialog generation as communication between an Answerer (A-er) who can 'see' the image and has the complete scene graph (say S a ), and a Questioner (Q-er), who does not 'see' the image and is trying to reconstruct the scene graph over rounds of dialog (say St q ).", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8702800273895264}]}, {"text": "As illustrated in, the dialog begins by A-er providing a grounded caption for the image, which conveys some but not all information about S a . The Q-er builds a partial scene graph S 0 q based on the caption, and follows up by asking questions: CLEVR-Dialog: we view dialog generation as communication between an Answerer (A-er) who can 'see' the image I and has the complete scene graph S a (far right), and a Questioner (Q-er), who does not 'see' the image.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 268, "end_pos": 285, "type": "TASK", "confidence": 0.8117806017398834}]}, {"text": "A-er begins the dialog with a grounded caption ('A cylinder is next to a yellow object').", "labels": [], "entities": []}, {"text": "The Q-er converts this caption into a partial scene graph S 0 q (far left, top), follows up with a question grounded in S 0 q ('What shape is the object?'), which the A-er answers, and the dialog progresses.", "labels": [], "entities": []}, {"text": "Questions at round tare generated based solely on St q , i.e., without looking at I or S a , which mimics real-life scenarios of visual dialog.", "labels": [], "entities": []}, {"text": "Note that while studying visual dialog on CLEVR-Dialog, models are forced to answer questions with just the image and dialog history as additional inputs, and do not have access to S a . grounded in S 0 q , which the A-er answers, and the dialog progresses.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9464787244796753}]}, {"text": "Our dialog grammar defines rules and templates for constructing this grounded dialog.", "labels": [], "entities": []}, {"text": "Note that A-er with access to S a (perfect vision) exists only during dialog generation to obtain ground truth answers.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7169913351535797}]}, {"text": "While studying visual dialog on CLEVR-Dialog, models are forced to answer questions with just the image and dialog history as additional inputs.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9248426556587219}]}, {"text": "In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for each of 70k (train) and 15k (val) CLEVR images, totaling to 3.5M (train) and 0.75M (val) question-answer pairs.", "labels": [], "entities": []}, {"text": "We benchmark several visual dialog models on CLEVR-Dialog as strong baselines for future work.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9270965456962585}]}, {"text": "The combination of CLEVR images (with full scene graph annotations) and our dialog grammar results in a dataset where all aspects of the visual dialog are fully annotated.", "labels": [], "entities": []}, {"text": "We use this to study one particularly difficult challenge in multi-dialog visual reasoning -of visual coreference resolution.", "labels": [], "entities": [{"text": "multi-dialog visual reasoning", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7061298290888468}, {"text": "visual coreference resolution", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.6204735537370046}]}, {"text": "A coreference arises when two or more phrases (coreferring phrases) in the conversation refer to the same entity (referent) in the image.", "labels": [], "entities": [{"text": "coreference", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.9628512859344482}]}, {"text": "For instance, in the question 'What about that cylinder?'", "labels": [], "entities": []}, {"text": "(Q3) from, the referent for the phrase 'that cylinder' can be inferred only after resolving the phrase correctly based on the dialog history, as there are multiple cylinders in the image.", "labels": [], "entities": []}, {"text": "We use CLEVRDialog to diagnose performance of different methods as a function of the history dependency (e.g., coreference distance-the number of rounds between successive mentions of the same object) and find that the performance of a state-of-art model (CorefNMN) is at least 30 points inferior for questions involving coreference resolution compared to those which do not, highlighting the challenging nature of our dataset.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 321, "end_pos": 343, "type": "TASK", "confidence": 0.8182698786258698}]}, {"text": "This is the first analysis of its kind for visual dialog that was simply not possible without this dataset.", "labels": [], "entities": []}, {"text": "We hope the findings from CLEVR-Dialog will help inform the development of future models for visual dialog.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8923216462135315}]}], "datasetContent": [{"text": "In this section, we describe the existing annotation for CLEVR images, then detail the generation process for CLEVR-Dialog, and present the dataset statistics in comparison to existing datasets.", "labels": [], "entities": []}, {"text": "the image (because if it did, it would not need to ask questions).", "labels": [], "entities": []}, {"text": "To mimic this setup, we condition our question generation at round t only on the partial scene graph St q that accumulates information received so far from the dialog history (and not on S a ).", "labels": [], "entities": [{"text": "question generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7093682289123535}]}, {"text": "Specifically, we use a set of caption {T Ci } and question {T Q i } templates, which serve as the structural units of our dialog grammar.", "labels": [], "entities": []}, {"text": "The role of the caption is to seed the dialog and initialize S 0 q . Each of the question templates is accompanied by a set of constraints on St q , which decide if a particular template can be selected at the current round.", "labels": [], "entities": []}, {"text": "For instance, a question 'What shape is the blue object?'", "labels": [], "entities": []}, {"text": "can be only be asked (generated) if the dialog so far has already mentioned a 'blue object', i.e., only if St q contains a (unique) 'blue object'.", "labels": [], "entities": []}, {"text": "The nature and difficulty of the dataset is highly dependent on these templates, thus making their selection crucial.", "labels": [], "entities": []}, {"text": "To this end, we carefully design four categories of caption templates: (a) Obj-unique mentions an object with unique set of attributes in the image, (b) Obj-count specifies the presence of a group of objects with common attributes, (c) Obj-extreme describes an object atone of the positional extremes of the image (right, left, fore, rear, center), (d) Obj-relation talks about the relationship between two objects along with their attributes in away that allows them to be uniquely identified in the complete scene graph S a . For the questions, we experiment with three different categories: (a) Count questions ask fora count of objects in the image satisfying specific conditions, e.g., 'How many objects share the same color as this one?', (b) Existence questions are yes/no binary questions that verify conditions in the image, e.g., 'Are there any other cubes?', and (c) Seek questions query attributes of objects, e.g., 'What color is that cylinder?'.", "labels": [], "entities": [{"text": "Obj-count", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.956951379776001}]}, {"text": "Note that CLEVRDialog represents not just a static dataset but also a recipe for constructing increasingly challenging grounded dialog by expanding this grammar.", "labels": [], "entities": []}, {"text": "Refer to the appendix for further details.", "labels": [], "entities": []}, {"text": "At a high level, dialog generation now 'simply' involves selecting a sequence of templates such that the accompanying constraints are satisfied by St q at all t.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9009391367435455}]}, {"text": "As a tractable approximation to this exponentially-large constraint satisfaction problem, we use beam search that finds a valid solution and enforces additional conditions to make the dialog interesting (see).", "labels": [], "entities": []}, {"text": "At every round of the dialog (after 3 rounds), we ensure that each of the question template types-count, existence, and seek-falls within a range (10% \u2212 20% for count/existence each, and 30% \u2212 60% for seek).", "labels": [], "entities": []}, {"text": "In addition, we identify independent questions that do not need history to answer them, e.g., 'How many objects are present in the image?', and limit their number to under 10%.", "labels": [], "entities": []}, {"text": "We found this to be effective both in terms of speed and dialog diversity.", "labels": [], "entities": [{"text": "speed", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9762188792228699}]}, {"text": "illustrates the diverse set of candidate questions generated at each round fora given image.", "labels": [], "entities": []}, {"text": "We compare CLEVR-Dialog to MNIST-Dialog and VisDial in Tab.", "labels": [], "entities": [{"text": "MNIST-Dialog", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.8887033462524414}]}, {"text": "1, but the key measure of coreference distance cannot be reported for VisDial as it is not annotated.", "labels": [], "entities": [{"text": "VisDial", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8822909593582153}]}, {"text": "Overall, CLEVR-Dialog has 3\u00d7 the questions and a striking 206\u00d7 the unique number of questions than MNIST-Dialog , indicating higher linguistic diversity.", "labels": [], "entities": [{"text": "MNIST-Dialog", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8140922784805298}]}, {"text": "CLEVR-Dialog questions are longer with a mean length of 10.6 compared to 8.9 for MNISTDialog.", "labels": [], "entities": [{"text": "CLEVR-Dialog questions", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8767255246639252}, {"text": "mean length", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.7039850652217865}, {"text": "MNISTDialog", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.929658830165863}]}, {"text": "Crucially, supporting our motivation, the mean distance (in terms of rounds) between the coreferring expressions in CLEVR-Dialog is 3.2\u00d7 compared to 1.0 in MNIST-Dialog.", "labels": [], "entities": [{"text": "MNIST-Dialog", "start_pos": 156, "end_pos": 168, "type": "DATASET", "confidence": 0.9129160046577454}]}, {"text": "Moreover, the distances (see) in CLEVR-Dialog vary (min of 1, max of 10), while it is constant (at 1) in MNIST-Dialog, making it easy for models to pick  Baselines.", "labels": [], "entities": [{"text": "MNIST-Dialog", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.812880277633667}]}, {"text": "To benchmark performance, we evaluate several models on CLEVR-Dialog.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9415012001991272}]}, {"text": "Random picks an answer at random.", "labels": [], "entities": []}, {"text": "Random-Q picks an answer at random among valid answers fora given question type (e.g., name of a color for color questions).", "labels": [], "entities": []}, {"text": "Further, we adapt the discriminative visual dialog models from     The best performing model, CorefNMN, outperforms Random-Q by 35%.", "labels": [], "entities": []}, {"text": "(b) History-agnostic models (LF-Q, LF-QI, NMN) also suffer in performance, highlighting the importance of history.", "labels": [], "entities": []}, {"text": "(c) Finally, we breakdown the performance of top-3 models on questions which depend on entire history (All), require coreference resolution (Coref ), and are history-independent (None), in.", "labels": [], "entities": []}, {"text": "We find that CorefNMN is 30% worse on Coref than None questions, signifying the complexity of CLEVRDialog as the former are qualitatively harder to answer than the latter.", "labels": [], "entities": []}, {"text": "(d) More interestingly, HRE-QIH, though inferior to CorefNMN on Coref, outperforms the latter on All questions ('How many other objects?') by around 20%.", "labels": [], "entities": [{"text": "HRE-QIH", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8204917311668396}, {"text": "Coref", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8390685319900513}]}, {"text": "A possible explanation is that the former, owing to its dialog-level RNN, captures global summaries more efficiently than the latter.", "labels": [], "entities": []}, {"text": "This is the first analysis of its kind for visual dialog that was simply not possible without this dataset.", "labels": [], "entities": []}, {"text": "Appendix provides a further analysis of model performances.", "labels": [], "entities": []}, {"text": "We proposed a large, synthetic dataset called CLEVR-Dialog, to study multiround reasoning in visual dialog, and in particular the challenge of visual coreference resolution.", "labels": [], "entities": [{"text": "visual coreference resolution", "start_pos": 143, "end_pos": 172, "type": "TASK", "confidence": 0.5951492587725321}]}, {"text": "We benchmarked several qualitatively different models from prior work on this dataset, which act as baselines for future work.", "labels": [], "entities": []}, {"text": "Our dataset opens the door to evaluate how well models do on visual coreference resolution, without the need to collect expensive annotations on real datasets.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7809188365936279}]}, {"text": "The appendix is organized as follows: \u2022 We begin with the description of CLEVR images in Sec.", "labels": [], "entities": []}, {"text": "B describes further details of the dialog generation, \u2022 Sec.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8564117252826691}]}, {"text": "C provides additional statistical analysis for CLEVR-Dialog, \u2022 Diagnostic model performance analysis in given in Sec.", "labels": [], "entities": [{"text": "CLEVR-Dialog", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7653718590736389}]}, {"text": "D, and finally \u2022 Implementation details can be found in Sec.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9786831140518188}]}, {"text": "E.  As noted in the main paper, an important characteristic of visual dialog that makes it suitable for practical applications is that the questioner does not 'see' the image (because if it did, it would not need to ask questions).", "labels": [], "entities": []}, {"text": "To mimic this setup, we condition our question generation at round t only on the partial scene graph St q that accumulates information received so far from the dialog history (and not on S a ).", "labels": [], "entities": [{"text": "question generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7093682289123535}]}, {"text": "Specifically, we use a set of caption {T Ci } and question {T Q i } templates (enumerated in Tab.", "labels": [], "entities": []}, {"text": "3), which serve as the basis for our dialog generation.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8649177253246307}]}, {"text": "Each of these templates in turn consists of primitives, composed together according to a generation grammar.", "labels": [], "entities": []}, {"text": "In what follows, we will first describe these primitives, discuss how they are used to generate a caption or a question at each round, and tie everything together to explain dialog generation in CLEVR-Dialog.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.8055858910083771}, {"text": "CLEVR-Dialog", "start_pos": 195, "end_pos": 207, "type": "DATASET", "confidence": 0.8946614861488342}]}, {"text": "The templates used to generate captions and questions are composed of intuitive and atomic operations called primitives.", "labels": [], "entities": []}, {"text": "Each of these primitives can have different instantiations depending on a parameter, and also take input arguments.", "labels": [], "entities": []}, {"text": "For example, all Filter primitives filter out objects from an input set of objects according to certain constraints.", "labels": [], "entities": []}, {"text": "In particular, Filter[color](blue) filters out blue objects from a given set of objects, while Filter[shape](sphere) filters out all spheres.", "labels": [], "entities": []}, {"text": "In our work, we use the following primitives: \u2022 Sample: sample an object/attribute, \u2022 Unique: identify unique objects/attributes, \u2022 Count: count the number of input objects, \u2022 Group: group objects based on attribute(s), \u2022 Filter: filter inputs according to a constraint, \u2022 Exist: check for existence of objects, \u2022 Relate: apply a relation (e.g., right of ).", "labels": [], "entities": [{"text": "Count", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9954854846000671}]}, {"text": "Note that each of these primitives inherently denotes a set of constraints, which when failed leads to a reset of the generation process for the current caption/question in the dialog.", "labels": [], "entities": []}, {"text": "For example, if the output of Filter[color](blue) is empty due to an absence of blue objects in the input, we abort generation for the current template and move onto the next template.", "labels": [], "entities": []}, {"text": "The role of the caption is to seed the dialog and initialize S 0 q . In other words, caption gives Q-er partial information about the image so that asking follow-up questions is possible.", "labels": [], "entities": []}, {"text": "Because A-er generates the caption, it uses the full scene graph S a . shows the caption grammar inaction, producing three different captions fora given image.", "labels": [], "entities": []}, {"text": "First, Sample[attribute] produces {shape, color} used by Unique to select objects from S a with unique shape and color attributes.", "labels": [], "entities": []}, {"text": "An object (gray cylinder) is then sampled from these using Sample.", "labels": [], "entities": [{"text": "Sample", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.5847957730293274}]}, {"text": "Next, a relation (in front of ) is enforced via a Relate primitive leading to the green cylinder in front of the gray cylinder.", "labels": [], "entities": []}, {"text": "Finally, Sample[attribute] samples one of the attributes to give us the caption, 'A green object stands in front of a gray cylinder.'", "labels": [], "entities": []}, {"text": "Unlike the caption, the questions are generated by the Q-er, having access only to a partial scene graph St q at round t.", "labels": [], "entities": []}, {"text": "This St q is an assimilation of information from the previous rounds of the dialog.", "labels": [], "entities": []}, {"text": "The primitives in the question template therefore take St q as the input scene graph, and the generation proceeds in a manner similar to that of the caption explained above.", "labels": [], "entities": []}, {"text": "As the dialog is driven by Q-er based on partial scene information, only a few questions are non-redundant (or even plausible) at a given round of the dialog.", "labels": [], "entities": []}, {"text": "To this  end, the inherent constraints associated with the primitives now play a bigger role in the template selection.", "labels": [], "entities": []}, {"text": "Consider that shows how the current question is generated using the primitives and grammar, given the caption and dialog history (question-answer pair for the first three rounds).", "labels": [], "entities": []}, {"text": "For the current round, the question 'What material is the green object at the back?' is clearly implausible (Q-er is unaware of the existence of a green object), while the question 'What shape is the red object?' is redundant.", "labels": [], "entities": []}, {"text": "For the templates visualized, Unique[object] returns a list of unique known object-attribute pairs (using St q ).", "labels": [], "entities": []}, {"text": "As specified in the main paper, we use beam search as a more tractable alternative to search through the exponential space of possible dialogs, by using additional constrains to retain only interesting dialogs.", "labels": [], "entities": [{"text": "beam search", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.846522182226181}]}, {"text": "At every round of the dialog (after 3 rounds), we ensure that each of the question template types-count, existence, and seek-falls within a range (10% \u2212 20% for count/existence each, and 30% \u2212 60% for seek).", "labels": [], "entities": []}, {"text": "In addition, we identify independent questions that do not need history to answer them, e.g., 'How many objects are present in the image?', and limit their number to under 10%.", "labels": [], "entities": []}, {"text": "Finally, to encourage questions that require reasoning over the history, e.g., seek-attr-sim-early and count-obj-excl-imm, we tailor our beam search objective so that dialogs containing such questions have a higher value.", "labels": [], "entities": []}, {"text": "We use abeam search with 100 beams for each dialog.", "labels": [], "entities": []}, {"text": "illustrates the diverse set of candidate questions generated at each round fora given image.", "labels": [], "entities": []}, {"text": "To summarize, the usage of primitives and a dialog grammar makes our generation procedure: (a) modular: each primitive has an intuitive meaning,    (b) expressive: complex templates can be broken down into these primitives, (c) computationally efficient: outputs can reused for templates sharing similar primitive structures (as seen in, thus allowing an easy extension to new primitives and templates.", "labels": [], "entities": []}, {"text": "We believe that CLEVR-Dialog represents not just a static dataset but also a recipe for constructing increasingly challenging grounded dialog by expanding this grammar.", "labels": [], "entities": []}, {"text": "As the dialog between Qer and A-er is initiated by the caption, care must betaken to ensure it is interesting enough to spawn clarifying questions from the Q-er.", "labels": [], "entities": []}, {"text": "To this end, we carefully design four different categories of caption templates (: (a) Obj-unique mentions an object with unique set of attributes in the image, (b) Obj-count specifies the presence of a group of objects with common attributes, (c) Obj-extreme describes an object atone of the positional extremes of the image (right, left, fore, rear, center), (d) Obj-relation talks about the relationship between two objects along with their attributes in away that allows them to be uniquely identified in the complete scene graph S a . Example captions are given in History Dependency.", "labels": [], "entities": [{"text": "Obj-unique", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9521124958992004}, {"text": "Obj-count", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9620347023010254}, {"text": "History Dependency", "start_pos": 570, "end_pos": 588, "type": "DATASET", "confidence": 0.9322418868541718}]}, {"text": "Recall that our motivation for CLEVR-Dialog to create a diagnostic dataset for multi-round reasoning in visual dialog.", "labels": [], "entities": []}, {"text": "As a result, a majority of questions in our dataset depend on the dialog history.", "labels": [], "entities": []}, {"text": "We identify three major kinds of history dependency for the questions: (a) Coreference occurs when a phrase within the current question refers to a earlier mentioned object (referent).", "labels": [], "entities": [{"text": "Coreference", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9793534874916077}]}, {"text": "We characterize coreferences by measuring the distance between the current and the earlier mention, in terms of dialog rounds.", "labels": [], "entities": []}, {"text": "This can range from 1 (e.g., 'What is its color?')", "labels": [], "entities": []}, {"text": "to 10 (a question in round 10 referring to an entity in the caption).", "labels": [], "entities": []}, {"text": "(b) All: When the question depends on the entire dialog history, e.g., 'How many other objects are present in the image?', (c) None: When the question is stand-alone and does not depend on the history, e.g., 'How many spheres does the scene have?'", "labels": [], "entities": []}, {"text": "The distribution of questions characterized according to the history dependency is shown in.", "labels": [], "entities": []}, {"text": "Unlike MNIST Dialog, CLEVR-Dialog contains a good distribution of reference distances beyond just 1, leading to a mean distance of 3.2.", "labels": [], "entities": [{"text": "MNIST Dialog", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8782705962657928}]}, {"text": "Thus, the models will need to reason through different rounds of dialog history in order to succeed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics comparing CLEVR-Dialog  to MNIST Dialog (Seo et al., 2017). Our dataset has  3\u00d7 the questions (larger), 206\u00d7 the unique number of  questions (more diverse), 3.2\u00d7 the mean coreference  distance (more complex), and longer question lengths.  Similar stats for VisDial are also shown. Coreference  distance can not be computed for VisDial due to lack  of annotations.", "labels": [], "entities": [{"text": "MNIST Dialog", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.8742964267730713}]}]}