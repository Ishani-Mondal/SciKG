{"title": [], "abstractContent": [{"text": "Recently, the Transformer model (Vaswani et al., 2017) that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.8303622404734293}]}, {"text": "However , recent studies reveal that the lack of recurrence hinders its further improvement of translation capacity (Chen et al., 2018; De-hghani et al., 2019).", "labels": [], "entities": [{"text": "translation", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.9613937139511108}]}, {"text": "In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder.", "labels": [], "entities": []}, {"text": "In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention and recurrent networks.", "labels": [], "entities": []}, {"text": "Experimental results on the widely-used WMT14 English\u21d2German and WMT17 Chinese\u21d2English translation tasks demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": [{"text": "WMT14 English\u21d2German", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.8404849767684937}, {"text": "WMT17 Chinese\u21d2English translation tasks", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.8358286619186401}]}, {"text": "Our studies also reveal that the proposed model benefits from a shortcut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently,) -a new network architecture based solely on attention mechanisms, has advanced the state-of-the-art on various translation tasks across language pairs.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.8985565602779388}]}, {"text": "Compared with the conventional recurrent neural network (RNN) based model that leverages recurrence as the basic building module (), Transformer replaces RNN with self-attention network (SAN) to model the dependencies among input elements.", "labels": [], "entities": []}, {"text": "One appealing strength of SAN is that it breaks * Zhaopeng Tu is the corresponding author of the paper.", "labels": [], "entities": [{"text": "SAN", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.7407822012901306}]}, {"text": "This work was conducted when Jie Hao and Baosong Yang were interning at Tencent AI Lab.", "labels": [], "entities": []}, {"text": "down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance.", "labels": [], "entities": []}, {"text": "However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality.", "labels": [], "entities": []}, {"text": "Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations) and positional encoding, which are exactly the weaknesses of SAN (.", "labels": [], "entities": [{"text": "positional encoding", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.6788384467363358}]}, {"text": "Recently,  show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models.", "labels": [], "entities": []}, {"text": "Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder.", "labels": [], "entities": []}, {"text": "The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional information source to the Transformer decoder.", "labels": [], "entities": []}, {"text": "In addition to the standard RNN, we propose to implement recurrence modeling with a novel attentive recurrent network (ARN), which combines advantages of both SAN and RNN.", "labels": [], "entities": [{"text": "recurrence modeling", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7898158133029938}]}, {"text": "Instead of recurring over the individual symbols of sequences like RNN, the ARN recurrently revises its representations over a set of feature vectors, which are extracted by an attention model from the input sequence.", "labels": [], "entities": []}, {"text": "Accordingly, ARN combines the strong global modeling capacity of SAN with the recurrent bias of RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.803524374961853}]}, {"text": "We evaluate the proposed approach on widelyused WMT14 English\u21d2German and WMT17 Chinese\u21d2English translation tasks.", "labels": [], "entities": [{"text": "WMT14 English\u21d2German", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8146739602088928}, {"text": "WMT17 Chinese\u21d2English translation tasks", "start_pos": 73, "end_pos": 112, "type": "TASK", "confidence": 0.8190373082955679}]}, {"text": "Experimental results show that the additional recurrence encoder, implemented with either RNN or ARN, consistently improves translation performance, demonstrating the necessity of modeling recurrence for Transformer.", "labels": [], "entities": []}, {"text": "Specifically, the ARN implementation outperforms its RNN counterpart, which confirms the strength of ARN.", "labels": [], "entities": [{"text": "ARN", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8679602146148682}]}, {"text": "Further analyses reveal that our approach benefits from a short-cut that bridges the source and target sequences with shorter path.", "labels": [], "entities": []}, {"text": "Among all the model variants, the implementation with shortest path performs best, in which the recurrence encoder is single layer and its output is only fed to the top decoder layer.", "labels": [], "entities": []}, {"text": "It consistently outperforms its multiple deep counterparts, such as multiple-layer recurrence encoder and feeding the output of recurrence encoder to all the decoder layers.", "labels": [], "entities": []}, {"text": "In addition, our approach indeed generates more informative encoder representations, especially representative on syntactic structure features, through conducting linguistic analyses on probing tasks ().", "labels": [], "entities": []}, {"text": "shows the model architecture of Transformer.", "labels": [], "entities": []}, {"text": "The encoder is composed of a stack of N identical layers, each of which has two sub-layers.", "labels": [], "entities": []}, {"text": "The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network.", "labels": [], "entities": []}, {"text": "A residual connection) is employed around each of two sublayers, followed by layer normalization (.", "labels": [], "entities": []}, {"text": "Formally, the output of the first sub-layer C n e and the second sub-layer H n e are sequentially calculated as:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of recurrence encoder implemen- tations. The output of recurrence encoder is fed to the  top decoder layer in a stack fusion. \"Speed\" denotes  the training speed (steps/second).", "labels": [], "entities": []}, {"text": " Table 3: Comparing with the existing NMT systems on WMT17 Zh\u21d2En and WMT14 En\u21d2De test sets. \"\u2191 / \u21d1\":  significant over the conventional self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8793813586235046}, {"text": "WMT14 En\u21d2De test sets", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.7323345641295115}]}, {"text": " Table 5: Classification accuracies on 10 probing tasks of evaluating linguistics embedded in the encoder outputs.", "labels": [], "entities": []}]}