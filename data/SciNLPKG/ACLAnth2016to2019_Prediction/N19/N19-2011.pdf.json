{"title": [{"text": "Neural Lexicons for Slot Tagging in Spoken Language Understanding", "labels": [], "entities": [{"text": "Slot Tagging", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9296405017375946}]}], "abstractContent": [{"text": "We explore the use of lexicons in neural models for slot tagging in spoken language understanding.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.8618671894073486}, {"text": "spoken language understanding", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6464653114477793}]}, {"text": "We develop models that encode lexicon information as features for use in a Long-short term memory neural network.", "labels": [], "entities": []}, {"text": "Experiments are performed on data from 4 domains from an intelligent assistant under conditions that often occur in an industry setting, where there may be: 1) large amounts of training data, 2) limited amounts of training data for new domains, and 3) cross domain training.", "labels": [], "entities": []}, {"text": "Results show that the use of neural lexicon information leads to a significant improvement in slot tagging, with improvements in the F-score of up to 12%.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.9158443510532379}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9967973828315735}]}], "introductionContent": [{"text": "Spoken language understanding (SLU) is an important component of systems that interface with users, such as intelligent assistants.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8724656800429026}]}, {"text": "These systems are becoming increasingly popular as a means for people to accomplish tasks in their homes and on mobile devices.", "labels": [], "entities": []}, {"text": "These tasks might include switching on the lights or booking a taxi.", "labels": [], "entities": []}, {"text": "Typically, an SLU system detects the domain, intent, and semantic slots of an utterance ( and uses the information to perform actions.", "labels": [], "entities": []}, {"text": "It is common to use lexicons (also known as gazettes or dictionaries) to improve the performance of SLU systems.", "labels": [], "entities": []}, {"text": "Lexicons are typically collections of phrases that are semantically related and thus provide knowledge that can aid the SLU system.", "labels": [], "entities": [{"text": "SLU", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9320377111434937}]}, {"text": "For instance, a lexicon called holidays might contain the phrases Thanksgiving, Christmas Eve, Labor Day.", "labels": [], "entities": []}, {"text": "Similarly, a lexicon called days of the week would contain Monday, Tuesday, Wednesday, etc.", "labels": [], "entities": []}, {"text": "There are many ways lexicons can be built, such as by using domain experts or by harvesting information from knowledge graphs, such as DBPedia . In an industry setting, it is also possible that lexicons already exist for other natural language applications.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9379329681396484}]}, {"text": "Previous work has shown how lexicons can be used to improve slot tagging with Conditional Random Fields (, where slot tagging refers to the process of identifying semantic entities of interest in an utterance.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.7708779573440552}, {"text": "slot tagging", "start_pos": 113, "end_pos": 125, "type": "TASK", "confidence": 0.7492509484291077}]}, {"text": "For instance, given the utterance \"book a taxi to the airport\", a slot tagging model might identify taxi as a transport type and airport as a destination.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the effect of integrating these types of lexicons into Long-Short Term Memory (LSTM) neural models in an industry setting.", "labels": [], "entities": []}, {"text": "We focus on LSTM models since they have been shown to produce state-of-the-art results in many natural language tasks.", "labels": [], "entities": []}, {"text": "We consider integrating lexicon features into a Long-short term memory neural network in two ways: 1) by considering lexicon membership as binary features and 2) by embedding the lexicons and allowing the model to learn the representation as part of the end-to-end training of the neural network.", "labels": [], "entities": []}, {"text": "To evaluate these approaches, we measure the performance of models on data from four domains belonging to an intelligent assistant under three data scenarios that commonly occur in production SLU systems.", "labels": [], "entities": []}, {"text": "The first scenario is when there is a considerable amount of training data available to train a SLU system, as may occur if a sizeable investment has been made to collect data.", "labels": [], "entities": [{"text": "SLU", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9567065834999084}]}, {"text": "The second scenario is when there is only a small amount of training data available, as maybe the case when the SLU is expanded to cover new domains for which very little training data exists.", "labels": [], "entities": []}, {"text": "The third scenario is cross domain slot prediction, where we use a model trained on utterances from one domain to identify entities in utterances belonging to another domain.", "labels": [], "entities": [{"text": "cross domain slot prediction", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8177202492952347}]}, {"text": "This setting commonly occurs when one attempts to leverage existing SLU models for use in anew domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments under three settings in order to evaluate how the lexicon models affect slot tagging performance.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.9069949686527252}]}, {"text": "\u2022 All Training Data: In this setting we use all of the available training data for each domain when training the slot tagging models.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 113, "end_pos": 125, "type": "TASK", "confidence": 0.7481798231601715}]}, {"text": "\u2022 Limited Training Data: In this setting we simulate the constrained data scenario that often occurs when expanding an SLU system to support new domains and limit the amount of training data available.", "labels": [], "entities": []}, {"text": "\u2022 Cross Domain: In this setting we consider cross domain prediction and train on one domain and then predict common slots in other domains.", "labels": [], "entities": [{"text": "cross domain prediction", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6314170161883036}]}, {"text": "This experiment uses all of the available training data.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For the Recipes domain, the highest F1 score is achieved by the M Embed model; however, the difference is not statistically significant compared to the baseline LSTM.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9875108897686005}]}, {"text": "For this domain, the LSTM models all outperform the CRF models.", "labels": [], "entities": []}, {"text": "For the Services domain, we observe that the MM ember model achieves the highest F1 score; however, it is also not statistically significant compared to the baseline LSTM.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9922274947166443}]}, {"text": "Once again, the LSTM models outperform the CRF models.", "labels": [], "entities": []}, {"text": "For the Location domain we observe a statistically significant improvement in performance for the M Embed model compared to the baseline LSTM.", "labels": [], "entities": [{"text": "M Embed model", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.8186024824778239}]}, {"text": "This improvement exceeds 1%.", "labels": [], "entities": []}, {"text": "However, the CRF model outperforms the LSTM models.", "labels": [], "entities": []}, {"text": "Lastly, for the the Time domain we observe a significant improvement in the F1 score compared to the baseline LSTM for the M Embed model.", "labels": [], "entities": [{"text": "Time", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9255943894386292}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9908373355865479}, {"text": "LSTM", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8940709233283997}, {"text": "M Embed model", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.8192795713742574}]}, {"text": "Furthermore, all LSTM models outperform the CRF baselines.", "labels": [], "entities": [{"text": "CRF baselines", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8782175183296204}]}, {"text": "The results in this experiment show that the M Embed model achieves a statistically significant improvement over the baseline in two of the four", "labels": [], "entities": [{"text": "M Embed model", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8006076415379842}]}], "tableCaptions": [{"text": " Table 1: Description of datasets.", "labels": [], "entities": []}, {"text": " Table 2: Prevalence of lexicon features in training data.", "labels": [], "entities": []}, {"text": " Table 3: F1 score for different models using full train- ing set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.980046808719635}]}, {"text": " Table 4: F1 score for different models using 1,000 sam- ples during each training iteration.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.975063145160675}]}, {"text": " Table 5: F1 score for models trained on LOCATION  and TIME and tested on SERVICES.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9726661741733551}, {"text": "LOCATION", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8753881454467773}, {"text": "TIME", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.7797819972038269}, {"text": "SERVICES", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9787275195121765}]}]}