{"title": [{"text": "CITE: A Corpus of Image-Text Discourse Relations", "labels": [], "entities": [{"text": "CITE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8951646089553833}]}], "abstractContent": [{"text": "This paper presents a novel crowd-sourced resource for multimodal discourse: our resource characterizes inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations.", "labels": [], "entities": []}, {"text": "Like previous corpora annotating discourse structure between text arguments, such as the Penn Discourse Treebank, our new corpus aids in establishing a better understanding of natural communication and common-sense reasoning, while our findings have implications fora wide range of applications, such as understanding and generation of multimodal documents.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.9852057298024496}, {"text": "common-sense reasoning", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.7495979964733124}, {"text": "understanding and generation of multimodal documents", "start_pos": 304, "end_pos": 356, "type": "TASK", "confidence": 0.6143797238667806}]}], "introductionContent": [{"text": "\"Sometimes a picture is worth the proverbial thousand words; sometimes a few well-chosen words are far more effective than a picture\" -Feiner and.", "labels": [], "entities": []}, {"text": "Modeling how visual and linguistic information can jointly contribute to coherent and effective communication is a longstanding open problem with implications across cognitive science.", "labels": [], "entities": []}, {"text": "As already observe, it is particularly important for automating the understanding and generation of text-image presentations.", "labels": [], "entities": [{"text": "understanding and generation of text-image presentations", "start_pos": 68, "end_pos": 124, "type": "TASK", "confidence": 0.7035131355126699}]}, {"text": "Theoretical models have suggested that images and text fit together into integrated presentations via coherence relations that are analogous to those that connect text spans in discourse; see and Section 2.", "labels": [], "entities": []}, {"text": "This paper follows up this theoretical perspective through systematic corpus investigation.", "labels": [], "entities": [{"text": "systematic corpus investigation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.7105262080828348}]}, {"text": "We are inspired by research on text discourse, which has led to large-scale corpora with information about discourse structure and discourse semantics.", "labels": [], "entities": []}, {"text": "The Penn Discourse Treebank (PDTB) is one of the most well-known examples.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9561119576295217}]}, {"text": "However, although multimodal corpora increasingly include discourse relations between linguistic and nonlinguistic contributions, particularly for utterances and other events in dialogue (, to date there has existed no dataset describing the coherence of text-image presentations.", "labels": [], "entities": []}, {"text": "In this paper, we describe the construction of an annotated corpus that fills this gap, and report initial analyses of the communicative inferences that connect text and accompanying images in this corpus.", "labels": [], "entities": []}, {"text": "As we describe in Section 2, our approach asks annotators to identify the presence of specific inferences linking text and images, rather than to use a taxonomy of coherence relations.", "labels": [], "entities": []}, {"text": "This enables us to deal with the distinctive discourse contributions of photographic imagery.", "labels": [], "entities": []}, {"text": "We describe our data collection process in Section 3, showing that our annotation scheme allows us to get reliable labels by crowdsourcing.", "labels": [], "entities": []}, {"text": "We present analyses in Section 4 that show that our annotation highlights a range of cases where text and images work together in distinctive and theoretically challenging ways, and discuss the implications of our work for the understanding and generation of multimodal documents.", "labels": [], "entities": [{"text": "understanding and generation of multimodal documents", "start_pos": 227, "end_pos": 279, "type": "TASK", "confidence": 0.7264050543308258}]}, {"text": "We conclude in Section 5 with a number of problems for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Rate of true answers for annotation questions Q2-Q10 across the corpus.", "labels": [], "entities": [{"text": "Rate", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9902578592300415}]}, {"text": " Table 2: SVM classification accuracy: bag-of-words features; 80-20 train-test split; 5-fold cross validation. For  the first question, this distinguishes highlighted text vs. its complement (excluded vs. included). For the rest of the  questions, this distinguishes text of true instances from text of false instances, and is different from majority class  baseline  *  at p < 0.04, t = \u22123.5 and  *  *  at p < 0.01, t > |2.49|.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8282376825809479}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9385828971862793}]}, {"text": " Table 3: Top five features of Multimodal Naive Bayes  classifier for two classification problems and their cor- responding log-probability estimates.", "labels": [], "entities": [{"text": "Multimodal Naive Bayes  classifier", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.7012610286474228}]}]}