{"title": [{"text": "On the Feasibility of Automated Detection of Allusive Text Reuse", "labels": [], "entities": [{"text": "Automated Detection of Allusive Text Reuse", "start_pos": 22, "end_pos": 64, "type": "TASK", "confidence": 0.7958356787761053}]}], "abstractContent": [{"text": "The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely-commonly based on none or very few shared words.", "labels": [], "entities": []}, {"text": "Arguably , lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity.", "labels": [], "entities": []}, {"text": "A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process.", "labels": [], "entities": []}, {"text": "In the present paper, we aim to elucidate the feasibility of automated allusion detection.", "labels": [], "entities": [{"text": "automated allusion detection", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.624077171087265}]}, {"text": "We approach the matter from an Information Retrieval perspective in which refer-encing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7214504480361938}, {"text": "benchmark corpus compilation", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.7182775735855103}, {"text": "query segmentation", "start_pos": 274, "end_pos": 292, "type": "TASK", "confidence": 0.7178836315870285}]}, {"text": "Furthermore, we investigate to what extent the integration of lexical semantic information derived from dis-tributional models and ontologies can aid retrieving cases of allusive reuse.", "labels": [], "entities": []}, {"text": "The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a win-dowing approach, and that (ii) retrieval performance can be moderately boosted with distri-butional semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the 20th century, intertextuality emerged as an influential concept in literary criticism.", "labels": [], "entities": [{"text": "literary criticism", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7557555735111237}]}, {"text": "Originally developed by French deconstructionist theorists, such as Kristeva and Barthes, the term broadly refers to the phenomenon where texts integrate (fragments of) other texts or allude to them.", "labels": [], "entities": []}, {"text": "In the minds of both authors and readers, intertexts can establish meaningful connections between works, evoking particular stylistic Reference (Vulgata,) \"scire etiam supereminentem scientiae caritatem Christi ut impleamini in omnem plenitudinem Dei\" \"and to know the love (caritas) of Christ that is beyond knowledge, such that you'd be filled with all fullness of God\" Reuse (Bernard, Sermo 8, 7.l) \"Osculum plane dilectionis et pacis, sed dilectio illa supereminet omni scientiae, et pax illa omnem sensum exsuperat\" \"It is a kiss of love and peace, but of that kind of love (dilectio) that is beyond any knowledge, and of that kind of peace that surpasses all senses.\" effects and interpretations of a text.", "labels": [], "entities": []}, {"text": "Existing categorizations ( emphasize the broad spectrum of intertexts, which can range from direct quotations, over paraphrased passages to highly subtle allusions.", "labels": [], "entities": []}, {"text": "With the emergence of computational methods in literary studies over the past decades, intertextuality has often been presented as a promising application, helping scholars identifying potential intertextual links that had previously gone unnoticed.", "labels": [], "entities": []}, {"text": "Much progress has been made in this area and a number of highly useful tools are now available-e.g. Tracer or Tesserae (.", "labels": [], "entities": []}, {"text": "This paper, however, aims to contribute to a number of open issues that still present significant challenges to the further development of the field.", "labels": [], "entities": []}, {"text": "Most scholarship continues to focus on the de-tection of relatively literal instances of so-called 'text reuse', as intertextuality is commonly -and somewhat restrictively -referred to in the field.", "labels": [], "entities": []}, {"text": "Such instances are relatively unambiguous and unproblematic to detect using n-gram matching, fingerprinting and string alignment algorithms.", "labels": [], "entities": [{"text": "string alignment", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.704316720366478}]}, {"text": "Much less research has been devoted to the detection of fuzzier instances of text reuse holding between passages that lack a significant lexical correspondence.", "labels": [], "entities": []}, {"text": "This situation is aggravated by the severe lack of openly available benchmark datasets.", "labels": [], "entities": []}, {"text": "An additional hindrance is that the establishment of intertextual links is to a high degree subjective -both regarding the existence of particular intertextual links and the exact scope of the correspondence in both fragments.", "labels": [], "entities": []}, {"text": "Studies of inter-annotator agreement are surprisingly rare in the field, which might be partially due to to the fact that existing agreement metrics are hard to port to this problem.", "labels": [], "entities": []}, {"text": "Contributions In this paper, we report on an empirical feasibility study, focusing on the annotation and automated detection of allusive text reuse.", "labels": [], "entities": []}, {"text": "We focus on biblical intertext in the works of, an influential medieval writer known for his pervasive references to the Bible.", "labels": [], "entities": []}, {"text": "The paper has two main parts.", "labels": [], "entities": []}, {"text": "In the first part, we formulate an adaptation of Fleiss's \u03ba that allows us to quantitatively estimate and discuss the level of inter-annotator agreement concerning the span of the intertexts.", "labels": [], "entities": []}, {"text": "While annotators show considerably low levels of agreement, We show that manual segmentation has nevertheless a big impact on the automatic retrieval of allusive reuse.", "labels": [], "entities": [{"text": "agreement", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9891080856323242}]}, {"text": "In the second part, we offer an evaluation of current Information Retrieval (IR) techniques for allusive text reuse detection.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8127785682678222}, {"text": "text reuse detection", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7959113021691641}]}, {"text": "We confirm that semantic retrieval models based on word and sentence embeddings do not present advantages over hand-crafted scoring functions from previous studies, and that both are outperformed by conventional retrieval models based on TfIdf.", "labels": [], "entities": []}, {"text": "Finally, we show how a recently introduced technique, soft cosine, allows us to combine lexical and semantic information to obtain significant improvements over any other considered model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The basis for the present study stems from the BiblIndex project, which aims to index biblical references found in Christian literature.", "labels": [], "entities": [{"text": "BiblIndex project", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.876282125711441}]}, {"text": "More specifically, we use a subset of manually identified biblical references from Bernard of Clairvaux which was kindly shared with us by Laurence Mellerin.", "labels": [], "entities": []}, {"text": "The provided data consists of 85 Sermons, totalling 199,508 words.", "labels": [], "entities": []}, {"text": "The data came already tokenized and lemmatized.", "labels": [], "entities": []}, {"text": "Bible references were tagged with a URL mapping to the corresponding Bible verse from the Vulgata edition of the medieval Bible in the online BiblIndex database.", "labels": [], "entities": [{"text": "Vulgata edition of the medieval Bible", "start_pos": 90, "end_pos": 127, "type": "DATASET", "confidence": 0.9553553064664205}, {"text": "BiblIndex database", "start_pos": 142, "end_pos": 160, "type": "DATASET", "confidence": 0.9077004194259644}]}, {"text": "We extracted the online text of the Vulgata and used the URLs to match references in Bernard with the corresponding Bible verses.", "labels": [], "entities": [{"text": "online text of the Vulgata", "start_pos": 17, "end_pos": 43, "type": "DATASET", "confidence": 0.7766919255256652}]}, {"text": "Since the online BiblIndex database does not provide lemmatized text, we applied an state-of-theart lemmatizer for Medieval Latin () to obtain a lemmatized version of the Vulgata.", "labels": [], "entities": [{"text": "BiblIndex database", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.9361449778079987}, {"text": "Vulgata", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.828726053237915}]}, {"text": "The resulting corpus data comprises a total of 34,835 verses totalling 586,285 tokens and amounting to a vocabulary size of 46,025 token types.", "labels": [], "entities": []}, {"text": "BiblIndex distinguishes three types of references: quotation, mention and allusion.", "labels": [], "entities": [{"text": "BiblIndex", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8985835313796997}]}, {"text": "While the links in the first two types are in their vast majority exact or near-exact lexical matches, the latter type comprises mostly references that fall into what is commonly known as allusive text reuse.", "labels": [], "entities": []}, {"text": "Although our focus lies on the allusive category, displays statistics about all these types in order to appreciate the characteristics of the task.", "labels": [], "entities": []}, {"text": "As shown in (last row), allusions are characterized by low Jaccard coefficients -in set-theoretical terms, the ratio of the intersection over the union of the sets of words of both passages.", "labels": [], "entities": []}, {"text": "On average, annotated allusions share 6% of the word forms with their targets and 12% of the lemmata.", "labels": [], "entities": []}, {"text": "In comparison, mentions and quotations have 25% or more tokens and 30% or more lemmata in common.", "labels": [], "entities": []}, {"text": "The full distribution of token and lemma overlap for allusions shown in indicates that more than 500 ( 65%) instances have at most 1 token in common; about more than 400 ( 50%) share at most 1 lemma.", "labels": [], "entities": []}, {"text": "The aim of the annotation was to determine the scope of a biblical reference identified by the editors in text by Bernard.", "labels": [], "entities": []}, {"text": "From an IR perspective, the annotation task consists of delineating the appropriate input query, given the anchor word in the source text and the corresponding Bible verse.", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9522196054458618}]}, {"text": "An example annotation is shown in where the anchor word provided by the editors is \"scientiae\" and the corresponding annotated query spans the subclause \"sed dilection illa supereminet omni scientiae\".", "labels": [], "entities": []}, {"text": "Naturally, such references not always correspond to full sentences and often go over sentence boundaries.", "labels": [], "entities": []}, {"text": "The dataset was distributed evenly across 4 annotators, who worked independently through a custom-built interface.", "labels": [], "entities": []}, {"text": "All annotators were proficient readers of Medieval Latin with expertise ranging from graduate student to professor.", "labels": [], "entities": []}, {"text": "The annotators were familiar with the text reuse detection task and were given explicit instructions that can be summarized as follows: given a previously identified allusion between the Bernardine passage surrounding an anchor word, on the one hand, and a specific Bible verse on the other hand, Jaccard(token) Jaccard annotate the minimal textual span in the Bernardine passage that is maximally allusive to the Bible verse.", "labels": [], "entities": [{"text": "text reuse detection", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7888553241888682}]}, {"text": "For the sake of simplicity, the interface only allowed continuous annotation spans and the annotated span had to include the pre-identified anchor token.", "labels": [], "entities": []}, {"text": "Of a total of 876 initial instances, we discarded 147 cases in which annotators expressed doubts on the existence of the alleged reference or could not precisely decide the span.", "labels": [], "entities": []}, {"text": "This decision was taken in order to ensure a high quality in the resulting benchmark data.", "labels": [], "entities": []}, {"text": "Determining the scope of an allusive reference is a relevant task for two reasons.", "labels": [], "entities": [{"text": "Determining the scope of an allusive reference", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7419201816831317}]}, {"text": "Firstly, we expect this task to be reader-dependent, and thus highly subjective, given the minimal lexical overlap between the source and target passage.", "labels": [], "entities": []}, {"text": "Measuring the agreement between annotators sheds new light on the overall feasibility of the task.", "labels": [], "entities": []}, {"text": "Secondly, the resulting annotations allow us to critically evaluate the performance of existing retrieval methods under near-perfect segmentation conditions: if the correct source query is given, what is the performance of existing methods when attempting to retrieve the correct Bible verse in the target data?", "labels": [], "entities": []}, {"text": "Measuring inter-annotator agreement Interannotator agreement coefficients such as Fleiss's \u03ba and Krippendorff's \u03b1 are typically defined in terms of labels assigned to items in a multi-class classification setup.", "labels": [], "entities": []}, {"text": "In the present case, however, the annotation involves making a decision on the span of words surrounding an anchor word that better captures the allusion and it is unclear how to quantify the variation in annotation performance.", "labels": [], "entities": []}, {"text": "A na\u00a8\u0131vena\u00a8\u0131ve approach defined in terms of number of overlaping words has a number of undesirable issues.", "labels": [], "entities": []}, {"text": "For example, since the annotations are centered around the anchor word, a relatively high amount of overlap is to be expected for short annotations.", "labels": [], "entities": [{"text": "overlap", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9654465913772583}]}, {"text": "Moreover, disagreements over otherwise largely agreeing long spans should weigh in less than disagreements over otherwise largely agreeing small spans.", "labels": [], "entities": []}, {"text": "Additionally, it is unclear how to quantify the rate of agreement expected under chance-level annotation, a quantity that needs to be corrected for in order to to obtain reliable and non-inflated interannotator agreement coefficients.", "labels": [], "entities": []}, {"text": "We have found that an extension of the Jaccard coefficient defined over sequences can help adapt Fleiss's \u03ba to our case and tackle such issues.", "labels": [], "entities": []}, {"text": "Given any pair of span annotations, sand t, we can define overlap in a similar way to the Jaccard index, as the intersection (i.e. the Longest Common Substring) over the union (i.e. the total number of selected tokens by both annotators): Interestingly, this quantity can be decomposed into an agreement A(s, t) = LCS(s, t) (number of tokens in common) and a disagreement score D(s, t) = |s| + |t| \u2212 2 \u00b7 LCS(s, t) (number of tokens not shared with the other annotator): The advantage of this reformulation is that it lets us see more easily how O is bounded between 0 and 1, and also that it gives us away of computing the expected overlap score O e by aggregating dataset-level A and D scores: where |s, t| refers to the number of unordered annotation pairs in the dataset . O e can be thus interpreted as the expected overlap between two arbitrary annotators.", "labels": [], "entities": []}, {"text": "The final inter-annotator agreement score is defined following Fleiss's: where O o refers to the dataset average of Eq.", "labels": [], "entities": [{"text": "O o", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9801844954490662}]}, {"text": "2. Inter-annotator agreement results and discussion In order to estimate \u03ba for our dataset, we extracted a random sample of 60 instances which were thoroughly annotated by 3 of the annotators.", "labels": [], "entities": []}, {"text": "We obtain a \u03ba = 0.22, which compares unfavorably with respect to commonly assumed reliability ranges.", "labels": [], "entities": []}, {"text": "For example, values in the range \u03ba \u2208 (0.67, 0.8) are considered fair agreement (.", "labels": [], "entities": []}, {"text": "While our result remains hard to assess in the absence of comparable work, it is low enough to cast doubts over the feasibility of the task, which is in fact rarely explicitly questioned.", "labels": [], "entities": []}, {"text": "The annotators informally reported that, against their expectations, the task was not straightforward and required a considerable level of concentration and interpretation.", "labels": [], "entities": []}, {"text": "Such situation maybe due to particularities of Bernard's usage of biblical language.", "labels": [], "entities": []}, {"text": "Besides conventional, direct allusions, Bernard is also known for pointed use of single, significant allusive words, which are hard to isolate.", "labels": [], "entities": []}, {"text": "Still it should be noted that in some instances inter-annotator agreement was high and, as shows, in 22% of all pairwise comparisons even perfect.", "labels": [], "entities": [{"text": "agreement", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9142920970916748}]}, {"text": "This suggests that there exist clear differences at the level of individual allusions.", "labels": [], "entities": []}, {"text": "We now turn to the question how well current retrieval approaches perform, given manually segmented queries.", "labels": [], "entities": []}, {"text": "Given the small amounts of lexical overlap in the allusive text reuse datasets (c.f., we aim to investigate and quantify to which extent semantic information can help improving retrieval of allusive references.", "labels": [], "entities": []}, {"text": "For this reason, we look into 3 types of models.", "labels": [], "entities": []}, {"text": "First, we look at purely lexicalbased approaches.", "labels": [], "entities": []}, {"text": "Secondly, approaches based on distributional semantics and, in particular, retrieval approaches that utilize word embeddings.", "labels": [], "entities": []}, {"text": "Finally, we look at hybrid approaches that can accommodate relative amounts of semantic informa-tion into what is otherwise a purely lexical model.", "labels": [], "entities": []}, {"text": "From the retrieval point of view, all approaches fall into one of two categories: retrieval methods based on similarity in vector space and retrieval methods using domain-specific similarity scoring functions.", "labels": [], "entities": []}, {"text": "Given a Bernardian reference as a query formulated by the annotators and the collection of Biblical candidate documents, all evaluated models produce a ranking.", "labels": [], "entities": []}, {"text": "Using such a ranking, we evaluate retrieval performance over the set of queries Q using Mean Reciprocal Rank 8 (M RR) (Voorhees, 1999) defined in Eq.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank 8 (M RR)", "start_pos": 88, "end_pos": 117, "type": "METRIC", "confidence": 0.9533266872167587}]}, {"text": "9: Additionally, we also report P recision@K-based on how often the system is expected to retrieve the relevant document within the first k results-since it is a more interpretable measure from the point of view of the retrieval system user.", "labels": [], "entities": [{"text": "P recision@K-based", "start_pos": 32, "end_pos": 50, "type": "METRIC", "confidence": 0.8939350545406342}]}, {"text": "It must be noted that P @K and M RR are not suitable metrics to evaluate a text reuse detection system on unrestricted data, since, in fact, most naturally occurring text is not allusive.", "labels": [], "entities": [{"text": "M RR", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8296962976455688}, {"text": "text reuse detection", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7925233840942383}]}, {"text": "However, the focus of the present paper lies on the feasibility of allusive text detection, which we aim to elucidate on the basis of a pre-annotated dataset in which each query is guaranteed to match to a relevant document in the target collection.", "labels": [], "entities": [{"text": "text detection", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7338638454675674}]}, {"text": "The results must therefore be interpreted taking into account the artificial situation, where the selected queries are already known to contain allusions and the question is how well different systems recognize the alluded verse.", "labels": [], "entities": []}, {"text": "Results As shown in, the best model overall is SC emb , achieving 21.95 M RR and 47.60 P @20, closely followed by another soft cosine-based hybrid approach: SC wn . Interestingly, a simple T f If d baseline over lemmatized input results in strong ranking performance, surpassing all other purely lexical -including the hand-crafted T esserae -and all purely semantic models.", "labels": [], "entities": [{"text": "RR", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.5904455184936523}, {"text": "P", "start_pos": 87, "end_pos": 88, "type": "METRIC", "confidence": 0.9170610904693604}]}, {"text": "In agreement with general expectations, all models benefit from lemmatized input and T f Idf transformation (both as input representation in purely lexical models and as a weighting scheme for the sentence embeddings in purely semantic approaches).", "labels": [], "entities": []}, {"text": "W MD outperforms any other purely semantic model, but as already pointed out, it compares negatively to the purely lexical T f Idf baseline.", "labels": [], "entities": []}, {"text": "The combination ) and a random similarity baseline (SC rnd ). of T esserae with W MD as back-off proves useful and outperforms both approaches in isolation, highlighting that they model complementary aspects of text reuse.", "labels": [], "entities": [{"text": "random similarity baseline (SC rnd ).", "start_pos": 24, "end_pos": 61, "type": "METRIC", "confidence": 0.7562144781861987}]}, {"text": "In order to test the specific contribution of the similarity function used to estimate S, we compare results with soft cosine using a random similarity matrix (S rnd ) defined by Eq.", "labels": [], "entities": []}, {"text": "10: We also investigate the effect of the word embedding algorithm by comparing to SC emb based on word2vec embeddings (Mikolov et al., 2013).", "labels": [], "entities": []}, {"text": "As shows, FastText embeddings, an algorithm known to capture not just semantic but also morphological relations, yields strong improvements over word2vec.", "labels": [], "entities": []}, {"text": "Moreover, a random approach produces strong results, only underperforming the word2vec model by a small margins, which questions the usefulness of the semantic relationships induced by word2vec for the present task.", "labels": [], "entities": []}, {"text": "Finally, we test the relative importance of the query segmentation to the retrieval of allusive text reuse.", "labels": [], "entities": []}, {"text": "For this purpose, we evaluate our best model (SC emb ) on aversion of the dataset in which the referencing text is segmented according to a window approach, selecting n words around the anchor expression.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Retrieval results for all considered models grouped by approach type. All models are evaluated with  tokens and lemmas as input except for SC wn which requires lemmatized input. Overall best numbers per metric  are shown in bold letters.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of soft cosine using FastText  embeddings (SC emb ), word2vec embeddings  (SC", "labels": [], "entities": []}, {"text": " Table 4: Comparison of best performing approach  SC emb across different segmentation types: manual  and automatic window of 3 (Win-3) and 10 (Win-10)  tokens to each side of the anchor word.", "labels": [], "entities": []}]}