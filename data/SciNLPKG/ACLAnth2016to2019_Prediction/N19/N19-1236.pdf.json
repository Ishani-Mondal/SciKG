{"title": [{"text": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation *", "labels": [], "entities": [{"text": "Neural Data-to-Text Generation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.6029749711354574}]}], "abstractContent": [{"text": "Data-to-text generation can be conceptually divided into two parts: ordering and struc-turing the information (planning), and generating fluent language describing the information (realization).", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7387585788965225}]}, {"text": "Modern neural generation systems conflate these two steps into a single end-to-end differentiable system.", "labels": [], "entities": []}, {"text": "We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization.", "labels": [], "entities": []}, {"text": "For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans.", "labels": [], "entities": []}, {"text": "For inference time, we describe a method for selecting high-quality text plans for new inputs.", "labels": [], "entities": []}, {"text": "We implement and evaluate our approach on the WebNLG benchmark.", "labels": [], "entities": [{"text": "WebNLG benchmark", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9808807075023651}]}, {"text": "Our results demonstrate that decoupling text planning from neural realization indeed improves the system's reliability and adequacy while maintaining fluent output.", "labels": [], "entities": [{"text": "text planning from neural realization", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.6967097282409668}, {"text": "reliability", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9865357279777527}]}, {"text": "We observe improvements both in BLEU scores and in manual evaluations.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9728274643421173}]}, {"text": "Another benefit of our approach is the ability to output diverse realiza-tions of the same input, paving the way to explicit control over the generated text structure.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The WebNLG challenge (Colin et al., 2016) consists of mapping sets of RDF triplets to text including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.7607594529787699}, {"text": "surface realization", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7864145934581757}, {"text": "sentence segmentation", "start_pos": 188, "end_pos": 209, "type": "TASK", "confidence": 0.7182765454053879}]}, {"text": "It contains sets with up to 7 triplets each along with one or more reference texts for each set.", "labels": [], "entities": []}, {"text": "The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.", "labels": [], "entities": []}, {"text": "While the unseen category is conceptually appealing, we view the seen category as the more relevant setup: generating fluent, adequate and diverse text fora mix of known relation types is enough of a challenge also without requiring the system to invent verbalizations for unknown relation types.", "labels": [], "entities": []}, {"text": "Any realistic generation system could afford to provide at least a few verbalizations for each relation of interest.", "labels": [], "entities": []}, {"text": "We thus focus our attention mostly on the seen case (though our system does also perform well on the unseen case).", "labels": [], "entities": []}, {"text": "Following Section 3.1, we manage to match a detailed in the appendix.", "labels": [], "entities": []}, {"text": "11 nlp.stanford.edu/data/glove.6B.zip consistent plan for 76% of the reference texts and use these plan-text pairs to train the plan realization NMT component.", "labels": [], "entities": []}, {"text": "Overall, the WebNLG training set contains 18, 102 RDF-text pairs while our plan-enhanced corpus contains 13, 828 plantext pairs.", "labels": [], "entities": [{"text": "WebNLG training set", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.9700293143590292}]}, {"text": "Next, we turn to manually evaluate our system's performance regarding faithfulness to the input on the one hand and fluency on the other.", "labels": [], "entities": []}, {"text": "We describe here the main points of the manual evaluation setup, with finer details in the appendix.", "labels": [], "entities": []}, {"text": "Faithfulness As explained in Section 3, the first benefit we expect of our plan-based architecture is to make the neural systems task simpler, helping it to remain faithful to the semantics expressed in the plan which in turn is guaranteed to be faithful to the original RDF input (by faithfulness, we mean expressing all facts in the graph and only facts from the graph: not dropping, repeating or hallucinating facts).", "labels": [], "entities": []}, {"text": "We conduct a manual evaluation over the seen portion of the WebNLG human evaluated test set (139 input sets).", "labels": [], "entities": [{"text": "WebNLG human evaluated test set", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.9603053808212281}]}, {"text": "We compare BestPlan and StrongNeural.", "labels": [], "entities": [{"text": "BestPlan", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7663795351982117}, {"text": "StrongNeural", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8636929392814636}]}, {"text": "For each output text, we manually mark which relations are expressed in it, which are omitted, and which relations exist with the wrong lexicalization.", "labels": [], "entities": []}, {"text": "We also count the number of relations the system over generated, either repeating facts or inventing new facts.", "labels": [], "entities": []}, {"text": "BestPlan reduces all error types compared to StrongNeural, by 85%, 56% and 90% respectively.", "labels": [], "entities": [{"text": "error types", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.8971813917160034}]}, {"text": "While on-par regarding automatic metrics, BestPlan substantially outperforms the new state-of-the-art end-to-end neural system in semantic faithfulness.", "labels": [], "entities": [{"text": "BestPlan", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.856434166431427}]}], "tableCaptions": [{"text": " Table 1: Results for all categories. Team color indicates  the type of system used (NMT  \u2666 , Rule-Based  \u2665 , Rule-", "labels": [], "entities": []}, {"text": " Table 2: Semantic faithfulness of each system regard- ing 440 RDF triplets from 139 input sets in the seen  part of the manually evaluated test set.", "labels": [], "entities": []}, {"text": " Table 4: Surface realizer performance. Entities: Per- cent of sentence plans that were realized with all the  requested entities. Order: of the sentences that were  realized with all requested entities, percentage of real- izations that followed the requested entity order.", "labels": [], "entities": [{"text": "Order", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.9544642567634583}]}]}