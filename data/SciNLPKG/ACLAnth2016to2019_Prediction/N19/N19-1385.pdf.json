{"title": [{"text": "Low-Resource Syntactic Transfer with Unsupervised Source Reordering", "labels": [], "entities": [{"text": "Low-Resource Syntactic Transfer", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6822981834411621}]}], "abstractContent": [{"text": "We describe a cross-lingual transfer method for dependency parsing that takes into account the problem of word order differences between source and target languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8448322117328644}]}, {"text": "Our model only relies on the Bible, a considerably smaller parallel data than the commonly used parallel data in transfer methods.", "labels": [], "entities": []}, {"text": "We use the concate-nation of projected trees from the Bible corpus , and the gold-standard treebanks in multiple source languages along with cross-lingual word representations.", "labels": [], "entities": [{"text": "Bible corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8808529675006866}]}, {"text": "We demonstrate that reordering the source treebanks before training on them fora target language improves the accuracy of languages outside the European language family.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9980480670928955}]}, {"text": "Our experiments on 68 tree-banks (38 languages) in the Universal Dependencies corpus achieve a high accuracy for all languages.", "labels": [], "entities": [{"text": "Universal Dependencies corpus", "start_pos": 55, "end_pos": 84, "type": "DATASET", "confidence": 0.8206341862678528}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993751645088196}]}, {"text": "Among them, our experiments on 16 treebanks of 12 non-European languages achieve an average UAS absolute improvement of 3.3% over a state-of-the-art method.", "labels": [], "entities": [{"text": "UAS absolute improvement", "start_pos": 92, "end_pos": 116, "type": "METRIC", "confidence": 0.7551957766215006}]}], "introductionContent": [{"text": "There has recently been a great deal of interest in cross-lingual transfer of dependency parsers, for which a parser is trained fora target language of interest using treebanks in other languages.", "labels": [], "entities": [{"text": "cross-lingual transfer of dependency parsers", "start_pos": 52, "end_pos": 96, "type": "TASK", "confidence": 0.8275755882263184}]}, {"text": "Crosslingual transfer can eliminate the need for the expensive and time-consuming task of treebank annotation for low-resource languages.", "labels": [], "entities": [{"text": "Crosslingual transfer", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7119085937738419}]}, {"text": "Approaches include annotation projection using parallel data sets), direct model transfer through learning of a delexicalized model from other treebanks, treebank translation (), using synthetic treebanks (), using cross-lingual word representations and using cross-lingual dictionaries).", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7488971054553986}, {"text": "direct model transfer", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.742597758769989}, {"text": "treebank translation", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.7534257471561432}]}, {"text": "Recent results from have shown accuracies exceeding 80% on unlabeled attachment accuracy (UAS) for several European languages.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9709704518318176}, {"text": "unlabeled attachment accuracy (UAS)", "start_pos": 59, "end_pos": 94, "type": "METRIC", "confidence": 0.7290683140357336}]}, {"text": "1 However non-European languages remain a significant challenge for crosslingual transfer.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8608314394950867}]}, {"text": "One hypothesis, which we investigate in this paper, is that word-order differences between languages area significant challenge for cross-lingual transfer methods.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.7595289647579193}]}, {"text": "The main goal of our work is therefore to reorder gold-standard source treebanks to make those treebanks syntactically more similar to the target language of interest.", "labels": [], "entities": []}, {"text": "We use two different approaches for source treebank reordering: 1) reordering based on dominant dependency directions according to the projected dependencies, 2) learning a classifier on the alignment data.", "labels": [], "entities": []}, {"text": "We show that an ensemble of these methods with the baseline method leads to higher performance for the majority of datasets in our experiments.", "labels": [], "entities": []}, {"text": "We show particularly significant improvements for non-European languages.", "labels": [], "entities": []}, {"text": "The main contributions of this work are as follows: \u2022 We propose two different syntactic reordering methods based on the dependencies projected using translation alignments.", "labels": [], "entities": []}, {"text": "The first model is based on the dominant dependency direction in the target language according to the projected dependencies.", "labels": [], "entities": []}, {"text": "The second model learns a reordering classifier from the small set of aligned sentences in the Bible parallel data.", "labels": [], "entities": []}, {"text": "\u2022 We run an extensive set of experiments on 68 treebanks for 38 languages.", "labels": [], "entities": []}, {"text": "We show that by just using the Bible data, we are able to achieve significant improvements in nonEuropean languages.", "labels": [], "entities": []}, {"text": "Our ensemble method is able to maintain a high accuracy in European languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9986679553985596}]}, {"text": "\u2022 We show that syntactic transfer methods can outperform a supervised model for cases in which the gold-standard treebank is very small.", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7133574485778809}]}, {"text": "This indicates the strength of these models when the language is truly lowresource.", "labels": [], "entities": []}, {"text": "Unlike most previous work for which a simple delexicalized model with gold part-of-speech tags are used, we use lexical features and automatic part-of-speech tags.", "labels": [], "entities": []}, {"text": "Our final model improves over two strong baselines, one with annotation projection and the other one inspired by the non-neural state-of-the-art model of.", "labels": [], "entities": []}, {"text": "Our final results improve the performance on non-European languages by an average UAS absolute improvement of 3.3% and LAS absolute improvement of 2.4%.", "labels": [], "entities": [{"text": "UAS absolute improvement", "start_pos": 82, "end_pos": 106, "type": "METRIC", "confidence": 0.8757343888282776}, {"text": "LAS absolute improvement", "start_pos": 119, "end_pos": 143, "type": "METRIC", "confidence": 0.9600635369618734}]}], "datasetContent": [{"text": "Datasets and Tools We use 68 datasets from 38 languages in the Universal Dependencies corpus version 2.0 ().", "labels": [], "entities": [{"text": "Universal Dependencies corpus version 2.0", "start_pos": 63, "end_pos": 104, "type": "DATASET", "confidence": 0.7960549116134643}]}, {"text": "The languages are Arabic (ar), Bulgarian (bg), Coptic (cop), Czech (cs), Danish (da), German (de), Greek (el), English (en), Spanish (es), Estonian (et), Basque (eu), Persian (fa), Finnish (fi), French (fr), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Latin (la), Lithuanian (lt), Latvian (lv), Dutch (nl), Norwegian (no), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Slovak (sk), Slovene (sl), Swedish (sv), Turkish (tr), Ukrainian (uk), Vietnamese (vi), and Chinese (zh).", "labels": [], "entities": []}, {"text": "We use the Bible data from Christodouloupoulos and Steedman (2014) for the 38 languages.", "labels": [], "entities": []}, {"text": "We extract word alignments using Giza++ default model.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.6894297301769257}]}, {"text": "Following, we obtain intersected alignments and apply soft POS consistency to filter potentially incorrect alignments.", "labels": [], "entities": [{"text": "POS consistency", "start_pos": 59, "end_pos": 74, "type": "METRIC", "confidence": 0.8128070831298828}]}, {"text": "We use the Wikipedia dump data to extract monolingual data for the languages in order to train monolingual embeddings.", "labels": [], "entities": [{"text": "Wikipedia dump data", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9198503494262695}]}, {"text": "We follow the method of to use the extracted dictionaries from the Bible and monolingual text from Wikipedia to create cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "We use the UDPipe pretrained models () to tokenize Wikipedia, and a reimplementation of the Perceptron tagger of to achieve automatic POS tags trained on the training data of the Universal Dependencies corpus (.", "labels": [], "entities": [{"text": "tokenize Wikipedia", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7149277478456497}, {"text": "Universal Dependencies corpus", "start_pos": 179, "end_pos": 208, "type": "DATASET", "confidence": 0.7661451896031698}]}, {"text": "We use word2vec ( to achieve embedding vectors both in monolingual and cross-lingual settings.", "labels": [], "entities": []}, {"text": "Supervised Parsing Models We trained our supervised models on the union of all datasets in a language to obtain a supervised model for each language.", "labels": [], "entities": []}, {"text": "It is worth noting that there are two major changes that we make to the neural parser of in our implementation 6 using the Dynet library (: first, we add a one-layer character BiL-STM to represent the character information for each word.", "labels": [], "entities": []}, {"text": "The final character representation is obtained by concatenating the forward representation of the last character and the backward representation of the first character.", "labels": [], "entities": []}, {"text": "The concatenated vector is summed with the randomly initialized as well as fixed pre-trained cross-lingual word embedding vectors.", "labels": [], "entities": []}, {"text": "Second, inspired by, we maintain the moving average parameters to obtain more robust parameters at decoding time.", "labels": [], "entities": []}, {"text": "We excluded the following languages from the set of source languages for annotation projection due to their low supervised accuracy: Estonian, Hungarian, Korean, Latin, Lithuanian, Latvian, Turkish, Ukrainian, Vietnamese, and Chinese.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.93111652135849}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9580317139625549}]}, {"text": "Baseline Transfer Models We use two baseline models: 1) Annotation projection: This model only trains on the projected dependencies.", "labels": [], "entities": []}, {"text": "2) Annotation projection + direct transfer: To speedup training, we sample at most thousand sentences from each treebank, comprising a training data of about 37K sentences.", "labels": [], "entities": [{"text": "Annotation projection", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.731362596154213}]}], "tableCaptions": [{"text": " Table 2: Dependency parsing results, in terms of unlabeled attachment accuracy (UAS) and labeled attachment  accuracy (LAS) after ignoring punctuations, on the Universal Dependencies v2 test sets (Nivre et al., 2017) using  supervised part-of-speech tags. The results are sorted by their \"difference\" between the ensemble model and the  baseline. The rows for non-European languages are highlighted with cyan. The rows that are highlighted by pink  are the ones that the transfer model outperforms the supervised model. For all of the non-European datasets except  \"hi\", our model outperforms significantly better in terms of UAS with p < 0.001 using McNemar's test.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8349743187427521}, {"text": "unlabeled attachment accuracy (UAS)", "start_pos": 50, "end_pos": 85, "type": "METRIC", "confidence": 0.7701651901006699}, {"text": "labeled attachment  accuracy (LAS)", "start_pos": 90, "end_pos": 124, "type": "METRIC", "confidence": 0.8302016456921896}, {"text": "Universal Dependencies v2 test sets", "start_pos": 161, "end_pos": 196, "type": "DATASET", "confidence": 0.6755083084106446}, {"text": "UAS", "start_pos": 627, "end_pos": 630, "type": "METRIC", "confidence": 0.8983691334724426}, {"text": "McNemar's test", "start_pos": 652, "end_pos": 666, "type": "DATASET", "confidence": 0.8216942151387533}]}, {"text": " Table 3: Unlabeled attachment f-score of POS tags  as heads for the baseline and the reordering ensemble  model. As shown in the table, our model improves over  the baseline in most cases.", "labels": [], "entities": []}]}