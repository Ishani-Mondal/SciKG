{"title": [{"text": "What's in a Name? Reducing Bias in Bios without Access to Protected Attributes", "labels": [], "entities": [{"text": "Reducing Bias in Bios without Access to Protected Attributes", "start_pos": 18, "end_pos": 78, "type": "TASK", "confidence": 0.7527178857061598}]}], "abstractContent": [{"text": "There is a growing body of work that proposes methods for mitigating bias in machine learning systems.", "labels": [], "entities": []}, {"text": "These methods typically rely on access to protected attributes such as race, gender, or age.", "labels": [], "entities": []}, {"text": "However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections.", "labels": [], "entities": []}, {"text": "In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual's true occupation and a word embedding of their name.", "labels": [], "entities": [{"text": "occupation classification", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6882616728544235}]}, {"text": "This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes.", "labels": [], "entities": []}, {"text": "Crucially, it only requires access to individuals' names at training time and not at deployment time.", "labels": [], "entities": []}, {"text": "We evaluate two variations of our proposed method using a large-scale dataset of online biographies.", "labels": [], "entities": []}, {"text": "We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.", "labels": [], "entities": [{"text": "true positive rate", "start_pos": 128, "end_pos": 146, "type": "METRIC", "confidence": 0.8008801142374674}]}], "introductionContent": [{"text": "In recent years, the performance of machine learning systems has improved substantially, leading to the widespread use of machine learning \"What's in a name?", "labels": [], "entities": [{"text": "machine learning \"What's in a name?", "start_pos": 122, "end_pos": 157, "type": "TASK", "confidence": 0.6976179016960992}]}, {"text": "That which we calla rose by any other name would smell as sweet.\"", "labels": [], "entities": []}, {"text": "-William Shakespeare, in many domains, including high-stakes domains such as healthcare, employment, and criminal justice (.", "labels": [], "entities": []}, {"text": "This increased prevalence has led many people to ask the question, \"accurate, but for whom?\".", "labels": [], "entities": [{"text": "prevalence", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9953972697257996}, {"text": "accurate", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9886091947555542}]}, {"text": "When the performance of a machine learning system differs substantially for different groups of people, a number of concerns arise.", "labels": [], "entities": []}, {"text": "First and foremost, there is a risk that the deployment of such a method may harm already marginalized groups and widen existing inequalities.", "labels": [], "entities": []}, {"text": "Recent work highlights this concern in the context of online recruiting and automated hiring . When predicting an individual's occupation from their online biography, the authors show that if occupation-specific gender gaps in true positive rates are correlated with existing gender imbalances in those occupations, then those imbalances will be compounded over timea phenomenon sometimes referred to as the \"leaky pipeline.\"", "labels": [], "entities": []}, {"text": "Second, the correlations that lead to performance differences between groups are often irrelevant.", "labels": [], "entities": []}, {"text": "For example, while an occupation classifier should predict a higher probability of software engineer if an individual's biography mentions coding experience, there is no good reason for it to predict a lower probability of software engineer if the biography also mentions softball.", "labels": [], "entities": []}, {"text": "Prompted by such concerns about bias in machine learning systems, there is a growing body of work on fairness in machine learning.", "labels": [], "entities": []}, {"text": "Some of the foundational papers in this area highlighted the limitations of trying to mitigate bias using methods that are \"unaware\" of protected attributes such as race, gender, or age (e.g.,.", "labels": [], "entities": []}, {"text": "As a result, subsequent work has primarily focused on introducing fairness constraints, defined in terms of protected attributes, that reduce incentives to rely on undesirable correlations (e.g.,.", "labels": [], "entities": []}, {"text": "This approach is particularly useful if similar performance can be achieved by slightly different means-i.e., fairness constraints may aid in model selection if there are many near-optima.", "labels": [], "entities": []}, {"text": "In practice, though, any approach that relies on protected attributes may stand at odds with antidiscrimination law, which limits the use of protected attributes in domains such as employment and education, even for the purpose of mitigating bias.", "labels": [], "entities": []}, {"text": "And, in other domains, protected attributes are often not available (.", "labels": [], "entities": []}, {"text": "Moreover, even when they are, it is usually desirable to simultaneously consider multiple protected attributes, as well as their intersections.", "labels": [], "entities": []}, {"text": "For example, showed that commercial gender classifiers have higher error rates for women with darker skin tones than for either women or people with darker skin tones overall.", "labels": [], "entities": [{"text": "error", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9844532608985901}]}, {"text": "We propose a method for reducing bias in machine learning classifiers without relying on protected attributes.", "labels": [], "entities": []}, {"text": "In the context of occupation classification, this method discourages a classifier from learning a correlation between the predicted probability of an individual's occupation and a word embedding of their name.", "labels": [], "entities": [{"text": "occupation classification", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7187006920576096}]}, {"text": "Intuitively, the probability of an individual's occupation should not depend on their name-nor on any protected attributes that maybe inferred from it.", "labels": [], "entities": []}, {"text": "We present two variations of our method-i.e., two loss functions that enforce this constraint-and show that they simultaneously reduce both race and gender biases with little reduction in classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9426921010017395}]}, {"text": "Although we are motivated by the need to mitigate bias in online recruiting and automated hiring, our method can be applied in any domain where individuals' names are available at training time.", "labels": [], "entities": []}, {"text": "Instead of relying on protected attributes, our method leverages the societal biases that are encoded in word embeddings ().", "labels": [], "entities": []}, {"text": "In particular, we build on the work of, which showed that word embeddings of names typically reflect the societal biases that are associated with those names, including race, gender, and age biases, as well encoding information about other factors that influence naming practices such as nationality and religion.", "labels": [], "entities": []}, {"text": "By using word embeddings of names as a tool for mitigating bias, our method is conceptually simple and empirically powerful.", "labels": [], "entities": []}, {"text": "Much like the \"proxy fairness\" approach of, it is applicable when protected attributes are not available; however, it additionally eliminates the need to specify which biases are to be mitigated, and allows simultaneous mitigation of multiple biases, including those that relate to group intersections.", "labels": [], "entities": []}, {"text": "Moreover, our method only requires access to proxy information (i.e., names) at training time and not at deployment time, which avoids disparate treatment concerns and extends fairness gains to individuals with ambiguous names.", "labels": [], "entities": []}, {"text": "For example, a method that explicitly or implicitly infers protected attributes from names at deployment time may fail to correctly infer that an individual named Alexis female and, in turn, fail to mitigate gender bias for her.", "labels": [], "entities": []}, {"text": "Methodologically, our work is also similar to that of, which promotes fairness by requiring that the covariance between a protected attribute and a data point's distance from a classifier's decision boundary is smaller than some constant.", "labels": [], "entities": []}, {"text": "However, unlike our method, it requires access to protected attributes, and does not facilitate simultaneous mitigation of multiple biases.", "labels": [], "entities": []}, {"text": "We present our method in Section 2.", "labels": [], "entities": []}, {"text": "In section 3, we describe our evaluation, followed by results in Section 4 and conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of our method's strengths is its ability to simultaneously mitigate multiple biases without access to protected attributes; however, this strength also poses a challenge for evaluation.", "labels": [], "entities": []}, {"text": "We are unable to quantify this ability without access to these attributes.", "labels": [], "entities": []}, {"text": "To facilitate evaluation, we focus on race and gender biases only because race and gender attributes are more readily available than attributes corresponding to other biases.", "labels": [], "entities": []}, {"text": "We further conceptualize both race and gender to be binary (\"white/non-white\" and \"male/female\") but note that these conceptualizations are unrealistic, reductive simplifications that fail to capture many aspects of race and gender, and erase anyone who does not fit within their assumptions.", "labels": [], "entities": []}, {"text": "We emphasize that we use race and gender attributes only for evaluation-they do not play a role in our method.", "labels": [], "entities": []}, {"text": "We use two datasets to evaluate our method: the adult income dataset from the UCI Machine Learning Repository (, where the task is to predict whether an individual earns more than $50k per year (i.e., whether their occupation is \"high status\"), and a dataset of online biographies (De-Arteaga et al., 2019), where the task is to predict an individual's occupation from the text of their online biography.", "labels": [], "entities": [{"text": "UCI Machine Learning Repository", "start_pos": 78, "end_pos": 109, "type": "DATASET", "confidence": 0.9132932424545288}]}, {"text": "Each data point in the Adult dataset consists of a set of binary, categorical, and continuous attributes, including race and gender.", "labels": [], "entities": [{"text": "Adult dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9342528283596039}]}, {"text": "We preprocess these attributes to more easily allow us to understand the classifier's decisions.", "labels": [], "entities": []}, {"text": "Specifically, we normalize continuous attributes to be in the range and we convert categorical attributes into binary indicator variables.", "labels": [], "entities": []}, {"text": "Because the data points do not have names associated with them, we generate synthetic first names using the race and gender attributes.", "labels": [], "entities": []}, {"text": "First, we use the dataset of Tzioumis (2018) to identify \"white\" and \"nonwhite\" names.", "labels": [], "entities": [{"text": "Tzioumis (2018)", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7989508807659149}]}, {"text": "For each name, if the proportion of \"white\" people with that name is higher than 0.5, we deem the name to be \"white;\" otherwise, we deem it to be \"non-white.\"", "labels": [], "entities": []}, {"text": "1 Next, we use Social Security Administration data about baby names (2018) to identify \"male\" and \"female\" names.", "labels": [], "entities": [{"text": "Social Security Administration data", "start_pos": 15, "end_pos": 50, "type": "DATASET", "confidence": 0.699642077088356}]}, {"text": "For each name, if the proportion of boys 1 For 90% of the names, the proportion of \"white\" people with that name is greater than 0.7 or less than 0.3, so there is a clear distinction between \"white\" and \"non-white\" names. with that name is higher than 0.5, we deem the name to be \"male;\" otherwise, we deem it to be \"female.\"", "labels": [], "entities": []}, {"text": "We then take the intersection of these two sets of names to yield a single set of names that is partitioned into four non-overlapping categories by (binary) race and gender.", "labels": [], "entities": []}, {"text": "Finally, we generate a synthetic first name for each data point by sampling a name from the relevant category.", "labels": [], "entities": []}, {"text": "Each data point in the Bios dataset consists of the text of an individual's biography, written in the third person.", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8507611751556396}]}, {"text": "We represent each biography as a vector of length V , where V is the size of the vocabulary.", "labels": [], "entities": []}, {"text": "Each element corresponds to a single word type and is equal to 1 if the biography contains that type (and 0 otherwise).", "labels": [], "entities": []}, {"text": "We limit the size of the vocabulary by discarding the 10% most common word types, as well as any word types that occur fewer than twenty times.", "labels": [], "entities": []}, {"text": "Unlike the Adult dataset, each data point has a name associated with it.", "labels": [], "entities": [{"text": "Adult dataset", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9640953242778778}]}, {"text": "And, because biographies are typically written in the third person and because pronouns are gendered in English, we can extract (likely) self-identified gender.", "labels": [], "entities": []}, {"text": "We infer race for each data point by sampling from a Bernoulli distribution with probability equal to the average of the probability that an individual with that first name is \"white\" (from the dataset of Tzioumis (2018), using a threshold of 0.5, as described above) and the probability that an individual with that last name is \"white\" (from the dataset of Comenetz (2016), also using a threshold of 0.5).", "labels": [], "entities": [{"text": "Comenetz", "start_pos": 359, "end_pos": 367, "type": "DATASET", "confidence": 0.9111051559448242}]}, {"text": "Finally, like De-Arteaga et al.", "labels": [], "entities": []}, {"text": "(2019), we consider two versions of the Bios dataset: one where first names and pronouns are available to the classifier and one where they are \"scrubbed.\"", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9217123687267303}]}, {"text": "Throughout our evaluation, we use the fastText word embeddings, pretrained on Common Crawl data (, to represent names.", "labels": [], "entities": [{"text": "Common Crawl data", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.7999284863471985}]}, {"text": "The results of our evaluation using the Adult dataset are shown in.", "labels": [], "entities": [{"text": "Adult dataset", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9791238307952881}]}, {"text": "The task is to predict whether an individual earns more than $50k per year (i.e., whether their occupation is \"high status\").", "labels": [], "entities": []}, {"text": "Because the dataset has a strong class imbalance, we report the balanced TPR-i.e., we compute the per-class TPR and then average over the classes.", "labels": [], "entities": [{"text": "TPR-i.e.", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9738121628761292}]}, {"text": "We experiment with different values of the hyperparameter \u03bb.", "labels": [], "entities": []}, {"text": "When \u03bb = 0, the method is equivalent to using the conventional weighted cross-entropy loss function.", "labels": [], "entities": []}, {"text": "Larger values of \u03bb increase the strength of the penalty, but may lead to  The results of our evaluation using the original and \"scrubbed\" (i.e., names and pronouns are \"scrubbed\") versions of the Bios dataset are shown in Tables 2 and 3, respectively.", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 196, "end_pos": 208, "type": "DATASET", "confidence": 0.9146512746810913}]}, {"text": "The task is to predict an individual's occupation from the text of their online biography.", "labels": [], "entities": []}, {"text": "Because the dataset has a strong class imbalance, we again report the balanced TPR.", "labels": [], "entities": [{"text": "TPR", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8651939034461975}]}, {"text": "CluCL and CoCL reduce race and gender biases for both versions of the dataset.", "labels": [], "entities": [{"text": "CluCL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9202138781547546}]}, {"text": "For the original version, CluCL reduces the root mean square TPR gender gap from 0.173 to 0.165 and the maximum TPR gender gap by 2.5%.", "labels": [], "entities": [{"text": "CluCL", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.7908082008361816}, {"text": "root mean square TPR gender gap", "start_pos": 44, "end_pos": 75, "type": "METRIC", "confidence": 0.716769297917684}, {"text": "TPR gender gap", "start_pos": 112, "end_pos": 126, "type": "METRIC", "confidence": 0.8373629450798035}]}, {"text": "Race bias is also reduced, though to a lesser extent.", "labels": [], "entities": [{"text": "Race bias", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6022082716226578}]}, {"text": "These reductions reduce the balanced TPR by 0.7%.", "labels": [], "entities": [{"text": "TPR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8976407647132874}]}, {"text": "For the \"scrubbed\" version, the reductions in race and gender biases are even smaller, likely because most of the information about race and gender has been removed by \"scrubbing\" names and pronouns.", "labels": [], "entities": []}, {"text": "We hypothesize that these smaller reductions in race and gender biases, compared to the Adult dataset, are because the Adult dataset has fewer attributes and classes than the Bios dataset, and contains explicit race and gender information, making the task of reducing biases much simpler.", "labels": [], "entities": [{"text": "Adult dataset", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9358469843864441}, {"text": "Adult dataset", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.9165815114974976}, {"text": "Bios dataset", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.916546106338501}]}, {"text": "We also note that each biography in the Bios dataset is represented as a vector of length V , where V is over 11,000.", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9444880485534668}]}, {"text": "This means that the corresponding classifier has a very large number of weights, and there is a strong overfitting effect.", "labels": [], "entities": []}, {"text": "Because this overfitting effect increases with \u03bb, we suspect it explains why CluCL has a larger root mean square TPR gender gap when \u03bb = 2 than when \u03bb = 1.", "labels": [], "entities": [{"text": "root mean square TPR gender gap", "start_pos": 96, "end_pos": 127, "type": "METRIC", "confidence": 0.6964230686426163}]}, {"text": "Indeed, the root mean square TPR gender gap for the training set is 0.05 when \u03bb = 2.", "labels": [], "entities": [{"text": "TPR gender gap", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.8944675326347351}]}, {"text": "Using dropout and 2 weight regularization lessened this effect, but did not eliminate it entirely.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the Adult dataset. Balanced TPR (i.e., per-occupation TPR, averaged over occupations), gender  bias quantified as Gap RMS", "labels": [], "entities": [{"text": "Adult dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8536723256111145}, {"text": "Balanced TPR", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9134287238121033}, {"text": "Gap RMS", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.8343138992786407}]}, {"text": " Table 2: Results for the original Bios dataset. Balanced TPR (i.e., per-occupation TPR, averaged over occupations),  gender bias quantified as Gap RMS", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8413849771022797}, {"text": "Balanced TPR", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9043024480342865}, {"text": "Gap RMS", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.8549119532108307}]}, {"text": " Table 3: Results for the \"scrubbed\" Bios dataset. Balanced TPR (i.e., per-occupation TPR, averaged over oc- cupations), gender bias quantified as Gap RMS", "labels": [], "entities": [{"text": "Bios dataset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8580625057220459}, {"text": "Balanced TPR", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.8719571828842163}, {"text": "Gap RMS", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.8061196506023407}]}]}