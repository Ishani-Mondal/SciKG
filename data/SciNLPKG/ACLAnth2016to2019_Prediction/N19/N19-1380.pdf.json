{"title": [{"text": "Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog", "labels": [], "entities": [{"text": "Cross-Lingual Transfer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7472537457942963}, {"text": "Multilingual Task Oriented Dialog", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6547574996948242}]}], "abstractContent": [{"text": "One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots.", "labels": [], "entities": [{"text": "utterance interpretation pipeline", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8806123733520508}]}, {"text": "Since data collection for machine learning models for this task is time-consuming, it is desirable to make use of existing data in a high-resource language to train models in low-resource languages.", "labels": [], "entities": []}, {"text": "However, development of such models has largely been hindered by the lack of multilingual training data.", "labels": [], "entities": []}, {"text": "In this paper, we present anew data set of 57k annotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the domains weather, alarm, and reminder.", "labels": [], "entities": []}, {"text": "We use this data set to evaluate three different cross-lingual transfer methods: (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations.", "labels": [], "entities": []}, {"text": "We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data.", "labels": [], "entities": []}, {"text": "Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings.", "labels": [], "entities": []}, {"text": "We also compare the cross-lingual methods to using monolingual resources in the form of con-textual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the first steps in many conversational AI systems that are used to parse utterances in personal assistants is the identification of what the user intends to do (the intent) as well as the arguments of the intent (the slots); * Work carried out during an internship at Facebook..", "labels": [], "entities": []}, {"text": "For example, fora request such as Set an alarm for tomorrow at 7am, a first step in fulfilling such a request is to identify that the user's intent is to set an alarm and that the required time argument of the request is expressed by the phrase tomorrow at 7am.", "labels": [], "entities": []}, {"text": "Given these properties of the task, the problem can be stated as a joint sentence classification (for intent classification) and sequence labeling (for slot detection) task and therefore naturally lends itself to using a biLSTM-CRF sequence labeling model ( where the biLSTM layer is also used as the input fora projection layer for intent detection.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.718254566192627}, {"text": "slot detection", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.7235538959503174}, {"text": "intent detection", "start_pos": 333, "end_pos": 349, "type": "TASK", "confidence": 0.7130914479494095}]}, {"text": "These models are very powerful and given enough training data, they achieve very high accuracy on the intent classification as well as the slot detection task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9991507530212402}, {"text": "intent classification", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.6872649043798447}, {"text": "slot detection task", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.9218602379163107}]}, {"text": "However, given the requirement of large amounts of labeled training data, developing a conversational AI system for many new languages is a very resource-intensive task and clearly not feasible to be done for the more than 6,500 languages that are currently spoken around the world.", "labels": [], "entities": []}, {"text": "For this reason, one would like to make use of methods that enable transfer learning from a highresource language to a low-resource language.", "labels": [], "entities": []}, {"text": "However, the development of sophisticated crosslingual transfer methods for intent detection and slot filling has so far been hindered by the lack of multilingual data sets that have been annotated according to the same guidelines.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8446590006351471}, {"text": "slot filling", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.8909164071083069}]}, {"text": "In this work, we therefore present a novel data set containing a large number of English utterances (the highresource data) as well as a smaller set of utterances in Spanish and in Thai (the low-resource data), which were annotated according to the same annotation scheme.", "labels": [], "entities": []}, {"text": "This data allows the systematic in-: Summary statistics of the data set.", "labels": [], "entities": []}, {"text": "The three values for the number of utterances correspond to the number of utterances in the training, development, and test splits.", "labels": [], "entities": []}, {"text": "Note that the slot type datetime is shared across all three domains and therefore the total number of slot types is only 11.", "labels": [], "entities": []}, {"text": "vestigation of cross-lingual transfer learning methods from high-resource languages to low-resource languages.", "labels": [], "entities": []}, {"text": "We use this data set to explore different strategies to make use of training data from a highresource language to improve intent and slot detection models for other languages.", "labels": [], "entities": [{"text": "slot detection", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7941754460334778}]}, {"text": "We investigate two previously proposed strategies for crosslingual transfer, namely using cross-lingual pretrained embeddings fora review) as well as automatically translating the English training data to the target language.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.762162059545517}]}, {"text": "Further, we present a novel technique that uses a bidirectional neural machine translation encoder as cross-lingual contextual word representations.", "labels": [], "entities": [{"text": "bidirectional neural machine translation encoder", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.8018839478492736}]}, {"text": "We compare the cross-lingual transfer methods to models that are only trained on the target language data.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6925849616527557}]}, {"text": "Across the two languages and the various transfer methods, we find that joint training on the highresource and the low-resource target language improves results on the target language.", "labels": [], "entities": []}, {"text": "We further find that the optimal choice of transfer method depends on the size of the available training data in the target language: In the zero-shot case where no target language data is available, translating the training data gives the best results.", "labels": [], "entities": []}, {"text": "However, if a small amount of training data is available, we find that jointly training on the high-resource and lowresource data works better than training on translated data.", "labels": [], "entities": []}, {"text": "We release the data at https://fb.me/ multilingual_task_oriented_data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary statistics of the data set. The three values for the number of utterances correspond to the number  of utterances in the training, development, and test splits. Note that the slot type datetime is shared across all three  domains and therefore the total number of slot types is only 11.", "labels": [], "entities": []}, {"text": " Table 2: Perplexities on validation set for different encoder models for the Spanish-English and Thai-English  language pairs. A hyphen means that an encoder was not trained for the corresponding language pair.", "labels": [], "entities": []}, {"text": " Table 3: Results using the full training data averaged over 5 training runs. The translate train models are trained  on the union of translated English and target language data; the cross-lingual models are trained on English and  target language data.", "labels": [], "entities": []}, {"text": " Table 4: Zero-shot results averaged over 5 training runs. All models were trained only on the English data. In the  case of the translate train models, the English data was automatically translated into the target language.", "labels": [], "entities": []}]}