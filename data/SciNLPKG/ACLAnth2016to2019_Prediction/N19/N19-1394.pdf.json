{"title": [{"text": "Recommendations for Datasets for Source Code Summarization", "labels": [], "entities": [{"text": "Source Code Summarization", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6452782154083252}]}], "abstractContent": [{"text": "Source Code Summarization is the task of writing short, natural language descriptions of source code.", "labels": [], "entities": [{"text": "Source Code Summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6099783678849539}]}, {"text": "The main use for these descriptions is in software documentation e.g. the one-sentence Java method descriptions in JavaDocs.", "labels": [], "entities": []}, {"text": "Code summarization is rapidly becoming a popular research problem, but progress is restrained due to alack of suitable datasets.", "labels": [], "entities": [{"text": "Code summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8015058636665344}]}, {"text": "In addition, alack of community standards for creating datasets leads to confusing and unreproducible research results-we observe swings in performance of more than 33% due only to changes in dataset design.", "labels": [], "entities": []}, {"text": "In this paper, we make recommendations for these standards from experimental results.", "labels": [], "entities": []}, {"text": "We release a dataset based on prior work of over 2.1m pairs of Java methods and one sentence method descriptions from over 28k Java projects.", "labels": [], "entities": []}, {"text": "We describe the dataset and point out key differences from natural language data, to guide and support future researchers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Source Code Summarization is the task of writing short, natural language descriptions of source code ().", "labels": [], "entities": [{"text": "Source Code Summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6302229364713033}]}, {"text": "The most common use for these descriptions is in software documentation, such as the summaries of Java methods in JavaDocs.", "labels": [], "entities": []}, {"text": "Automatic generation of code summaries is a rapidly-expanding research area at the crossroads of Computational Linguistics and Software Engineering, as a growing tally of new workshops and NSF-sponsored meetings have recognized.", "labels": [], "entities": [{"text": "Automatic generation of code summaries", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8836589694023133}]}, {"text": "The reason, in a nutshell, is that the vast majority of code summarization techniques are adaptations of techniques originally designed to solve NLP problems.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.6986332833766937}]}, {"text": "A major barrier to ongoing research is alack of standardized datasets.", "labels": [], "entities": []}, {"text": "In many NLP tasks such as Machine Translation there are large, curated datasets (e.g. Europarl) used by several research groups.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8682125806808472}, {"text": "Europarl", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9666813015937805}]}, {"text": "The benefit of these standardized datasets is twofold: First, scientists are able to evaluate new techniques using the same test conditions as older techniques.", "labels": [], "entities": []}, {"text": "And second, the datasets tend to conform to community customs of best practice, which avoids errors during evaluation.", "labels": [], "entities": []}, {"text": "These benefits are generally not yet available to code summarization researchers; while large, public code repositories do exist, most research projects must parse and process these repositories on their own, leading to significant differences on one project to another.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6547969877719879}]}, {"text": "The result is that research progress is slowed as reproducibilty of earlier results is difficult.", "labels": [], "entities": []}, {"text": "Inevitably, differences in dataset creation also occur that can mislead researchers and over or understate the performance of some techniques.", "labels": [], "entities": []}, {"text": "For example, a recent source code summarization paper reports achieving 25 BLEU when generating English descriptions of Java methods with an existing technique (, which is 5 points higher than the original paper reports.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9991306662559509}]}, {"text": "The paper also reports 35+ BLEU fora vanilla seq2seq NMT model, which is 16 points higher than what we are able to replicate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9990849494934082}]}, {"text": "While it is not our intent to single out anyone paper, we do wish to call attention to a problem in the research area generally: alack of standard datasets leads to results that are difficult to interpret and replicate.", "labels": [], "entities": []}, {"text": "In this paper, we propose a set of guidelines for building datasets for source code summarization techniques.", "labels": [], "entities": [{"text": "source code summarization", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.5546000500520071}]}, {"text": "We support our guidelines with related literature or experimentation where strong literary consensus is not available.", "labels": [], "entities": []}, {"text": "We also compute several metrics related to word usage to guide future researchers who use the dataset.", "labels": [], "entities": []}, {"text": "We have made a dataset of over 2.1m Java methods and summaries from over 28k Java projects available via an online appendix (URL in Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we use in this paper is based on the dataset provided by) in a pre-release.", "labels": [], "entities": []}, {"text": "We used this dataset because it is both the largest and most recent in source code summarization.", "labels": [], "entities": []}, {"text": "That dataset has its origins in the Sourcerer project by, which includes over 51 million Java methods.", "labels": [], "entities": []}, {"text": "LeClair et al. provided the dataset after minimal initial processing that filtered for Java methods with JavaDoc comments in English, and removed methods over 100 words long and comments >13 and <3 words.", "labels": [], "entities": []}, {"text": "The result is a dataset of 2.1m Java methods and associated comments.", "labels": [], "entities": []}, {"text": "LeClair et al. do additional processing, but do not quantify the effects of their decisions -this is a problem because other researchers would not know which of the decisions to follow.", "labels": [], "entities": []}, {"text": "We explore the following research questions to help provide guidelines and justifications for our design decisions in creating the dataset.", "labels": [], "entities": []}, {"text": "We make three observations about the dataset that, in our view, are likely to affect how researchers design source code summarization algorithms.", "labels": [], "entities": [{"text": "source code summarization", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.5900663336118063}]}, {"text": "First, as depicted in, words appear to be used more often in code as compared to natural language -there are fewer words used only one or two times, and in general more used 3+ times.", "labels": [], "entities": []}, {"text": "At the same time), the pattern for word occurrences per document appears similar, implying that even though words in code are repeated, they are repeated often in the same method and not across methods.", "labels": [], "entities": []}, {"text": "Even though this may suggest that the occurrence of unique words in source code is isolated enough to have little affect on BLEU score, we show in Section 4 that this word overlap causes BLEU score inflation when you split by function.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9846412539482117}, {"text": "BLEU score inflation", "start_pos": 187, "end_pos": 207, "type": "METRIC", "confidence": 0.9814495245615641}]}, {"text": "This is important because the typical MT use case assumes that a \"dictionary\" can be created (e.g., via attention) to map words in a source to words in a target language.", "labels": [], "entities": [{"text": "MT use", "start_pos": 38, "end_pos": 44, "type": "TASK", "confidence": 0.8999562561511993}]}, {"text": "An algorithm applied to code summarization needs to tolerate multiple occurrences of the same words.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.575051411986351}]}, {"text": "To compare the source code, comments, and natural language datasets we tokenized our data by removing all special characters, lower casing, and for source code -splitting camel case into separate tokens.", "labels": [], "entities": []}, {"text": "A related observation is that Java methods tend to be much longer than comments areas (c) and (d)).", "labels": [], "entities": []}, {"text": "Typically, code summarization tools take inspiration from NMT algorithms designed for cases of similar encoder/decoder sequence length.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7361747920513153}]}, {"text": "Many algorithms such as recurrent networks are sensitive to sequence length, and may not be optimal off-the-shelf.", "labels": [], "entities": []}, {"text": "A third observation is that the words in methods and comments tend to overlap, but in fact avast majority of words are different (70% of words in code summary comments do not occur in the code method, see area (b)).", "labels": [], "entities": []}, {"text": "This situation makes the code summarization problem quite difficult because the words in the comments represent high level concepts, while the words in the source code represent low level implementation details -a situation known as the \"concept assignment problem\" (.", "labels": [], "entities": []}, {"text": "A code summarization algorithm cannot only learn a word dictionary as it might in atypical NMT setting, or select summarizing words from the method fora summary as a natural language summarization tool might.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.6026411056518555}]}, {"text": "A code summarization algorithm must learn to identify concepts from code details, and assign high level terms to those concepts.", "labels": [], "entities": [{"text": "code summarization", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.6778784990310669}]}, {"text": "In this section, we answer our Research Questions and provide supporting evidence and rational.", "labels": [], "entities": []}, {"text": "In our online appendix we have made three downloadable sets available.", "labels": [], "entities": []}, {"text": "The first is our SQL database, generated using the tool from, that contains the filename, method comment, and start/end lines for each method, we call this dataset our \"Raw Dataset\".", "labels": [], "entities": []}, {"text": "We also provide a link to the Sourcerer dataset ( which is used as abase for the dataset in.", "labels": [], "entities": [{"text": "Sourcerer dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9322225749492645}]}, {"text": "In addition to the Raw Dataset, we also provide a \"Filtered Dataset\" that consists of a set of 2.1m method comment pairs.", "labels": [], "entities": []}, {"text": "In the Filtered Dataset we removed auto-generated source code files, as well all method's that do not have an associated comment.", "labels": [], "entities": [{"text": "Filtered Dataset", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8326852321624756}]}, {"text": "No preprocessing was applied to the source code and comment strings in the Filtered Dataset.", "labels": [], "entities": [{"text": "Filtered Dataset", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.9308324754238129}]}, {"text": "The third downloadable set we supply is the \"Tokenized Dataset\".", "labels": [], "entities": [{"text": "Tokenized Dataset\"", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.9248985648155212}]}, {"text": "In the Tokenized Dataset, we processed the source code and comments from the Filtered Dataset identically to the tokenization scheme described in Section 5 of (.", "labels": [], "entities": [{"text": "Tokenized Dataset", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8573510646820068}, {"text": "Filtered Dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.7190441936254501}]}, {"text": "This set also provides a training, validation, and test set as well as a script to easily reshuffle these sets.", "labels": [], "entities": []}, {"text": "The URL for download is: http://leclair.tech/data/funcom", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average BLEU Scores from 15 epochs for each of the four sets.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9557053446769714}, {"text": "BLEU Scores", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.940042644739151}]}, {"text": " Table 2: Number of method-comment pairs in the train, validation, test sets used in each random split set when  split by project (BP) and by function (BF).", "labels": [], "entities": []}]}