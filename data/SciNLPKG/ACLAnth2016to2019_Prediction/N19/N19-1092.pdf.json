{"title": [{"text": "How to Avoid Sentences Spelling Boring? Towards a Neural Approach to Unsupervised Metaphor Generation", "labels": [], "entities": [{"text": "Avoid Sentences Spelling Boring", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.7161639407277107}]}], "abstractContent": [{"text": "Metaphor generation attempts to replicate human creativity with language, which is an attractive but challengeable text generation task.", "labels": [], "entities": [{"text": "Metaphor generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.872978150844574}, {"text": "text generation", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.7568335831165314}]}, {"text": "Previous efforts mainly focus on template-based or rule-based methods and result in alack of linguistic subtlety.", "labels": [], "entities": []}, {"text": "In order to create novel metaphors, we propose a neural approach to metaphor generation and explore the shared inferential structure of a metaphorical usage and a literal usage of a verb.", "labels": [], "entities": [{"text": "metaphor generation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7468638122081757}]}, {"text": "Our approach does not require any manually annotated metaphors for training.", "labels": [], "entities": []}, {"text": "We extract the me-taphorically used verbs with their metaphori-cal senses in an unsupervised way and train a neural language model from wiki corpus.", "labels": [], "entities": []}, {"text": "Then we generate metaphors conveying the assigned metaphorical senses with an improved decoding algorithm.", "labels": [], "entities": []}, {"text": "Automatic metrics and human evaluations demonstrate that our approach can generate metaphors with good readability and creativity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Metaphor is a kind of language filled with vitality and elasticity.", "labels": [], "entities": []}, {"text": "It employs words in away that deviates from their normal meaning to represent another concept (.", "labels": [], "entities": []}, {"text": "Using metaphor allows us to express not just information but also real feelings and complex attitudes.", "labels": [], "entities": []}, {"text": "There is a clear need in computational metaphor generation whose insightful results can be used in many applications from entertainment to education.", "labels": [], "entities": [{"text": "computational metaphor generation", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.704013466835022}]}, {"text": "Besides, a unified metaphor annotation procedure and creation of a large publicly available metaphor corpus are in great demands.", "labels": [], "entities": []}, {"text": "Such resources make it possible to interpretate the identified metaphorical expressions and enhance the performance on other Natural Language Processing (NLP) applications.", "labels": [], "entities": [{"text": "interpretate the identified metaphorical expressions", "start_pos": 35, "end_pos": 87, "type": "TASK", "confidence": 0.7610390424728394}]}, {"text": "Although metaphor has along history of academic studies in both philosophy and linguistics, it still remains a tough problem for the NLP community.", "labels": [], "entities": []}, {"text": "As the metaphor is hardly to be well-defined and modeled, little work focuses on the metaphor generation.", "labels": [], "entities": []}, {"text": "Most of the previous efforts rely on hand-coded knowledge , which heavily constrains the diversity of generated metaphors.", "labels": [], "entities": []}, {"text": "The end-to-end approach presented to sequence learning has been proved effective on the generation tasks like machine translation), abstractive summarization (, product review generation ( and multi-label classification (.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7730028927326202}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8007257878780365}, {"text": "abstractive summarization", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.705233097076416}, {"text": "product review generation", "start_pos": 161, "end_pos": 186, "type": "TASK", "confidence": 0.8220709562301636}, {"text": "multi-label classification", "start_pos": 193, "end_pos": 219, "type": "TASK", "confidence": 0.7899533212184906}]}, {"text": "The approach is able to train a language model which can generate fluent and creative sentences with sufficient corpus.", "labels": [], "entities": []}, {"text": "Unfortunately, in spite of the industrious exploration of the metaphor corpus, the annotated data available now is still far from training a good language model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there has been no work combining metaphor generation with the end-toend approach.", "labels": [], "entities": [{"text": "metaphor generation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7906771898269653}]}, {"text": "In this paper, we propose a neural approach for metaphor generation trained with Wiki corpus rather than the limited annotated metaphor corpus, which assures the quality of the language model.", "labels": [], "entities": [{"text": "metaphor generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8896643221378326}, {"text": "Wiki corpus", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9215144515037537}]}, {"text": "The approach is shown in.", "labels": [], "entities": []}, {"text": "Relevant sta-tistics demonstrate that the most frequent type of metaphor is expressed by a verb.", "labels": [], "entities": []}, {"text": "In this paper, we focus on generating verb-oriented metaphors.", "labels": [], "entities": []}, {"text": "We use an unsupervised method to extract the metaphorically used verbs in Wiki corpus.", "labels": [], "entities": [{"text": "Wiki corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9047896862030029}]}, {"text": "A metaphorical pair consists of a target verb (e.g. \"devoured\") and a fit word (e.g. \"enjoyed\").", "labels": [], "entities": []}, {"text": "And it is used to model the metaphorical usage of the target verb.", "labels": [], "entities": []}, {"text": "According to, a metaphorical usage and a literal usage share inferential structure.", "labels": [], "entities": []}, {"text": "We follow this intuition to find an intersection between the metaphorical usage and the literal usage of a word.", "labels": [], "entities": []}, {"text": "For example, in \"she devoured (enjoyed) his novels\", the literal sense of \"enjoyed\" represents the contextual sense of \"devoured\" in such contexts.", "labels": [], "entities": []}, {"text": "But the similarity between \"enjoyed\" and \"devoured\" is low, which means the target verb \"devoured\" is merely used in such sense and can be seen as a metaphorically used verb.", "labels": [], "entities": []}, {"text": "For metaphor generation, we first propose a POS constrained language model to generate a sentence containing a given verb and use an elaborately designed algorithm to consider its fit word simultaneously while decoding.", "labels": [], "entities": [{"text": "metaphor generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8945260643959045}]}, {"text": "For a profound exploration, we introduce automatic metrics as well as manual ways to evaluate the generation results.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our approach is capable of generating fluent and seemly metaphors.", "labels": [], "entities": []}, {"text": "All the generated metaphors are novel and do not exist in the corpus.", "labels": [], "entities": []}, {"text": "To summarise, the contributions of our work are as follows 1 : \u2022 As far as we know, our work is the first endeavor to adopt the end-to-end framework on metaphor generation.", "labels": [], "entities": [{"text": "metaphor generation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7960394024848938}]}, {"text": "Besides, the proposed method does not require any manually labeled metaphor corpus.", "labels": [], "entities": []}, {"text": "\u2022 We automatically extract the verbs and their fit words in the corpus in an unsupervised way and use them (e.g. \"devoured\", \"enjoyed\") to model the metaphorical senses of the verbs for the generation process.", "labels": [], "entities": []}, {"text": "We further explore the semantic shift of a verb by changing the adjustable factors.", "labels": [], "entities": []}, {"text": "\u2022 Both automatic metrics and human evaluation results demonstrate the efficacy of our mo-del.", "labels": [], "entities": []}, {"text": "Our model outperforms the baseline models on 3 aspects significantly 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We extract the metaphorical pairs automatically in the normal corpus and conduct experiments.", "labels": [], "entities": []}, {"text": "The automatic evaluation results are shown in Each target verb may form distinct metaphorical pairs with different fit words.", "labels": [], "entities": []}, {"text": "Both the normal model and the POS constrained model generate the same sentences for the same target verbs while the fit word model generates the same sentences for the same fit words.", "labels": [], "entities": []}, {"text": "Taking the metaphorical pairs into consideration, adjustable joint model can generate various sentences when the target words are the same but used in diffe- rent senses.", "labels": [], "entities": []}, {"text": "For a fair quantitative analysis, each model generates 1555 sentences for distinct target verbs.", "labels": [], "entities": []}, {"text": "Length-average (l.ave) is the average length of the generated sentences of each model.", "labels": [], "entities": [{"text": "Length-average (l.ave)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.8862428069114685}]}, {"text": "Word.number(w.num) is the total number of the distinct words in the generated sentences, and the adjustable joint model decodes words with the highest diversity compared to the other models.", "labels": [], "entities": []}, {"text": "We use the language modeling toolkit SRILM 6 to evaluate the perplexity scores (ppl.).", "labels": [], "entities": [{"text": "SRILM 6", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7731554210186005}]}, {"text": "Theoretically, the normal model should generate more fluent sentences without considering the constraint that a given word must appear in the outputs, however the ppl. is calculated as Eq 5 shows: where P (T ) denotes the probabilities of all the sentences, s.num denotes the total number of the sentences.", "labels": [], "entities": []}, {"text": "OOV s denotes the number of out-ofvocabulary words.", "labels": [], "entities": [{"text": "OOV s", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9607563018798828}]}, {"text": "The outputs of the metaphor based models tend to be shorter which results in a higher P (T ) and thus a low ppl.", "labels": [], "entities": [{"text": "P (T )", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9356798380613327}]}, {"text": "Although the adjustable joint model generates sentences with lexical constraints, it chooses the candidates with the highest joint score at each time step and uses more distinct words, and thus it obtains a high P (T ) as well as a large w.num.", "labels": [], "entities": [{"text": "P (T )", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.9198055863380432}]}, {"text": "It achieves a relatively low ppl.", "labels": [], "entities": []}, {"text": "What's more, all the sentences generated by the adjustable joint model are novel and do not exist in the corpus according to the duplicate checking . The fit word model decodes according http://www.speech.sri.com/projects/srilm/ to the fit words and then the fit words in the sentences are replaced by their target words directly, which results in a higher ppl.", "labels": [], "entities": []}, {"text": "Since the amount of the training data corresponding to the inputs of the uncommon sense model is the least, the generated sentences are not so fluent.", "labels": [], "entities": []}, {"text": "For a thorough comparison, we select 80 gold metaphors with high confidence (>0.6) from the data set proposed by.", "labels": [], "entities": []}, {"text": "Each verb in the data set was annotated for metaphoricity by 10 annotators and we use the verbs in the selected metaphors as the target words.", "labels": [], "entities": []}, {"text": "As metaphor is such a creative and delicate language that automatic evaluation is not adequate.", "labels": [], "entities": []}, {"text": "We ask native English speakers on Amazon Mechanical Turk to evaluate all the sentences generated by the neural models and corresponding gold metaphors in four aspects.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.9567438761393229}]}, {"text": "Each sentence is scored from 1 to 5 by 5 judges.", "labels": [], "entities": []}, {"text": "Readability(read.) indicates whether the sentence is fluent and consistent with the rules of grammar.", "labels": [], "entities": [{"text": "Readability(read.)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7985701709985733}]}, {"text": "Creativity(crea.) indicates whether the sentence is distinct and creative.", "labels": [], "entities": [{"text": "Creativity(crea.)", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8970726877450943}]}, {"text": "Metaphorical or Literal Usage of the Verb(meta.-v) indicates whether the target word is used literally or metaphorically.", "labels": [], "entities": [{"text": "Metaphorical or Literal Usage of the Verb(meta.-v)", "start_pos": 0, "end_pos": 50, "type": "METRIC", "confidence": 0.5576848859588305}]}, {"text": "1 denotes that the usage of the verb is definitely literal and 5 denotes the verb is obviously metaphorically used.", "labels": [], "entities": []}, {"text": "We display typical properties of metaphorical and literal senses as follows: literal usages tend to be more basic, straightforward meaning, more physical and closely tied to our senses; Metaphorical usages tend to be more abstract, more vague and often surprising, someti-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results for generated me- taphors based on the automatically extracted verbs.", "labels": [], "entities": []}]}