{"title": [{"text": "Fluent Translations from Disfluent Speech in End-to-End Speech Translation", "labels": [], "entities": [{"text": "Fluent Translations from Disfluent Speech in End-to-End Speech Translation", "start_pos": 0, "end_pos": 74, "type": "TASK", "confidence": 0.7747741705841489}]}], "abstractContent": [{"text": "Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of dis-fluencies.", "labels": [], "entities": [{"text": "Spoken language translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7719905773798624}]}, {"text": "With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.735075831413269}, {"text": "speech recognition and machine translation", "start_pos": 149, "end_pos": 191, "type": "TASK", "confidence": 0.6473017513751984}]}, {"text": "We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with dis-fluencies removed using the recently collected 'copy-edited' references for the Fisher Spanish-English dataset.", "labels": [], "entities": [{"text": "Fisher Spanish-English dataset", "start_pos": 182, "end_pos": 212, "type": "DATASET", "confidence": 0.9087822039922079}]}, {"text": "We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task.", "labels": [], "entities": []}, {"text": "This work provides a baseline fora new task, the translation of conversational speech with joint removal of disfluencies.", "labels": [], "entities": [{"text": "translation of conversational speech", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.884941503405571}]}], "introductionContent": [], "datasetContent": [{"text": "We focus on the problem of translating directly from noisy speech to clean references without a separate disfluency removal step.", "labels": [], "entities": [{"text": "translating directly from noisy speech to clean references", "start_pos": 27, "end_pos": 85, "type": "TASK", "confidence": 0.8137684464454651}]}, {"text": "We first demonstrate the efficacy of our models on the original disfluent Fisher Spanish-English task, comparing to the previously reported numbers on the SLT task ().", "labels": [], "entities": []}, {"text": "We then compare these results with models trained using the collected 'clean' target data with disfluencies removed.", "labels": [], "entities": []}, {"text": "Finally, we look at the mismatched case where we train on disfluent data and evaluate on a cleaned test set; this is a more realistic scenario, as clean training data is difficult to collect, and we cannot expect to have it for each language and use case we encounter.", "labels": [], "entities": []}, {"text": "We evaluate using both BLEU () and METEOR to compare different aspects of model behavior on our two tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9990208148956299}, {"text": "METEOR", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9912334084510803}]}, {"text": "2 BLEU assesses how well predicted translations match a set of reference translations using modified n-gram precision, weighted by a brevity penalty in place of recall to penalize short hypothesis translations without full coverage.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9985083937644958}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9924052357673645}]}, {"text": "The brevity penalty is computed as e (1\u2212r/c) , where r is the length of the reference and c the candidate translation.", "labels": [], "entities": []}, {"text": "For our task of implicitly removing disfluencies during translation, our generated translations should contain much of the same content but with certain tokens removed, creating shorter translations.", "labels": [], "entities": []}, {"text": "When scoring fluent output against the original disfluent references, then, differences in BLEU score will come from two sources: shorter n-gram matches, and the brevity penalty.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9823144972324371}]}, {"text": "METEOR, on the other hand, can be considered a more 'semantic' evaluation metric.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7860665321350098}]}, {"text": "It uses a harmonic mean of precision and recall, with greater weight given to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9997624754905701}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9994682669639587}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.996878981590271}]}, {"text": "Further, while BLEU uses exact n-gram matches, METEOR also takes into account stem, synonym, and paraphrase matches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9813135862350464}, {"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9538514018058777}]}, {"text": "For our fluent task, we aim to maintain semantic meaning while removing disfluent tokens.", "labels": [], "entities": []}, {"text": "Accordingly, when scored against the fluent target references, we hope to see similar METEOR scores between the disfluent models and fluent models.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9991174340248108}]}, {"text": "Both metrics are used fora holistic view of the problem: METEOR will indicate if meaning is maintained, but not assess disfluency removal, while BLEU changes will indicate whether disfluencies have been removed.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9851213693618774}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9986955523490906}]}, {"text": "We provide both multi-reference and singlereference BLEU and METEOR scores: the original Fisher target data has four reference translations for the dev and test sets, which boosts scores considerably as hypothesis n-grams can match in any of the references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9833602905273438}, {"text": "METEOR", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9918549060821533}, {"text": "Fisher target data", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.7135632137457529}]}, {"text": "The fluent target data has two references, so the single reference scores better enable comparison between the two tasks.", "labels": [], "entities": []}, {"text": "shows our results on the original disfluent data with comparisons to and compares performance of speech translation models trained with the fluent target translations to models trained with the original disfluent translations, as scored on the fluent references.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7332395315170288}]}, {"text": "Comparing the disfluent and fluent models, we see that METEOR scores are almost the same while BLEU scores are lower with the disfluent model.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9960620999336243}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9993082284927368}]}, {"text": "This is as we would hope: with our fluent model, we want to generate translations that are semantically the same but with disfluencies removed.", "labels": [], "entities": []}, {"text": "Therefore similar METEOR scores with similar recall (52) on the fluent references are encouraging.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9926927089691162}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9988027811050415}]}, {"text": "For BLEU, however, the disfluencies generated by the disfluent model breakup n-grams in the fluent references, thereby lowering scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9892605543136597}]}, {"text": "Comparing single-reference scores with, we see that they are distinctly lower.", "labels": [], "entities": []}, {"text": "This is to be expected with the shorter fluent references; a difference of a single token carries greater weight.", "labels": [], "entities": []}, {"text": "Translating directly to the fluent references is a more challenging task.", "labels": [], "entities": []}, {"text": "As shown in, the original English translations and Spanish speech are very one-to-one while the edited translations introduce deletions and reorderings.", "labels": [], "entities": []}, {"text": "In learning to generate fluent translations, the model needs to learn to handle these more inconsistent behaviors.", "labels": [], "entities": []}, {"text": "shows a visual comparison between outputs generated by the two models.", "labels": [], "entities": []}, {"text": "Using the fluent target data to train constrains the model output vocabulary, so filler words such as 'um', 'ah', 'mhm' are not generated.", "labels": [], "entities": []}, {"text": "We also see significant reductions in repetitions of both words and phrases from the model trained with fluent reference translations.", "labels": [], "entities": []}, {"text": "Further, we also see instances where the fluent model generates a shorter paraphrase of a disfluent phrase, as in the 2nd example.", "labels": [], "entities": []}, {"text": "Disfluency removal for speech translation has traditionally been done as an intermediate step between ASR and MT to better-match additional clean corpora used for MT training; we do not compare to a pipeline approach here.", "labels": [], "entities": [{"text": "Disfluency removal", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6775154173374176}, {"text": "speech translation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7469713389873505}, {"text": "ASR", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.8840485215187073}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9655027985572815}, {"text": "MT training", "start_pos": 163, "end_pos": 174, "type": "TASK", "confidence": 0.9241515398025513}]}, {"text": "However, to contextualize these results, we compare disfluency removal as a post-processing step after end-to-end speech translation with the original disfluent par-  allel data.", "labels": [], "entities": [{"text": "disfluency removal", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8220022022724152}]}, {"text": "Simply filtering filler words and repetitions from the disfluent model (Filter) outputs as a post-processing step, the dev scores improve slightly, but test stays the same or decreases.", "labels": [], "entities": []}, {"text": "In some cases, treating disfluency removal as a filtering task can reduce the fluency of an utterance:", "labels": [], "entities": [{"text": "disfluency removal", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7770256400108337}]}], "tableCaptions": [{"text": " Table 2: Single task end-to-end speech translation us- ing original disfluent references to train and evaluate.  Comparing multi-reference scores using all four refer- ences (4Ref) vs average single reference score (1Ref).", "labels": [], "entities": [{"text": "Single task end-to-end speech translation", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.5560766577720642}]}, {"text": " Table 3: End-to-end model performance evaluated with  new fluent references. Comparing average single ref- erence scores (1Ref) vs multi-reference scores using  both generated references (2Ref).", "labels": [], "entities": [{"text": "single ref- erence scores (1Ref)", "start_pos": 96, "end_pos": 128, "type": "METRIC", "confidence": 0.7870847433805466}]}, {"text": " Table 4: End-to-end disfluent model with different post- processing steps. Performance evaluated with new flu- ent references.", "labels": [], "entities": []}, {"text": " Table 5: Performance evaluated with original disflu- ent references. Comparing average single reference  scores (1Ref) vs multi-reference scores using all refer- ences (4Ref).", "labels": [], "entities": []}]}