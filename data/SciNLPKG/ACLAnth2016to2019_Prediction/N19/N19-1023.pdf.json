{"title": [{"text": "Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings", "labels": [], "entities": [{"text": "Phrase Elliptical Sentence Embeddings", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.6751153916120529}]}], "abstractContent": [{"text": "Ellipsis is a natural language phenomenon where part of a sentence is missing and its information must be recovered from its surrounding context, as in \"Cats chase dogs and so do foxes.\".", "labels": [], "entities": []}, {"text": "Formal semantics has different methods for resolving ellipsis and recovering the missing information, but the problem has not been considered for distributional semantics, where words have vector embed-dings and combinations thereof provide em-beddings for sentences.", "labels": [], "entities": []}, {"text": "In elliptical sentences these combinations go beyond linear as copying of elided information is necessary.", "labels": [], "entities": []}, {"text": "In this paper, we develop different models for embedding VP-elliptical sentences.", "labels": [], "entities": []}, {"text": "We extend existing verb disambiguation and sentence similarity datasets to ones containing elliptical phrases and evaluate our models on these datasets fora variety of non-linear combinations and their linear counterparts.", "labels": [], "entities": []}, {"text": "We compare results of these compositional models to state of the art holistic sentence en-coders.", "labels": [], "entities": []}, {"text": "Our results show that non-linear addition and a non-linear tensor-based composition outperform the naive non-compositional base-lines and the linear models, and that sentence encoders perform well on sentence similarity, but not on verb disambiguation.", "labels": [], "entities": [{"text": "verb disambiguation", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.705350250005722}]}], "introductionContent": [{"text": "Compositional distributional semantics has so far relied on a tight connection between syntactic and semantic resources.", "labels": [], "entities": [{"text": "Compositional distributional semantics", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6689343849817911}]}, {"text": "Based on the assembly principle of compositionality, these models assign a sentence vector by applying a linear map to the individual word embeddings therein.", "labels": [], "entities": []}, {"text": "The meaning of \"cats chase dogs\" is as follows in (1) additive, (2) multiplicative, and (3) tensor-based models: Some linguistic phenomena, however, rely on copying resources while computing meaning; canonical examples thereof are anaphora and ellipsis, exemplified below: (a) Cats clean themselves.", "labels": [], "entities": []}, {"text": "(b) Cats chase dogs, children do too.", "labels": [], "entities": []}, {"text": "More complex examples involve a structural ambiguity such as the following: (c) Cats chase their tail, dogs too.", "labels": [], "entities": []}, {"text": "These lend themselves to a strict (dogs chase the cat's tail) and a sloppy reading (dogs chase their own tail).", "labels": [], "entities": [{"text": "strict", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9853764176368713}]}, {"text": "In these examples, the meaning of at least one part of the sentence is used twice, e.g. the subject in a, the verb phrase \"chase dogs\" in b.", "labels": [], "entities": []}, {"text": "Such cases can often be extended to a situation in which a meaning is used more than twice, e.g. in \"Cats chase their tail, dogs too, and so do foxes\".", "labels": [], "entities": []}, {"text": "In order to develop distributional semantics for such sentences while respecting the principle of compositionality, one has a choice between a linear or a non-linear composition of resources.", "labels": [], "entities": []}, {"text": "In the linear case, no information is copied, resulting in vector embeddings such as the following one (when only considering content words): One has the same choice when dealing with multiplicative and tensor-based models.", "labels": [], "entities": []}, {"text": "The question is which of these composition frameworks, i.e. linear versus non-linear, provides a better choice for embedding elliptical sentences.", "labels": [], "entities": []}, {"text": "To our knowledge, this has remained an open question: although some theoretical work has been done to model verb phrase ellipsis in compositional distributional semantics, none of the existing datasets or evaluation methods for distributional semantics focus on elliptical phenomena.", "labels": [], "entities": []}, {"text": "In this paper, we provide some answers.", "labels": [], "entities": []}, {"text": "Our starting point is the lambda logical forms of sentences, e.g. those produced by the approach of, which uses a higher order unification algorithm to resolve ellipsis.", "labels": [], "entities": []}, {"text": "We apply to these the lambdas-to-vectors mapping of to homomorphically map the lambda terms into concrete vector embeddings resulting from a multitude of composition operators, such as addition, multiplication, and tensor-based.", "labels": [], "entities": []}, {"text": "We work with four vector spaces (count-based, Word2Vec, GloVe, FastText) and three different verb embeddings, and contrast our compositional models with state of the art holistic sentence encoders.", "labels": [], "entities": []}, {"text": "We evaluate the sentence embeddings by using them in a verb disambiguation and in a sentence similarity task, created by extending previous SVO tasks from and to an elliptical setting, and obtaining new human judgements using the Amazon Mechanical Turk crowdsourcing tool.", "labels": [], "entities": []}, {"text": "Our experiments show that in both tasks, the models that use a non-linear form of composition perform better than the models whose composition framework is linear, suggesting that resolving ellipsis contributes to the quality of the sentence embedding.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the model(s) in the previous section, we built two new datasets and experimented with count based and neural vector spaces, and sentence encoders.", "labels": [], "entities": []}, {"text": "3 .  In order to experiment with ellipsis, we extended the verb disambiguation dataset of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity dataset of, henceforth GS2011 and KS2013.", "labels": [], "entities": [{"text": "GS2011", "start_pos": 187, "end_pos": 193, "type": "DATASET", "confidence": 0.9626383781433105}, {"text": "KS2013", "start_pos": 198, "end_pos": 204, "type": "DATASET", "confidence": 0.8406150937080383}]}], "tableCaptions": [{"text": " Table 2: Spearman \u03c1 scores on word similarity tasks.", "labels": [], "entities": [{"text": "Spearman \u03c1 scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7433947523434957}, {"text": "word similarity tasks", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.788839062054952}]}, {"text": " Table 3: Spearman \u03c1 scores for the ellipsis disambigua- tion experiment. CB: count based, W2V: Word2Vec,  FT: FastText.", "labels": [], "entities": [{"text": "Spearman \u03c1 scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7063501179218292}, {"text": "Word2Vec", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.906950056552887}, {"text": "FT", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.6416884660720825}]}, {"text": " Table 4: Spearman \u03c1 scores for the ellipsis disambigua- tion experiment. D2V1: Doc2Vec1, D2V2: Doc2Vec  2, ST: Skip-Thought, IS1: InferSent 1, IS2: InferSent  2, USE: Universal Sentence Encoder.", "labels": [], "entities": [{"text": "USE", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.7574939131736755}]}, {"text": " Table 5: Spearman \u03c1 scores for the ellipsis similarity  experiment.", "labels": [], "entities": [{"text": "Spearman \u03c1 scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8744542002677917}]}, {"text": " Table 6: Spearman \u03c1 scores for the ellipsis similarity  experiment. D2V1: Doc2Vec1, D2V2: Doc2Vec 2, ST:  Skip-Thought, IS1: InferSent 1, IS2: InferSent 2, USE:  Universal Sentence Encoder.", "labels": [], "entities": [{"text": "USE", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.7888965010643005}]}]}