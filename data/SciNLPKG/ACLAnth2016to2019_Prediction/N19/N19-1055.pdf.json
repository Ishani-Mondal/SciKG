{"title": [{"text": "Joint Multiple Intent Detection and Slot Labeling for Goal-Oriented Dialog", "labels": [], "entities": [{"text": "Slot Labeling", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.8150587975978851}]}], "abstractContent": [{"text": "Neural network models have recently gained traction for sentence-level intent classification and token-based slot-label identification.", "labels": [], "entities": [{"text": "sentence-level intent classification", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.7123385369777679}, {"text": "token-based slot-label identification", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.64887801806132}]}, {"text": "In many real-world scenarios, users have multiple intents in the same utterance, and a token-level slot label can belong to more than one intent.", "labels": [], "entities": []}, {"text": "We investigate an attention-based neu-ral network model that performs multi-label classification for identifying multiple intents and produces labels for both intents and slot-labels at the token-level.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7313131093978882}]}, {"text": "We show state-of-the-art performance for both intent detection and slot-label identification by comparing against strong, recently proposed models.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.8532156348228455}, {"text": "slot-label identification", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.8258457779884338}]}, {"text": "Our model provides a small but statistically significant improvement of 0.2% on the predominantly single-intent ATIS public data set, and 55% intent accuracy improvement on an internal multi-intent dataset.", "labels": [], "entities": [{"text": "ATIS public data set", "start_pos": 112, "end_pos": 132, "type": "DATASET", "confidence": 0.8888648003339767}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.7896046042442322}]}], "introductionContent": [{"text": "In dialog systems, the natural language understanding component (NLU) is responsible for identifying the user's request and creating a semantic frame that succinctly summarizes the user's needs.", "labels": [], "entities": []}, {"text": "These semantic frames are typically constructed using intents and slot-labels.", "labels": [], "entities": []}, {"text": "As the names imply, an intent captures the intention of the user and slot-labels capture any additional information or constraints the user provides.", "labels": [], "entities": []}, {"text": "These constraints must be satisfied in order to fulfill the user's request.", "labels": [], "entities": []}, {"text": "The example below shows a user's request, \"how is the weather in Dallas ?\".", "labels": [], "entities": []}, {"text": "We need to identify the intent (\"GetWeatherInfo\") as well as the values for the slot-labels (SL), here, \"City\" (value=\"Dallas\").", "labels": [], "entities": []}, {"text": "It is crucial that intents and slot-labels are identified with high accuracy as an error made by the NLU component propagates through downstream components such as the dialog state tracker, the dialog policy and the natural language generator components, leading to a substantial degradation of the performance of the entire dialog system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9819791316986084}, {"text": "dialog state tracker", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.6589568058649699}]}, {"text": "Intent detection has been modeled as a sentence classification task where an intent (y I ) is assigned to the user's utterance.", "labels": [], "entities": [{"text": "Intent detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9422697126865387}, {"text": "sentence classification task", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7958892186482748}]}, {"text": "Slot labeling is typically modeled as a sequential labeling problem, where a user's sentence, x 1 , x 2 , ...x N , is labeled with y S 1 , y S 2 , ..y S N , and y Si is the slot label assigned to the token at position i (x i ).", "labels": [], "entities": [{"text": "Slot labeling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.947038471698761}]}, {"text": "In the example above, the sequence of slot labels would be, \"O O O O O City O\", where, \"O\" stands for \"Other\".", "labels": [], "entities": []}, {"text": "Sequential models such as Maximum Entropy Markov models and Conditional Random Fields, CRFs ( are popular approaches for slot-labeling while intent prediction is often performed using standard classification approaches such as Support Vector Machines ( or logistic regression).", "labels": [], "entities": [{"text": "intent prediction", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.8315218091011047}]}, {"text": "More recently, neural network-based models have been shown to significantly outperform previous approaches.", "labels": [], "entities": []}, {"text": "These models are also appealing since a single model is trained end-toend to perform both intent detection and slot label identification.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7947196662425995}, {"text": "slot label identification", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.7512799501419067}]}, {"text": "Jointly modeling intent and slot label identification () has been shown to significantly outperform other neural network-based approaches.", "labels": [], "entities": [{"text": "slot label identification", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7734699249267578}]}, {"text": "This is intuitive since slot labels could depend on the intent.", "labels": [], "entities": []}, {"text": "Most neural network-based approaches, with the exception of (Xu and Sarikaya, 2013a), predict a single intent fora user's utterance.", "labels": [], "entities": []}, {"text": "In real-world scenarios, users indicate multiple intents in the same utterance.", "labels": [], "entities": []}, {"text": "For example, a user's utterance such as, \"show me flights from Dallas to New York and the cost\", clearly has two intents, one for obtaining the price of the flights (\"GetFlightCost\") and another for the flight information.", "labels": [], "entities": []}, {"text": "It is critical to understand and model such scenarios to allow more natural interaction with users.", "labels": [], "entities": []}, {"text": "In this paper, we treat the intent detection task as a multi-label classification problem and suggest various neural network models to obtain multiple intents.", "labels": [], "entities": [{"text": "intent detection task", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8517208695411682}, {"text": "multi-label classification", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.735094279050827}]}], "datasetContent": [{"text": "In all our experiments, we set the hidden vectors to a dimension of 64 and use the adam optimizer with an early stopping strategy.", "labels": [], "entities": []}, {"text": "We use a drop-out rate of 0.5 for regularization and the maximum norm for gradient clipping is set to 5.", "labels": [], "entities": []}, {"text": "The results are obtained by averaging the performance of the models over 10 runs.", "labels": [], "entities": []}, {"text": "To do a fair comparison against existing models, we do not pre-train our word embeddings), instead we use an embedding layer in the model which is trained along with the rest of the model's parameters.", "labels": [], "entities": []}, {"text": "As done in the NLU community, we report F1 scores for slot labeling.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9786407649517059}, {"text": "slot labeling", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9449656009674072}]}, {"text": "We use F1 scores for intent detection at the token-level and accuracy for sentence-level intent detection.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9804176390171051}, {"text": "intent detection", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.8156644105911255}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9993976354598999}, {"text": "sentence-level intent detection", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.6166803141434988}]}, {"text": "We use two widely used public datasets, ATIS (Airline Travel Information System) ( and SNIPS 1 . The ATIS dataset contains audio recordings of people requesting flight reservations, with 21 intent types and 120 slot labels.", "labels": [], "entities": [{"text": "ATIS (Airline Travel Information System)", "start_pos": 40, "end_pos": 80, "type": "DATASET", "confidence": 0.7331334607941764}, {"text": "ATIS dataset", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.8475929498672485}]}, {"text": "There are 4,478 utterances in the training set, 893 in the test set and 500 utterances in the development set.", "labels": [], "entities": []}, {"text": "The SNIPS data was collected from the SNIPS personal voice assistant, with 7 intent types and 72 slot labels.", "labels": [], "entities": [{"text": "SNIPS data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7531551420688629}]}, {"text": "The training set contains 13,084 utterances, the test set contains 700 utterances and the development set also contains 700 utterances.", "labels": [], "entities": []}, {"text": "The ATIS dataset contains utterances with multi intents, while the SNIPS is only single intent.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8761103749275208}, {"text": "SNIPS", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.6996684074401855}]}, {"text": "In order to demonstrate that our approach does not degrade performance on single intent datasets, we also perform evaluations on the SNIPS dataset.", "labels": [], "entities": [{"text": "SNIPS dataset", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.8472064733505249}]}, {"text": "We also test the performance of the models on an internal dataset.", "labels": [], "entities": []}, {"text": "In this dataset, about 1 https://github.com/snipsco/nlubenchmark/tree/master/2017-06-custom-intent-engines 52% of examples are multi-intent compared to ATIS which has \u22482% of test examples with multiintents.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 152, "end_pos": 156, "type": "DATASET", "confidence": 0.8539705872535706}]}, {"text": "The average number of intents per utterance in the internal dataset is 1.6.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the model against Model 1 and Model 2. We report F1 scores for slot labeling. For intent  detection, we use F1 scores for intent detection at the token-level (T-level) and accuracies (acc) for sentence-level  (S-level) intent detection. N/A: as Models 1 and 2 perform single intent detection only at S-level.", "labels": [], "entities": [{"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9984112977981567}, {"text": "slot labeling", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7763974666595459}, {"text": "intent  detection", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9113836884498596}, {"text": "F1", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9979655742645264}, {"text": "S-level) intent detection", "start_pos": 235, "end_pos": 260, "type": "TASK", "confidence": 0.6413087993860245}]}]}