{"title": [], "abstractContent": [{"text": "We consider the problem of learning distributed representations for entities and relations of multi-relational data so as to predict missing links therein.", "labels": [], "entities": []}, {"text": "Convolutional neural networks have recently shown their superiority for this problem, bringing increased model expressiveness while remaining parameter efficient.", "labels": [], "entities": []}, {"text": "Despite the success, previous convolu-tion designs fail to model full interactions between input entities and relations, which potentially limits the performance of link prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.73575659096241}]}, {"text": "In this work we introduce ConvR, an adaptive convolutional network designed to maximize entity-relation interactions in a convolutional fashion.", "labels": [], "entities": []}, {"text": "ConvR adaptively constructs convolu-tion filters from relation representations, and applies these filters across entity representations to generate convolutional features.", "labels": [], "entities": []}, {"text": "As such, ConvR enables rich interactions between entity and relation representations at diverse regions , and all the convolutional features generated will be able to capture such interactions.", "labels": [], "entities": []}, {"text": "We evaluate ConvR on multiple benchmark datasets.", "labels": [], "entities": []}, {"text": "Experimental results show that: (1) ConvR performs substantially better than competitive baselines in almost all the metrics and on all the datasets; (2) Compared with state-of-the-art convolutional models, ConvR is not only more effective but also more efficient.", "labels": [], "entities": []}, {"text": "It offers a 7% increase in MRR and a 6% increase in Hits@10, while saving 12% in parameter storage.", "labels": [], "entities": [{"text": "MRR", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8472765684127808}, {"text": "Hits", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9807379841804504}]}], "introductionContent": [{"text": "Multi-relational data refers to directed graphs whose nodes correspond to entities and edges different types of relations between entities.", "labels": [], "entities": []}, {"text": "An edge of the form (subject, relation, object) indicates that there exists a specific relation between the subject and object entities.", "labels": [], "entities": []}, {"text": "Learning with multi-relational data plays a pivotal role in many application domains, ranging from social networks or recommender systems to large-scale knowledge bases (KBs) (.", "labels": [], "entities": []}, {"text": "This work focuses on modeling multi-relational data from KBs, with the aim of predicting missing facts on KBs, a challenging task known as link prediction in statistical relational learning (SRL).", "labels": [], "entities": [{"text": "predicting missing facts", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8769434889157613}, {"text": "link prediction in statistical relational learning (SRL)", "start_pos": 139, "end_pos": 195, "type": "TASK", "confidence": 0.6937091880374484}]}, {"text": "Various SRL techniques () have been proposed for this task, among which vector space embedding models (  are gaining increasing attention due to their superior performance and potential scalability.", "labels": [], "entities": [{"text": "SRL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9732359647750854}]}, {"text": "The key idea there is to learn and operate on latent features (embeddings) of entities and relations, so as to uncover non-trivial connectivity patterns in multirelational data.", "labels": [], "entities": []}, {"text": "Previous works of this kind tend to adopt shallow, simple models to extract latent features, e.g., the translation based models () or the bilinear models and their variants (.", "labels": [], "entities": []}, {"text": "Using these simple models allows one to easily handle largescale KBs, but usually at the cost of learning less expressive features.", "labels": [], "entities": []}, {"text": "In fact, such simple models typically generate a single feature with each entry of the embeddings.", "labels": [], "entities": []}, {"text": "The only way to increase the number of features (and thus their expressiveness) is to increase the embedding size.", "labels": [], "entities": []}, {"text": "This potentially limits the performance of link prediction with a given number of parameters.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.8085798919200897}]}, {"text": "To increase model expressiveness, there emerge some deeper, more complicated designs, in particular those on the basis of neural network architectures ().", "labels": [], "entities": []}, {"text": "Such approaches, however, often have more parameters and are prone to overfit, at least on the (relatively small) benchmark datasets used by the scientific community (.", "labels": [], "entities": []}, {"text": "Recently, devised ConvE, a multi-layer convolutional network which enables expressive feature learning while remaining highly parameter efficient.", "labels": [], "entities": [{"text": "expressive feature learning", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6296156148115793}]}, {"text": "Given a subject-relation-object triple (s, r, o), ConvE first reshapes the vector representation of the subject sand that of the relation r into 2D matrices, and then concatenates the two matrices and feeds them into a 2D convolutional layer to extract higher-level, non-linear features, as illustrated in(a).", "labels": [], "entities": []}, {"text": "The resultant convolutional features are finally projected and matched with the vector representation of the object o via an inner product.", "labels": [], "entities": []}, {"text": "Note that by sliding across the embeddings using small-sized filters, the convolution operator can easily generate much more features without increasing the embedding size.", "labels": [], "entities": []}, {"text": "As such, ConvE offers increased expressiveness and achieves competitive performance in link prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.8083389699459076}]}, {"text": "Nevertheless, despite its success, ConvE is still insufficient to fully capture the interactions between input entities and relations, which has long been recognized as crucial for modeling multirelational data.", "labels": [], "entities": []}, {"text": "In ConvE, the (reshaped) representations of input entities and relations are simply stacked together and fed into a convolutional layer.", "labels": [], "entities": []}, {"text": "Although 2D convolution is better than 1D convolution in modeling entity-relation interactions, typical 2D convolution with global filters on such a stacked matrix, however, can only model interactions around the concatenation line.", "labels": [], "entities": []}, {"text": "Consider the example in(a), where two matrices of size 3 \u00d7 3 are formed after reshaping, stacked, and fed as input to a convolutional layer.", "labels": [], "entities": []}, {"text": "Convolving across the input with a global filter of size 2 \u00d7 2 will then be able to model interactions only in the regions where the two matrices adjoin (e.g., the region outlined in red).", "labels": [], "entities": []}, {"text": "That means, only a small proportion of the output convolutional features (20% in this example, striped with orange and blue) will effectively capture entity-relation interactions, and the vast majority others will be entity-or relation-independent.", "labels": [], "entities": []}, {"text": "This poses potential negative impacts on the link prediction task.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8832947015762329}]}, {"text": "This paper, aiming at maximizing the interactions between input entities and relations, introduces ConvR, an adaptive convolutional network specifically designed for multi-relational data.", "labels": [], "entities": []}, {"text": "As illustrated in, the key idea of ConvR is to facilitate convolution across entity representations with its filters adaptively constructed from relation representations.", "labels": [], "entities": []}, {"text": "Such adaptive convolution will model the interactions between the two types of input not only more naturally but also more effectively.", "labels": [], "entities": []}, {"text": "Specifically, given a triple, the vector representation of the subject is reshaped and fed as input to a convolutional layer, while that of the relation is split and reshaped into a set of filters.", "labels": [], "entities": []}, {"text": "ConvR then convolves across the input with these filters, enabling each filter (a part of the relation representation) to interact with diverse regions of the input (the entity representation).", "labels": [], "entities": []}, {"text": "Through this adaptive convolution process, all the features generated will be able to capture entity-relation interactions (striped with orange and blue in).", "labels": [], "entities": []}, {"text": "These convolutional features are finally projected and matched with the representation of the object.", "labels": [], "entities": []}, {"text": "Besides being more effective, adaptive convolution enables potentially more efficient modeling (in terms of the number of parameters).", "labels": [], "entities": []}, {"text": "Compared with ConvE), ConvR) needs no global filters and generates smaller feature maps, making the follow-up projection layer roughly half as large as that of ConvE.", "labels": [], "entities": []}, {"text": "The idea of adaptive convolution, in fact, is rather generic for the multi-relational scenario.", "labels": [], "entities": []}, {"text": "By splitting and reshaping relation vectors, ConvR can be easily generalized to other paradigms such as 1D or 3D convolution, not restricted to the 2D setting.", "labels": [], "entities": []}, {"text": "To facilitate a direct and fair comparison to ConvE where only 2D convolution is considered and tested, this paper takes the 2D setting as an example, and shows the superiority of ConvR over ConvE in this setting.", "labels": [], "entities": []}, {"text": "We will investigate higher dimensional convolution in our future work.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "We propose a novel adaptive convolution model for learning with multi-relational data.", "labels": [], "entities": []}, {"text": "Our approach, ConvR, takes full advantage of entity-relation interactions in a convolutional fashion, while still remaining highly parameter efficient.", "labels": [], "entities": []}, {"text": "(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8622281551361084}, {"text": "WN18", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.9643301963806152}, {"text": "FB15K", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.8257502913475037}, {"text": "WN18RR", "start_pos": 202, "end_pos": 208, "type": "DATASET", "confidence": 0.9259323477745056}, {"text": "FB15K-237", "start_pos": 216, "end_pos": 225, "type": "DATASET", "confidence": 0.8920133113861084}, {"text": "MRR", "start_pos": 242, "end_pos": 245, "type": "METRIC", "confidence": 0.9418152570724487}, {"text": "Hits", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9791142344474792}]}], "datasetContent": [{"text": "In this section, we evaluate ConvR against competitive baselines in the link prediction task on multiple benchmark KBs.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.8332858284314474}]}, {"text": "We also investigate parameter efficiency of ConvR against ConvE to further show its superiority.", "labels": [], "entities": []}, {"text": "Datasets We use four datasets for our experiments.", "labels": [], "entities": []}, {"text": "The first two are the popular WN18 and FB15k, both released by Evaluation protocol We adopt the ranking process proposed in () for evaluation.", "labels": [], "entities": [{"text": "WN18", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.941541850566864}, {"text": "FB15k", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.6673187613487244}]}, {"text": "For each triple (s, r, o) in the test set, we replace the subject s with every entity e \u2208 E, and calculate a score for the corrupted triple (e, r, o).", "labels": [], "entities": []}, {"text": "Then we sort these scores in descending order to get the rank of the correct subject s.", "labels": [], "entities": []}, {"text": "Since corrupted triples may also be valid, we remove those that already exist in either the training, validation, or test set during ranking, i.e., the filtered setting as called by.", "labels": [], "entities": []}, {"text": "This whole procedure is repeated while replacing the object o.", "labels": [], "entities": []}, {"text": "We aggregate overall test triples, and report the mean reciprocal rank (MRR) and the proportion of correct entities ranked in the top n (Hits@n), with n = 1, 3, 10.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 50, "end_pos": 76, "type": "METRIC", "confidence": 0.8521352708339691}]}, {"text": "Implementation details We implement ConvR in PyTorch.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9498509764671326}, {"text": "PyTorch", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8850545287132263}]}, {"text": "In our experiments, we fix mini-batch  size to 128, initial learning rate to 0.001, and label smoothing coefficient to 0.1.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 52, "end_pos": 73, "type": "METRIC", "confidence": 0.8993038932482401}]}, {"text": "Other hyperparameters are selected with grid search on the validation set.", "labels": [], "entities": []}, {"text": "Specifically, we tune the entity embedding size d e \u2208 {100, 200}, filter number c \u2208 {50, 100, 150, 200}, and filter size h \u00d7 w \u2208 {3 \u00d7 3, 4 \u00d7 4, 5 \u00d7 5}.", "labels": [], "entities": []}, {"text": "All dropout ratios, i.e., \u03c1 1 on reshaped subject representations, \u03c1 2 on convolutional feature maps, and \u03c1 3 on projected vectors after the fully-connected layer, are tuned in {0.1, 0.2, 0.3, 0.4, 0.5}.", "labels": [], "entities": []}, {"text": "On each dataset, we choose the optimal configuration with the highest MRR on the validation set within 1000 epochs, and report its performance on the test set.", "labels": [], "entities": [{"text": "MRR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9955117106437683}]}, {"text": "lists the optimal configurations of ConvR on the four datasets.", "labels": [], "entities": []}, {"text": "Baseline methods We compare ConvR against a variety of competitive baselines, which can be roughly categorized into two groups: \u2022 Methods that further introduce multi-layer structures and non-linearity, in particular those based on neural networks, including R-GCN (), Neural LP ( ),, and ConvKB (Nguyen et al., 2018).", "labels": [], "entities": []}, {"text": "the results on WN18RR and FB15k-237.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9650089740753174}, {"text": "FB15k-237", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.6648766994476318}]}, {"text": "On all the four datasets, the results for the baselines are taken directly from previous literature to avoid re-implementation bias.", "labels": [], "entities": []}, {"text": "Since not all baselines have their results reported on all the four datasets, we cannot make the two sets of baselines compared in exactly the same.", "labels": [], "entities": []}, {"text": "From the results, we can see that: (1) On WN18 and FB15k, ConvR performs better than or at least as well as the baselines in almost all the metrics.", "labels": [], "entities": [{"text": "WN18", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9636513590812683}, {"text": "FB15k", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.6504546403884888}]}, {"text": "(2) Compared to ConvE, it offers a 5% increase in MRR, a 7% increase in Hits@1, and a 2% increase in Hits@10 on FB15k.", "labels": [], "entities": [{"text": "ConvE", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.8486788868904114}, {"text": "MRR", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9942612648010254}, {"text": "Hits@1", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.7915412982304891}, {"text": "FB15k", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9695118069648743}]}, {"text": "On the more difficult WN18RR and FB15k-237, ConvR consistently outperforms most of the baselines, except for MRR score of ConvKB on FB15k-237.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.9430850148200989}, {"text": "FB15k-237", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9213882684707642}, {"text": "ConvR", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9980255365371704}, {"text": "MRR score", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9787755310535431}, {"text": "ConvKB", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9234157800674438}, {"text": "FB15k-237", "start_pos": 132, "end_pos": 141, "type": "DATASET", "confidence": 0.9562207460403442}]}, {"text": "However, on WN18RR ConvR outperforms ConvKB on all known metrices, especially MRR.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8694223761558533}, {"text": "MRR", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.7417919039726257}]}, {"text": "This discrepancy maybe attributed to ConvKB's initialization with TransE on FB15k-237.", "labels": [], "entities": [{"text": "TransE", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.8639733791351318}, {"text": "FB15k-237", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9612793326377869}]}, {"text": "(4) Compared to ConvE, it offers a 3% increase in MRR, a 14% increase in Hits@1, a 12% increase in Hits@10 on WN18RR, and an 11% increase in MRR, a 9% increase in Hits@1, an 8% increase in Hits@10 on FB15k-237.", "labels": [], "entities": [{"text": "ConvE", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9130086898803711}, {"text": "MRR", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9399697780609131}, {"text": "WN18RR", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9786812663078308}, {"text": "MRR", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.8332899808883667}, {"text": "FB15k-237", "start_pos": 200, "end_pos": 209, "type": "DATASET", "confidence": 0.9918670058250427}]}], "tableCaptions": [{"text": " Table 1: Statistics of the four datasets. Columns stand  for the number of relations, number of entities, and  number of triples in training/validation/test sets.", "labels": [], "entities": []}, {"text": " Table 2: Optimal configurations of ConvR on the four  datasets. Columns are the entity embedding size, num- ber and size of filters, and dropout ratios.", "labels": [], "entities": []}, {"text": " Table 3: Link prediction results on the test sets of WN18 and FB15k. Results marked by  \u2020 are taken from  (Trouillon et al., 2016). Other results are taken from the original papers. Missing scores not reported are denoted  by \"-\". Best scores highlighted in bold.", "labels": [], "entities": [{"text": "WN18", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9729348421096802}, {"text": "FB15k", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9014931321144104}]}, {"text": " Table 4: Link prediction results on the test sets of WN18RR and FB15k-237. Results marked by  \u2021 are taken  from (Dettmers et al., 2018). Other results are taken from the original papers. KBGAN refers to the \"TransD +  DistMult\" setting which shows best performance. ConvKB is initialized with TransE on FB15k-237, and randomly  on WN18RR. Missing scores not reported are denoted by \"-\". Best scores highlighted in bold.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9666352868080139}, {"text": "FB15k-237", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9725558161735535}, {"text": "FB15k-237", "start_pos": 304, "end_pos": 313, "type": "DATASET", "confidence": 0.9766632914543152}, {"text": "WN18RR", "start_pos": 332, "end_pos": 338, "type": "DATASET", "confidence": 0.9748059511184692}]}, {"text": " Table 5: Parameter efficiency on FB15k-237. Each cell reports number of parameters, MRR, and Hits@10 in turn.  Results of ConvE are taken from (Dettmers et al., 2018).", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9700955152511597}, {"text": "MRR", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9988635778427124}]}]}