{"title": [{"text": "Structural Neural Encoders for AMR-to-text Generation", "labels": [], "entities": [{"text": "AMR-to-text", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9555466175079346}]}], "abstractContent": [{"text": "AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9373688995838165}, {"text": "Abstract Meaning Representation (AMR) graphs", "start_pos": 126, "end_pos": 170, "type": "TASK", "confidence": 0.7481533714703151}]}, {"text": "Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings.", "labels": [], "entities": [{"text": "AMR graphs", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.7835856378078461}]}, {"text": "Approaching the problem while working directly with graphs requires the use of graph-to-sequence models that encode the AMR graph into a vector representation.", "labels": [], "entities": []}, {"text": "Such encoding has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs.", "labels": [], "entities": [{"text": "AMR graphs", "start_pos": 154, "end_pos": 164, "type": "DATASET", "confidence": 0.7252163589000702}]}, {"text": "We investigate the extent to which reentrancies (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where reentrancies are not preserved.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.9835777878761292}]}, {"text": "We show that improvements in the treatment of reentrancies and long-range dependencies contribute to higher overall scores for graph encoders.", "labels": [], "entities": []}, {"text": "Our best model achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the state of the art by 1.24 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9992813467979431}, {"text": "LDC2015E86", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.8921182155609131}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9991058707237244}, {"text": "LDC2017T10", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.8962553143501282}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR;) is a semantic graph representation that abstracts away from the syntactic realization of a sentence, where nodes in the graph represent concepts and edges represent semantic relations between them.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8320685029029846}]}, {"text": "AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies.", "labels": [], "entities": []}, {"text": "For instance, the AMR of(a) contains a reentrancy between finger and he, caused by the possessive pronoun his.", "labels": [], "entities": []}, {"text": "AMR-to-text generation is the task of automatically generating natural language from AMR graphs.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8836305737495422}]}, {"text": "Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.8143208026885986}]}, {"text": "In order to use sequence-to-sequence models, reduce the AMR graphs to sequences, while and directly encode them as graphs.", "labels": [], "entities": []}, {"text": "Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs.", "labels": [], "entities": [{"text": "Graph encoding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6884809583425522}]}, {"text": "While central to AMR, reentrancies are often hard to treat both in parsing and in generation.", "labels": [], "entities": [{"text": "AMR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.95354825258255}, {"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9635875821113586}]}, {"text": "Previous work either removed them from the graphs, hence obtaining sequential () or tree-structured () data, while other work maintained them but did not analyze their impact on performance (e.g.,.", "labels": [], "entities": []}, {"text": "showed that state-of-the-art parsers do not perform well in predicting reentrant structures, while van compared different pre-and post-processing techniques to improve the performance of sequenceto-sequence parsers with respect to reentrancies.", "labels": [], "entities": []}, {"text": "It is not yet clear whether explicit encoding of reentrancies is beneficial for generation.", "labels": [], "entities": []}, {"text": "In this paper, we compare three types of encoders for AMR: 1) sequential encoders, which reduce AMR graphs to sequences; 2) tree encoders, which ignore reentrancies; and 3) graph encoders.", "labels": [], "entities": []}, {"text": "We pay particular attention to two phenomena: reentrancies, which mark co-reference and control structures, and long-range dependencies in the AMR graphs, which are expected to benefit from structural encoding.", "labels": [], "entities": [{"text": "reentrancies", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9449790120124817}]}, {"text": "The contributions of the paper are two-fold: \u2022 We present structural encoders for the encoder/decoder framework and show the benefits of graph encoders not only compared to sequential encoders but also compared to tree encoders, which have not been studied so far for AMR-to-text generation.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 268, "end_pos": 290, "type": "TASK", "confidence": 0.958315372467041}]}, {"text": "\u2022 We show that better treatment of reentrancies and long-range dependencies contributes to improvements in the graph encoders.", "labels": [], "entities": []}, {"text": "Our best model, based on a graph encoder, achieves state-of-the-art results for both the LDC2015E86 dataset (24.40 on BLEU and 23.79 on Meteor) and the LDC2017T10 dataset (24.54 on BLEU and 24.07 on Meteor).", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.9769773781299591}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.971935510635376}, {"text": "Meteor", "start_pos": 136, "end_pos": 142, "type": "DATASET", "confidence": 0.9648696780204773}, {"text": "LDC2017T10 dataset", "start_pos": 152, "end_pos": 170, "type": "DATASET", "confidence": 0.9732256829738617}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.5752676725387573}, {"text": "Meteor", "start_pos": 199, "end_pos": 205, "type": "DATASET", "confidence": 0.9783387184143066}]}], "datasetContent": [{"text": "We use both BLEU () and Meteor () as evaluation metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9984672665596008}, {"text": "Meteor", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.8228941559791565}]}, {"text": "We report results on the AMR dataset LDC2015E86 and LDC2017T10.", "labels": [], "entities": [{"text": "AMR dataset LDC2015E86", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.896959662437439}, {"text": "LDC2017T10", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.8068996071815491}]}, {"text": "All systems are implemented in PyTorch () using the framework OpenNMT-py (.", "labels": [], "entities": []}, {"text": "Hyperparameters of each model were tuned on the development set of LDC2015E86.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9519027471542358}]}, {"text": "For the GCN components, we use two layers, ReLU activations, and tanh highway layers.", "labels": [], "entities": []}, {"text": "We use single layer LSTMs.", "labels": [], "entities": []}, {"text": "We train with SGD with the initial learning rate set to 1 and decay to 0.8.", "labels": [], "entities": []}, {"text": "Batch size is set to 100.", "labels": [], "entities": [{"text": "size", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.5885732769966125}]}, {"text": "We first evaluate the overall performance of the models, after which we focus on two phenomena that we expect to benefit most from structural encoders: reentrancies and long-range dependencies.", "labels": [], "entities": []}, {"text": "shows the comparison on the development split of the LDC2015E86 dataset between sequential, tree and graph encoders.", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9622890949249268}]}, {"text": "The sequential encoder (SEQ) is a re-implementation of.", "labels": [], "entities": []}, {"text": "We test both approaches of stacking structural and sequential components: structure on top of sequence (SEQTREELSTM and SEQGCN), and sequence on top of structure (TREELSTMSEQ and GCNSEQ).", "labels": [], "entities": [{"text": "TREELSTMSEQ", "start_pos": 163, "end_pos": 174, "type": "METRIC", "confidence": 0.7565038800239563}, {"text": "GCNSEQ", "start_pos": 179, "end_pos": 185, "type": "DATASET", "confidence": 0.701423168182373}]}, {"text": "To inspect the effect of the sequential component, we run ablation tests by removing the RNNs altogether (TREELSTM and GCN).", "labels": [], "entities": [{"text": "TREELSTM", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.996066153049469}, {"text": "GCN", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.8082939386367798}]}, {"text": "GCN-based models are used both as tree encoders (reentrancies are removed) and graph encoders (reentrancies are maintained).", "labels": [], "entities": []}, {"text": "For both TreeLSTM-based and GCN-based models, our proposed approach of applying the structural encoder before the RNN achieves better scores.", "labels": [], "entities": []}, {"text": "This is especially true for GCN-based models, for which we also note a drastic drop in performance when the RNN is removed, highlighting the importance of a sequential component.", "labels": [], "entities": []}, {"text": "On the other hand, RNN layers seem to have less impact on TreeLSTM-based models.", "labels": [], "entities": []}, {"text": "This outcome is not unexpected, as TreeLSTMs already use LSTM gates in their computation.", "labels": [], "entities": []}, {"text": "The results show a clear advantage of tree and graph encoders over the sequential encoder.", "labels": [], "entities": []}, {"text": "The best performing model is GCNSEQ, both as a tree and as a graph encoder, with the latter obtaining the highest results.", "labels": [], "entities": [{"text": "GCNSEQ", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.8289381861686707}]}, {"text": "shows the comparison between our best sequential (SEQ), tree (GCNSEQ without reentrancies, henceforth called TREE) and graph en-   coders (GCNSEQ with reentrancies, henceforth called GRAPH) on the test set of LDC2015E86 and LDC2017T10.", "labels": [], "entities": [{"text": "TREE", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9794678092002869}, {"text": "LDC2017T10", "start_pos": 224, "end_pos": 234, "type": "DATASET", "confidence": 0.9071332812309265}]}, {"text": "We also include state-of-the-art results reported on these datasets for sequential encoding () and graph encoding (.", "labels": [], "entities": [{"text": "graph encoding", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6992541998624802}]}, {"text": "In order to mitigate the effects of random seeds, we train five models with different random seeds and report the results of the median model, according to their BLEU score on the development set (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9990436434745789}]}, {"text": "We achieve state-of-the-art results with both tree and graph encoders, demonstrating the efficacy of our GCNSeq approach.", "labels": [], "entities": []}, {"text": "The graph encoder outperforms the other systems and previous work on both datasets.", "labels": [], "entities": []}, {"text": "These results demonstrate the benefit of structural encoders over purely sequential ones as well as the advantage of explicitly including reentrancies.", "labels": [], "entities": []}, {"text": "The differences between our graph encoder and that of and were discussed in Section 3.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU and Meteor (%) scores on the develop- ment split of LDC2015E86.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.997194766998291}, {"text": "Meteor", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.6753953099250793}, {"text": "develop- ment split of LDC2015E86", "start_pos": 44, "end_pos": 77, "type": "METRIC", "confidence": 0.8231247166792551}]}, {"text": " Table 2: Scores on the test split of LDC2015E86 and  LDC2017T10. TREE is the tree-based GCNSEQ and  GRAPH is the graph-based GCNSEQ.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9037359356880188}, {"text": "TREE", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9946164488792419}, {"text": "GRAPH", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.5933251976966858}]}, {"text": " Table 3: Counts of reentrancies for the development  and test split of LDC2017T10", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9889673590660095}, {"text": "LDC2017T10", "start_pos": 72, "end_pos": 82, "type": "DATASET", "confidence": 0.766395092010498}]}, {"text": " Table 4: Differences, with respect to the sequen- tial baseline, in the Meteor score of the test split of  LDC2017T10 as a function of the number of reentran- cies.", "labels": [], "entities": [{"text": "Differences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9672902226448059}]}, {"text": " Table 5: Examples of generation from AMR graphs containing reentrancies. REF is the reference sentence.", "labels": [], "entities": [{"text": "REF", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9851163029670715}]}, {"text": " Table 6: Accuracy (%) of models, on the test split of  LDC201T10, for different categories of contrastive er- rors: antecedent (Antec.), pronoun type (Type), num- ber (Num.), and gender (Gender).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975773692131042}]}, {"text": " Table 7: Counts of longest dependencies for the devel- opment and test split of LDC2017T10", "labels": [], "entities": [{"text": "LDC2017T10", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.8050699234008789}]}, {"text": " Table 8: Differences, with respect to the sequen- tial baseline, in the Meteor score of the test split of  LDC2017T10 as a function of the maximum depen- dency length.", "labels": [], "entities": [{"text": "Differences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9683113694190979}, {"text": "LDC2017T10", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.6113946437835693}]}]}