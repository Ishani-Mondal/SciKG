{"title": [{"text": "Differentiable Sampling with Flexible Reference Word Order for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7168297171592712}]}], "abstractContent": [{"text": "Despite some empirical success at correcting exposure bias in machine translation, scheduled sampling algorithms suffer from a major drawback: they incorrectly assume that words in the reference translations and in sampled sequences are aligned at each time step.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7561065852642059}]}, {"text": "Our new differentiable sampling algorithm addresses this issue by optimizing the probability that the reference can be aligned with the sampled output, based on a soft alignment predicted by the model itself.", "labels": [], "entities": []}, {"text": "As a result, the output distribution at each time step is evaluated with respect to the whole predicted sequence.", "labels": [], "entities": []}, {"text": "Experiments on IWSLT translation tasks show that our approach improves BLEU compared to maximum likelihood and scheduled sampling baselines.", "labels": [], "entities": [{"text": "IWSLT translation tasks", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8106316924095154}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9994352459907532}]}, {"text": "In addition, our approach is simpler to train with no need for sampling schedule and yields models that achieve larger improvements with smaller beam sizes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) models are typically trained to maximize the likelihood of reference translations.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7678009072939554}]}, {"text": "While simple and effective, this objective suffers from the exposure bias problem (: the model is only exposed to reference target sequences during training, but has to rely on its own predictions at inference.", "labels": [], "entities": []}, {"text": "As a result, errors can accumulate along the generated sequence at inference time.", "labels": [], "entities": []}, {"text": "This is a well-known issue in sequential decision making, i.a.) and it has been addressed in past work by incorporating the previous decoding choices into the training scheme, using imitation learning; Ross et al., The code is available at https://github.com/ Izecson/saml-nmt 2011; and reinforcement learning ( techniques.", "labels": [], "entities": [{"text": "sequential decision making", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6891709168752035}]}, {"text": "In this paper, we focus on a simple and computationally inexpensive family of approaches, known as Data as Demonstrator () and scheduled sampling (.", "labels": [], "entities": []}, {"text": "The algorithms use a stochastic mixture of the reference words and model predictions with an annealing schedule controlling the mixture probability.", "labels": [], "entities": []}, {"text": "Despite their empirical success in various sequence prediction tasks, they are based on an assumption that does not hold for machine translation: they assume that words in the reference translations and in sampled sequences are aligned at each time step, which results in weak and sometimes misleading training signals.", "labels": [], "entities": [{"text": "sequence prediction tasks", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.842287023862203}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7915197312831879}]}, {"text": "In this paper, we introduce a differentiable sampling algorithm that exposes machine translation models to their own predictions during training, and allows for differences in word order when comparing model outputs with reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6977291405200958}]}, {"text": "We compute the probability that the reference can be aligned with the sampled output using a soft alignment predicted based on the model states, so that the model will not be punished too severely for producing hypotheses that deviate from the reference, as long as the hypotheses can still be aligned with the reference.", "labels": [], "entities": []}, {"text": "Experiments on three IWSLT tasks (GermanEnglish, English-German and VietnameseEnglish) show that our approach significantly improves BLEU compared to both maximum likelihood and scheduled sampling baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9992851614952087}]}, {"text": "We also provide evidence that our approach addresses exposure bias by decoding with varying beam sizes, and show that our approach is simpler to train than scheduled sampling as it requires no annealing schedule.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We evaluate our approach on IWSLT 2014 German-English (de-en) as prior work (, as well as two additional tasks: IWSLT 2014 English-German (en-de) and IWSLT Method Anneal de-en en-de vi-en: BLEU scores of our approach (SAML) and three baselines including the maximum likelihood (ML) baseline, scheduled sampling (SS), and differentiable scheduled sampling (DSS).", "labels": [], "entities": [{"text": "IWSLT 2014 German-English (de-en)", "start_pos": 33, "end_pos": 66, "type": "DATASET", "confidence": 0.9223200579484304}, {"text": "IWSLT 2014 English-German", "start_pos": 117, "end_pos": 142, "type": "DATASET", "confidence": 0.8822271227836609}, {"text": "IWSLT", "start_pos": 155, "end_pos": 160, "type": "DATASET", "confidence": 0.6744568347930908}, {"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.998501181602478}, {"text": "scheduled sampling (SS)", "start_pos": 297, "end_pos": 320, "type": "METRIC", "confidence": 0.7715764880180359}, {"text": "differentiable scheduled sampling (DSS)", "start_pos": 326, "end_pos": 365, "type": "METRIC", "confidence": 0.7165367752313614}]}, {"text": "The Anneal column indicates whether the sampling rate is annealed.", "labels": [], "entities": [{"text": "Anneal", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9881814122200012}]}, {"text": "For each task, we report the mean and standard deviation over 5 runs with different random seeds.", "labels": [], "entities": [{"text": "mean", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.960465669631958}]}, {"text": "SAML achieves the best BLEU scores and is simpler to train than SS and DSS, as it requires no annealing schedule.", "labels": [], "entities": [{"text": "SAML", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5444368124008179}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9983701109886169}]}, {"text": "2015 Vietnamese-English (vi-en).", "labels": [], "entities": []}, {"text": "For de-en and en-de, we follow the preprocessing steps in.", "labels": [], "entities": []}, {"text": "For vi-en, we use the data preprocessed by, with test2012 for validation and test2013 for testing.", "labels": [], "entities": []}, {"text": "Setup Our translation models are attentional RNNs () built on Sockeye ().", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9265397191047668}]}, {"text": "We use bi-directional LSTM encoder and single-layer LSTM decoder with 256 hidden units, embeddings of size 256, and multilayer perceptron attention with a layer size of 256.", "labels": [], "entities": []}, {"text": "We apply layer normalization () and label smoothing (0.1).", "labels": [], "entities": []}, {"text": "We add dropout to embeddings (0.1) and decoder hidden states (0.2).", "labels": [], "entities": []}, {"text": "For ST Gumbel-Softmax, we use temperature \u03b3 = 1 and noise scale \u03b2 = 0.5.", "labels": [], "entities": [{"text": "ST Gumbel-Softmax", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.5574720948934555}, {"text": "noise scale \u03b2", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9535788098971049}]}, {"text": "The decoding beam size is 5 unless stated otherwise.", "labels": [], "entities": []}, {"text": "We train the models using the Adam optimizer () with a batch size of 1024 words.", "labels": [], "entities": []}, {"text": "We checkpoint models every 1000 updates.", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.0002, and it is reduced by 30% after 4 checkpoints without validation perplexity improvement.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7443779905637106}]}, {"text": "Training stops after 12 checkpoints without improvement.", "labels": [], "entities": []}, {"text": "For training efficiency, we first pre-train a baseline model for each task using only J M Land fine-tune it using different approaches.", "labels": [], "entities": []}, {"text": "In the fine-tuning phase, we inherit all settings except that we initialize the learning rate to 0.00002 and set the minimum number of checkpoints before early stopping to 24.", "labels": [], "entities": []}, {"text": "We fine-tune each randomly seeded model independently.", "labels": [], "entities": []}, {"text": "Baselines We compare our model against three baselines: (1) a standard baseline trained with the ML objective, and models fine-tuned with (2) scheduled sampling (SS) (  and differentiable scheduled sampling (DSS) (.", "labels": [], "entities": [{"text": "scheduled sampling (SS)", "start_pos": 142, "end_pos": 165, "type": "METRIC", "confidence": 0.7603976368904114}]}, {"text": "In SS and DSS, the probability of using reference words sis annealed using inverse sigmoid decay ( ): s = k/(k + exp(i/k)) at the i-th checkpoint with k = 10.", "labels": [], "entities": []}, {"text": "Results shows that the SAML improves over the ML baseline by +0.5 BLEU on de-en, +0.7 BLEU on en-de, and +1.0 BLEU on vi-en task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9986786246299744}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9978321194648743}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9976460337638855}]}, {"text": "In addition, SAML consistently improves over both the scheduled sampling and differentiable scheduled sampling on all tasks.", "labels": [], "entities": [{"text": "SAML", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.8755459785461426}]}, {"text": "All improvements are significant with p < 0.002.", "labels": [], "entities": []}, {"text": "Interestingly, differentiable scheduled sampling performs no better than scheduled sampling in our experiments, unlike in.", "labels": [], "entities": []}, {"text": "Unlike scheduled sampling, our approach does not require an annealing schedule, and it is therefore simpler to train.", "labels": [], "entities": []}, {"text": "We verify that the annealing schedule is needed in scheduled sampling by training a contrastive model with the same objective as scheduled sampling, but without annealing schedule.", "labels": [], "entities": []}, {"text": "We set the sampling rate to 0.5.", "labels": [], "entities": []}, {"text": "The contrastive model hurts BLEU scores by at least 4.0 points compared to both the ML baseline and models fine-tuned with scheduled sampling, confirming that scheduled sampling needs the annealing schedule to work well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9991169571876526}, {"text": "ML baseline", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.632500171661377}]}, {"text": "We further examine the performance gain of different approaches over the baseline with varying beam sizes).", "labels": [], "entities": []}, {"text": "Our approach yields larger BLEU improvements when decoding with greedy search and smaller beams, while there is no clear pattern for scheduled sampling models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9992523789405823}]}, {"text": "These results support the hypothesis that our approach mitigates exposure bias, as it yields bigger improvements in settings where systems have fewer opportunities to recover from early errors.", "labels": [], "entities": []}, {"text": "gorithms require an expert policy, which produces the best next token given any model predicted prefix, and assume that policy can be efficiently computed from the reference.", "labels": [], "entities": []}, {"text": "However, for structured prediction tasks such as machine translation with large vocabulary and complex loss functions, it is intractable to find the best next token given any prefix.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7955735921859741}]}, {"text": "For time series modeling, the Data as Demonstrator algorithm) derives the expert policy directly from the reference sequences which are aligned with the sampled sequences at each time step.", "labels": [], "entities": [{"text": "time series modeling", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6196252206961314}]}, {"text": "Scheduled sampling algorithms ( use the same strategy to train neural sequence-to-sequence models fora broader range of language generation tasks, even though the time alignment between reference and sampled sequences does not hold.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.7887047131856283}]}, {"text": "proposed to complete a predicted prefix with all possible reference suffixes and picking the reference suffix that yields the highest BLEU-1 score.", "labels": [], "entities": [{"text": "BLEU-1 score", "start_pos": 134, "end_pos": 146, "type": "METRIC", "confidence": 0.9779609441757202}]}, {"text": "However, they found that this approach performs well only when the prefix is close to the reference.", "labels": [], "entities": []}, {"text": "Reinforcement learning (RL) algorithms address exposure bias by directly optimizing a sentence-level reward for the model generated sequences.", "labels": [], "entities": [{"text": "Reinforcement learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8216045141220093}]}, {"text": "Evaluation metrics such as BLEU can be used as rewards, but they are discontinuous and hard to optimize.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9986498951911926}]}, {"text": "Techniques such as policy gradient and actor-critic ( are thus required to find an unbiased estimation of the gradient to optimize the model.", "labels": [], "entities": []}, {"text": "Due to the high variance of the gradient estimation, training with RL can be slow and unstable (.", "labels": [], "entities": []}, {"text": "Recent alternatives use data augmentation to incorporate the sentence-level reward into the training objective more efficiently (.", "labels": [], "entities": []}, {"text": "Finally, our SAML loss shares the idea of flexible reference word order with the bag-of-word loss introduced by to improve source coverage.", "labels": [], "entities": []}, {"text": "However, their loss is computed with teacher forcing and therefore does not address exposure bias.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: We evaluate on two translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9066334068775177}]}, {"text": " Table 2: BLEU scores of our approach (SAML)  and three baselines including the maximum likelihood  (ML) baseline, scheduled sampling (SS), and differ- entiable scheduled sampling (DSS). The Anneal col- umn indicates whether the sampling rate is annealed.  For each task, we report the mean and standard devi- ation over 5 runs with different random seeds. SAML  achieves the best BLEU scores and is simpler to train  than SS and DSS, as it requires no annealing schedule.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.995823860168457}, {"text": "differ- entiable scheduled sampling (DSS)", "start_pos": 144, "end_pos": 185, "type": "METRIC", "confidence": 0.725042000412941}, {"text": "Anneal col- umn", "start_pos": 191, "end_pos": 206, "type": "METRIC", "confidence": 0.930239275097847}, {"text": "BLEU", "start_pos": 381, "end_pos": 385, "type": "METRIC", "confidence": 0.9911847710609436}]}]}