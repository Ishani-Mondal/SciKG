{"title": [{"text": "FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling", "labels": [], "entities": [{"text": "FAIRSEQ", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7569970488548279}, {"text": "Sequence Modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.9290281534194946}]}], "abstractContent": [{"text": "FAIRSEQ is an open-source sequence model-ing toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks.", "labels": [], "entities": [{"text": "FAIRSEQ", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8614978790283203}, {"text": "translation", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.975341260433197}, {"text": "summarization", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.6912153959274292}, {"text": "language modeling", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.6468157172203064}, {"text": "text generation", "start_pos": 176, "end_pos": 191, "type": "TASK", "confidence": 0.6398043185472488}]}, {"text": "The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9001917839050293}]}, {"text": "We also support fast mixed-precision training and inference on modern GPUs.", "labels": [], "entities": []}, {"text": "A demo video can be found here: https://www.youtube.", "labels": [], "entities": []}, {"text": "com/watch?v=OtgDdWtHvto.", "labels": [], "entities": [{"text": "OtgDdWtHvto", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.5753133893013}]}], "introductionContent": [{"text": "Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling.", "labels": [], "entities": [{"text": "text generation", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7227088660001755}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7808243036270142}, {"text": "abstractive document summarization", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.640357087055842}, {"text": "language modeling", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7710602581501007}]}, {"text": "Accordingly, both researchers and industry professionals can benefit from a fast and easily extensible sequence modeling toolkit.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7420334815979004}]}, {"text": "There are several toolkits with similar basic functionality, but they differ in focus area and intended audiences.", "labels": [], "entities": []}, {"text": "For example,) is a community-built toolkit written in multiple languages with an emphasis on extensibility.", "labels": [], "entities": []}, {"text": "MarianNMT) focuses on performance and the backend is written in C++ for fast automatic differentiation.", "labels": [], "entities": [{"text": "MarianNMT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9649544954299927}]}, {"text": "OpenSeq2Seq ( ) provides reference implementations for fast distributed and mixed precision training.) and Sockeye () focus on production-readiness.", "labels": [], "entities": [{"text": "OpenSeq2Seq", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9234212636947632}, {"text": "Sockeye", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.8589677810668945}]}, {"text": "In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production.", "labels": [], "entities": [{"text": "FAIRSEQ", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9558057188987732}, {"text": "sequence modeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7686598002910614}, {"text": "PyTorch", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.931911289691925}]}, {"text": "FAIRSEQ features: (i) a common interface across models and tasks that can be extended * equal contribution \u2020 Work done while at Facebook AI Research. with user-supplied plug-ins ( \u00a72); (ii) efficient distributed and mixed precision training, enabling training over datasets with hundreds of millions of sentences on current hardware ( \u00a73); (iii) stateof-the-art implementations and pretrained models for machine translation, summarization, and language modeling ( \u00a74); and (iv) optimized inference with multiple supported search algorithms, including beam search, diverse beam search (, and top-k sampling.", "labels": [], "entities": [{"text": "FAIRSEQ", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8018693923950195}, {"text": "machine translation, summarization", "start_pos": 404, "end_pos": 438, "type": "TASK", "confidence": 0.6966896653175354}]}, {"text": "FAIRSEQ is distributed with a BSD license and is available on GitHub at https://github.com/ pytorch/fairseq.", "labels": [], "entities": [{"text": "FAIRSEQ", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8269709944725037}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU on news2014 for WMT English- German (En-De) and English-French (En-Fr). All re- sults are based on WMT'14 training data, except for  En-De (b), (c), (d) and our models which were trained  on WMT'16. Train times based on V100 GPUs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994571805000305}, {"text": "news2014", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9646130800247192}, {"text": "WMT English- German", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6796882599592209}, {"text": "re- sults", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9287748336791992}, {"text": "WMT'14 training data", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.908049464225769}, {"text": "WMT'16", "start_pos": 206, "end_pos": 212, "type": "DATASET", "confidence": 0.9865694642066956}]}, {"text": " Table 3: Test perplexity on WikiText-103 (cf. Table 4).", "labels": [], "entities": [{"text": "WikiText-103", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9280804991722107}]}, {"text": " Table 4: Test perplexity on the One Billion Word  benchmark. Adaptive inputs share parameters with an  adaptive softmax.", "labels": [], "entities": []}, {"text": " Table 5: Abstractive summarization results on the full- text version of CNN-DailyMail dataset.", "labels": [], "entities": [{"text": "Abstractive", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9915255904197693}, {"text": "CNN-DailyMail dataset", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9202786684036255}]}]}