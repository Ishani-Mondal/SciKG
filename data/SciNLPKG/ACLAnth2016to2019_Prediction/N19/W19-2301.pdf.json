{"title": [{"text": "An Adversarial Learning Framework For A Persona-Based Multi-Turn Dialogue Model", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and soon.", "labels": [], "entities": []}, {"text": "The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discrimi-nator.", "labels": [], "entities": []}, {"text": "We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discrimi-nator, collaboratively predicts the attribute(s) that generated the input utterance.", "labels": [], "entities": []}, {"text": "To demonstrate the superior performance of phredGAN over the persona Seq2Seq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends.", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus (UDC)", "start_pos": 136, "end_pos": 164, "type": "DATASET", "confidence": 0.7435863266388575}]}, {"text": "Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation.", "labels": [], "entities": []}, {"text": "We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).", "labels": [], "entities": [{"text": "Ubuntu dataset", "start_pos": 246, "end_pos": 260, "type": "DATASET", "confidence": 0.9150131046772003}]}], "introductionContent": [{"text": "Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6141163210074106}, {"text": "dialogue modeling", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7939682006835938}]}, {"text": "Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage.", "labels": [], "entities": []}, {"text": "Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset.", "labels": [], "entities": []}, {"text": "However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.", "labels": [], "entities": []}, {"text": "Most work in this domain has primarily focused on optimizing dialogue consistency.", "labels": [], "entities": [{"text": "dialogue consistency", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7214213460683823}]}, {"text": "For example, and introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue.", "labels": [], "entities": []}, {"text": "However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated.", "labels": [], "entities": []}, {"text": "tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output.", "labels": [], "entities": []}, {"text": "While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.", "labels": [], "entities": []}, {"text": "On the other hand, there has been some recent work on introducing persona into dialogue models.", "labels": [], "entities": []}, {"text": "For example, integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model.", "labels": [], "entities": [{"text": "generative dialogue", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7082580626010895}]}, {"text": "In this work, Li et al. consider persona models, one with Speaker-only representation and the other with Speaker and Addressee representations (SpeakerAddressee model), both of which capture certain speaker identity and interactions.", "labels": [], "entities": []}, {"text": "continue along the same line of thought by considering a Seq2Seq dialogue model with Responder-only representation.", "labels": [], "entities": []}, {"text": "In both of these cases, the attribute representation is learned during the system training.", "labels": [], "entities": []}, {"text": "proposed a slightly different approach.", "labels": [], "entities": []}, {"text": "Here, the attributes area set of sentences describing the profile of the speaker.", "labels": [], "entities": []}, {"text": "In this case, the attributes representation is not learned.", "labels": [], "entities": []}, {"text": "The system however learns how to attend to different parts of the attributes during training.", "labels": [], "entities": []}, {"text": "Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context.", "labels": [], "entities": []}, {"text": "This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.", "labels": [], "entities": []}, {"text": "In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation.", "labels": [], "entities": []}, {"text": "In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator () with additional utterance attribute representation at its encoder and decoder inputs as depicted in.", "labels": [], "entities": []}, {"text": "Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system.", "labels": [], "entities": []}, {"text": "The attribute representation is an embedding that is learned together with the rest of model parameters similar to.", "labels": [], "entities": []}, {"text": "Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns.", "labels": [], "entities": []}, {"text": "Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses.", "labels": [], "entities": []}, {"text": "The difference between the two systems is in the discriminator architecture based on how the attribute is treated.", "labels": [], "entities": []}, {"text": "We train and sample both variants of phredGAN similar to the procedure for hredGAN (.", "labels": [], "entities": []}, {"text": "To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis.", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus (UDC)", "start_pos": 89, "end_pos": 117, "type": "DATASET", "confidence": 0.8117434481779734}]}, {"text": "We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of human evaluation of dialogue response quality as well as automatic evaluations with perplexity, BLEU, ROUGE and distinct n-gram scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.9993388056755066}, {"text": "ROUGE", "start_pos": 292, "end_pos": 297, "type": "METRIC", "confidence": 0.9934991598129272}]}], "datasetContent": [{"text": "In this section, we explore the performance of PHRED, phredGAN a and phredGAN don two conversational datasets and compare their performances to non-adversarial persona Seq2seq models () as well as to the adversarial hredGAN () with no explicit persona.", "labels": [], "entities": []}, {"text": "TV Series Transcripts dataset (.", "labels": [], "entities": [{"text": "TV Series Transcripts dataset", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.9505535960197449}]}, {"text": "We train all models on transcripts from two popular TV drama series, Big Bang Theory and Friends.", "labels": [], "entities": [{"text": "Big Bang Theory", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.8146919012069702}]}, {"text": "Following a similar preprocessing setup in, we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue.", "labels": [], "entities": []}, {"text": "We split the corpus into training, development, and test set with a 94%, 3%, and 3% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset.", "labels": [], "entities": []}, {"text": "Due to the small size of the combined transcripts dataset, we first train the models on the larger Movie Triplets Corpus (MTC) by Banchs (2012) which consists of 240,000 dialogue triples.", "labels": [], "entities": [{"text": "Movie Triplets Corpus (MTC) by Banchs (2012)", "start_pos": 99, "end_pos": 143, "type": "DATASET", "confidence": 0.8604606742208655}]}, {"text": "We pre-train the models on this dataset to initialize the model parameters to avoid overfitting on a relatively small persona TV series dataset.", "labels": [], "entities": [{"text": "persona TV series dataset", "start_pos": 118, "end_pos": 143, "type": "DATASET", "confidence": 0.7554673925042152}]}, {"text": "After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization for training on the combined person TV series dataset.", "labels": [], "entities": [{"text": "MTC", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.5328986644744873}, {"text": "combined person TV series dataset", "start_pos": 167, "end_pos": 200, "type": "DATASET", "confidence": 0.6102499127388}]}, {"text": "Ubuntu Dialogue Corpus (UDC) dataset).", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus (UDC) dataset", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.9680089695113046}]}, {"text": "We train the models on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation.", "labels": [], "entities": [{"text": "Ubuntu community hub", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.9592269659042358}]}, {"text": "We assign two types of speaker IDs to utterances in this dataset: questioner and helper.", "labels": [], "entities": []}, {"text": "We follow a similar training, development, and test split as the UDC dataset in, with 90%, 5%, and 5% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that do not classify under either a questioner or helper speaker type.", "labels": [], "entities": [{"text": "UDC dataset", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9513133466243744}]}, {"text": "In order to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner.", "labels": [], "entities": []}, {"text": "This simplifying assumption does introduce a degree of noise into each persona model's ability to construct attribute embeddings.", "labels": [], "entities": []}, {"text": "However, our experiment results demonstrate that both phredGAN a and phredGAN dare still able to differentiate between the larger two speaker types in the dataset.", "labels": [], "entities": []}, {"text": "We use similar evaluation metrics as in including perplexity, BLEU), ROUGE (Lin, 2014), distinct n-gram () and normalized average sequence length (NASL) scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9992122650146484}, {"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9983501434326172}, {"text": "normalized average sequence length (NASL) scores", "start_pos": 111, "end_pos": 159, "type": "METRIC", "confidence": 0.8246144950389862}]}, {"text": "For human evaluation, we follow a similar setup as, employing crowd-sourced judges to evaluate a random selection of 200 samples.", "labels": [], "entities": []}, {"text": "We present both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance, informativeness, and persona.", "labels": [], "entities": []}, {"text": "For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1.", "labels": [], "entities": []}, {"text": "The scores are normalized between 0 and 1 and averaged over the total number of samples and judges.", "labels": [], "entities": []}, {"text": "For each model, we also estimate the per sample score variance between judges and then average over the number of samples, i.e., sum of variances divided by the square of number of samples (assuming sample independence).", "labels": [], "entities": []}, {"text": "The square root of result is reported as the standard error of the human judgement for the model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: phredGAN vs. Li et al. (2016b) on BBT Friends TV Transcripts.", "labels": [], "entities": [{"text": "BBT Friends TV Transcripts", "start_pos": 44, "end_pos": 70, "type": "DATASET", "confidence": 0.9696682095527649}]}, {"text": " Table 2: phredGAN vs. Li et al. (2016b) on UDC.", "labels": [], "entities": [{"text": "UDC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7348493337631226}]}]}