{"title": [{"text": "Rethinking Complex Neural Network Architectures for Document Classification", "labels": [], "entities": [{"text": "Rethinking Complex Neural Network Architectures", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.9131571769714355}, {"text": "Document Classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.8267947137355804}]}], "abstractContent": [{"text": "Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult.", "labels": [], "entities": []}, {"text": "A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective.", "labels": [], "entities": []}, {"text": "We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regu-larization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets.", "labels": [], "entities": [{"text": "document classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7717517018318176}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9995842576026917}, {"text": "F 1", "start_pos": 231, "end_pos": 234, "type": "METRIC", "confidence": 0.9952812790870667}]}, {"text": "Surprisingly , our simple model is able to achieve these results without attention mechanisms.", "labels": [], "entities": []}, {"text": "While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context.", "labels": [], "entities": []}, {"text": "Our work provides an open-source platform and the foundation for future work in document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7404640913009644}]}], "introductionContent": [{"text": "Recent developments in neural architectures fora wide range of NLP tasks can be characterized as a drive towards increasingly complex network components and modeling techniques.", "labels": [], "entities": []}, {"text": "Worryingly, these new models are accompanied by smaller and smaller improvements in effectiveness on standard benchmark datasets, which leads us to wonder if observed improvements are \"real\".", "labels": [], "entities": []}, {"text": "There is, however, ample evidence to the contrary.", "labels": [], "entities": []}, {"text": "To provide a few examples: report that standard LSTM architectures outperform more recent models when properly tuned.", "labels": [], "entities": []}, {"text": "show that sequence transduction using encoder-decoder networks with attention mechanisms work just as well with the attention module only, making most of the complex * Equal contribution.", "labels": [], "entities": []}, {"text": "show that simple RNN-and CNN-based models yield accuracies rivaling far more complex architectures in simple question answering over knowledge graphs.", "labels": [], "entities": [{"text": "question answering", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.733648955821991}]}, {"text": "Perhaps most damning are the indictments of, who lament the lack of empirical rigor in our field and cite even more examples where improvements can be attributed to far more mundane reasons (e.g., hyperparameter tuning) or are simply noise.", "labels": [], "entities": []}, {"text": "concur with these sentiments, adding that authors often use fancy mathematics to obfuscate or to impress (reviewers) rather than to clarify.", "labels": [], "entities": []}, {"text": "Complex architectures are more difficult to train, more sensitive to hyperparameters, and brittle with respect to domains with different data characteristics-thus both exacerbating the \"crisis of reproducibility\" and making it difficult for practitioners to deploy networks that tackle real-world problems in production environments.", "labels": [], "entities": []}, {"text": "Like the papers cited above, we question the need for overly complex neural architectures, focusing on the problem of document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7500973343849182}]}, {"text": "Starting with a large-scale reproducibility study of several recent neural models, we find that a simple bi-directional LSTM (BiLSTM) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9996163845062256}, {"text": "F 1", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.9950591027736664}]}, {"text": "As the closest comparison point, we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms.", "labels": [], "entities": []}, {"text": "While these regularization techniques, borrowed from language modeling, are not novel, we are to our knowledge the first to apply them in this context.", "labels": [], "entities": []}, {"text": "Our work provides an opensource platform and the foundation for future work in document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7353394627571106}]}], "datasetContent": [{"text": "We conduct a large-scale reproducibility study involving HAN, XML-CNN, KimCNN, and SGM.", "labels": [], "entities": [{"text": "HAN", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8175209164619446}, {"text": "KimCNN", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.9412669539451599}]}, {"text": "These are compared to our proposed model, referred to as LSTM reg , as well as an ablated variant without regularization, denoted LSTM base . The implementation of our model as well as fromscratch reimplementations of all the comparison models (except for SGM) are provided in our toolkit called Hedwig, which we make publicly available to serve as the foundation for future work.", "labels": [], "entities": [{"text": "Hedwig", "start_pos": 296, "end_pos": 302, "type": "DATASET", "confidence": 0.9376106262207031}]}, {"text": "1 In addition, we compare the neural approaches to logistic regression (LR) and support vector machines (SVMs).", "labels": [], "entities": []}, {"text": "The LR model is trained using a one-vs-rest multi-label objective, while the SVM is trained with a linear kernel.", "labels": [], "entities": []}, {"text": "Both of these methods use word-level tf-idf vectors of the documents as features.", "labels": [], "entities": []}, {"text": "All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs, with PyTorch 0.4.1 as the backend framework.", "labels": [], "entities": [{"text": "Nvidia GTX 1080 and RTX 2080 Ti GPUs", "start_pos": 40, "end_pos": 76, "type": "DATASET", "confidence": 0.8175248466432095}]}, {"text": "We use Scikitlearn 0.19.2 for computing the tf-idf vectors and implementing LR and SVMs.", "labels": [], "entities": []}, {"text": "We evaluate our models on the following four datasets: Reuters-21578, arXiv Abstract Paper dataset (AAPD), IMDB, and Yelp 2014.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.973559558391571}, {"text": "arXiv Abstract Paper dataset (AAPD)", "start_pos": 70, "end_pos": 105, "type": "DATASET", "confidence": 0.824904339654105}, {"text": "IMDB", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.7035272717475891}, {"text": "Yelp 2014", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.9092358946800232}]}, {"text": "Reuters and AAPD are multi-label datasets, whereas IMDB and Yelp are single-label ones.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9699572920799255}, {"text": "AAPD", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8992952108383179}, {"text": "IMDB", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9323832988739014}, {"text": "Yelp", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9255516529083252}]}, {"text": "For IMDB and Yelp, we use random sampling to split the dataset such that 80% is used for training, 10% for validation, and 10% for test.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6957540512084961}, {"text": "Yelp", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9099277853965759}]}, {"text": "We use the standard ModApte splits for the Reuters dataset, and author-defined splits for AAPD (.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.982717752456665}, {"text": "AAPD", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.8177192211151123}]}, {"text": "We summarize the statistics of these datasets in.", "labels": [], "entities": []}, {"text": "Unfortunately, there is little consensus within the natural language processing community for choosing the splits of IMDB and Yelp 2014.", "labels": [], "entities": [{"text": "IMDB and Yelp 2014", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.7374482825398445}]}, {"text": "Furthermore, they are often unreported in modeling papers, hence preventing direct comparison with past results.", "labels": [], "entities": []}, {"text": "We are notable to find the exact splits  For the multi-label datasets, we report the wellknown micro-averaged F 1 score, which is the class-weighted harmonic mean between recall and precision.", "labels": [], "entities": [{"text": "micro-averaged F 1 score", "start_pos": 95, "end_pos": 119, "type": "METRIC", "confidence": 0.8355492949485779}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.978245735168457}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.906636118888855}]}, {"text": "For the single-label datasets, we compare the models using accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995593428611755}]}], "tableCaptions": [{"text": " Table 1: Summary of the datasets. C denotes the num- ber of classes in the dataset, N the number of samples,  and W and S the average number of words and sen- tences per document, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Results for each model on the validation and test sets; best values are bolded in blue. Repl. reports mean  \u00b1 SD of five runs from our reimplementations; Orig. refers to point estimates from  \u2020 Yang et al. (2018),  \u2021 Yang  et al. (2016), and  \u2020 \u2020 Tang et al. (2015).", "labels": [], "entities": [{"text": "mean  \u00b1 SD", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.6437003016471863}, {"text": "Orig.", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.7243056893348694}]}]}