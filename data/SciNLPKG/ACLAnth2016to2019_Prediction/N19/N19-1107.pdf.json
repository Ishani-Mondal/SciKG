{"title": [{"text": "Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant Supervision", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9843860566616058}, {"text": "Memory Augmented Distant Supervision", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6991006731987}]}], "abstractContent": [{"text": "Distant supervision (DS) is an important paradigm for automatically extracting relations.", "labels": [], "entities": [{"text": "Distant supervision (DS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8093336939811706}, {"text": "automatically extracting relations", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.6423500378926595}]}, {"text": "It utilizes existing knowledge base to collect examples for the relation we intend to extract, and then uses these examples to automatically generate the training data.", "labels": [], "entities": []}, {"text": "However , the examples collected can be very noisy, and pose significant challenge for obtaining high quality labels.", "labels": [], "entities": []}, {"text": "Previous work has made remarkable progress in predicting the relation from distant supervision, but typically ignores the temporal relations among those supervising instances.", "labels": [], "entities": [{"text": "predicting", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9632274508476257}]}, {"text": "This paper formulates the problem of relation extraction with temporal reasoning and proposes a solution to predict whether two given entities participate in a relation at a given time spot.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8373548090457916}]}, {"text": "For this purpose, we construct a dataset called WIKI-TIME 1 which additionally includes the valid period of a certain relation of two entities in the knowledge base.", "labels": [], "entities": []}, {"text": "We propose a novel neu-ral model to incorporate both the temporal information encoding and sequential reasoning.", "labels": [], "entities": []}, {"text": "The experimental results show that, compared with the best of existing models, our model achieves better performance in both WIKI-TIME dataset and the well-studied NYT-10 dataset.", "labels": [], "entities": [{"text": "WIKI-TIME dataset", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.939206063747406}, {"text": "NYT-10 dataset", "start_pos": 164, "end_pos": 178, "type": "DATASET", "confidence": 0.9768485724925995}]}], "introductionContent": [{"text": "As an important technique to automatically complete the knowledge base and reduce labeling efforts, distant supervision (DS) for relation extraction has drawn much attention.", "labels": [], "entities": [{"text": "distant supervision (DS)", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.5362406432628631}, {"text": "relation extraction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.9298106729984283}]}, {"text": "In DS, we align the entity pair (head, tail) from a triple head, rel, tail extracted from a huge knowledge base (e.g., Freebase, Wikidata) with sentences from free texts (e.g., Wikipedia, New York Times) * Jian Li is the corresponding author.", "labels": [], "entities": []}, {"text": "1 https://github.com/ElliottYan/DS_ Temporal to obtain the training examples, and the label of such an example is the corresponding relation rel.", "labels": [], "entities": [{"text": "ElliottYan", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9729022979736328}, {"text": "Temporal", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8651641011238098}]}, {"text": "Therefore, DS can automatically create a set of training data for each entity pair.", "labels": [], "entities": []}, {"text": "However, the noisy training data problem () significantly affects the performance of DS.", "labels": [], "entities": [{"text": "DS", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9743152856826782}]}, {"text": "Therefore, most of the recent approaches () follow a common assumption called the at-least-once assumption, which treats all aligned sentences of each entity pair as one training sample.", "labels": [], "entities": []}, {"text": "We refer to a sentence as an instance and all sentences aligned to one entity pair as a mention set in the following, respectively.", "labels": [], "entities": []}, {"text": "The models in previous work) generally include two parts, encoding and fusion.", "labels": [], "entities": []}, {"text": "The former encodes each instance into a low-dimensional representation.", "labels": [], "entities": []}, {"text": "The latter combines representation of each instance.", "labels": [], "entities": []}, {"text": "Then, their combination is used to predict the relation.", "labels": [], "entities": []}, {"text": "Although the approaches mentioned above seem promising, they have the following limitations: 1.", "labels": [], "entities": []}, {"text": "They all use a separate but identical encoding module among instances and introduce no difference temporally.", "labels": [], "entities": []}, {"text": "2. They only adopt single step of fusion and introduce no sentence-level reasoning.", "labels": [], "entities": []}, {"text": "We remark that the aforementioned approaches maybe enough for the standard NYT-10 dataset (, because the dataset only extracts instances from New York Times corpus from the year 2005 to 2007 and consists of few mention sets with longtime span.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9768112897872925}, {"text": "New York Times corpus from the year 2005", "start_pos": 142, "end_pos": 182, "type": "DATASET", "confidence": 0.7871219888329506}]}, {"text": "However, as one can easily imagine, ignoring temporal information may cause inaccurate predictions, especially when a mention set has along time span and some instances express different relations.", "labels": [], "entities": []}, {"text": "For example, suppose we want to predict the relation between Angelina Jolie and Brad Pitt (using Wikidata).", "labels": [], "entities": []}, {"text": "The knowledge base contains a factual relation of spouse between them with the valid period from August 2014 to September 2016.", "labels": [], "entities": []}, {"text": "However, the extracted mention set contains instances about their marriage in 2014, as well as their divorce in 2016.", "labels": [], "entities": []}, {"text": "Because existing models do not encode temporal information, the relation they extract is likely to be the one with highest confidence.", "labels": [], "entities": []}, {"text": "In this example, their models may predict the relation of marriage since the instances may suggest a higher confidence for the relation of marriage.", "labels": [], "entities": []}, {"text": "But the correct prediction should be divorce.", "labels": [], "entities": []}, {"text": "As shown in the above example, we can see it is necessary to include temporal information in DS.", "labels": [], "entities": []}, {"text": "On the other hand, in fusion module, most existing work focused on denoising using methods such as attention or reinforcement learning.", "labels": [], "entities": []}, {"text": "We want to argue that a sentence-level reasoning can also be useful since there are instances which are not direct positive examples for the given relation, but can provide supporting evidence.", "labels": [], "entities": []}, {"text": "We call them remote instances.", "labels": [], "entities": []}, {"text": "Consider the Jolie-Pitt example again.", "labels": [], "entities": []}, {"text": "Suppose we are to predict their relation after their divorce.", "labels": [], "entities": []}, {"text": "The instances about their marriage also indirectly help to infer their divorce since marriage is the premise of divorce.", "labels": [], "entities": []}, {"text": "Hence, we need an algorithm that can incorporate temporal information and perform reasoning over remote instances.", "labels": [], "entities": []}, {"text": "In this paper, we address both limitations and extend the task to predict the relation of a particular entity pair at any specific time spot.", "labels": [], "entities": []}, {"text": "The problem can be formulated as a sequence labeling problem (See \u00a7 2).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6093339622020721}]}, {"text": "We propose a novel relation extraction architecture that can address both aforementioned limitations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8758076429367065}]}, {"text": "Our model follows the popular encoding-fusion architecture, but makes two crucial modifications.", "labels": [], "entities": []}, {"text": "Firstly, we introduce temporal encoding to model the temporal information among the instances in the encoding.", "labels": [], "entities": []}, {"text": "Secondly, we use the Memory Network () to iteratively reason over temporally augmented encodings in the fusion part.", "labels": [], "entities": []}, {"text": "Moreover, we evaluate our model on the widely studied NYT-10 dataset () and anew WIKI-TIME dataset.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9856725931167603}, {"text": "WIKI-TIME dataset", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.9679338932037354}]}, {"text": "The construction of WIKI-TIME is similar to that of the NYT-10 dataset except for two important differences.", "labels": [], "entities": [{"text": "WIKI-TIME", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.9047096371650696}, {"text": "NYT-10 dataset", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9811583459377289}]}, {"text": "One is that we only consider triples head, rel, tail with the valid period (T 1, T 2).", "labels": [], "entities": []}, {"text": "For example, the triple Jolie, married, Pitt has a valid period of.", "labels": [], "entities": []}, {"text": "The other is that we extract contextual temporal information for each aligned instance.", "labels": [], "entities": []}, {"text": "We use Wikidata as knowledge base and Wikipedia as free corpus.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.922760009765625}]}, {"text": "Both automatic and manual evaluation are applied in the experiments.", "labels": [], "entities": []}, {"text": "The experimental results show that, compared with existing models, our model can achieve comparable/better performance in both WIKI-TIME and standard NYT-10 datasets.", "labels": [], "entities": [{"text": "WIKI-TIME", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.8755760788917542}, {"text": "NYT-10 datasets", "start_pos": 150, "end_pos": 165, "type": "DATASET", "confidence": 0.9635032713413239}]}, {"text": "Our main contributions can be summarized as follows: \u2022 We introduce anew task aiming to solve the problem of relation extraction with temporal information.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8246289491653442}]}, {"text": "\u2022 We propose a novel relation extraction architecture, which encodes both the temporal and semantic information and includes remote instances for temporal reasoning.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.831665962934494}]}, {"text": "\u2022 We construct anew WIKI-TIME dataset by aligning Wikidata to Wikipedia, which is specially designed for the task of relation extraction with temporal information.", "labels": [], "entities": [{"text": "WIKI-TIME dataset", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.8890777230262756}, {"text": "relation extraction", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7816748023033142}]}, {"text": "\u2022 The experiment results show that, compared with the best of existing models, our model achieves comparable/better performance both in WIKI-TIME dataset and standart NYT-10 dataset.", "labels": [], "entities": [{"text": "WIKI-TIME dataset", "start_pos": 136, "end_pos": 153, "type": "DATASET", "confidence": 0.9632634520530701}, {"text": "NYT-10 dataset", "start_pos": 167, "end_pos": 181, "type": "DATASET", "confidence": 0.9619072377681732}]}], "datasetContent": [{"text": "We evaluate our model on two datasets, the widely used NYT-10 dataset which is developed by () and the WIKI-TIME dataset we created.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9844751358032227}, {"text": "WIKI-TIME dataset", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9617984890937805}]}, {"text": "Hyper-Parameter Settings For WIKI-TIME experiments, we construct query over each appeared time spot in the mention set.", "labels": [], "entities": []}, {"text": "On the other hand, for NYT-10 experiments, we adopt a single query without temporal encoding to compare results with other baseline methods since the dataset only contains one label for each mention set.", "labels": [], "entities": [{"text": "NYT-10", "start_pos": 23, "end_pos": 29, "type": "TASK", "confidence": 0.4935171902179718}]}, {"text": "Among all experiments, we use 230 convolution kernels with windows size 3.", "labels": [], "entities": []}, {"text": "The dropout probability pd is set to 0.5.", "labels": [], "entities": [{"text": "dropout probability pd", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9208783904711405}]}, {"text": "We try various max hops values H (from 1 to 5) to test how reasoning works in our model.", "labels": [], "entities": []}, {"text": "We train the models with 20 epochs and 50 epochs for NYT-10 dataset and WIKI-TIME dataset and report the best performance.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9666960835456848}, {"text": "WIKI-TIME dataset", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9290298819541931}]}, {"text": "As for optimization step, we adopt SGD with gradient plus Gaussian noise with standard deviation of 0.01, which helps to better generalize.", "labels": [], "entities": []}, {"text": "Also, we apply gradient decay of rate (\u03c1 = 0.5) over every \u03c4 = 10 epochs.", "labels": [], "entities": []}, {"text": "The learning rates for NYT-10 and WIKI-TIME experiments are set to 0.001 and 0.01, respectively.", "labels": [], "entities": [{"text": "NYT-10", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.919708788394928}, {"text": "WIKI-TIME", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.8530579805374146}]}, {"text": "The performance of comparative experiment is reported by precision-recall (PR) curve.", "labels": [], "entities": [{"text": "precision-recall (PR) curve", "start_pos": 57, "end_pos": 84, "type": "METRIC", "confidence": 0.9585642099380494}]}, {"text": "Specifically, we sort the prediction scores of the model in The details of construction of WIKI-TIME can be found in Appendix A.  Since the WIKI-TIME is distantly collected, we want to obtain a more precise view of how the models perform.", "labels": [], "entities": [{"text": "WIKI-TIME", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.9156093597412109}, {"text": "WIKI-TIME", "start_pos": 140, "end_pos": 149, "type": "DATASET", "confidence": 0.9207527041435242}]}, {"text": "So, we apply the manual evaluation to verify our experimental results.", "labels": [], "entities": []}, {"text": "We randomly pick 200 mention sets in the test set of WIKI-TIME and ask two annotators to label the relation for each instance.", "labels": [], "entities": [{"text": "WIKI-TIME", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9237467646598816}]}, {"text": "The annotation rule is to label the instance with the relation that can be inferred from the instance itself or previous instances.", "labels": [], "entities": []}, {"text": "As shown in, the manual evaluated F1 scores are basically consistent with the PR curves in, which indirectly proves the WIKI-TIME's quality.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9985698461532593}, {"text": "PR", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9492764472961426}, {"text": "WIKI-TIME", "start_pos": 120, "end_pos": 129, "type": "DATASET", "confidence": 0.5738252401351929}]}, {"text": "Also, we find that the TempMEM + P achieves the best performance and shows obvious advantages in both query-level and bag-level F1 scores over the naive TempMEM (i.e., with no temporal encodings).", "labels": [], "entities": [{"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.8724104762077332}]}, {"text": "This proves the effectiveness of our temporal encodings.", "labels": [], "entities": []}, {"text": "In this section, we report our results on the wellstudied NYT-10 dataset.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9471810460090637}]}, {"text": "By evaluating our model in the NYT-10 dataset, our objective is to prove the power of reasoning among remote instances.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9861226677894592}]}, {"text": "Note that, in the NYT-10 dataset, there is no temporal information for each instance, so we only use one query for each mention set and there's no  temporal encoding for each instance.", "labels": [], "entities": [{"text": "NYT-10 dataset", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.9725346565246582}]}, {"text": "Also, we do not use the entity embedding for the NYT-10 experiments.", "labels": [], "entities": [{"text": "NYT-10 experiments", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9379082322120667}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For both CNN and PCNN models, We can see that our models exceed the performance of all other models (CNN ATT, CNN ONE, CNN AVE, PCNN ATT, PCNN ONE, PCNN AVE) in the range of low recall values.", "labels": [], "entities": [{"text": "CNN ATT", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8620237112045288}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9987589120864868}]}, {"text": "In the high recall range, our models also have results about the same as the best model among others.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.999178946018219}]}, {"text": "This suggests that even without the temporal encoding, reasoning over remote instances is indeed useful in relation extraction task.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.9360657036304474}]}], "tableCaptions": [{"text": " Table 2: Comparison with previous models.  P@N 100/200/300 refers to the precision for the  highest 100, 200 and 300 predictions in WIKI-TIME.", "labels": [], "entities": [{"text": "P", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.9916248917579651}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9990977048873901}, {"text": "WIKI-TIME", "start_pos": 133, "end_pos": 142, "type": "DATASET", "confidence": 0.9239911437034607}]}, {"text": " Table 3: Manual evaluation of Bag-level and  Query-level F1 scores in WIKI-TIME.", "labels": [], "entities": [{"text": "Query-level F1 scores", "start_pos": 46, "end_pos": 67, "type": "METRIC", "confidence": 0.7651967406272888}, {"text": "WIKI-TIME", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.906306803226471}]}]}