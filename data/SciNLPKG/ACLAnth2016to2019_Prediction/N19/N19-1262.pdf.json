{"title": [{"text": "Data-efficient Neural Text Compression with Interactive Learning", "labels": [], "entities": [{"text": "Neural Text Compression", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.5853051443894705}]}], "abstractContent": [{"text": "Neural sequence-to-sequence models have been successfully applied to text compression.", "labels": [], "entities": [{"text": "text compression", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.8240358829498291}]}, {"text": "However, these models were trained on huge automatically induced parallel corpora, which are only available fora few domains and tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel interactive setup to neural text compression that enables transferring a model to new domains and compression tasks with minimal human supervision.", "labels": [], "entities": [{"text": "neural text compression", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6761577725410461}]}, {"text": "This is achieved by employing active learning, which intelligently samples from a large pool of unlabeled data.", "labels": [], "entities": []}, {"text": "Using this setup, we can successfully adapt a model trained on small data of 40k samples fora headline generation task to a general text compression dataset at an acceptable compression quality with just 500 sampled instances annotated by a human.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7995205521583557}]}], "introductionContent": [{"text": "Text compression is the task of condensing one or multiple sentences into a shorter text of a given length preserving the most important information.", "labels": [], "entities": [{"text": "Text compression", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7650370895862579}]}, {"text": "In natural language generation applications, such as summarization, text compression is a major step to condense the extracted important content of the source documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9927477240562439}, {"text": "text compression", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7068129330873489}]}, {"text": "But text compression can also be applied in a wide range of related applications, including the generation of headlines (), captions, subtitles (, and the compression of text for small screens).", "labels": [], "entities": [{"text": "text compression", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7242840826511383}]}, {"text": "Neural sequence-to-sequence (Seq2Seq) models have shown remarkable success in many areas of natural language processing and specifically in natural language generation tasks, including text compression (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.6574355860551199}, {"text": "text compression", "start_pos": 185, "end_pos": 201, "type": "TASK", "confidence": 0.7931493520736694}]}, {"text": "Despite their success, Seq2Seq models have a major drawback, as they require huge parallel corpora with pairs of source and compressed text to be able to learn the parameters for the model.", "labels": [], "entities": []}, {"text": "So far, the size of the training data has been proportional to the increase in the model's performance (, which is a major hurdle if only limited annotation capacities are available to manually produce a corpus.", "labels": [], "entities": []}, {"text": "That is why existing research employs large-scale automatically extracted compression pairs, such as the first sentence and the presumably shorter headline of a news article.", "labels": [], "entities": []}, {"text": "However, such easy-to-extract source data is only available fora few tasks, domains, and genres and the corresponding models do not generalize well from the task of headline generation to other text compression tasks.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.8177977800369263}]}, {"text": "In this paper, we propose an interactive setup to neural text compression, which learns to compress based on user feedback acquired during training time.", "labels": [], "entities": [{"text": "neural text compression", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6138926645119985}]}, {"text": "For the first time, we apply active learning (AL) methods to neural text compression, which greatly reduces the amount of the required training data and thus yields a much more data-efficient training and annotation workflow.", "labels": [], "entities": [{"text": "neural text compression", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.6499851743380228}]}, {"text": "In our experiments, we find that this approach enables the successful transfer of a model trained on headline generation data to a general text compression task with a minimum of parallel training instances.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7837471067905426}, {"text": "text compression", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.7249173671007156}]}, {"text": "The objective of AL is to efficiently select unlabeled instances that a user should annotate to advance the training.", "labels": [], "entities": []}, {"text": "A key component of AL is the choice of the sampling strategy, which curates the samples in order to maximize the model's performance with a minimum amount of user interaction.", "labels": [], "entities": [{"text": "AL", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9467823505401611}]}, {"text": "Many AL sampling strategies have proven effective for human-supervised natural language processing tasks other than compression.", "labels": [], "entities": [{"text": "AL sampling", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8268767297267914}]}, {"text": "In our work, we exploit the application of uncertainty-based sampling using attention disper-sion and structural similarity for choosing samples to be annotated for our interactive Seq2Seq text compression model.", "labels": [], "entities": [{"text": "Seq2Seq text compression", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.5696746706962585}]}, {"text": "We employ the AL strategies for (a) learning a model with a minimum data, and (b) adapting a pretrained model with few user inputs to anew domain.", "labels": [], "entities": []}, {"text": "In the remaining paper, we first discuss related work and introduce the state-of-the-art Seq2Seq architecture for the neural text compression task.", "labels": [], "entities": [{"text": "neural text compression task", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.6927868276834488}]}, {"text": "Then, we propose our novel interactive compression approach and demonstrate how batch mode AL can be integrated with neural Seq2Seq models for text compression.", "labels": [], "entities": [{"text": "text compression", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7908954620361328}]}, {"text": "In section 4, we introduce our experimental setup, and in section 5, we evaluate our AL strategies and show that our approach successfully enables (a) learning the Seq2Seq model with a minimum of data, (b) transfer of a pretrained headline generation model to anew compression task and dataset with minimal user interaction.", "labels": [], "entities": []}, {"text": "To encourage further research and enable reproducing our results, we publish our code as open-source software.", "labels": [], "entities": []}], "datasetContent": [{"text": "Google News 195,000 5,000 10,000 MSR-OANC 5,000 448 785 For evaluating the compressions against the reference compressions, we use a Python wrapper 4 of the ROUGE metric) with the parameters suggested by yielding high correlation with human judgments (i.e., with stemming and without stopword removal).", "labels": [], "entities": [{"text": "Google News 195,000 5,000 10,000 MSR-OANC 5,000 448 785", "start_pos": 0, "end_pos": 55, "type": "DATASET", "confidence": 0.922655443350474}, {"text": "ROUGE", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.8897292017936707}]}], "tableCaptions": [{"text": " Table 2: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) achieved by the state-of-the-art models using our  sampling strategies evaluated on the Google compression test set. Bold marks best AL strategy.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9538012742996216}, {"text": "ROUGE-2 (R2)", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9013462364673615}, {"text": "ROUGE-L (RL)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9453261941671371}, {"text": "Google compression test set", "start_pos": 142, "end_pos": 169, "type": "DATASET", "confidence": 0.7856494039297104}]}, {"text": " Table 3: In-domain active learning example sentence and compressions for Google News compression dataset  when using 20% of labelled compressions with Random, Coverage-AL, Diversity-AL sampling strategies", "labels": [], "entities": [{"text": "Google News compression dataset", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.8857505917549133}]}, {"text": " Table 4: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) achieved by the state-of-the-art models using our  sampling strategies when interactively retrained using 10% of the MSR-OANC training set. The results are in  comparison to the models trained on in-domain training set (MSR-OANC ID). Bold marks best AL strategy.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9350013732910156}, {"text": "ROUGE-2", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9440882205963135}, {"text": "ROUGE-L (RL)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9446468651294708}, {"text": "MSR-OANC training set", "start_pos": 171, "end_pos": 192, "type": "DATASET", "confidence": 0.7622329394022623}]}]}