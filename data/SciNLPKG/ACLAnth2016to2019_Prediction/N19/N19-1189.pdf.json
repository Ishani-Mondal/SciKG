{"title": [{"text": "Curriculum Learning for Domain Adaptation in Neural Machine Translation", "labels": [], "entities": [{"text": "Domain Adaptation in Neural Machine Translation", "start_pos": 24, "end_pos": 71, "type": "TASK", "confidence": 0.665836105744044}]}], "abstractContent": [{"text": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain.", "labels": [], "entities": []}, {"text": "Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule.", "labels": [], "entities": []}, {"text": "This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) performance often drops when training and test domains do not match and when in-domain training data is scarce (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8117110828558604}]}, {"text": "Tailoring the NMT system to each domain could improve performance, but unfortunately high-quality parallel data does not exist for all domains.", "labels": [], "entities": []}, {"text": "Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.789866030216217}, {"text": "indomain translation", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.7887201309204102}]}, {"text": "One approach to exploit unlabeled-domain bitext is to apply data selection techniques () to find bitext that are similar to in-domain data.", "labels": [], "entities": []}, {"text": "This selected data can additionally be combined with in-domain bitext and trained in a continued training framework, as shown in.", "labels": [], "entities": []}, {"text": "Continued training or fine-tuning () is an adaptation technique where a model is first trained on the large general domain data, then used as initialization of anew model which is further trained on in-domain bitext.", "labels": [], "entities": []}, {"text": "In our framework, the selected samples are concatenated with in-domain data, then used for continued training.", "labels": [], "entities": []}, {"text": "This effectively increases the in-domain training size with \"pseudo\" in-domain samples, and is helpful in continued training (.", "labels": [], "entities": []}, {"text": "A challenge with employing data selection in continued training is that there exists no clear-cut way to define whether a sample is sufficiently similar to in-domain data to be included.", "labels": [], "entities": []}, {"text": "In practice, one has to define a threshold based on similarity scores, and even so the continued training algorithm maybe faced with samples of diverse similarities.", "labels": [], "entities": []}, {"text": "We introduce anew domain adaptation technique that addresses this challenge.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7287032902240753}]}, {"text": "Inspired by curriculum learning (, we use the similarity scores given by data selection to rearrange the order of training samples, such that more similar examples are seen earlier and more frequently during training.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work applying curriculum learning to domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7184832394123077}]}, {"text": "We demonstrate the effectiveness of our approach on TED Talks and patent abstracts for German-English and Russian-English pairs, using two distinct data selection methods, Moore-Lewis method and cynical data selection.", "labels": [], "entities": []}, {"text": "Results show that our approach consistently outperforms standard continued training, with up to 3.22 BLEU improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9996938705444336}]}, {"text": "Our S 4 error analysis () reveal that this approach reduces a reasonable number of SENSE and SCORE errors.", "labels": [], "entities": [{"text": "SENSE", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9196475148200989}]}, {"text": "provide guidelines for curriculum learning: \"A practical curriculum learning method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking.\"", "labels": [], "entities": []}, {"text": "For domain adaptation we choose to estimate the difficulty of a training sample based on its distance to the in-domain data, which can be quantified by existing data selection methods (Section 2.1).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.827905923128128}]}, {"text": "For the sampling procedure, we adopt a probabilistic curriculum training (CL) strategy that takes advantage of the spirit of curriculum learning in a nondeterministic fashion without discarding the good practice of original standard training policy, like bucketing and mini-batching.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on four domain adaptation tasks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7469925582408905}]}, {"text": "The code base is provided to ensure reproducibility.", "labels": [], "entities": []}, {"text": "Our goal is to empirically test whether the proposed curriculum learning method improves translation quality in the continued training setup of github.com/awslabs/sockeye The Adam optimizer for continued training model is initialized without reloading from the trained generic model.", "labels": [], "entities": []}, {"text": "Appendix D also shows the effect of using language models built from target side and both sides.", "labels": [], "entities": []}, {"text": "8 After experimenting with various values from 5 to 100 (Appendix B), we found best performance can be achieved at 40 shards.", "labels": [], "entities": [{"text": "Appendix B)", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9715833266576132}]}, {"text": ", and a standard continued training model using a random subset (rather than ML or CDS scores) of the concatenated in-domain and Paracrawl data (std rand).", "labels": [], "entities": [{"text": "Paracrawl data", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.9287381768226624}]}, {"text": "summarizes the key results, where we continue train on 15k in-domain samples and 4096k Paracrawl samples (for de) or 2048k Paracrawl samples (for ru):", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU of unadapted & adapted models. \u2206  shows improvement of CL over std.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989519119262695}]}, {"text": " Table 2: The top ranked sentences selected from German-English Paracrawl corpus.", "labels": [], "entities": [{"text": "German-English Paracrawl corpus", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.8235499660174052}]}, {"text": " Table 3: In-domain data statistics.", "labels": [], "entities": []}, {"text": " Table 3. In this paper, we  uniformly sample 15k in-domain data from each  dataset. We choose the amount of 15k, which  makes up a relatively small percentage of the orig- inal corpora, in order to evaluate the extreme case  of low-resource domain adaptation settings. Un- der this setting, the positive effect of adding more  selected unlabeled-domain data into training cor- pus is more obvious in terms of the performance  improvement of NMT models. Our pilot exper- iments show that curriculum learning can scale  with more in-domain data-it consistently outper- forms the standard training policy, but with less  improvement. This is not surprising, as when  there is enough in-domain data, continued training  on only the in-domain data can already achieve a  pretty good performance, and we do not need to  use extra unlabeled-domain data to augment it any  more, neither does curriculum learning.", "labels": [], "entities": []}]}