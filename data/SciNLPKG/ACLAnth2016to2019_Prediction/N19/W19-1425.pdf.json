{"title": [{"text": "Cross-lingual Annotation Projection Is Effective for Neural Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Cross-lingual Annotation Projection", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7441331744194031}, {"text": "Neural Part-of-Speech Tagging", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.6398301521937052}]}], "abstractContent": [{"text": "We tackle the important task of part-of-speech tagging using a neural model in the zero-resource scenario, where we have no access to gold-standard POS training data.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.79063281416893}, {"text": "POS training data", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.598179558912913}]}, {"text": "We compare this scenario with the low-resource scenario , where we have access to a small amount of gold-standard POS training data.", "labels": [], "entities": [{"text": "POS training data", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.6422624289989471}]}, {"text": "Our experiments focus on Ukrainian as a representative of under-resourced languages.", "labels": [], "entities": []}, {"text": "Russian is highly related to Ukrainian, so we exploit gold-standard Russian POS tags.", "labels": [], "entities": []}, {"text": "We consider four techniques to perform Ukrainian POS tagging: zero-shot tagging and cross-lingual annotation projection (for the zero-resource scenario), and compare these with self-training and multilingual learning (for the low-resource scenario).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.7795539498329163}, {"text": "zero-shot tagging", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6898999214172363}, {"text": "cross-lingual annotation projection", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.621379941701889}]}, {"text": "We find that cross-lingual annotation projection works particularly well in the zero-resource scenario.", "labels": [], "entities": [{"text": "cross-lingual annotation projection", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.7127745350201925}]}], "introductionContent": [{"text": "Little or no hand-annotated part-of-speech training data exists for the vast majority of languages in the world.", "labels": [], "entities": []}, {"text": "This work investigates POS-tagging for under-resourced languages with a state-of-theart neural network model.", "labels": [], "entities": []}, {"text": "We consider how best to deal with the zero-resource scenario (i.e., no availability of any POS-labeled training data for the targeted language).", "labels": [], "entities": []}, {"text": "To better understand this scenario, we compare it with the lowresource scenario (i.e., availability of a small POSlabeled training corpus).", "labels": [], "entities": []}, {"text": "We thoroughly compare four techniques, including: zero-shot tagging and cross-lingual annotation projection from a linguistically related higher-resource language (for the zero-resource scenario), as well as selftraining and multilingual learning (for the lowresource scenario).", "labels": [], "entities": [{"text": "zero-shot tagging", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7165574580430984}, {"text": "cross-lingual annotation projection", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.6203884482383728}]}, {"text": "A controlled experimental design is established for our study.", "labels": [], "entities": []}, {"text": "We aim for immediate comparability of all tested tagging strategies of both scenarios, zero-resource and low-resource.", "labels": [], "entities": []}, {"text": "We therefore opt to carryout both the zero-resource and the low-resource experiments on the same language, Ukrainian, and measure tagging accuracy on one common test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9814018607139587}]}, {"text": "A small amount of manually POS-annotated Ukrainian training data is available, which we use for supervised low-resource training.", "labels": [], "entities": [{"text": "POS-annotated Ukrainian training data", "start_pos": 27, "end_pos": 64, "type": "DATASET", "confidence": 0.6111056432127953}]}, {"text": "We simulate the zero-resource scenario by not using any POS-annotated Ukrainian training data.", "labels": [], "entities": [{"text": "POS-annotated Ukrainian training data", "start_pos": 56, "end_pos": 93, "type": "DATASET", "confidence": 0.7822055220603943}]}, {"text": "Russian is a higher-resource language which is linguistically closely related to Ukrainian.", "labels": [], "entities": []}, {"text": "We use a larger POS-annotated Russian corpus for multilingual learning and zero-shot tagging experiments, and an unlabeled RussianUkrainian parallel corpus for the cross-lingual projection annotation experiment.", "labels": [], "entities": [{"text": "POS-annotated Russian corpus", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.6846940914789835}, {"text": "RussianUkrainian parallel corpus", "start_pos": 123, "end_pos": 155, "type": "DATASET", "confidence": 0.8124766945838928}]}, {"text": "To strengthen the upper-bound result for low-resource tagging, we consider the improvements possible through self-training, for which we use the Ukrainian side of the Russian-Ukrainian parallel corpus in order to maintain comparability.", "labels": [], "entities": []}, {"text": "Our experimental design allows us to directly assess whether the tagging quality of any zero-resource strategy is approaching the accuracies of supervised lowresource strategies.", "labels": [], "entities": []}, {"text": "We find that zero-shot tagging does not yield satisfactory quality, even if we operate on a higher linguistic abstraction level with word stems, which are often very similar in Ukrainian and Russian.", "labels": [], "entities": [{"text": "zero-shot tagging", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7188969254493713}]}, {"text": "But the empirical results show that annotation projection from a closelyrelated language is a very effective strategy for training neural POS taggers.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7020521461963654}, {"text": "POS taggers", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7596455216407776}]}], "datasetContent": [{"text": "We now present the results for all the investigated techniques.", "labels": [], "entities": []}, {"text": "The tagging accuracies for all experiments on the Ukrainian test set are collectively shown in.", "labels": [], "entities": [{"text": "Ukrainian test set", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.947873572508494}]}, {"text": "Some further empirical observations, e.g. on the taggers' ability to correctly handle OOV words, will also be discussed below.", "labels": [], "entities": []}, {"text": "Supplementary tagging accuracies of Russian POS-taggers measured on a Russian test set are reported in.", "labels": [], "entities": [{"text": "Russian test set", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.773205171028773}]}], "tableCaptions": [{"text": " Table 3: Shared vocabulary before and after stemming.", "labels": [], "entities": []}]}