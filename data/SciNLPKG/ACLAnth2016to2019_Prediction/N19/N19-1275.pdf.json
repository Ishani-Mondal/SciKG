{"title": [{"text": "Bridging the Gap: Attending to Discontinuity in Identification of Multiword Expressions", "labels": [], "entities": [{"text": "Identification of Multiword Expressions", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.7884103059768677}]}], "abstractContent": [{"text": "We introduce anew method to tag Multiword Expressions (MWEs) using a linguistically in-terpretable language-independent deep learning architecture.", "labels": [], "entities": [{"text": "tag Multiword Expressions (MWEs)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.7541970511277517}]}, {"text": "We specifically target dis-continuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs.", "labels": [], "entities": []}, {"text": "Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention.", "labels": [], "entities": []}, {"text": "GCN leverages dependency parse information, and self-attention attends to long-range relations.", "labels": [], "entities": [{"text": "GCN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5934226512908936}, {"text": "dependency parse information", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.798432469367981}]}, {"text": "We finally propose a combined model that integrates complementary information from both, through a gating mechanism.", "labels": [], "entities": []}, {"text": "The experiments on a standard multilingual dataset for verbal MWEs show that our model outper-forms the baselines not only in the case of dis-continuous MWEs but also in overall F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 178, "end_pos": 185, "type": "METRIC", "confidence": 0.9931459426879883}]}], "introductionContent": [{"text": "Multiword expressions (MWEs) are linguistic units composed of more than one word whose meanings cannot be fully determined by the semantics of their components ().", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7456439733505249}]}, {"text": "As they are fraught with syntactic and semantic idiosyncrasies, their automatic identification remains a major challenge).", "labels": [], "entities": [{"text": "automatic identification", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.5567666292190552}]}, {"text": "Occurrences of discontinuous MWEs are particularly elusive as they involve relationships between non-adjacent tokens (e.g. put one of the blue masks on).", "labels": [], "entities": []}, {"text": "While some previous studies disregard discontinuous MWEs (, others stress the importance of factoring them in ().", "labels": [], "entities": []}, {"text": "Using a CRF-based and a transition-based approach respectively, and Al try to * *The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "The code is available on https://github.com/ omidrohanian/gappy-mwes.", "labels": [], "entities": []}, {"text": "capture discontinuous occurrences with help from dependency parse information.", "labels": [], "entities": [{"text": "dependency parse information", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7933547099431356}]}, {"text": "Previously explored neural MWE identification models) suffer from limitations in dealing with discontinuity, which can be attributed to their inherently sequential nature.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9018280804157257}]}, {"text": "More sophisticated architectures are yet to be investigated.", "labels": [], "entities": []}, {"text": "Graph convolutional neural networks (GCNs) (  and attention-based neural sequence labeling () are methodologies suited for modeling non-adjacent relations and are hence adapted to MWE identification in this study.) uses a global graph structure for the entire input.", "labels": [], "entities": [{"text": "attention-based neural sequence labeling", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.6098103821277618}, {"text": "MWE identification", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.9867224395275116}]}, {"text": "We modify it such that GCN filters convolve nodes of dependency parse tree on a per-sentence basis.", "labels": [], "entities": []}, {"text": "Self-attention, on the other hand, learns representations by relating different parts of the same sequence.", "labels": [], "entities": []}, {"text": "Each position in a sequence is linked to any other position with O(1) operations, minimising maximum path (compared to RNN's O(n)) which facilitates gradient flow and makes it theoretically well-suited for learning long-range dependencies (.", "labels": [], "entities": [{"text": "O", "start_pos": 125, "end_pos": 126, "type": "METRIC", "confidence": 0.8059530854225159}]}, {"text": "The difference in the two approaches motivates our attempt to incorporate them into a hybrid model with an eye to exploiting their individual strengths.", "labels": [], "entities": []}, {"text": "Other studies that used related syntax-aware methods in sequence labeling include and where GCN and self-attention were separately applied to semantic role labelling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.6602904200553894}, {"text": "semantic role labelling", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.6816807190577189}]}, {"text": "Our contribution in this study, is to show for the first time, how GCNs can be successfully applied to MWE identification, especially to tackle discontinuous ones.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9882440268993378}]}, {"text": "Furthermore, we propose a novel architecture that integrates GCN with selfattention outperforming state-of-the-art.", "labels": [], "entities": []}, {"text": "The resulting models not only prove superior to existing methods in terms of overall performance but also are more robust in handling cases with gaps.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with datasets from the shared task on automatic identification of verbal Multiword Expressions ( ).", "labels": [], "entities": [{"text": "automatic identification of verbal Multiword Expressions", "start_pos": 52, "end_pos": 108, "type": "TASK", "confidence": 0.7193336933851242}]}, {"text": "The datasets are tagged for different kinds of verbal MWEs including idioms, verb particle constructions, and light verb constructions among others.", "labels": [], "entities": []}, {"text": "We focus on annotated corpora of four languages: French (FR), German (DE), English (EN), and Persian (FA) due to their variety in size and proportion of discontinuous MWEs.", "labels": [], "entities": []}, {"text": "Tags in the datasets are converted to a variation of IOB which includes the tags B (beginning of MWEs), I (other components of MWEs), and O (tokens outside MWEs), with the addition of G for arbitrary tokens in between the MWE components e.g. make important decisions . ELMo.", "labels": [], "entities": []}, {"text": "In our experiments, we make use of ELMo embeddings () which are contextualised and token-based as opposed to  type-based word representations like word2vec or GLoVe where each word type is assigned a single vector.", "labels": [], "entities": []}, {"text": "Token-based embeddings better reflect the syntax and semantics of each word in its context compared to traditional type-based ones.", "labels": [], "entities": []}, {"text": "We use the implementation by to train ELMo embeddings on our data.", "labels": [], "entities": []}, {"text": "In the validation phase, we start with a strong baseline which is a CNN + Bi-LSTM model based on the top performing system in the VMWE shared task . Our implemented baseline differs in that we employ ELMo rather than word2vec resulting in a significant improvement.", "labels": [], "entities": []}, {"text": "We perform hyperparameter optimisation and make comparisons among our systems, including GCN + Bi-LSTM (GCN-based), CNN + attention + Bi-LSTM (Attbased), and their combination using a highway layer (H-combined) in.", "labels": [], "entities": []}, {"text": "Systems are evaluated using two types of precision, recall and F-score measures: strict MWEbased scores (every component of an MWE should be correctly tagged to be considered as true positive), and token-based scores (a partial match between a predicted and a gold MWE would be considered as true positive).", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9991369843482971}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9910796284675598}, {"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9757055640220642}]}, {"text": "We report results for all MWEs as well as discontinuous ones specifically.", "labels": [], "entities": []}, {"text": "According to, GCN-based outperforms Att-based and they both outperform the strong baseline in terms of MWE-based F-score in three out of four languages.", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.5235025882720947}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.8672186732292175}]}, {"text": "Combining GCN with attention using highway networks results in further improvements for EN, FR and FA.", "labels": [], "entities": [{"text": "EN", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.8764370679855347}, {"text": "FR", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9923065304756165}, {"text": "FA", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9980636239051819}]}, {"text": "The Hcombined model consistently exceeds the baseline for all languages.", "labels": [], "entities": []}, {"text": "As can be seen in, GCN and H-combined models each show significant improvement with regard to discontinuous MWEs, regardless of the proportion of such expressions.", "labels": [], "entities": []}, {"text": "In we show the superior performance (in terms of MWE-based F-score) of our top systems on the test data compared to the baseline and stateof-the-art systems, namely, ATILF-LLF and SHOMA.", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.6696389317512512}, {"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.785017192363739}, {"text": "ATILF-LLF", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.5650080442428589}, {"text": "SHOMA", "start_pos": 180, "end_pos": 185, "type": "DATASET", "confidence": 0.8710061311721802}]}, {"text": "GCN works the best for discontinuous MWEs in EN and FA, while H-combined outperforms based on results for all MWEs except for FA.", "labels": [], "entities": [{"text": "GCN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8871660232543945}, {"text": "FA", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9411160945892334}]}, {"text": "The findings are further discussed in Section 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model performance (P, R and F) for devel- opment sets for all MWE and only discontinuous ones  (%: proportion of discontinuous MWES)", "labels": [], "entities": []}, {"text": " Table 2: Comparing the performance of the systems on test data in terms of MWE-based F-score", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.8714317083358765}]}]}