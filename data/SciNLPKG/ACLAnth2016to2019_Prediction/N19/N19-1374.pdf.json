{"title": [], "abstractContent": [{"text": "The majority of current systems for end-to-end dialog generation focus on response quality without an explicit control over the affec-tive content of the responses.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7563063502311707}]}, {"text": "In this paper, we present an affect-driven dialog system, which generates emotional responses in a controlled manner using a continuous representation of emotions.", "labels": [], "entities": []}, {"text": "The system achieves this by mod-eling emotions at a word and sequence level using: (1) a vector representation of the desired emotion, (2) an affect regularizer, which penalizes neutral words, and (3) an affect sampling method, which forces the neural network to generate diverse words that are emotionally relevant.", "labels": [], "entities": []}, {"text": "During inference, we use a re-ranking procedure that aims to extract the most emotionally relevant responses using a human-in-the-loop optimization process.", "labels": [], "entities": []}, {"text": "We study the performance of our system in terms of both quantitative (BLEU score and response diversity), and qualitative (emotional appropriate-ness) measures.", "labels": [], "entities": [{"text": "BLEU score and response diversity", "start_pos": 70, "end_pos": 103, "type": "METRIC", "confidence": 0.8711556434631348}]}], "introductionContent": [{"text": "Recent breakthroughs in deep learning techniques have had an impact on end-to-end conversational systems (.", "labels": [], "entities": []}, {"text": "Current research is mainly focused on functional aspects of conversational systems: keyword extraction, natural language understanding, and pertinence of generated responses (.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7683801054954529}, {"text": "natural language understanding", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6499715050061544}]}, {"text": "Although these aspects are indeed key features for building a commercial system, most existing solutions lack social intelligence.", "labels": [], "entities": []}, {"text": "Conversational systems could benefit from incorporating social intelligence by: (1) avoiding interaction problems that may arise when the system does not understand the user's request (e.g., inappropriate responses that cause user anger) (, and (2) building rapport * Both authors contributed equally to this with the user (.", "labels": [], "entities": []}, {"text": "Our method makes such conversational systems more social by outputting responses expressing emotion in a controlled manner, without sacrificing grammatical correctness, coherence, or relevance.", "labels": [], "entities": []}, {"text": "Existing sequence-to-sequence (seq2seq) architectures, either recurrent-(, attention-( or convolutional neural network (CNN)-based, do not provide a straightforward way to generate emotionally relevant output in a controlled manner.", "labels": [], "entities": []}, {"text": "We introduce EMOTIonal CONversational System (EMOTICONS), which generates emotion-specific responses.", "labels": [], "entities": []}, {"text": "It is based on novel contributions presented in this paper which fall in two main categories: explicit models which allow a controlled emotion-based response generation (e.g., methods based on emotion embeddings, affective sampling, and affective re-ranking), and implicit models with no direct control over the desired emotion (i.e., affective regularizer).", "labels": [], "entities": []}, {"text": "We show that EMOTICONS outperforms both the system proposed by (current state of the art for our task) and the vanilla seq2seq in terms of BLEU score () (improvement up to 7.7%) and response diversity (improvement up to 52%).", "labels": [], "entities": [{"text": "EMOTICONS", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.7503398060798645}, {"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9844337701797485}]}, {"text": "Additionally, we qualitatively evaluate the emotional content of the generated text (see example responses in).", "labels": [], "entities": []}, {"text": "The user study (22 people) demonstrates that EMOTICONS is able to generate grammatically correct, coherent, emotionally-rich text in a controlled manner.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate language models, we use BLEU score (computed using 1-to 4-grams), as it has been shown to correlate well with human judgment).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9891704618930817}]}, {"text": "Perplexity does not provide a fair comparison across the models: during the training of the baseline seq2seq model, we minimize the cross entropy loss (logarithm of perplexity), whereas in other models (e.g., WI) we aim to minimize a different loss not directly related to perplexity (cross entropy extended with the affective regularizer).", "labels": [], "entities": []}, {"text": "Having more diverse responses makes the affective re-ranking more efficient, to evaluate diversity we count the number of distinct unigrams (distinct-1) and bigrams (distinct-2), normalized by the total number of generated tokens.", "labels": [], "entities": []}, {"text": "The performance of different models introduced in \u00a73 are presented in.", "labels": [], "entities": []}, {"text": "refers to a system that re-ranks responses based on Equation 2, where both p(R C |S) and p(S|R C ) are baseline seq2seq models.", "labels": [], "entities": []}, {"text": "EMOTICONS is a system based on Equation 3, where p(R C |S, E 0 ) is computed using a composition of Word-Level Implicit Model (WI) and Word-Level Explicit Model (WE), and p(S|R C ) is computed using WI (as we are not interested in explicitly using the input emotion).", "labels": [], "entities": [{"text": "EMOTICONS", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8525263667106628}]}, {"text": "We optimize \u03b1 and \u03b2 on the validation set using BLEU score, since have shown that adding MMI during inference improves the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9787901043891907}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.998975396156311}]}, {"text": "We set \u03b3 = 0 and find optimal values \u03b1 opt = 50.0 and \u03b2 opt = 0.001 using grid search.", "labels": [], "entities": []}, {"text": "Improving BLEU score and diversity was not the goal of our work, but the observed improvement (after adding emotions) shows that the different systems are able to extract and use emotional patterns to improve the general language model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9689719974994659}]}, {"text": "We asked annotators to evaluate (using an AffectButton) the generated responses.", "labels": [], "entities": []}, {"text": "We use Affect- Button (), a reliable affective tool for assigning emotions, which, to our knowledge, has never been used for estimating the emotional content of the generated responses.", "labels": [], "entities": [{"text": "Affect- Button", "start_pos": 7, "end_pos": 21, "type": "METRIC", "confidence": 0.9688353737195333}]}, {"text": "In our experiment, the AffectButton lets users choose a facial expression from a continuous space (see), that best matches the emotional state associated with the sequence, which is then mapped into the VAD space.", "labels": [], "entities": []}, {"text": "In order to conduct the experiment, we chose a pool of 12 annotators, who annotated a total of 400 sequences.", "labels": [], "entities": []}, {"text": "The prompts were randomly chosen from the test set of Cornell, among the 200 sequences that create the most diverse responses in terms of distinct-2.", "labels": [], "entities": [{"text": "Cornell", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8942691087722778}]}, {"text": "The more diverse the responses are, the more likely we are to select a response carrying a desired emotion.", "labels": [], "entities": []}, {"text": "The responses for the prompts were generated using EMOTICONS where the target emotion was either fear, anger, joy, or surprise; the four corners of the AffectButton.", "labels": [], "entities": [{"text": "AffectButton", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.8551945090293884}]}, {"text": "\u03b3 was randomly chosen among 20 uniformly sampled values in.", "labels": [], "entities": []}, {"text": "In, we present the difference between the VAD value according to the face assigned by the user, and the desired emotion for the response.", "labels": [], "entities": [{"text": "VAD", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9972823858261108}]}, {"text": "process ER C is estimated using the emotion classifier () which detects joy more accurately than anger (77% versus 57%), surprise (62%) and fear (69%).", "labels": [], "entities": [{"text": "ER C", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9067531824111938}]}, {"text": "In this section, we qualitatively evaluate the emotional content and correctness of the responses generated by EMOTICONS \u03b3=\u03b3opt compared to the ones from MMI bas. through a user study.", "labels": [], "entities": [{"text": "correctness", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9670947790145874}, {"text": "MMI bas.", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.8229624032974243}]}, {"text": "It consists of three different experiments which measure grammatical correctness, user preference, and emotional appropriateness.", "labels": [], "entities": []}, {"text": "For all experiments, we chose prompts from the test set of Cornell, for which the most diverse responses were created by MMI bas.", "labels": [], "entities": [{"text": "Cornell", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.7892873883247375}]}, {"text": "We test EMOTICONS by generating responses according to four emotions: fear, anger, joy, and surprise (beam size of 200).", "labels": [], "entities": [{"text": "surprise", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9544458985328674}]}], "tableCaptions": [{"text": " Table 2: Quantitative results: Results for all proposed models trained on Cornell (C) and OpenSubtitles (OS).  distinct-1 and distinct-2 count the number of distinct unigrams and bigrams, respectively, normalized by the total  number of generated tokens in 200 candidate responses. The performance boost is computed with respect to the  vanilla seq2seq model.", "labels": [], "entities": [{"text": "Cornell (C)", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9253031611442566}]}, {"text": " Table 3: User study results: Grammatical Correct- ness shows the ratio of grammatically correct sentences  among all generated responses, whereas User Prefer- ence shows the number of times each model was pre- ferred by the users.", "labels": [], "entities": [{"text": "Grammatical Correct- ness", "start_pos": 30, "end_pos": 55, "type": "METRIC", "confidence": 0.8368248045444489}]}]}