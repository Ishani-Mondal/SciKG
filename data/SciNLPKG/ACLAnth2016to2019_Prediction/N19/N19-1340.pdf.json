{"title": [{"text": "Semantic Role Labeling with Associated Memory Network", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7714671492576599}]}], "abstractContent": [{"text": "Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of lat-est works were presented.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8577056527137756}]}, {"text": "This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of memory to further enhance dependency-based SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.936813473701477}, {"text": "SRL", "start_pos": 248, "end_pos": 251, "type": "TASK", "confidence": 0.6905249953269958}]}, {"text": "In detail , we use sentences and their labels from train dataset as an associated memory cue to help label the target sentence.", "labels": [], "entities": []}, {"text": "Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them.", "labels": [], "entities": []}, {"text": "By lever-aging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing anew effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.", "labels": [], "entities": [{"text": "CoNLL-2009 benchmark datasets", "start_pos": 105, "end_pos": 134, "type": "DATASET", "confidence": 0.973308245340983}, {"text": "SRL enhancement", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.9282459914684296}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a given sentence and its predicates.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8583218256632487}]}, {"text": "It is a shallow semantic parsing task, which has been widely used in a series of natural language processing (NLP) tasks, such as information extraction ( and question answering (.", "labels": [], "entities": [{"text": "shallow semantic parsing task", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.7714083641767502}, {"text": "information extraction", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8207390308380127}, {"text": "question answering", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.8485052585601807}]}, {"text": "Generally, SRL is decomposed into four classification subtasks in pipeline systems, consisting of predicate identification, predicate disambiguation, argument identification, and argument classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9771446585655212}, {"text": "predicate identification", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7118724435567856}, {"text": "argument identification", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.7108177989721298}, {"text": "argument classification", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.7331734299659729}]}, {"text": "In recent years, great attention ( has been turned to deep learning method, especially Long Short-term Memory (LSTM) network for learning with automatically extracted features.", "labels": [], "entities": []}, {"text": "( proposed the first end-to-end recurrent neural network (RNN) to solve the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9274374842643738}]}, {"text": "( ) studied several predicate-specified embedding and decoding methods.", "labels": [], "entities": []}, {"text": "() delivered a full study on the influence of RNN training and decoding strategies.", "labels": [], "entities": []}, {"text": "Whether to use the syntactic information for SRL is also studied actively (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9864267706871033}]}, {"text": "Since the recent work of , which surprisingly shows syntax-agnostic dependency SRL for the first time can be rival of syntax-aware models, SRL has been more and more formulized into standard sequence labeling task on a basis of keeping syntax unavailable.", "labels": [], "entities": [{"text": "SRL", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.7846125960350037}, {"text": "SRL", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9765642881393433}]}, {"text": "A series of work on SRL received further performance improvement following this line through further refining neural model design ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9324448108673096}]}, {"text": "Different from all previous work, we propose to introduce an associated memory network which builds memory from known data through the inter-sentence attention to enhance syntaxagnostic model even further.", "labels": [], "entities": []}, {"text": "Inspired by the observation that people always refer to other similar problems and their solutions when dealing with a problem they have never seen, like query in their memory, we want to utilize similar known samples which include the associated sentences and their annotated labels to help model label target sentence.", "labels": [], "entities": []}, {"text": "To reach such a goal, we adopt a memory network component, and use inter-sentence attention to fully exploit the information in memory.", "labels": [], "entities": []}, {"text": "Based on Memory Network (, proposed Key-Value Memory Network (KV-MemNN) to solve Question Answering problem and gain large progress.", "labels": [], "entities": [{"text": "Question Answering problem", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.8585519194602966}]}, {"text": "Our proposed method is similar to KV-MemNN, but with a different definition of key-value and different information distilling process.", "labels": [], "entities": []}, {"text": "Thus, we propose a carefully designed inter-sentence attention mechanism to handle it.", "labels": [], "entities": []}, {"text": "Recently, there are also some attempts to make use of attention mechanism in SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.9093863666057587}]}, {"text": "() focus on selfattention, which only uses the information of the input sentence as the source of attention.) makes use of biaffine attention) for decoding in SRL, which was the current state-of-the-art (SOTA) in CoNLL-2009 benchmark as this work was embarking.", "labels": [], "entities": [{"text": "SRL", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.7587697505950928}, {"text": "CoNLL-2009 benchmark", "start_pos": 213, "end_pos": 233, "type": "DATASET", "confidence": 0.8409076631069183}]}, {"text": "Different from all previous work, we utilize inter-sentence attention to help model leverage associated information from other known sentences in the memory.", "labels": [], "entities": []}, {"text": "To our best knowledge, this is the first time to use memory network in the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 75, "end_pos": 83, "type": "TASK", "confidence": 0.9256181716918945}]}, {"text": "Our evaluation on CoNLL-2009 benchmarks shows that our model outperforms or reaches other syntaxagnostic models on English, and achieves competitive results on Chinese, which indicates that memory network learning from known data is indeed helpful to SRL task.", "labels": [], "entities": [{"text": "CoNLL-2009 benchmarks", "start_pos": 18, "end_pos": 39, "type": "DATASET", "confidence": 0.9112300276756287}, {"text": "SRL task", "start_pos": 251, "end_pos": 259, "type": "TASK", "confidence": 0.8766483068466187}]}, {"text": "There are several SRL annotation conventions, such as PropBank () and FrameNet (.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8911058902740479}, {"text": "FrameNet", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8140870928764343}]}, {"text": "This paper focuses on the former convention.", "labels": [], "entities": []}, {"text": "Under PropBank convention, there are two role representation forms, which are span-based SRL, such as CoNLL 2005 and CoNLL 2012 shared tasks, and dependencybased SRL, such as CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "CoNLL 2005 and CoNLL 2012 shared tasks", "start_pos": 102, "end_pos": 140, "type": "DATASET", "confidence": 0.8886108994483948}, {"text": "CoNLL 2009 shared task", "start_pos": 175, "end_pos": 197, "type": "DATASET", "confidence": 0.8641168475151062}]}, {"text": "The former uses span to represent argument, while the latter uses the headword of the span to represent the argument.", "labels": [], "entities": []}, {"text": "As the latter has been more actively studied due to dependency style SRL for convenient machine learning, we will focus on dependency SRL only in this work.", "labels": [], "entities": [{"text": "dependency SRL", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.6385524272918701}]}, {"text": "Given a sentence S, the goal of dependency SRL task is to find all the predicate-argument pairs (p, a).", "labels": [], "entities": [{"text": "dependency SRL", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.6741989254951477}]}, {"text": "The following shows an example sentence with semantic role labels marked in subscripts.", "labels": [], "entities": []}, {"text": "She A0 has lost v it A1 just ARGM \u2212M NR as quickly.", "labels": [], "entities": [{"text": "A0", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9278649687767029}, {"text": "A1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9607226848602295}, {"text": "ARGM \u2212M NR", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.8869104236364365}]}, {"text": "Here, v means the predicate, A0 means the agent, A1 means the patient and ARGM-MNR means how an action v is performed.", "labels": [], "entities": [{"text": "ARGM-MNR", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9923685193061829}]}, {"text": "In the rest of this paper, we will describe our model in Section 2.", "labels": [], "entities": []}, {"text": "Then, the experiment set-up and results are given in Section 3.", "labels": [], "entities": []}, {"text": "Related works about SRL and attention mechanism will be given in Section 4.", "labels": [], "entities": [{"text": "SRL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9957679510116577}]}, {"text": "Conclusions and future work are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on   we obtain the result with similar parameters as for the best model in English.", "labels": [], "entities": []}, {"text": "The English and Chinese GloVe word embeddings are both trained on Wikipedia.", "labels": [], "entities": []}, {"text": "The pretrained English ELMo model is from, and the Chinese one is from (Che et al., 2018), which is hosted at).", "labels": [], "entities": []}, {"text": "The model is trained for maximum 20 epochs for the nearly best model based on development set results.", "labels": [], "entities": []}, {"text": "We re-run our model using different initialized parameters for 4 times and report the average performance 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyper-parameter settings (signal #x means  number of x).", "labels": [], "entities": []}, {"text": " Table 2: Results on CoNLL-2009 English in-domain  (WSJ) test set.", "labels": [], "entities": [{"text": "CoNLL-2009 English in-domain  (WSJ) test set", "start_pos": 21, "end_pos": 65, "type": "DATASET", "confidence": 0.8488841950893402}]}, {"text": " Table 3: Results on CoNLL-2009 English out-of- domain (Brown) test set.", "labels": [], "entities": [{"text": "CoNLL-2009 English out-of- domain (Brown) test set", "start_pos": 21, "end_pos": 71, "type": "DATASET", "confidence": 0.8699672520160675}]}, {"text": " Table 4: Results on CoNLL-2009 Chinese test set.", "labels": [], "entities": [{"text": "CoNLL-2009 Chinese test set", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9366818070411682}]}, {"text": " Table 5: Ablations about distance on CoNLL-2009 En- glish development set. ED means edit distance, WMD  means word moving distance, SD means SIF distance,  RD means random distance.", "labels": [], "entities": [{"text": "CoNLL-2009 En- glish development set", "start_pos": 38, "end_pos": 74, "type": "DATASET", "confidence": 0.918047179778417}, {"text": "ED", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9692286252975464}]}, {"text": " Table 6: Ablations about label merging method on  CoNLL-2009 English development set.", "labels": [], "entities": [{"text": "label merging", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7597256302833557}, {"text": "CoNLL-2009 English development set", "start_pos": 51, "end_pos": 85, "type": "DATASET", "confidence": 0.9685181677341461}]}, {"text": " Table 7: AMN vs. ELMo, the performance compari- son on English development set.", "labels": [], "entities": [{"text": "English development set", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.7679388920466105}]}]}