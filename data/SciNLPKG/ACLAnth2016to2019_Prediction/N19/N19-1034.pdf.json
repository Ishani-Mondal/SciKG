{"title": [{"text": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis", "labels": [], "entities": [{"text": "Multi-modal Emotion Recognition", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6399724980195364}, {"text": "Sentiment Analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.836266815662384}]}], "abstractContent": [{"text": "Related tasks often have interdependence on each other and perform better when solved in a joint framework.", "labels": [], "entities": []}, {"text": "In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both.", "labels": [], "entities": [{"text": "sentiment and emotion analysis", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6871771812438965}]}, {"text": "The multi-modal inputs (i.e., text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making.", "labels": [], "entities": []}, {"text": "We propose a context-level intermodal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance.", "labels": [], "entities": [{"text": "predicting the sentiment and expressed emotions of an utterance", "start_pos": 77, "end_pos": 140, "type": "TASK", "confidence": 0.6989153623580933}]}, {"text": "We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9683858156204224}, {"text": "multi-modal sentiment and emotion analysis", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.6217239618301391}]}, {"text": "Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework.", "labels": [], "entities": []}, {"text": "The proposed approach reports new state-of-the-art performance for both sentiment analysis and emotion analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9666718542575836}, {"text": "emotion analysis", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7211950123310089}]}], "introductionContent": [{"text": "With the rapid growth of social media video platforms such as Youtube, Vimeo, users now tend to upload videos on these platforms.", "labels": [], "entities": [{"text": "Youtube", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9591889381408691}]}, {"text": "Such video platforms offer users an opportunity to express their opinions on any topic.", "labels": [], "entities": []}, {"text": "Videos usually consist of audio and visual modalities, and thus can be considered as a source of multi-modal information.", "labels": [], "entities": []}, {"text": "Although videos contain more information than text, fusing multiple modalities is a major challenge.", "labels": [], "entities": []}, {"text": "A common practice in sentiment analysis and emotion recognition or affective computing, in general, is to analyze textual opinions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9704702496528625}, {"text": "emotion recognition", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.718153789639473}]}, {"text": "However, in recent days multi-modal affect analysis has gained a major attention (.", "labels": [], "entities": [{"text": "multi-modal affect analysis", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6234730581442515}]}, {"text": "In these works, in addition to the visual frames, other sources of information such as acoustic and textual (transcript) representation of the spoken languages are also incorporated in the analysis.", "labels": [], "entities": []}, {"text": "Multi-modal analysis (e.g. sentiment analysis, emotion recognition) is an emerging field of study, that utilizes multiple information sources for solving a problem.", "labels": [], "entities": [{"text": "Multi-modal analysis (e.g. sentiment analysis", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.780309850970904}, {"text": "emotion recognition)", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8049476444721222}]}, {"text": "These sources (e.g., text, visual, acoustic, etc.) offer a diverse and often distinct piece of information that a system can leverage on.", "labels": [], "entities": []}, {"text": "For example, 'text' carries semantic information of the spoken sentence, whereas 'acoustic' information reveals the emphasis (pitch, voice quality) on each word.", "labels": [], "entities": []}, {"text": "In contrast, the 'visual' information (image or video frame) extracts the gesture and posture of the speaker.", "labels": [], "entities": []}, {"text": "Traditionally, 'text' has been the key factor in any Natural Language Processing (NLP) tasks including sentiment and emotion analysis.", "labels": [], "entities": [{"text": "sentiment and emotion analysis", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.8338354676961899}]}, {"text": "However, with the recent emergence of social media platforms and their available multi-modal contents, an interdisciplinary study involving text, acoustic and visual features have drawn significant interest among the research community.", "labels": [], "entities": []}, {"text": "Effectively fusing this diverse information is non-trivial and poses several challenges to the underlying problem.", "labels": [], "entities": []}, {"text": "In our current work, we propose a multi-task model to extract both sentiment (i.e. positive or negative) and emotion (i.e. anger, disgust, fear, happy, sad or surprise) of a speaker in a video.", "labels": [], "entities": []}, {"text": "In multi-task framework, we aim to leverage the inter-dependence of these two tasks to increase the confidence of individual task in prediction.", "labels": [], "entities": []}, {"text": "For e.g., information about anger emotion can help in prediction of negative sentiment and vice-versa.", "labels": [], "entities": [{"text": "prediction of negative sentiment", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8257210850715637}]}, {"text": "A speaker can utter multiple utterances (a unit of speech bounded by breathes or pauses) in a single video and these utterances can have different sentiments and emotions.", "labels": [], "entities": []}, {"text": "We hypothesize that the sentiment (or, emotion) of an utterance often has inter-dependence on other contextual utterances i.e. the knowledge of sentiment (or, emo-tion) for an utterance can assist in classifying its neighbor utterances.", "labels": [], "entities": []}, {"text": "We utilize all three modalities (i.e. text, acoustic and visual) for the analysis.", "labels": [], "entities": []}, {"text": "Although all these sources of information are crucial, they are not equally beneficial for each individual instance.", "labels": [], "entities": []}, {"text": "Few examples are presented in.", "labels": [], "entities": []}, {"text": "In the first example, visual frames provide important clues than textual information for finding the sentiment of a sarcastic sentence \"Thanks for putting me on hold!", "labels": [], "entities": [{"text": "finding the sentiment of a sarcastic sentence", "start_pos": 89, "end_pos": 134, "type": "TASK", "confidence": 0.7252195647784642}]}, {"text": "I've all the time in the world.\".", "labels": [], "entities": []}, {"text": "Similarly, the textual representation of second example \"I'm fine.'", "labels": [], "entities": []}, {"text": "does not reveal the exact emotion of a sad person.", "labels": [], "entities": []}, {"text": "For this particular case, acoustic or visual information such as low tone voice, facial expression etc.", "labels": [], "entities": []}, {"text": "have bigger role to play for the classification.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets used for our experiments and report the results along with necessary analysis.", "labels": [], "entities": []}, {"text": "We evaluate our proposed approach on the benchmark datasets of sentiment and emotion analysis, namely CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset ( Each utterance has six emotion values associated with it, representing the degree of emotion for anger, disgust, fear, happy, sad and surprise.", "labels": [], "entities": [{"text": "sentiment and emotion analysis", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.6843041852116585}, {"text": "CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset", "start_pos": 102, "end_pos": 177, "type": "DATASET", "confidence": 0.540046518499201}]}, {"text": "Emotion labels for an utterance are identified as all non-zero intensity values, i.e. if an utterance has three emotions with non-zero values, we take all three emotions as multi-labels.", "labels": [], "entities": []}, {"text": "Further, an utterance that has no emotion label represents the absence of emotion.", "labels": [], "entities": []}, {"text": "For experiments, we adopt 7-classes (6 emotions + 1 no emotion) and pose it as multi-label classification problem, where we try to   optimize the binary-cross entropy for each of the class.", "labels": [], "entities": []}, {"text": "A brief statistics for multi-label emotions is presented in.", "labels": [], "entities": []}, {"text": "In contrast, the sentiment values for each utterance are disjoint, i.e. value < 0 and value \u2265 0 represent the negative and positive sentiments, respectively.", "labels": [], "entities": []}, {"text": "A detailed statistics of the CMU-MOSEI dataset is shown in: Single-task learning (STL) and Multi-task (MTL) learning frameworks for the proposed approach.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.942467600107193}]}, {"text": "T: Text, V: Visual, A: Acoustic.", "labels": [], "entities": []}, {"text": "Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also inline with the other existing works (Zadeh et al., 2018c).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9830194115638733}]}, {"text": "We evaluate our proposed approach on the datasets of CMU-MOSEI.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9216105341911316}]}, {"text": "We use the Python based Keras library for the implementation.", "labels": [], "entities": []}, {"text": "We compute F1-score and accuracy values for sentiment classification and F1-score and weighted accuracy (Tong et al., 2017) for emotion classification.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.99615877866745}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9712215065956116}, {"text": "sentiment classification", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.9332108199596405}, {"text": "F1-score", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9981174468994141}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.6547636389732361}, {"text": "emotion classification", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7619104087352753}]}, {"text": "Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also inline with the other existing works ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.985697865486145}]}, {"text": "To obtain multi-labels for emotion classification, we use 7 sigmoid neurons (corresponds to 6 emotions + 1 no-emotion) with binary crossentropy loss function.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7227926254272461}]}, {"text": "Finally, we take all the emotions whose respective values are above a threshold.", "labels": [], "entities": []}, {"text": "We optimize and cross-validate both the evaluation metrics (i.e. F1-score and weighted accuracy) and set the threshold as 0.4 & 0.2 for F1-score and weighted accuracy, respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9955040812492371}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.7902965545654297}, {"text": "F1-score", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9924494028091431}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.8888691067695618}]}, {"text": "We show our model configurations in.", "labels": [], "entities": []}, {"text": "As stated earlier, our proposed approach requires at least two modalities to compute bimodal attention.", "labels": [], "entities": []}, {"text": "Hence, we experiment with bimodal and tri-modal input combinations for the proposed approach i.e. taking text-visual, textacoustic, acoustic-visual and text-visual-acoustic at a time.", "labels": [], "entities": []}, {"text": "For completeness (i.e., uni-modal in-  puts), we also experiment with a variant of the proposed approach where we apply self-attention on the utterances of each modality separately.", "labels": [], "entities": []}, {"text": "The self-attention unit utilizes the contextual information of the utterances (i.e., it receives u\u00d7d hidden representations), applies attention and forward it to the output layer for classification.", "labels": [], "entities": []}, {"text": "We report the experimental results of both single-task (STL) and multi-task (MTL) learning framework in Table 4.", "labels": [], "entities": []}, {"text": "In the single-task framework, we build separate systems for sentiment and emotion analysis, whereas in multi-task framework a joint-model is learned for both of these problems.", "labels": [], "entities": [{"text": "sentiment and emotion analysis", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.7721188962459564}]}, {"text": "For sentiment classification, our single-task framework reports an F1-score of 77.67% and accuracy value of 79.8% for the tri-modal inputs.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9791279435157776}, {"text": "F1-score", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995911717414856}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994485974311829}]}, {"text": "Similarly, we obtain 77.71% F1-score and 60.88% weighted accuracy for emotion classification.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995461106300354}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9773176908493042}, {"text": "emotion classification", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7917969524860382}]}, {"text": "Comparatively, when both the problems are learned and evaluated in a multi-task learning framework, we observe performance enhancement for both sentiments as well as emotion classification.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.7137705385684967}]}, {"text": "MTL reports 78.86% F1-score and 80.47% accuracy value in comparison to 77.67% and 79.8% of STL with tri-modal inputs, respectively.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9329050183296204}, {"text": "F1-score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996476173400879}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.998478353023529}]}, {"text": "For emotion classification, we also observe an improved F-score (78.6 (MTL) vs. 77.7 (STL)) and weighted accuracy (62.8 (MTL) vs. 60.8 (STL)): Single-task learning (STL) and Multi-task (MTL) learning frameworks for the proposed approach.: Example video for heatmap analysis of the contextual inter-modal (CIM) attention mechanism of the proposed MTMM-ES framework.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.810590386390686}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9971691966056824}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9185105562210083}]}, {"text": "depicts the heatmaps for the above video.", "labels": [], "entities": []}, {"text": "It is evident from that multi-task learning framework successfully leverages the inter-dependence of both the tasks in improving the overall performance in comparison to single-task learning.", "labels": [], "entities": []}, {"text": "The improvements of MTL over STL framework is also statistically significant with p-value < 0.05 (c.f.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Dataset statistics for CMU-MOSEI. Each ut- terance contains multi-modal information.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8948675394058228}]}, {"text": " Table 3: Statistics of multi-label emotions.", "labels": [], "entities": []}, {"text": " Table 4: Single-task learning (STL) and Multi-task (MTL) learning frameworks for the proposed approach. T:  Text, V: Visual, A: Acoustic. Weighted accuracy as a metric is chosen due to unbalanced samples across various  emotions and it is also in line with the other existing works (Zadeh et al., 2018c).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9314475059509277}]}, {"text": " Table 5.  As stated earlier, our proposed approach re- quires at least two modalities to compute bi- modal attention. Hence, we experiment with bi- modal and tri-modal input combinations for the  proposed approach i.e. taking text-visual, text- acoustic, acoustic-visual and text-visual-acoustic  at a time. For completeness (i.e., uni-modal in-", "labels": [], "entities": []}, {"text": " Table 6: Example video for heatmap analysis of the contextual inter-modal (CIM) attention mechanism of the  proposed MTMM-ES framework.", "labels": [], "entities": []}]}