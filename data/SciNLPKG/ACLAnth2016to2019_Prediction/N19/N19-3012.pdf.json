{"title": [{"text": "Multimodal Machine Translation with Embedding Prediction", "labels": [], "entities": [{"text": "Multimodal Machine Translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.687729279200236}]}], "abstractContent": [{"text": "Multimodal machine translation is an attractive application of neural machine translation (NMT).", "labels": [], "entities": [{"text": "Multimodal machine translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6848776837189993}, {"text": "neural machine translation (NMT)", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.827310303846995}]}, {"text": "It helps computers to deeply understand visual objects and their relations with natural languages.", "labels": [], "entities": []}, {"text": "However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words.", "labels": [], "entities": [{"text": "translating rare words", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.8745922446250916}]}, {"text": "In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem.", "labels": [], "entities": []}, {"text": "In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pre-trained word embeddings to better translate rare words.", "labels": [], "entities": []}, {"text": "We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 F-score for rare word translation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9947152733802795}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9984310269355774}, {"text": "F-score", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.998701810836792}, {"text": "rare word translation", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7123974561691284}]}], "introductionContent": [{"text": "In multimodal machine translation, a target sentence is translated from a source sentence together with related nonlinguistic information such as visual information.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.635737806558609}]}, {"text": "Recently, neural machine translation (NMT) has superseded traditional statistical machine translation owing to the introduction of the attentional encoder-decoder model, in which machine translation is treated as a sequence-tosequence learning problem and is trained to pay attention to the source sentence while decoding (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8302293916543325}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7222901781400045}, {"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.729744553565979}]}, {"text": "Most previous studies on multimodal machine translation are classified into two categories: visual feature adaptation and data augmentation.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6323643823464712}, {"text": "visual feature adaptation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.6406051317850748}, {"text": "data augmentation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7431924939155579}]}, {"text": "In visual feature adaptation, multitask learning and feature integration architecture) are proposed to improve neural network models.", "labels": [], "entities": [{"text": "visual feature adaptation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6735563774903616}]}, {"text": "Data augmentation aims to deal with the fact that the size of available datasets for multimodal translation is quite small.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7393425107002258}, {"text": "multimodal translation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.695260763168335}]}, {"text": "To alleviate this problem, parallel corpora without a visual source and pseudo-parallel corpora obtained using back-translation () are used as additional learning resources.", "labels": [], "entities": []}, {"text": "Due to the availability of parallel corpora for NMT, suggested that initializing the encoder with pretrained word embedding improves the translation performance in low-resource language pairs.", "labels": [], "entities": []}, {"text": "Recently, proposed an NMT model that predicts the embedding of output words and searches for the output word instead of calculating the probability using the softmax function.", "labels": [], "entities": []}, {"text": "This model performed as well as conventional NMT, and it significantly improved the translation accuracy for rare words.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9482805132865906}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9248951077461243}]}, {"text": "In this study, we introduce an NMT model with embedding prediction for multimodal machine translation that fully uses pretrained embeddings to improve the translation accuracy for rare words.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.6302377581596375}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9170625805854797}]}, {"text": "The main contributions of this study are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose a novel multimodal machine translation model with embedding prediction and explore various settings to take full advantage of word embeddings.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6381396253903707}]}, {"text": "2. We show that pretrained word embeddings improve the model performance, especially when translating rare words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train, validate, and test our model with the) dataset published in the WMT17 Shared Task.", "labels": [], "entities": [{"text": "WMT17 Shared Task", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.8776924808820089}]}, {"text": "We choose French as the source language and English as the target one.", "labels": [], "entities": []}, {"text": "The vocabulary size of both the source and the target languages is 10,000.", "labels": [], "entities": []}, {"text": "Following, byte pair encoding () is not applied.", "labels": [], "entities": []}, {"text": "The source and target sentences are preprocessed with lower-casing, tokenizing and normalizing the punctuation.", "labels": [], "entities": []}, {"text": "Visual features are extracted using pretrained ResNet (.", "labels": [], "entities": []}, {"text": "Specifically, we encode all images in Multi30k with ResNet-50 and pick out the hidden state in the pool5 layer as a 2,048-dimension visual feature.", "labels": [], "entities": []}, {"text": "We calculate the centroid of visual features in the training dataset as the bias vector and subtract the bias vector from all visual features in the training, validation and test datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Multi30k validation and test  dataset. NMT denotes the text-only conventional  NMT model (Bahdanau et al., 2015) and IMAG+ de- notes our reimplementation of the IMAGINATION  (Elliott and K\u00e1d\u00e1r, 2017) model. \"+ pretrained\" mod- els are initialized with pretrained embeddings.", "labels": [], "entities": []}, {"text": " Table 2: Results on test dataset with variations of  model initialization and fine-tuning in decoder.", "labels": [], "entities": []}, {"text": " Table 3: Ablation experiments of visual features. \"\u2212  Debias\" denotes the result without subtracting the bias  vector. \"\u2212 Images\" shows the result of text-only NMT  with embedding prediction.", "labels": [], "entities": []}]}