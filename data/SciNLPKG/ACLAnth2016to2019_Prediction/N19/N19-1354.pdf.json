{"title": [{"text": "Bayesian Learning for Neural Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.827820897102356}]}], "abstractContent": [{"text": "While neural dependency parsers provide state-of-the-art accuracy for several languages, they still rely on large amounts of costly labeled training data.", "labels": [], "entities": [{"text": "neural dependency parsers", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.6431946059068044}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9974464178085327}]}, {"text": "We demonstrate that in the small data regime, where uncertainty around parameter estimation and model prediction matters the most, Bayesian neural modeling is very effective.", "labels": [], "entities": [{"text": "model prediction", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7325797230005264}, {"text": "Bayesian neural modeling", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.6461539467175802}]}, {"text": "In order to overcome the computational and statistical costs of the approximate inference step in this framework, we utilize an efficient sampling procedure via stochastic gradient Langevin dynamics to generate samples from the approximated posterior.", "labels": [], "entities": []}, {"text": "Moreover , we show that our Bayesian neural parser can be further improved when integrated into a multi-task parsing and POS tagging framework , designed to minimize task interference via an adversarial procedure.", "labels": [], "entities": [{"text": "Bayesian neural parser", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.569869707028071}, {"text": "multi-task parsing", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7121105790138245}, {"text": "POS tagging", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.7102570980787277}]}, {"text": "When trained and tested on 6 languages with less than 5 training instances, our parser consistently out-performs the strong BiLSTM baseline (Kiper-wasser and Goldberg, 2016).", "labels": [], "entities": []}, {"text": "Compared with the BiAFFINE parser (Dozat et al., 2017) our model achieves an improvement of up to 3% for Vietnamese and Irish, while our multi-task model achieves an improvement of up to 9% across five languages: Farsi, Russian, Turkish, Vietnamese, and Irish.", "labels": [], "entities": [{"text": "BiAFFINE", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6457580327987671}]}], "introductionContent": [{"text": "Dependency parsing is essential for many Natural Language Processing (NLP) tasks (.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.866412341594696}]}, {"text": "While earlier work on dependency parsing required careful feature engineering, this has become less of a concern in recent years with the emergence of deep neural networks.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8723264038562775}]}, {"text": "Nonetheless, an accurate parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization.", "labels": [], "entities": []}, {"text": "Several approaches for parsing in the small data regime have been proposed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9866956472396851}]}, {"text": "These include augmenting input data with pretrained embedding (, leveraging unannotated data via semi-supervised learning (, predicting based on a pool of high probability trees (, and transferring annotation or model across languages ().", "labels": [], "entities": []}, {"text": "Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and attest time as the model ignores the uncertainty around the estimated parameters.", "labels": [], "entities": [{"text": "maximum likelihood parameter estimation (MLE", "start_pos": 88, "end_pos": 132, "type": "METRIC", "confidence": 0.7513506313165029}, {"text": "attest time", "start_pos": 266, "end_pos": 277, "type": "METRIC", "confidence": 0.9478680789470673}]}, {"text": "When training data is small these challenges are more pronounced.", "labels": [], "entities": []}, {"text": "The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absence of sufficient data, and (ii) predicting under the full posterior distribution of model parameters which offers the desired degree of uncertainty by exploring the posterior space during inference.", "labels": [], "entities": [{"text": "predicting", "start_pos": 186, "end_pos": 196, "type": "TASK", "confidence": 0.9662501811981201}]}, {"text": "However, this solution comes with a high computational cost, specifically in DNNs, and is often replaced by regularization techniques such as dropout () as well as ensemble learning and prediction averaging ( . Bayesian neural networks (BNNs) have attracted some attention ().", "labels": [], "entities": [{"text": "prediction averaging", "start_pos": 186, "end_pos": 206, "type": "TASK", "confidence": 0.949583113193512}]}, {"text": "Yet, its current application to NLP is limited to language modeling (, and BNNs have not been developed for structured prediction tasks such as dependency parsing.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7117788642644882}, {"text": "dependency parsing", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7965522408485413}]}, {"text": "In this paper we aim to close this gap and propose the first BNN for dependency parsing (BNNP).", "labels": [], "entities": [{"text": "dependency parsing (BNNP)", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7846824765205384}]}, {"text": "To address the costs of inference step, we apply an efficient sampling procedure via stochastic gradient Langevin dynamics (SGLD) (.", "labels": [], "entities": []}, {"text": "At training, samples from the posterior distribution of the parser parameters are generated via controlled noise injection to the maximum a posteriori (MAP) gradient update.", "labels": [], "entities": [{"text": "maximum a posteriori (MAP) gradient update", "start_pos": 130, "end_pos": 172, "type": "METRIC", "confidence": 0.7828516736626625}]}, {"text": "The generated samples are then used during the inference step to create multiple viable parses based on which the final dependency parse is generated.", "labels": [], "entities": []}, {"text": "Another means of directing a model towards more accurate predictions in the small data regime is via multi-task learning.", "labels": [], "entities": []}, {"text": "Such a framework allows models for multiple tasks to reinforce each other towards more accurate joint solutions, which is particularly useful when training data is scarce.", "labels": [], "entities": []}, {"text": "We hence present a multi-task framework where our BNNP is integrated with a POS tagger through an adversarial procedure designed to guide the two models towards improved joint solutions.", "labels": [], "entities": []}, {"text": "Our experiments with monolingual and delexicalized cross-lingual parsing using the Universal Dependency treebank () demonstrate the effectiveness of our approach.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.667895033955574}, {"text": "Universal Dependency treebank", "start_pos": 83, "end_pos": 112, "type": "DATASET", "confidence": 0.5692850550015768}]}, {"text": "Particularly, our BNNP consistently outperforms the single task BiLSTM baseline, while outperforming the BiAFFINE parser () by up to 3% on Vietnamese and Irish.", "labels": [], "entities": [{"text": "BNNP", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.5613343119621277}, {"text": "BiLSTM baseline", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.7658691108226776}]}, {"text": "Additionally, our multi-task model achieves an improvement of up to 9% over the BiAFFINE parser for five low-resource languages: Farsi, Russian, Turkish, Vietnamese, and Irish.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with mono-lingual and cross-lingual dependency parsing using the treebanks of the CoNLL 2017 shared task on parsing to Universal Dependencies (UD) ().", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.608543187379837}, {"text": "CoNLL 2017 shared task", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.8833277076482773}, {"text": "parsing to Universal Dependencies (UD)", "start_pos": 122, "end_pos": 160, "type": "TASK", "confidence": 0.7259037324360439}]}, {"text": "We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data).", "labels": [], "entities": [{"text": "UDPipe baseline outputs", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.9350052277247111}, {"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9687906503677368}, {"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8374687433242798}]}, {"text": "While segmentation and POS errors substantially impact the quality of the final predicted parse, their exploration is beyond our scope.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9744404554367065}]}, {"text": "Our evaluation metric is Labeled Attachment Score (LAS), computed by the shared task evaluation script.", "labels": [], "entities": [{"text": "Labeled Attachment Score (LAS)", "start_pos": 25, "end_pos": 55, "type": "METRIC", "confidence": 0.8170421620210012}]}, {"text": "Statistical significance, when mentioned, is computed over 20 runs, via the Kolmogorov-Smirnov test) with = 0.01.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.6103979349136353}]}, {"text": "We experiment with Persian (fa), Korean (ko), Russian (ru), Turkish (tr), Vietnamese (vi) and Irish (ga), all with less than 5 training sentences).", "labels": [], "entities": []}, {"text": "For comparison we report the scores published by the top system of the CoNLL 2017 shared task,, noting the following differences between their input and output and ours.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.8784387111663818}]}, {"text": "The BiAFFINE parser: (i) uses the UDPipe outputs for segmentation but corrects POS errors before parsing, (ii) includes both language specific and universal POS tags in the input layer while we only include the universal POS tags, and (iii) applies post-process correction for non-projective languages.", "labels": [], "entities": []}, {"text": "Cross-Lingual Experiments We use the English (en), French (fr), Russian (ru), and Persian (fa) datasets of the UD treebanks as our training and test data, with the addition of 3 languages for which we did not have any training data: Kurmanji (kmr), Buriat (bxr), and Northern Sami (sme).", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9268640875816345}, {"text": "Buriat", "start_pos": 249, "end_pos": 255, "type": "METRIC", "confidence": 0.9355013370513916}]}, {"text": "We also report the results for each language, where the combination of training datasets for the rest of the languages (marked as +) was used for training.", "labels": [], "entities": []}, {"text": "The cross-lingual experiments are done on delexicalized For train and dev sets , test set (1-2184), and pretrained embeddings ) see: https: //lindat.mff.cuni.cz/repository/xmlui/ handle/11234/ parses after replacing the words with their Universal POS tags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mono-Lingual results. Models that outper- form the BiAFFINE parser are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 2: Cross-Lingual Parsing Evaluation. LAS of an  ensemble of = 9 SINGLE TASK (BASE++) models,  and the performance gain by the Bayesian MULTI TASK  (+PRECOND), placed as subscript.", "labels": [], "entities": [{"text": "Cross-Lingual Parsing Evaluation", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8128728270530701}, {"text": "LAS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9974122643470764}, {"text": "SINGLE TASK (BASE", "start_pos": 71, "end_pos": 88, "type": "METRIC", "confidence": 0.6983371526002884}, {"text": "MULTI TASK  (+PRECOND)", "start_pos": 142, "end_pos": 164, "type": "METRIC", "confidence": 0.8465507388114929}]}]}