{"title": [{"text": "Abusive Language Detection with Graph Convolutional Networks", "labels": [], "entities": [{"text": "Abusive Language Detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8528786301612854}]}], "abstractContent": [{"text": "Abuse on the Internet represents a significant societal problem of our time.", "labels": [], "entities": []}, {"text": "Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task.", "labels": [], "entities": [{"text": "automated abusive language detection", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.6766101270914078}]}, {"text": "However, existing approaches only capture shallow properties of online communities by modeling follower-following relationships.", "labels": [], "entities": []}, {"text": "In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them.", "labels": [], "entities": []}, {"text": "We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 130, "end_pos": 156, "type": "TASK", "confidence": 0.7507139643033346}]}], "introductionContent": [{"text": "Matthew Zook (2012) carried out an interesting study showing that the racist tweets posted in response to President Obama's re-election were not distributed uniformly across the United States but instead formed clusters.", "labels": [], "entities": []}, {"text": "This phenomenon is known as homophily: i.e., people, both in real life and online, tend to cluster with those who appear similar to themselves.", "labels": [], "entities": []}, {"text": "To model homophily, recent research in abusive language detection on Twitter () incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6894758343696594}]}, {"text": "The embeddings (called author profiles) are generated by applying anode embedding framework to an undirected unlabeled community graph where nodes denote the authors and edges the follower-following relationships amongst them on Twitter.", "labels": [], "entities": []}, {"text": "However, these profiles do not capture the linguistic behavior of the authors and their communities and do not convey whether their tweets tend to be abusive or not.", "labels": [], "entities": []}, {"text": "In contrast, we represent the community of authors as a heterogeneous graph consisting of two types of nodes, authors and their tweets, rather than a homogeneous community graph of authors only.", "labels": [], "entities": []}, {"text": "The primary advantage of such heterogeneous representations is that they enable us to model both community structure as well as the linguistic behavior of authors in these communities.", "labels": [], "entities": []}, {"text": "To generate richer author profiles, we then propose a semi-supervised learning approach based on graph convolutional networks (GCNs) applied to the heterogeneous graph representation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our work is the first to use GCNs to model online communities in social media.", "labels": [], "entities": []}, {"text": "We demonstrate that our methods provide significant improvements over existing techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work), we experiment with a subset of the Twitter dataset compiled by.", "labels": [], "entities": [{"text": "Twitter dataset compiled", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8932443062464396}]}, {"text": "Waseem and Hovy released a list of 16, 907 tweet IDs along with their corresponding annotations, 1 labeling each tweet as racist, sexist or neither (clean).", "labels": [], "entities": []}, {"text": "Recently, could only retrieve 16, 202 of these tweets since some of them are no longer available.", "labels": [], "entities": []}, {"text": "This is the dataset we use in our experiments.", "labels": [], "entities": []}, {"text": "1, 939 (12%) of 16, 202 tweets are racist, 3, 148 (19.4%) are sexist, and the remaining 11, 115 (68.6%) are clean.", "labels": [], "entities": []}, {"text": "The tweets have been authored by a total of 1, 875 unique users.", "labels": [], "entities": []}, {"text": "Tweets in the racist class come from 5 of the users, while those in the sexist class come from 527 of them.", "labels": [], "entities": []}, {"text": "We run every method 10 times with random initializations and stratified train-test splits.", "labels": [], "entities": []}, {"text": "Specifically, in each run, the dataset is split into a randomly-sampled train set (90%) and test set (10%) with identical distributions of the 3 classes in each.", "labels": [], "entities": []}, {"text": "In methods involving our GCN, a small part of the train set is held out as validation data to prevent over-fitting using early-stopping regularization.", "labels": [], "entities": [{"text": "GCN", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.835379421710968}]}, {"text": "When training the: The baselines (LR, LR + AUTH/EXTD) vs. our GCN approaches ( \u2020 ) on the racism and sexism classes.", "labels": [], "entities": [{"text": "AUTH/EXTD)", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.7799456715583801}]}, {"text": "Overall shows the macro-averaged metrics computed over the 3 classes: sexism, racism, and clean.", "labels": [], "entities": []}, {"text": "labeled tweet nodes for those tweets in the extended graph that are part of the train set.", "labels": [], "entities": []}, {"text": "Our GCN is trained using the parameters from the original paper: Glorot initialization (, ADAM optimizer () with a learning rate of 0.01, dropout regularization () rate of 0.5, 200 training epochs with an early-stopping patience of 10 epochs.", "labels": [], "entities": [{"text": "Glorot", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9816111326217651}]}], "tableCaptions": [{"text": " Table 1: The baselines (LR, LR + AUTH/EXTD) vs. our GCN approaches (  \u2020 ) on the racism and sexism classes.  Overall shows the macro-averaged metrics computed over the 3 classes: sexism, racism, and clean.", "labels": [], "entities": [{"text": "AUTH", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9051749110221863}, {"text": "EXTD", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.7690655589103699}]}]}