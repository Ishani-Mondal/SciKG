{"title": [{"text": "Continuous Learning for Large-scale Personalized Domain Classification", "labels": [], "entities": [{"text": "Personalized Domain Classification", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.714655856291453}]}], "abstractContent": [{"text": "Domain classification is the task of mapping spoken language utterances to one of the natural language understanding domains in intelligent personal digital assistants (IPDAs).", "labels": [], "entities": [{"text": "Domain classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7771512866020203}]}, {"text": "This is a major component in mainstream IPDAs in industry.", "labels": [], "entities": []}, {"text": "Apart from official domains, thousands of third-party domains are also created by external developers to enhance the capability of IPDAs.", "labels": [], "entities": []}, {"text": "As more domains are developed rapidly, the question of how to continuously accommodate the new domains still remains challenging.", "labels": [], "entities": []}, {"text": "Moreover, existing continual learning approaches do not address the problem of incorporating personalized information dynamically for better domain classification.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7399304807186127}]}, {"text": "In this paper, we propose CONDA, a neural network based approach for domain classification that supports incremental learning of new classes.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.720203623175621}]}, {"text": "Empirical evaluation shows that CONDA achieves high accuracy and outperforms baselines by a large margin on both incrementally added new domains and existing domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9991598129272461}]}], "introductionContent": [{"text": "Domain classification is the task of mapping spoken language utterances to one of the natural language understanding (NLU) domains in intelligent personal digital assistants (IPDAs), such as Amazon Alexa, Google Assistant, and Microsoft Cortana, etc..", "labels": [], "entities": [{"text": "Domain classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7525407671928406}]}, {"text": "Here a domain is defined in terms of a specific application or functionality such as weather, calendar or music, which narrows down the scope of NLU.", "labels": [], "entities": []}, {"text": "For example, given an utterance \"Ask Uber to get me a ride\" from a user, the appropriate domain would be one that invokes the \"Uber\" app.", "labels": [], "entities": []}, {"text": "Traditionally IPDAs have only supported dozens of well-separated domains, where each is defined in terms of a specific application or functionality such as calendar and weather).", "labels": [], "entities": []}, {"text": "In order to increase the domain coverage and extend the capabilities of the IPDAs, mainstream IPDAs released tools to allow thirdparty developers to build new domains.", "labels": [], "entities": []}, {"text": "Amazons Alexa Skills Kit, Googles Actions and Microsofts Cortana Skills Kit are examples of such tools.", "labels": [], "entities": [{"text": "Amazons Alexa Skills Kit", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8603539168834686}, {"text": "Microsofts Cortana Skills Kit", "start_pos": 46, "end_pos": 75, "type": "DATASET", "confidence": 0.8269729614257812}]}, {"text": "To handle the influx of new domains, largescale domain classification methods like SHORT-LISTER () have been proposed and have achieved good performance.", "labels": [], "entities": [{"text": "largescale domain classification", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.6554539203643799}]}, {"text": "As more new domains are developed rapidly, one of the major challenges in large-scale domain classification is how to quickly accommodate the new domains without losing the learned prediction power on the known ones.", "labels": [], "entities": [{"text": "large-scale domain classification", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.7126566569010416}]}, {"text": "A straightforward solution is to simply retraining the whole model whenever new domains are available.", "labels": [], "entities": []}, {"text": "However, this is not desirable since retraining is often time consuming.", "labels": [], "entities": []}, {"text": "Another approach is to utilize continual learning where we dynamically evolve the model whenever anew domain is available.", "labels": [], "entities": []}, {"text": "There is extensive work on the topic of continual learning, however there is very little on incrementally adding new domains to a domain classification system.", "labels": [], "entities": []}, {"text": "To mitigate this gap, in this paper we propose the CONDA solution for continuous domain adaptation.", "labels": [], "entities": [{"text": "continuous domain adaptation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6359481712182363}]}, {"text": "Given anew domain, we keep all learned parameters, but only add and update new parameters for the new domain.", "labels": [], "entities": []}, {"text": "This enables much faster model updates and faster deployment of new features to customers.", "labels": [], "entities": []}, {"text": "To preserve the learned knowledge on existing domains to avoid the notorious catastrophic forgetting problem (, we propose cosine normalization for output prediction and domain embedding regularization for regularizing the new domain embedding.", "labels": [], "entities": []}, {"text": "Also, we summarize the data for existing domains by sampling exemplars, which will be used together with the new domain data for continuous domain adaptation.", "labels": [], "entities": [{"text": "continuous domain adaptation", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6526277561982473}]}, {"text": "This is shown to further alleviate the overfitting on the new domain data.", "labels": [], "entities": []}, {"text": "Empirical evaluation on real data with 900 domains for initial training and 100 for continuous adaptation shows that CONDA out performs the baselines by a large margin, achieving 95.6% prediction accuracy on average for the 100 new domains and 88.2% accuracy for all seen domains after 100 new domains have been accommodated (only 3.6% lower than the upperbound by retraining the model using all domain data).", "labels": [], "entities": [{"text": "prediction", "start_pos": 185, "end_pos": 195, "type": "METRIC", "confidence": 0.905941903591156}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.6229439973831177}, {"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9972317814826965}]}, {"text": "To summarize, we make the following contributions in this paper: \u2022 We introduce the problem of continuous domain adaptation for large-scale personalized domain classification.", "labels": [], "entities": [{"text": "continuous domain adaptation", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6982717315355936}, {"text": "personalized domain classification", "start_pos": 140, "end_pos": 174, "type": "TASK", "confidence": 0.6141710182030996}]}, {"text": "\u2022 We describe CONDA, anew solution for continuous domain adaptation with Cosine normalization, domain embedding regularization and negative exemplar sampling techniques.", "labels": [], "entities": [{"text": "CONDA", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.6177791357040405}, {"text": "continuous domain adaptation", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7214326659838358}]}, {"text": "Our solution advances the research in continuous domain adaptation.", "labels": [], "entities": [{"text": "continuous domain adaptation", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6349983215332031}]}, {"text": "\u2022 We conduct extensive experiments showing that CONDA achieves good accuracy on both new and existing domains, and outperforms the baselines by a large margin.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9989913105964661}]}], "datasetContent": [{"text": "Dataset: We use a dataset defined on 1000 domains for our experiments which has 2.53M utterances, and we split them into two parts.", "labels": [], "entities": []}, {"text": "The   part contains 900 domains where we use it for the initial training of the model.", "labels": [], "entities": []}, {"text": "It has 2.06M utterances, and we split into training, development and test sets with ratio of 8:1:1.", "labels": [], "entities": []}, {"text": "We refer to this dataset as \"InitTrain\".", "labels": [], "entities": []}, {"text": "The second part consists of 100 domains and is used for the online domain adaptation.", "labels": [], "entities": [{"text": "online domain adaptation", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.633339395125707}]}, {"text": "It has 478K utterances and we split into training, development and test sets with the same 8:1:1 ratio.", "labels": [], "entities": []}, {"text": "We refer to this dataset as \"IncTrain\".", "labels": [], "entities": [{"text": "IncTrain", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9792895317077637}]}, {"text": "Training Setup: We implement the model in PyTorch ().", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.8770869970321655}]}, {"text": "All of the experiments are conducted on an Amazon AWS p3.16xlarge 1 cluster with 8 Tesla V100 GPUs.", "labels": [], "entities": [{"text": "Amazon AWS p3.16xlarge 1 cluster", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9301302194595337}]}, {"text": "For initial training, we train the model for 20 epochs with learning rate 0.001, batch size 512.", "labels": [], "entities": [{"text": "learning rate 0.001", "start_pos": 60, "end_pos": 79, "type": "METRIC", "confidence": 0.9564352234204611}]}, {"text": "For the continuous domain adaptation, we add the new domains in a random order.", "labels": [], "entities": [{"text": "continuous domain adaptation", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.7152005632718405}]}, {"text": "Each domain data will be trained independently one-by-one for 10 epochs, with learning rate 0.01 and batch size 128.", "labels": [], "entities": [{"text": "learning rate 0.01", "start_pos": 78, "end_pos": 96, "type": "METRIC", "confidence": 0.9490249951680502}]}, {"text": "For both training procedures, we use Adam as the optimizer.", "labels": [], "entities": []}, {"text": "The development data is used to pick the best model in different epoch runs.", "labels": [], "entities": []}, {"text": "We evaluate the classification accuracy on the test set.", "labels": [], "entities": [{"text": "classification", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.928349494934082}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9427819848060608}]}], "tableCaptions": [{"text": " Table 1: Linear dot product versus Cosine normaliza- tion on initial training for different number of domains.", "labels": [], "entities": []}]}