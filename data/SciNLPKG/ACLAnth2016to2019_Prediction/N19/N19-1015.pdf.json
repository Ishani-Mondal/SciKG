{"title": [{"text": "Topic-Guided Variational Autoencoders for Text Generation", "labels": [], "entities": [{"text": "Text Generation", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7727684080600739}]}], "abstractContent": [{"text": "We propose a topic-guided variational au-toencoder (TGVAE) model for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.8233040571212769}]}, {"text": "Distinct from existing variational au-toencoder (VAE) based approaches, which assume a simple Gaussian prior for the latent code, our model specifies the prior as a Gaus-sian mixture model (GMM) parametrized by a neural topic module.", "labels": [], "entities": []}, {"text": "Each mixture component corresponds to a latent topic, which provides guidance to generate sentences under the topic.", "labels": [], "entities": []}, {"text": "The neural topic module and the VAE-based neural sequence module in our model are learned jointly.", "labels": [], "entities": [{"text": "VAE-based", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.6762479543685913}]}, {"text": "In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during model inference.", "labels": [], "entities": []}, {"text": "Experimental results show that our TGVAE outperforms alternative approaches on both unconditional and conditional text generation, which can generate semantically-meaningful sentences with various topics.", "labels": [], "entities": [{"text": "conditional text generation", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.8045760989189148}]}], "introductionContent": [{"text": "Text generation plays an important role in various natural language processing (NLP) applications, such as machine translation (), dialogue generation ( , and text summarization (.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.771014541387558}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8044824004173279}, {"text": "dialogue generation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8945674300193787}, {"text": "text summarization", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7675976455211639}]}, {"text": "As a competitive solution to this task, the variational autoencoder (VAE) () has been widely used in text-generation systems ().", "labels": [], "entities": [{"text": "variational autoencoder (VAE)", "start_pos": 44, "end_pos": 73, "type": "METRIC", "confidence": 0.6807934165000915}]}, {"text": "In particular, VAE defines a generative model that propagates latent codes drawn from a simple prior through a decoder to manifest data samples.", "labels": [], "entities": [{"text": "VAE", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7623078227043152}]}, {"text": "The generative model is further augmented with an inference network, that feeds observed data samples through an encoder to yield a distribution on the corresponding latent codes.", "labels": [], "entities": []}, {"text": "Compared with other potential methods, e.g., those based on generative adversarial networks (GANs) (, VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful latent codes from text.", "labels": [], "entities": [{"text": "VAE", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.5644492506980896}, {"text": "text generation", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.7426320910453796}]}, {"text": "Ideally, semanticallymeaningful latent codes can provide high-level guidance while generating sentences.", "labels": [], "entities": []}, {"text": "For example, when generating text, the vocabulary could potentially be narrowed down if the input latent code corresponds to a certain topic (e.g., the word \"military\" is unlikely to appear in a sports-related document).", "labels": [], "entities": []}, {"text": "However, in practice this desirable property is not fully achieved by existing VAE-based text generative models, because of the following two challenges.", "labels": [], "entities": [{"text": "VAE-based text generative", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.6409358580907186}]}, {"text": "First, the sentences in documents may associate with different semantic information (e.g., topic, sentiment, etc.) while the latent codes of existing VAE-based text generative models often employ a simple Gaussian prior, which cannot indicate the semantic structure among sentences and may reduce the generative power of the decoder.", "labels": [], "entities": [{"text": "VAE-based text generative", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.6714136799176534}]}, {"text": "Although some variants of VAE try to impose some structure on the latent codes (, they are often designed with pre-defined parameter settings without incorporating semantic meanings into the latent codes, which may lead to over-regularization (.", "labels": [], "entities": []}, {"text": "The second issue associated with VAE-based text generation is \"posterior collapse,\" first identified in.", "labels": [], "entities": [{"text": "VAE-based text generation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8819942673047384}]}, {"text": "With a strong auto-regressive decoder network (e.g., LSTM), the model tends to ignore the information from the latent code and merely depends on previous generated tokens for prediction.", "labels": [], "entities": []}, {"text": "Several strategies are proposed to mitigate this problem, including making the decoder network less auto-regressive (i.e.,    < la t ex it sh a 1 _ b a s e 6 4 = \" Zn S Z L 6 Y AI 0 t 2 E L 8 7 t 1 4 2 8 X 7 n a f k = \" > AA AC E H i c b V C 7 T s M w F H X K q 5 R X g J HF o k I q S 5 U g J G Cr Y GE s E q G V mi h y X K e 1 a s e R 7 S B V U X + B h V 9 h Y Q DE y s j G 3 + C 0 G W j L k S w f n X O v 7 r 0 n S h l V 2 n F + r Mr K 6 tr 6 Rn W z tr W 9 s 7 tn 7", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our TGVAE on text generation and text summarization, and interpret its improvements both quantitatively and qualitatively.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.802430123090744}, {"text": "text summarization", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7623071372509003}]}], "tableCaptions": [{"text": " Table 1: Summary statistics for APNEWS, IMDB and BNC.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9334322214126587}, {"text": "IMDB", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.891750693321228}, {"text": "BNC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8534358739852905}]}, {"text": " Table 2: test-BLEU (higher is better) and self -BLEU (lower is better) scores over three corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.6368806958198547}]}, {"text": " Table 3: Perplexity and averaged KL scores over three  corpora. KL in our TGVAE denotes U KL in Eqn. (12).", "labels": [], "entities": [{"text": "averaged KL scores", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.7910500367482504}, {"text": "TGVAE", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.4903184473514557}]}, {"text": " Table 6: Example generated summaries on GIGAWORDS.  D is the source article, R means the reference sum- mary, Seq2seq represents the summary generated from the  Seq2Seq model.", "labels": [], "entities": [{"text": "GIGAWORDS", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8693951964378357}]}, {"text": " Table 7: Results on Gigawords and DUC-2004.", "labels": [], "entities": [{"text": "Gigawords", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.8107464909553528}, {"text": "DUC-2004", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.49512845277786255}]}]}