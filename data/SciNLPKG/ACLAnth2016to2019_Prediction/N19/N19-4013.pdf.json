{"title": [{"text": "End-to-End Open-Domain Question Answering with BERTserini", "labels": [], "entities": [{"text": "End-to-End Open-Domain Question Answering", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.49681826680898666}, {"text": "BERTserini", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.925467312335968}]}], "abstractContent": [{"text": "We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit.", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7852193415164948}, {"text": "BERT", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9898338317871094}, {"text": "Anserini information retrieval", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.522336483001709}]}, {"text": "In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion.", "labels": [], "entities": [{"text": "question answering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7738436758518219}, {"text": "BERT-based", "start_pos": 179, "end_pos": 189, "type": "METRIC", "confidence": 0.9871286153793335}]}, {"text": "We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.", "labels": [], "entities": [{"text": "BERT", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9700556993484497}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9952752590179443}]}], "introductionContent": [{"text": "BERT (, the latest refinement of a series of neural models that make heavy use of pretraining (, has led to impressive gains in many natural language processing tasks, ranging from sentence classification to question answering to sequence labeling.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9758347272872925}, {"text": "sentence classification", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.7614602446556091}, {"text": "question answering", "start_pos": 208, "end_pos": 226, "type": "TASK", "confidence": 0.7286723852157593}, {"text": "sequence labeling", "start_pos": 230, "end_pos": 247, "type": "TASK", "confidence": 0.6857169568538666}]}, {"text": "Most relevant to our task, showed impressive gains in using BERT for query-based passage reranking.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9940289258956909}, {"text": "query-based passage reranking", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6726594765981039}]}, {"text": "In this demonstration, we integrate BERT with the open-source Anserini IR toolkit to create BERTserini, an end-to-end open-domain question answering (QA) system.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9503852725028992}, {"text": "BERTserini", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9590862989425659}, {"text": "question answering (QA)", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.8455184757709503}]}, {"text": "Unlike most QA or reading comprehension models, which are best described as rerankers or extractors since they assume as input relatively small amounts of text (an article, top k sentences or passages, etc.), our system operates directly on a large corpus of Wikipedia articles.", "labels": [], "entities": []}, {"text": "We integrate best practices from the information retrieval community with BERT to produce an end-to-end system, and experiments on a standard benchmark * equal contribution test collection show large improvements over previous work.", "labels": [], "entities": [{"text": "information retrieval community", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.7924773891766866}, {"text": "BERT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7914357781410217}]}, {"text": "Our results show that fine-tuning pretrained BERT with SQuAD () is sufficient to achieve high accuracy in identifying answer spans.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.972459614276886}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9980366826057434}, {"text": "identifying answer spans", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.8119985461235046}]}, {"text": "The simplicity of this design is one major feature of our architecture.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9787635803222656}]}, {"text": "We have deployed BERTserini as a chatbot that users can interact with on diverse platforms, from laptops to mobile phones.", "labels": [], "entities": [{"text": "BERTserini", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9738239645957947}]}], "datasetContent": [{"text": "We adopt exactly the same evaluation methodology as, which was also used in subsequent work.", "labels": [], "entities": []}, {"text": "Test questions come from the development set of SQuAD; since our answers come from different texts, we only evaluate with respect to the SQuAD answer spans (i.e., the passage context is ignored).", "labels": [], "entities": []}, {"text": "Our evaluation metrics are also the same as: exact match (EM) score and F1 score (at the token level).", "labels": [], "entities": [{"text": "exact match (EM) score", "start_pos": 45, "end_pos": 67, "type": "METRIC", "confidence": 0.9594436585903168}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9930601119995117}]}, {"text": "In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved segment; this is what Chen Our main results are shown in, where we report metrics with different Anserini retrieval conditions (article, paragraphs, and sentences).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9772194921970367}]}, {"text": "We compare article retrieval at k = 5, paragraph retrieval at k = 29, and sentence retrieval at k = 78.", "labels": [], "entities": [{"text": "article retrieval", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8067047297954559}, {"text": "paragraph retrieval", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.698812335729599}, {"text": "sentence retrieval", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7228907495737076}]}, {"text": "The article setting matches the retrieval condition in.", "labels": [], "entities": []}, {"text": "The values of k for the paragraph and sentence conditions are selected so that the reader considers approximately the same amount of text: each paragraph contains 2.7 sentences on average, and each article contains 5.8 paragraphs on average.", "labels": [], "entities": []}, {"text": "The table also copies results from previous work for comparison.", "labels": [], "entities": []}, {"text": "We see that article retrieval underperforms paragraph retrieval by a large margin: the reason, we believe, is that articles are long and contain many non-relevant sentences that serve as distractors to the BERT reader.", "labels": [], "entities": [{"text": "article retrieval", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7429659366607666}, {"text": "paragraph retrieval", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7558729350566864}, {"text": "BERT", "start_pos": 206, "end_pos": 210, "type": "DATASET", "confidence": 0.4453301429748535}]}, {"text": "Sentences perform reasonably but not as well as paragraphs because they often lack the context for the reader to identify the answer span.", "labels": [], "entities": []}, {"text": "Paragraphs seem to represent a \"sweet spot\", yielding a large improvement inexact match score over previous results.", "labels": [], "entities": [{"text": "Paragraphs", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9792363047599792}, {"text": "match score", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.8485502898693085}]}, {"text": "Our next experiment examined the effects of varying k, the number of text segments considered by the BERT reader.", "labels": [], "entities": [{"text": "BERT reader", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.6889045238494873}]}, {"text": "Here, we focus only on the paragraph condition, with \u00b5 = 0.5 (the value learned via cross validation).", "labels": [], "entities": []}, {"text": "plots three metrics with respect to k: recall, top k exact match, and top exact match.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9992579817771912}, {"text": "exact match", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.8534878790378571}]}, {"text": "Recall measures the fraction of questions for which the correct answer appears in any retrieved segment, exactly as in.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9609224796295166}]}, {"text": "Top k exact match represents a lenient condition where the system receives credit fora correctly-identified span in any retrieved segment.", "labels": [], "entities": []}, {"text": "Finally, top exact match is evaluated with respect to the top-scoring span, comparable to the results reported in.", "labels": [], "entities": []}, {"text": "Scores for the paragraph condition at k = 100 are also reported in the table: we note that the exact match score is substantially higher than the previously-published best result that we are aware of.", "labels": [], "entities": [{"text": "exact match score", "start_pos": 95, "end_pos": 112, "type": "METRIC", "confidence": 0.8940074443817139}]}, {"text": "We see that, as expected, scores increase with larger k values.", "labels": [], "entities": []}, {"text": "However, the top exact match score doesn't appear to increase much after around k = 10.", "labels": [], "entities": [{"text": "exact match score", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.7851005593935648}]}, {"text": "The top k exact match score continues growing a bit longer but also reaches saturation.", "labels": [], "entities": [{"text": "exact match score", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7992745041847229}]}, {"text": "Recall appears to continue increasing all the way up to k = 100, albeit more slowly ask increases.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9851449728012085}, {"text": "ask", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.958162248134613}]}, {"text": "This means that the BERT reader is unable to take advantage of these additional answer passages that appear in the candidate pool.", "labels": [], "entities": [{"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9428843259811401}]}, {"text": "These curves also provide a failure analysis: The top recall curve (in blue) represents the upper bound with the current Anserini retriever.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9966128468513489}]}, {"text": "At k = 100, it is able to return at least one relevant paragraph around 86% of the time, and thus we can conclude that passage retrieval does not appear to be the bottleneck in overall effectiveness in the current implementation.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.8900357484817505}]}, {"text": "The gap between the top blue recall curve and the top k exact match curve (in red) quantifies the room for improvement with the BERT reader; these represent cases in which the reader did not identify the correct answer in any paragraph.", "labels": [], "entities": [{"text": "recall curve", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9702926874160767}, {"text": "BERT", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.8925812244415283}]}, {"text": "Finally, the gap between the red curve and the bottom top exact match curve (in purple) represents cases where BERT did identify the correct answer, but not as the top-scoring span.", "labels": [], "entities": [{"text": "BERT", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9946545362472534}]}, {"text": "This gap can be characterized as failures in scoring or score aggregation, and it seems to be the biggest area for improvement-suggesting that our current approach (weighted interpolation between the BERT and Anserini scores) is insufficient.", "labels": [], "entities": [{"text": "BERT", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9948257803916931}, {"text": "Anserini", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.8441995978355408}]}, {"text": "We are exploring reranking models that are capable of integrating more relevance signals.", "labels": [], "entities": []}, {"text": "One final caveat: this error analysis is based on the SQuAD ground truth.", "labels": [], "entities": [{"text": "SQuAD ground truth", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.7354407707850138}]}, {"text": "Although our answers might not match the SQuAD answer spans, they may nevertheless be acceptable (for example, different answers to time-dependent questions).", "labels": [], "entities": []}, {"text": "In future work we plan on manually examining a sample of the errors to produce a more accurate classification of the failures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on SQuAD development questions.", "labels": [], "entities": [{"text": "SQuAD development", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8897058963775635}]}]}