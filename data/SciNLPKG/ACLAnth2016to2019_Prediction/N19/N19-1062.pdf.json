{"title": [], "abstractContent": [{"text": "Online texts-across genres, registers, domains , and styles-are riddled with human stereotypes, expressed in overt or subtle ways.", "labels": [], "entities": []}, {"text": "Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features.", "labels": [], "entities": []}, {"text": "In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Boluk-basi et al., 2016) from the binary setting, such as binary gender.", "labels": [], "entities": []}, {"text": "Next, we propose a novel methodology for the evaluation of multiclass debiasing.", "labels": [], "entities": []}, {"text": "We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.", "labels": [], "entities": []}, {"text": "Sonja M Brown Givens and Jennifer L Monahan.", "labels": [], "entities": []}, {"text": "2005. Priming mammies, jezebels, and other controlling images: An examination of the influence of mediated stereotypes on perceptions of an african ameri-can woman.", "labels": [], "entities": [{"text": "Priming mammies, jezebels, and other controlling images", "start_pos": 6, "end_pos": 61, "type": "TASK", "confidence": 0.7079290482732985}]}, {"text": "Media Psychology, 7(1):87-106.", "labels": [], "entities": [{"text": "Media Psychology, 7", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8242828845977783}]}], "introductionContent": [{"text": "In addition to possessing informative features useful fora variety of NLP tasks, word embeddings reflect and propagate social biases present in training corpora.", "labels": [], "entities": []}, {"text": "Machine learning systems that use embeddings can further amplify biases (, discriminating against users, particularly those from disadvantaged social groups.", "labels": [], "entities": []}, {"text": "() introduced a method to debias embeddings by removing components that lie in stereotype-related embedding subspaces.", "labels": [], "entities": []}, {"text": "They demonstrate the effectiveness of the approach by removing gender bias from word2vec embeddings (, preserving the utility of embeddings and potentially alleviating biases in downstream tasks.", "labels": [], "entities": []}, {"text": "However, this method was only for binary labels (e.g., male/female), whereas most real-world demographic attributes, including gender, race, religion, are not binary but continuous or categorical, with more than two categories.", "labels": [], "entities": []}, {"text": "In this work, we show a generalization of which enables multiclass debiasing, while preserving utility of embeddings ( \u00a73).", "labels": [], "entities": []}, {"text": "We train word2vec embeddings using the Reddit L2 corpus ( and apply multiclass debiasing using lexicons from studies on bias in NLP and social science ( \u00a74.2).", "labels": [], "entities": [{"text": "Reddit L2 corpus", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.8043285806973776}]}, {"text": "We introduce a novel metric for evaluation of bias in collections of word embeddings ( \u00a75).", "labels": [], "entities": []}, {"text": "Finally, we validate that the utility of debiased embeddings in the tasks of part-of-speech (POS) tagging, named entity recognition (NER), and POS chunking is on par with off-the-shelf embeddings.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.670454740524292}, {"text": "named entity recognition (NER)", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7942459732294083}, {"text": "POS chunking", "start_pos": 143, "end_pos": 155, "type": "TASK", "confidence": 0.7954920530319214}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The associated mean average cosine similarity  (MAC) (defined in Section 3.2) and P-Values for debi- asing methods for gender, race, and religious bias.", "labels": [], "entities": [{"text": "associated mean average cosine similarity  (MAC)", "start_pos": 14, "end_pos": 62, "type": "METRIC", "confidence": 0.8945149332284927}, {"text": "P-Values", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9586873054504395}]}, {"text": " Table 3: The performance of embeddings the downstream tasks of NER, POS Tagging, and POS Chunking.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.532862588763237}]}]}