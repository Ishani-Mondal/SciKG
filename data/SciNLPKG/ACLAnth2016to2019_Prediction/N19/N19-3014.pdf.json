{"title": [{"text": "Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language Understanding", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6961150914430618}]}], "abstractContent": [{"text": "One of the main challenges in Spoken Language Understanding (SLU) is dealing with 'open-vocabulary' slots.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.9230760037899017}]}, {"text": "Recently, SLU models based on neural network were proposed, but it is still difficult to recognize the slots of unknown words or 'open-vocabulary' slots because of the high cost of creating a manually tagged SLU dataset.", "labels": [], "entities": []}, {"text": "This paper proposes data noising, which reflects the characteristics of the 'open-vocabulary' slots, for data augmentation.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7545512318611145}]}, {"text": "We applied it to an attention based bi-directional recurrent neural network (Liu and Lane, 2016) and experimented with three datasets: Airline Travel Information System (ATIS), Snips, and MIT-Restaurant.", "labels": [], "entities": [{"text": "MIT-Restaurant", "start_pos": 188, "end_pos": 202, "type": "DATASET", "confidence": 0.9734426736831665}]}, {"text": "We achieved performance improvements of up to 0.57% and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively.", "labels": [], "entities": [{"text": "intent prediction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7532447576522827}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9350704550743103}, {"text": "slot filling (f1-score)", "start_pos": 97, "end_pos": 120, "type": "METRIC", "confidence": 0.6381716072559357}]}, {"text": "Our method is advantageous because it does not require additional memory and it can be applied simultaneously with the training process of the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialog processing enables dialogue between humans and voice assistants such as 'Siri' and 'Alexa'.", "labels": [], "entities": [{"text": "Dialog processing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9082912802696228}]}, {"text": "In dialogue processing, spoken language understanding (SLU) is aimed at understanding and generating the user intention from an utterance.", "labels": [], "entities": [{"text": "dialogue processing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.803745299577713}, {"text": "spoken language understanding (SLU)", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.8361068169275919}]}, {"text": "The user intention consists of an intent and slots, which are semantic entities, and it is generally defined variously according to the domain.", "labels": [], "entities": []}, {"text": "Neural networks (NNs) have been actively studied and applied to SLU. and proposed models for SLU using Convolutional Neural Networks and many other researchers used recurrent neural networks (RNNs).", "labels": [], "entities": [{"text": "SLU.", "start_pos": 64, "end_pos": 68, "type": "TASK", "confidence": 0.9273089170455933}]}, {"text": "used bi-directional RNN models and applied the attention mechanism.", "labels": [], "entities": []}, {"text": "They showed good results by using a joint learning method for both slot filling and intent prediction tasks.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.8568152785301208}, {"text": "intent prediction tasks", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7715319295724233}]}, {"text": "For considering the relationship between the two tasks, constructed two models for each task and  used a 'slot-gate'.", "labels": [], "entities": []}, {"text": "Training an SLU model using an NN requires a large amount of training data labeled with slots and intents, which is expensive to build.", "labels": [], "entities": []}, {"text": "In particular, plenty of corpora or dictionaries are required to recognize the value of an 'open-vocabulary' slot, such as a song title.", "labels": [], "entities": []}, {"text": "In addition, it is difficult to predict the slot type of words used in the 'open-vocabulary' slot because there is neither a semantic restriction nor a length limit.", "labels": [], "entities": []}, {"text": "presented the features and examples of 'open-vocabulary' slot and proposed anew model that could effectively predict this type of slot.", "labels": [], "entities": []}, {"text": "They exploited a long-term aware attention structure and positional encoding with multi-task learning of a character-based language model and intent detection model to focus more on relatively global information within a sentence.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 142, "end_pos": 158, "type": "TASK", "confidence": 0.6833526790142059}]}, {"text": "The objective is to recognize slots, including 'open-vocabulary' slots, and predict the intent effectively by data augmentation.", "labels": [], "entities": []}, {"text": "We propose a data noising method that reflects the characteristics of the 'open-vocabulary' slots.", "labels": [], "entities": []}, {"text": "This method is advantageous in that it does not require additional memory.", "labels": [], "entities": []}, {"text": "Moreover, it is performed simultaneously with the training of the model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We set the attention based bi-directional LSTM model () without the label dependency as the baseline for the experiment.", "labels": [], "entities": []}, {"text": "We applied our data augmentation method to the baseline and evaluated the following two cases: Just add noise (+Noise) , Add noise and use context window (+Noise, cw).", "labels": [], "entities": []}, {"text": "We followed the set-up in.", "labels": [], "entities": []}, {"text": "We set the number of LSTM cells to 128, the batch size was 16, and the dropout rate was 0.5.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 71, "end_pos": 83, "type": "METRIC", "confidence": 0.9183862209320068}]}, {"text": "We considered one layer and used the Adam optimizer ( for parameter optimization.", "labels": [], "entities": []}, {"text": "The word embeddings were randomly initialized and then fine-tuned and their size was 128 for experiments with ATIS and MR, and 64 for experiments with Snips, for comparison with previous studies.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.8740591406822205}, {"text": "MR", "start_pos": 119, "end_pos": 121, "type": "DATASET", "confidence": 0.6150837540626526}]}, {"text": "The noise vector was created with probability p and it defined the number of augmented data.", "labels": [], "entities": []}, {"text": "In this paper, because we performed the data augmentation in batches during the training, the number of augmented utterances was defined as (the number of step \u00d7 batch size) \u00d7 p.", "labels": [], "entities": []}, {"text": "The probability was set to 0.25, 0.5, 0.75, or 1.0.", "labels": [], "entities": []}, {"text": "The noise vector was sampled randomly from the normal distribution or uniform distribution and its size was equal to the word embedding size.", "labels": [], "entities": []}, {"text": "We set the mean of the normal distribution to 0.0, and the \u03c3 value to 0.1, 0.", "labels": [], "entities": []}, {"text": "As in previous SLU studies, we used the F1-score and the accuracy to evaluate the performance of the slot filling (SF) and the intent prediction(IP), respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994188547134399}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9997116923332214}, {"text": "slot filling (SF)", "start_pos": 101, "end_pos": 118, "type": "METRIC", "confidence": 0.6259829998016357}, {"text": "intent prediction(IP)", "start_pos": 127, "end_pos": 148, "type": "METRIC", "confidence": 0.66523876786232}]}, {"text": "98.21 95.98 98.54 95.93 98  fective when the size of the training data is small.", "labels": [], "entities": [{"text": "fective", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9724181294441223}]}, {"text": "shows the performance according to the training data size with ATIS.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.5724037885665894}]}, {"text": "shows the comparison of the performance of previous studies with each data set.", "labels": [], "entities": []}, {"text": "In the case of the ATIS dataset, our method shows better performance than the 'Baseline' study () and a study targeting 'open-vocabulary' slots (), but it is not state-of-theart.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9532696604728699}]}, {"text": "However, we achieve the best performance among studies using the Snips and MR datasets.", "labels": [], "entities": [{"text": "Snips and MR datasets", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.76964221149683}]}], "tableCaptions": [{"text": " Table 1: Statistics of ATIS, Snips, and MR datasets.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7284680008888245}, {"text": "MR datasets", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.7941259145736694}]}, {"text": " Table 2 shows the performance improvements  achieved by the proposed method versus the base- line. The proposed method showed clear improve- ments for the Snips and MR datasets. It is con- sidered that the proposed method is effective in  the two datasets because they have larger length  of slots (as shown in Table 1) and more 'open- vocabulary' slots. Experimental results show that  our approach improves slot filling of an unknown  word or 'open-vocabulary' slots by learning the  patterns of utterances. Examples illustrating the  results can be found in Table 4. They show that  the proposed method improves the slot filling of  unknown words and 'open-vocabulary' slots by  learning the utterance patterns.  Additionally, the proposed method is more ef-", "labels": [], "entities": [{"text": "Snips and MR datasets", "start_pos": 156, "end_pos": 177, "type": "DATASET", "confidence": 0.7771318703889847}, {"text": "slot filling of an unknown  word", "start_pos": 410, "end_pos": 442, "type": "TASK", "confidence": 0.806993325551351}, {"text": "slot filling of  unknown words", "start_pos": 620, "end_pos": 650, "type": "TASK", "confidence": 0.8195644974708557}]}, {"text": " Table 3: Comparison of the results for each dataset.", "labels": [], "entities": []}]}