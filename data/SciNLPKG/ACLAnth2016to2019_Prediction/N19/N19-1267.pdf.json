{"title": [{"text": "Strong and Simple Baselines for Multimodal Utterance Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Human language is a rich multimodal signal consisting of spoken words, facial expressions, body gestures, and vocal intonations.", "labels": [], "entities": []}, {"text": "Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information.", "labels": [], "entities": [{"text": "Learning representations for these spoken utterances", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7266123195489248}]}, {"text": "Recent advances in multimodal learning have followed the general trend of building more complex models that utilize various attention , memory and recurrent components.", "labels": [], "entities": []}, {"text": "In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances.", "labels": [], "entities": []}, {"text": "The first baseline assumes a conditional factorization of the utterance into uni-modal factors.", "labels": [], "entities": []}, {"text": "Each unimodal factor is mod-eled using the simple form of a likelihood function obtained via a linear transformation of the embedding.", "labels": [], "entities": []}, {"text": "We show that the optimal embedding can be derived in closed form by taking a weighted average of the unimodal features.", "labels": [], "entities": []}, {"text": "In order to capture richer representations , our second baseline extends the first by factorizing into unimodal, bimodal, and tri-modal factors, while retaining simplicity and efficiency during learning and inference.", "labels": [], "entities": []}, {"text": "From a set of experiments across two tasks, we show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during inference.", "labels": [], "entities": []}, {"text": "Overall, we believe that our strong baseline models offer new benchmarking options for future research in multimodal learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human language is a rich multimodal signal consisting of spoken words, facial expressions, body gestures, and vocal intonations.", "labels": [], "entities": []}, {"text": "At the heart of many multimodal modeling tasks lies the challenge of learning rich representations of spoken utterances from multiple modalities ().", "labels": [], "entities": []}, {"text": "However, learning repre-* authors contributed equally sentations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information (.", "labels": [], "entities": []}, {"text": "This challenging yet crucial research area has real-world applications in robotics), dialogue systems (), intelligent tutoring systems (, and healthcare diagnosis.", "labels": [], "entities": [{"text": "healthcare diagnosis", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.7042338848114014}]}, {"text": "Recent progress on multimodal representation learning has investigated various neural models that utilize one or more of attention, memory and recurrent components (.", "labels": [], "entities": [{"text": "multimodal representation learning", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.7785850962003072}]}, {"text": "There has also been a general trend of building more complicated models for improved performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances.", "labels": [], "entities": []}, {"text": "The first baseline assumes a factorization of the utterance into unimodal factors conditioned on the joint embedding.", "labels": [], "entities": []}, {"text": "Each unimodal factor is modeled using the simple form of a likelihood function obtained via a linear transformation of the utterance embedding.", "labels": [], "entities": []}, {"text": "We derive a coordinate-ascent style algorithm to learn the optimal multimodal embeddings under our model.", "labels": [], "entities": []}, {"text": "We show that, under some assumptions, maximum likelihood estimation for the utterance embedding can be derived in closed form and is equivalent to computing a weighted average of the language, visual and acoustic features.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 38, "end_pos": 67, "type": "METRIC", "confidence": 0.8300740917523702}]}, {"text": "Only a few linear transformation parameters need to be learned.", "labels": [], "entities": []}, {"text": "In order to capture bimodal and trimodal representations, our second baseline extends the first one by assuming a factorization into unimodal, bimodal, and trimodal factors (.", "labels": [], "entities": []}, {"text": "To summarize, our simple baselines 1) consist primarily of linear functions, 2) have few parameters, and 3) can be approximately solved in a closed form solution.", "labels": [], "entities": []}, {"text": "As a result, they demonstrate simplicity and efficiency during learning and inference.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9789531230926514}]}, {"text": "We perform a set of experiments across two tasks and datasets spanning multimodal personality traits recognition) and multimodal sentiment analysis (.", "labels": [], "entities": [{"text": "multimodal personality traits recognition", "start_pos": 71, "end_pos": 112, "type": "TASK", "confidence": 0.6391171663999557}, {"text": "multimodal sentiment analysis", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7139627834161123}]}, {"text": "Our proposed baseline models 1) achieve competitive performance on supervised multimodal learning, 2) improve upon classical deep autoencoders for semisupervised multimodal learning, and 3) are up to 10 times faster during inference.", "labels": [], "entities": []}, {"text": "Overall, we believe that our baseline models offer new benchmarks for future multimodal research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the generalization of our models, we perform experiments on multimodal speaker traits recognition and multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal speaker traits recognition", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.5717077627778053}, {"text": "multimodal sentiment analysis", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.7485941449801127}]}, {"text": "The code for our experiments is released at https://github.com/ yaochie/multimodal-baselines, and all datasets for our experiments can be downloaded at https://github.com/A2Zadeh/ CMU-MultimodalSDK.", "labels": [], "entities": []}, {"text": "For classification, we report multiclass classification accuracy A(c) where c denotes the number of classes and F1 score.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9783880114555359}, {"text": "multiclass classification accuracy A", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.5928820446133614}, {"text": "F1 score", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9846483767032623}]}, {"text": "For regression, we report Mean Absolute Error (MAE) and Pearson's correlation (r).", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 26, "end_pos": 51, "type": "METRIC", "confidence": 0.9368860522905985}, {"text": "Pearson's correlation (r)", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.9601606925328573}]}, {"text": "For MAE lower values indicate better performance.", "labels": [], "entities": [{"text": "MAE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8338064551353455}]}, {"text": "For all remaining metrics, higher values indicate better performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for multimodal personality trait recognition on POM (left) and multimodal sentiment analysis  on CMU-MOSI (right). EF-LSTM () and HCRF () denote the best result obtained from the LSTM and HCRF  variants respectively. The top two results are highlighted in bold. Our proposed baseline model (MMB2), despite  its simplicity, often ranks in the top two models and outperforms many large neural models such as C-MKL, DF,  SAL-CNN, EF-LSTM, MV-LSTM, BC-LSTM, TFN, and MFN.", "labels": [], "entities": [{"text": "multimodal personality trait recognition", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.5890353098511696}, {"text": "multimodal sentiment analysis", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6641392211119334}, {"text": "CMU-MOSI", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.935485303401947}]}, {"text": " Table 2: Semi-supervised sentiment prediction results  on CMU-MOSI. Our model outperforms deep autoen- coders (AE) and their recurrent variant (seq2seq), re- maining strong despite limited labeled data.", "labels": [], "entities": [{"text": "Semi-supervised sentiment prediction", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6349737644195557}, {"text": "CMU-MOSI", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9086551666259766}]}, {"text": " Table 3: Average time taken for inference on the CMU- MOSI test set and Inferences Per Second (IPS) on a  single Nvidia GeForce GTX 1080 Ti GPU, averaged  over 5 trials. Our proposed baselines are more than 10  times faster than the closest neural model (EF-LSTM).", "labels": [], "entities": [{"text": "CMU- MOSI test set", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.8854891180992126}, {"text": "Inferences Per Second (IPS)", "start_pos": 73, "end_pos": 100, "type": "METRIC", "confidence": 0.9368326465288798}, {"text": "Nvidia GeForce GTX 1080 Ti GPU", "start_pos": 114, "end_pos": 144, "type": "DATASET", "confidence": 0.7904839913050333}]}, {"text": " Table 4: Ablation studies on CMU-MOSI test set. In- corporating nonverbal (visual and acoustic) features,  positional encodings (PE), and task-specific fine tuning  (FT) are important for good prediction performance.", "labels": [], "entities": [{"text": "CMU-MOSI test set", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.8747635086377462}, {"text": "task-specific fine tuning  (FT)", "start_pos": 139, "end_pos": 170, "type": "METRIC", "confidence": 0.7401969532171885}]}]}