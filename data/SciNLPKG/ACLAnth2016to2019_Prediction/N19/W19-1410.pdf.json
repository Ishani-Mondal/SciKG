{"title": [{"text": "Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code Switching Data", "labels": [], "entities": [{"text": "Part-of-Speech Tagging of Code Switching Data", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.8475363105535507}]}], "abstractContent": [{"text": "Linguistic Code Switching (CS) is a phenomenon that occurs when multilingual speakers alternate between two or more lan-guages/dialects within a single conversation.", "labels": [], "entities": [{"text": "Linguistic Code Switching (CS)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7726899832487106}]}, {"text": "Processing CS data is especially challenging in intra-sentential data given state-of-the-art monolingual NLP technologies since such technologies are geared toward the processing of one language at a time.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of Part-of-Speech tagging (POS) in the context of linguistic code switching (CS).", "labels": [], "entities": [{"text": "Part-of-Speech tagging (POS)", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8266611218452453}, {"text": "linguistic code switching (CS)", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.8011101384957632}]}, {"text": "We explore leveraging multiple neural network architectures to measure the impact of different pre-trained embeddings methods on POS tagging CS data.", "labels": [], "entities": [{"text": "POS tagging CS", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.8273701071739197}]}, {"text": "We investigate the landscape in four CS language pairs, Spanish-English, Hindi-English, Modern Standard Arabic-Egyptian Arabic dialect (MSA-EGY), and Modern Standard Arabic-Levantine Arabic dialect (MSA-LEV).", "labels": [], "entities": [{"text": "Modern Standard Arabic-Levantine Arabic dialect (MSA-LEV", "start_pos": 150, "end_pos": 206, "type": "DATASET", "confidence": 0.6651144070284707}]}, {"text": "Our results show that multilingual embedding (e.g., MSA-EGY and MSA-LEV) helps closely related languages (EGY/LEV) but adds noise to the languages that are distant (SPA/HIN).", "labels": [], "entities": []}, {"text": "Finally , we show that our proposed models out-perform state-of-the-art CS taggers for MSA-EGY language pair.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code Switching (CS) is a common linguistic behavior where two or more languages/dialects are used interchangeably in either spoken or written form.", "labels": [], "entities": [{"text": "Code Switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8240898847579956}]}, {"text": "CS is typically present at various levels of linguistic structure: across sentence boundaries (i.e., inter-sentential), within the same utterances, mixing two or more languages (i.e., intrasentential), or at the words/morphemes level.", "labels": [], "entities": []}, {"text": "The CS phenomenon is noticeable and common in countries that have large immigrant groups, naturally leading to bilingualism.", "labels": [], "entities": []}, {"text": "Typically people who code switch master two (or more) languages: a common first language (lang1) and another prevalent language as a second language (lang2).", "labels": [], "entities": []}, {"text": "The languages could be completely distinct such as Mandarin and English, or Hindi and English, or they can be variants of one another such as in the case of Modern Standard Arabic (MSA) and Arabic regional dialects (e.g. Egyptian dialect-EGY).", "labels": [], "entities": []}, {"text": "CS is traditionally prevalent in the spoken modality but with the ubiquity of the Internet and proliferation of social media, CS is becoming ubiquitous in written modalities and genres (.", "labels": [], "entities": []}, {"text": "This new situation has created an unusual deluge of CS textual data on the Internet.", "labels": [], "entities": []}, {"text": "This data brings in its wake new opportunities, but it poses serious challenges for different NLP tasks; traditional techniques trained for one language tend to breakdown when the input text happens to include two or more languages.", "labels": [], "entities": []}, {"text": "The performance of NLP models that are currently expected to yield good results (e.g., Part-of-Speech Tagging) would degrade at a rate proportional to the amount and level of mixed-language present.", "labels": [], "entities": [{"text": "Part-of-Speech Tagging)", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8071185946464539}]}, {"text": "This is a result of out-ofvocabulary words in one language and new hybrid grammar structures, and in some cases shared cognates or ambiguous words that exist in both language lexicons.", "labels": [], "entities": []}, {"text": "POS tagging is a vital component of any Natural Language Understanding system, and one of the first tasks researchers employ to process data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7484780251979828}]}, {"text": "POS tagging is an enabling technology needed by higher-up NLP tools such as chunkers and parsers -syntactic, semantic and discourse level processing; all of which are used for such applications as sentiment analysis and subjectivity, text summarization, information extraction, automatic speech recognition, and machine translation among others.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7132624834775925}, {"text": "sentiment analysis", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.89845210313797}, {"text": "text summarization", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.7498155832290649}, {"text": "information extraction", "start_pos": 254, "end_pos": 276, "type": "TASK", "confidence": 0.8485770225524902}, {"text": "automatic speech recognition", "start_pos": 278, "end_pos": 306, "type": "TASK", "confidence": 0.6297930081685384}, {"text": "machine translation", "start_pos": 312, "end_pos": 331, "type": "TASK", "confidence": 0.7729620337486267}]}, {"text": "As such, it is crucial that POS taggers be able to process CS textual data.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.7950243651866913}]}, {"text": "In this paper, we address the problem of Partof-Speech tagging (POS) for CS data on the intrasentential level for multiple language pairs.", "labels": [], "entities": [{"text": "Partof-Speech tagging (POS)", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.857599937915802}]}, {"text": "We explore the effect of using various embeddings setups and multiple neural network architectures in order to mitigate the problem of the scarcity of CS annotated data.", "labels": [], "entities": []}, {"text": "We propose multiple word embedding techniques that could help in tackling POS tagging of CS data.", "labels": [], "entities": [{"text": "tackling POS tagging of CS data", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.8097331821918488}]}, {"text": "In order to examine the generalization of our approaches across language pairs, we conduct our study on four different evaluation CS data sets, covering four language pairs: Modern Standard Arabic and the Egyptian Arabic dialect (MSA-EGY), Modern Standard Arabic-and the Levantine Arabic dialect (MSA-LEV), Spanish-English (SPA-ENG) and Hindi-English (HIN-ENG).", "labels": [], "entities": []}, {"text": "We use the same POS tag sets for all language pairs, namely, the Universal POS tag set ().", "labels": [], "entities": []}, {"text": "Our contributions are the following: a) We use a state-of-the-art bidirectional recurrent neural networks; b) We explore different strategies to leverage raw textual resources for creating pretrained embeddings for POS tagging CS data; c) We examine the effect of language identifiers for joint POS tagging and language identification; d) We present the first empirical evaluation on POS tagging with four different language pairs.", "labels": [], "entities": [{"text": "POS tagging CS", "start_pos": 215, "end_pos": 229, "type": "TASK", "confidence": 0.8419033686319987}, {"text": "POS tagging", "start_pos": 295, "end_pos": 306, "type": "TASK", "confidence": 0.8352620005607605}, {"text": "language identification", "start_pos": 311, "end_pos": 334, "type": "TASK", "confidence": 0.7405042201280594}, {"text": "POS tagging", "start_pos": 384, "end_pos": 395, "type": "TASK", "confidence": 0.8757603168487549}]}, {"text": "All of the previous work focused on a single or two language pair combinations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Monolingual embedding (baseline): We train word embeddings using the monolingual corpora for each language involved in the four language pairs.", "labels": [], "entities": []}, {"text": "The results of this approach are six separate pre-trained embeddings, MSA, EGY, LEV, ENG, SPA, and HIN pre-trained embeddings.", "labels": [], "entities": [{"text": "EGY", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9286876320838928}, {"text": "LEV", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9546512365341187}]}, {"text": "For each language, we train a BiLSTM-CRF model using one of the six pre-trained embeddings.", "labels": [], "entities": []}, {"text": "We consider these models as baseline systems.", "labels": [], "entities": []}, {"text": "The baseline performance is the POS tagging accuracy of the monolingual models with no special training for CS data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7289499044418335}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8998430967330933}]}], "tableCaptions": [{"text": " Table 1: Datasets distribution for the four lan- guage pairs", "labels": [], "entities": []}, {"text": " Table 2: POS tagging accuracy (%) on the four corpora. Average over five runs with different random  seeds. Bold and italics font indicates the best result in our experiments, while bold font indicates the best  results compared to the state-of-the-art systems. We refer to BiLSTM-CRF Tagger as (1), MTL-POS  Tagger that learns POS tag for related languages as (2), and MTL-POS+LID that learns jointly POS  tagging and language identification as (3). Random-Initi-Embed refers to Random initialized embedding", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8459354341030121}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9760138988494873}, {"text": "language identification", "start_pos": 420, "end_pos": 443, "type": "TASK", "confidence": 0.6596900373697281}]}, {"text": " Table 3: LID accuracy (%) on the four corpora. Average over five runs with different random seeds.", "labels": [], "entities": [{"text": "LID", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9975436329841614}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.910528302192688}]}, {"text": " Table 4: Most common errors for the best systems for all language pairs (Gold-POS > Predicted-POS)", "labels": [], "entities": []}]}