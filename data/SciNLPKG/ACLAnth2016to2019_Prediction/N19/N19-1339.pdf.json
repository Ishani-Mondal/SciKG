{"title": [], "abstractContent": [{"text": "Neural word representations are at the core of many state-of-the-art natural language processing models.", "labels": [], "entities": [{"text": "Neural word representations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7195414304733276}]}, {"text": "A widely used approach is to pre-train, store and lookup word or character embedding matrices.", "labels": [], "entities": []}, {"text": "While useful, such representations occupy huge memory making it hard to deploy on-device and often do not generalize to unknown words due to vocabulary pruning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a skip-gram based architecture coupled with Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically computable representations.", "labels": [], "entities": []}, {"text": "Our model does not need to store lookup tables as representations are computed on-the-fly and require low memory footprint.", "labels": [], "entities": []}, {"text": "The representations can be trained in an unsupervised fashion and can be easily transferred to other NLP tasks.", "labels": [], "entities": []}, {"text": "For qualitative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings.", "labels": [], "entities": []}, {"text": "For quantitative evaluation , we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained word representations are at the core of many neural language understanding models.", "labels": [], "entities": [{"text": "neural language understanding", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.7929273446400961}]}, {"text": "Among the most popular and widely used word embeddings are word2vec (), GloVe () and ELMO ().", "labels": [], "entities": []}, {"text": "The biggest challenge with word embedding is that they require lookup and a large memory footprint, as we have to store one entry (d-dim vector) per word and it blows up.", "labels": [], "entities": []}, {"text": "In parallel, the tremendous success of deep learning models and the explosion of mobile, IoT de- * Work done during internship at Google.", "labels": [], "entities": []}, {"text": "vices coupled together with the growing user privacy concerns have led to the need for deploying deep learning models on-device for inference.", "labels": [], "entities": []}, {"text": "This has led to new research in compressing large and complex deep learning models for low power ondevice deployment.", "labels": [], "entities": []}, {"text": "Recently,) developed an on-device neural text classification model.", "labels": [], "entities": [{"text": "neural text classification", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6842385232448578}]}, {"text": "They proposed to reduce the memory footprint of large neural networks by replacing the input word embeddings with projection based representations.", "labels": [], "entities": []}, {"text": "( used n-gram features to generate binary LSH) randomized projections on the fly surpassing the need to store word emebdding tables and reducing the memory size.", "labels": [], "entities": []}, {"text": "The projection models reduce the memory occupied by the model from O(|V |) to O(n P ), where |V | refers to the vocabulary size and n P refers to number of projection operations.", "labels": [], "entities": []}, {"text": "Two key advantages of the projection based representations over word embeddings are: (1) they are fixed and have low memory size; (2) they can handle out of vocabulary words.", "labels": [], "entities": []}, {"text": "However, the projections in ( are static and currently do not leverage pre-training on large unsupervised corpora, which is an important property to make the projections transferable to new tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine the best of both worlds by learning transferable neural projection representations over randomized LSH projections.", "labels": [], "entities": []}, {"text": "We do this by introducing new neural architecture inspired by the skip gram model of () and combined with a deep MLP plugged on top of LSH projections.", "labels": [], "entities": []}, {"text": "In order to make this model train better, we introduce new regularizing loss function, which minimizes the cosine similarities of the words within a mini-batch.", "labels": [], "entities": []}, {"text": "The loss function is critical for generalization.", "labels": [], "entities": [{"text": "generalization", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9656856656074524}]}, {"text": "In summary, our model (1) requires a fixed and low memory footprint, (2) can handle out of vo-cabulary words and misspellings, (3) captures semantic and syntactic properties of words; (4) can be easily plugged to other NLP models and (5) can support training with data augmentation by perturbing characters of input words.", "labels": [], "entities": []}, {"text": "To validate the performance of our approach, we conduct a qualitative analysis of the nearest neighbours in the learned representation spaces and a quantitative evaluation via similarity, language modeling and NLP tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our skipgram models on the wikipedia data XML dump, enwik9 1 . We extract the normalized English text from the XML dump using the Matt Mahoneys pre-processing perl script 2 . We fix the vocabulary to the top 100k frequently occurring words.", "labels": [], "entities": [{"text": "wikipedia data XML dump", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7702684327960014}]}, {"text": "We sub-sample words in the training corpus, dropping them with probability, P(w) = 1 \u2212 t/f req(w), where f req(w) is the frequency of occurrence of win the corpus and we set the threshold, t to 10 \u22125 . We perturb the input words with a probability of 0.4 using a randomly chosen perturbation described in Section 2.5.", "labels": [], "entities": []}, {"text": "We show both qualitative and quantitative evaluation on multiple tasks for the NP-SG model.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors produced by NP-SG for select words.", "labels": [], "entities": []}, {"text": "Independent of whether it is an original or misspelled word, our NP-SG model accurately retrieves relevant and semantically similar words.", "labels": [], "entities": []}, {"text": "We evaluate our NP-SG model on similarity, language modeling and text classification tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7420266568660736}, {"text": "text classification", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.768040120601654}]}, {"text": "Similarity tests the ability to capture words, while language modeling and classification warrant the ability to transfer the neural projections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Similarity Tasks: # of params, 100k vocabulary size for skipgram baseline, 100 embedding size.", "labels": [], "entities": []}]}