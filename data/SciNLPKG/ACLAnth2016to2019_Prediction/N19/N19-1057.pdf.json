{"title": [{"text": "Improving Dialogue State Tracking by Discerning the Relevant Context", "labels": [], "entities": [{"text": "Improving Dialogue State Tracking", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9013590514659882}]}], "abstractContent": [{"text": "A typical conversation comprises of multiple turns between participants where they go back-and-forth between different topics.", "labels": [], "entities": []}, {"text": "At each user turn, dialogue state tracking (DST) aims to estimate user's goal by processing the current utterance.", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.7611488848924637}]}, {"text": "However, in many turns, users implicitly refer to the previous goal, necessitating the use of relevant dialogue history.", "labels": [], "entities": []}, {"text": "Nonetheless, distinguishing relevant history is challenging and a popular method of using dialogue recency for that is inefficient.", "labels": [], "entities": [{"text": "distinguishing relevant history", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.8788952430089315}]}, {"text": "We, therefore , propose a novel framework for DST that identifies relevant historical context by referring to the past utterances where a particular slot-value changes and uses that together with weighted system utterance to identify the relevant context.", "labels": [], "entities": [{"text": "DST", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9873737692832947}]}, {"text": "Specifically, we use the current user utterance and the most recent system utterance to determine the relevance of a system utterance.", "labels": [], "entities": []}, {"text": "Empirical analyses show that our method improves joint goal accuracy by 2.75% and 2.36% on WoZ 2.0 and Mul-tiWoZ 2.0 restaurant domain datasets respectively over the previous state-of-the-art GLAD model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.96507728099823}, {"text": "WoZ", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9057751297950745}, {"text": "Mul-tiWoZ 2.0 restaurant domain datasets", "start_pos": 103, "end_pos": 143, "type": "DATASET", "confidence": 0.8633538365364075}]}], "introductionContent": [{"text": "Dialog state tracking (DST) is a vital component in the task-oriented dialog systems which is used to estimate user's goals and requests in order to plan next action and respond accordingly.", "labels": [], "entities": [{"text": "Dialog state tracking (DST)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7786715279022852}]}, {"text": "At each turn, DST aims to identify the set of goals that a user aims to achieve and requests that are represented as slot-value pairs.", "labels": [], "entities": [{"text": "DST", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9395163655281067}]}, {"text": "Typically, this decision is made by considering user utterance in the current turn or system actions in the previous turn.", "labels": [], "entities": []}, {"text": "However, in many cases, the considered user utterance or system actions do not present enough information and refers to a previous utterance.", "labels": [], "entities": []}, {"text": "As shown through an example in, while exploring different available options, user  can go back-and-forth between the currently and previously discussed facts.", "labels": [], "entities": []}, {"text": "For instance, when offered with two different restaurant options namely Nirala (food=indian) and Golden Wok (food=chinese) in the second turn, user first inquires about the details of Golden Wok.", "labels": [], "entities": []}, {"text": "And after getting relevant details about the Golden Wok in the following two turns, user refers back to the second option provided in second turn and asks about Nirala restaurant.", "labels": [], "entities": [{"text": "Golden Wok", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9421416521072388}]}, {"text": "To predict the correct slot-value pair food=indian in the dialog state of the fifth turn, the system is required to refer back to the second turn again to find information about Nirala, as the context obtained from the current dialog turn is insufficient.", "labels": [], "entities": []}, {"text": "Identifying such implicitly referenced historical turns is challenging since implicit references are not local and most recent turns are often not informative.", "labels": [], "entities": []}, {"text": "Therefore, the traditional approach of modeling dialogue recency  may not suffice.", "labels": [], "entities": [{"text": "dialogue recency", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7189524471759796}]}, {"text": "Instead, we propose to model implicit references by storing links to the past turn where each of the slots was modified.", "labels": [], "entities": []}, {"text": "Then at each turn, we lookup though the stored links to find the previous turn which may provide additional cues for predicting the appropriate slot-value.", "labels": [], "entities": []}, {"text": "Moreover, the dialogue system often asks polar questions with yes-no answers.", "labels": [], "entities": []}, {"text": "For instance, the DST system should update the dialogue state with food=indian when a user replies Yes to a system utterance Do you want Indian food?.", "labels": [], "entities": [{"text": "DST", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.6980311870574951}]}, {"text": "In such cases, neither the user utterance nor system acts (food in this example) contain any information about the actual slot-value.", "labels": [], "entities": []}, {"text": "This makes utilization of both system and user utterance eminent for dialog state tracking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.8655903140703837}]}, {"text": "However, utilizing the previous system utterance together with the current user utterance always at each turn may add noise.", "labels": [], "entities": []}, {"text": "Therefore, we use a gating mechanism based on both utterances to determine the relevance of the previous system utterance in the current turn.", "labels": [], "entities": []}, {"text": "The evaluation shows that identifying the relevant context is essential for dialogue state tracking.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8653935591379801}]}, {"text": "Our novel model that discerns important details in non-adjacent dialogue turns and the previous system utterance from a dialog history is able to improve the previous state-of-the-art GLAD () model on all evaluation metrics for both WoZ and MultiWoZ (restaurant) datasets.", "labels": [], "entities": [{"text": "GLAD", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.8993997573852539}]}, {"text": "Furthermore, we empirically show that a simple self-attention based biLSTM model, using only one-third of the number of parameters as GLAD, outperforms GLAD by identifying and incorporating the relevant context.", "labels": [], "entities": []}], "datasetContent": [{"text": "We primarily use WoZ 2.0 ( ) restaurant reservation task dataset that consists of 1200 dialogues for training and evaluation.", "labels": [], "entities": [{"text": "WoZ 2.0 ( ) restaurant reservation task dataset", "start_pos": 17, "end_pos": 64, "type": "DATASET", "confidence": 0.7323773950338364}]}, {"text": "Each dialogue has an average of eight turns, where each turn contains system utterance transcript, user utterance transcript, turn label and belief state.", "labels": [], "entities": []}, {"text": "All the dialogue states and actions are based on a task ontology that supports three different informable slot-types namely price range with 4 values, food with 72 values, area with 7 values, and requests of 7 different types like address and phone.", "labels": [], "entities": []}, {"text": "Following the standard settings, we use 600 dialogues for training, 200 for validation and the remaining 400 for testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracy of baselines and proposed  models on WoZ 2.0 restaurant reservation dataset.   \u2020Retrained using docker container provided by the au- thors with exactly same hyper-parameters. We also  experimented with different versions of PyTorch and  cuDNN and found that results had high variance.  Therefore, we report the average performance over 5  runs with different initializations for GLAD and all our  models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9980359673500061}, {"text": "WoZ 2.0 restaurant reservation dataset", "start_pos": 61, "end_pos": 99, "type": "DATASET", "confidence": 0.9076153755187988}, {"text": "cuDNN", "start_pos": 261, "end_pos": 266, "type": "DATASET", "confidence": 0.8860206604003906}, {"text": "GLAD", "start_pos": 403, "end_pos": 407, "type": "DATASET", "confidence": 0.7935767769813538}]}, {"text": " Table 2: Test accuracy of GLAD and proposed models  on MultiWoZ 2.0 restaurant domain dataset. Note that  we considered all 276 slot-values for evaluating mod- els. Budzianowski et al. (2018) reported joint goal ac- curacy of 80.9 on MultiWoZ 2.0 (restaurant) dataset.  We believe they didn't include restaurant name slot in  their evaluation and only considered presence of three  slot-types-book time, book day and book people-and  not their values.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9881830811500549}, {"text": "MultiWoZ 2.0 restaurant domain dataset", "start_pos": 56, "end_pos": 94, "type": "DATASET", "confidence": 0.6424512922763824}, {"text": "ac- curacy", "start_pos": 213, "end_pos": 223, "type": "METRIC", "confidence": 0.9161303440729777}]}]}