{"title": [], "abstractContent": [{"text": "We propose anew type of representation learning method that models words, phrases and sentences seamlessly.", "labels": [], "entities": []}, {"text": "Our method does not depend on word segmentation and any human-annotated resources (e.g., word dictionaries), yet it is very effective for noisy corpora written in unsegmented languages such as Chinese and Japanese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7095425575971603}]}, {"text": "The main idea of our method is to ignore word boundaries completely (i.e., segmentation-free), and construct representations for all character n-grams in a raw corpus with embeddings of compositional sub-n-grams.", "labels": [], "entities": []}, {"text": "Although the idea is simple, our experiments on various benchmarks and real-world datasets show the efficacy of our proposal.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most existing word embedding models () take a sequence of words as their input.", "labels": [], "entities": []}, {"text": "Therefore, the conventional models are dependent on word segmentation (, which is a process of converting a raw corpus (i.e., a sequence of characters) into a sequence of segmented character n-grams.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7500638067722321}]}, {"text": "After the segmentation, the segmented character n-grams are assumed to be words, and each word's representation is constructed from distribution of neighbour words that co-occur together across the estimated word boundaries.", "labels": [], "entities": []}, {"text": "However, in practice, this kind of approach has several problems.", "labels": [], "entities": []}, {"text": "First, word segmentation is difficult especially when texts in a corpus are noisy or unsegmented (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7760260701179504}]}, {"text": "For example, word segmentation on social network service (SNS) corpora, such as Twitter, is a challenging task since it tends to include many misspellings, informal words, neologisms, and even emoticons.", "labels": [], "entities": [{"text": "word segmentation on social network service (SNS) corpora", "start_pos": 13, "end_pos": 70, "type": "TASK", "confidence": 0.8156338810920716}]}, {"text": "This problem becomes more severe in unsegmented languages, such as Chinese and Japanese, whose word boundaries are not explicitly indicated.", "labels": [], "entities": []}, {"text": "Second, word segmentation has ambiguities (;.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7479462623596191}]}, {"text": "For example, a compound word (linear algebra) can be seen as a single word or sequence of words, such as | (linear | algebra).", "labels": [], "entities": []}, {"text": "Word segmentation errors negatively influence subsequent processes ().", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6635191440582275}]}, {"text": "For example, we may lose some words in training corpora, leading to a larger Out-Of-Vocabulary (OOV) rate).", "labels": [], "entities": [{"text": "Out-Of-Vocabulary (OOV) rate", "start_pos": 77, "end_pos": 105, "type": "METRIC", "confidence": 0.9644737482070923}]}, {"text": "Moreover, segmentation errors, such as segmenting (yesterday) as | (tree | brain), produce false co-occurrence information.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9683015942573547}]}, {"text": "This problem is crucial for most existing word embedding methods as they are based on distributional hypothesis, which can be summarized as: \"a word is characterized by the company it keeps\".", "labels": [], "entities": []}, {"text": "To enhance word segmentation, some recent works) made rich resources publicly available.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7640889883041382}]}, {"text": "However, maintaining them up-to-date is difficult and it is infeasible for them to coverall types of words.", "labels": [], "entities": []}, {"text": "To avoid the negative impacts of word segmentation errors, proposed a word embedding method called segmentation-free word embedding (sembei).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7316597700119019}]}, {"text": "The key idea of sembei is to directly embed frequent character n-grams from a raw corpus without conducting word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7013442516326904}]}, {"text": "However, most of the frequent n-grams are non-words (, and hence sembei still suffers from the OOV problems.", "labels": [], "entities": [{"text": "OOV", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.917620837688446}]}, {"text": "The fundamental problem also lies in its extension (, although it uses external resources to reduce the number of OOV.", "labels": [], "entities": []}, {"text": "To handle OOV problems, proposed a novel compositional word embedding method with subword modeling, called subword-information skipgram (sisg).", "labels": [], "entities": []}, {"text": "The key idea of sisg is to extend the notion of vocabulary to include subwords,: A Japanese tweet with manual segmentation.", "labels": [], "entities": []}, {"text": "(a) is the segmentation result of a widely-used word segmenter which conventional word embedding methods are dependent on.", "labels": [], "entities": []}, {"text": "(b) and (c) show the embedding targets and their co-occurrence information to be considered in our proposed method scne on the boundaries of | and |.", "labels": [], "entities": []}, {"text": "Unlike conventional word embedding methods, scne considers all possible character n-grams on all boundaries (e.g., |, |, |, \u00b7 \u00b7 \u00b7 ) in the raw corpus without segmentation.", "labels": [], "entities": []}, {"text": "namely, substrings of words, for enriching the representations of words by the embeddings of its subwords.", "labels": [], "entities": []}, {"text": "In sisg, the embeddings of OOV (or unseen) words are computed from the embedings of their subwords.", "labels": [], "entities": []}, {"text": "However, sisg requires word segmentation as a prepossessing step, and the way of collecting co-occurrence information is dependent on the results of explicit word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7209968715906143}]}, {"text": "For solving the issues of word segmentation and OOV, we propose a simple but effective unsupervised representation learning method for words, phrases and sentences, called segmentation-free compositional n-gram embedding (scne).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7302347123622894}, {"text": "OOV", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8356428146362305}]}, {"text": "The key idea of scne is to train embeddings of character n-grams to compose representations of all character n-grams in a raw corpus, and it enables treating all words, phrases and sentences seamlessly (see for an illustrative explanation).", "labels": [], "entities": []}, {"text": "Our experimental results on a range of datasets suggest that scne can compute high-quality representations for words and sentences although it does not consider any word boundaries and is not dependent on any human annotated resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we perform two intrinsic and two extrinsic tasks at both word and sentence level, focusing on unsegmented languages.", "labels": [], "entities": []}, {"text": "The implementation of our method is available on GitHub 1 .   In the main paper, three movie review datasets are used to evaluate the quality of sentence embeddings.", "labels": [], "entities": []}, {"text": "We used 101,114, 55,837 and 200,000 movie reviews and their rating scores from Yahoo , Yahoo!", "labels": [], "entities": []}, {"text": "23 and Naver Movies for Chinese, Japanese and Korean, respectively.", "labels": [], "entities": [{"text": "Naver Movies", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8768414556980133}]}, {"text": "In this section, we show the results of Japanese word similarity experiments.", "labels": [], "entities": [{"text": "Japanese word similarity", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6001241008440653}]}, {"text": "We use the datasets of.", "labels": [], "entities": []}, {"text": "It contains 4427 pairs of words with human similarity scores.", "labels": [], "entities": []}, {"text": "We omit sentence similarity task since there is no public widely-used benchmark dataset for Japanese yet.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.6873717159032822}]}, {"text": "Following the main paper, given a set of word pairs and their human annotated similarity scores, we calculated Spearman's rank correlation between the cosine similarities of the embeddings and the human scores.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 111, "end_pos": 138, "type": "METRIC", "confidence": 0.6186166554689407}]}], "tableCaptions": [{"text": " Table 1: Spearman rank correlations of the word sim- ilarity task on two different Chinese corpora. Best  scores are boldface and 2nd best scores are underlined.", "labels": [], "entities": []}, {"text": " Table 2: Noun category prediction accuracies (higher is better) and coverages [%] (in parentheses, higher is better).", "labels": [], "entities": [{"text": "Noun category prediction", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6305065552393595}, {"text": "coverages", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9963705539703369}]}, {"text": " Table 3: Sentiment classification accuracies [%].", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9369320869445801}, {"text": "accuracies", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.5914398431777954}]}]}