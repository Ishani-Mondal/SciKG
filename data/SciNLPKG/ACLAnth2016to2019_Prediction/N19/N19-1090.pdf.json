{"title": [{"text": "Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting", "labels": [], "entities": [{"text": "Improved Lexically Constrained Decoding", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8511017560958862}, {"text": "Translation and Monolingual Rewriting", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.6812533065676689}]}], "abstractContent": [{"text": "Lexically-constrained sequence decoding allows for explicit positive or negative phrase-based constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting.", "labels": [], "entities": [{"text": "Lexically-constrained sequence decoding", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7109194000562032}, {"text": "machine translation or monolingual text rewriting", "start_pos": 172, "end_pos": 221, "type": "TASK", "confidence": 0.6775532563527426}]}, {"text": "We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints.", "labels": [], "entities": [{"text": "vectorized dynamic beam allocation", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.5792863816022873}]}, {"text": "Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference , question answering and machine translation , showing improvements in all three.", "labels": [], "entities": [{"text": "question answering", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.8934832811355591}, {"text": "machine translation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.758331298828125}]}], "introductionContent": [{"text": "For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.7521905899047852}]}, {"text": "Examples include terminology databases in Machine Translation (MT), names (and generic responses) in dialogue generation (, objects in image captioning, and facts in abstractive summarization (.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8729798674583436}, {"text": "dialogue generation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7695721685886383}]}, {"text": "One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly.", "labels": [], "entities": []}, {"text": "These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided).", "labels": [], "entities": []}, {"text": "While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward.", "labels": [], "entities": []}, {"text": "We improve upon previous work by vectorizing the dynamic beam allocation (DBA) algorithm from Post and and by incorporating multi-state tries, which track a subset of nodes at each decoding timestep.", "labels": [], "entities": [{"text": "dynamic beam allocation (DBA)", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.7944829265276591}]}, {"text": "These improvements lead to a five-fold speedup in decoding with positive constraints and in some cases better constraint placements (with respect to BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9975409507751465}]}, {"text": "Post and Vilar (2018) motivated the utility of lexically-constrained decoding in MT for scenarios such as interactive translation and domain adaptation.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9686003923416138}, {"text": "interactive translation", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.6528009623289108}, {"text": "domain adaptation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7187945991754532}]}, {"text": "Translation applications handling large amounts of data will clearly benefit from improvements in speed: the same is true for large-scale data augmentation via rewriting.", "labels": [], "entities": [{"text": "speed", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.983649492263794}]}, {"text": "In this case, a practitioner will ideally explore various task-specific rewriting strategies that may lead to improvements as observed during development, and then incorporate the best strategy into a test-final model.", "labels": [], "entities": []}, {"text": "Recently, sentential paraphrasing gained the ability to enforce lexical constraints (), but constrained decoding was still too inefficient to be practical) at a large scale.", "labels": [], "entities": []}, {"text": "Even with the approach described by Post and Vilar, exploring the space of possible rewriting strategies on a taskspecific basis maybe overly time consuming: our performance improvements to their algorithm lowers the barrier of entry, where one may more practically experiment with various strategies during development.", "labels": [], "entities": []}, {"text": "To illustrate our point, we build an improved monolingual sentential rewriter that can be conditioned on arbitrary positive and negative lexical constraints and use this to augment data for three external NLP tasks with different strategies: Natural Language Inference (NLI), Question Answering (QA) and MT.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 276, "end_pos": 299, "type": "TASK", "confidence": 0.8691031217575074}, {"text": "MT", "start_pos": 304, "end_pos": 306, "type": "TASK", "confidence": 0.9826996326446533}]}, {"text": "Our main contributions are: \u2022 A more efficient and robust approach to lexically-constrained decoding with vectorized DBA and trie representations; \u2022 A trained and freely available lexically-constrained monolingual rewriter 1 with improvements in both human-judged semantic similarity and fluency over the initial PARA-BANK rewriter (); \u2022 Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT.", "labels": [], "entities": [{"text": "NLI / QA / MT", "start_pos": 441, "end_pos": 454, "type": "DATASET", "confidence": 0.5937170147895813}]}], "datasetContent": [{"text": "We use SOCKEYE (Hieber et al., 2017) 4 for our evaluations.", "labels": [], "entities": []}, {"text": "We trained a 6-layer German-English Transformer using the default settings on the WMT'18 training data and the newstest2018 test set for evaluation.", "labels": [], "entities": [{"text": "WMT'18 training data", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.9609339833259583}, {"text": "newstest2018 test set", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.940323273340861}]}, {"text": "Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: \"rand3\", which selects 3 random words from the reference, and \"phr4\", which selects a single 4-word phrase.", "labels": [], "entities": []}, {"text": "We report decoding speed (in sentences per second) and BLEU score (), as measured by.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9802184402942657}]}, {"text": "The results are https://github.com/awslabs/sockeye/ shown in.", "labels": [], "entities": []}, {"text": "Our approach is faster than existing approaches when decoding with positive constraints and produces the same or higher BLEU scores, which we take as a sign of more fluent and natural hypotheses under constraints.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9991995692253113}]}, {"text": "Without batching, there is no speedup, but at a batch size of 20, we see roughly a 5\u00d7 speedup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between the baseline implementation (SOCKEYE 1.18.57, commit 59180f3) and our approach  with multi-state tries (MST) and vectorized Dynamic Beam Allocation (VDBA) in placing different constraints  randomly extracted from the reference. All runs use a beam size of 10. The pruning threshold was set to 0 for no  constraints (\"none\") and 30 for \"phr4\" and \"rand3\". Decoding speed is measured on an NVIDIA GTX 1080 Ti in  sentences per second (the higher the better). Output quality is measured using SacreBLEU (the higher the better).", "labels": [], "entities": []}, {"text": " Table 2: Comparison between three monolingual  rewriting systems. Systems will \"alpha\" are trained on  PARABANK alpha, which the other one is trained on  the full data. Similarity is the mean human-judged se- mantic similarity score; the higher the better. STD de- scribed the standard deviation of similarity. Fluency is  the percentage of paraphrases judged to be both gram- matical and meaningful.", "labels": [], "entities": [{"text": "PARABANK", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9338219165802002}, {"text": "STD", "start_pos": 258, "end_pos": 261, "type": "METRIC", "confidence": 0.7657096982002258}, {"text": "Fluency", "start_pos": 312, "end_pos": 319, "type": "METRIC", "confidence": 0.9985679388046265}]}, {"text": " Table 3: F1 scores on MNLI. +Train denotes training  on augmented data; +Agg. denotes using a weighted  aggregation. Scores on the development set are a  weighted average between the matched (m) and mis- matched (mm) portions of the dataset, while the test set  scores are additionally broken down into each category.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992349147796631}, {"text": "MNLI", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9473750591278076}, {"text": "Agg.", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9902726411819458}]}, {"text": " Table 4: Percentage of changed predictions on the  MNLI development set using the baseline model with- out (top) and with (bottom) ELMo. \u2212 \u2192 + (correct af- ter rewrite), + \u2192 \u2212 (originally correct), and \u2212 1 \u2192 \u2212 2  (different incorrect) are changes after rewriting. +Agg.  denotes the predictions after weighted aggregation.", "labels": [], "entities": [{"text": "MNLI development set", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.962739884853363}, {"text": "ELMo", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9861415028572083}, {"text": "Agg.", "start_pos": 266, "end_pos": 270, "type": "METRIC", "confidence": 0.9644784331321716}]}, {"text": " Table 5: Experimental results on QA selection.", "labels": [], "entities": [{"text": "QA selection", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9539845585823059}]}]}