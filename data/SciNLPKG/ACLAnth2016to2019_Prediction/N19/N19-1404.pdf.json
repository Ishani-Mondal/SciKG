{"title": [{"text": "Saliency Learning: Teaching the Model Where to Pay Attention", "labels": [], "entities": [{"text": "Saliency Learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.89398592710495}]}], "abstractContent": [{"text": "Deep learning has emerged as a compelling solution to many NLP tasks with remarkable performances.", "labels": [], "entities": []}, {"text": "However, due to their opacity , such models are hard to interpret and trust.", "labels": [], "entities": []}, {"text": "Recent work on explaining deep models has introduced approaches to provide insights toward the model's behaviour and predictions, which are helpful for assessing the reliability of the model's predictions.", "labels": [], "entities": []}, {"text": "However, such methods do not improve the model's reliability.", "labels": [], "entities": []}, {"text": "In this paper, we aim to teach the model to make the right prediction for the right reason by providing explanation training and ensuring the alignment of the model's explanation with the ground truth explanation.", "labels": [], "entities": []}, {"text": "Our experimental results on multiple tasks and datasets demonstrate the effectiveness of the proposed method, which produces more reliable predictions while delivering better results compared to traditionally trained models.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is unfortunate that our data is often plagued by meaningless or even harmful statistical biases.", "labels": [], "entities": []}, {"text": "When we train a model on such data, it is possible that the classifier focuses on irrelevant biases to achieve high performance on the biased data.", "labels": [], "entities": []}, {"text": "Recent studies demonstrate that deep learning models noticeably suffer from this issue (.", "labels": [], "entities": []}, {"text": "Due to the black-box nature of deep models and the high dimensionality of their inherent representations, it is difficult to interpret and trust their behaviour and predictions.", "labels": [], "entities": []}, {"text": "Recent work on explanation and interpretation has introduced a few approaches () for explanation.", "labels": [], "entities": [{"text": "explanation and interpretation", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.8030746976534525}]}, {"text": "Such methods provide insights toward the model's behaviour, which is helpful for detecting biases in our models.", "labels": [], "entities": []}, {"text": "However, they do not correct them.", "labels": [], "entities": []}, {"text": "Here, we investigate how to incorporate explanations into the learning process to ensure that our model not only makes correct predictions but also makes them for the right reason.", "labels": [], "entities": []}, {"text": "Specifically, we propose to train a deep model using both ground truth labels and additional annotations suggesting the desired explanation.", "labels": [], "entities": []}, {"text": "The learning is achieved via a novel method called saliency learning, which regulates the model's behaviour using saliency to ensure that the most critical factors impacting the model's prediction are aligned with the desired explanation.", "labels": [], "entities": []}, {"text": "Our work is closely related to, which also uses the gradient/saliency information to regularize model's behaviour.", "labels": [], "entities": []}, {"text": "However, we differ in the following points: 1) is limited to regularizing model with gradient of the model's input.", "labels": [], "entities": []}, {"text": "In contrast, we extend this concept to the intermediate layers of deep models, which is demonstrated to be beneficial based on the experimental results; 2) considers annotation at the dimension level, which is not appropriate for NLP tasks since the individual dimensions of the word embeddings are not interpretable; 3) most importantly, learns from annotations of irrelevant parts of the data, whereas we focus on positive annotations identifying parts of the data that contributes positive evidence toward a specific class.", "labels": [], "entities": []}, {"text": "In textual data, it is often unrealistic to annotate a word (even a stop word) to be completely irrelevant.", "labels": [], "entities": []}, {"text": "On the other hand, it can be reasonably easy to identify group of words that are positively linked to a class.", "labels": [], "entities": []}, {"text": "We make the following contributions: 1) we propose anew method for teaching the model whereto pay attention; 2) we evaluate our method on multiple tasks and datasets and demonstrate that our method achieves more reliable predictions while delivering better results than traditionally trained models; 3) we verify the sensitivity of our saliency-trained model to perturbations introduced on part of the data that contributes to the explanation.", "labels": [], "entities": []}], "datasetContent": [{"text": "To teach the model whereto pay attention, we need ground-truth explanation annotation Z, which is difficult to come by.", "labels": [], "entities": []}, {"text": "As a proof of concept, we modify two well known real tasks (Event Extraction and Cloze-Style Question Answering) to simulate approximate annotations for explanation.", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6738581359386444}, {"text": "Question Answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.6607440859079361}]}, {"text": "Details of the main tasks and datasets could be found in section B of the Appendix.", "labels": [], "entities": []}, {"text": "We describe the modified tasks as follows: 1) Event Extraction: Given a sentence, the goal is to determine whether the sentence contains an event.", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.6937770992517471}]}, {"text": "Note that event extraction benchmarks contain the annotation of event triggers, which we use to build the annotation Z.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7894210815429688}]}, {"text": "In particular, the Z value of every word is annotated to be zero unless it belongs to an event trigger.", "labels": [], "entities": []}, {"text": "For this task, we consider two well known event extraction datasets, namely ACE 2005 and Rich ERE 2015.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7103112041950226}, {"text": "ACE 2005", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9632498323917389}, {"text": "Rich ERE 2015", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9207948048909506}]}, {"text": "2) Cloze-Style Question Answering: Given a sentence and a query with a blank, the goal is to determine whether the sentence contains the correct replacement for the blank.", "labels": [], "entities": [{"text": "Cloze-Style Question Answering", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.5353998939196268}]}, {"text": "Here, annotation of each word is zero unless it belongs to the gold Here, we only consider the simple binary tasks as a first attempt to examine the effectiveness of our method.", "labels": [], "entities": [{"text": "annotation", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9653623700141907}]}, {"text": "However, our method is not restricted to binary tasks.", "labels": [], "entities": []}, {"text": "In multi-class problems, each class can be treated as the positive class of the binary classification.", "labels": [], "entities": []}, {"text": "In such a setting, each class would have its own explanation and annotation Z.", "labels": [], "entities": []}, {"text": "Note that for both tasks if an example is negative, its explanation annotation will be all zero.", "labels": [], "entities": []}, {"text": "In other words, for negative examples we have C = L.", "labels": [], "entities": []}, {"text": "Here, we first describe the main and real Event Extraction and Close-Style Question Answering tasks (before our modification).", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6332267969846725}, {"text": "Question Answering", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.685574859380722}]}, {"text": "Next, we provide data statistics of the modified version of ACE, ERE, CBT-NE, and CBT-CN datasets in.", "labels": [], "entities": [{"text": "ACE", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9218840003013611}, {"text": "CBT-NE", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.8533377647399902}, {"text": "CBT-CN datasets", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.9403266310691833}]}, {"text": "\u2022 Event Extraction: Given a set of ontologized event types (e.g. Movement, Transaction, Conflict, etc.), the goal of event extraction is to identify the mentions of different events along with their types from natural texts ().", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.700633704662323}, {"text": "event extraction", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7302082628011703}]}, {"text": "\u2022 Cloze-Style Question Answering: Documents in CBT consist of 20 contiguous sentences from the body of a popular children book and queries are formed by replacing a token from the 21 st sentence with a blank.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6445774286985397}]}, {"text": "Given a document, a query, and a set of candidates, the goal is to find the correct replacement for blank in the query among the given  candidates.", "labels": [], "entities": []}, {"text": "To avoid having too many negative examples in our modified datasets, we only consider sentences that contain at least one candidate.", "labels": [], "entities": []}, {"text": "To be more clear, each sample from the CBT dataset is split to at most 20 samples -each sentence of the main sample as long as it contains one of the candidates).", "labels": [], "entities": [{"text": "CBT dataset", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8932584524154663}]}], "tableCaptions": [{"text": " Table 1: Performance of trained models on multiple  datasets using traditional method and saliency learning.", "labels": [], "entities": []}, {"text": " Table 2: Saliency accuracy of different layer of our  models trained on ACE, ERE, CBT-NE, CBT-CN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9096133708953857}, {"text": "ACE", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7430758476257324}, {"text": "ERE", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.6972618103027344}, {"text": "CBT-NE", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8984516859054565}, {"text": "CBT-CN", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.8749002814292908}]}, {"text": " Table 3: Top 6 salient words visualization of data samples from ACE for the baseline and the saliency-trained  models.", "labels": [], "entities": [{"text": "ACE", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8777086138725281}]}, {"text": " Table 4: True positive rate and true positive rate change  of the trained models before and after removing the  contributory word(s).", "labels": [], "entities": [{"text": "True positive rate", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7502601146697998}, {"text": "true positive rate change", "start_pos": 33, "end_pos": 58, "type": "METRIC", "confidence": 0.6993049010634422}]}, {"text": " Table 5: Dataset statistics of the modified tasks and  datasets.", "labels": [], "entities": []}]}