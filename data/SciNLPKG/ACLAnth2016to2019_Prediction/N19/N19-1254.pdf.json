{"title": [{"text": "A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations", "labels": [], "entities": [{"text": "Disentangling Syntax", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8740725517272949}, {"text": "Sentence Representations", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.705834835767746}]}], "abstractContent": [{"text": "We propose a generative model fora sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics.", "labels": [], "entities": []}, {"text": "We show we can achieve better disentangle-ment between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphras-tic sentences and word-order information.", "labels": [], "entities": []}, {"text": "We also investigate the effect of moving from bag-of-words to recurrent neural network modules.", "labels": [], "entities": []}, {"text": "We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks.", "labels": [], "entities": []}, {"text": "Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "As generative latent variable models, especially of the continuous variety (), have become increasingly important in natural language processing (, there has been increased interest in learning models where the latent representations are disentangled (.", "labels": [], "entities": []}, {"text": "Much of the recent NLP work on learning disentangled representations of text has focused on disentangling the representation of attributes such as sentiment from the representation of content, typically in an effort to better control text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 234, "end_pos": 249, "type": "TASK", "confidence": 0.7372074723243713}]}, {"text": "In this work, we instead focus on learning sentence representations that disentangle the syntax and the semantics of a sentence.", "labels": [], "entities": []}, {"text": "We are moreover interested in disentangling these representa-tions not for the purpose of controlling generation, but for the purpose of calculating semantic or syntactic similarity between sentences (but not both).", "labels": [], "entities": []}, {"text": "To this end, we propose a generative model of a sentence which makes use of both semantic and syntactic latent variables, and we evaluate the induced representations on both standard semantic similarity tasks and on several novel syntactic similarity tasks.", "labels": [], "entities": []}, {"text": "We use a deep generative model consisting of von Mises Fisher (vMF) and Gaussian priors on the semantic and syntactic latent variables (respectively) and a deep bag-of-words decoder that conditions on these latent variables.", "labels": [], "entities": []}, {"text": "Following much recent work, we learn this model by optimizing the ELBO with a VAE-like () approach.", "labels": [], "entities": [{"text": "VAE-like", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8788486123085022}]}, {"text": "Our learned semantic representations are evaluated on the SemEval semantic textual similarity (STS) tasks.", "labels": [], "entities": [{"text": "SemEval semantic textual similarity (STS)", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.8349069697516305}]}, {"text": "Because there has been less work on evaluating syntactic representations of sentences, we propose several new syntactic evaluation tasks, which involve predicting the syntactic analysis of an unseen sentence to be the syntactic analysis of its nearest neighbor (as determined by the latent syntactic representation) in a large set of annotated sentences.", "labels": [], "entities": []}, {"text": "In order to improve the quality and disentanglement of the learned representations, we incorporate simple additional losses in our training, which are designed to force the latent representations to capture different information.", "labels": [], "entities": []}, {"text": "In particular, our semantic multi-task losses make use of aligned paraphrase data, whereas our syntactic multi-task loss makes use of word-order information.", "labels": [], "entities": []}, {"text": "Additionally, we explore different encoder and decoder architectures for learning better syntactic representations.", "labels": [], "entities": []}, {"text": "Experimentally, we find that by training in this way we are able to force the learned represen-tations to capture different information (as measured by the performance gap between the latent representations on each task).", "labels": [], "entities": []}, {"text": "Moreover, we find that we achieve the best performance on all tasks when the learned representations are most disentangled.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlation (%) for STS test sets. bm:  STS benchmark test set. avg: the average of Pearson  correlation for each domain in the STS test sets from  2012 to 2016. Results are in bold if they are high- est in the \"semantic variable\" columns or lowest in the  \"syntactic variable\" columns. \"ALL\" indicates all of  the multi-task losses are used.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9353199601173401}, {"text": "STS benchmark test set", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.8738565295934677}, {"text": "Pearson  correlation", "start_pos": 102, "end_pos": 122, "type": "METRIC", "confidence": 0.9518325328826904}, {"text": "STS test sets from  2012", "start_pos": 146, "end_pos": 170, "type": "DATASET", "confidence": 0.8196804404258728}, {"text": "ALL", "start_pos": 306, "end_pos": 309, "type": "METRIC", "confidence": 0.9929337501525879}]}, {"text": " Table 2: Syntactic similarity evaluations, showing tree edit distance (TED) and labeled F 1 score for constituent  parsing, and accuracy (%) for part-of-speech tagging. Numbers are bolded if they are worst in the \"semantic  variable\" column or best in the \"syntactic variable\" column. \"ALL\" indicates all the multi-task losses are used.", "labels": [], "entities": [{"text": "tree edit distance (TED)", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.7820576628049215}, {"text": "F 1 score", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9662854870160421}, {"text": "constituent  parsing", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.70393306016922}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9995028972625732}, {"text": "part-of-speech tagging", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.7215116918087006}, {"text": "ALL", "start_pos": 287, "end_pos": 290, "type": "METRIC", "confidence": 0.9893603324890137}]}]}