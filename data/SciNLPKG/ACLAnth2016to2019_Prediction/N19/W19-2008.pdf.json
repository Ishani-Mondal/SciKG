{"title": [{"text": "CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense", "labels": [], "entities": [{"text": "CODAH", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8023010492324829}, {"text": "Question Answering", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7084261178970337}, {"text": "Common Sense", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.7190206348896027}]}], "abstractContent": [{"text": "Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense.", "labels": [], "entities": [{"text": "Commonsense reasoning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8251753151416779}]}, {"text": "Recent neu-ral question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks.", "labels": [], "entities": [{"text": "neu-ral question answering", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6092531780401865}]}, {"text": "These systems do not possess human-level commonsense, but are able to exploit limitations of the datasets to achieve human-level scores.", "labels": [], "entities": []}, {"text": "We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing commonsense.", "labels": [], "entities": [{"text": "CODAH dataset", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9557162225246429}]}, {"text": "CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8923318982124329}, {"text": "SWAG dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7932582795619965}]}, {"text": "To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems.", "labels": [], "entities": [{"text": "question acquisition", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7820033431053162}, {"text": "question answering", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7746469378471375}]}, {"text": "Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation).", "labels": [], "entities": []}, {"text": "We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset.", "labels": [], "entities": [{"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.742005318403244}]}, {"text": "We observe a significant gap between human performance, which is 95.3%, and the performance of the best baseline accuracy of 65.3% by the OpenAI GPT model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9861875772476196}, {"text": "OpenAI GPT model", "start_pos": 138, "end_pos": 154, "type": "DATASET", "confidence": 0.944952646891276}]}], "introductionContent": [{"text": "Enabling commonsense reasoning in machines is a longstanding challenge in AI.", "labels": [], "entities": [{"text": "Enabling commonsense reasoning", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7396170496940613}]}, {"text": "The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text.", "labels": [], "entities": [{"text": "commonsense reasoning over text", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.8461467325687408}]}, {"text": "The Situations With Adversarial Generations (SWAG) dataset () introduced a large-scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video.", "labels": [], "entities": [{"text": "commonsense question answering", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.7755675911903381}, {"text": "multiple choice sentence completion questions describing situations", "start_pos": 147, "end_pos": 214, "type": "TASK", "confidence": 0.7935810855456761}]}, {"text": "However, while SWAG was constructed to be resistant to certain baseline algorithms, powerful subsequent methods were able to perform very well on the dataset.", "labels": [], "entities": []}, {"text": "In particular, the development of the transformer architecture () has led to powerful pre-trained language model representations, including the OpenAI Transformer Language Model () and the Bidirectional Encoder Representations from Transformers (BERT) model.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 189, "end_pos": 251, "type": "TASK", "confidence": 0.6962051540613174}]}, {"text": "BERT achieved new state-of-the-art performance on SWAG that exceeded even that of a human expert.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9455855488777161}, {"text": "SWAG", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7360045909881592}]}, {"text": "However, BERT does not possess human-level commonsense in general, as our experiments demonstrate.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9652106165885925}]}, {"text": "It is instead able to exploit regularities in the SWAG dataset to score high.", "labels": [], "entities": [{"text": "SWAG dataset", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8897400200366974}]}, {"text": "This motivates the construction of additional datasets that pose new challenges, and serve as more reliable benchmarks for commonsense reasoning systems.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.8131109774112701}]}, {"text": "In this work, we introduce the COmmonsense Dataset Adversarially-authored by Humans (CODAH) for commonsense question answering in the style of SWAG multiple choice sentence completion.", "labels": [], "entities": [{"text": "COmmonsense Dataset Adversarially-authored by Humans (CODAH)", "start_pos": 31, "end_pos": 91, "type": "DATASET", "confidence": 0.8728303052484989}, {"text": "commonsense question answering", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.731495996316274}, {"text": "SWAG multiple choice sentence completion", "start_pos": 143, "end_pos": 183, "type": "TASK", "confidence": 0.6589920878410339}]}, {"text": "We propose a novel method for question generation, in which human annotators are educated on the workings of a state-of-the-art question answering model, and are asked to submit questions that adversarially target the weaknesses.", "labels": [], "entities": [{"text": "question generation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8702002167701721}, {"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7390017807483673}]}, {"text": "Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine-tuning on a sample of the submitted questions, encouraging the creation of questions that are not easily learnable.", "labels": [], "entities": []}, {"text": "We experimentally demonstrate that CODAH's generation procedure produces a dataset with a large gap between system performance and human expert accuracy, even when using state-ofthe-art pre-trained language models with and without fine-tuning on the large SWAG dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9200989007949829}, {"text": "SWAG dataset", "start_pos": 256, "end_pos": 268, "type": "DATASET", "confidence": 0.8467642962932587}]}, {"text": "Using a model initially fine-tuned on SWAG, we find that the OpenAI GPT-1 and BERT neural question answering models yield 65.3% and 64.5% accuracy, respectively, on the CODAH dataset in cross-validation.", "labels": [], "entities": [{"text": "OpenAI GPT-1", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9110954403877258}, {"text": "BERT neural question answering", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.7276540771126747}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9992824196815491}, {"text": "CODAH dataset", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.9793543815612793}]}, {"text": "Thus, cross-validating on CO-DAH can form a challenging additional evaluation for SWAG-style commonsense QA systems.", "labels": [], "entities": [{"text": "SWAG-style commonsense QA", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.573825071255366}]}, {"text": "Human evaluators achieve 95.3% accuracy, which is substantially higher than the 85.0% () and 87.7% ( human performance on the SWAG and SNLI natural language inference tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9992202520370483}, {"text": "SNLI natural language inference tasks", "start_pos": 135, "end_pos": 172, "type": "TASK", "confidence": 0.5713648557662964}]}, {"text": "The high human performance suggests that answers to the CODAH questions are in fact commonsense knowledge.", "labels": [], "entities": [{"text": "CODAH questions", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.8025249242782593}]}, {"text": "Finally, we also analyze differences in performance across questions that target different types of commonsense reasoning, including quantitative, negation, and object reference, showing consistency in performance for BERT and GPT on the proposed categories.", "labels": [], "entities": [{"text": "BERT", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.994860053062439}, {"text": "GPT", "start_pos": 227, "end_pos": 230, "type": "DATASET", "confidence": 0.8067770004272461}]}], "datasetContent": [{"text": "Our dataset contains multiple choice sentence completion questions in the format of the SWAG dataset.", "labels": [], "entities": [{"text": "multiple choice sentence completion questions", "start_pos": 21, "end_pos": 66, "type": "TASK", "confidence": 0.7231849312782288}, {"text": "SWAG dataset", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9132937788963318}]}, {"text": "Examples of the questions are shown in.", "labels": [], "entities": []}, {"text": "Each question consists of a prompt sentence, the subject of the subsequent sentence, and four candidate completions, such that exactly one candidate completion is consistent with commonsense.", "labels": [], "entities": []}, {"text": "This task definition allows for easy evaluation by many state-of-the-art models, such as BERT and GPT-1, and enables us to utilize the large SWAG dataset for pre-training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9881958961486816}, {"text": "GPT-1", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9021986722946167}, {"text": "SWAG dataset", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.7937602698802948}]}, {"text": "The full dataset is available at https://github.com/ Websail-NU/CODAH.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.6071971654891968}]}, {"text": "We evaluate the dataset on state-of-the-art neural question answering systems built on the BERT and GPT-1 architecture and provide multiple baselines.", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7220168709754944}, {"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9665883779525757}]}, {"text": "The models and experiment setups are discussed below.", "labels": [], "entities": []}, {"text": "We also analyze the questions to identify distinctive categories of commonsense reasoning that provide a finer-grained understanding of model performances.", "labels": [], "entities": []}, {"text": "In addition, the ablation experiments on dataset size and the use of fine-tuning on SWAG data allow us to further un-derstand the impact of the relatively small size of CODAH.", "labels": [], "entities": [{"text": "ablation", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.946062445640564}, {"text": "SWAG data", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.7400733828544617}, {"text": "CODAH", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.9043688774108887}]}, {"text": "We evaluate the models on several different train and test configurations described below.", "labels": [], "entities": []}, {"text": "The CO-DAH dataset is evaluated in 5-fold stratified crossvalidation which balances the distribution of question categories in each fold.", "labels": [], "entities": [{"text": "CO-DAH dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8222635984420776}]}, {"text": "\u2022 CODAH: Cross-validation fine-tuning on the CODAH dataset.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.8043608665466309}, {"text": "CODAH dataset", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.977595865726471}]}, {"text": "The CODAH 80% experiment represents the standard cross-validation setting on the full dataset, training on 80% of the data in each fold and evaluating on the remaining 20%.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.48685410618782043}]}, {"text": "The 60%, 40% and 20% ablation experiments are trained on a smaller portion of the CODAH dataset for each fold, but are evaluated in on the same test set which consists of 20% of the full dataset.", "labels": [], "entities": [{"text": "CODAH dataset", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9720974266529083}]}, {"text": "The question categories are balanced in both training set and test set.", "labels": [], "entities": []}, {"text": "This makes the results from the experiments more comparable with each other.", "labels": [], "entities": []}, {"text": "Three trials are conducted for all settings; the mean and standard deviation of the model accuracy are reported in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9778324961662292}]}, {"text": "\u2022 SWAG+CODAH: Fine-tuned on SWAG first, then fine-tuned again in cross-validation on CODAH.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.586772084236145}, {"text": "CODAH", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9361402988433838}]}, {"text": "Ablation experiments are conducted in the same way as in the CODAHonly setting above, with the same dataset splits for training.", "labels": [], "entities": [{"text": "CODAHonly setting", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9232018887996674}]}, {"text": "The mean and standard deviation of the three trials are reported in.", "labels": [], "entities": []}, {"text": "\u2022 SWAG only: Fine-tuned on SWAG and evaluated on CODAH.", "labels": [], "entities": [{"text": "SWAG", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.841399073600769}, {"text": "CODAH", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8352422118186951}]}, {"text": "Only one trial is conducted.", "labels": [], "entities": []}, {"text": "\u2022 Answer only: Cross-validation fine-tuning on the full CODAH dataset with the questions left blank (in both training and testing).", "labels": [], "entities": [{"text": "Answer", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9161443710327148}, {"text": "CODAH dataset", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9538012742996216}]}, {"text": "Only one trial is conducted.", "labels": [], "entities": []}, {"text": "Results for the above configurations are shown in.", "labels": [], "entities": []}, {"text": "As a baseline, we evaluate both models on the full SWAG training and validation sets, providing an accuracy of 83.7% on BERT and 80.2% on GPT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9996603727340698}, {"text": "BERT", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9968066215515137}, {"text": "GPT", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.4894443452358246}]}, {"text": "To adjust for the difference in size between our dataset and SWAG, we also train the models on a sample of 2,241 SWAG questions (the size of the training set in each of CODAH's cross-validation folds) and evaluate them on the full SWAG validation set.", "labels": [], "entities": [{"text": "SWAG validation set", "start_pos": 231, "end_pos": 250, "type": "DATASET", "confidence": 0.6316401660442352}]}, {"text": "This produces an accuracy of 28.7% for BERT and 63.6% for GPT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9997301697731018}, {"text": "BERT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9974216222763062}, {"text": "GPT", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.665346622467041}]}, {"text": "For each category, we measure the accuracy of the BERT and GPT models trained on SWAG+CODAH.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.999607264995575}, {"text": "BERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.996893048286438}, {"text": "GPT", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.5416525602340698}, {"text": "SWAG+CODAH", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.5979017217954}]}, {"text": "We also measure human accuracy as a baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9953888654708862}]}, {"text": "Human accuracy was calculated as the mean accuracy of three human annotators, covering 707 dataset questions in total.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9896833896636963}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8187849521636963}]}, {"text": "Human annotators answered 95.3% of questions correctly, presenting a 7-fold reduction in error compared to the fine-turned BERT model.", "labels": [], "entities": [{"text": "error", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9969075322151184}, {"text": "BERT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9749680161476135}]}, {"text": "Interannotator agreement was computed over a set of 50 additional questions with a pairwise average Cohen-Kappa score of 0.89, which is interpreted as almost perfect agreement by some guidelines.", "labels": [], "entities": []}, {"text": "displays the accuracy of the human annotators and neural networks on each category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994329810142517}]}, {"text": "Our experiments show that CODAH forms a challenging extension to the existing SWAG dataset.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.6853571534156799}, {"text": "SWAG dataset", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.8615425825119019}]}, {"text": "Even when we train a system to perform near human-level on SWAG, and then fine-tune on CO-DAH, the system still struggles to answer CO-DAH questions correctly.", "labels": [], "entities": []}, {"text": "However, CODAH is also smaller than SWAG.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.669868528842926}, {"text": "SWAG", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7906016707420349}]}, {"text": "Our results do not suggest that CODAH questions are more difficult than SWAG questions if dataset size is equalized.", "labels": [], "entities": []}, {"text": "When we restrict to a subset of SWAG of the same number of questions as CODAH, we find that SWAG has comparable accuracy for GPT (63.6% on reduced-size SWAG vs 62.4% for CODAH) and much lower accuracy for BERT (28.7% vs 49.6%).", "labels": [], "entities": [{"text": "CODAH", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8917293548583984}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9993225336074829}, {"text": "GPT", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.7804535031318665}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9991959929466248}, {"text": "BERT", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9957712292671204}]}, {"text": "This shows that CODAH questions are distinct from and complementary to SWAG questions, but taken in isolation are not necessarily more challenging.", "labels": [], "entities": []}, {"text": "Our results suggest two recommendations for dataset construction which we hope to evaluate in future work.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7289101928472519}]}, {"text": "The first is, rather than using a single protocol to collect one monolithic dataset, the community maybe able to obtain more challenging data by aggregating a variety of distinct, independently-gathered datasets that follow a similar format.", "labels": [], "entities": []}, {"text": "For example, pre-training on SWAG and evaluating on CODAH forms a more challenging benchmark than training and testing on SWAG alone.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.938916027545929}]}, {"text": "Secondly, if we wish to use our adversarial collection approach to grow CODAH to tens of thousands of examples, we should update our system as new data arrives, so that contributors are able to tune their questions to remain difficult for the strongest, most up-to-date version of the system.", "labels": [], "entities": []}, {"text": "Under such a data collection scheme, we may need to increase the reward for fooling the model in cross-validation compared to that for fooling the current model (whereas, these two rewards were equal in CODAH), in order to disincentivize adversarial attacks that manipulate the current model to make it easy to fool on subsequent questions.", "labels": [], "entities": [{"text": "CODAH", "start_pos": 203, "end_pos": 208, "type": "DATASET", "confidence": 0.6273898482322693}]}, {"text": "Our experiments on different sizes of CO-DAH produce very different results for BERT and GPT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9985792636871338}, {"text": "GPT", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.5134075880050659}]}, {"text": "Unsurprisingly, GPT performance improves with more data on both the CODAH-only and SWAG+CODAH experiments, with the rate of improvement slowing down as data size increases.", "labels": [], "entities": [{"text": "GPT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9143218994140625}, {"text": "CODAH-only and SWAG+CODAH experiments", "start_pos": 68, "end_pos": 105, "type": "DATASET", "confidence": 0.6034079094727834}]}, {"text": "However, the BERT results are more challenging to interpret.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9876575469970703}]}, {"text": "On the CODAH-only setting, BERT appears to improve with data size, but the extremely high variance prevents us from being certain of any trend in BERT's performance on this setting.", "labels": [], "entities": [{"text": "CODAH-only setting", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.9107300341129303}, {"text": "BERT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9975709319114685}, {"text": "BERT", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9357410669326782}]}, {"text": "The variance is lower in the SWAG+CODAH setting and accuracy increases as data size goes from 20% to 60%, but accuracy decreases between SWAG+CODAH-60% and SWAG+CODAH-80% settings (although the SWAG+CODAH-80% setting has high variance and the true mean maybe higher).", "labels": [], "entities": [{"text": "SWAG+CODAH setting", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.6051345318555832}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994933605194092}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.999263346195221}]}, {"text": "The inconsistency in improvement with more CODAH data after training on SWAG+CODAH-60% for BERT and the reduced rate of performance gain for GPT suggest that it is unclear whether the performance of all models will improve dramatically with an even larger CODAH dataset size.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9812421798706055}, {"text": "GPT", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.7701823115348816}, {"text": "CODAH dataset", "start_pos": 256, "end_pos": 269, "type": "DATASET", "confidence": 0.7263454496860504}]}], "tableCaptions": [{"text": " Table 1. Four human  annotators (the authors) categorized the questions,  and we calculated a Feiss' Kappa score of 0.63 be- tween the annotators over an additional 50 ques- tions.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of question categories.", "labels": [], "entities": [{"text": "Distribution of question categories", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8289893716573715}]}, {"text": " Table 3. As a baseline, we evaluate both models  on the full SWAG training and validation sets, pro- viding an accuracy of 83.7% on BERT and 80.2%  on GPT. To adjust for the difference in size be- tween our dataset and SWAG, we also train the  models on a sample of 2,241 SWAG questions  (the size of the training set in each of CODAH's  cross-validation folds) and evaluate them on the  full SWAG validation set. This produces an accu- racy of 28.7% for BERT and 63.6% for GPT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9992851614952087}, {"text": "BERT", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9856041669845581}, {"text": "GPT", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.7849221229553223}, {"text": "CODAH", "start_pos": 330, "end_pos": 335, "type": "DATASET", "confidence": 0.8776435256004333}, {"text": "SWAG validation set", "start_pos": 394, "end_pos": 413, "type": "DATASET", "confidence": 0.6503304541110992}, {"text": "accu- racy", "start_pos": 432, "end_pos": 442, "type": "METRIC", "confidence": 0.9689383308092753}, {"text": "BERT", "start_pos": 456, "end_pos": 460, "type": "METRIC", "confidence": 0.9631536602973938}, {"text": "GPT", "start_pos": 475, "end_pos": 478, "type": "DATASET", "confidence": 0.8296219110488892}]}, {"text": " Table 3: Accuracy of BERT and GPT on different  training settings when tested on CODAH. Numbers in  parentheses represent the standard deviation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991457462310791}, {"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9976093769073486}, {"text": "GPT", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9906450510025024}, {"text": "CODAH", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9062983989715576}]}, {"text": " Table 4: Class-wise and overall accuracy of human  annotators and neural network models, sorted by BERT  performance on the proposed categories. Numbers in  parentheses represent the standard deviation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9995013475418091}, {"text": "BERT", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9991602897644043}]}]}