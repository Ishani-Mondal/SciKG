{"title": [{"text": "Typological Features for Multilingual Delexicalised Dependency Parsing", "labels": [], "entities": [{"text": "Multilingual Delexicalised Dependency Parsing", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.554194949567318}]}], "abstractContent": [{"text": "The existence of universal models to describe the syntax of languages has been debated for decades.", "labels": [], "entities": []}, {"text": "The availability of resources such as the Universal Dependencies treebanks and the World Atlas of Language Structures make it possible to study the plausibility of universal grammar from the perspective of dependency parsing.", "labels": [], "entities": [{"text": "World Atlas of Language Structures", "start_pos": 83, "end_pos": 117, "type": "DATASET", "confidence": 0.9391136050224305}, {"text": "dependency parsing", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.8091853260993958}]}, {"text": "Our work investigates the use of high-level language descriptions in the form of typological features for multilingual dependency parsing.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.6213011741638184}]}, {"text": "Our experiments on multilingual parsing for 40 languages show that typo-logical information can indeed guide parsers to share information between similar languages beyond simple language identification.", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.6686376631259918}, {"text": "language identification", "start_pos": 178, "end_pos": 201, "type": "TASK", "confidence": 0.7708491384983063}]}], "introductionContent": [{"text": "Human languages may share some syntactic features, but differ on others.", "labels": [], "entities": []}, {"text": "For example, some languages tend to place the subject before the verb (e.g., English) whereas others favour the reverse order (e.g., Arabic), and some do not exhibit a clear preference (e.g., Polish).", "labels": [], "entities": []}, {"text": "These features can be viewed as the parameters of a language's syntax.", "labels": [], "entities": []}, {"text": "When training a multilingual parser, it could be interesting to explicitly represent these parameters, and to integrate them into the parsing model.", "labels": [], "entities": []}, {"text": "If a successful strategy to do so was found, then, a parser could be trained simultaneously on several languages whose syntactic parameters have been explicitly represented.", "labels": [], "entities": []}, {"text": "Such parser could then use a single model to parse texts in any language with known syntactic parameters.", "labels": [], "entities": []}, {"text": "In theory, if we had at our disposal a set of parameters that completely describes the syntax of languages as well as treebanks that explore the whole space of parameters and their values, then such a universal parser could be designed.", "labels": [], "entities": []}, {"text": "To make such a program realistic, though, several issues have to be addressed.", "labels": [], "entities": []}, {"text": "In this paper, we propose to study the feasibility of learning such multilingual parser by addressing some of these issues.", "labels": [], "entities": []}, {"text": "The first one is the choice of syntactic parameters that will be used.", "labels": [], "entities": []}, {"text": "In our work, we approximate these parameters by extracting syntactic information from the World Atlas of Language Structures (WALS).", "labels": [], "entities": [{"text": "World Atlas of Language Structures (WALS)", "start_pos": 90, "end_pos": 131, "type": "DATASET", "confidence": 0.9462218284606934}]}, {"text": "1 A language is represented by a vector containing the values it selects in the WALS.", "labels": [], "entities": []}, {"text": "This vector plays the role of the parameters mentioned above.", "labels": [], "entities": []}, {"text": "The second issue is the design of a unified scheme for representing syntax.", "labels": [], "entities": []}, {"text": "Our natural choice is the Universal Dependencies (UD) initiative.", "labels": [], "entities": [{"text": "Universal Dependencies (UD)", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.5666360497474671}]}, {"text": "UD specifically proposes a set of universal dependency relations, part-of-speech tags and morphological features).", "labels": [], "entities": [{"text": "UD", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7159643769264221}]}, {"text": "The UD treebanks are available for many languages, annotated according to common guidelines.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8713584840297699}]}, {"text": "The third issue is the lexicon.", "labels": [], "entities": []}, {"text": "UD proposes a common language for describing languages' morpho-syntax, but we do not dispose of a \"universal lexicon\" to which we can map the lexical units of different languages.", "labels": [], "entities": [{"text": "UD", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8171635270118713}]}, {"text": "The solution adopted in this work is to resort to delexicalised parsing.", "labels": [], "entities": []}, {"text": "This technique consists in ignoring the lexicon when training a parser.", "labels": [], "entities": []}, {"text": "Such impoverishment of the data leads to less accurate parsers, but offers a simple solution to the lexicon issue.", "labels": [], "entities": []}, {"text": "Using an alternative solution for representing words in different languages, such as multilingual word embeddings, would have introduced in our experimental setting some biases that are difficult to assess and would have prevented to measure the precise influence of the typological features on the behaviour of the parser.", "labels": [], "entities": []}, {"text": "The fourth issue concerns the parser, which must be language independent and produce syntactic trees based on combinations of parameter values and sentential configurations.", "labels": [], "entities": []}, {"text": "We use a transition-based parser with a multi-layer perceptron classifier), responsible for proposing how parameter values match observable patterns in the data.", "labels": [], "entities": []}, {"text": "Our research hypotheses are: (a) features derived from the WALS enable cross-lingual sharing in multilingual parsing, and (b) these features do more than acting as mere language identifiers.", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.5988270491361618}]}, {"text": "Our main contributions are to reassess the utility of the WALS as informant of typological features of parsed languages, to evaluate their benefit in a controlled multilingual setting with full supervision, and to perform a set of analyses to better understand how they interact with the parser model.", "labels": [], "entities": []}, {"text": "In addition to multilingual parsing, our method is suitable for zero-shot learning for under-resourced languages.", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.5725512206554413}]}, {"text": "After discussing related work (Sec. 2), we describe UD (Sec. 3), the WALS (Sec. 4) and our parser (Sec. 5).", "labels": [], "entities": []}, {"text": "The experimental setup (Sec. 6) precedes our results (Sec. 7), analyses (Sec. 8) and conclusions (Sec. 9).", "labels": [], "entities": []}], "datasetContent": [{"text": "Corpora Our experiments were performed on the CoNLL 2017 shared task data (), on gold tokenisation and ignoring contractions (i.e., ranges).", "labels": [], "entities": [{"text": "CoNLL 2017 shared task data", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.9439129590988159}]}, {"text": "We evaluate our models individually on each of the 40 languages for which we have a W (l) vector (section 4), using the original CoNLL 2017 shared task test sets.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task test sets", "start_pos": 129, "end_pos": 161, "type": "DATASET", "confidence": 0.9277404844760895}]}, {"text": "The test corpora for each language are simply the concatenation of all test treebanks for that language.", "labels": [], "entities": []}, {"text": "Training and development are performed on multilingual corpora (henceforth TRAIN-ML and DEV-ML) derived from the training and development treebanks of 37 UD languages.", "labels": [], "entities": [{"text": "TRAIN-ML", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9596844911575317}]}, {"text": "The UD training and development corpora have different sizes for different languages, ranging from 529 words for Kazakh (kk) to 1,842,867 for Czech (cs).", "labels": [], "entities": []}, {"text": "Thus, simply concatenating all corpora to constitute TRAIN-ML and DEV-ML would overrepresent certain languages and possibly bias the parser towards them.", "labels": [], "entities": []}, {"text": "This is why we have decided to balance TRAIN-ML and DEV-ML across languages.", "labels": [], "entities": [{"text": "TRAIN-ML", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9439020156860352}]}, {"text": "First, all available training and development corpora of the 37 languages have been concatenated.", "labels": [], "entities": []}, {"text": "From this large corpus, we build two new intermediate corpora, PRE-TRAIN-ML and PRE-DEV-ML, with each sentence having 90% chances to belong to PRE-TRAIN-ML, and 10% chances Our parser cannot predict non projective trees, systematically generating a wrong parse attest time.", "labels": [], "entities": []}, {"text": "The average non projectivity rate of the test corpora is equal to 1%, with a standard deviation of 1% among the 40 languages.", "labels": [], "entities": []}, {"text": "We ran some tests with pseudo projective tree transformation), but it had a negligible impact on the results, so we have decided to keep the original projective algorithm.", "labels": [], "entities": []}, {"text": "Three languages among our 40 target languages have no corresponding training nor development data (bxr, kmr, sme).", "labels": [], "entities": []}, {"text": "Second, we build TRAIN-ML (respectively DEV-ML) by randomly selecting sentences from PRE-TRAIN-ML (resp.", "labels": [], "entities": []}, {"text": "PRE-DEV-ML) until the number of tokens exceeds 20,000 (resp. 2,000 for DEV-ML) per language.", "labels": [], "entities": []}, {"text": "At the end, we shuffle the selected sentences to obtain the final training and development corpora TRAIN-ML and DEV-ML.", "labels": [], "entities": [{"text": "TRAIN-ML", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9921954870223999}, {"text": "DEV-ML", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.7105920910835266}]}, {"text": "Using this procedure, the same sentence can appear several times in a corpus.", "labels": [], "entities": []}, {"text": "Nonetheless, this method guarantees a balanced representation of every language in TRAIN-ML and DEV-ML.", "labels": [], "entities": [{"text": "DEV-ML", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9007605314254761}]}, {"text": "Metrics The quality of the predicted trees is assessed with a standard measure for dependency parsing: labelled attachment score (LAS).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7541263997554779}, {"text": "labelled attachment score (LAS)", "start_pos": 103, "end_pos": 134, "type": "METRIC", "confidence": 0.9290574292341868}]}, {"text": "We report LAS per language, as well as MACRO-LAS which is the macro-average of LAS on all languages that have a training set.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9714487791061401}, {"text": "MACRO-LAS", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.6693517565727234}]}, {"text": "This measure is therefore independent of the size of the test corpus of each language, and is not biased towards over-represented languages in the test sets.", "labels": [], "entities": []}, {"text": "12 Using the CoNLL shared task 2017 evaluation script.", "labels": [], "entities": [{"text": "CoNLL shared task 2017 evaluation script", "start_pos": 13, "end_pos": 53, "type": "DATASET", "confidence": 0.8708199063936869}]}, {"text": "The decision to use gold POS tags and morphological features may seem unrealistic.", "labels": [], "entities": []}, {"text": "This article is the first step of a process in which we intend to predict the POS tags and the morphological features in the same fashion.", "labels": [], "entities": []}, {"text": "\u03a3 W N , \u03a3 W 80 : Multilingual corpus + WALS.", "labels": [], "entities": []}, {"text": "Two EXTENDED parsers are trained on the TRAIN-ML corpus, with W N (resp.", "labels": [], "entities": [{"text": "TRAIN-ML corpus", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7758463025093079}]}, {"text": "W 80 ) vectors derived from the WALS attached to each word.", "labels": [], "entities": [{"text": "WALS", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.619753360748291}]}], "tableCaptions": [{"text": " Table 1: MID values of typological language genus  compared to Random. Random MID is the average of  the MID for 50,000 sets of 6 languages randomly se- lected.", "labels": [], "entities": [{"text": "MID", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9617128968238831}]}, {"text": " Table 2. We comment below the re- sults for L, and compare the results of meaningful  pairs of experiments, summarised in", "labels": [], "entities": [{"text": "re- sults", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9478465716044108}]}, {"text": " Table 2: LAS for each language and MACRO LAS,  for the five configurations. Languages followed by a  S belong to the Slavic genus, G belong to the German  genus and R belong to the Romance genus.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9916649460792542}, {"text": "MACRO LAS", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9322566986083984}]}, {"text": " Table 3: Differences between X and Y configurations:  average (X \u2212 Y ), standard deviation (\u03c3), minimum  and maximum with corresponding languages.", "labels": [], "entities": [{"text": "standard deviation (\u03c3)", "start_pos": 73, "end_pos": 95, "type": "METRIC", "confidence": 0.9694544911384583}]}, {"text": " Table 4: Language identification accuracy for a logistic  regression classifier trained on the activations after the  hidden layer, or at the input. The classifier is trained on  the development set, results are reported on the test set.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7231818735599518}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9611761569976807}]}, {"text": " Table 5: Neuron-level JSD statistics between activa- tions at the hidden layer of the parser models for se- lected pairs of languages.", "labels": [], "entities": [{"text": "JSD", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8378111124038696}]}]}