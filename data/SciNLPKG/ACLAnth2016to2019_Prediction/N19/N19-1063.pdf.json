{"title": [{"text": "On Measuring Social Biases in Sentence Encoders", "labels": [], "entities": [{"text": "Measuring Social Biases in Sentence Encoders", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.8217481970787048}]}], "abstractContent": [{"text": "The Word Embedding Association Test shows that GloVe and word2vec word embed-dings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017).", "labels": [], "entities": []}, {"text": "Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption.", "labels": [], "entities": []}, {"text": "Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders.", "labels": [], "entities": []}, {"text": "We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level.", "labels": [], "entities": [{"text": "BERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9959561228752136}]}, {"text": "We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general.", "labels": [], "entities": []}, {"text": "We conclude by proposing directions for future work on measuring bias in sentence encoders.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings quickly achieved wide adoption in natural language processing (NLP), precipitating the development of efficient, word-level neural models of human language.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7913261353969574}]}, {"text": "However, prominent word embeddings such as word2vec () and GloVe () encode systematic biases against women and black people (, i.a.), implicating many NLP systems in scaling up social injustice.", "labels": [], "entities": []}, {"text": "We investigate whether sentence encoders, which extend the word embedding approach to sentences, are similarly biased.", "labels": [], "entities": []}, {"text": "The previously developed Word Embedding Association Test measures bias in word embeddings by comparing two sets of target-concept words to two sets of attribute words.", "labels": [], "entities": []}, {"text": "We propose a simple generaliza-1 While encoder training data may contain perspectives from outside the U.S., we focus on biases in U.S. contexts.", "labels": [], "entities": []}, {"text": "tion of WEAT to phrases and sentences: the Sentence Encoder Association Test (SEAT).", "labels": [], "entities": [{"text": "WEAT", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9426959156990051}]}, {"text": "We apply SEAT to sentences generated by inserting individual words from Caliskan et al.'s tests into simple templates such as \"This is a <word>.\"", "labels": [], "entities": []}, {"text": "To demonstrate the new potential of a sentencelevel approach and advance the discourse on bias in NLP, we also introduce tests of two biases that are less amenable to word-level representation: the angry black woman stereotype) and a double bind on women in professional settings).", "labels": [], "entities": []}, {"text": "The use of sentence-level contexts also facilitates testing the impact of different experimental designs.", "labels": [], "entities": []}, {"text": "For example, several of Caliskan et al.'s tests rely on given names associated with European American and African American people or rely on terms referring to women and men as groups (such as \"woman\" and \"man\").", "labels": [], "entities": []}, {"text": "We explore the effect of using given names versus group terms by creating alternate versions of several bias tests that swap the two.", "labels": [], "entities": []}, {"text": "This is not generally feasible with WEAT, as categories like African Americans lack common single-word group terms.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.5881075859069824}]}, {"text": "We find varying evidence of human-like bias in sentence encoders using SEAT.", "labels": [], "entities": [{"text": "SEAT", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.7356299161911011}]}, {"text": "Sentence-tovector encoders largely exhibit the angry black woman stereotype and Caliskan biases, and to a lesser degree the double bind biases.", "labels": [], "entities": []}, {"text": "Recent sentence encoders such as BERT ( display limited evidence of the tested biases.", "labels": [], "entities": [{"text": "BERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9679237604141235}]}, {"text": "However, while SEAT can confirm the existence of bias, negative results do not indicate the model is bias-free.", "labels": [], "entities": [{"text": "SEAT", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.5445476770401001}]}, {"text": "Furthermore, discrepancies in the results suggest that the confirmed biases may not generalize beyond the specific words and sentences in our test data, and in particular that cosine similarity may not be a suitable measure of representational similarity in recent models, indicating a need for alternate bias detection techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply SEAT to seven sentence encoders (listed in) including simple bag-of-words encoders, sentence-to-vector models, and state-ofthe-art sequence models.", "labels": [], "entities": [{"text": "SEAT", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.6916725635528564}]}, {"text": "For all models, we use publicly available pretrained parameters.", "labels": [], "entities": []}, {"text": "shows effect size and significance at 0.01 before and after applying the HolmBonferroni multiple testing correction) fora subset of tests and models; complete results are provided in the supplement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: SEAT effect sizes for select tests, including word-level (word), bleached sentence-level (sent), and un- bleached sentence-level (sent (u)) versions. CN : test from Caliskan et al. (2017, Table 1) row N ; *: significant at  0.01, **: significant at 0.01 after multiple testing correction.", "labels": [], "entities": [{"text": "SEAT effect sizes", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7053148746490479}]}]}