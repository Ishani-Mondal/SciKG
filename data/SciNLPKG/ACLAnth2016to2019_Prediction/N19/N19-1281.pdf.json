{"title": [{"text": "Shrinking Japanese Morphological Analyzers With Neural Networks and Semi-supervised Learning", "labels": [], "entities": [{"text": "Shrinking Japanese Morphological Analyzers", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8776431083679199}]}], "abstractContent": [{"text": "For languages without natural word boundaries , like Japanese and Chinese, word seg-mentation is a prerequisite for downstream analysis.", "labels": [], "entities": []}, {"text": "For Japanese, segmentation is often done jointly with part of speech tagging, and this process is usually referred to as morphological analysis.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.739813357591629}, {"text": "morphological analysis", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7571765780448914}]}, {"text": "Morphological analyzers are trained on data hand-annotated with segmenta-tion boundaries and part of speech tags.", "labels": [], "entities": [{"text": "Morphological analyzers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.87789186835289}]}, {"text": "A seg-mentation dictionary or character n-gram information is also provided as additional inputs to the model.", "labels": [], "entities": []}, {"text": "Incorporating this extra information makes models large.", "labels": [], "entities": []}, {"text": "Modern neural morphological analyzers can consume gigabytes of memory.", "labels": [], "entities": [{"text": "neural morphological analyzers", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.6730455458164215}]}, {"text": "We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations.", "labels": [], "entities": []}, {"text": "The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network , and independently infers both segmen-tation and part of speech information.", "labels": [], "entities": []}, {"text": "The model is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data.", "labels": [], "entities": []}, {"text": "Our model itself is significantly smaller than the dictionary-based one: it uses less than 15 megabytes of space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Languages with a continuous script, like Japanese and Chinese, do not have natural word boundaries inmost cases.", "labels": [], "entities": []}, {"text": "Natural language processing for such languages requires to perform some variation of word segmentation.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6864949464797974}, {"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7226857095956802}]}, {"text": "Although some NLP applications, like neural machine translation, started to use unsuper- vised segmentation methods (, resulting segmentation often has decisions which are not natural to humans.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.6872947017351786}]}, {"text": "Supervised segmentation based on a human-defined standard is essential for applications which are designed for interaction on a word-level granularity, for example, full-text search.", "labels": [], "entities": []}, {"text": "Segmentation is commonly done jointly with part of speech (POS) tagging and usually referred to as Morphological Analysis.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9675355553627014}, {"text": "part of speech (POS) tagging", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6818022983414787}, {"text": "Morphological Analysis", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7906933426856995}]}, {"text": "Modern Japanese Morphological Analyzers (MA) are very accurate, having a >99 segmentation tokenwise F1 score on news domain and a >98.5 F1 on web domain ().", "labels": [], "entities": [{"text": "Japanese Morphological Analyzers (MA)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.6898604432741801}, {"text": "F1 score", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9596931338310242}, {"text": "F1", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9586438536643982}]}, {"text": "They often use segmentation dictionaries which define possible words.", "labels": [], "entities": []}, {"text": "Also, their models are generally large and unwieldy, spanning hundreds of megabytes in case of traditional symbolic featurebased approaches.", "labels": [], "entities": []}, {"text": "Neural models with word or n-gram embeddings are even larger, easily reaching gigabytes.", "labels": [], "entities": []}, {"text": "This makes it difficult to deploy MA in space-constrained environments such as mobile applications and browsers.", "labels": [], "entities": []}, {"text": "It has been shown that simple or straightforward models can match or outperform complex models when using a large number of training data.", "labels": [], "entities": []}, {"text": "For example, a straightforward backoff technique rivals a complicated smoothing technique for language models.", "labels": [], "entities": []}, {"text": "Pretraining a bidirectional language model on a large dataset helps to solve a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Our approach is inspired by this line of work.", "labels": [], "entities": []}, {"text": "Contributions We propose a very straightforward fully-neural morphological analyzer which uses only character unigrams as its input . Such an analyzer, when trained only on human-annotated gold data has low accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9991746544837952}]}, {"text": "However, when trained on a large amount of automatically tagged silver data, the analyzer rivals and even outperforms, albeit slightly, the bootstrapping analyzer.", "labels": [], "entities": []}, {"text": "We conclude that there is no need for rich input representation.", "labels": [], "entities": []}, {"text": "Neural networks learn the information to combine characters into words by themselves when given enough data.", "labels": [], "entities": []}, {"text": "Ignoring explicit dictionary information and rich input representations makes it possible to make analyzers that are highly accurate and very compact at the same time.", "labels": [], "entities": []}, {"text": "We also perform ablation experiments which show that the encoder component of such an analyzer is more important than character embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on Japanese morphological analysis.", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6925948858261108}]}, {"text": "For training we use two data sources.", "labels": [], "entities": []}, {"text": "The first is usual human-annotated gold training data.", "labels": [], "entities": []}, {"text": "The second is silver data from the results of automatic analysis.", "labels": [], "entities": []}, {"text": "We use Juman++ V2 -the current state-of-the-art analyzer for the JUMAN segmentation standard as the bootstrap analyzer.", "labels": [], "entities": [{"text": "Juman++ V2", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8508079051971436}, {"text": "JUMAN segmentation", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7315637469291687}]}, {"text": "We use two gold corpora.", "labels": [], "entities": []}, {"text": "The first is the Kyoto University Text Corpus (, referred to as KU), containing newspaper data.", "labels": [], "entities": [{"text": "Kyoto University Text Corpus", "start_pos": 17, "end_pos": 45, "type": "DATASET", "confidence": 0.954212486743927}]}, {"text": "The second is the Kyoto University Web Document Leads Corpus (, referred to as Leads) which consists of web documents.", "labels": [], "entities": [{"text": "Kyoto University Web Document Leads Corpus", "start_pos": 18, "end_pos": 60, "type": "DATASET", "confidence": 0.9416185120741526}]}, {"text": "Corpus statistics are shown in.", "labels": [], "entities": [{"text": "Corpus", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7955927848815918}]}, {"text": "We denote models which use gold training data by G. We take raw data to generate our silver annotated data from a crawled web corpus of 9.8B unique sentences.", "labels": [], "entities": []}, {"text": "We sample 3B sentences randomly from it and analyze them using the Juman++ baseline model.", "labels": [], "entities": [{"text": "Juman++ baseline model", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.9029530137777328}]}, {"text": "From it we sample 500M sentences, which become our training silver data, prioritizing sentences which contain at least one not very frequent word.", "labels": [], "entities": []}, {"text": "We prepare both top-scored (denoted as T) and non-ambigous in beam (denoted as B) variants of the silver data.", "labels": [], "entities": []}, {"text": "Our silver data is indomain for Leads and out-of-domain for KU.", "labels": [], "entities": [{"text": "Leads", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8625926375389099}, {"text": "KU", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.8913116455078125}]}, {"text": "KyTea and Juman++ we train a model using the same dictionary and merged training sections of KU and Leads, which is evaluated on each corpus independently.", "labels": [], "entities": [{"text": "KyTea", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9225796461105347}]}, {"text": "The proposed MA achieves high accuracy while having very compact models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9985764026641846}]}, {"text": "The inputs do not contain any information on how to combine characters into words and we assume that the model learns it from the data.", "labels": [], "entities": []}, {"text": "To get the model size even smaller, we check which model parts contribute more to the resulting analysis accuracy, meaning that they contain the dictionary knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9936076402664185}]}, {"text": "We perform ablation experiments on the SAN model by varying its hyperparameters and checking how it affects the accuracy of the resulting analyzer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9986947178840637}]}, {"text": "The LSTM model could not converge in this setting.", "labels": [], "entities": []}, {"text": "We used 2.5M of silver training data for these experiments.", "labels": [], "entities": []}, {"text": "shows the segmentation F1 score when varying input embedding, shared representation and SAN hidden dimension sizes.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8465332984924316}]}, {"text": "JUMAN score, as a lowest acceptable baseline, is shown in red.", "labels": [], "entities": [{"text": "JUMAN score", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5967412292957306}]}, {"text": "The embedding size seems to have a lower impact on accuracy than the shared representation and the SAN hidden dimension size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9988604784011841}]}, {"text": "Namely, the (128-16) model with the embedding size of 16 has higher accuracy than the (128-4) model with the embed- ding size of 128.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9990803003311157}]}, {"text": "Accordingly, we believe that the encoder contributes much stronger to learning the dictionary than character embeddings.", "labels": [], "entities": []}, {"text": "One more interesting observation is that the models are still better than JUMAN, while having much less parameters than our base model.", "labels": [], "entities": [{"text": "JUMAN", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.7614673376083374}]}, {"text": "We explore more extreme settings of the SAN hidden state, shown in.", "labels": [], "entities": []}, {"text": "We fix embedding and shared representation dimensions to 128 and vary the SAN hidden and projection dimensions.", "labels": [], "entities": []}, {"text": "The lower subgraph is a scale-up version of top graph.", "labels": [], "entities": []}, {"text": "The point at SAN hidden size equal to 0 means that we directly use unigram embeddings to predict segmentation without any encoder.", "labels": [], "entities": []}, {"text": "The SAN projection size is consistent with accuracy, especially on smaller SAN hidden sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995666146278381}]}, {"text": "An interesting observation here is that the SAN model seems to work even with hidden dimension of 2.", "labels": [], "entities": []}, {"text": "When the hidden dimension size reaches 4, the extremely small model accuracy is higher than the JUMAN baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9948773980140686}, {"text": "JUMAN baseline", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.8028033673763275}]}, {"text": "This shows that it is possible to create an extremely small MA with acceptable accuracy.", "labels": [], "entities": [{"text": "MA", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9755040407180786}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9976511597633362}]}, {"text": "Label Uncertainty and Error Analysis Because our neural models infer all tags independently, they can be inconsistent, for example, a word can have different POS tags on different characters.", "labels": [], "entities": []}, {"text": "We looked into frequent 3-grams where the central word has inconsistent tags (POS tags are not the same for all characters, or they do not form a correct 4-layered tag).", "labels": [], "entities": []}, {"text": "Most of these trigrams occur in ambiguous situations.", "labels": [], "entities": []}, {"text": "We have picked several examples which are actually errors in Juman++ segmentation as well.", "labels": [], "entities": [{"text": "Juman++ segmentation", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7396468917528788}]}, {"text": "In Japanese, words often have several orthographic forms.", "labels": [], "entities": []}, {"text": "The most common variant is usage of hiragana (phonetic script) instead of kanji (ideographic characters).", "labels": [], "entities": []}, {"text": "Verbs can have different possible endings, e.g. \u66f2 \u304c\u308b and \u66f2\u308b (magaru -to turn or bend) are two orthographic variants of a single verb.", "labels": [], "entities": []}, {"text": "There are also colloquial variants; namely the verb \u8a00\u3046 is usually read as \u3044\u3046 (iu -to say), but can also be written as \u3086\u3046 because the pronunciation is close.", "labels": [], "entities": []}, {"text": "These phenomena are relatively common in web and user-generated texts, but corpus and segmentation dictionary coverage of them is not very good.", "labels": [], "entities": []}, {"text": "The first two examples contain alternative colloquial spellings of words \u3053\u3046\u3044\u3046 (ko:iu -such) and \u3059\u3054\u3044 (sugoi -awesome).", "labels": [], "entities": []}, {"text": "In the first example the system incorrectly recognizes \u66f2|\u3063\u3066 (kyoku tte) as \u66f2\u3063\u3066 (magatte) -a conjugation of \u66f2\u308b.", "labels": [], "entities": []}, {"text": "The fourth example (a chanto asobitai/ac-chan to asobitai -ah!", "labels": [], "entities": []}, {"text": "want to play properly/[I] want to play with ac-chan <person name>) is actually ambiguous and can have two meanings.", "labels": [], "entities": []}, {"text": "The second one is more probable though.", "labels": [], "entities": []}, {"text": "The fact that frequent words with uncertain POS tags are Juman++ errors as well implies that insufficient gold data causes the uncertainty.", "labels": [], "entities": []}, {"text": "We also compare differences between Juman++ and our models to get an insight on general problems with proposed methods.", "labels": [], "entities": []}, {"text": "Neural models make many errors in hiragana words.", "labels": [], "entities": []}, {"text": "For example, both neural models make errors in the sentence \u5f31\u8005|\u304c |\u3068\u3046\u305f|\u3055|\u308c\u3066 (jyakusya ga to:ta sarete -weaklings lose to natural selection).", "labels": [], "entities": []}, {"text": "LSTM makes a segmentation mistake (\u3068|\u3046\u305f\u3055) and SAN does a POS tagging mistake, while Juman++ produces the correct answer.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.6957954466342926}]}, {"text": "It knows that \u3068\u3046\u305f is a special type of noun that is often followed by \u3055\u308c\u3066 from POS tags.", "labels": [], "entities": [{"text": "\u3068\u3046\u305f", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.7868513464927673}]}, {"text": "Hiragana-based spellings of most content words are somewhat rare in Japanese, and NN models do not have enough training data for these spellings.", "labels": [], "entities": []}, {"text": "It could be possible to improve the situation by using data augmentation techniques.", "labels": [], "entities": []}, {"text": "Another frequent problem is segmentation and tagging of proper nouns.", "labels": [], "entities": [{"text": "tagging of proper nouns", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.751781091094017}]}, {"text": "We believe that this problem could be solved by data augmentation, but we leave this as future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Benchmark corpora sizes", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters for neural models", "labels": [], "entities": []}, {"text": " Table 3: Test F1 score comparison on benchmark cor- pora. Legend: bi-[L]STM, [S]AN, [G]old data, [T]op- only and [B]eam-non-ambigous silver data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9585270583629608}, {"text": "bi-[L]STM", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.6723212122917175}, {"text": "AN", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.8928211331367493}, {"text": "T]op- only", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.69558664560318}]}, {"text": " Table 4: MA model sizes for Jumandic", "labels": [], "entities": [{"text": "Jumandic", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.7032181620597839}]}, {"text": " Table 5: KyTea test Seg F1 comparison. -D models do  not use the dictionary. T models use silver data (2M  sentences, created like in the main experiment)", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.6321160793304443}]}]}