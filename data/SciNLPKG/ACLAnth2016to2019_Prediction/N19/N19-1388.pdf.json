{"title": [], "abstractContent": [{"text": "Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages.", "labels": [], "entities": [{"text": "Multilingual neural machine translation (NMT)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7411076085908073}]}, {"text": "In this paper, we push the limits of multilingual NMT in terms of the number of languages being used.", "labels": [], "entities": []}, {"text": "We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model.", "labels": [], "entities": []}, {"text": "We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions.", "labels": [], "entities": []}, {"text": "We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages.", "labels": [], "entities": [{"text": "TED talks multilingual corpus", "start_pos": 44, "end_pos": 73, "type": "DATASET", "confidence": 0.8498721718788147}]}, {"text": "Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT)) is the current state-ofthe-art approach for machine translation in both academia ( and industry (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.781918024023374}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7660289406776428}]}, {"text": "Recent works () extended the approach to support multilingual translation, i.e. training a single model that is capable of translating between multiple language pairs.", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.7421891391277313}]}, {"text": "Multilingual models are appealing for several reasons.", "labels": [], "entities": []}, {"text": "First, they are more efficient in terms * Work carried out during an internship at of the number of required models and model parameters, enabling simpler deployment.", "labels": [], "entities": []}, {"text": "Another benefit is transfer learning; when low-resource language pairs are trained together with highresource ones, the translation quality may improve ().", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9542489945888519}]}, {"text": "An extreme case of such transfer learning is zero-shot translation, where multilingual models are able to translate between language pairs that were never seen during training.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.891275018453598}, {"text": "zero-shot translation", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.6901794224977493}]}, {"text": "While very promising, it is still unclear how far one can scale multilingual NMT in terms of the number of languages involved.", "labels": [], "entities": []}, {"text": "Previous works on multilingual NMT typically trained models with up to 7 languages () and up to 20 trained directions ( simultaneously.", "labels": [], "entities": []}, {"text": "One recent exception is who trained many-to-one models from 58 languages into English.", "labels": [], "entities": []}, {"text": "While utilizing significantly more languages than previous works, their experiments were restricted to many-to-one models in a lowresource setting with up to 214k examples per language-pair and were evaluated only on four translation directions.", "labels": [], "entities": []}, {"text": "In this work, we take a step towards practical \"universal\" NMT -training massively multilingual models which support up to 102 languages and with up to one million examples per languagepair simultaneously.", "labels": [], "entities": []}, {"text": "Specifically, we focus on training \"English-centric\" many-to-many models, in which the training data is composed of many language pairs that contain English either on the source side or the target side.", "labels": [], "entities": []}, {"text": "This is a realistic setting since English parallel data is widely available for many language pairs.", "labels": [], "entities": []}, {"text": "We restrict our experiments to Transformer models ( as they were shown to be very effective in recent benchmarks, also in the context of multilingual models (.", "labels": [], "entities": []}, {"text": "We evaluate the performance of such massively multilingual models while varying factors like model capacity, the number of trained directions (tasks) and low-resource vs. high-resource settings.", "labels": [], "entities": []}, {"text": "Our experiments on the publicly available TED talks dataset ( show that massively multilingual many-to-many models with up to 58 languages to-and-from English are very effective in low resource settings, allowing to use high-capacity models while avoiding overfitting and achieving superior results to the current stateof-the-art on this dataset) when translating into English.", "labels": [], "entities": [{"text": "TED talks dataset", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8953921596209208}]}, {"text": "We then turn to experiment with models trained on 103 languages in a high-resource setting.", "labels": [], "entities": []}, {"text": "For this purpose we compile an English-centric inhouse dataset, including 102 languages aligned to-and-from English with up to one million examples per language pair.", "labels": [], "entities": []}, {"text": "We then train a single model on the resulting 204 translation directions and find that such models outperform strong bilingual baselines by more than 2 BLEU averaged across 10 diverse language pairs, both toand-from English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9986612796783447}]}, {"text": "Finally, we analyze the tradeoffs between the number of involved languages and translation accuracy in such settings, showing that massively multilingual models generalize better to zero-shot scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9091289043426514}]}, {"text": "We hope these results will encourage future research on massively multilingual NMT.", "labels": [], "entities": []}, {"text": "2 Low-Resource Setting: 59 Languages", "labels": [], "entities": []}], "datasetContent": [{"text": "In this setting we scale the number of languages and examples per language pair further when training a single massively multilingual model.", "labels": [], "entities": []}, {"text": "Since we are not aware of a publicly available resource for this purpose, we construct an in-house dataset.", "labels": [], "entities": []}, {"text": "This dataset includes 102 language pairs which we \"mirror\" to-and-from English, with up to one million examples per language pair.", "labels": [], "entities": []}, {"text": "This results in 103 languages in total, and 204 translation directions which we train simultaneously.", "labels": [], "entities": []}, {"text": "More details about this dataset are available in in the supplementary material details all the languages in the dataset.", "labels": [], "entities": []}, {"text": "Similarly to our previous experiments, we compare the massively multilingual models to bilingual baselines trained on the same data.", "labels": [], "entities": []}, {"text": "We tokenize the data using an in-house tokenizer and then apply joint subword segmentation to achieve an open-vocabulary.", "labels": [], "entities": []}, {"text": "In this setting we used a vocabulary of 64k subwords rather than 32k.", "labels": [], "entities": []}, {"text": "Since the dataset contains 24k unique characters, a 32k symbol vocabulary will consist of mostly characters, thereby increasing the average sequence length.", "labels": [], "entities": []}, {"text": "Regarding the model, for these experiments we use a larger Transformer model with 6 layers in both the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192, and 16 attention heads.", "labels": [], "entities": [{"text": "hidden dimension size", "start_pos": 157, "end_pos": 178, "type": "METRIC", "confidence": 0.8648533225059509}]}, {"text": "This results in a model with approximately 473.7M parameters.", "labels": [], "entities": []}, {"text": "Since the model and data are much larger in this case, we used a dropout rate of 0.1 for our multilingual models and tuned it to 0.3 for our baseline models as it improved the translation quality on the development set.", "labels": [], "entities": []}, {"text": "We evaluate our models on 10 languages from different typological families: Semitic -Arabic (Ar), Hebrew (He), Romance -Galician (Gl), Italian (It), Romanian (Ro), Germanic -German (De), Dutch (Nl), Slavic -Belarusian (Be), Slovak (Sk) and Turkic -Azerbaijani (Az) and Turkish (Tr).", "labels": [], "entities": []}, {"text": "We evaluate both to-and-from English, where each language pair is trained on up to one million examples.", "labels": [], "entities": []}, {"text": "As in the previous experiment, we report test results from the model that performed best in terms of BLEU on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.999147891998291}]}, {"text": "describes the results when translating into English.", "labels": [], "entities": []}, {"text": "First, we can see that both multilingual models perform better than the baselines in terms of average BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9979662895202637}]}, {"text": "This shows that massively multilingual many-to-many models can work well in realistic settings with millions of training examples, 102 languages and 204 jointly trained directions to-and-from English.", "labels": [], "entities": []}, {"text": "Looking more closely, we note several different behaviors in comparison to the low-resource experiments on the TED Talks corpus.", "labels": [], "entities": [{"text": "TED Talks corpus", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.7660137414932251}]}, {"text": "First, the many-to-one model here performs better than the many-to-many model.", "labels": [], "entities": []}, {"text": "This shows that the previous result was indeed due to the pathologies of the low-resource dataset; when the training data is large enough and not multiway-parallel there is no overfitting in the many-toone model, and it outperforms the many-to-many model inmost cases while they are trained identically.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: X\u2192En test BLEU on the TED Talks corpus,  for the language pairs from Neubig and Hu (2018)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9495383501052856}, {"text": "TED Talks corpus", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.826652983824412}]}, {"text": " Table 2: X\u2192En test BLEU on the TED Talks corpus,  for language pairs with more than 167k examples", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9484456181526184}, {"text": "TED Talks corpus", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.8359533747037252}]}, {"text": " Table 3: En\u2192X test BLEU on the TED Talks corpus", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9992715716362}, {"text": "TED Talks", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.7776581048965454}]}, {"text": " Table 5: X\u2192En test BLEU on the 103-language corpus", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8785102367401123}]}, {"text": " Table 7: Supervised performance while varying the number of languages involved", "labels": [], "entities": []}, {"text": " Table 8: Zero-Shot performance while varying the  number of languages involved", "labels": [], "entities": []}]}