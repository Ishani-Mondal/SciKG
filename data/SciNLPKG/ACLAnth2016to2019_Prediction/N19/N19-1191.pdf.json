{"title": [{"text": "Non-Parametric Adaptation for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.735596239566803}]}], "abstractContent": [{"text": "Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process.", "labels": [], "entities": []}, {"text": "In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.8327225744724274}, {"text": "rare phrase translation", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.5987918078899384}]}, {"text": "On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT.", "labels": [], "entities": []}, {"text": "However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval.", "labels": [], "entities": []}, {"text": "We propose a novel n-gram level retrieval approach that relies on local phrase level similarities , allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low.", "labels": [], "entities": [{"text": "n-gram level retrieval", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.6956257224082947}]}, {"text": "We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context.", "labels": [], "entities": []}, {"text": "We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets.", "labels": [], "entities": [{"text": "WMT", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8536063432693481}, {"text": "IWSLT", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.705062985420227}, {"text": "JRC-Acquis", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.8906409740447998}]}, {"text": "The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last few years, neural sequence to sequence models) have revolutionized the field of machine translation by significantly improving translation quality over their phrase based counterparts (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7413003444671631}]}, {"text": "With more gains arising from continued research on new neural network architectures and accompanying training techniques (, NMT researchers, both in industry and academia, have doubled down on their ability to train high capacity models on large corpora with gradient based optimization.", "labels": [], "entities": []}, {"text": "However, despite huge improvements in overall translation quality NMT has shown some glaring weaknesses, including idiom processing, and rare word or phrase translation ( tasks that should be easy if the model could retain learned information from individual training examples.", "labels": [], "entities": [{"text": "NMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.7638832926750183}, {"text": "rare word or phrase translation", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.6112330913543701}]}, {"text": "NMT has also been shown to perform poorly when dealing with multi-domain data).", "labels": [], "entities": []}, {"text": "This 'catastrophic forgetting' problem has been well-studied in traditional neural network literature, caused by parameter shift during the training process.", "labels": [], "entities": []}, {"text": "Nonparametric methods, on the other hand, are resistant to forgetting but are prone to over-fitting due to their reliance on individual training examples.", "labels": [], "entities": []}, {"text": "We focus on a non-parametric extension to NMT, hoping to combine the generalization ability of neural networks with the eidetic memory of non-parametric methods.", "labels": [], "entities": []}, {"text": "Given a translation query, we rely on an external retrieval mechanism to find similar source-target instances in the training corpus, which are then utilized by the model.", "labels": [], "entities": []}, {"text": "There has been some work on semi-parametric NMT (, but its effectiveness has been confined to narrow domain datasets.", "labels": [], "entities": []}, {"text": "Existing approaches have relied on sentence level similarity metrics for retrieval, which works well for domains with high train-test overlap, but fails to retrieve useful candidates for broad domains.", "labels": [], "entities": []}, {"text": "Even if we could find training instances with overlapping phrases it's likely that the information inmost retrieved source-target pairs is noise for the pur-pose of translating the current query.", "labels": [], "entities": [{"text": "translating", "start_pos": 165, "end_pos": 176, "type": "TASK", "confidence": 0.9774266481399536}]}, {"text": "To retrieve useful candidates when sentence similarity is low, we use n-gram retrieval instead of sentence retrieval.", "labels": [], "entities": []}, {"text": "This results in neighbors which have high local overlap with the source sentence, even if they are significantly different in terms of overall sentence similarity.", "labels": [], "entities": []}, {"text": "This is intuitively similar to utilizing information from a phrase table () within NMT (, without losing the global context lost when constructing the phrase table.", "labels": [], "entities": []}, {"text": "We also propose another simple extension using dense vectors for n-gram retrieval which allows us to exploit similarities beyond lexical overlap.", "labels": [], "entities": []}, {"text": "To effectively extract the signal from the noisy retrieved neighbors, we develop an extension of the approach proposed in. encode the retrieved targets without any context, we incorporate information from the current and retrieved sources while encoding the retrieved target, in order to distinguish useful information from noise.", "labels": [], "entities": []}, {"text": "We evaluate our semi-parametric NMT approach on two tasks.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our approach on a multi-domain English-French corpus constructed from narrow domain datasets like JRC-Acquis (; Tiedemann) and OpenSubtitles (Tiedemann, 2009) 1 , and the standard IWSLT and WMT bilingual corpora, as described in Sections 3 and 4.", "labels": [], "entities": [{"text": "JRC-Acquis", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9102048277854919}, {"text": "IWSLT", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.8788899183273315}, {"text": "WMT bilingual corpora", "start_pos": 204, "end_pos": 225, "type": "DATASET", "confidence": 0.7336666782697042}]}, {"text": "Our results, for the first time, indicate that semi-parametric NMT can be beneficial beyond narrow domain tasks, demonstrating gains of around 0.5 BLEU on WMT, and huge gains ranging from 2-10 BLEU points on IWSLT, JRCAcquis and OpenSubtitles, when compared to a strong sequence to sequence baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9974148273468018}, {"text": "WMT", "start_pos": 155, "end_pos": 158, "type": "DATASET", "confidence": 0.7405201196670532}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9978688955307007}, {"text": "IWSLT", "start_pos": 208, "end_pos": 213, "type": "DATASET", "confidence": 0.9359267950057983}]}, {"text": "\u2022 The semi-parametric nature of our model enables non-parametric inference-time adaptation to new datasets, without the need for any parameter updates.", "labels": [], "entities": []}, {"text": "When trained on WMT and evaluated on the other datasets, our model out-performs fine-tuning based adaptation ( on JRC-Acquis and OpenSubtitles, and significantly improves performance over the nonadapted model on IWSLT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8988106846809387}, {"text": "JRC-Acquis", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9058273434638977}, {"text": "IWSLT", "start_pos": 212, "end_pos": 217, "type": "DATASET", "confidence": 0.9667275547981262}]}], "datasetContent": [{"text": "We compare the performance of a standard Transformer Base model and our semi-parametric NMT approach on an English-French translation task.", "labels": [], "entities": [{"text": "English-French translation task", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.7010050813357035}]}, {"text": "We create anew heterogeneous dataset, constructed from a combination of the WMT training set (36M pairs), the IWSLT bilingual corpus (237k pairs), JRC-Acquis (797k pairs) 2 and OpenSubtitles (33M pairs) . For WMT, we use newstest 13 for validation and newstest 14 for test.", "labels": [], "entities": [{"text": "WMT training set", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.8950685262680054}, {"text": "IWSLT bilingual corpus", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.8709495862325033}, {"text": "JRC-Acquis", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.8191545605659485}]}, {"text": "For IWSLT, we use a combination of the test corpora from 2012-14 for validation and test 2015 for eval.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6040694713592529}]}, {"text": "For OpenSubtitles and JRC-Acquis, we create our own splits for validation and test, since no benchmark split is publicly available.", "labels": [], "entities": [{"text": "JRC-Acquis", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.9006748199462891}]}, {"text": "After deduping, the JRC-Acquis test and validation set contain 6574 and 5121 sentence pairs respectively.", "labels": [], "entities": [{"text": "JRC-Acquis test", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.7478082180023193}]}, {"text": "The OpenSubtitles test and validation sets contain 3975 and 3488 pairs.", "labels": [], "entities": [{"text": "OpenSubtitles test and validation sets", "start_pos": 4, "end_pos": 42, "type": "DATASET", "confidence": 0.8526969313621521}]}, {"text": "For multi-domain training, the validation set is a concatenation of the four individual validation sets.", "labels": [], "entities": []}, {"text": "All datasets are tokenized with the Moses tokenizer ( and mixed without any sampling.", "labels": [], "entities": []}, {"text": "We use a shared vocabulary SentencePiece Model ( for sub-word tokenization, with a vocabulary size of 32000 tokens.", "labels": [], "entities": []}, {"text": "We train each model for 1M steps, and choose the best checkpoint from the last 5 checkpoints based on validation performance.", "labels": [], "entities": []}, {"text": "BLEU scores are computed with tokenized truecased output and references with multi-bleu.perl from Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.981192946434021}]}, {"text": "For IDF based sentence retrieval, for each sentence in the training, dev and test corpus, we use N = 10 neighbors per example during both, training and evaluation.", "labels": [], "entities": [{"text": "IDF based sentence retrieval", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.5915151461958885}]}, {"text": "For the N-Gram level retrieval strategies, we used N = 10 neighbors dur- source 'The top copy of the passenger waybill shall be kept on the bus or coach throughout the journey to which it refers .' neighbor source 'The top copy of the journey form shall be kept on the vehicle during the whole of the journey to which it refers .' baseline translation 'La copie sup\u00e9rieure de la lettre de transport de voyageurs doit\u00eatredoit\u02c6doit\u00eatre conserv\u00e9e dans l' autobus ou l' autocar tout au long du voyage auquel elle se rapporte .' neighbor target 'L' original de la feuille de route doit se trouver\u00e0trouver`trouver\u00e0 bord du v\u00e9hicule pendant toute la dur\u00e9e du voyage pour lequel elle a \u00b4 et\u00e9et\u00e9\u00b4et\u00e9\u00e9tablie .' translation 'L' original de la feuille de route doit se trouver\u00e0trouver`trouver\u00e0 bord de l' autobus ou de l' autocar pendant toute la dur\u00e9e du voyage pour lequel elle a \u00b4 et\u00e9et\u00e9\u00b4et\u00e9\u00e9tablie .' reference 'L' original de la feuille de route doit se trouver\u00e0trouver`trouver\u00e0 bord de l' autobus ou de l' autocar pendant toute la dur\u00e9e du voyage pour lequel elle a \u00b4 et\u00e9et\u00e9\u00b4et\u00e9\u00e9tablie .' ing training, and neighbors corresponding to all ngrams during decoding.", "labels": [], "entities": []}, {"text": "This was meant to limit memory requirements and enable the model to fit on P100s during training.", "labels": [], "entities": []}, {"text": "We used n-gram width, n = {6, 10, 18}, for both IDF and dense vector based n-gram retrieval approaches.", "labels": [], "entities": []}, {"text": "For scalability reasons, we restricted the retrieval set to the indomain training corpus, i.e. neighbors for all train, dev and test sentences in the JRC-Acquis corpus were retrieved from the JRC-Acquis training split, and similarly for the other datasets.", "labels": [], "entities": [{"text": "JRC-Acquis corpus", "start_pos": 150, "end_pos": 167, "type": "DATASET", "confidence": 0.9584787786006927}, {"text": "JRC-Acquis training split", "start_pos": 192, "end_pos": 217, "type": "DATASET", "confidence": 0.9304953813552856}]}, {"text": "We report the performance of the various memory ablations in.", "labels": [], "entities": []}, {"text": "We first remove the retrieved sources, X i , from the CSTM, resulting in an architecture where the encoding of a retrieved target, Y i , only incorporates information from the source X, represented by the row CTM in the table.", "labels": [], "entities": []}, {"text": "This results in a clear drop in performance on all datasets.", "labels": [], "entities": []}, {"text": "We ablate further by removing the attention to the original source X, resulting in a slightly smaller drop in performance (represented by TM).", "labels": [], "entities": [{"text": "TM", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.7514408230781555}]}, {"text": "These experiments indicate that incorporating context from the sources significantly contributes to performance, by allowing the model to distinguish between relevant context and noise.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of test translation quality (BLEU) with different retrieval strategies. Multi-domain is a con- catenation of all 4 datasets. IDF Sentence, IDF-NGram and Dense N-Gram correspond to multi-domain datasets  constructed with the different retrieval strategies.", "labels": [], "entities": [{"text": "Comparison of test translation quality (BLEU)", "start_pos": 10, "end_pos": 55, "type": "TASK", "confidence": 0.7066910415887833}]}, {"text": " Table 5: A comparison of model outputs on a sample from WMT. This model was trained using dense vector based  n-gram retrieval with Conditional Source Target Memory. Dense vector based n-gram retrieval allows us to find  semantically similar phrases, even when the lexical context is dissimilar.", "labels": [], "entities": [{"text": "WMT", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9108685851097107}]}, {"text": " Table 6: Comparison of test translation quality (BLEU) with different memory architectures. All models are  trained on the Dense N-Gram Multi-Domain dataset. CSTM corresponds to the proposed Conditional Source  Target Memory. CTM corresponds to Conditional Target Memory, where we ignore the retrieved sources while  encoding the retrieved targets, and directly attend the encoding of the current source, X. TM corresponds to  encoding the retrieved targets without any context.", "labels": [], "entities": [{"text": "Comparison of test translation quality (BLEU", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.638023819242205}]}, {"text": " Table 7: Comparison of test translation quality (BLEU) with different adaptation strategies. The base model  (Transformer Base) is trained on the WMT dataset. Fine-tuning corresponds to fine-tuning based adaptation,  where we initialize the domain-specific model from the WMT pre-trained Base model, and fine-tune it on the  in-domain dataset for a few epochs. Non-parametric corresponds to our semi-parametric NMT model, adapted to  in-domain data during inference by retrieving neighbors from the in-domain training corpus.", "labels": [], "entities": [{"text": "Comparison of test translation quality (BLEU", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.6534659351621356}, {"text": "WMT dataset", "start_pos": 147, "end_pos": 158, "type": "DATASET", "confidence": 0.9648344814777374}]}]}