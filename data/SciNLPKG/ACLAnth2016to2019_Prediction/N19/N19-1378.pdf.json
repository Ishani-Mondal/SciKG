{"title": [{"text": "What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue", "labels": [], "entities": [{"text": "Insights from Entity Linking", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6529451459646225}]}], "abstractContent": [{"text": "Humans use language to refer to entities in the external world.", "labels": [], "entities": []}, {"text": "Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed.", "labels": [], "entities": []}, {"text": "Such entity-centric models have shown empirical success, but we still know little about why.", "labels": [], "entities": []}, {"text": "In this paper we analyze the behavior of two recently proposed entity-centric models in a ref-erential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4).", "labels": [], "entities": [{"text": "Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4)", "start_pos": 109, "end_pos": 169, "type": "TASK", "confidence": 0.8026633235541257}]}, {"text": "We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size.", "labels": [], "entities": []}, {"text": "We argue that making models entity-centric naturally fosters good architectural decisions.", "labels": [], "entities": []}, {"text": "However, we also show that these models do not really build entity representations and that they make poor use of linguistic context.", "labels": [], "entities": []}, {"text": "These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed .", "labels": [], "entities": [{"text": "model analysis", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7227067053318024}]}], "introductionContent": [{"text": "Modeling reference to entities is arguably crucial for language understanding, as humans use language to talk about things in the world.", "labels": [], "entities": [{"text": "Modeling reference to entities", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8866777569055557}, {"text": "language understanding", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7220001667737961}]}, {"text": "A hypothesis in recent work on referential tasks such as co-reference resolution and entity linking ( is that encouraging models to learn and use entity representations will help them better carryout referential tasks.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7428213357925415}, {"text": "entity linking", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7375115156173706}]}, {"text": "To illustrate, creating an entity representation with the relevant information upon reading a woman should make it easier to * denotes equal contribution.", "labels": [], "entities": []}, {"text": "JOEY TRIBBIANI resolve a pronoun mention like she.", "labels": [], "entities": [{"text": "JOEY", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7465661764144897}, {"text": "TRIBBIANI", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.8452512621879578}, {"text": "resolve", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.8247239589691162}]}, {"text": "1 In the mentioned work, several models have been proposed that incorporate an explicit bias towards entity representations.", "labels": [], "entities": []}, {"text": "Such entity-centric models have shown empirical success, but we still know little about what it is that they effectively learn to model.", "labels": [], "entities": []}, {"text": "In this analysis paper, we adapt two previous entity-centric models () fora recently proposed referential task and show that, despite their strengths, they are still very far from modeling entities.", "labels": [], "entities": []}, {"text": "The task is character identification on multiparty dialogue as posed in.", "labels": [], "entities": [{"text": "character identification", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.859046459197998}]}, {"text": "Models are given dialogues from the TV show Friends and asked to link entity mentions (nominal expressions like I, she or the woman) to the characters to which they refer in each case.", "labels": [], "entities": []}, {"text": "shows an example, where the mentions Ross and you are linked to entity 335, mention Ito entity 183, etc.", "labels": [], "entities": [{"text": "Ito entity 183", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.8423689405123392}]}, {"text": "Since the TV series revolves around a set of entities that recur over many scenes and episodes, it is a good benchmark to analyze whether entity-centric models learn and use entity representations for referential tasks.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: First, we adapt two previous entity-centric models and show that they do better on lower frequency entities (a significant challenge for current data-hungry models) than a counterpart model that is not entitycentric, with the same model size.", "labels": [], "entities": []}, {"text": "Second, through analysis we provide insights into how they achieve these improvements, and argue that making models entity-centric fosters architectural decisions that result in good inductive biases.", "labels": [], "entities": []}, {"text": "Third, we create a dataset and task to evaluate the models' ability to encode entity information such as gender, and show that models fail at it.", "labels": [], "entities": []}, {"text": "More generally, our paper underscores the need for the analysis of model behavior, not only through ablation studies, but also through the targeted probing of model representations ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Model parameters and results on the char- acter identification task. First block: top systems at  SemEval 2018. Results in the second block marked  with  *  are statistically significantly better than BIL- STM at p < 0.001 (approximate randomization tests,", "labels": [], "entities": [{"text": "char- acter identification task", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6885656535625457}, {"text": "BIL", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.936338484287262}]}, {"text": " Table 2: RSA correlation between speaker/referent em- beddings W e and token embeddings W t of the entities'  names, for main entities vs. all entities (right)", "labels": [], "entities": [{"text": "RSA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7665553092956543}]}, {"text": " Table 3: Average cosine similarity of mentions with the  same referent.", "labels": [], "entities": [{"text": "Average cosine similarity", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6380416651566824}]}, {"text": " Table 4: Results on the attribute and relation prediction  task: percentage accuracy for natural language descrip- tions, mean reciprocal rank of characters for single at- tributes (lower is worse).", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6868802011013031}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9717337489128113}]}, {"text": " Table 5: Results on the attribute prediction task (mean reciprocal rank; from 0 (worst) to 1 (best)). The number of  considered test items and candidate values, respectively, are given in the parentheses. For gender, we used (wo)man  and (s)he as word cues for the values (fe)male.", "labels": [], "entities": [{"text": "attribute prediction task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7685772677262624}]}]}