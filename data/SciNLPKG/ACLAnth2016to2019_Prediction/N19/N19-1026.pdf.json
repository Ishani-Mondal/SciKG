{"title": [{"text": "A Study of Incorrect Paraphrases in Crowdsourced User Utterances", "labels": [], "entities": []}], "abstractContent": [{"text": "Developing bots demands high quality training samples, typically in the form of user utterances and their associated intents.", "labels": [], "entities": []}, {"text": "Given the fuzzy nature of human language, such datasets ideally must coverall possible utterances of each single intent.", "labels": [], "entities": []}, {"text": "Crowdsourcing has widely been used to collect such inclusive datasets by paraphrasing an initial utterance.", "labels": [], "entities": []}, {"text": "However, the quality of this approach often suffers from various issues, particularly language errors produced by unqualified crowd workers.", "labels": [], "entities": []}, {"text": "More so, since workers are tasked to write open-ended text, it is very challenging to automatically asses the quality of paraphrased utterances.", "labels": [], "entities": []}, {"text": "In this paper, we investigate common crowd-sourced paraphrasing issues, and propose an annotated dataset called Para-Quality, for detecting the quality issues.", "labels": [], "entities": []}, {"text": "We also investigate existing tools and services to provide baselines for detecting each category of issues.", "labels": [], "entities": []}, {"text": "In all, this work presents a data-driven view of incorrect paraphrases during the bot development process, and we pave the way towards automatic detection of unqualified paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increasing advances in deep learning as well as natural language processing, anew generation of conversational agents is attracting significant attention.", "labels": [], "entities": []}, {"text": "Also known as dialogue systems, virtual assistants, chatbots or simply bots, some advanced bots are now designed to perform complex tasks (e.g., flight booking), many of which are built using machine learning techniques.", "labels": [], "entities": [{"text": "flight booking)", "start_pos": 145, "end_pos": 160, "type": "TASK", "confidence": 0.8317181269327799}]}, {"text": "At the heart of building such task-oriented bots lies the challenge of accurately capturing the user's intent (e.g., find cafes in Chicago), and then extracting its entities to service the request (e.g term= \"cafes\", location=\"Chicago\").", "labels": [], "entities": []}, {"text": "However, its success relies heavily on obtaining both, large and high quality corpora of training samples showing mappings between sample utterances and intents.", "labels": [], "entities": []}, {"text": "This is necessary given the ambiguous nature of the human language () and large variations of expressions (.", "labels": [], "entities": []}, {"text": "A lack of variations in training samples can result in incorrect intent detection and consequently execution of undesirable tasks (e.g., booking an expensive hotel instead of a cheap room).", "labels": [], "entities": [{"text": "intent detection", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6205814778804779}]}, {"text": "Likewise, quality issues in the training samples can lead to unmitigated disasters ( as it happened to Microsoft's Tay by making a huge number of offensive commentaries due to biases in the training data.", "labels": [], "entities": []}, {"text": "It is therefore not surprising that research and development into training data acquisition for bots has received significant consideration.", "labels": [], "entities": [{"text": "training data acquisition", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.671762098868688}]}, {"text": "Collecting training samples usually involves two primary steps: (i) firstly, obtaining an initial utterance fora given user intent (e.g., find a cafe in Chicago); and (ii) secondly, paraphrasing this initial expression into multiple variations (.", "labels": [], "entities": [{"text": "Collecting training", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8808263540267944}]}, {"text": "Paraphrasing is thus vital to cover the variety of ways an expression can be specified ().", "labels": [], "entities": []}, {"text": "As summarized in), a quality paraphrases has three components: semantic completeness, lexical difference, and syntactic difference.", "labels": [], "entities": []}, {"text": "To obtain lexically and syntactically diverse paraphrase, crowdsourcing paraphrases has gained popularity in recent years.", "labels": [], "entities": []}, {"text": "However, crowdsourced paraphrases need to be checked for quality, given that they are produced by unknown workers with varied skills and motivations).", "labels": [], "entities": []}, {"text": "For example, spammers, malicious and even inexperi-enced crowd-workers may provide misleading, erroneous, and semantically invalid paraphrases (.", "labels": [], "entities": []}, {"text": "Quality issues may also stem from misunderstanding the intent or not covering important information such as values of the intent parameters (.", "labels": [], "entities": []}, {"text": "The common practice for quality assessment of crowdsourced paraphrases is to design another crowdsourcing task in which workers validate the output from others.", "labels": [], "entities": []}, {"text": "However, this approach is costly having to pay for the task twice, making domain-independent automated techniques a very appealing alternative.", "labels": [], "entities": []}, {"text": "Moreover, quality control is especially desirable if done before workers submit their paraphrases, since low quality workers can be removed early on without any payment.", "labels": [], "entities": []}, {"text": "This can also allow crowdsourcing tasks to provide feedback to users in order to assist them in generating high quality paraphrases (.", "labels": [], "entities": []}, {"text": "To achieve this, it is therefore necessary to automatically recognize quality issues in crowdsourced paraphrases during the process of bot development.", "labels": [], "entities": []}, {"text": "In this paper, we investigate common paraphrasing errors when using crowdsourcing, and we propose an annotated dataset called ParaQuality in which each paraphrase is labelled with the error categories.", "labels": [], "entities": []}, {"text": "Accordingly, this work presents a quantitative data-driven study of incorrect paraphrases in bot development process and paves the way towards enhanced automated detection of unqualified paraphrased utterances.", "labels": [], "entities": []}, {"text": "More specifically, our contributions are two-folded: \u2022 We obtained a sample set of 6000 paraphrases using crowdsourcing.", "labels": [], "entities": []}, {"text": "To aim fora broad diversity of samples, the initial expressions were sourced from 40 expressions of highly popular APIs from various domains.", "labels": [], "entities": []}, {"text": "Next, we examined and analyzed these samples in order to identify a taxonomy of common paraphrase errors errors (e.g., cheating, misspelling, linguistic errors).", "labels": [], "entities": []}, {"text": "Accordingly, we constructed an annotated dataset called Para-Quality (using both crowdsourcing and manual verification), in which the paraphrases were labeled with a range of different categorized errors.", "labels": [], "entities": []}, {"text": "\u2022 We investigated existing tools and services (e.g., spell and grammar checkers, language identifiers) to detect potential errors.", "labels": [], "entities": [{"text": "spell and grammar checkers", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.589660219848156}]}, {"text": "We formulated baselines for each category of errors to determine if they were capable to automatically detect such issues.", "labels": [], "entities": []}, {"text": "Our experiments indicate that existing tools often have low precision and recall, and hence our results advocates the need for new approaches in effective detection of paraphrasing issues.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9989118576049805}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9989504814147949}]}, {"text": "We then launched a paraphrasing task on Figure-Eight 2 . Workers were asked to provide three paraphrases fora given expression (, which is common practice in crowdsourced paraphrasing to reduce repetitive results).", "labels": [], "entities": []}, {"text": "In the provided expression, parameter values were highlighted and crowd-workers were asked to preserve them.", "labels": [], "entities": []}, {"text": "Each worker's paraphrases for an initial utterance are normalized by lowercasing and removing punctuation.", "labels": [], "entities": []}, {"text": "Next, the initial utterance and the paraphrases are compared to forbid submitting empty strings or repeated paraphrases, and checked if they contain highlighted parameter values (which is also a common practice to avoid missing parameter values) ().", "labels": [], "entities": []}, {"text": "We collected paraphrases from workers in English speaking countries, and created a dataset containing 6000 paraphrases (2000 triple-paraphrases) in total 3 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, we designed another crowdsourcing task to annotate the collected paraphrases according to the category of issues devised above.", "labels": [], "entities": []}, {"text": "Namely, using following labels: Correct, Semantic Error, Misspelling, Linguistic Error, Translation, Answering, and Cheating.", "labels": [], "entities": [{"text": "Translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9404473304748535}, {"text": "Answering", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.6912922859191895}]}, {"text": "We split the category of misunderstanding issues into Translation and Answering because they require different methods to detect.", "labels": [], "entities": [{"text": "Translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9785484075546265}, {"text": "Answering", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.6659591197967529}]}, {"text": "In the annotation task, crowd workers were instructed to label each paraphrase with the paraphrasing issues.", "labels": [], "entities": []}, {"text": "Next, to further increase the quality of annotations 5 , two authors of this paper manually re-annotated the paraphrases to resolve disagreements between crowd annotators.", "labels": [], "entities": []}, {"text": "Moreover, contradictory labels (e.g., a paraphrase cannot be labeled both Correct and Misspelling simultaneously) were checked to ensure consistency.", "labels": [], "entities": []}, {"text": "The overall Kappa test showed a high agreement coefficient between the annotators (McHugh, 2012) by Kappa being 0.85..", "labels": [], "entities": [{"text": "agreement coefficient", "start_pos": 37, "end_pos": 58, "type": "METRIC", "confidence": 0.9760558009147644}]}, {"text": "Next, the authors discussed and revised the re-annotated labels to further increase the quality of annotations by discussing and resolving disagreements.", "labels": [], "entities": []}, {"text": "shows the frequencies of each label in the crowdsourced paraphrases as well as their co-occurrences in an UpSet plot () using Intervene ().", "labels": [], "entities": []}, {"text": "Accordingly we infer that only 61% of paraphrases are labeled Correct.", "labels": [], "entities": [{"text": "Correct", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9780293703079224}]}, {"text": "This plot also shows how many times two labels co-occurred.", "labels": [], "entities": []}, {"text": "For example, all paraphrases which are labeled Translation (24 times), are also labeled Cheating 6 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pairwise Inter-Annotator Agreement", "labels": [], "entities": []}, {"text": " Table 3: Comparison of Spell Checkers", "labels": [], "entities": [{"text": "Comparison of Spell Checkers", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6384773552417755}]}, {"text": " Table 4: Comparison of Grammar Checkers", "labels": [], "entities": []}, {"text": " Table 6: Summary of Feature Library", "labels": [], "entities": []}, {"text": " Table 7: Automatic Answering Detection", "labels": [], "entities": [{"text": "Automatic Answering Detection", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7393398880958557}]}, {"text": " Table 8: Automatic Semantic Error Detection", "labels": [], "entities": [{"text": "Automatic Semantic Error Detection", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6109181046485901}]}, {"text": " Table 9: Automatic Cheating Detection", "labels": [], "entities": [{"text": "Automatic Cheating Detection", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7259702483812968}]}, {"text": " Table 10: Automatic Incorrect Paraphrase Detection", "labels": [], "entities": [{"text": "Automatic Incorrect Paraphrase Detection", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.5435875803232193}]}]}