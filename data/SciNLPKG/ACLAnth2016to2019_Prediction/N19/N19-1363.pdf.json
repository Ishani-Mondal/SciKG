{"title": [{"text": "Submodular Optimization-based Diverse Paraphrasing and its Effectiveness in Data Augmentation", "labels": [], "entities": [{"text": "Submodular Optimization-based Diverse Paraphrasing", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7528763264417648}]}], "abstractContent": [{"text": "Inducing diversity in the task of paraphrasing is an important problem in NLP with applications in data augmentation and conversational agents.", "labels": [], "entities": []}, {"text": "Previous paraphrasing approaches have mainly focused on the issue of generating semantically similar paraphrases, while paying little attention towards diversity.", "labels": [], "entities": []}, {"text": "In fact, most of the methods rely solely on top-k beam search sequences to obtain a set of paraphrases.", "labels": [], "entities": []}, {"text": "The resulting set, however, contains many structurally similar sentences.", "labels": [], "entities": []}, {"text": "In this work, we focus on the task of obtaining highly diverse paraphrases while not compromising on paraphrasing quality.", "labels": [], "entities": []}, {"text": "We provide a novel formulation of the problem in terms of monotone submodular function maximization, specifically targeted towards the task of paraphrasing.", "labels": [], "entities": []}, {"text": "Additionally, we demonstrate the effectiveness of our method for data augmentation on multiple tasks such as intent classification and paraphrase recognition.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7498186826705933}, {"text": "paraphrase recognition", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.8673143088817596}]}, {"text": "In order to drive further research, we have made the source code available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered.", "labels": [], "entities": []}, {"text": "Paraphrasing Quality can be attributed to two key characteristics -fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences.", "labels": [], "entities": []}, {"text": "Many previous works) address the task of obtaining semantically similar paraphrases.", "labels": [], "entities": []}, {"text": "While it is essential to produce paraphrases with high fidelity, it is equally important, and in many * Equal Contribution \u2020 This research was conducted during the author's internship at the Indian Institute of Science, Bangalore.", "labels": [], "entities": [{"text": "Equal Contribution", "start_pos": 104, "end_pos": 122, "type": "METRIC", "confidence": 0.9657151699066162}]}, {"text": "cases desirable, to produce lexically diverse ones.", "labels": [], "entities": []}, {"text": "Diversity in paraphrase generation finds applications in text simplification (, document summarization (, QA systems, data augmentation (;, conversational agents ( and information retrieval).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8992733657360077}, {"text": "text simplification", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7293255627155304}, {"text": "document summarization", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7175669372081757}, {"text": "information retrieval", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7205787748098373}]}, {"text": "To obtain a set of multiple paraphrases, most of the current paraphrasing models rely solely on topk beam search sequences.", "labels": [], "entities": []}, {"text": "The resulting set, however, contains many structurally similar sentences with only minor, word level changes.", "labels": [], "entities": []}, {"text": "There have been some prior works () which address the notion of diversity in NLP, including in sequence learning frameworks (.", "labels": [], "entities": []}, {"text": "Although address the issue of diversity in the scenario of neural conversation models using determinantal point processes (DPP), it could be naturally used for paraphrasing.", "labels": [], "entities": []}, {"text": "On similar lines, subset selection based on Simultaneous Sparse Recovery (SSR) () can also be easily adapted for the same task.", "labels": [], "entities": [{"text": "Simultaneous Sparse Recovery (SSR)", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.6777668446302414}]}, {"text": "Though these methods are helpful in maximizing diversity, they are restrictive in terms of re-taining fidelity with respect to the source sentence.", "labels": [], "entities": []}, {"text": "Addressing the task of diverse paraphrasing through the lens of monotone submodular function maximization alleviates this problem and also provides a few additional benefits.", "labels": [], "entities": []}, {"text": "Firstly, the submodular objective offers better flexibility in terms of controlling diversity as well as fidelity.", "labels": [], "entities": []}, {"text": "Secondly, there exists a simple greedy algorithm for solving monotone submodular function maximization (, which guarantees the diverse solution to be almost as good as the optimal solution.", "labels": [], "entities": [{"text": "solving monotone submodular function maximization", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.5967923164367676}]}, {"text": "Finally, many submodular programs are fast and scalable to large datasets.", "labels": [], "entities": []}, {"text": "Below, we list the main contributions of our paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we outline the datasets used for evaluating our proposed method.", "labels": [], "entities": []}, {"text": "We specify the actual splits in  We compare our method against recent paraphrasing models as well as multiple diversity inducing schemes.", "labels": [], "entities": []}, {"text": "DiPS outperforms these baseline models in terms of fidelity metrics namely BLEU, ME-TEOR and TERp.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9989956021308899}, {"text": "ME-TEOR", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.970344603061676}, {"text": "TERp", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9883855581283569}]}, {"text": "A high METEOR score and a low TERp score indicate the presence of not only exact words but also synonyms and semantically similar phrases.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.9820895195007324}, {"text": "TERp score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9882251620292664}]}, {"text": "Notably, our model is not only able to achieve substantial gains over other diversity inducing schemes but is also able to do so without compromising on fidelity.", "labels": [], "entities": []}, {"text": "Diversity and fidelity scores are reported in, respectively.", "labels": [], "entities": [{"text": "Diversity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9739470481872559}, {"text": "fidelity", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9602492451667786}]}, {"text": "As described in Section 5.3, we evaluate the accuracy of paraphrase recognition models when provided with training data augmented using different schemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9987936019897461}, {"text": "paraphrase recognition", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8907293677330017}]}, {"text": "It is reasonable to expect that high quality paraphrases would tend to yield better results on in-domain paraphrase recognition task.", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.7820329666137695}]}, {"text": "We observe that using the paraphrases generated by DiPS helps in achieving substantial gains inaccuracy over other baseline schemes.", "labels": [], "entities": [{"text": "DiPS", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7801316380500793}]}, {"text": "showcases the effect of using paraphrases generated by our method as compared to other competitive paraphrasing methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Based on the task, we cat- egorize them into the following:", "labels": [], "entities": []}, {"text": " Table 3: Results on Quora-Div and Twitter dataset. Higher\u2191 BLEU and METEOR score is better whereas lower\u2193  TERp score is better. Please see Section 6 for details.", "labels": [], "entities": [{"text": "Quora-Div and Twitter dataset", "start_pos": 21, "end_pos": 50, "type": "DATASET", "confidence": 0.7123751044273376}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.999327540397644}, {"text": "METEOR score", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.9832152724266052}, {"text": "TERp score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9869600534439087}]}, {"text": " Table 4: Results on Quora-Div and Twitter dataset. Higher distinct scores imply better lexical diversity. Please see  Section 6 for details.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8340001404285431}]}, {"text": " Table 5: Accuracy scores of two classification models  on various data-augmentation schemes. Please see Sec- tion 6 for details", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9942535758018494}]}]}