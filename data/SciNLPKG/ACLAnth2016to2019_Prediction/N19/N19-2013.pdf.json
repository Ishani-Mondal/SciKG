{"title": [{"text": "Scaling Multi-Domain Dialogue State Tracking via Query Reformulation", "labels": [], "entities": [{"text": "Scaling Multi-Domain Dialogue State Tracking", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8726570010185242}, {"text": "Query Reformulation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7208448201417923}]}], "abstractContent": [{"text": "We present a novel approach to dialogue state tracking and referring expression resolution tasks.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7462252179781595}, {"text": "referring expression resolution tasks", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.8360527753829956}]}, {"text": "Successful contextual understanding of multi-turn spoken dialogues requires resolving referring expressions across turns and tracking the entities relevant to the conversation across turns.", "labels": [], "entities": []}, {"text": "Tracking conversational state is particularly challenging in a multi-domain scenario when there exist multiple spoken language understanding (SLU) subsystems , and each SLU subsystem operates on its domain-specific meaning representation.", "labels": [], "entities": [{"text": "Tracking conversational state", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9146647254625956}]}, {"text": "While previous approaches have addressed the disparate schema issue by learning candidate transformations of the meaning representation, in this paper , we instead model the reference resolution as a dialogue context-aware user query reformu-lation task-the dialog state is serialized to a sequence of natural language tokens representing the conversation.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.772838681936264}]}, {"text": "We develop our model for query reformulation using a pointer-generator network and a novel multi-task learning setup.", "labels": [], "entities": [{"text": "query reformulation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7995911538600922}]}, {"text": "In our experiments, we show a significant improvement in absolute F1 on an internal as well as a, soon to be released public corpora respectively .", "labels": [], "entities": [{"text": "absolute", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9661980867385864}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9003990292549133}]}], "introductionContent": [{"text": "Dialogue assistants are used by millions of people today to fulfill a variety of tasks.", "labels": [], "entities": []}, {"text": "Such assistants also serve as a digital marketplace 1 () where any developer can build a domainspecific, task-oriented, dialogue agent offering a service such as booking cabs, ordering food, listening to music, shopping etc.", "labels": [], "entities": []}, {"text": "Also, these agents may interact with each other, when completing a task on behalf of the user.", "labels": [], "entities": []}, {"text": "shows one such interaction where the agent -ShopBot -must interpret the output of the agent -WikiBot.", "labels": [], "entities": [{"text": "WikiBot", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9256114959716797}]}, {"text": "Often * Work done while the author was at Alexa AI 1 https://dialogflow.com", "labels": [], "entities": [{"text": "Alexa AI 1", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9148934086163839}]}], "datasetContent": [{"text": "In this section we will describe how we created the golden rewrites {Y * t | \u2200t} for each of the above datasets and our pre-processing steps that we found crucial to our success.", "labels": [], "entities": []}, {"text": "We used two datasets to evaluate our method.", "labels": [], "entities": []}, {"text": "The first is a public dataset (Regan et al., 2019) we call INCAR, which is an extension to  Entity F1: This measures micro F1 between entities in the hypothesized rewrite and gold rewrite.", "labels": [], "entities": [{"text": "INCAR", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.8484483361244202}]}, {"text": "This is different from F1 reported by as they evaluate F1 over system entities, whereas here we evaluate the entities over the user turn.", "labels": [], "entities": [{"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9969760179519653}, {"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9644263982772827}]}, {"text": "We employ a recent state-of-art bidirectional LSTM with CRF decoding) to implement our SLU system.", "labels": [], "entities": []}, {"text": "On INTERNAL dataset we show CQR significantly improves over) in.", "labels": [], "entities": [{"text": "INTERNAL dataset", "start_pos": 3, "end_pos": 19, "type": "DATASET", "confidence": 0.9445732533931732}, {"text": "CQR", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7396277785301208}]}, {"text": "CQR also improves F1 for current turn slots as it can leverage context and distill necessary information to improve SLU.", "labels": [], "entities": [{"text": "CQR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8962539434432983}, {"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9995064735412598}]}, {"text": "Further, we can see that most improvements upon the baseline PGN model (M0) come from pre-processing steps like canonicalizing entities.", "labels": [], "entities": []}, {"text": "In the baseline model, it has to learn to generate entity tokens individually, whereas in M1 the model only has to learn to copy tokens like USER ENT 1.", "labels": [], "entities": []}, {"text": "Finally, our proposed multi-task learning model (CQR) improves both BLEU and EntityF1 at most distances.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9987071752548218}, {"text": "EntityF1", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.7089716792106628}]}, {"text": "Specifically, we see an improvement of 4.2% over M2 for slots at distances \u22653.", "labels": [], "entities": []}, {"text": "In distance is measured differently from, here we count User and System turns individually to showcase how distance affects EntityF1.", "labels": [], "entities": [{"text": "EntityF1", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8245404362678528}]}, {"text": "If an entity is repeated multiple times in the context, we consider its closest occurrence to report results.", "labels": [], "entities": []}, {"text": "For INCAR dataset we pick the best model CQR from and re-train on the respective dataset.", "labels": [], "entities": [{"text": "INCAR dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8996361792087555}]}, {"text": "On the navigation domain we observe significant: Examples of generated responses for Internal Dataset improvement.", "labels": [], "entities": []}, {"text": "We believe this is because there are on average 2.3 slots were referred from history in rewrites requiring copy from dialog as compared to 1.3 and 1.1 in schedule and weather domain respectively.", "labels": [], "entities": []}, {"text": "Also, we compare with an oracle CQR (i.e., gold-rewrite from our data collection, instead of predicted re-write) to measure the potential of query-rewriting and motivate further research on this topic.", "labels": [], "entities": []}, {"text": "We can see that the CQR model performs better than the Mem2Seq * model, indicating that query rewriting is a viable alternative to dialogue state tracking.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.6746443112691244}]}, {"text": "This is important in environments where changing the NLU systems to leverage memory structures is not always feasible.", "labels": [], "entities": []}, {"text": "We claim that query rewriting is a simpler approach in such situations, with no loss in performance.", "labels": [], "entities": [{"text": "query rewriting", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.8438931405544281}]}], "tableCaptions": [{"text": " Table 2: INTERNAL data statistics. Each turn consists of a", "labels": [], "entities": [{"text": "INTERNAL data statistics", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.827135443687439}]}, {"text": " Table 4: Comparison of Pointer-Generator variants to traditional state tracking approach on the INTERNAL dataset.  We measure entity F1 across slots from different distances separately. Slot distance is counted per utterance starting  from the current user utterance. Therefore, slots at d=0 are slots from the current user utterance that should have  been copied. d=1 refers to slots from system response in the last turn, d=2 refers to slots from the user in last turn  and d\u22653 aggregates all other turns. d \u22653 is the most challenging test-subset where CQR has the highest benefit.", "labels": [], "entities": [{"text": "INTERNAL dataset", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.9898865818977356}, {"text": "Slot distance", "start_pos": 187, "end_pos": 200, "type": "METRIC", "confidence": 0.9182944595813751}]}, {"text": " Table 5: Comparison of PGN variants proposed in this paper on the INCAR dataset in comparison to the state  tracking approach. Our proposed CQR model outperforms the MemSeq* system, which is a stronger baseline than  the Mem2Seq results published in Madotto et al. (2018).", "labels": [], "entities": [{"text": "INCAR dataset", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9577730000019073}, {"text": "state  tracking", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.6884719729423523}]}]}