{"title": [{"text": "A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a corpus of anaphoric information (coreference) crowdsourced through a game-with-a-purpose.", "labels": [], "entities": []}, {"text": "The corpus, containing annotations for about 108,000 markables, is one of the largest corpora for coreference for English, and one of the largest crowdsourced NLP corpora , but its main feature is the large number of judgments per markable: 20 on average , and over 2.2M in total.", "labels": [], "entities": []}, {"text": "This characteristic makes the corpus a unique resource for the study of disagreements on anaphoric interpretation.", "labels": [], "entities": [{"text": "anaphoric interpretation", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7162621319293976}]}, {"text": "A second distinctive feature is its rich annotation scheme, covering single-tons, expletives, and split-antecedent plurals.", "labels": [], "entities": []}, {"text": "Finally, the corpus also comes with labels inferred using a recently proposed probabilistic model of annotation for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9657278060913086}]}, {"text": "The labels are of high quality and make it possible to successfully train a state of the art corefer-ence resolver, including training on singletons and non-referring expressions.", "labels": [], "entities": [{"text": "corefer-ence resolver", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.6359704583883286}]}, {"text": "The annotation model can also result in more than one label, or no label, being proposed fora markable, thus serving as a baseline method for automatically identifying ambiguous markables.", "labels": [], "entities": []}, {"text": "A preliminary analysis of the results is presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "A number of datasets for anaphora resolution / coreference now exist (, including ONTONOTES that has been the de facto standard since the CONLL shared tasks in 2011 and 2012, and the just introduced and very substantial PRECO corpus).", "labels": [], "entities": [{"text": "anaphora resolution / coreference", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6915463656187057}, {"text": "ONTONOTES", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9031813144683838}, {"text": "PRECO corpus", "start_pos": 220, "end_pos": 232, "type": "DATASET", "confidence": 0.8732334077358246}]}, {"text": "None of these datasets however take into account the research challenging the idea that a 'gold standard' interpretation can be obtained through adjudication, in particular for anaphora.", "labels": [], "entities": []}, {"text": "Virtually every project devoted to large-scale annotation of discourse or semantic phenomena has reached the conclusion that genuine disagreements are widespread.", "labels": [], "entities": []}, {"text": "This has long been known for anaphora) (see also the analysis of disagreements in ONTONOTES in () and wordsenses (), but more recent work has provided evidence that disagreements are frequent for virtually every aspect of language interpretation, not just in subjective tasks such as sentiment analysis), but even in the case of tasks such as part-of-speech tagging ().", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.7833458781242371}, {"text": "language interpretation", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.7476394176483154}, {"text": "sentiment analysis", "start_pos": 284, "end_pos": 302, "type": "TASK", "confidence": 0.9216655194759369}, {"text": "part-of-speech tagging", "start_pos": 343, "end_pos": 365, "type": "TASK", "confidence": 0.7044227123260498}]}, {"text": "In fact, researchers in the CrowdTruth project view disagreement as positive, arguing that \"disagreement is signal, not noise\".", "labels": [], "entities": [{"text": "CrowdTruth project", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.8674459457397461}]}, {"text": "In this paper we present what to our knowledge is the largest corpus containing alternative anaphoric judgments: 20.6 judgments per markable on average (up to 90 judgments in some cases) for about 108,000 markables.", "labels": [], "entities": []}, {"text": "We are not aware of any comparable resource for studying disagreement and ambiguity in anaphora or indeed any other area of NLP.", "labels": [], "entities": []}, {"text": "We present some preliminary analysis in the paper.", "labels": [], "entities": []}, {"text": "The corpus presented in this paper is also the largest corpus for anaphora / coreference entirely created through crowdsourcing, and one of the largest corpus of coreference information for English in terms of markables.", "labels": [], "entities": []}, {"text": "So far, only fairly small coreference corpora have been created using crowdsourcing).", "labels": [], "entities": []}, {"text": "The corpus presented here provides annotations for about 108,000 markables, 55% of the number of markables in ONTONOTES.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 110, "end_pos": 119, "type": "DATASET", "confidence": 0.8695100545883179}]}, {"text": "Another novelty is that the corpus was created through a 'quasi' Game-With-A-Purpose (GWAP), Phrase Detectives (, and is, to our knowledge, the largest GWAP-created corpus for NLP.", "labels": [], "entities": []}, {"text": "(So far, the success of GWAPs in other areas of science has not been replicated in NLP.)", "labels": [], "entities": []}, {"text": "Finally, the corpus is notable fora richer annotation scheme than the other large coreference corpora.", "labels": [], "entities": []}, {"text": "Singletons were marked as well as mentions participating in coreference chains (the omission of singletons being one of the main problems with ONTONOTES).", "labels": [], "entities": []}, {"text": "Non-referring expressions were also annotated: both expletives (not annotated either in ONTONOTES or PRECO) and predicative NPs.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.5623132586479187}]}, {"text": "Finally, all types of plurals were annotated, including also split-antecedent plurals as in John met with Mary, and they went to dinner, which again are not annotated either in ONTONOTES or PRECO.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.6713286638259888}]}, {"text": "Turning a crowdsourced corpus into a highquality dataset suitable to train and evaluate NLP systems requires, however, an aggregation method appropriate to the data and capable of achieving sufficient quality, something that simple majority voting typically cannot guarantee.", "labels": [], "entities": []}, {"text": "What made it possible to extract such a dataset from the collected judgments was the recent development of a probabilistic method for aggregating coreference annotations called MPA ().", "labels": [], "entities": []}, {"text": "MPA extracts silver labels from a coreference annotation and associates them with a probability, allowing for multiple labels in cases of ambiguity.", "labels": [], "entities": []}, {"text": "As far as we know, ours is the first use of MPA to create a large-scale dataset.", "labels": [], "entities": []}, {"text": "We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.7589089870452881}]}], "datasetContent": [{"text": "Since the two CONLL shared tasks (), ONTONOTES has become the dominant resource for anaphora resolution research.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.84293532371521}, {"text": "anaphora resolution research", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.9176281889279684}]}, {"text": "ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7463468313217163}]}, {"text": "One frequently discussed limitation of ONTONOTES is the absence of singletons, which makes it harder to train models for mention detection.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.5841513276100159}, {"text": "mention detection", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.8741002678871155}]}, {"text": "Another limitation is that expletives are not annotated.", "labels": [], "entities": []}, {"text": "As a consequence, downstream applications such as machine translation () that require pronoun interpretation have to adopt various workarounds.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7954447269439697}, {"text": "pronoun interpretation", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7424727976322174}]}, {"text": "Because of these two restrictions, ONTONOTES only has 195K markables, and a low markable density (0.12 markable/token).", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.587817907333374}]}, {"text": "A number of smaller corpora provide linguistically richer information (.", "labels": [], "entities": []}, {"text": "Examples include ANCORA for Spanish, TUBA-D/Z for German), the Prague Dependency Treebank for Czech and English (, and ARRAU for English (Uryupina et al., To Appear).", "labels": [], "entities": [{"text": "ANCORA", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9748900532722473}, {"text": "TUBA-D", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9798313975334167}, {"text": "Prague Dependency Treebank", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.9261256655057272}, {"text": "ARRAU", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9968575239181519}]}, {"text": "In ARRAU, for example, singletons and expletives are annotated as well, as are split antecedent plurals, generic coreference, discourse deixis, and bridging references.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.910773754119873}]}, {"text": "The AR-RAU corpus is relatively small in terms of tokens (350K), but has a higher markable density than ONTONOTES (0.29 markable/token), so it has around 100K markables, half the number of ONTONOTES.", "labels": [], "entities": [{"text": "AR-RAU corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9555200934410095}, {"text": "ONTONOTES", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.6472858786582947}]}, {"text": "ARRAU was recently used in the CRAC 2018 shared task ( to evaluate a number of anaphora resolution tasks.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7911142110824585}, {"text": "CRAC 2018 shared task", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.831211268901825}, {"text": "anaphora resolution tasks", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.82351686557134}]}, {"text": "The recently introduced PRECO corpus) is the largest existing coreference corpus, consisting of 35,000 documents fora total of 12.5M tokens and 3.8M markables, half of which are singletons.", "labels": [], "entities": [{"text": "PRECO corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8063726723194122}]}, {"text": "However, the corpus is not intended as a general purpose dataset as only the 3000 most common English words appear in the documents (the majority -2/3 -of the documents are from Chinese high-school English tests).", "labels": [], "entities": []}, {"text": "The corpus's annotation scheme mainly follows the ONTONOTES guidelines, with a few important differences: singleton mentions and generic coreference are annotated, event anaphora is not, and predicative NPs are annotated as co-referring with their argument, as previously done in the MUC ( and ACE () corpora.", "labels": [], "entities": [{"text": "MUC", "start_pos": 284, "end_pos": 287, "type": "DATASET", "confidence": 0.9405441284179688}]}, {"text": "As one could expect, the corpus is relatively easy for coreference systems.", "labels": [], "entities": []}, {"text": "The system trained and tested on PRECO achieves an av-erage CONLL score of 81.5%, whereas the same system trained and tested on ONTONOTES only achieves a score of 70.4%.", "labels": [], "entities": [{"text": "PRECO", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8050373792648315}, {"text": "CONLL score", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9026848375797272}, {"text": "ONTONOTES", "start_pos": 128, "end_pos": 137, "type": "DATASET", "confidence": 0.8152608871459961}]}, {"text": "We randomly chose 1/20 of PD silver as a development set and use the rest as the training set; PD gold was used as test set.", "labels": [], "entities": []}, {"text": "To get a baseline, we compare the results of our system on a simplified version of the corpus without singletons and expletives with those obtained by the current state-of-the-art system on ONTONOTES,  trained and tested on the same data.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 190, "end_pos": 199, "type": "DATASET", "confidence": 0.905142068862915}]}, {"text": "shows the results of both systems on the simplified corpus.", "labels": [], "entities": []}, {"text": "Our cluster ranking system achieved an average CONLL score of 60.5%, outperforming the  system by 2 percentage points.", "labels": [], "entities": [{"text": "CONLL score", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.8218427300453186}]}, {"text": "Note that the  system achieved a higher score on the CONLL data, which suggests that the present corpus is different from that dataset.", "labels": [], "entities": [{"text": "CONLL data", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.976517915725708}]}], "tableCaptions": [{"text": " Table 2: Summary of the contents of the current re- lease. The numbers in parentheses indicate the total  number of markables that are non-singletons.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of markables with X distinct inter- pretations", "labels": [], "entities": []}, {"text": " Table 4: A per class evaluation of aggregated interpretations against expert annotations.", "labels": [], "entities": []}, {"text": " Table 5: The quality of the coreference chains for the PD gold subset.", "labels": [], "entities": [{"text": "PD gold subset", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.8289487957954407}]}, {"text": " Table 6: The CoNLL scores for our systems trained on PD silver and tested on PD gold . * indicates the models trained  on the simplified corpus.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.5861926674842834}]}, {"text": " Table 7: Non-referring scores for our model", "labels": [], "entities": []}, {"text": " Table 8: Ambiguity in the corpus according to MPA", "labels": [], "entities": [{"text": "MPA", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7717202305793762}]}, {"text": " Table 9: Analysis of disagreements in two corpus doc- uments", "labels": [], "entities": []}]}