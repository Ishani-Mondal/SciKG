{"title": [{"text": "Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus", "labels": [], "entities": [{"text": "Reinforcement Learning Based Text Style Transfer", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8826463520526886}]}], "abstractContent": [{"text": "Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning.", "labels": [], "entities": [{"text": "Text style transfer rephrases a text", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7801483074824015}]}, {"text": "Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus.", "labels": [], "entities": []}, {"text": "In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture.", "labels": [], "entities": []}, {"text": "Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style.", "labels": [], "entities": []}, {"text": "Our evaluator is an adversarially trained style dis-criminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.7164415568113327}]}, {"text": "Experimental results on two different style transfer tasks (sentiment transfer and formality transfer) show that our model outperforms state-of-the-art approaches.", "labels": [], "entities": [{"text": "style transfer tasks", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7772841850916544}, {"text": "sentiment transfer", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9043949842453003}, {"text": "formality transfer", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7691287994384766}]}, {"text": "Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text style transfer is the task of rewriting apiece of text to a particular style while retaining the meaning of the original text.", "labels": [], "entities": [{"text": "Text style transfer", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6003439923127493}]}, {"text": "It is a challenging task of natural language generation and is at the heart of many recent NLP applications, such as personalized responses in dialogue system (, formalized texts (, cyberspace purification by rewriting offensive texts (, and poetry generation (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6554143130779266}, {"text": "poetry generation", "start_pos": 242, "end_pos": 259, "type": "TASK", "confidence": 0.801517903804779}]}, {"text": "Recent works on supervised style transfer with a parallel corpus have demonstrated considerable success (.", "labels": [], "entities": [{"text": "supervised style transfer", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.6103622317314148}]}, {"text": "However, a parallel corpus may not always be available fora transfer task.", "labels": [], "entities": []}, {"text": "This has prompted studies on style transfer without parallel corpora.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8609766662120819}]}, {"text": "These hinge on the common idea of separating the content from the style of the text.", "labels": [], "entities": []}, {"text": "This line of research first encodes the context via a style-independent representation, and then transfers sentences by combining the encoded content with style information.", "labels": [], "entities": []}, {"text": "In addition, an appropriate training loss is chosen to change the style while preserving the content.", "labels": [], "entities": []}, {"text": "However, these approaches are limited by their use of loss functions that must be differentiable with respect to the model parameters, since they rely on gradient descent to update the parameters.", "labels": [], "entities": []}, {"text": "Furthermore, since focusing only on semantic and style metrics in style transfer, they ignore other important aspects of quality in text generation, such as language fluency.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7281108349561691}, {"text": "text generation", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7273268699645996}]}, {"text": "In this paper, we propose a system trained using reinforcement-learning (RL) that performs text style transfer without accessing to a parallel corpus.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6741575598716736}]}, {"text": "Our model has a generator-evaluator structure with one generator and one evaluator with multiple modules.", "labels": [], "entities": []}, {"text": "The generator takes a sentence in a source style as input and transfers it to the target style.", "labels": [], "entities": []}, {"text": "It is an attention-based sequence-tosequence model, which is widely used in generation tasks such as machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.811800479888916}]}, {"text": "More advanced model such as graphto-sequence model can also exploited for this generation task ( ).", "labels": [], "entities": []}, {"text": "The evaluator consists of a style module, a semantic module and a language model for evaluating the transferred sentences in terms of style, semantic content, and fluency, respectively.", "labels": [], "entities": []}, {"text": "Feedback from each evaluator is sent to the generator so it can be updated to improve the transfer quality.", "labels": [], "entities": []}, {"text": "Our style module is a style discriminator built using a recurrent neural network, predicting the likelihood that the given input is in the target style.", "labels": [], "entities": []}, {"text": "We train the style module adversarially to be a target style classifier while regarding the transferred sentences as adversarial samples.", "labels": [], "entities": []}, {"text": "An adversarial training renders style classification more robust and accurate.", "labels": [], "entities": [{"text": "style classification", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8945484757423401}]}, {"text": "As for the semantic module, we used word movers' distance (WMD), a stateof-the-art unsupervised algorithm for comparing semantic similarity between two sentences, to evaluate the semantic similarity between input sentences in the source style and the transferred sentences in the target style.", "labels": [], "entities": [{"text": "word movers' distance (WMD)", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.6912060330311457}]}, {"text": "We also engaged a language model to evaluate the fluency of the transferred sentences.", "labels": [], "entities": []}, {"text": "Unlike prior studies that separated content from style to guarantee content preservation and transfer strength, we impose explicit semantic, style and fluency constraints on our transfer model.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7327848076820374}]}, {"text": "Moreover, employing RL allows us to use other evaluation metrics accounting for the quality of the transferred sentences, including non-differentiable ones.", "labels": [], "entities": []}, {"text": "We summarize our contributions below: (1) We propose an RL framework for text style transfer.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7053730686505636}]}, {"text": "It is versatile to include a diverse set of evaluation metrics as the training objective in our model.", "labels": [], "entities": []}, {"text": "(2) Our model does not rely on the availability of a parallel training corpus, thus addressing the important challenge of lacking parallel data in many transfer tasks.", "labels": [], "entities": []}, {"text": "(3) The proposed model achieves state-of-the-art performance in terms of content preservation and transfer strength in text style transfer.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7027329355478287}, {"text": "text style transfer", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7066684166590372}]}, {"text": "The rest of our paper is organized as follows: we discuss related works on style transfer in Section 2.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.8420799970626831}]}, {"text": "The proposed text style transfer model and the reinforcement learning framework is introduced in Section 3.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7265379428863525}]}, {"text": "Our system is empirically evaluated on sentiment and formality transfer tasks in Section 4.", "labels": [], "entities": [{"text": "sentiment and formality transfer", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.772027924656868}]}, {"text": "We report and discuss the results in Section 5 and Section 6.", "labels": [], "entities": []}, {"text": "The paper is concluded in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we considered two textual style transfer tasks, that of sentiment transfer (ST, involving negative and positive sentiments) and formality transfer (FT, involving informal and formal styles) using two curated datasets.", "labels": [], "entities": [{"text": "textual style transfer", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.6697598894437155}, {"text": "sentiment transfer", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8162685632705688}, {"text": "formality transfer", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7306950688362122}]}, {"text": "We experimented with both transfer directions: positiveto-negative, negative-and-positive, informal-toformal and formal-to-informal.", "labels": [], "entities": []}, {"text": "For our experiments with style transfer we used a sentiment corpus and a formality corpus described below., which is a collection of sentences posted in a question-answer forum (Yahoo Answers) and written in an informal style.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7540550529956818}]}, {"text": "In addition, these sentences have been manually rewritten in a formal style.", "labels": [], "entities": []}, {"text": "We used the data from the section family and relationships.", "labels": [], "entities": []}, {"text": "Note that even though the corpus is parallel, we did not use the parallel information.", "labels": [], "entities": []}, {"text": "shows the train, dev and test data sizes as well as the vocabulary sizes of the corpora used in this work.", "labels": [], "entities": []}, {"text": "The word embeddings used in this work were of dimension 50.", "labels": [], "entities": []}, {"text": "They were first trained on the English WikiCorpus and then tuned on the training dataset.", "labels": [], "entities": [{"text": "English WikiCorpus", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.8123803436756134}]}, {"text": "The width of the beam search (parameter k) was 8 during the RL and the inference stage.", "labels": [], "entities": [{"text": "RL", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.894392192363739}]}, {"text": "Formal-to-Informal I believe you 're a good man most likely she loves you quite a bit.", "labels": [], "entities": [{"text": "Formal-to-Informal", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.866993248462677}]}, {"text": "I think you 're a good man she kinda loves you . Pre-training.", "labels": [], "entities": []}, {"text": "We pre-trained the generator, the style discriminator and the language model before the reinforcement learning stage.", "labels": [], "entities": []}, {"text": "We discuss each of these steps below.", "labels": [], "entities": []}, {"text": "We used both automatic and human evaluation to validate our system in terms of content preservation, transfer strength and fluency.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7472756505012512}]}, {"text": "Aligning with prior work, we used the automatic metrics of content preservation, transfer and fluency that have been found to be well correlated with human judgments (  carried out and was quantified using a classifier.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.6927748918533325}]}, {"text": "An LSTM-based classifier was trained for style classification on a training corpus.", "labels": [], "entities": [{"text": "style classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8231821060180664}]}, {"text": "The classifier predicts the style of the generated sentences with a threshold of 0.5.", "labels": [], "entities": []}, {"text": "The prediction accuracy is defined as the percentage of generated sentences that were classified to be in the target style.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8863266110420227}]}, {"text": "The accuracy was used to evaluate transfer strength, and the higher the accuracy is, the better the generated sentences fit in target style.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994779229164124}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9984750151634216}]}, {"text": "We would like to point out that there is a trade-off between content preservation and transfer strength.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7486552298069}]}, {"text": "This is because the outputs resulting from unchanged input sentences show the best content preservation while having poor transfer strength.", "labels": [], "entities": []}, {"text": "Likewise, forgiven inputs, sentences sampled from the target corpora have the strongest transfer strength while barely preserving any content if at all.", "labels": [], "entities": []}, {"text": "To combine the evaluation of semantics and style, we use the overall score s overall , which is defined as a function of s sem and s style : s overall = ssem * s style ssem+s style (.", "labels": [], "entities": []}, {"text": "This is usually evaluated with a language model in many NLP applications.", "labels": [], "entities": []}, {"text": "We used a two-layer recurrent neural network with gated recurrent units as a language model, and trained it on the target style part of the corpus.", "labels": [], "entities": []}, {"text": "The language model gives an estimation of perplexity (PPL) over each generated sentence.", "labels": [], "entities": [{"text": "estimation of perplexity (PPL)", "start_pos": 28, "end_pos": 58, "type": "METRIC", "confidence": 0.8835866451263428}]}, {"text": "Given a word sequence of M words {w 1 , . .", "labels": [], "entities": []}, {"text": ", w M } and the sequence probability p(w 1 , . .", "labels": [], "entities": []}, {"text": ", w M ) estimated by the language model, the perplexity is defined as: The lower the perplexity on a sentence, the more fluent the sentence is.", "labels": [], "entities": []}, {"text": "We reported the automatic evaluation results of all text style transfer systems in, where we used the evaluation metrics adopted by previous works ().", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6156933804353079}]}, {"text": "Here we report the style and semantic scores given by the evaluator in our system in.", "labels": [], "entities": []}, {"text": "Recall that semantic score given by our evaluator was the negative of word movers' distance between the generated sentence and the source sentence divided by the sentence length.", "labels": [], "entities": []}, {"text": "The larger the semantic score was, the better the content was preserved in the generated sentence.", "labels": [], "entities": []}, {"text": "As for the style evaluation, we used a bidirectional recurrent neural network as style classifier.", "labels": [], "entities": []}, {"text": "It predicted the likelihood that an input sentence was in target style, which was taken as the style score of the generated sentences.", "labels": [], "entities": []}, {"text": "Again, the larger the style score was, the better the generated sentence fitted in target style.", "labels": [], "entities": [{"text": "style score", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9432060718536377}]}, {"text": "As shown in, the results given by the semantic and style modules of our evaluator are very similar to those given by Fu et al..", "labels": [], "entities": []}, {"text": "In sentiment transfer task, CA model does best in content preservation and MDS does best in transfer strength.", "labels": [], "entities": [{"text": "sentiment transfer task", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.912808875242869}, {"text": "content preservation", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7696943581104279}]}, {"text": "As for FT, our model outperforms the two baselines in terms of semantic and style scores.", "labels": [], "entities": [{"text": "FT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.8155229091644287}]}, {"text": "We list some example transferred sentences given by our model and two baseline systems in.", "labels": [], "entities": []}, {"text": "In the first example of negative-to-positive transfer, our model adheres to the topic of food service while baselines change to topic of food.", "labels": [], "entities": []}, {"text": "Similarly in the first example of positive-to-negative transfer, our model preserves the topic of chicken while CA model talks about pizza and MDS model talks about customer service.", "labels": [], "entities": []}, {"text": "Semantic similarity as explicit semantic constraints in our model is shown to be better at preserving the topic of source sentences.", "labels": [], "entities": []}, {"text": "There is still space to improve content preservation in all models.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7554486095905304}]}, {"text": "In the second example of informal-to-formal transfer, all transferred sentences miss the segment of \"take a deep breathe\" in the source sentence.", "labels": [], "entities": [{"text": "informal-to-formal transfer", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7271310985088348}]}, {"text": "In the second example of formal-to-informal transfer, the three transferred sentences miss part of source information.", "labels": [], "entities": [{"text": "formal-to-informal transfer", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7420750260353088}]}, {"text": "The source sentence is a rhetorical question, which truly means \"people hardly understand the meaning behind their behavior\".", "labels": [], "entities": []}, {"text": "This is a hard example, and all models do not capture its semantic meaning accurately.", "labels": [], "entities": []}, {"text": "FT task is more challenging compared with ST given that the sentence structure is more complicated with a larger vocabulary in the formality dataset.", "labels": [], "entities": [{"text": "FT task", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.5766842663288116}]}], "tableCaptions": [{"text": " Table 1: Data sizes of sentiment and formality transfer.", "labels": [], "entities": [{"text": "formality transfer", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.757011204957962}]}, {"text": " Table 3: Automatic evaluation of text style transfer systems on sentiment and formality transfer.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6213517884413401}, {"text": "sentiment and formality transfer", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.7365957051515579}]}, {"text": " Table 4: Human judgments of transferred sentences", "labels": [], "entities": []}]}