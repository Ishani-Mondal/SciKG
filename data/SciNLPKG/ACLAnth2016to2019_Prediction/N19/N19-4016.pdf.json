{"title": [{"text": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation", "labels": [], "entities": [{"text": "Open-Domain Story Generation", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6087673902511597}]}], "abstractContent": [{"text": "Story composition is a challenging problem for machines and even for humans.", "labels": [], "entities": [{"text": "Story composition", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8468950986862183}]}, {"text": "We present a neural narrative generation system that interacts with humans to generate stories.", "labels": [], "entities": []}, {"text": "Our system has different levels of human interaction , which enables us to understand at what stage of story-writing human collaboration is most productive, both to improving story quality and human engagement in the writing process.", "labels": [], "entities": []}, {"text": "We compare different varieties of interaction in story-writing, story-planning, and diversity controls under time constraints, and show that increased types of human collaboration at both planning and writing stages results in a 10-50% improvement in story quality as compared to less interactive baselines.", "labels": [], "entities": []}, {"text": "We also show an accompanying increase in user engagement and satisfaction with stories as compared to our own less interactive systems and to previous turn-taking approaches to interaction.", "labels": [], "entities": []}, {"text": "Finally, we find that humans tasked with collaboratively improving a particular characteristic of a story are in fact able to do so, which has implications for future uses of human-in-the-loop systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Collaborative human-machine story-writing has had a recent resurgence of attention from the research community.", "labels": [], "entities": []}, {"text": "It represents a frontier for AI research; as a research community we have developed convincing NLP systems for some generative tasks like machine translation, but lag behind in creative areas like open-domain storytelling.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.6895892769098282}]}, {"text": "Collaborative open-domain storytelling incorporates human interactivity for one of two aims: to improve human creativity via the aid of a machine, or to improve machine quality via the aid of a human.", "labels": [], "entities": []}, {"text": "Previously existing approaches treat the former aim, and have shown that storytelling systems are not yet developed enough to help human writers.", "labels": [], "entities": []}, {"text": "We attempt the latter, with the goal of investigating at what stage human collaboration is most helpful.", "labels": [], "entities": []}, {"text": "use an information retrieval based system to write by alternating turns between a human and their system.", "labels": [], "entities": []}, {"text": "use a similar turn-taking approach to interactivity, but employ a neural model for generation and allow the user to edit the generated sentence before accepting it.", "labels": [], "entities": []}, {"text": "They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction.", "labels": [], "entities": []}, {"text": "(2017) experiment with a userdriven setup, where the machine doesn't generate until the user requests it to, and then the user can editor delete at will.", "labels": [], "entities": []}, {"text": "They leverage useracceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful generation.", "labels": [], "entities": []}, {"text": "All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user's ability to knit a coherent overall story together out of locally related sentences.", "labels": [], "entities": []}, {"text": "They also do not allow a user to control the novelty or \"unexpectedness\" of the generations, which find to be a weakness.", "labels": [], "entities": []}, {"text": "Nor do they enable iteration; a user cannot revise earlier sentences and have the system update later generations.", "labels": [], "entities": []}, {"text": "We develop a system 1 that allows a user to interact in all of these ways that were limitations in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty.", "labels": [], "entities": []}, {"text": "We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.", "labels": [], "entities": []}, {"text": "We have two main interfaces that enable hu- Figure 1: Diagram of human-computer interaction mediated by the the demo system.", "labels": [], "entities": []}, {"text": "The dotted arrows represent optional interactions that the user can take.", "labels": [], "entities": []}, {"text": "Depending on the set-up, the user may choose to interact with one or all story models.", "labels": [], "entities": []}, {"text": "man interaction with the computer.", "labels": [], "entities": []}, {"text": "There is crossmodel interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models fora human to compare.", "labels": [], "entities": []}, {"text": "The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity.", "labels": [], "entities": []}, {"text": "The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories.", "labels": [], "entities": []}, {"text": "The full range of interactions available to a user is: select a model, provide a topic, change diversity of content, collaborate on the planning for the story, and collaborate on the story sentences.", "labels": [], "entities": []}, {"text": "It is entirely user-driven, as the users control how much is their own work and how much is the machine's at every stage.", "labels": [], "entities": []}, {"text": "It supports revision; a user can modify an earlier part of a written story or of the story plan at any point, and observe how this affects later generations.", "labels": [], "entities": []}, {"text": "2 System Description 2.1 System Overview shows a diagram of the interaction system.", "labels": [], "entities": []}, {"text": "The dotted arrows represent optional user interactions.", "labels": [], "entities": []}, {"text": "Cross-model mode requires the user to enter a topic, such as \"the not so haunted house\", and can optionally vary the diversity used in the STORY-LINE PLANNER or the STORY WRITER.", "labels": [], "entities": [{"text": "STORY WRITER", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.8088144361972809}]}, {"text": "Diversity numbers correspond directly to softmax temperatures, which we restrict to a reasonable range, determined empirically.", "labels": [], "entities": []}, {"text": "The settings are sent to the STORYLINE PLANNER module, which generates a storyline for the story in the form of a sequence of phrases as per the method of.", "labels": [], "entities": [{"text": "STORYLINE PLANNER", "start_pos": 29, "end_pos": 46, "type": "METRIC", "confidence": 0.6704038679599762}]}, {"text": "Everything is then sent to the STORY WRITER, which will return three stories.", "labels": [], "entities": [{"text": "STORY WRITER", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.8015308380126953}]}, {"text": "Intra-model mode enables advanced interactions with one story system of the user's choice.", "labels": [], "entities": []}, {"text": "The STORYLINE PLANNER returns either one storyline phrase or many, and composes the final storyline out of the combination of phrases the system generated, the user has written, and edits the user has made.", "labels": [], "entities": [{"text": "STORYLINE PLANNER", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.7725180089473724}]}, {"text": "These are sent to the STORY WRITER, which returns either a single sentence or a full story as per user's request.", "labels": [], "entities": [{"text": "STORY WRITER", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.7311243712902069}]}, {"text": "The process is flexible and iterative.", "labels": [], "entities": []}, {"text": "The user can choose how much or little content they want to provide, edit, or re-generate, and they can return to any step at anytime until they decide they are done.", "labels": [], "entities": []}, {"text": "Pre-/Post-processing and OOV handling To enable interactive flexibility, the system must handle open-domain user input.", "labels": [], "entities": [{"text": "OOV handling", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7107890844345093}]}, {"text": "User input is lowercased and tokenized to match the model training data via spaCy 2 . Model output is naively detokenized via Moses () based on feedback from users that this was more natural.", "labels": [], "entities": []}, {"text": "User input OOV handling is done via WordNet by recursively searching for hypernyms and hyponyms (in that order) until either an in-vocabulary word is found or until a maximum distance from the initial word is reached.", "labels": [], "entities": [{"text": "OOV handling", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.9452718794345856}, {"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9552062749862671}]}, {"text": "We additionally experimented with using cosine similarity to GloVe vectors (), but found that to be slower and not qualitatively better for this domain.", "labels": [], "entities": []}, {"text": "shows the variety of interactions a user can take in intra-model interaction, and is annotated with an example-in-action.", "labels": [], "entities": []}, {"text": "User inserted text is underlined in blue, generated text that has been removed by the user is in grey strike-through.", "labels": [], "entities": []}, {"text": "The refresh symbol marks areas that the user re-generated to get a different sentence (presumably after being unhappy with the first result).", "labels": [], "entities": []}, {"text": "As can be seen in this example, minor user involvement can result in a significantly better story.", "labels": [], "entities": []}, {"text": "The Title-to-Story system is a baseline, which generates directly from topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with six types of interaction: five variations created by restricting different capabilities of our system, and a sixth turn-taking baseline that mimics the interaction of the previous work).", "labels": [], "entities": []}, {"text": "We choose our experiments to address the research questions: What type of interaction is most engaging?", "labels": [], "entities": []}, {"text": "Which type results in the best stories?", "labels": [], "entities": []}, {"text": "Can a human tasked with correcting for certain weaknesses of a model successfully do so?", "labels": [], "entities": []}, {"text": "The variations on interactions that we tested are: 1.", "labels": [], "entities": []}, {"text": "Machine only: no human-in-loop.", "labels": [], "entities": []}, {"text": "2. Diversity only: user can compare and select models but only diversity is modifiable.", "labels": [], "entities": []}, {"text": "3. Storyline only: user collaborates on storyline but not story.", "labels": [], "entities": []}, {"text": "4. Story only: user collaborates on story but not storyline.", "labels": [], "entities": []}, {"text": "5. All: user can modify everything.", "labels": [], "entities": []}, {"text": "6. Turn-taking: user and machine take turns writing a sentence each (user starts).", "labels": [], "entities": []}, {"text": "user can edit the machine-generations, but once they move onto later sentences, previous sentences are read-only.", "labels": [], "entities": []}, {"text": "We expand experiment 5 to answer the question of whether a human-in-the-loop interactive sys-tem can address specific shortcomings of generated stories.", "labels": [], "entities": []}, {"text": "We identify three types of weaknesses common to generation systems -Creativity, Relevance, and Causal & Temporal Coherence, and conduct experiments where the human is instructed to focus on improving specifically one of them.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9825314879417419}]}, {"text": "The targeted human improvement areas intentionally match the Plan-and-Revise discriminators, so that, if successful, the \"human discriminator\" data can assist in training the machine discriminators.", "labels": [], "entities": []}, {"text": "All experiments (save experiment 2, which lets the user pick between models) use the Plan-and-Revise system.", "labels": [], "entities": []}, {"text": "Story Quality As shown in, human involvement of any kind under tight constraints helps story quality across all metrics, with mostly better results the more collaboration is allowed.", "labels": [], "entities": []}, {"text": "The exception to this trend is Story only collaboration, which performs best or close to best across the board.", "labels": [], "entities": []}, {"text": "This was unexpected; it is possible that these users benefited from having to learn to control only one model, instead of both, given the limited time.", "labels": [], "entities": []}, {"text": "It is also possible that being forced to be reliant on system storylines made these users more creative.", "labels": [], "entities": []}, {"text": "Turn-taking Baseline The turn-taking baseline performs comparably in overall quality and relevance to other equally interactive experiments (Story only, All, All+).", "labels": [], "entities": []}, {"text": "It achieves highest scores in relevance, though the top five systems for relevance are not statistically significantly different.", "labels": [], "entities": []}, {"text": "It is outperformed on creativity and causal-temporal coherence by the strong Story only variation, as well as the All, All+ systems.", "labels": [], "entities": []}, {"text": "This suggests that local sentence-level editing is sufficient to keep a story on topic and to write well, but that creativity and causal-temporal coherence require some degree of global cohesion that is assisted by iterative editing.", "labels": [], "entities": []}, {"text": "The same observation as to the strength of Story only over All applies here as well; turntaking is the least complex of the interactive systems, and may have boosted performance from being simpler since time was constrained and users used the system only once.", "labels": [], "entities": []}, {"text": "Thus a turn-based system is a good choice fora scenario where users use a system infrequently or only once, but the comparative performance may decrease in future experiments with more relaxed time constraints or where users use the system repeatedly.", "labels": [], "entities": []}, {"text": "Targeted Improvements The results within the All and All + setups confirm that stories can be improved with respect to a particular metric.", "labels": [], "entities": []}, {"text": "The diagonal of strong scores displays this trend, where the creativity-focused experiment has high creativity, etc.", "labels": [], "entities": []}, {"text": "An interesting side effect to note is that focusing on anything tends to produce better stories, reflected by higher overall ratings.", "labels": [], "entities": []}, {"text": "All + Relevance is an exception which does not help creativity or overall (perhaps because relevance instantly becomes very high as soon a human is involved), but apart from that All + experiments are better across all metrics than All.", "labels": [], "entities": []}, {"text": "This could mean a few things: that when a user improves a story in one aspect, they improve it along the other axes, or that users reading stories have trouble rating aspects entirely independently.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: User self-reported scores, from 1-5. E: En- tertainment value, Q: Quality of Story, S: Satisfaction  with Story. Note that the final column Use Again is  based on converting \"no\" to 0, \"conditional\" to 1, and  \"yes\" to 2.", "labels": [], "entities": [{"text": "En- tertainment value", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.9592278748750687}]}, {"text": " Table 2: Results for all experiments, from 1-5. Best  scores per metric are bolded, scores not significantly  different (\u03b1 = 0.1, per Wilcoxon Signed-Rank Test)  are starred. C-T stands for Causal-Temporal Coher- ence, the + experiments are the extensions where the  user focuses on improving a particular quality.", "labels": [], "entities": [{"text": "Wilcoxon Signed-Rank Test", "start_pos": 135, "end_pos": 160, "type": "DATASET", "confidence": 0.7291224797566732}]}, {"text": " Table 3: Training parameters for models used in demo.", "labels": [], "entities": []}]}