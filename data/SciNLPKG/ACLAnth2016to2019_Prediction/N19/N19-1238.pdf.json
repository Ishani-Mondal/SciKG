{"title": [{"text": "Block and Group Regularized Sparse Modeling for Dictionary Learning Knowledge (dictionary learning, CONJUNCTION, sparse coding) ; (optimization problems, USED-FOR, dictionary learning) ; (optimization problems, USED-FOR, sparse coding). . . GraphWriter Sparse representations have recently been shown to be effective in many optimization problems. However, existing dictionary learning methods are limited in the number of dictionary blocks, which can be expensive to obtain. In this paper, we propose a novel approach to dictionary learning based on sparse coding . . .", "labels": [], "entities": [{"text": "USED-FOR", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.941484272480011}, {"text": "USED-FOR", "start_pos": 211, "end_pos": 219, "type": "DATASET", "confidence": 0.9406600594520569}]}], "abstractContent": [{"text": "Generating texts which express complex ideas spanning multiple sentences requires a struc-tured representation of their content (docu-ment plan), but these representations are prohibitively expensive to manually produce.", "labels": [], "entities": []}, {"text": "In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7256042659282684}]}, {"text": "Graph-ical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety.", "labels": [], "entities": [{"text": "text generation", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7513998746871948}]}, {"text": "We introduce a novel graph transforming en-coder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints.", "labels": [], "entities": []}, {"text": "Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text.", "labels": [], "entities": [{"text": "graph-to-text generation", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.7918732166290283}]}, {"text": "Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Increases in computing power and model capacity have made it possible to generate mostlygrammatical sentence-length strings of natural language text.", "labels": [], "entities": []}, {"text": "However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge.", "labels": [], "entities": []}, {"text": "The difficulties are compounded in domains of interest such as scientific writing.", "labels": [], "entities": []}, {"text": "Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider the problem of generating a text from automatically extracted information (knowledge).", "labels": [], "entities": []}, {"text": "IE systems can produce high quality knowledge fora variety of domains, synthesizing information from across sentence and even document boundaries.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9599924683570862}]}, {"text": "Generating coherent text from knowledge requires a model which considers global characteristics of the knowledge as well as local characteristics of each entity.", "labels": [], "entities": []}, {"text": "This feature of the task motivates our use of graphs for representing knowledge, where neighborhoods localize important information and paths through the graph build connections between distant nodes through intermediate ones.", "labels": [], "entities": []}, {"text": "An example knowledge graph can be seen in.", "labels": [], "entities": []}, {"text": "We formulate our problem as follows: given the title of a scientific article and a knowledge graph constructed by an automatic information extraction system, the goal is to generate an abstract that a) is appropriate for the given title and b) expresses the content of the knowledge graph in natural language text.", "labels": [], "entities": []}, {"text": "To evaluate how well a model accomplishes this goal, we introduce the Abstract GENeration DAtaset (AGENDA), a dataset of knowledge graphs paired with scientific abstracts.", "labels": [], "entities": [{"text": "Abstract GENeration DAtaset (AGENDA)", "start_pos": 70, "end_pos": 106, "type": "METRIC", "confidence": 0.883225699265798}]}, {"text": "Our dataset consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences.", "labels": [], "entities": []}, {"text": "For each abstract, we create a knowledge graph in two steps.", "labels": [], "entities": []}, {"text": "First, we apply the SciIE system of, a state-of-the-art sciencedomain information extraction system.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6925611793994904}]}, {"text": "This system provides named entity recognition for scientific terms, with entity types Task, Method, Metric, Material, or Other Scientific Term.", "labels": [], "entities": []}, {"text": "The model also produces co-reference annotations as well as seven relations that can obtain between different entities (Compare, Used-for, Feature-of, Hyponymof, Evaluate-for, and Conjunction).", "labels": [], "entities": []}, {"text": "For example, in, the node labeled \"SemEval 2011 Task 11\" is of type 'Task', \"HMM Models\" is of type 'Model', and there is a 'Evaluate-For' relation showing that the models are evaluated on the task.", "labels": [], "entities": [{"text": "SemEval 2011 Task 11", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.5773274451494217}]}, {"text": "We form these annotations into knowledge graphs.", "labels": [], "entities": []}, {"text": "We collapse co-referential entities into a single node associated with the longest mention (on the assumption that these will be the most informative).", "labels": [], "entities": []}, {"text": "We then connect nodes to one another using the relation annotations, treating these as labeled edges in the graph.", "labels": [], "entities": []}, {"text": "The result is a possibly unconnected graph representation of the SciIE annotations fora given abstract.", "labels": [], "entities": []}, {"text": "Statistics of the AGENDA dataset are available in.", "labels": [], "entities": [{"text": "AGENDA dataset", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.8657965064048767}]}, {"text": "We split the AGENDA dataset into 38,720 training, 1000 validation, and 1000 test datapoints.", "labels": [], "entities": [{"text": "AGENDA dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.8143102824687958}]}, {"text": "We offer standardized data splits to facilitate comparison.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We evaluate using a combination of human and automatic evaluations.", "labels": [], "entities": []}, {"text": "For human evaluation, participants were asked to compare abstracts generated by various models and those written by the authors of the scientific articles.", "labels": [], "entities": []}, {"text": "We used Best-Worst Scaling (BWS;), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales.", "labels": [], "entities": [{"text": "Best-Worst Scaling (BWS;)", "start_pos": 8, "end_pos": 33, "type": "METRIC", "confidence": 0.8072345912456512}]}, {"text": "Participants were presented with two or three abstracts and asked to decide which one was better and which one was worse in order of grammar and fluency (is the abstract written in well-formed English?), coherence (does the abstract have an introduction, state the problem or task, describe a solution, and discuss evaluations or results?), and informativeness (does the abstract relate to the provided title and make use of appropriate scientific terms?).", "labels": [], "entities": []}, {"text": "We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria.", "labels": [], "entities": []}, {"text": "Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints.", "labels": [], "entities": []}, {"text": "The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged.", "labels": [], "entities": []}, {"text": "To supplement this, we also provide automatic metrics.", "labels": [], "entities": []}, {"text": "We use BLEU (), an n-gram overlap measure popular in text generation tasks, and METEOR), a machine translation with paraphrase and language-specific considerations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.998993456363678}, {"text": "text generation tasks", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8045964042345682}, {"text": "METEOR", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9828743934631348}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7090652883052826}]}, {"text": "Comparisons We compare our GraphWriter against several strong baselines.", "labels": [], "entities": []}, {"text": "In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of.", "labels": [], "entities": [{"text": "GAT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.733413577079773}]}, {"text": "This encoder consists of PReLU activations stacked between 6 self-attention layers.", "labels": [], "entities": []}, {"text": "To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter).", "labels": [], "entities": []}, {"text": "Finally, we compare with the gated rewriter model of.", "labels": [], "entities": []}, {"text": "This model uses only the document title to iteratively rewrite drafts of its output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics of our AGENDA dataset. Aver- ages are computed per instance.", "labels": [], "entities": [{"text": "AGENDA dataset", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9120289087295532}, {"text": "Aver- ages", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9654478828112284}]}, {"text": " Table 2: Automatic Evaluations of Generation Sys- tems.", "labels": [], "entities": []}, {"text": " Table 3: Does knowledge improve generation? Human  evaluations of best and worst abstract.", "labels": [], "entities": []}, {"text": " Table 4: Human Judgments of GraphWriter and Enti- tyWriter models.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of generation without knowledge  and with Inferred Knowledge (InferEntityWriter)", "labels": [], "entities": []}]}