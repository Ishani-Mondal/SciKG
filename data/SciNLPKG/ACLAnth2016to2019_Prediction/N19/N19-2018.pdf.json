{"title": [{"text": "Improving Knowledge Base Construction from Robust Infobox Extraction", "labels": [], "entities": [{"text": "Improving Knowledge Base Construction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8944159597158432}]}], "abstractContent": [{"text": "A capable, automatic Question Answering (QA) system can provide more complete and accurate answers using a comprehensive knowledge base (KB).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7989420354366302}]}, {"text": "One important approach to constructing a comprehensive knowledge base is to extract information from Wikipedia infobox tables to populate an existing KB.", "labels": [], "entities": []}, {"text": "Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DB-pedia), three major challenges remain: 1) De-terministic extraction patterns used in DBpe-dia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB.", "labels": [], "entities": [{"text": "Infobox Extraction (IBE) problem", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.779391681154569}, {"text": "entity disambiguation", "start_pos": 259, "end_pos": 280, "type": "TASK", "confidence": 0.7019259184598923}, {"text": "Heuristic-based extraction of unlinkable entities", "start_pos": 292, "end_pos": 341, "type": "TASK", "confidence": 0.7656319260597229}, {"text": "precision", "start_pos": 353, "end_pos": 362, "type": "METRIC", "confidence": 0.9965915679931641}, {"text": "accuracy", "start_pos": 377, "end_pos": 385, "type": "METRIC", "confidence": 0.9989683628082275}]}, {"text": "This paper presents a robust approach that tackles all three challenges.", "labels": [], "entities": []}, {"text": "We build probabilistic models to predict relations between entity mentions directly from the infobox tables in HTML.", "labels": [], "entities": []}, {"text": "The entity mentions are linked to identifiers in an existing KB if possible.", "labels": [], "entities": []}, {"text": "The unlinkable ones are also parsed and preserved in the final output.", "labels": [], "entities": []}, {"text": "Training data for both the relation extraction and the entity linking models are automatically generated using distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8342710137367249}]}, {"text": "We demonstrate the empirical effectiveness of the proposed method in both precision and recall compared to a strong IBE baseline, DBpe-dia, with an absolute improvement of 41.3% in average F 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995697140693665}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9994254112243652}, {"text": "F 1", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9780064523220062}]}, {"text": "We also show that our extraction makes the final KB significantly more complete, improving the completeness score of list-value relation types by 61.4%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most existing knowledge bases (KBs) are largely incomplete.", "labels": [], "entities": []}, {"text": "This can be seen in Wikidata, which is a widely used * Work performed at Apple Inc.", "labels": [], "entities": []}, {"text": "knowledge graph created largely by human editors.", "labels": [], "entities": []}, {"text": "Only 46% of person entities in Wikidata have birth places available . An estimate of 584 million facts are maintained in Wikipedia, not in Wikidata (.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.9271470904350281}]}, {"text": "A downstream application such as Question Answering (QA) will suffer from this incompleteness, and fail to answer certain questions or even provide an incorrect answer especially fora question about a list of entities due to a closed-world assumption.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8545698881149292}]}, {"text": "Previous work on enriching and growing existing knowledge bases includes relation extraction on natural language text (), knowledge base reasoning from existing facts, and many others (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.9062852561473846}, {"text": "knowledge base reasoning from existing facts", "start_pos": 122, "end_pos": 166, "type": "TASK", "confidence": 0.7339304536581039}]}, {"text": "Wikipedia (https://wikipedia.org) has been one of the key resources used for knowledge base construction.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9025835394859314}, {"text": "knowledge base construction", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6440446674823761}]}, {"text": "In many Wikipedia pages, a summary table of the subject, called an infobox table, is presented in the top right region of the page (see the leftmost table in for the infobox table of The_Beatles).", "labels": [], "entities": []}, {"text": "Infobox tables offer a unique opportunity for extracting information and populating a knowledge base.", "labels": [], "entities": []}, {"text": "An infobox table is structurally formatted as an HTML table and therefore it is often not necessary to parse the text into a syntactic parse tree as in natural language extraction.", "labels": [], "entities": [{"text": "natural language extraction", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7964756290117899}]}, {"text": "Intra-Wikipedia anchor links are prevalent in infobox tables, often providing unambiguous references to entities.", "labels": [], "entities": []}, {"text": "Most importantly, a significant amount of information represented in the infobox tables are not otherwise available in a more traditional structured knowledge base, such as Wikidata.", "labels": [], "entities": []}, {"text": "We are not the first to use infobox tables for knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.674124280611674}]}, {"text": "The pioneering work of) 2 extracts canonical knowledge triples (subject, relation type, object) from infobox tables with a manually created mapping from Mediawiki templates to relation types.", "labels": [], "entities": []}, {"text": "Despite the success of the DBpedia project, three major challenges remain.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8890865445137024}]}, {"text": "First, deterministic mappings are sensitive to template changes.", "labels": [], "entities": []}, {"text": "If Wikipedia modifies an infobox template (e.g., the attribute \"birthDate\" is renamed to \"dateOfBirth\"), the DBpedia mappings need to be manually updated.", "labels": [], "entities": []}, {"text": "Secondly, while Wikipedia anchor links facilitate disambiguation of string values in the infobox tables, blindly trusting the anchor links can cause errors.", "labels": [], "entities": []}, {"text": "For instance, both \"Sasha\" and \"Malia,\" children of Barack Obama, are linked to a section of the Wikipedia page of Barack_Obama, rather than their own pages.", "labels": [], "entities": []}, {"text": "Finally, little attention has been paid to the extraction of unlinkable entities.", "labels": [], "entities": []}, {"text": "For example, Larry King has married seven women, only one of which can be linked to a Wikipedia page.", "labels": [], "entities": []}, {"text": "A knowledge base without the information of the other six entities will provide an incorrect answer to the question \"How many women has Larry King married?\"", "labels": [], "entities": []}, {"text": "In this paper, we present a system, RIBE, to tackle all three challenges: 1) We build probabilistic models to predict relations and object entity links.", "labels": [], "entities": []}, {"text": "The learned models are more robust to changes in the underlying data representation than manually maintained mappings.", "labels": [], "entities": []}, {"text": "2) We incorporate the information from HTML anchors and build an entity linking system to link string values to entities rather than fully relying on the anchor links.", "labels": [], "entities": []}, {"text": "3) We produce high-quality extractions even when the objects are unlinkable, which improves the completeness of the final knowledge base.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed method is effective in extracting over 50 relation types.", "labels": [], "entities": []}, {"text": "Compared to a strong IBE baseline, DBpedia, our extractions achieve significant improvements on both precision and recall, with a 41.3% increase in average F 1 score.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.7173370718955994}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9996800422668457}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9994685053825378}, {"text": "F 1 score", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9605422019958496}]}, {"text": "We also show that the extracted triples add a great value to an existing knowledge Throughout the paper, we use \"DBpedia\" to refer to its infobox extraction component rather than the DBpedia knowledge base unless specified.", "labels": [], "entities": [{"text": "DBpedia knowledge base", "start_pos": 183, "end_pos": 205, "type": "DATASET", "confidence": 0.9160674015680949}]}, {"text": "We use Wikidata as the baseline knowledge base for our experiments since it is updated more frequently.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9171355962753296}]}, {"text": "The last release DBpedia knowledge base was in 2016.", "labels": [], "entities": [{"text": "DBpedia knowledge base", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.9525763988494873}]}, {"text": "3 Mediawiki is a markup language that defines the page layout and presentation of Wikipedia pages.", "labels": [], "entities": []}, {"text": "base, Wikidata, improving an average recall of list-value relation types by 61.4%.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.8768366575241089}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9741787314414978}]}, {"text": "To summarize, our contributions are three-fold: \u2022 RIBE produces high-quality extractions, achieving higher precision and recall compared to DBpedia.", "labels": [], "entities": [{"text": "RIBE", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.47856736183166504}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9993300437927246}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9993711113929749}, {"text": "DBpedia", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.8907827734947205}]}, {"text": "\u2022 Our extractions make Wikidata more complete by adding a significant number of triples for 51 relation types.", "labels": [], "entities": []}, {"text": "\u2022 RIBE extracts relations with unlinkable entities, which are crucial for the completeness of list-value relation types and the question answering capability from a knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7611306309700012}]}], "datasetContent": [{"text": "In this section, we conduct experiments to answer the following questions: \u2022 Does RIBE produce high-quality extractions?", "labels": [], "entities": [{"text": "RIBE", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.4939112365245819}]}, {"text": "(Sec. 5.2) \u2022 Are these extractions a significant addition to Wikidata?", "labels": [], "entities": []}, {"text": "(Sec. 5.3) \u2022 How does the extraction with unlinkable entities affect the quality of the KB especially for the list-valued relation types?", "labels": [], "entities": []}, {"text": "(Sec. 5.4)  We evaluate the extraction quality for each relation type r \u2208 RT using four metrics: yield, precision, recall, and F 1 score.", "labels": [], "entities": [{"text": "yield", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9857218265533447}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.993117094039917}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9956040382385254}, {"text": "F 1 score", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9887428283691406}]}, {"text": "Yield (Y) is defined as the number of all uniquely extracted relations r(e 1 , e 2 ).", "labels": [], "entities": [{"text": "Yield (Y)", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8177207857370377}]}, {"text": "To compute precision and recall, we first collect aground truth set of relations G = {G r |r \u2208 RT }.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9991551637649536}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.993802547454834}]}, {"text": "We create a union of extractions from RIBE and DBpedia, and randomly sample around 100 entities with at least one relation extracted from either system.", "labels": [], "entities": [{"text": "RIBE", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.9165541529655457}, {"text": "DBpedia", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.7448943853378296}]}, {"text": "This is similar to the pooling methodology used in the TAC KBP evaluation process.", "labels": [], "entities": [{"text": "TAC KBP evaluation process", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.5949766784906387}]}, {"text": "The sampled extractions are graded by human annotators.", "labels": [], "entities": []}, {"text": "According to our annotation guidelines, we mark an extraction incorrect if one of the the following is met.", "labels": [], "entities": []}, {"text": "\u2022 The relation is not expressed in the infobox.", "labels": [], "entities": []}, {"text": "\u2022 The object entity has an incorrect identifier.", "labels": [], "entities": []}, {"text": "\u2022 The object of a relation triple is unlinked by the system but should be linked in the ground truth.", "labels": [], "entities": []}, {"text": "For instance, a string \"United States\" not linked to its entity identifier in Wikidata is considered incorrect.", "labels": [], "entities": []}, {"text": "\u2022 The object is incorrectly parsed.", "labels": [], "entities": []}, {"text": "For example, \"Sasha and Malia\" would bean incorrect extraction for the child relation of Barack Obama.", "labels": [], "entities": []}, {"text": "The final set Gr consists of all correct relations of the sampled entities from both approaches.", "labels": [], "entities": []}, {"text": "The number of labels varies from 83 for event:country to 557 for band:has_member, resulting in a total of 4,858 labels on triples and an average of 194 per relation type.", "labels": [], "entities": []}, {"text": "The Precision (P) of a system s, RIBE or DBpedia, is computed as P s r = |U s r \u2229Gr| |U s r | , where U s r is the set of all extracted relations of r by system s.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9462464302778244}, {"text": "RIBE", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.7955893874168396}, {"text": "DBpedia", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.6211422085762024}]}, {"text": "An absolute Recall (R) of the universe is difficult to compute.", "labels": [], "entities": [{"text": "absolute Recall (R)", "start_pos": 3, "end_pos": 22, "type": "METRIC", "confidence": 0.8682923793792725}]}, {"text": "We compute an estimated recall R s r = |U s r \u2229Gr| |Gr| w.r.t the ground truth set.", "labels": [], "entities": []}, {"text": "The standard F1 Score (F 1 ) is computed as a harmonic mean of Pr and R r . shows that RIBE achieves better precision, recall, F 1 , and yield for almost all relation types.", "labels": [], "entities": [{"text": "F1 Score (F 1 )", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9317979017893473}, {"text": "RIBE", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9461130499839783}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9995287656784058}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9996479749679565}, {"text": "F 1", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.995100200176239}, {"text": "yield", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9980016350746155}]}, {"text": "The DBpedia extractor underperforms for two reasons.", "labels": [], "entities": [{"text": "DBpedia extractor", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8527580797672272}]}, {"text": "First, the extraction fully relies on Wikipedia anchor links for entity linking, which not only hurts the precision due to erroneous links, but also results in a low linked ratio since mentions without anchor links will not be linked.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.6993053108453751}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.999498724937439}]}, {"text": "Secondly, it treats each row in an infobox table as one mention without proper chunking in the absence of anchor links.", "labels": [], "entities": []}, {"text": "This approach hurts both precision and recall, since an extracted string value of \"Sasha and Malia\" for child not only misses the correct entities for both Sasha and Malia, but also provides false information that \"Sasha and Malia\" represents one single person.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9991311430931091}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9992563128471375}]}, {"text": "In contrast, RIBE identifies object mentions from infobox rows and predicts entity links even when no anchor link exists.", "labels": [], "entities": []}, {"text": "Also, RIBE is able to consistently link to entities whose types are compatible to the target relation type.", "labels": [], "entities": []}, {"text": "For example, we consistently link to an occupation object (e.g., Lawyer) for occupation rather than to a discipline (e.g., Law).", "labels": [], "entities": []}, {"text": "shows that the percentage of relation triples with their objects linked to Wikidata entities (\"N/A\" if the object type is a literal value).", "labels": [], "entities": []}, {"text": "* We use P527 to represent band has member where the subject entity is a band and the object entity is a current or past member of the band.", "labels": [], "entities": []}, {"text": "In this appendix, we present the complete version of: Comparison with Wikidata.", "labels": [], "entities": []}, {"text": "The column Wikidata yield shows the total number of relation triples in Wikidata per relation type.", "labels": [], "entities": []}, {"text": "RIBE yield shows the total number of relation triples extracted by RIBE.", "labels": [], "entities": [{"text": "RIBE", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.891451358795166}]}, {"text": "+Yield shows the number of relation triples we extract that are not in Wikidata.", "labels": [], "entities": []}, {"text": "Linked (%) shows that the percentage of relation triples with their objects linked to Wikidata entities.", "labels": [], "entities": []}, {"text": "* We use P527 to represent band has member where the subject entity is a band and the object entity is a current or past member of the band.", "labels": [], "entities": []}, {"text": "Note that parent and largest city in do not currently exist in Wikidata.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9261471033096313}]}, {"text": "** We combine relations for Wikidata predicates father (P22) and mother (P25) to form the parent relation type.", "labels": [], "entities": []}, {"text": "*** We collect all the cities located in every location entity, e, in Wikidata that contains cities and use the city with the largest and latest population number as the largest city of e.", "labels": [], "entities": []}, {"text": "This way, we construct a total of 242 largest city relations from Wikidata.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Infobox subject entity type breakdown.", "labels": [], "entities": [{"text": "Infobox subject entity type breakdown", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.5522101759910584}]}, {"text": " Table 3: Comparison with DBpedia. P /R/F1 denotes Precision/Recall/F1 measures. C denotes the completeness score for", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7924556136131287}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.8954523205757141}, {"text": "Precision/Recall/F1 measures", "start_pos": 51, "end_pos": 79, "type": "METRIC", "confidence": 0.7745997905731201}]}, {"text": " Table 4: Comparison with Wikidata for a sample of relation types. See Table 8 in Appendix C for the full list. The column", "labels": [], "entities": []}, {"text": " Table 8: Comparison with Wikidata. The column Wikidata yield shows the total number of relation triples in  Wikidata per relation type. RIBE yield shows the total number of relation triples extracted by RIBE. +Yield shows  the number of relation triples we extract that are not in Wikidata. Linked (%) shows that the percentage of relation  triples with their objects linked to Wikidata entities.  *  We use P527 to represent band has member where the  subject entity is a band and the object entity is a current or past member of the band. Note that parent and largest  city in", "labels": [], "entities": [{"text": "RIBE yield", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9243339002132416}]}]}