{"title": [{"text": "Quantifiers in a Multimodal World: Hallucinating Vision with Language and Sound", "labels": [], "entities": []}], "abstractContent": [{"text": "Inspired by the literature on multisensory integration , we develop a computational model to ground quantifiers in perception.", "labels": [], "entities": [{"text": "multisensory integration", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.807460218667984}]}, {"text": "The model learns to pick out of nine quantifiers ('few', 'many', 'all', etc.) the one that is more likely to describe the percent of animals in a visual-auditory input containing both animals and ar-tifacts.", "labels": [], "entities": []}, {"text": "We show that relying on concurrent sensory inputs increases model performance on the quantification task.", "labels": [], "entities": []}, {"text": "Moreover, we evaluate the model in a situation in which only the auditory modality is given, while the visual one is 'hallucinanted' either from the auditory input itself or from a linguistic caption describing the quantity of entities in the auditory input.", "labels": [], "entities": []}, {"text": "This way, the model exploits prior associations between modalities.", "labels": [], "entities": []}, {"text": "We show that the model profits from the prior knowledge and outperforms the auditory-only setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quantifiers (words like 'some', 'most', 'all') have long been the holy grail of formal semanticists (see for an overview).", "labels": [], "entities": []}, {"text": "More recently, they have caught the attention of cognitive scientists, who showed that these expressions are handled by children quite early in life, even before developing the ability to count).", "labels": [], "entities": []}, {"text": "Though some effort has been paid to model these highfrequency expressions from their use in big corpora of texts (, relatively little work has focused on the models' ability to quantify using these words.", "labels": [], "entities": []}, {"text": "In computer vision, some focus to the task of extracting quantities from images has been expressed through visual question answering, whose benchmark dataset ( contains 'count questions' (e.g., 'How many Xs have the property Y?') that repeatedly turned out to be rather challenging (.", "labels": [], "entities": [{"text": "visual question answering", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7443302869796753}]}, {"text": "While this work paid little attention to quantifiers, a few recent studies specifically investigated their computational learning from visual inputs (.", "labels": [], "entities": []}, {"text": "These works built on the evidence that (part of) the meaning of quantifiers is grounded in perception.", "labels": [], "entities": []}, {"text": "However, they only experimented with the visual modality, though the numerical representations humans derive from sensory inputs have been shown to be shared across modalities, e.g., vision and sound).", "labels": [], "entities": []}, {"text": "In the literature on multisensory integration it is well established that redundant information conveyed through different sensory inputs leads to a better performance on semantic tasks.", "labels": [], "entities": [{"text": "multisensory integration", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8357979953289032}]}, {"text": "These findings have brought researchers to propose the 'Hub and Spoke' model (hence, H&S): concepts are learned by mutual interaction of the representation produced by sensory specific processors, the 'spokes', with a transmodal 'hub' (.", "labels": [], "entities": []}, {"text": "The role of the cross-modal hub is to take each of the spokes' output and to reproduce the correct information across the others by back-propagation (.", "labels": [], "entities": []}, {"text": "There is evidence that memory recall is affected by the multisensory context in which the concept was learned.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9086586833000183}]}, {"text": "In particular, it has been shown that a congruent pair of audiovisual inputs may facilitate subsequent recall.", "labels": [], "entities": []}, {"text": "In other words, we learn to process a sound (e.g., 'meow' or 'woof') and to associate it to the visual representation of the entity we see making it, and this facilitates the recall of the corresponding concept (i.e., 'cat' or 'dog').", "labels": [], "entities": [{"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.8782978653907776}]}, {"text": "In this work, we apply the H&S model to the conceptual learning of quantifiers and study how the hub learns to integrate the visual and auditory spoke representations (as illustrated in) to perform the quantification task.", "labels": [], "entities": []}, {"text": "That is, the model has to learn to say that 'none', 'few', 'most', etc. of the objects in the visual and auditory inputs belong to a given category, that of animals.", "labels": [], "entities": []}, {"text": "We focus on 9 common quantifiers and experiment with visual and auditory inputs strongly aligned (viz., aligned at the entity level).", "labels": [], "entities": []}, {"text": "We show that \u2022 Using congruent audiovisual inputs increases the performance of the model in learning quantifiers within single-sensory models; \u2022 The H&S model can generalize to unseen data quite well.", "labels": [], "entities": []}, {"text": "In particular, it generalizes better when trained on small combinations and tested on large ones than vice versa.", "labels": [], "entities": []}, {"text": "Furthermore, a second part of our work is based on an ongoing debate in multisensory integration, namely whether the processing of sensory inputs is passive or rather influenced by previous experience that creates cross-sensory associations.", "labels": [], "entities": [{"text": "multisensory integration", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.7897672951221466}]}, {"text": "Within this debate, one of the most influential frameworks is the Predictive Coding Model (hence, PCM), according to which prior knowledge affects the representation of perceptual inputs.", "labels": [], "entities": []}, {"text": "There is a general agreement on the predictive effects between visual and auditory inputs, whereas the role of language in priming visual perception is still under debate (see for an overview).", "labels": [], "entities": []}, {"text": "Inspired by this work, we compare a single auditory sensory model with a model in which the processing of the auditory stimuli is facilitated by prior expectation elicited by either the visual spoke (implemented as a mapping from the experienced auditory input to its corresponding visual representation) or the language input (again implemented as a mapping from language to visual representations).", "labels": [], "entities": []}, {"text": "In, the 'prior' arrow illustrates this predictive factor.", "labels": [], "entities": []}, {"text": "Simplifying somewhat, we simulate a setting where a model, trained to quantify from co-occurring synchronous audiovisual inputs, is tested on a situation where (a) it hears but does not seethe entities (audio-vision association prior) or (b) it reads a description of the entities and hears their sounds but does not see them (language-vision association prior).", "labels": [], "entities": []}, {"text": "We show that \u2022 Using priors hallucinating the visual representation improves the performance of the model compared to when it receives only auditory inputs; \u2022 Language prior is slightly more effective than sound prior to hallucinate concurring vision.", "labels": [], "entities": []}, {"text": "Overall, these works repeatedly showed that combining information from language and vision leads to representations that are beneficial in virtually any task.", "labels": [], "entities": []}, {"text": "A relatively recent strand of research focused on the integration of visual and sound information, where the latter is, e.g., the 'roar' of a fast car (.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following, our datasets consist of scenes containing animals and artifacts with a minimum of 3 and a maximum of 20 entities in total.", "labels": [], "entities": []}, {"text": "There are in total 17 proportions, out of which 8 contain more animals than artifacts, 8 contain more artifacts than animals, and 1 contains an equal number of them.", "labels": [], "entities": []}, {"text": "1 For each proportion we generated scenes containing all possible combinations of cardinalities: For the proportion 0%, for example, 17 combinations were built, ranging from 0:3 (0 animals, 3 artifacts) to 0:20.", "labels": [], "entities": []}, {"text": "We built visual and auditory datasets aligned at the entity level: For each image, we created the corresponding auditory datapoint containing the sound of each entity in the image.", "labels": [], "entities": []}, {"text": "By so doing, using the terminology of (, we obtained strongly aligned visual and auditory datasets.", "labels": [], "entities": []}, {"text": "In total, we used 55 unique animals and 55 unique artifacts.", "labels": [], "entities": []}, {"text": "We only used those entities for which we could have whole-depicting images (not just parts) and for which we had a corresponding sound.", "labels": [], "entities": []}, {"text": "Furthermore, for each audio-visual input we created a corresponding linguistic caption describing the quantities of the entities in it.", "labels": [], "entities": []}, {"text": "Details on the three datasets are provided below.", "labels": [], "entities": []}, {"text": "Visual Dataset Similarly to, we built a large dataset of synthetic visual scenes depicting a variable number of animals and artifacts on top of a neutral grey background (see).", "labels": [], "entities": [{"text": "Visual Dataset", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.829695463180542}]}, {"text": "The scenes were automatically generated using the following pipeline: (a) Natural images depicting target objects (e.g., a dog) or distractors (e.g., a car) were randomly picked up from the 110 entities pre-selected from the dataset by.", "labels": [], "entities": []}, {"text": "As opposed to the synthetic dataset of, where multiple copies of the same animal/artifact were reproduced in the scene, we have different target/distractor instances in each scenario (e.g, different instances of 'car' as in).", "labels": [], "entities": []}, {"text": "However, we do not vary the size and orientation of entities; (b) The proportion of targets in the scene was chosen by selecting only those matching the 17 pre-defined proportions mentioned above.", "labels": [], "entities": []}, {"text": "We generated 17K scenes balanced per proportion (1K scenes/proportion), and split them into train (70%), validation (10%), and test (20%) sets.", "labels": [], "entities": []}, {"text": "The distribution of proportions per total number of objects in the training set is illustrated in.", "labels": [], "entities": []}, {"text": "We followed a similar procedure to build the auditory scenes.", "labels": [], "entities": []}, {"text": "We took Audioset (Gemmeke et al., 2017) as our starting point to obtain sounds corresponding to the entities since it contains a huge collection of humanlabeled 10-sec sound clips.", "labels": [], "entities": []}, {"text": "It is organized as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.", "labels": [], "entities": []}, {"text": "We took sounds belonging to the categories of 'animals' and 'tools'.", "labels": [], "entities": []}, {"text": "We built our auditory dataset starting from the visual one described above and obtained the strongly aligned auditory version.", "labels": [], "entities": []}, {"text": "Hence, as in the case of the visual datapoint, an auditory datapoint can contain different instances of the same type of animal/artifact.", "labels": [], "entities": []}, {"text": "The auditory dataset consists of 17K scenes again balanced per proportion (1K scenes/proportion), with the same split as the visual one and each 'scene' containing min 3 max 20 entities out of 110 entities.", "labels": [], "entities": []}, {"text": "Linguistic Dataset For each aligned visual and auditory input pair, we built a linguistic caption describing the exact quantities of the entities present in it (for instance, for the image in (left), we obtain 'There are one butterfly, two automobiles and two mammals').", "labels": [], "entities": []}, {"text": "The procedure, illustrated in, is as following: (a) We manually annotated each of the 110 entities used to For the auditory dataset, we built the representation of each entity and the scenes containing them as following.", "labels": [], "entities": []}, {"text": "We started from the audio features computed with the VGG-inspired auditory model described in which has been trained on a preliminary version of YouTube-8M.", "labels": [], "entities": []}, {"text": "For each second of a sound clip, the model produces a 128-d vector; hence each 10-sec sound clip of the Audioset dataset (Gemmeke et al., 2017) would be represented by a 1280-d Note that in the case of animals, this hierarchy is much more easier to build (e.g. Linnaean taxonomy) while for the artifacts the 3 nouns are generally more often synonyms and often do not represent areal hierarchy/taxonomy.", "labels": [], "entities": [{"text": "Audioset dataset", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9705649316310883}]}, {"text": "As for the linguistic scenes, for each caption we extracted the features through the Universal Sentence Encoder (USE) () producing 512 dimensional vectors for each sentence.", "labels": [], "entities": []}, {"text": "Alternatively, we could have used LSTM modules to process from scratch both the linguistic and acoustic inputs exploiting their sequential nature.", "labels": [], "entities": []}, {"text": "We rejected this alternative mainly to avoid that, during the training process, the neural network learns task-dependent representations and arbitrary associations.", "labels": [], "entities": []}, {"text": "It has been shown (e.g., in) that USE provides sentence-level embeddings with strong transfer performance on several NLP tasks.", "labels": [], "entities": [{"text": "USE", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.6379224061965942}]}, {"text": "We consider this point as a strong motivation for our choice: in this way, we get more consistent representations across different modalities and the overall architecture turns out to be easier, more scalable and less prone to learn taskspecific representations.", "labels": [], "entities": []}, {"text": "The semantic spaces containing the entity representations of the three modalities are rather different.", "labels": [], "entities": []}, {"text": "It is interesting to note that the auditory dataset is much more dense than either the visual or the linguistic one: The average cosine similarity between entity pairs is 0.73 for sound vs. 0.44 for vision and 0.43 for language.", "labels": [], "entities": []}, {"text": "In other words, entities are visually and linguistically much more distinct than auditorily.", "labels": [], "entities": []}, {"text": "This could be possibly due to the fact that, as highlighted by, sound undergoes less transformations than vision, which is affected by, for instance, lighting, scene composition, and viewing angle.", "labels": [], "entities": []}, {"text": "In other words, sound could be denser than vision since it 'abstracts' from all the possible visual transformations that we encounter in the other modality.", "labels": [], "entities": []}, {"text": "It follows that integrating these modalities requires some degree of generalization over a variety of transformations, which is intuitively not trivial.", "labels": [], "entities": []}, {"text": "Evaluation All models are evaluated by computing the Pearson product-moment correlation coefficient between the Softmax probabilities and the 9-d vectors from, which encode the probability of each quantifier to be used with respect to a given proportion based on human choices.", "labels": [], "entities": [{"text": "Pearson product-moment correlation coefficient", "start_pos": 53, "end_pos": 99, "type": "METRIC", "confidence": 0.9162462502717972}, {"text": "Softmax probabilities", "start_pos": 112, "end_pos": 133, "type": "DATASET", "confidence": 0.9078203439712524}]}, {"text": "Unimodal vs. multimodal models Testing the models on the unimodal and multimodal data might lead to results that are influenced by the different sizes of data seen during training.", "labels": [], "entities": []}, {"text": "To rule out this possibility, we use unimodal and multimodal datasets of equal size.", "labels": [], "entities": []}, {"text": "We take 11,900 datapoints for each single modality; and in the multimodal model, we use 5950 instances for each modality which sum up to 11,900 datapoints.", "labels": [], "entities": []}, {"text": "Incongruent visual-auditory inputs In order to test the effectiveness of the integration of the two modalities, we take the H&S trained on aligned (congruent) visual-auditory data and we test it with incongruent data, viz.", "labels": [], "entities": []}, {"text": "inputs that do not have the same proportion of animals.", "labels": [], "entities": []}, {"text": "Given a visual input containing, e.g., 3 animals and 2 artifacts (as in left), we pair it with an auditory input having 3 artifacts and 2 animals.", "labels": [], "entities": []}, {"text": "This way, the corresponding probability distributions are different, hence we refer to these pairs as incongruent auditory input.", "labels": [], "entities": []}, {"text": "Similarly, we generate incongruent vi- sual inputs by pairing an auditory input with, e.g, a 3:2 proportion with a visual input with a proportion of 2:3, and consider as the correct probability distribution the one corresponding to the 3:2 proportion encoded by the auditory input.", "labels": [], "entities": []}, {"text": "To ensure that the difference between the two modalities is high, we avoid pairing proportions with extremely similar probability distributions.", "labels": [], "entities": []}, {"text": "Rather, we focus on a subset of proportion pairs, namely 0-100%, 10-90%, and 17-83%.", "labels": [], "entities": []}, {"text": "If the hub exploits the alignment between the modalities, we expect the model to perform poorly in this setting (lower is better).", "labels": [], "entities": []}, {"text": "Unseen combinations We evaluate the generalization power of the models by testing them on unseen data.", "labels": [], "entities": []}, {"text": "We want to study how well the model generalizes from (a) small cardinalities to larger ones and (b) vice versa.", "labels": [], "entities": []}, {"text": "To this end, we divide the training and test sets as following: For each of the 17 proportions, we use as the test set the scenes containing (a) the largest possible number of objects (e.g., for proportion 0%, we test on 0:20 and train on all the other combinations); (b) the smallest possible number of objects (e.g., for proportion 0%, we test on 0:3 and train on all the other combinations).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's r correlation results -human judg- ments used as target results. Unimodal vs. multimodal  model trained and tested on datasets of equal size.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7027994617819786}]}, {"text": " Table 2: Unimodal vs. multimodal models tested  on unseen combinations which have smaller or larger  number of entities than the seen data.", "labels": [], "entities": []}]}