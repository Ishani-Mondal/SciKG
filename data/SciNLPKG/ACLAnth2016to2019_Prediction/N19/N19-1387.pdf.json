{"title": [{"text": "Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages", "labels": [], "entities": [{"text": "Addressing Word-order Divergence", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7121482392152151}, {"text": "Multilingual Neural Machine Translation", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.6223666742444038}]}], "abstractContent": [{"text": "Transfer learning approaches for Neural Machine Translation (NMT) trains a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.8134560883045197}]}, {"text": "In many cases, the assisting language has a different word order from the source language.", "labels": [], "entities": []}, {"text": "We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.8841721713542938}]}, {"text": "To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model.", "labels": [], "entities": []}, {"text": "Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transfer learning for multilingual Neural Machine Translation (NMT) ( attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task).", "labels": [], "entities": [{"text": "multilingual Neural Machine Translation (NMT)", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.813742356640952}]}, {"text": "Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model.", "labels": [], "entities": []}, {"text": "If source-target language pair parallel corpus is available, the child model can further be fine-tuned.", "labels": [], "entities": []}, {"text": "The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowledge from the parent task, thereby improving the performance on the child task.", "labels": [], "entities": [{"text": "weight initialization", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.615718349814415}]}, {"text": "However, the divergence between the source and the assisting language can adversely impact the benefits obtained from transfer learning.", "labels": [], "entities": []}, {"text": "Multiple studies have shown that transfer learning works best when the languages are related (.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9293647408485413}]}, {"text": "studied the influence of language divergence between languages chosen for training the parent and the child model, and showed that choosing similar languages for training the parent and the child model leads to better improvements from transfer learning.", "labels": [], "entities": []}, {"text": "Several studies have tried to address the lexical divergence between the source and the target languages either by using Byte Pair Encoding (BPE) as basic input representation units or character-level NMT system () or bilingual embeddings (.", "labels": [], "entities": []}, {"text": "However, the effect of word order divergence and its mitigation has not been explored.", "labels": [], "entities": [{"text": "word order divergence", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.791460116704305}]}, {"text": "Ina practical setting, it is not uncommon to have source and assisting languages with different word order.", "labels": [], "entities": []}, {"text": "For instance, it is possible to find parallel corpora between English (SVO word order) and some Indian (SOV word order) languages, but very little parallel corpora between Indian languages.", "labels": [], "entities": []}, {"text": "Hence, it is natural to use English as an assisting language for inter-Indian language translation.", "labels": [], "entities": [{"text": "inter-Indian language translation", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.7418212294578552}]}, {"text": "To address the word order divergence, we propose to pre-order the assisting language sentences (SVO) to match the word order of the source language (SOV).", "labels": [], "entities": [{"text": "word order divergence", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.6382260223229727}]}, {"text": "We consider an extremely resourceconstrained scenario, where there is no parallel corpus for the child task.", "labels": [], "entities": []}, {"text": "From our experiments, we show that there is a significant increase in the translation accuracy for the unseen source-target language pair.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9171975255012512}]}], "datasetContent": [{"text": "In this section, we describe the languages experimented with, datasets used, the network hyperparameters used in our experiments.", "labels": [], "entities": []}, {"text": "Languages: We experimented with English \u2192 Hindi translation as the parent task.", "labels": [], "entities": [{"text": "English \u2192 Hindi translation", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6820582747459412}]}, {"text": "English is the assisting source language.", "labels": [], "entities": []}, {"text": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks.", "labels": [], "entities": []}, {"text": "Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages.", "labels": [], "entities": []}, {"text": "All these languages have a canonical SOV word order.", "labels": [], "entities": [{"text": "SOV word order", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8395376404126486}]}, {"text": "We use the 520-sentence dev-set of the IITB parallel corpus for validation.", "labels": [], "entities": [{"text": "IITB parallel corpus", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.8603093226750692}]}, {"text": "For each child task, we use 2K sentences from ILCI corpus as test set.", "labels": [], "entities": [{"text": "ILCI corpus", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.8734394609928131}]}, {"text": "Network: We use OpenNMT-Torch ( to train the NMT system.", "labels": [], "entities": []}, {"text": "We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (.", "labels": [], "entities": []}, {"text": "The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each.", "labels": [], "entities": []}, {"text": "We use a mini-batch of size 50 and a dropout layer.", "labels": [], "entities": []}, {"text": "We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001.", "labels": [], "entities": []}, {"text": "The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively.", "labels": [], "entities": []}, {"text": "For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations).", "labels": [], "entities": []}, {"text": "In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings (   fluence of word-order divergence on Multilingual NMT.", "labels": [], "entities": []}, {"text": "We do not want bilingual embeddings quality or bilingual dictionary coverage to influence the experiments, rendering our conclusions unreliable.", "labels": [], "entities": []}, {"text": "Hence, we use the above mentioned largecoverage bilingual dictionary.", "labels": [], "entities": [{"text": "largecoverage bilingual dictionary", "start_pos": 34, "end_pos": 68, "type": "DATASET", "confidence": 0.7549712657928467}]}, {"text": "Pre-ordering: We use CFILT-preorder 3 for prereordering English sentences.", "labels": [], "entities": [{"text": "CFILT-preorder", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.6116077899932861}]}, {"text": "It contains two preordering configurations: (1) generic rules (G) that apply to all Indian languages (, and (2) hindi-tuned rules (HT) which improves generic rules by incorporating improvements found through error analysis of EnglishHindi reordering ().", "labels": [], "entities": []}, {"text": "The Hindituned rules improve translation for other English to Indian language pairs too ( ).", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9635089039802551}]}], "tableCaptions": [{"text": " Table 3: Number of UNK tokens generated by each  model on the test set.", "labels": [], "entities": []}, {"text": " Table 2. We report BLEU scores and LeBLEU 4", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9695017039775848}, {"text": "LeBLEU", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9947788715362549}]}, {"text": " Table 5: Transfer learning results (BLEU) for In- dian Language-Hindi pair, fine-tuned with varying  number of Indian Language-Hindi parallel sentences.   \u2020Indicates statistically significant difference between  Pre-ordered and No Pre-ordered results using paired  bootstrap resampling (Koehn, 2004) for a p-value less  than 0.05. No Transfer Learning model refers to  training the model on varying number of Indian Lan- guage-Hindi parallel sentences with randomly initial- ized weights.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9679595828056335}]}]}