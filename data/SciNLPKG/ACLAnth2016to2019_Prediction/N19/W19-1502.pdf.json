{"title": [{"text": "Tracking Discrete and Continuous Entity State for Process Understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "Procedural text, which describes entities and their interactions as they undergo some process , depicts entities in a uniquely nuanced way.", "labels": [], "entities": []}, {"text": "First, each entity may have some observable discrete attributes, such as its state or location; modeling these involves imposing global structure and enforcing consistency.", "labels": [], "entities": []}, {"text": "Second, an entity may have properties which are not made explicit but can be effectively induced and tracked by neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a structured neural architecture that reflects this dual nature of entity evolution.", "labels": [], "entities": []}, {"text": "The model tracks each entity recurrently , updating its hidden continuous representation at each step to contain relevant state information.", "labels": [], "entities": []}, {"text": "The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity.", "labels": [], "entities": []}, {"text": "This CRF can explicitly capture constraints on entity states overtime, enforcing that, for example, an entity cannot move to a location after it is destroyed.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our proposed model on QA tasks over process paragraphs in the PROPARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results.", "labels": [], "entities": [{"text": "PROPARA dataset", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9223662912845612}]}], "introductionContent": [{"text": "Many reading comprehension question answering tasks () require looking at primarily one point in the passage to answer each question, or sometimes two or three (.", "labels": [], "entities": [{"text": "reading comprehension question answering", "start_pos": 5, "end_pos": 45, "type": "TASK", "confidence": 0.6958863437175751}]}, {"text": "As a result, modeling surfacelevel correspondences can work well ( and holistic passage comprehension is not necessary.", "labels": [], "entities": []}, {"text": "However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states overtime, combination in recipes ( , and participation in scientific processes ( . These settings then suggest more highly structured models as away of dealing with the more highly structured tasks.", "labels": [], "entities": []}, {"text": "One crucial aspect of such texts is the way an entity's state evolves with both discrete (observable state and location changes) and continuous (changes in unobserved hidden attributes) phenomena going on.", "labels": [], "entities": []}, {"text": "Additionally, the discrete changes unfold in away that maintains the state consistency: an entity cannot be destroyed before it even starts to exist.", "labels": [], "entities": []}, {"text": "In this work, we present a model which both recurrently tracks the entity in a continuous space while imposing discrete constraints using a conditional random field (CRF).", "labels": [], "entities": []}, {"text": "We focus on the scientific process understanding setting introduced in . For each entity, we instantiate a sentence-level LSTM to distill continuous state information from each of that entity's mentions.", "labels": [], "entities": [{"text": "scientific process understanding", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6693029999732971}]}, {"text": "Separate LSTMs integrate entity-location information into this process.", "labels": [], "entities": []}, {"text": "These continuous components then produce potentials fora sequential CRF tagging layer, which predicts discrete entity states.", "labels": [], "entities": [{"text": "CRF tagging layer", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7980735500653585}]}, {"text": "The CRF's problem-specific tag scheme, along with transition constraints, ensures that the model's predictions of these observed entity properties are structurally coherent.", "labels": [], "entities": []}, {"text": "For example, in procedural texts, this involves ensuring existence before destruction and unique creation and destruction points.", "labels": [], "entities": []}, {"text": "Because we use global inference, identifying implicit event creation or destruction is made easier, since the model resolves conflicts among competing time steps and chooses the best time step for these events during sequence prediction.", "labels": [], "entities": [{"text": "identifying implicit event creation or destruction", "start_pos": 33, "end_pos": 83, "type": "TASK", "confidence": 0.8213300406932831}, {"text": "sequence prediction", "start_pos": 217, "end_pos": 236, "type": "TASK", "confidence": 0.6537017524242401}]}, {"text": "Past approaches in the literature have typically been end-to-end continuous task specific frameworks (, sometimes for tasks that are simpler and more syn- thetic, or continuous entitycentric neural language models).", "labels": [], "entities": []}, {"text": "For process understanding specifically, past work has effectively captured global information ( ) and temporal characteristics (.", "labels": [], "entities": [{"text": "process understanding", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7125269770622253}]}, {"text": "However, these models do not leverage the structure constraints of the problem, or only handle them heuristically (Tandon et al., 2018).", "labels": [], "entities": []}, {"text": "We find that our model outperforms these past approaches on the PROPARA dataset of  with a significant boost in questions concerning entity state, regardless of the location.", "labels": [], "entities": [{"text": "PROPARA dataset", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9407566785812378}]}], "datasetContent": [{"text": "We evaluate the performance of the proposed model on the two comprehension tasks of the PROPARA dataset ( . This dataset consists of 488 crowdsourced real world process paragraphs about 183 distinct topics in the science genre.", "labels": [], "entities": [{"text": "PROPARA dataset", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.922958105802536}]}, {"text": "The names of the participating entities and their existence spans are identified by expert annotators.", "labels": [], "entities": []}, {"text": "Finally, crowd workers label locations of participant entities at each time step (sentence).", "labels": [], "entities": []}, {"text": "The final data consists of 3.3k sentence with an average of 6.7 sentences and 4.17 entities per process paragraph.", "labels": [], "entities": []}, {"text": "We compare our model, the Neural CRF Entity Tracking (NCET) model, with benchmark systems from past work.", "labels": [], "entities": [{"text": "Neural CRF Entity Tracking (NCET)", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7546860149928502}]}], "tableCaptions": [{"text": " Table 2: Results on the sentence-level (Task-1) and document-level (Task-2) evaluation task of the PROPARA  dataset on the test set. Our proposed CRF-based model achieves state of the art results on both the tasks compared  to the previous work in (", "labels": [], "entities": [{"text": "PROPARA  dataset", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.8801351189613342}]}, {"text": " Table 3: Ablation studies for the proposed architecture.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9814279079437256}]}]}