{"title": [{"text": "Structural Supervision Improves Learning of Non-Local Grammatical Dependencies", "labels": [], "entities": [{"text": "Structural Supervision Improves Learning of Non-Local Grammatical Dependencies", "start_pos": 0, "end_pos": 78, "type": "TASK", "confidence": 0.9424765482544899}]}], "abstractContent": [{"text": "State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success.", "labels": [], "entities": []}, {"text": "Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement.", "labels": [], "entities": []}, {"text": "Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus two models that represent hierarchical structure and deploy it in left-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et al., 2016) and a incrementalized version of the Parsing-as-Language-Modeling configuration from Charniak et al.", "labels": [], "entities": []}, {"text": "Models are tested on a diverse range of configurations for two classes of non-local grammatical dependencies in English-Negative Polarity licensing and Filler-Gap Dependencies.", "labels": [], "entities": []}, {"text": "Using the same training data across models, we find that structurally-supervised models outperform the LSTM, with the RNNG demonstrating best results on both types of grammatical dependencies and even learning many of the Island Constraints on the filler-gap dependency.", "labels": [], "entities": []}, {"text": "Structural supervision thus provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Long Short-Term Memory Recurrent Neural Networks (LSTMs)) have achieved state of the art language modeling performance and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing.", "labels": [], "entities": [{"text": "anaphoric pronoun licensing", "start_pos": 345, "end_pos": 372, "type": "TASK", "confidence": 0.7212831576665243}]}, {"text": "LSTMs, however, require large amounts of training data and remain relatively uninterpretable.", "labels": [], "entities": []}, {"text": "One model that attempts to address both these issues is the Recurrent Neural Network Grammar ( . RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing.", "labels": [], "entities": []}, {"text": "They can achieve state-of-the-art broad-coverage scores on language modeling and phrase structure parsing tasks, learn Noun Phrase headedness ( , and outperform linear models at learning subject-verb number agreement ( . In this work, we comparatively evaluate LSTMs, RNNGs and a third model trained using syntactic supervision-similar to the Parsing-asLanguage-Modeling configuration from-by conducting side-by-side tests on two novel English grammatical dependencies, deploying methodology from psycholinguistics.", "labels": [], "entities": [{"text": "phrase structure parsing tasks", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.7382405400276184}, {"text": "Noun Phrase headedness", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.6348871390024821}]}, {"text": "In this paradigm, the language models are fed with hand-crafted sentences, designed to draw out behavior that belies whether they have learned the underlying syntactic dependency.", "labels": [], "entities": []}, {"text": "For example, and  assessed how well neural language models were able to learn subject-verb number agreement by feeding the prefix The keys to the cabinet...", "labels": [], "entities": []}, {"text": "If the model assigns a relatively higher probability to the grammatical plural verb are than the ungrammatical singular is it can be said to have learned the agreement dependency.", "labels": [], "entities": []}, {"text": "Here, we investigate two non-local dependencies that remain untested for RNNGs: Negative Polarity Item (NPI) licensing is the dependency between a negative licensor-such as not or none-and a Negative Polarity Item such as any or ever.", "labels": [], "entities": []}, {"text": "The filler-gap dependency is the dependency between a filler-such as who or what-and a gap, which is an empty syntactic position.", "labels": [], "entities": []}, {"text": "Both dependencies have been shown to be learnable by LSTMs trained on large amounts of data (.", "labels": [], "entities": []}, {"text": "Here, we investigate whether, after controlling for size of the training data, explicit hierarchical representation results in learning advantages.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Filler-Gap Dependency Statistics for the  Penn Treebank training data (used for both models).", "labels": [], "entities": [{"text": "Penn Treebank training data", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.9922610819339752}]}]}