{"title": [], "abstractContent": [{"text": "Research in the area of style transfer for text is currently bottlenecked by alack of standard evaluation practices.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.706183671951294}]}, {"text": "This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset.", "labels": [], "entities": [{"text": "Yelp sentiment dataset", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.9458443721135458}]}, {"text": "We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7494032382965088}]}, {"text": "We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover's Distance, Word Mover's Distance on style-masked texts, and adversarial classification for the respective aspects.", "labels": [], "entities": []}, {"text": "We also show that the three examined models exhibit tradeoffs between aspects of interest , demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots.", "labels": [], "entities": []}, {"text": "We release software with our evaluation metrics to facilitate research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Style transfer in text is the task of changing an attribute (style) of an input, while retaining nonattribute related content (referred to simply as content for brevity in this paper).", "labels": [], "entities": [{"text": "Style transfer", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7322876751422882}]}, {"text": "For instance, previous work has modified text to make it more positive (), romantic (, or politically slanted (.", "labels": [], "entities": []}, {"text": "Some style transfer models enable modifications by manipulating latent representations of the text, while others identify and replace stylerelated words directly ().", "labels": [], "entities": [{"text": "style transfer", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.6937967091798782}]}, {"text": "Regardless of approach, they are hard to compare as there is currently neither a standard set of evaluation practices, nor a clear definition of which exact aspects to evaluate.", "labels": [], "entities": []}, {"text": "In Section 2, we define three key aspects to consider.", "labels": [], "entities": []}, {"text": "In Section 3, we summarize issues with previously used metrics.", "labels": [], "entities": []}, {"text": "Many rely on human ratings, which can be expensive and timeconsuming to obtain.", "labels": [], "entities": []}, {"text": "To address these issues, in Section 4, we consider how to obtain more reliable measures of human judgment for aspects of interest, and automated methods more strongly correlated with human judgment than previously used methods.", "labels": [], "entities": []}, {"text": "Lastly, in Section 5, we show that the three examined models exhibit aspect tradeoffs, highlighting the importance of evaluating style transfer models at specific points of their tradeoff plots.", "labels": [], "entities": []}, {"text": "We release software with our evaluation metrics at https://github.com/passeul/ style-transfer-model-evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider three aspects of interest on which to evaluate output text x of a style transfer model, potentially with respect to input text x: 1.", "labels": [], "entities": []}, {"text": "style transfer intensity ST I(SC(x), SC(x )) quantifies the difference in style, where SC(\u00b7) maps an input to a style distribution 2.", "labels": [], "entities": [{"text": "style transfer intensity ST I", "start_pos": 0, "end_pos": 29, "type": "METRIC", "confidence": 0.5124297797679901}]}, {"text": "content preservation CP (x, x ) quantifies the similarity in content between the input and the output 3.", "labels": [], "entities": [{"text": "content preservation CP", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6670420666535696}]}, {"text": "naturalness NT (x ) quantifies the degree to which the output appears as if it could have been written by humans Style transfer models should be compared across all three aspects to properly characterize differences.", "labels": [], "entities": []}, {"text": "For instance, if a model transfers from negative to positive sentiment, but alters content such as place names, it preserves content poorly.: Summary of past evaluation techniques.", "labels": [], "entities": []}, {"text": "HRC is human rating on a continuous scale (e.g. 1 to 5).", "labels": [], "entities": [{"text": "HRC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5960699319839478}]}, {"text": "HRD is on discrete options (e.g. positive/negative).", "labels": [], "entities": [{"text": "HRD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8584076762199402}]}, {"text": "HRR is human ranking (most to least similar) of outputs, with respect to given input x.", "labels": [], "entities": []}, {"text": "{x } is the set of x from models trained on different parameters.", "labels": [], "entities": []}, {"text": "SC is a style classifier.", "labels": [], "entities": []}, {"text": "Superscripts denote that evaluation is done for fluency (F) or grammar (G), which we consider subsets of naturalness.", "labels": [], "entities": []}, {"text": "Readers can seethe original papers for details on methods falling under these techniques.", "labels": [], "entities": []}, {"text": "If it preserves content well, but sequentially repeats words such as \"the\", the output is unnatural.", "labels": [], "entities": []}, {"text": "Conversely, a model that overemphasizes text reconstruction would yield high content preservation and possibly high naturalness, but little to no style transfer.", "labels": [], "entities": [{"text": "text reconstruction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7976326942443848}, {"text": "style transfer", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.6688544452190399}]}, {"text": "All three aspects are thus critical to analyze in a system of style transfer evaluation.", "labels": [], "entities": [{"text": "style transfer evaluation", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.8418086568514506}]}, {"text": "For style transfer intensity, the relative scoring task (rating the degree of stylistic difference between x and x ) did not have greater rater reliability than the previously used task of rating output texts on an absolute scale.", "labels": [], "entities": [{"text": "style transfer intensity", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8475128213564554}, {"text": "rater reliability", "start_pos": 138, "end_pos": 155, "type": "METRIC", "confidence": 0.7389153838157654}]}, {"text": "This is likely due to task complexity or rater uncertainty, which motivates the need for further exploration of task design for this particular aspect of interest.", "labels": [], "entities": []}, {"text": "For content preservation, our form of human evaluation operates on texts whose style words are masked out, unlike the previous approach (no masking).", "labels": [], "entities": [{"text": "content preservation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7857573628425598}]}, {"text": "Our approach addresses the unintentional variable of rater-dependent style identification that could lead to noisy, less reliable ratings.", "labels": [], "entities": [{"text": "rater-dependent style identification", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.5622439881165823}]}, {"text": "Identification and masking of words was made possible with a style lexicon.", "labels": [], "entities": [{"text": "Identification and masking of words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7640851676464081}]}, {"text": "We automatically constructed the lexicon in away that can be done for any style dataset, as long as style labels are available (Section 4.1).", "labels": [], "entities": []}, {"text": "We acknowledge a tradeoff between filling the lexicon with more style words and being conservative in order to avoid capturing content words.", "labels": [], "entities": []}, {"text": "We justify taking a more conservative approach as content words are naturally critical to evaluations of content preservation.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.6992501616477966}]}, {"text": "For naturalness, we introduced a paradigm of relative scoring that uses both the output and input texts.", "labels": [], "entities": [{"text": "relative scoring", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7742887139320374}]}, {"text": "This achieved a higher inter-rater reliability than did absolute scoring, the previous approach.", "labels": [], "entities": [{"text": "reliability", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8215997815132141}]}, {"text": "For style transfer intensity, we proposed using a metric with EMD as the basis to acknowledge the spectrum of styles that can appear in outputs and to handle both binary and non-binary datasets.", "labels": [], "entities": [{"text": "style transfer intensity", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7353395720322927}, {"text": "EMD", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.983620285987854}]}, {"text": "The metric also accounts for direction by penalizing scores in the cases where the style distribution of the output text explicitly moves away from the target style.", "labels": [], "entities": []}, {"text": "Previous work used external classifiers, whose style distributions for x and x can be used to calculate direction-corrected EMD, making it a simple addition to the evaluation workflow.", "labels": [], "entities": []}, {"text": "For content preservation, WMD (based on EMD) works in a similar fashion, but with word embeddings of x and of x . BLEU, used widely in previous work, may yield weaker correlations with human judgment in comparison as it was designed to have multiple reference texts per candidate text ().", "labels": [], "entities": [{"text": "content preservation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7606114745140076}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9974459409713745}]}, {"text": "Several reference texts, which are more common in machine translation tasks, increase the chance of n-gram (such as n \u2265 3) overlap with the candidate.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.8053433299064636}]}, {"text": "In the style transfer setting, however, the only reference text for x is x.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.7098708301782608}]}, {"text": "Having a single reference text reduces the likelihood of overlap and the overall effectiveness of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9764000177383423}]}, {"text": "For naturalness, strong agreement of adversarial classifiers with relative scores assigned by humans suggest that classifiers are suitable for automated evaluation.", "labels": [], "entities": []}, {"text": "One might assume input texts would almost always be rated as more natural by both humans and classifiers, biasing the agreement.", "labels": [], "entities": []}, {"text": "This is not the case, as we justify our rating scheme with evidence of outputs being rated as more natural across several models).", "labels": [], "entities": []}, {"text": "Output texts classified as more natural indicate some success fora style transfer model, as it can produce texts with a quality like that of human-generated inputs, which are, by definition, natural.", "labels": [], "entities": []}, {"text": "Finally, with aspect tradeoff plots constructed using scores from the automated metrics, we can directly compare models with respect to multiple aspects simultaneously.", "labels": [], "entities": []}, {"text": "Points of intersection, or near intersection, for different models signify that they, at the hyperparameters that yielded those points, can achieve similar results for various aspects.", "labels": [], "entities": []}, {"text": "These parameters can be useful for understanding the impact of decisions made during model design and optimization phases.", "labels": [], "entities": []}, {"text": "Due to high costs of human evaluation, we focus on CAAE, ARAE, and DAR models with transfer tasks based on samples from the Yelp binary sentiment dataset).", "labels": [], "entities": [{"text": "ARAE", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8944036364555359}, {"text": "Yelp binary sentiment dataset", "start_pos": 124, "end_pos": 153, "type": "DATASET", "confidence": 0.8980070948600769}]}, {"text": "Italicized words are style-related, according to a style lexicon.", "labels": [], "entities": []}, {"text": "They can be masked or removed in evaluations of content preservation (Section 4.3).", "labels": [], "entities": [{"text": "content preservation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7050162255764008}]}, {"text": "the range of parameters each model is trained on in order to compare evaluation practices and generate aspect tradeoff plots.", "labels": [], "entities": []}, {"text": "Each of three Amazon Turk raters evaluated 244 texts per aspect, per model.", "labels": [], "entities": [{"text": "Amazon Turk raters", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.922838826974233}]}, {"text": "Of those texts, half are originally of positive sentiment transferred to negative, and vice versa.", "labels": [], "entities": []}, {"text": "For brevity, we reference average scores (correlation, kappa, and agreement, each of which is described below) from across all models in our analysis of results.", "labels": [], "entities": [{"text": "correlation", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9939498901367188}, {"text": "agreement", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9251063466072083}]}, {"text": "For detailed scores per model, please refer to the corresponding tables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Sample outputs of a negative to positive sentiment style transfer task. Italicized words are style-related,  according to a style lexicon. They can be masked or removed in evaluations of content preservation (Section 4.3).", "labels": [], "entities": [{"text": "content preservation", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.7232661843299866}]}, {"text": " Table 4.  CAAE uses autoencoders (", "labels": [], "entities": []}, {"text": " Table 5: Fleiss' kappas for human judgments of content  preservation of unmasked and style-masked texts.", "labels": [], "entities": []}, {"text": " Table 6: Fleiss' kappas for human judgments of abso- lute naturalness and relative naturalness of texts.", "labels": [], "entities": []}, {"text": " Table 7: Correlations of automated style transfer intensity metrics with human scores.", "labels": [], "entities": []}, {"text": " Table 9: Absolute correlations of content preservation metrics with human scores on texts with style masking.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9569855332374573}, {"text": "content preservation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6626804769039154}, {"text": "style masking", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.7120485752820969}]}, {"text": " Table 10: Percent agreement between adversarial clas- sifiers and human scores on the naturalness of texts.", "labels": [], "entities": []}]}