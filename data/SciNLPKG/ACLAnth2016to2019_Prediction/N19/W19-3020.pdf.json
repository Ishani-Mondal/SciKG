{"title": [{"text": "CLPsych2019 Shared Task: Predicting Users' Suicide Risk Levels from Their Reddit Posts on Multiple Forums", "labels": [], "entities": [{"text": "Predicting Users' Suicide Risk Levels from Their Reddit Posts on Multiple Forums", "start_pos": 25, "end_pos": 105, "type": "TASK", "confidence": 0.7868168900410334}]}], "abstractContent": [{"text": "We aimed to predict an individual suicide risk level from longitudinal posts on Reddit discussion forums.", "labels": [], "entities": []}, {"text": "Through participating in a shared task competition hosted by CLPsych2019, we received two annotated datasets: a training dataset with 496 users (31,553 posts) and a test dataset with 125 users (9610 posts).", "labels": [], "entities": [{"text": "CLPsych2019", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9560953974723816}]}, {"text": "We submitted results from our three best-performing machine-learning models: SVM, Na\u00efve Bayes, and an ensemble model.", "labels": [], "entities": []}, {"text": "Each model provided a user's suicide risk level in four categories, i.e., no risk, low risk, moderate risk, and severe risk.", "labels": [], "entities": []}, {"text": "Among the three models, the ensemble model had the best macro-averaged F1 score 0.379 when tested on the holdout test dataset.", "labels": [], "entities": [{"text": "F1 score 0.379", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.9754898349444071}]}, {"text": "The NB model had the best performance in two additional binary-classification tasks, i.e., no risk vs. flagged risk (any risk level other than no risk) with F1 score 0.836 and no or low risk vs. urgent risk (moderate or severe risk) with F1 score 0.736.", "labels": [], "entities": [{"text": "NB", "start_pos": 4, "end_pos": 6, "type": "DATASET", "confidence": 0.861278235912323}, {"text": "F1 score 0.836", "start_pos": 157, "end_pos": 171, "type": "METRIC", "confidence": 0.9775744080543518}, {"text": "F1 score 0.736", "start_pos": 238, "end_pos": 252, "type": "METRIC", "confidence": 0.9699253439903259}]}, {"text": "We conclude that the NB model may serve as a tool for identifying users with flagged or urgent suicide risk based on longitudinal posts on Reddit discussion forums.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We received two datasets from the CLPsych2019(Zirikly et al., 2019): 1) a training dataset and 2) a test dataset.", "labels": [], "entities": [{"text": "CLPsych2019(Zirikly et al., 2019)", "start_pos": 34, "end_pos": 67, "type": "DATASET", "confidence": 0.910474568605423}]}, {"text": "Both datasets comprised annotated posts on the Reddit discussion form and its sub-discussion forms, also known as subreddits.", "labels": [], "entities": [{"text": "Reddit discussion form", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.8746046423912048}]}, {"text": "The training dataset study period is between posts from 496 Reddit users with the cohort definition: a user had at least one post on the SuicideWatch subreddit; users who posted on the SuicideWatch may not be of risk to suicide.", "labels": [], "entities": [{"text": "SuicideWatch subreddit", "start_pos": 137, "end_pos": 159, "type": "DATASET", "confidence": 0.9580798447132111}]}, {"text": "The data elements in the training dataset included a user id, a subreddit name, a post title and body from the user's posts in any subreddit, and post timestamp in a unified timezone.", "labels": [], "entities": []}, {"text": "The CLPsych2019 organization provided the gold standard for the training dataset.() Following the same cohort definition, the test dataset comprised 9,610 posts from 125 Reddit users.", "labels": [], "entities": [{"text": "CLPsych2019 organization", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9544880092144012}]}, {"text": "We received the training and test datasets one month and five days before the competition deadline, respectively.", "labels": [], "entities": []}, {"text": "The study is approved under the Children's Hospital of Philadelphia IRB.", "labels": [], "entities": [{"text": "Children's Hospital of Philadelphia IRB", "start_pos": 32, "end_pos": 71, "type": "DATASET", "confidence": 0.6093541781107584}]}, {"text": "We developed seven machine learning models: Na\u00efve Bayes (NB), gradient boosting (GB), random forest (RF), support vector machine (SVM), and deep neural networks including augmented convolution neural networks (CNN) and long short-term memory neural networks (LSTM).", "labels": [], "entities": []}, {"text": "Unlike conventional deep neural networks, we developed augmented deep neural networks included input not only from freetext posts (Doc2Vec) but also the user-level aggregate statistics defined in Section II.2.", "labels": [], "entities": []}, {"text": "For the competition, each team was limited to submit up to three models' results, we chose top two models and added an ensemble model based on our three best-performing models.", "labels": [], "entities": []}, {"text": "We used the 5-fold average of macroaveraged F1 scores to evaluate each model.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9754105806350708}]}, {"text": "The models used to submit results to the competition were re-trained with the full training dataset following the same approach used during cross-validation.", "labels": [], "entities": []}, {"text": "We created an ensemble classifier from our best 3 performing models.", "labels": [], "entities": []}, {"text": "Predictions from these models were 165 used to tally votes and generate the final predictions of the ensemble classifier.", "labels": [], "entities": []}, {"text": "Since there were more risk categories (4) than the number of classifiers in the ensemble, it is possible that all models produce different predictions.", "labels": [], "entities": []}, {"text": "In this scenario, we created a rule by favoring the classes that were likely to be misclassified.", "labels": [], "entities": []}, {"text": "Besides macro-averaged F1, our evaluation metrics include macro-averaged accuracy, precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.8926832675933838}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9850790500640869}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.999647855758667}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9989344477653503}]}, {"text": "We further compared the performance based on binary classifications, i.e., flagged risk (low, moderate, and severe risks) vs. no risk, and urgent risk (moderate, and severe risks) vs. others.", "labels": [], "entities": []}, {"text": "Our macro-averaged F1 scores from the training dataset ranged from 0.147 to 0.43.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9843198657035828}]}, {"text": "summarizes the performance of the 6 models.", "labels": [], "entities": []}, {"text": "SVM and NB had best F1 scores.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9684832692146301}, {"text": "NB", "start_pos": 8, "end_pos": 10, "type": "DATASET", "confidence": 0.8122910261154175}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9992892742156982}]}, {"text": "Based on these results, we applied three models to the test dataset: SVM, NB, and an ensemble model built from the top three models: NB, SVM, and GB.", "labels": [], "entities": []}, {"text": "The rule for breaking the tie in the ensemble model was to set the order of the preference: B (highest), C, A, then D (lowest).", "labels": [], "entities": []}, {"text": "show the confusion matrices of the 3 models, and summarizes the performance of the three models submitted to the competition.", "labels": [], "entities": []}, {"text": "These models' macro-averaged F1 scores on the holdout test dataset ranged from 0.338 to 0.379.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9839714765548706}, {"text": "holdout test dataset", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.7158288757006327}]}, {"text": "The ensemle model had the best macro-F1 score 0.379, which was ranked 3 rd among the particpating teams for this shared task competition.(Zirikly et al., 2019)  The NB model had the best performance in two additional binary-classification tasks, i.e., no risk vs. flagged risk (any risk level other than no risk) with F1 score 0.836 and no or low risk vs. urgent risk (moderate or severe risk) with F1 score 0.736.", "labels": [], "entities": [{"text": "F1 score 0.836", "start_pos": 318, "end_pos": 332, "type": "METRIC", "confidence": 0.9743901888529459}, {"text": "F1 score 0.736", "start_pos": 399, "end_pos": 413, "type": "METRIC", "confidence": 0.9704744021097819}]}], "tableCaptions": [{"text": " Table 3. Risk level distributions in two datasets.", "labels": [], "entities": []}, {"text": " Table 4. Average 5-fold predictive model performance  from the training dataset, measured by the macro-av- eraged F1 score followed by the number of variables  (features) used by a model in parentheses.", "labels": [], "entities": [{"text": "macro-av- eraged F1 score", "start_pos": 98, "end_pos": 123, "type": "METRIC", "confidence": 0.6542686641216278}]}, {"text": " Table 5. Model performance from the test dataset.  Level A-D represent no risk, low risk, moderate risk,  and severe risk, respectively.", "labels": [], "entities": []}, {"text": " Table 6. Top 10 features from the feature space", "labels": [], "entities": []}]}