{"title": [{"text": "compare-mt: A Tool for Holistic Comparison of Language Generation Systems", "labels": [], "entities": [{"text": "Holistic Comparison of Language Generation", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.6219472825527191}]}], "abstractContent": [{"text": "In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7765511473019918}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6936361640691757}]}, {"text": "The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement.", "labels": [], "entities": []}, {"text": "It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9982411861419678}]}, {"text": "It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis.", "labels": [], "entities": []}, {"text": "compare-mt is a pure-Python open source package, 1 that has already proven useful to generate analyses that have been used in our published papers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT;), language generation from structured data), summarization, dialog response generation), image captioning (.", "labels": [], "entities": [{"text": "machine translation (MT", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7459673061966896}, {"text": "language generation from structured data", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.8120122730731965}, {"text": "summarization", "start_pos": 154, "end_pos": 167, "type": "TASK", "confidence": 0.9846071600914001}, {"text": "dialog response generation", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.685518721739451}, {"text": "image captioning", "start_pos": 198, "end_pos": 214, "type": "TASK", "confidence": 0.7611694931983948}]}, {"text": "Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are corrector not.", "labels": [], "entities": [{"text": "text classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.6572676599025726}]}, {"text": "Evaluation measures such as BLEU (), ROUGE), METEOR, and many others attempt to give an Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9988229870796204}, {"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.986902117729187}, {"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9783713817596436}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Aggregate score analysis with scores, confi- dence intervals, and pairwise significance tests.", "labels": [], "entities": []}, {"text": " Table 2: Examples discovered by n-gram analysis", "labels": [], "entities": []}]}