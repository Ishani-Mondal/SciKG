{"title": [{"text": "Exploring Fine-Tuned Embeddings that Model Intensifiers for Emotion Analysis", "labels": [], "entities": [{"text": "Emotion Analysis", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.9123318493366241}]}], "abstractContent": [{"text": "Adjective phrases like \"a little bit surprised\", \"completely shocked\", or \"not stunned at all\" are not handled properly by currently published state-of-the-art emotion classification and intensity prediction systems which use predominantly non-contextualized word embed-dings as input.", "labels": [], "entities": [{"text": "emotion classification and intensity prediction", "start_pos": 160, "end_pos": 207, "type": "TASK", "confidence": 0.7706199645996094}]}, {"text": "Based on this finding, we analyze differences between embeddings used by these systems in regard to their capability of handling such cases.", "labels": [], "entities": []}, {"text": "Furthermore, we argue that intensifiers in context of emotion words need special treatment, as is established for sentiment polarity classification, but not for more fine-grained emotion prediction.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7969168225924174}, {"text": "fine-grained emotion prediction", "start_pos": 166, "end_pos": 197, "type": "TASK", "confidence": 0.7543521722157797}]}, {"text": "To resolve this issue, we analyze different aspects of a post-processing pipeline which enriches the word representations of such phrases.", "labels": [], "entities": []}, {"text": "This includes expansion of semantic spaces at the phrase level and sub-word level followed by retrofitting to emotion lexica.", "labels": [], "entities": []}, {"text": "We evaluate the impact of these steps with`Awith` with`A La Carte and Bag-of-Substrings extensions based on pretrained GloVe, Word2vec, and fastText embeddings against a crowd-sourced corpus of intensity annotations for tweets containing our focus phrases.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.9400947093963623}]}, {"text": "We show that the fastText-based models do not gain from handling these specific phrases under inspection.", "labels": [], "entities": []}, {"text": "For Word2vec em-beddings, we show that our post-processing pipeline improves the results by up to 8% on a novel dataset densely populated with intensi-fiers.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9138873815536499}]}], "introductionContent": [{"text": "Emotion detection in text includes tasks of mapping words, sentences, and documents to a discrete set of emotions following a psychological model such as those proposed by and, or to intensity scores or continuous values of valence-arousal-dominance ().", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9204818308353424}]}, {"text": "The shared task on intensity prediction for discrete classes proposed to combine both.", "labels": [], "entities": [{"text": "intensity prediction", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7767141759395599}]}, {"text": "In this task a tweet and an emotion are given and the goal is to determine an intensity score between 0 and 1.", "labels": [], "entities": [{"text": "intensity score", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.961819589138031}]}, {"text": "Especially, but not only in social media, users use degree adverbs (also called intensifiers, for instance in \"I am kinda happy\" vs. \"I am very happy.\" to express different levels of emotion intensity.", "labels": [], "entities": []}, {"text": "This is a relevant task: 10% of tweets containing an emotion word are modified with such an adverb in the corpus we describe in Section 3.1.", "labels": [], "entities": []}, {"text": "In this paper, we challenge the assumption that models developed for intensity prediction perform well on tweets containing such phrases and analyze which of the established embedding methods Word2vec (), GloVe (, and fastText embeddings () performs well when predicting intensities for tweets containing such phrases.", "labels": [], "entities": [{"text": "intensity prediction", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.6443059146404266}, {"text": "Word2vec", "start_pos": 192, "end_pos": 200, "type": "DATASET", "confidence": 0.9608360528945923}]}, {"text": "We will see that the performance of the popular and fast-to-train Word2vec method can be increased with a simple postprocessing pipeline which we present in this paper.", "labels": [], "entities": []}, {"text": "As a motivating example, the DeepMoji model () predicts anger for both the example sentences \"I am not angry.\" and \"I am angry.\"", "labels": [], "entities": []}, {"text": "Using the model by (one of the state-of-the-art intensity prediction models from, building their model on top of Word2vec embeddings) we also obtain anger as having the highest intensity for both examples.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9383389949798584}]}, {"text": "We argue that the models should be more sensitive to the difference between negations, downtoners and amplifiers.", "labels": [], "entities": []}, {"text": "With this paper, we contribute to alleviate this situation in three aspects.", "labels": [], "entities": []}, {"text": "Firstly, we provide an analysis of the distribution of degree adverbs (in-cluding negations) with emotion words and show that not all such combinations are equally common.", "labels": [], "entities": []}, {"text": "Secondly, we perform a crowdsourcing experiment in which we collect scores for different combinations of degree adverbs and emotion adjectives.", "labels": [], "entities": []}, {"text": "We use these data, which we make publicly available, as an additional challenging test set for the task of intensity prediction for English.", "labels": [], "entities": [{"text": "intensity prediction", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.6385720074176788}]}, {"text": "Thirdly, we use a state-of-the-art intensity prediction model ( on this test set and evaluate two methods to improve these predictions, namely the inclusion ( and n-gram embeddings vi\u00e0 A La Carte of additional subword information with Bag-of-Substrings ().", "labels": [], "entities": []}, {"text": "We evaluate based on Word2vec, GloVe and fastText embeddings and show that particularly the first two benefit from these changes, but to different extents.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9779205322265625}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example queries with their BWS crowd- sourced scores for the modifiers \"so\", \"kinda\" and the  negation \"not\". For every focus phrase we have an  intensity score between \ud97b\udf591 and +1 for each emotion.  The focus phrases are shown in groups made around  the emotion adjectives.", "labels": [], "entities": [{"text": "BWS", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7219088673591614}]}, {"text": " Table 2: Split-half reliabilities and Spearman's rank  correlation between these settings.", "labels": [], "entities": [{"text": "Spearman's rank  correlation", "start_pos": 39, "end_pos": 67, "type": "METRIC", "confidence": 0.7415777519345284}]}, {"text": " Table 3: Evaluation: Spearman's rank correlation be- tween predicted emotion intensity scores and annotated  scores on our dataset (T) or the EmoInt dataset (EI). We  report results only for the 4 emotions annotated in the  EmoInt data.", "labels": [], "entities": [{"text": "EmoInt dataset (EI)", "start_pos": 143, "end_pos": 162, "type": "DATASET", "confidence": 0.9063328027725219}, {"text": "EmoInt data", "start_pos": 225, "end_pos": 236, "type": "DATASET", "confidence": 0.9191308319568634}]}]}