{"title": [], "abstractContent": [{"text": "Claims are the central component of an argument.", "labels": [], "entities": []}, {"text": "Detecting claims across different domains or data sets can often be challenging due to their varying conceptualization.", "labels": [], "entities": [{"text": "Detecting claims", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9142076969146729}]}, {"text": "We propose to alleviate this problem by fine tuning a language model using a Reddit corpus of 5.5 million opinionated claims.", "labels": [], "entities": [{"text": "Reddit corpus", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.7836413681507111}]}, {"text": "These claims are self-labeled by their authors using the in-ternet acronyms IMO/IMHO (in my (humble) opinion).", "labels": [], "entities": [{"text": "IMO/IMHO", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.6492315729459127}]}, {"text": "Empirical results show that using this approach improves the state of art performance across four benchmark argumentation data sets by an average of 4 absolute F1 points in claim detection.", "labels": [], "entities": [{"text": "F1", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9377866983413696}, {"text": "claim detection", "start_pos": 173, "end_pos": 188, "type": "TASK", "confidence": 0.7356073409318924}]}, {"text": "As these data sets include diverse domains such as social media and student essays this improvement demonstrates the robustness of fine-tuning on this novel corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Toulmin's influential work on argumentation (2003) introduced a claim as an assertion that deserves our attention.", "labels": [], "entities": [{"text": "argumentation (2003)", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8656865805387497}]}, {"text": "More recent work describes a claim as a statement that is in dispute and that we are trying to support with reasons.", "labels": [], "entities": []}, {"text": "While some traits of claims are defined by their context, such as that claims usually need some support to makeup a 'complete' argument (e.g., premises, evidence, or justifications), the exact definition of a claim may vary depending on the domain, register, or task.", "labels": [], "entities": []}, {"text": "try to solve the problem of claim conceptualization by training models across one data set and testing on others, but their cross-domain claim detection experiments mostly led to decreased results over in-domain experiments.", "labels": [], "entities": [{"text": "claim conceptualization", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.714393749833107}, {"text": "cross-domain claim detection", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.6223160723845164}]}, {"text": "To demonstrate that some properties of claims are shared across domains, we create a diverse and rich corpus mined from Reddit and evaluate on held out datasets from different sources.", "labels": [], "entities": []}, {"text": "We use Universal Language Model Fine-Tuning (ULMFiT), which pre-trains a language model (LM) on a large general-domain corpus and fine-tunes it on our Reddit corpus before training a final classifier to identify claims on various data sets.", "labels": [], "entities": [{"text": "Reddit corpus", "start_pos": 151, "end_pos": 164, "type": "DATASET", "confidence": 0.8462467789649963}]}, {"text": "We make the following contributions: \u2022 We release a dataset of 5.5 million opinionated claims from Reddit, 1 which we hope will be useful for computational argumentation.", "labels": [], "entities": []}, {"text": "\u2022 We show transfer learning helps in the detection of claims with varying definitions and conceptualizations across data sets from diverse domains such as social media and student essays.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9140904545783997}]}, {"text": "\u2022 Empirical results show that using the Reddit corpus for language model fine-tuning improves the state-of-the-art performance across four benchmark argumentation data sets by an average of 4 absolute F1 points in claim detection.", "labels": [], "entities": [{"text": "Reddit corpus", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8244715929031372}, {"text": "F1", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9317544102668762}, {"text": "claim detection", "start_pos": 214, "end_pos": 229, "type": "TASK", "confidence": 0.7532868981361389}]}], "datasetContent": [{"text": "We obtain statistically significant results (p < 0.05 using Chi Squared Test) overall CNN models trained only on the task-specific datasets.", "labels": [], "entities": []}, {"text": "We also find that for all models, IMHO LM FineTuning even performs better than Task-Specific LM Fine-Tuning, and is significantly better for the MT and WD datasets (which both contain very few claims).", "labels": [], "entities": [{"text": "IMHO LM FineTuning", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.49178722500801086}, {"text": "MT and WD datasets", "start_pos": 145, "end_pos": 163, "type": "DATASET", "confidence": 0.580110639333725}]}, {"text": "For the MT and WD datasets, TaskSpecific LM Fine-Tuning actually performs worse than the CNN models.", "labels": [], "entities": [{"text": "MT and WD datasets", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.5170472115278244}]}], "tableCaptions": [{"text": " Table 1: Table showing number of claims and total  number of sentences in the data sets along with the per- centage of claims in them", "labels": [], "entities": []}, {"text": " Table 2: Table showing the results on four data sets. Each cell contains the Precision (P), Recall (R) and F-score  (F) for Claims as well as the Macro Precision, Recall and F-score for the binary classification.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.954826220870018}, {"text": "Recall (R) and F-score  (F)", "start_pos": 93, "end_pos": 120, "type": "METRIC", "confidence": 0.8577484786510468}, {"text": "F-score", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.981023371219635}]}]}