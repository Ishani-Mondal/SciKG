{"title": [{"text": "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models", "labels": [], "entities": [{"text": "Sequence-to-Sequence Models", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.9071952402591705}]}], "abstractContent": [{"text": "Adversarial examples-perturbations to the input of a model that elicit large changes in the output-have been shown to bean effective way of assessing the robustness of sequence-to-sequence (seq2seq) models.", "labels": [], "entities": []}, {"text": "However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output.", "labels": [], "entities": []}, {"text": "This fact has largely been ignored in the evaluations of the growing body of related literature.", "labels": [], "entities": []}, {"text": "Using the example of untargeted attacks on machine translation (MT), we propose anew evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre-and post-perturbation input into account.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8660548806190491}]}, {"text": "Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output.", "labels": [], "entities": []}, {"text": "We further use this framework to demonstrate that adding additional constraints on attacks allows for ad-versarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence.", "labels": [], "entities": []}, {"text": "Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Attacking a machine learning model with adversarial perturbations is the process of making changes to its input to maximize an adversarial goal, such as mis-classification () or mis-translation ().", "labels": [], "entities": []}, {"text": "These attacks provide insight into the vulnerabilities of machine learning models and their brittleness to samples outside the training distribution.", "labels": [], "entities": []}, {"text": "Lack of robustness to these attacks poses security concerns to safety-critical applications, e.g. self-driving cars (.", "labels": [], "entities": []}, {"text": "Adversarial attacks were first defined and investigated for computer vision systems (;; inter alia), where the input space is continuous, making minuscule perturbations largely imperceptible to the human eye.", "labels": [], "entities": []}, {"text": "In discrete spaces such as natural language sentences, the situation is more problematic; even a flip of a single word or character is generally perceptible by a human reader.", "labels": [], "entities": []}, {"text": "Thus, most of the mathematical framework in previous work is not directly applicable to discrete text data.", "labels": [], "entities": []}, {"text": "Moreover, there is no canonical distance metric for textual data like the p norm in real-valued vector spaces such as images, and evaluating the level of semantic similarity between two sentences is afield of research of its own.", "labels": [], "entities": []}, {"text": "This elicits a natural question: what does the term \"adversarial perturbation\" mean in the context of natural language processing (NLP)?", "labels": [], "entities": []}, {"text": "We propose a simple but natural criterion for adversarial examples in NLP, particularly untargeted 2 attacks on seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side.", "labels": [], "entities": []}, {"text": "The focus on explicitly evaluating meaning preservation is in contrast to previous work on adversarial examples for seq2seq models).", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8392319679260254}]}, {"text": "Nonetheless, this feature is extremely important; given two sentences with equivalent meaning, we would expect a good model to produce two outputs with equivalent meaning.", "labels": [], "entities": []}, {"text": "In other words, any meaningpreserving perturbation that results in the model output changing drastically highlights a fault of the model.", "labels": [], "entities": []}, {"text": "A first technical contribution of this paper is to layout a method for formalizing this concept of meaning-preserving perturbations ( \u00a72).", "labels": [], "entities": []}, {"text": "This makes it possible to evaluate the effectiveness of adversarial attacks or defenses either using goldstandard human evaluation, or approximations that can be calculated without human intervention.", "labels": [], "entities": [{"text": "goldstandard human evaluation", "start_pos": 101, "end_pos": 130, "type": "METRIC", "confidence": 0.8824018637339274}]}, {"text": "We further propose a simple method of imbuing gradient-based word substitution attacks ( \u00a73.1) with simple constraints aimed at increasing the chance that the meaning is preserved ( \u00a73.2).", "labels": [], "entities": [{"text": "word substitution", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7032045125961304}]}, {"text": "Our experiments are designed to answer several questions about meaning preservation in seq2seq models.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.8287584781646729}]}, {"text": "First, we evaluate our proposed \"sourcemeaning-preserving, target-meaning-destroying\" criterion for adversarial examples using both manual and automatic evaluation ( \u00a74.2) and find that a less widely used evaluation metric (chrF) provides significantly better correlation with human judgments than the more widely used BLEU and ME-TEOR metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 319, "end_pos": 323, "type": "METRIC", "confidence": 0.9932706952095032}]}, {"text": "We proceed to perform an evaluation of adversarial example generation techniques, finding that chrF does help to distinguish between perturbations that are more meaning-preserving across a variety of languages and models ( \u00a74.3).", "labels": [], "entities": [{"text": "adversarial example generation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.7207459012667338}]}, {"text": "Finally, we apply existing methods for adversarial training to the adversarial examples with these constraints and show that making adversarial inputs more semantically similar to the source is beneficial for robustness to adversarial attacks and does not decrease test performance on the original data distribution ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments serve two purposes.", "labels": [], "entities": []}, {"text": "First, we examine our proposed framework of evaluating adversarial attacks ( \u00a72), and also elucidate which automatic metrics correlate better with human judgment for the purpose of evaluating adversarial attacks ( \u00a74.2).", "labels": [], "entities": []}, {"text": "Second, we use this evaluation framework to compare various adversarial attacks and demonstrate that adversarial attacks that are explicitly constrained to preserve meaning receive better assessment scores ( \u00a74.3).", "labels": [], "entities": []}, {"text": "Data: Following previous work on adversarial examples for seq2seq models), we perform all experiments on the IWSLT2016 dataset () in the {French,German,Czech}\u2192English directions (fr-en, de-en and cs-en).", "labels": [], "entities": [{"text": "IWSLT2016 dataset", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.978824257850647}]}, {"text": "We compile all previous IWSLT test sets before 2015 as validation data, and keep the 2015 and 2016 test sets as test data.", "labels": [], "entities": [{"text": "IWSLT test sets", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9312309821446737}]}, {"text": "The data is tokenized with the Moses tokenizer (.", "labels": [], "entities": []}, {"text": "The exact data statistics can be found in Appendix A.2.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8669330179691315}]}, {"text": "MT Models: We perform experiments with two common neural machine translation (NMT) models.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7877993583679199}]}, {"text": "The first is an LSTM based encoderdecoder architecture with attention (.", "labels": [], "entities": []}, {"text": "It uses 2-layer encoders and decoders, and dot-product attention.", "labels": [], "entities": []}, {"text": "We set the word embedding dimension to 300 and all others to 500.", "labels": [], "entities": []}, {"text": "The second model is a self-attentional Transformer (, with 6 1024-dimensional encoder and decoder layers and 512 dimensional word embeddings.", "labels": [], "entities": []}, {"text": "Both the models are trained with Adam (), dropout () of probability 0.3 and label smoothing () with value 0.1.", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6304078847169876}]}, {"text": "We experiment with both word based models (vocabulary size fixed at 40k) and subword based models (BPE () with 30k operations).", "labels": [], "entities": [{"text": "BPE", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.7671718597412109}]}, {"text": "For word-based models, we perform <unk> replacement, replacing <unk> tokens in the translated sentences with the source words with the highest attention value during inference.", "labels": [], "entities": []}, {"text": "The full experimental setup and source code are available at https://github.", "labels": [], "entities": []}, {"text": "com/pmichel31415/translate/tree/ paul/pytorch_translate/research/ adversarial/experiments.", "labels": [], "entities": []}, {"text": "Automatic Metric Implementations: To evaluate both sentence and corpus level BLEU score, we first de-tokenize the output and use sacreBLEU 8 (Post, 2018) with its internal intl tokenization, to keep BLEU scores agnostic to tokenization.", "labels": [], "entities": [{"text": "Automatic Metric Implementations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6493791143099467}]}, {"text": "We compute METEOR using the official implementation . ChrF is reported with the sacreBLEU implementation on detokenized text with default parameters.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7982221841812134}]}, {"text": "A toolkit implementing the evaluation framework described in \u00a72.1 for these metrics is released at https://github.", "labels": [], "entities": []}, {"text": "com/pmichel31415/teapot-nlp.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Target RDchrF and source chrF scores for all the attacks on all our models (word-and subword-based  LSTM and Transformer).", "labels": [], "entities": [{"text": "Target RDchrF", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7277612388134003}]}, {"text": " Table 5: chrF (BLEU) scores on the original test set be- fore/after adversarial training of the word-based LSTM  model.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9781887531280518}]}, {"text": " Table 6: Robustness to CharSwap attacks on the val- idation set with/without adversarial training (RDchrF).  Lower is better.", "labels": [], "entities": []}, {"text": " Table 7: IWSLT2016 data statistics.", "labels": [], "entities": [{"text": "IWSLT2016 data statistics", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9484337369600931}]}, {"text": " Table 8: Correlation of automatic metrics to human  judgment of semantic similarity between original and  adversarial source sentences, broken down by number  of perturbed words. \"  *  \" indicates that the correlation is  significantly better than the next-best one.", "labels": [], "entities": []}, {"text": " Table 9: Correlation of automatic metrics to human  judgment of semantic similarity between original and  adversarial source sentences, broken down by type of  constraint on the perturbation. \"  *  \" indicates that the  correlation is significantly better than the next-best one.", "labels": [], "entities": []}]}