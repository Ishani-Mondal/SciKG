{"title": [{"text": "An Integrated Approach for Keyphrase Generation via Exploring the Power of Retrieval and Extraction", "labels": [], "entities": [{"text": "Keyphrase Generation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.9237281680107117}]}], "abstractContent": [{"text": "In this paper, we present a novel integrated approach for keyphrase generation (KG).", "labels": [], "entities": [{"text": "keyphrase generation (KG)", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.846816110610962}]}, {"text": "Unlike previous works which are purely extractive or generative, we first propose anew multi-task learning framework that jointly learns an extractive model and a generative model.", "labels": [], "entities": []}, {"text": "Besides extracting keyphrases, the output of the extractive model is also employed to rectify the copy probability distribution of the generative model, such that the generative model can better identify important contents from the given document.", "labels": [], "entities": []}, {"text": "Moreover, we retrieve similar documents with the given document from training data and use their associated keyphrases as external knowledge for the generative model to produce more accurate keyphrases.", "labels": [], "entities": []}, {"text": "For further exploiting the power of extraction and retrieval, we propose a neural-based merging module to combine and re-rank the predicted keyphrases from the enhanced generative model, the extractive model, and the retrieved keyphrases.", "labels": [], "entities": []}, {"text": "Experiments on the five KG benchmarks demonstrate that our integrated approach outperforms the state-of-the-art methods.", "labels": [], "entities": [{"text": "KG benchmarks", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8847661912441254}]}], "introductionContent": [{"text": "Keyphrases are short text pieces that can quickly express the key ideas of a given document.", "labels": [], "entities": []}, {"text": "The keyphrase generation task aims at automatically generating a set of keyphrases given a document.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8550260663032532}]}, {"text": "As shown in the upper part of, the input is a document and the output is a set of keyphrases.", "labels": [], "entities": []}, {"text": "Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (;, sentiment analysis (, and document clustering).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7808230519294739}, {"text": "sentiment analysis", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.9554769396781921}, {"text": "document clustering", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7270806133747101}]}, {"text": "Existing methods on keyphrase generation can be divided into two categories: extractive and generative.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.79414102435112}]}, {"text": "Extractive methods () identify present keyphrases that appear in the source text like \"parameter control\" in.", "labels": [], "entities": []}, {"text": "Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like \"optimization\" in.", "labels": [], "entities": []}, {"text": "Generative methods () adopt the wellknown encoder-decoder generative model () with copy mechanism () to produce keyphrases.", "labels": [], "entities": []}, {"text": "Ina generative model, the decoder generates keyphrases word byword through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism.", "labels": [], "entities": []}, {"text": "Thus, these generative methods are capable of generating both present and absent keyphrases.", "labels": [], "entities": []}, {"text": "From a high-level perspective, extractive methods directly locate essential phrases in the docu-ment while generative models try to understand the document first and then produce keyphrases.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, these two kinds of methods have been developing independently without any combinations among them.", "labels": [], "entities": []}, {"text": "However, when human annotators are asked to assign keyphrases to a document, they usually first obtain a global sense about which parts of the document are important and then write down the keyphrases word byword based on a more detailed understanding.", "labels": [], "entities": []}, {"text": "To achieve such a goal, we propose a multi-task learning framework to take advantage of both extractive and generative models.", "labels": [], "entities": []}, {"text": "For keyphrase extraction, we adopt a neural sequence labeling model to output the likelihood of each word in the source text to be a keyphrase word (or the importance score of each word).", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8813929855823517}]}, {"text": "These importance scores are then employed to rectify the copy probability distribution of the generative model.", "labels": [], "entities": []}, {"text": "Since the extractive model is explicitly trained to identify keyphrases from the source text, its importance scores can help the copy mechanism to identify important source text words more accurately.", "labels": [], "entities": []}, {"text": "Different from the copy probability distribution which is dynamic at each generation step, these importance scores are static.", "labels": [], "entities": []}, {"text": "Therefore, they can provide a global sense about which parts of the document are important.", "labels": [], "entities": []}, {"text": "In addition, these scores are also utilized to extract present keyphrases which will be exploited by the merging module.", "labels": [], "entities": []}, {"text": "Moreover, human annotators can also incorporate relevant external knowledge like the keyphrases of similar documents that they read before to assign more appropriate keyphrases.", "labels": [], "entities": []}, {"text": "Correspondingly, to incorporate external knowledge, we propose a retriever to retrieve similar documents of the given document from training data.", "labels": [], "entities": []}, {"text": "For instance, as shown in, we retrieve a document from the KP20k training dataset that has the highest similarity with the upper document.", "labels": [], "entities": [{"text": "KP20k training dataset", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.9276281595230103}]}, {"text": "The retrieved document is assigned with almost the same keyphrases as the upper document.", "labels": [], "entities": []}, {"text": "Therefore, keyphrases from similar documents (i.e., retrieved keyphrases) can give useful knowledge to guide the generation of keyphrases for the given document.", "labels": [], "entities": []}, {"text": "More concretely, we encode the retrieved keyphrases as vector representations and use them as an external memory for the decoder of the generative model in our multitask learning framework.", "labels": [], "entities": []}, {"text": "Besides providing external knowledge, the retrieved keyphrases themselves are regarded as a kind of keyphrase prediction and can be utilized by the merging module.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7611566781997681}]}, {"text": "Finally, to imitate the integrated keyphrase assignment process of humans more comprehensively, we further exploit the extractive model and the retrieved keyphrases by proposing a merging module.", "labels": [], "entities": [{"text": "keyphrase assignment process", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7757010261217753}]}, {"text": "This merging module collects and re-ranks the predictions from our aforementioned components.", "labels": [], "entities": []}, {"text": "First, keyphrase candidates are collected from three different sources: (1) keyphrases generated by the enhanced generative model; (2) keyphrases extracted by the extractive model; and (3) the retrieved keyphrases.", "labels": [], "entities": []}, {"text": "Then, we design a neural-based merging algorithm to merge and re-rank all the keyphrase candidates, and consequently return the top-ranked candidates as our final keyphrases.", "labels": [], "entities": []}, {"text": "We extensively evaluate the performance of our proposed approach on five popular benchmarks.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness of the extractive model and the retrieved keyphrases in our multi-task learning framework.", "labels": [], "entities": []}, {"text": "Furthermore, after introducing the merging module, our integrated approach consistently outperforms all the baselines and becomes the new stateof-the-art approach for keyphrase generation.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 167, "end_pos": 187, "type": "TASK", "confidence": 0.8936388790607452}]}, {"text": "In summary, our main contributions include: (1) anew multi-task learning framework that leverages an extractive model and external knowledge to improve keyphrase generation; (2) a novel neural-based merging module that combines the predicted keyphrases from extractive, generative, and retrieval methods to further improve the performance; and (3) the new state-of-the-art performance on five real-world benchmarks.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.8222420811653137}]}], "datasetContent": [{"text": "For a comprehensive evaluation, we compare our methods with the traditional extractive baselines and the state-of-the-art generative methods.", "labels": [], "entities": []}, {"text": "The extractive baselines include two unsupervised methods (i.e., TF-IDF and TextRank (Mihalcea and Tarau, 2004)) and one supervised method Maui ().", "labels": [], "entities": [{"text": "Maui", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8543652892112732}]}, {"text": "The generative baselines consist of and).", "labels": [], "entities": []}, {"text": "We also conduct several ablation studies as follows: \u2022 KG-KE.", "labels": [], "entities": []}, {"text": "The joint extraction and generation model without using the retrieved keyphrases and merging process.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7024625539779663}]}, {"text": "The encoder-decoder generative model with retrieved keyphrases as external knowledge, but without combining with the extractive model and using the merging process.", "labels": [], "entities": []}, {"text": "The joint extraction and generation model with the retrieved keyphrases without using the merging process.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7208809554576874}]}, {"text": "All the above ablation models directly use the generated candidates as the final predictions.", "labels": [], "entities": []}, {"text": "We denote our final integrated method which combines all the proposed modules as KG-KE-KR-M.", "labels": [], "entities": []}, {"text": "Similar to CopyRNN and CorrRNN, we adopt macro-averaged recall (R) and F-measure (F 1 ) as our evaluation metrics.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9337771683931351}, {"text": "F-measure (F 1 )", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.9553907513618469}]}, {"text": "In addition, we also apply Porter Stemmer before determining whether two keyphrases are identical.", "labels": [], "entities": []}, {"text": "Duplications are removed after stemming.", "labels": [], "entities": [{"text": "Duplications", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9570762515068054}]}], "tableCaptions": [{"text": " Table 1: Total keyphrase prediction results on all testing datasets. The best results are bold and the second best  results are underlined. The subscripts are corresponding standard deviations for neural-based models (e.g. 0.257 2  means 0.257\u00b10.002). The ' * ' indicates our implementations based on Luong et al. (2015) attention and See et al.  (2017) copying. The implementations of our proposed models are based on \"CopyRNN * \".", "labels": [], "entities": []}, {"text": " Table 2: MAP@10 scores of total keyphrase predic- tions. The best results are bold and the second best  results are underlined. The meanings of the subscripts  and the ' * ' are the same as in Table 1.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8758444786071777}]}, {"text": " Table 3: Ablation study of the candidate sources of Al- gorithm 1 on Krapivin dataset. \"no merging\" means we  do not use the merging algorithm.", "labels": [], "entities": [{"text": "Al- gorithm 1", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.8333002924919128}, {"text": "Krapivin dataset", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.9239221215248108}]}, {"text": " Table 4: Ablation study of the scoring method of Algo- rithm 1 on Krapivin dataset. \"Only gs, es, rs\" means  we do not use the scorer. \"Only scorer\" represents we  directly use the scores predicted by the scorer as the  final importance scores.", "labels": [], "entities": [{"text": "Algo- rithm 1", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8880636990070343}, {"text": "Krapivin dataset", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9527890682220459}]}]}