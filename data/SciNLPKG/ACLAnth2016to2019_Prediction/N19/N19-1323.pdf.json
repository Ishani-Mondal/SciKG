{"title": [{"text": "Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction", "labels": [], "entities": [{"text": "Neural Relation Extraction", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.8221209446589152}]}], "abstractContent": [{"text": "Knowledge Bases (KBs) require constant updating to reflect changes to the world they represent.", "labels": [], "entities": []}, {"text": "For general purpose KBs, this is often done through Relation Extraction (RE), the task of predicting KB relations expressed in text mentioning entities known to the KB.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7318618655204773}, {"text": "predicting KB relations expressed in text mentioning entities", "start_pos": 90, "end_pos": 151, "type": "TASK", "confidence": 0.8253643587231636}]}, {"text": "One way to improve RE is to use KB Embeddings (KBE) for link prediction.", "labels": [], "entities": [{"text": "RE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9776901602745056}, {"text": "link prediction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7971287965774536}]}, {"text": "However, despite clear connections between RE and KBE, little has been done toward properly unifying these models systematically.", "labels": [], "entities": []}, {"text": "We help close the gap with a framework that unifies the learning of RE and KBE models leading to significant improvements over the state-of-the-art in RE.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge Bases (KBs) contain structured information about the world and are used in support of many natural language processing applications such as semantic search and question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.9004158675670624}]}, {"text": "Building KBs is a never-ending challenge because, as the world changes, new knowledge needs to be harvested while old knowledge needs to be revised.", "labels": [], "entities": []}, {"text": "This motivates the work on the Relation Extraction (RE) task, whose goal is to assign a KB relation to a phrase connecting a pair of entities, which in turn can be used for updating the KB.", "labels": [], "entities": [{"text": "Relation Extraction (RE) task", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8342784692843755}]}, {"text": "The state-of-the-art in RE builds on neural models using distant (a.k.a. weak) supervision) on large-scale corpora for training.", "labels": [], "entities": [{"text": "RE", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9736019372940063}]}, {"text": "A task related to RE is that of Knowledge Base Embedding (KBE), which is concerned with representing KB entities and relations in a vector space for predicting missing links in the graph.", "labels": [], "entities": [{"text": "RE", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9458624124526978}]}, {"text": "Aiming to leverage the similarities between these tasks,  were the first to show that combining predictions from RE and KBE models was beneficial for RE.", "labels": [], "entities": [{"text": "RE", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9691302180290222}]}, {"text": "However, the way in which they combine RE and KBE predictions is rather naive (namely, by adding those scores).", "labels": [], "entities": [{"text": "RE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.6728796362876892}]}, {"text": "To the best of our knowledge, there have been no systematic attempts to further unify RE and KBE, particularly during model training.", "labels": [], "entities": [{"text": "RE and KBE", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.3900239169597626}]}, {"text": "We seek to close this gap with HRERE (Heterogeneous REpresentations for neural Relation Extraction), a novel neural RE framework that learns language and knowledge representations jointly.", "labels": [], "entities": [{"text": "HRERE", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9163180589675903}, {"text": "neural Relation Extraction)", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.7619900777935982}]}, {"text": "HRERE's backbone is a bi-directional long short term memory (LSTM) network with multiple levels of attention to learn representations of text expressing relations.", "labels": [], "entities": []}, {"text": "The knowledge representation machinery, borrowed from, nudges the language model to agree with facts in the KB.", "labels": [], "entities": []}, {"text": "Joint learning is guided by three loss functions: one for the language representation, another for the knowledge representation, and a third one to ensure these representations do not diverge.", "labels": [], "entities": [{"text": "Joint learning", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8780958950519562}]}, {"text": "In effect, this contributes to HRERE's generalization power by preventing over-fitting by either model.", "labels": [], "entities": [{"text": "HRERE", "start_pos": 31, "end_pos": 36, "type": "TASK", "confidence": 0.7541413307189941}]}, {"text": "We build on state-of-the-art methods for learning the separate RE and KBE representations and on learning tools that allow us to scale to a moderately large training corpus.", "labels": [], "entities": []}, {"text": "(We use a subset of Freebase with 3M entities as our KB.)", "labels": [], "entities": []}, {"text": "We validate our approach on an established benchmark against state-of-the-art methods for RE, observing not only that our base model significantly outperforms previous methods, but also the fact that jointly learning the heterogeneous representations consistently brings in improvements.", "labels": [], "entities": [{"text": "RE", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9891574382781982}]}, {"text": "To the best of our knowledge, ours is the first principled framework to combine and jointly learn heterogeneous representations from both language and knowledge for the RE task.", "labels": [], "entities": [{"text": "RE task", "start_pos": 169, "end_pos": 176, "type": "TASK", "confidence": 0.9355896711349487}]}, {"text": "This paper describes and evaluates a novel neural framework for jointly learning representations for RE and KBE tasks that uses a cross-entropy loss function to ensure both representations are learned together, resulting in significant improvements over the current state-of-theart for the RE task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on the widely used NYT dataset ( to help the model generalize to words not appearing in the training set.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9053860008716583}]}, {"text": "Hyperparameter Settings: For hyperparameter tuning, we randonly sampled 10% of the training set as a development set.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.715355634689331}]}, {"text": "All the hyperparameters were obtained by evaluating the model on the development set.", "labels": [], "entities": []}, {"text": "With the well-tuned hyperparameter setting, we run each model five times on the whole training set and report the average P@N.", "labels": [], "entities": [{"text": "P", "start_pos": 122, "end_pos": 123, "type": "METRIC", "confidence": 0.9615215063095093}]}, {"text": "For Precision/Recall curves, we just select the results from the first run of each model.", "labels": [], "entities": [{"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.6002559661865234}]}, {"text": "For training, learning rate on \u0398 (L) lr1 5 \u00d7 10 \u22124 learning rate on \u0398 (K) lr2 1 \u00d7 10 \u22125 size of word position embedding dp 25 state size for LSTM layers ds 320 input dropout keep probability pi 0.9 output dropout keep probability po 0.7 L2 regularization parameter \u03bb 0.0003 combining weight parameter \u03b1 0.6  P@N(%) 10% 30% 50% Weston 79.3 68.6 60.9 HRERE-base 81.8 70.1 60.7 HRERE-naive 83.6 74.4 65.7 HRERE-full 86.1 76.6 68.1  with the methods stated in this paper directly without joint learning.", "labels": [], "entities": []}, {"text": "shows the Precision/Recall curves for all the above methods.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9874206185340881}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.6689501404762268}]}, {"text": "As one can see, HRERE-base significantly outperforms previous state-of-the-art neural models and Weston over the entire range of recall.", "labels": [], "entities": [{"text": "HRERE-base", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9030725955963135}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9946881532669067}]}, {"text": "However, HREREbase performs worst compared to all other variants, while HRERE-full always performs best as shown in and.", "labels": [], "entities": [{"text": "HREREbase", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.8013443350791931}]}, {"text": "This suggests that introducing knowledge representation consistently results in improvements, which validates our motivating hypothesis.", "labels": [], "entities": []}, {"text": "HRERE-naive simply optimizes both local and global loss at the same time without attempting to connect them.", "labels": [], "entities": []}, {"text": "We can see that HRERE-full is not only consistently superior but also more stable than HRERE-naive when the recall is less than 0.1.", "labels": [], "entities": [{"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9991874098777771}]}, {"text": "One possible reason for the instability is that the results maybe dominated by one of the representations and biased toward it.", "labels": [], "entities": []}, {"text": "This suggests that (1) jointly learning the heterogeneous representations bring mutual benefits which are out of reach of previous methods that learn each independently; (2) connecting heterogeneous representations can increase the robustness of the framework.", "labels": [], "entities": []}, {"text": "shows two examples in the testing data.", "labels": [], "entities": []}, {"text": "For each example, we show the relation, the sentence along with entity mentions and the corresponding probabilities predicted by HRERE-base and HRERE-full.", "labels": [], "entities": []}, {"text": "The entity pairs in the sentence are highlighted with bold formatting.", "labels": [], "entities": []}, {"text": "From the table, we have the following observations: (1) The predicted probabilities of three variants of our model in the table match the observations and corroborate our analysis.", "labels": [], "entities": []}, {"text": "(2) From the text of the two sentences, we can easily infer that middle east contains Iran and Henry Fonda was born in Omaha.", "labels": [], "entities": []}, {"text": "However, HRERE-base fails to detect these relations, suggesting that it is hard for models based on language representations alone to detect implicit relations, which is reasonable to expect.", "labels": [], "entities": []}, {"text": "With the help of KBE, the model can effectively identify implicit relations present in the text.", "labels": [], "entities": [{"text": "KBE", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.6148076057434082}]}, {"text": "(3) It may happen that the relation cannot be inferred by the text as shown in the last example.", "labels": [], "entities": []}, {"text": "It's a common wrong labeled case caused by distant supervision.", "labels": [], "entities": []}, {"text": "It is a case of an incorrectly labeled instance, atypical occurrence in distant supervision.", "labels": [], "entities": []}, {"text": "However, the fact is obviously true in the KBs.", "labels": [], "entities": []}, {"text": "As a result, HRERE-full gives the underlying relation according to the KBs.", "labels": [], "entities": [{"text": "HRERE-full", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8004870414733887}]}, {"text": "This observation may point to one direction of de-noising weakly labeled textual mentions generated by distant supervision.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Some examples in NYT corpus and the predicted probabilities of the true relations.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.9425534605979919}]}]}