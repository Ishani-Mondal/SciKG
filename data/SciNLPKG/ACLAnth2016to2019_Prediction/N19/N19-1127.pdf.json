{"title": [{"text": "Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural networks equipped with self-attention have parallelizable computation, lightweight structure, and the ability to capture both long-range and local dependencies.", "labels": [], "entities": []}, {"text": "Further, their expressive power and performance can be boosted by using a vector to measure pair-wise dependency, but this requires to expand the alignment matrix to a tensor, which results in memory and computation bottlenecks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel attention mechanism called \"Multi-mask Tensorized Self-Attention\" (MTSA), which is as fast and as memory-efficient as a CNN, but significantly outperforms previous CNN-/RNN-/attention-based models.", "labels": [], "entities": []}, {"text": "MTSA 1) captures both pair-wise (token2token) and global (source2token) dependencies by a novel compatibility function composed of dot-product and additive attentions, 2) uses a tensor to represent the feature-wise alignment scores for better expressive power but only requires paralleliz-able matrix multiplications, and 3) combines multi-head with multi-dimensional attentions, and applies a distinct positional mask to each head (subspace), so the memory and computation can be distributed to multiple heads, each with sequential information encoded independently.", "labels": [], "entities": [{"text": "MTSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9445376396179199}]}, {"text": "The experiments show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or competitive performance on nine NLP benchmarks with compelling memory-and time-efficiency.", "labels": [], "entities": [{"text": "MTSA", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.7224979996681213}]}], "introductionContent": [{"text": "Recurrent neural network (RNN) and convolutional neural network (CNN) have been broadly used as context fusion modules for natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Recently, RNN/CNN in conjunction with an attention mechanism has been proven to be effective for contextual feature modeling in a wide range of NLP tasks, including sentiment classification, machine translation (, reading comprehension (, etc.", "labels": [], "entities": [{"text": "contextual feature modeling", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.6507186492284139}, {"text": "sentiment classification", "start_pos": 165, "end_pos": 189, "type": "TASK", "confidence": 0.9454658627510071}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7965072989463806}]}, {"text": "More recently, self-attention mechanisms have been developed for context fusion and syntactic dependency modeling with the advantage of fewer parameters, more parallelizable computation, and better empirical performance ().", "labels": [], "entities": [{"text": "context fusion", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7672823369503021}, {"text": "syntactic dependency modeling", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.6586654583613077}]}, {"text": "In addition, neural networks based solely on self-attention mechanisms have achieved state-of-the-art quality on many NLP tasks, e.g., machine translation (, sentence embedding) and semantic role labeling (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7964410185813904}, {"text": "semantic role labeling", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.6861648360888163}]}, {"text": "Self-attention mechanisms can be categorized into two classes according to the type of dependency each aims to model.", "labels": [], "entities": []}, {"text": "The first category is token2token self-attention () that captures syntactic dependency between every two tokens in a sequence.", "labels": [], "entities": []}, {"text": "An efficient dot-product compatibility function is usually deployed to measure this pairwise dependency (.", "labels": [], "entities": []}, {"text": "In contrast, additive compatibility function captures the dependency by multi-layer perceptron (MLP), and can usually achieve better performance (.", "labels": [], "entities": []}, {"text": "Its expressive power can be further improved if expanded to multiple dimensions ().", "labels": [], "entities": []}, {"text": "This multi-dim self-attention empirically surpasses dot-product one, but suffers from expensive computation and memory, which grow linearly with the number of features and quadratically with the sequence length.", "labels": [], "entities": []}, {"text": "Hence, it is not scalable to long sequences in practice.", "labels": [], "entities": []}, {"text": "The second category is source2token selfattention () aiming to capture global dependency, i.e., the importance of each token to the entire sequence fora specific task.", "labels": [], "entities": []}, {"text": "Its time and space complexities grow linearly, rather than quadratically, with the sequence length.", "labels": [], "entities": []}, {"text": "Hence, it is empirically efficient in terms of memory and computation even if expanded to multiple dimensions, i.e., using a vector of feature-wise scores instead of a scalar for the global dependency.", "labels": [], "entities": []}, {"text": "But, it is hard to reach state-of-the-art performance on NLP tasks due to the lack of pairwise and local dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel attention mechanism called multi-mask tensorized self-attention (MTSA), for context fusion.", "labels": [], "entities": [{"text": "context fusion", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7387102693319321}]}, {"text": "In MTSA, 1) the pairwise dependency is captured by an efficient dot-product based token2token selfattention, while the global dependency is modeled by a feature-wise multi-dim source2token selfattention, so they can work jointly to encode rich contextual features; 2) self-attention alignment scores are tensorized for more expressive power in that each pair of tokens has one score for each feature, but no tensor computation is required other than simple and efficient matrix multiplications when implemented; 3) the tensors above are computed in multiple subspaces (i.e., in a multi-head fashion) rather than in the original input space, so the required memory and computation can be distributed to multiple subspaces; and 4) a distinct positional mask is applied to each head in order to encode rich structural information such as the sequential order and relative position of tokens.", "labels": [], "entities": []}, {"text": "In the experiments, we build CNN/RNN-free neural networks based on MTSA for sentence embedding and sequence tagging tasks, including natural language inference, semantic role labeling, sentiment analysis, question-type classification, machine translation, etc.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.7095268070697784}, {"text": "semantic role labeling", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.6488878130912781}, {"text": "sentiment analysis", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.9304601848125458}, {"text": "question-type classification", "start_pos": 205, "end_pos": 233, "type": "TASK", "confidence": 0.8068718016147614}, {"text": "machine translation", "start_pos": 235, "end_pos": 254, "type": "TASK", "confidence": 0.8042892813682556}]}, {"text": "The results demonstrate that MTSA achieves state-of-the-art or competitive performance on nine benchmark datasets.", "labels": [], "entities": [{"text": "MTSA", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.9028953313827515}]}, {"text": "To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in(a) and 1(b) on synthetic data (batch size of 64 and feature channels of 300).", "labels": [], "entities": [{"text": "MTSA", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9149835109710693}, {"text": "sequence length", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.8691027164459229}]}, {"text": "On the SNLI (), a public dataset for language inference, as shown in(c), MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section 4).", "labels": [], "entities": []}, {"text": "Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare MTSA with commonly-used context fusion baselines on several NLP tasks 1 . When addressing a sentence embedding problem, a multidim source2token self-attention is applied on the top of context fusion module to produce the sequence embedding.", "labels": [], "entities": []}, {"text": "Codes are implemented in Python with Tensorflow and executed on a single NVIDIA GTX 1080Ti graphics card.", "labels": [], "entities": []}, {"text": "In addition, data for both time cost and memory consumption are collected under Tensorflow-1.7 with CUDA9 and cuDNN7.", "labels": [], "entities": [{"text": "CUDA9", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9404348134994507}, {"text": "cuDNN7", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9273980259895325}]}, {"text": "The context fusion baselines include 1) Bi-LSTM (): 600D bidirectional block self-attention with intra-/interblock self-attention, aiming to reduce the time and space complexities of multi-dim self-attention by using hierarchical structure.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9513483047485352}]}], "tableCaptions": [{"text": " Table 1: Experimental results for different methods with comparative parameter number on SNLI. |\u03b8|: the number  of parameters (excluding word embedding part); Time/Epoch: averaged training time per epoch with batch size  128; Inf. Time: averaged dev inference time with batch size 128; Memory: memory load on synthetic data of  sequence length 64 and batch size 64 with back-propagation considered; Train Acc. and Test Acc.: the accuracies  on training/test sets. All state-of-the-art methods in leaderboard are listed in Table 1&2 up to Sep. 2018.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.8762363195419312}, {"text": "Time/Epoch", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.8593746821085612}, {"text": "Inf. Time", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.8386115233103434}, {"text": "Train Acc.", "start_pos": 400, "end_pos": 410, "type": "METRIC", "confidence": 0.8821446597576141}, {"text": "Test Acc.", "start_pos": 415, "end_pos": 424, "type": "METRIC", "confidence": 0.8003300726413727}]}, {"text": " Table 2: Experimental results on sentence-encoding  based SNLI and MultiNLI benchmark tasks. \"Trans- fer\" denotes pretrained language model on large cor- pus for transfer learning, which detailed by Radford  et al. (2018). References: a (Nie and Bansal, 2017),  b (Chen et al., 2018), c (Talman et al., 2018).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.9455209970474243}]}, {"text": " Table 3: An ablation study of MTSA on SNLI.", "labels": [], "entities": [{"text": "MTSA", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.8168460130691528}, {"text": "SNLI", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8264145255088806}]}, {"text": " Table 4: Experimental Results of SRL for single models on CoNLL-05 with gold predicates.  *  Multi-head baseline  is equivalent to the model in Tan et al. (2017). For fair comparisons, first, we use the hyper-parameters provided  by Tan et al. (2017) instead of tuning them; second, all listed models are independent of external linguistics  information, e.g., PoS, dependency parsing.", "labels": [], "entities": [{"text": "SRL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9870937466621399}, {"text": "CoNLL-05", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9338425993919373}, {"text": "dependency parsing", "start_pos": 367, "end_pos": 385, "type": "TASK", "confidence": 0.7643548548221588}]}, {"text": " Table 6: Results for the Transformer with either multi- head self-attention or proposed MTSA. The reported  BLEU values for Setup 1 and 2 are the mean of 5 and  3 runs respectively.", "labels": [], "entities": [{"text": "MTSA", "start_pos": 89, "end_pos": 93, "type": "TASK", "confidence": 0.4430904686450958}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9992102384567261}]}]}