{"title": [{"text": "Vector of Locally Aggregated Embeddings for Text Representation", "labels": [], "entities": [{"text": "Text Representation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7477801442146301}]}], "abstractContent": [{"text": "We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately , lossless representation of textual content.", "labels": [], "entities": []}, {"text": "Our model encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts.", "labels": [], "entities": []}, {"text": "The proposed model generates high quality representation of textual content and improves the classification performance of current state-of-the-art deep averaging networks across several text classification tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7133585512638092}]}], "introductionContent": [{"text": "Representation learning algorithms can reveal intrinsic low-dimensional structure in data).", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9078219830989838}]}, {"text": "In particular, deep averaging networks (DANs) are effective for text classification).", "labels": [], "entities": [{"text": "text classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8401318788528442}]}, {"text": "They achieve their improvement through use of word embeddings, weighted averaging, and deepening networks.", "labels": [], "entities": [{"text": "weighted averaging", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.713938981294632}]}, {"text": "The above works show that DANs can outperform RNNs and CNNs in text classification while taking only a fraction of their training time.", "labels": [], "entities": [{"text": "text classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8306889832019806}]}, {"text": "In this work, with a special focus on DANs, we study the effect of information loss associated with average word embeddings and develop algorithms that are robust against information loss for text representation.", "labels": [], "entities": []}, {"text": "We show that divergence of word embeddings from their average can be considered as a good proxy to quantify information loss; in particular, longer documents suffer from significant information loss when represented by average word embeddings.", "labels": [], "entities": []}, {"text": "These results inspire our work to develop a novel representation learning approach based on Vector of Locally Aggregated Descriptors (VLAD) ()-an effective approach to integrate image descriptors for large scale image datasets.", "labels": [], "entities": []}, {"text": "Our model identifies semantically-relevant parts of documents and locally integrates their representations through clustering and autoencoding.", "labels": [], "entities": []}, {"text": "In contrast to averaging, our model prevents larger semantically-relevant parts of inputs to dominate final representations.", "labels": [], "entities": []}, {"text": "It improves DANs by 5.30 macro-F1 points in classifying longer texts and show comparable performance to them on shorter text.", "labels": [], "entities": [{"text": "DANs", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9208953380584717}, {"text": "classifying longer texts", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8796687722206116}]}], "datasetContent": [{"text": "Data: We investigate VLAEs in three binary classification tasks: sentiment classification on IMDb (Maas et al., 2011), disease-text classification on Reddit, where the task is to classify reddit posts as relevant or irrelevant to specific diseases, and churn prediction on Twitter (, where the task is to classify/predict if given tweets indicate user intention about leaving brands, e.g. the tweet \"my days with BRAND are numbered\" is a churny tweet.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.917888343334198}, {"text": "disease-text classification", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.6947249621152878}, {"text": "churn prediction", "start_pos": 253, "end_pos": 269, "type": "TASK", "confidence": 0.7182186394929886}, {"text": "BRAND", "start_pos": 413, "end_pos": 418, "type": "METRIC", "confidence": 0.9656929969787598}]}, {"text": "For pre-processing, we change all texts to lowercase, and remove stop words, user names, and URLs from texts.", "labels": [], "entities": []}, {"text": "Settings: We use validation data for hyperparameter tuning and model selection.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.704615443944931}, {"text": "model selection", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7119521647691727}]}, {"text": "We use 300-dimensional word embeddings (d = 300) provided by Google (, and for greater number of ds, we train word2vec on unlabeled data, see.", "labels": [], "entities": []}, {"text": "In addition, we set the dimensionality reduction parameter m from {1 . .", "labels": [], "entities": []}, {"text": "4} using validation data.", "labels": [], "entities": []}, {"text": "The best value of m is the same across tasks/datasets, m = 2.", "labels": [], "entities": []}, {"text": "Furthermore, we determine the number of clusters k for VLAEs by choosing the optimal k from {2 i , i = {1 . .", "labels": [], "entities": []}, {"text": "7}} using validation data of each dataset.", "labels": [], "entities": []}, {"text": "We learn optimal k with respect to task, but not embedding space, due to significant density of the semantic space of word embeddings, see Note on Clustering Word Embeddings.", "labels": [], "entities": []}, {"text": "Baselines: We consider two versions of DANs as baselines: Avg small and Avg large which represent documents by average word embedding of size d = 300 and d = m \u00d7 300 respectively.", "labels": [], "entities": []}, {"text": "Note that, for fair comparison, Avg large has the exact same size as our model (VLAE); however, depending on m, their network size is 1.3-1.6 times greater than that of Avg small due to difference in input dimensions.", "labels": [], "entities": []}, {"text": "We use 3 hidden layers of size 300 for above networks.", "labels": [], "entities": []}, {"text": "Also, to directly evaluate the effect of averaging, we do not adjust initial word embeddings during training.", "labels": [], "entities": []}, {"text": "Experimental Results: shows the performance of different models across datasets.", "labels": [], "entities": []}, {"text": "The results show that VLAE significantly outperforms Avg small and Avg large by 2.6 and 7.2 points in Macro-F1 on IMDb.", "labels": [], "entities": [{"text": "VLAE", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8012816309928894}, {"text": "Avg small", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9567474126815796}, {"text": "Avg large", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9592869281768799}]}, {"text": "The corresponding values on Reddit dataset are 6.7 and 3.4 points respectively.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9639551341533661}]}, {"text": "We believe these improvements are due to more effective and lossless representation of inputs.", "labels": [], "entities": []}, {"text": "We note that Avg large performs worse than Avg small on IMDb.", "labels": [], "entities": []}, {"text": "This could be attributed to the size of training data which may not be enough to train Avg large, or to lower quality of input representations in Avg large compared to Avg small in case of IMDb.", "labels": [], "entities": []}, {"text": "Note that although VLAE has the same number of parameters as Avg large, it uses autoencoding to effectively filter redundant information.", "labels": [], "entities": []}, {"text": "Verify-  ing these hypotheses will be the subject of future work.", "labels": [], "entities": []}, {"text": "In addition, VLAE show lower performance than Avg large on Twitter dataset, F1 of 72.62 versus 73.08.", "labels": [], "entities": [{"text": "VLAE", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.5223215222358704}, {"text": "Twitter dataset", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9295369982719421}, {"text": "F1", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9998003840446472}]}, {"text": "We attribute this result to the shorter length of tweets for which, as we experimentally showed before, averaging does not cause major divergence in representations.", "labels": [], "entities": []}, {"text": "On average, VLAE improves Avg small and Avg large by 4.7 and 5.3 F1 points on IMDb and Reddit (longer texts) respectively.", "labels": [], "entities": [{"text": "Avg small", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9701330065727234}, {"text": "Avg large", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9682751893997192}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9968627691268921}]}, {"text": "It also shows comparable performance to best performing model on Twitter (shorter texts).", "labels": [], "entities": []}, {"text": "We also compare models in terms of the quality of their representations.", "labels": [], "entities": []}, {"text": "For this comparison, we ignore input preparation time and assume a model that generates better representations should converge faster than other models; note that the overall turnaround time of VLAE is greater than that of Avg small or Avg large because of its input preparation time which we ignore for the purpose of this experiment.", "labels": [], "entities": []}, {"text": "The result show that VLAE leads to 7.5, 1.3, and 1.3 times faster convergence than Avg small and 14.9, 2.6, and 1.8 times faster convergence than Avg large on IMDb, Reddit, and Twitter datasets respectively.", "labels": [], "entities": [{"text": "VLAE", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.5722400546073914}]}, {"text": "Considering the size of these networks, these results indicate that representations obtained from VLAE are much better than those of its counterparts.", "labels": [], "entities": []}, {"text": "Note on Clustering Word Embeddings: In experiments, we observe clusters obtained from word embeddings are often very dense.", "labels": [], "entities": []}, {"text": "This is a challenge for our model because with small number of clusters (ks) potentially dissimilar words can appear in the same cluster, while with large ks semantically-similar words may appear in different clusters.", "labels": [], "entities": []}, {"text": "Neither of these are desired.", "labels": [], "entities": []}, {"text": "To illustrate the above challenge, we report Silhouette Coefficient (SC)   by a clustering model.", "labels": [], "entities": []}, {"text": "It is calculated using the mean intra-cluster distance and the mean nearestcluster distance for each sample.", "labels": [], "entities": []}, {"text": "Specifically, the mean distance between each embedding and all other embeddings in the same cluster (mc), and the mean distance between the embedding and all other embeddings in the next nearest cluster (the nearest cluster that the embedding is not part of) (mn) are used to measure SC for the embedding: The best and worst SC scores are 1 and \u22121 which indicate ideal and worst clustering respectively.", "labels": [], "entities": [{"text": "SC", "start_pos": 284, "end_pos": 286, "type": "METRIC", "confidence": 0.9740145802497864}]}, {"text": "Also, values near 0 indicate overlapping clusters.", "labels": [], "entities": []}, {"text": "shows the mean SCs computed overall word embeddings for IMDb and Reddit datasets.", "labels": [], "entities": [{"text": "IMDb and Reddit datasets", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.6863972246646881}]}, {"text": "The results show that (a): the best number of clusters is k = 2 on both datasets, and (b): Silhouette Coefficient scores generally home in on values close to zero as the number of clusters increases.", "labels": [], "entities": []}, {"text": "These results show significant density of embeddings in semantic space.", "labels": [], "entities": []}, {"text": "Therefore, we optimize the number of clusters for creating VLAEs by resorting to validation data and measuring taskspecific performance.", "labels": [], "entities": []}, {"text": "From these results, we conclude that a hierarchical clustering approach that recursively combines pairs of semanticallysimilar clusters could help better defining these clusters and perhaps improve the performance of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Macro-F1 performance of different models  across datasets. Asterisk mark (*) indicates signifi- cant difference between top two systems. VLAE out- performs other models on longer texts, and show com- parable performance to Avg large on shorter text.", "labels": [], "entities": [{"text": "Asterisk mark (*) indicates signifi- cant difference", "start_pos": 69, "end_pos": 121, "type": "METRIC", "confidence": 0.8412434570491314}]}]}