{"title": [{"text": "Argument Mining for Understanding Peer Reviews", "labels": [], "entities": [{"text": "Argument Mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7836653888225555}]}], "abstractContent": [{"text": "Peer-review plays a critical role in the scientific writing and publication ecosystem.", "labels": [], "entities": []}, {"text": "To assess the efficiency and efficacy of the reviewing process, one essential element is to understand and evaluate the reviews themselves.", "labels": [], "entities": []}, {"text": "In this work, we study the content and structure of peer reviews under the argument mining framework, through automatically detecting (1) argumentative propositions put forward by reviewers, and (2) their types (e.g., evaluating the work or making suggestions for improvement).", "labels": [], "entities": []}, {"text": "We first collect 14.2K reviews from major machine learning and natural language processing venues.", "labels": [], "entities": []}, {"text": "400 reviews are annotated with 10, 386 propositions and corresponding types of EVALUATION, REQUEST, FACT, REFERENCE, or QUOTE.", "labels": [], "entities": [{"text": "EVALUATION", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9420011639595032}, {"text": "REQUEST", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9926705360412598}, {"text": "FACT", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8550437092781067}, {"text": "REFERENCE", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9833664894104004}]}, {"text": "We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain, motivating future directions for argument mining.", "labels": [], "entities": [{"text": "proposition segmentation and classification", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.7677858769893646}, {"text": "argument mining", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.7569448053836823}]}, {"text": "Further experiments show that proposition usage varies across venues in amount, type, and topic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Peer review is a process where domain experts scrutinize the quality of research work in their field, and it is a cornerstone of scientific discovery (.", "labels": [], "entities": [{"text": "Peer review", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7346745878458023}]}, {"text": "In 2015 alone, approximately 63.4 million hours were spent on peer reviews ().", "labels": [], "entities": []}, {"text": "To maximize their benefit to the scientific community, it is crucial to understand and evaluate the construction and limitation of reviews themselves.", "labels": [], "entities": []}, {"text": "However, minimal work has been done to analyze reviews' content and structure, let alone to evaluate their qualities.", "labels": [], "entities": []}, {"text": "As seen in, peer reviews resemble arguments: they contain argumentative propositions (henceforth propositions) that convey re-  viewers' interpretation and evaluation of the research.", "labels": [], "entities": []}, {"text": "Constructive reviews, e.g., review #2, often contain in-depth analysis as well as concrete suggestions.", "labels": [], "entities": []}, {"text": "As a result, automatically identifying propositions and their types would be useful to understand the composition of peer reviews.", "labels": [], "entities": []}, {"text": "Therefore, we propose an argument miningbased approach to understand the content and structure of peer reviews.", "labels": [], "entities": []}, {"text": "Argument mining studies the automatic detection of argumentative components and structure within discourse.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8291807770729065}, {"text": "automatic detection of argumentative components and structure within discourse", "start_pos": 28, "end_pos": 106, "type": "TASK", "confidence": 0.7913095023896959}]}, {"text": "Specifically, argument types (e.g. evidence and reasoning) and their arrangement are indicative of argument quality (.", "labels": [], "entities": []}, {"text": "In this work, we focus on two specific tasks: (1) proposition segmentation-detecting elementary argumentative discourse units that are propositions, and (2) proposition classificationlabeling the propositions according to their types (e.g., evaluation vs. request).", "labels": [], "entities": [{"text": "proposition segmentation-detecting elementary argumentative discourse units that are propositions", "start_pos": 50, "end_pos": 147, "type": "TASK", "confidence": 0.8742238283157349}, {"text": "proposition classificationlabeling the propositions", "start_pos": 157, "end_pos": 208, "type": "TASK", "confidence": 0.7940905913710594}]}, {"text": "Since there was no annotated dataset for peer reviews, as part of this study, we first collect 14.2K reviews from major machine learning (ML) and natural language processing (NLP) venues.", "labels": [], "entities": []}, {"text": "We create a dataset, AMPERE (Argument Mining for PEer REviews), by annotating 400 reviews with 10, 386 propositions and labeling each proposition with the type of EVALUATION, REQUEST, FACT, REFERENCE, QUOTE, or NON-ARG.", "labels": [], "entities": [{"text": "REQUEST", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.9136191606521606}, {"text": "FACT", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.6527968645095825}, {"text": "REFERENCE", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.8968679904937744}, {"text": "QUOTE", "start_pos": 201, "end_pos": 206, "type": "METRIC", "confidence": 0.8803919553756714}]}, {"text": "1 Significant inter-annotator agreement is achieved for proposition segmentation (Cohen's \u03ba = 0.93), with good consensus level for type annotation (Krippendorf's \u03b1 U = 0.61).", "labels": [], "entities": [{"text": "proposition segmentation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7860395908355713}]}, {"text": "We benchmark our new dataset with state-ofthe-art and popular argument mining models to better understand the challenges posed in this new domain.", "labels": [], "entities": []}, {"text": "We observe a significant drop of performance for proposition segmentation on AMPERE, mainly due to its different argument structure.", "labels": [], "entities": [{"text": "proposition segmentation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.7917712032794952}, {"text": "AMPERE", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9033291339874268}]}, {"text": "For instance, 25% of the sentences contain more than one proposition, compared to that of 8% for essays (, motivating new solutions for segmentation and classification.", "labels": [], "entities": []}, {"text": "We further investigate review structure difference across venues based on proposition usage, and uncover several patterns.", "labels": [], "entities": []}, {"text": "For instance, ACL reviews tend to contain more propositions than those in ML venues, especially with more requests but fewer facts.", "labels": [], "entities": []}, {"text": "We further find that reviews with extreme ratings, i.e., strong reject or accept, tend to be shorter and make much fewer requests.", "labels": [], "entities": []}, {"text": "Moreover, we probe the salient words for different proposition types.", "labels": [], "entities": []}, {"text": "For example, ACL reviewers ask for more \"examples\" when making requests, while ICLR reviews contain more evaluation of \"network\" and how models are \"trained\".", "labels": [], "entities": []}], "datasetContent": [{"text": "We collect review data from three sources: (1) openreview.net-an online peer reviewing platform for ICLR 2017, ICLR 2018, and UAI 2018 2 ; (2) reviews released for accepted papers at NeurIPS from 2013 to 2017; and (3) opted-in reviews for ACL 2017 from.", "labels": [], "entities": [{"text": "ICLR", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.7785519361495972}]}, {"text": "In total, 14, 202 reviews are collected (ICLR: 4, 057; UAI: 718; ACL: 275; and NeurIPS: 9, 152).", "labels": [], "entities": [{"text": "ICLR", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9922879934310913}, {"text": "UAI", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8326702117919922}]}, {"text": "All venues except NeurIPS have paper rating scores attached to the reviews.", "labels": [], "entities": [{"text": "NeurIPS", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.939123272895813}]}, {"text": "For proposition segmentation, we adopt the concepts from and instruct the annotators to identify elementary argumentative discourse units on sentence or subsentence level, based on their discourse functions and topics.", "labels": [], "entities": [{"text": "proposition segmentation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8257351219654083}]}, {"text": "They then classify the propositions into five types with an additional non-argument category, as explained in 400 ICLR 2018 reviews are sampled for annotation, with similar distributions of length and rating to those of the full dataset.", "labels": [], "entities": [{"text": "ICLR 2018 reviews", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.8490057786305746}]}, {"text": "Two annotators who are fluent English speakers first label the 400 reviews with proposition segments and types, and a third annotator then resolves disagreements.", "labels": [], "entities": []}, {"text": "We calculate the inter-annotator agreement between the two annotators.", "labels": [], "entities": []}, {"text": "A Cohen's \u03ba of 0.93 is achieved for proposition segmentation, with each review treated as a BIO sequence.", "labels": [], "entities": [{"text": "proposition segmentation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.8306373357772827}]}, {"text": "For classification, unitized Krippendorf's \u03b1 U, which considers disagreements among segmentation, is calculated per review and then averaged overall samples, and the value is 0.61.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9739497900009155}, {"text": "unitized Krippendorf's \u03b1 U", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.6913771748542785}]}, {"text": "Among the exactly matched proposition segments, we report a Cohen's \u03ba of 0.64.", "labels": [], "entities": []}, {"text": "We benchmark AMPERE with popular and stateof-the-art models for proposition segmentation and classification.", "labels": [], "entities": [{"text": "proposition segmentation and classification", "start_pos": 64, "end_pos": 107, "type": "TASK", "confidence": 0.7082060426473618}]}, {"text": "Both tasks can be treated as sequence tagging problems with the setup similar to.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.6986401677131653}]}, {"text": "For experiments, 320 reviews (7, 999 propositions) are used for training and 80 reviews (2, 387 propositions) are used for testing.", "labels": [], "entities": []}, {"text": "Following, 5-fold cross validation on the training set is used for hyperparameter tuning.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.758937656879425}]}, {"text": "To improve the accuracy of tokenization, we manually replace mathematical formulas, variables, URL links, and formatted citation with special tokens such as <EQN>, <VAR>, <URL>, and <CIT>.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9980815649032593}, {"text": "VAR", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.9399970769882202}]}, {"text": "Parameters, lexicons, and features used for the models are described in the supplementary material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Proposition types and examples.", "labels": [], "entities": []}, {"text": " Table 2: Statistics for AMPERE and some argument  mining corpora, including # of annotated propositions.", "labels": [], "entities": [{"text": "AMPERE", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.5549147725105286}]}, {"text": " Table 3: Number of propositions per type in AMPERE.", "labels": [], "entities": [{"text": "AMPERE", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8549763560295105}]}, {"text": " Table 4: Proposition segmentation results. Result that  is significantly better than all comparisons is marked  with  *  (p < 10 \u22126 , McNemar test).", "labels": [], "entities": [{"text": "Proposition segmentation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8728078305721283}]}, {"text": " Table 5: Proposition classification F1 scores. Re- sults that are significant better than other methods are  marked with  *  (p < 10 \u22126 , McNemar test).", "labels": [], "entities": [{"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.966884970664978}, {"text": "Re- sults", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9171502788861593}]}, {"text": " Table 6: Salient words (\u03b1 = 0.001, \u03c7 2 test) per proposition type. Top 5 frequent words that are unique for each  venue are shown. \"<EQN>\", \"<URL>\", and \"<VAR>\" are equations, URL links, and variables.", "labels": [], "entities": [{"text": "VAR", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9867241978645325}]}]}