{"title": [{"text": "Analyzing the Perceived Severity of Cybersecurity Threats Reported on Social Media", "labels": [], "entities": [{"text": "Analyzing the Perceived Severity of Cybersecurity Threats Reported on Social Media", "start_pos": 0, "end_pos": 82, "type": "TASK", "confidence": 0.7737098959359255}]}], "abstractContent": [{"text": "Breaking cybersecurity events are shared across a range of websites, including security blogs (FireEye, Kaspersky, etc.), in addition to social media platforms such as Face-book and Twitter.", "labels": [], "entities": [{"text": "Face-book", "start_pos": 168, "end_pos": 177, "type": "DATASET", "confidence": 0.9566031694412231}]}, {"text": "In this paper, we investigate methods to analyze the severity of cyber-security threats based on the language that is used to describe them online.", "labels": [], "entities": []}, {"text": "A corpus of 6,000 tweets describing software vulnerabili-ties is annotated with authors' opinions toward their severity.", "labels": [], "entities": []}, {"text": "We show that our corpus supports the development of automatic classifiers with high precision for this task.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9950927495956421}]}, {"text": "Furthermore, we demonstrate the value of analyzing users' opinions about the severity of threats reported online as an early indicator of important software vulnerabilities.", "labels": [], "entities": []}, {"text": "We present a simple, yet effective method for linking software vulner-abilities reported in tweets to Common Vul-nerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD).", "labels": [], "entities": [{"text": "National Vulnerability Database (NVD)", "start_pos": 154, "end_pos": 191, "type": "DATASET", "confidence": 0.8575474818547567}]}, {"text": "Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume.", "labels": [], "entities": [{"text": "severity", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9894148111343384}, {"text": "Precision@50", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.9824878970781962}]}, {"text": "Finally we show how reports of severe vulnerabilities online are pre-dictive of real-world exploits.", "labels": [], "entities": []}], "introductionContent": [{"text": "Software vulnerabilities are flaws in computer systems that leave users open to attack; vulnerabilities are generally unknown at the time apiece of software is first published, but are gradually identified overtime.", "labels": [], "entities": []}, {"text": "As new vulnerabilities are discovered and verified they are assigned CVE numbers (unique identifiers), and entered into the National Vulnerability Database (NVD).", "labels": [], "entities": [{"text": "National Vulnerability Database (NVD)", "start_pos": 124, "end_pos": 161, "type": "DATASET", "confidence": 0.8838805754979452}]}, {"text": "To help prioritize Our code and data are available at https://github.com/viczong/ cybersecurity threat severity analysis.", "labels": [], "entities": []}, {"text": "2 https://nvd.nist.gov/ response efforts, vulnerabilities in the NVD are assigned severity scores using the Common Vulnerability and Scoring System (CVSS).", "labels": [], "entities": [{"text": "severity scores", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9639674127101898}]}, {"text": "As the rate of discovered vulnerabilities has increased in recent years, 3 the need for efficient identification and prioritization has become more crucial.", "labels": [], "entities": []}, {"text": "However, it is well known that a large time delay exists between the time a vulnerability is first publicly disclosed to when it is published in the NVD; a recent study found that the median delay between the time a vulnerability is first reported online and the time it is published in the NVD is seven days; also, 75% of threats are first disclosed online giving attackers time to exploit the vulnerability.", "labels": [], "entities": [{"text": "NVD", "start_pos": 149, "end_pos": 152, "type": "DATASET", "confidence": 0.9548745155334473}, {"text": "NVD", "start_pos": 291, "end_pos": 294, "type": "DATASET", "confidence": 0.9743033647537231}]}, {"text": "In this paper we present the first study of whether natural language processing techniques can be used to analyze users' opinions about the severity of software vulnerabilities reported online.", "labels": [], "entities": []}, {"text": "We present a corpus of 6,000 tweets annotated with opinions toward threat severity, and empirically demonstrate that this dataset supports automatic classification.", "labels": [], "entities": [{"text": "threat severity", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.6942354291677475}, {"text": "automatic classification", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.5718019902706146}]}, {"text": "Furthermore, we propose a simple, yet effective method for linking software vulnerabilities reported on Twitter to entries in the NVD, using CVEs found in linked URLs.", "labels": [], "entities": [{"text": "NVD", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.7008500695228577}]}, {"text": "We then use our threat severity analyzer to conduct a large-scale study to validate the accuracy of users' opinions online against experts' severity ratings (CVSS scores) found in the NVD.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9954296350479126}, {"text": "NVD", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9392067790031433}]}, {"text": "Finally, we show that our approach can provide an early indication of vulnerabilities that result in real exploits in the wild as measured by the existence of Symantec virus signatures associated with CVEs; we also show how our approach can be used to retrospectively identify Twitter accounts that provide reliable warnings about severe vulnerabilities.", "labels": [], "entities": []}, {"text": "Recently there has been increasing interest in developing NLP tools to identify cybersecurity events reported online, including denial of service attacks, data breaches and more (.", "labels": [], "entities": []}, {"text": "Our proposed approach in this paper builds on this line of work by evaluating users opinions toward the severity of cybersecurity threats.", "labels": [], "entities": []}, {"text": "Prior work has also explored forecasting software vulnerabilities that will be exploited in the wild (.", "labels": [], "entities": [{"text": "forecasting software vulnerabilities", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.866214374701182}]}, {"text": "Features included structured data sources (e.g., NVD), in addition to the volume of tweets mentioning a list of 31 keywords.", "labels": [], "entities": []}, {"text": "Rather than relying on a fixed set of keywords, we analyze message content to determine whether the author believes a vulnerability is severe.", "labels": [], "entities": []}, {"text": "As discussed by, methods that rely on tracking keywords and message volume are vulnerable to adversarial attacks from Twitter bots or sockpuppet accounts (.", "labels": [], "entities": []}, {"text": "In contrast, our method is somewhat less prone to such attacks; by extracting users' opinions expressed in individual tweets, we can track the provenance of information associated with our forecasts for display to an analyst, who can then determine whether or not they trust the source of information.", "labels": [], "entities": []}], "datasetContent": [{"text": "For threat existence classification, we randomly split our dataset of 6,000 tweets into a training set of 4,000 tweets, a development set of 1,000 tweets, and test set of 1,000 tweets.", "labels": [], "entities": [{"text": "threat existence classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.9326987465222677}]}, {"text": "For the threat severity classifier, we only used data from 2nd phase of annotation.", "labels": [], "entities": [{"text": "threat severity classifier", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.8332095543543497}]}, {"text": "This dataset consists of 1,966 tweets that were judged by the mechanical turk workers to describe a cybersecurity threat towards the target entity.", "labels": [], "entities": []}, {"text": "We randomly split this dataset into a training set of 1,200 tweets, a development set of 300 tweets, and a test set of 466 tweets.", "labels": [], "entities": []}, {"text": "We collapsed the three annotated labels into two categories based on whether or not the author expresses an opinion that the threat towards the target entity is severe.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of annotated tweets with break-down percentages to each category. In 1st annotation, a tweet  contains a threat if more than 3 workers vote for it. In 2nd annotation, a threat is severe if more than 6 workers  agree on it. Number of workers cut-offs are determined by comparing to our golden annotations in pilot studies.", "labels": [], "entities": []}, {"text": " Table 3: Top five threats extracted with highest confidence on Nov. 22, 2018. For each entity we aggregate tweets,  and average threat existence scores. The tweet with the maximum threat severity score is shown in each instance.", "labels": [], "entities": []}, {"text": " Table 4: High-weight n-gram features from logistic re- gression model for threat severity classification task.", "labels": [], "entities": [{"text": "threat severity classification task", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.864066481590271}]}, {"text": " Table 5: Performance of our threat existence and sever- ity classifiers. We show area under the precision-recall  curve (AUC) for both development and test sets.", "labels": [], "entities": [{"text": "precision-recall  curve (AUC)", "start_pos": 97, "end_pos": 126, "type": "METRIC", "confidence": 0.9772254467010498}]}, {"text": " Table 7: Model performance of identifying severe  threats (CVSS scores \u2265 7.0) with Precision@k and  area under the precision-recall curve (AUC) metrics.  For majority random baseline, we average over 10  trails.", "labels": [], "entities": [{"text": "Precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9986831545829773}, {"text": "precision-recall curve (AUC)", "start_pos": 116, "end_pos": 144, "type": "METRIC", "confidence": 0.966005265712738}]}, {"text": " Table 8: Top 4 threats identified by our forecast model. Severity scores are generated by using threat severity  classifier in Section 2.3.", "labels": [], "entities": []}, {"text": " Table 9: Model performance against real-world ex- ploited threats identified by Symantec and Exploit-DB.  \"True CVSS\" refers to ranking CVEs based on actual  CVSS scores in NVD. This model is only for reference  and can not be used in real practice, as we do not know  true CVSS scores when forecasting.", "labels": [], "entities": []}, {"text": " Table 10: List of users with top accuracies on forecast- ing severe cybersecurity threats.", "labels": [], "entities": []}, {"text": " Table 11: Some examples of forecast errors made by our model. (a) False negative examples: there is no clear  language clue for demonstrating the severity of threats, experts are needed for threats of this kind. (b) False positive  examples: there exist some signals captured by our model for being severe threats, but actual severity might be  overestimated.", "labels": [], "entities": []}, {"text": " Table 12: Top ranked log-odds ratio of subjective ad- jectives describing severe threats (CVSS scores \u2265 7.0)  versus non-severe threats (CVSS scores < 7.0). Sub- jective adjectives are identified by using Subjectivity  Lexicon (SUB) (", "labels": [], "entities": []}]}