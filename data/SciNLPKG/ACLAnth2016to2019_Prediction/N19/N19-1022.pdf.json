{"title": [{"text": "Recurrent Models and Lower Bounds for Projective Syntactic Decoding", "labels": [], "entities": [{"text": "Projective Syntactic Decoding", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6250245074431101}]}], "abstractContent": [{"text": "The current state-of-the-art in neural graph-based parsing uses only approximate decoding at the training phase.", "labels": [], "entities": [{"text": "neural graph-based parsing", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6740871469179789}]}, {"text": "In this paper aim to understand this result better.", "labels": [], "entities": []}, {"text": "We show how recurrent models can carryout projective maximum spanning tree decoding.", "labels": [], "entities": []}, {"text": "This result holds for both current state-of-the-art models for shift-reduce and graph-based parsers, projective or not.", "labels": [], "entities": []}, {"text": "We also provide the first proof on the lower bounds of projective maximum spanning tree, DAG, and digraph decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "For several years, the NLP field has seen widespread investigation into the application of Neural Networks to NLP tasks, and with this, much, rather inexplicable progress.", "labels": [], "entities": []}, {"text": "A string of very recent work (for example,;;), has attempted to delve into the formal properties of neural network topology choices, in attempts to both motivate, predict, and explain associated research in the field.", "labels": [], "entities": []}, {"text": "This paper aims to further contribute along this line of research.", "labels": [], "entities": []}, {"text": "We present the results of our study into the ability of state-of-the-art first-order neural graphbased parsers, with seemingly simple architectures, to explicitly forego structured learning and prediction.", "labels": [], "entities": []}, {"text": "In particular, this is not due to a significantly faster, simpler, algorithm for projective maximum spanning tree (MST) decoding than Eisner (1996)'s algorithm, which we formally prove to be impossible, given the Exponential Time Hypothesis.", "labels": [], "entities": [{"text": "projective maximum spanning tree (MST) decoding", "start_pos": 81, "end_pos": 128, "type": "TASK", "confidence": 0.6748910695314407}]}, {"text": "But rather, this is due to the capacity of recurrent components of these architectures to implicitly discover a projective MST.", "labels": [], "entities": []}, {"text": "We prove this formally by showing how these re-1 For the remainder of this paper, all decoding algorithms discussed are first-order.", "labels": [], "entities": []}, {"text": "current components can intrinsically simulate exact projective decoding.", "labels": [], "entities": []}, {"text": "The current state-of-the-art for graph-based syntactic dependency parsing is a seemingly basic neural model by.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.6271692713101705}]}, {"text": "The parser's performance is an improvement on the first, even simpler, rather engineering-free, neural graph-based parser by.", "labels": [], "entities": []}, {"text": "This latter parser updates with respect to an output structure: projective decoding over a matrix of arc scores coupled with hinge loss between predicted and gold arcs, reporting parser performance of, for example, 93.32% UAS and 91.2% LAS on the converted Penn Treebank.", "labels": [], "entities": [{"text": "UAS", "start_pos": 222, "end_pos": 225, "type": "METRIC", "confidence": 0.9926928281784058}, {"text": "LAS", "start_pos": 236, "end_pos": 239, "type": "METRIC", "confidence": 0.992154061794281}, {"text": "Penn Treebank", "start_pos": 257, "end_pos": 270, "type": "DATASET", "confidence": 0.9882470369338989}]}, {"text": "Remarkably, the former parser by forgoes entirely any structural learning, employing simple cross-entropy at training time, and saving (unconstrained) maximum spanning tree decoding for test time.", "labels": [], "entities": []}, {"text": "We further optimised's parser and extended it for cross-entropy learning, as is done by.", "labels": [], "entities": []}, {"text": "At test time, instead of any explicit decoding algorithm over the arc score matrix, we simply take the maximum weighted incoming arc for each word; that is, the parser is highly streamlined, without any heavy neural network engineering, but now also without any structured learning, nor without any structural decoding attest time.", "labels": [], "entities": []}, {"text": "The resulting neural parser still achieves an impressively competitive UAS of 92.61% evaluated on the converted Penn Treebank data, without recourse to any pre-trained embeddings, unlike the systems by and.", "labels": [], "entities": [{"text": "UAS", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9990053772926331}, {"text": "Penn Treebank data", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.9946873585383097}]}, {"text": "Using GloVe 100-dimensional Wikipedia and Gigaword corpus (6 billion tokens) pretrained embeddings, without updates, but linearly projected through a single linear dense layer to the same dimension, the structure-less parser achieves 93.18% UAS.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.90821972489357}, {"text": "UAS", "start_pos": 241, "end_pos": 244, "type": "METRIC", "confidence": 0.9717864990234375}]}, {"text": "With this paper, we shed light on these surprising results from seemingly simple architectures.", "labels": [], "entities": []}, {"text": "The insights we present here apply to any neural architecture that first encodes input words of a sentence using some type of recurrent neural network-i.e., all current state-ofthe-art graph-based or shift reduce neural parsers.", "labels": [], "entities": []}, {"text": "This paper presents results for understanding the surprisingly superior performance of structure-free learning and prediction in syntactic (tree) dependency parsing.", "labels": [], "entities": [{"text": "syntactic (tree) dependency parsing", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.6005938400824865}]}, {"text": "1. We provide a formal proof that there will never bean algorithm that carries out projective MST decoding in sub-cubic time, unless a widely believed assumption in computational complexity theory, the Exponential Time Hypothesis (ETH), is false.", "labels": [], "entities": [{"text": "MST decoding", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.9122631251811981}]}, {"text": "Hence, computationally, we provide convincing evidence that these neural parsing architectures cannot be as simple as they appear.", "labels": [], "entities": [{"text": "neural parsing", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7811551690101624}]}, {"text": "These results are then extended to projective maximum spanning DAG and digraph decoding.", "labels": [], "entities": []}, {"text": "2. In particular, we then show how to simulate Eisner's algorithm using a single recurrent neural network.", "labels": [], "entities": []}, {"text": "This shows how, in particular, the LSTM stacked architectures for graph-based parsing by,,,, and, are capable of intrinsically decoding over arc scores.", "labels": [], "entities": []}, {"text": "This therefore provides one practical application where RNNs do not need supplementary approximation considerations ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}