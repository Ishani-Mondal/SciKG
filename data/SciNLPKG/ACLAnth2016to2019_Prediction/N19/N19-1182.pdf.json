{"title": [{"text": "Glocal: Incorporating Global Information in Local Convolution for Keyphrase Extraction", "labels": [], "entities": [{"text": "Keyphrase Extraction", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.6964660286903381}]}], "abstractContent": [{"text": "Graph Convolutional Networks (GCNs) area class of spectral clustering techniques that leverage localized convolution filters to perform supervised classification directly on graphical structures.", "labels": [], "entities": []}, {"text": "While such methods model nodes' local pairwise importance, they lack the capability to model global importance relative to other nodes of the graph.", "labels": [], "entities": []}, {"text": "This causes such models to miss critical information in tasks where global ranking is a key component for the task, such as in keyphrase extraction.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7965983748435974}]}, {"text": "We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights.", "labels": [], "entities": []}, {"text": "In the context of keyphrase extraction, incorporating global random walk scores obtained from Tex-tRank boosts performance significantly.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.8953334987163544}, {"text": "Tex-tRank", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.9733066558837891}]}, {"text": "With our proposed method, we achieve state-of-the-art results, bettering a strong baseline by an absolute 2% increase in F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9890349507331848}]}], "introductionContent": [{"text": "Learning directly on a graphical structure is a crucial requirement in many domains.", "labels": [], "entities": []}, {"text": "These graphs represent information in many forms, ranging from interconnected user groups to contextually linked documents to a central document by shared vocabulary.", "labels": [], "entities": []}, {"text": "Learning on graphs has been studied extensively in the form of spectral clustering ().", "labels": [], "entities": []}, {"text": "The potential of learning directly on graphs has realized in semi-supervised settings where labels for only a few of the nodes are available.", "labels": [], "entities": []}, {"text": "Some prior work formulates such setup as propagating the label information using some form of graph-based regularization.", "labels": [], "entities": []}, {"text": "Recently proposed works have updated such methods to be end-to-end learnable in the deep learning style by employing gradient descent on nodes within a fixed neighborhood, approximating spectral clustering's means of approximating the graph's eigenvectors) by aggregating neighborhood features.", "labels": [], "entities": []}, {"text": "Recent advancements in normalizing the gradient range further improve the efficiency of such solutions.", "labels": [], "entities": []}, {"text": "However, these techniques can only exploit local features within the neighborhood of individual nodes.", "labels": [], "entities": []}, {"text": "For some tasks, such simplified local feature aggregation maybe sufficient, but insufficient for tasks that need global relative importance information.", "labels": [], "entities": []}, {"text": "One such important graph-based task is keyphrase extraction.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8511319160461426}]}, {"text": "In this task, individual words or phrases serve as graph nodes, and edges represent some form of co-occurrence.", "labels": [], "entities": []}, {"text": "Keyphrase extraction has been extensively studied, in both supervised (classification) and unsupervised (ranking) modes.", "labels": [], "entities": [{"text": "Keyphrase extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9279074966907501}]}, {"text": "Depending on the length of the text and the final application of the task, solutions can be sample-based classification, pairwise ranking or sequential labeling.", "labels": [], "entities": []}, {"text": "For example, explore the case of extracting top keyphrases from complete documents for downstream indexing, while connects its usage for knowledge base generation, aiming to extract all plausible keyphrases within a short excerpt.", "labels": [], "entities": [{"text": "knowledge base generation", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.6405818661053976}]}, {"text": "Treating a full-text scenario is arguably more challenging than the treatment of an excerpt scenario, as it requires the understanding of the much larger scale of text and extracting its most salient aspects.", "labels": [], "entities": []}, {"text": "Traditional supervised models employ a host of hand-engineered features -tf.idf , candidate length, POS tags, sectional information, frequency, among others ( -trained with a wide range of classifiers.", "labels": [], "entities": []}, {"text": "As they typically model the task as a binary classification task (i.e., keyphrase, \u00ackeyphrase), they suffer severely from class imbalance as keyphrases are the excep-tion among most plausible candidates.", "labels": [], "entities": []}, {"text": "Unsupervised methods use co-occurrence as a signal for the labels.", "labels": [], "entities": []}, {"text": "Under the hypothesis that keyphrase saliency is strongly correlated with repetition, graphical methods for unsupervised keyphrase extraction employ centrality measures and random walk techniques to rank prospective keyphrases ().", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.7184144705533981}]}, {"text": "This hypothesis is widely exploited, with proposed extensions further enriching the graph by incorporating topic, section and/or position information, among other forms of side information.", "labels": [], "entities": []}, {"text": "With these in mind, we make two important observations about the existing keyphrase extraction techniques: \u2022 In the supervised setting, word importance is captured in metrics and engineered features, as is local random walk scores.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7966318130493164}]}, {"text": "However, the structure of the graph formed by the text is not exploited.", "labels": [], "entities": []}, {"text": "\u2022 In the unsupervised setting, most techniques do not tightly incorporate the rich semantic features common in the supervised setting.", "labels": [], "entities": []}, {"text": "Furthermore, random walk scores are used as-is, without the capability of being finetuned by downstream supervision.", "labels": [], "entities": []}, {"text": "From this dichotomy, we see there is a gap to close in merging the advantages of both.", "labels": [], "entities": []}, {"text": "We propose a Glocal (global-local portmanteau) technique which incorporates both components directly over the word-graph.", "labels": [], "entities": [{"text": "Glocal", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.8340266346931458}]}, {"text": "Specifically, we contribute a neural model that elegantly incorporates the random walk scores, while incorporating parameters to fit keyphrase labels.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our model is the only supervised fulltext keyphrase extraction model that operates directly on the word-graph.", "labels": [], "entities": [{"text": "fulltext keyphrase extraction", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.6294041077295939}]}], "datasetContent": [{"text": "Total We investigate keyphrase extraction, using the most commonly reported full-text datasets, as shown in.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.869294285774231}]}, {"text": "We divide the training data in 80 : 20 fraction for train and validation splits.", "labels": [], "entities": []}, {"text": "Our complete pipeline comprises the following steps: 1.", "labels": [], "entities": []}, {"text": "First we perform TextRank on the complete text of each document, retaining only tokens that are nouns and adjectives, filtering out other words (equivalent to simplex noun phrases).", "labels": [], "entities": []}, {"text": "We use the gensim library to perform TextRank and compute the scores.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8988282680511475}]}, {"text": "This process helps in two ways -first, it gives us the node importance value for each keyphrase, needed by Glocal; second, it helps to manage the graph size and lessen the label skew on the minority positive label by removing extraneous tokens.", "labels": [], "entities": []}, {"text": "For documents larger than a max size (1200 tokens) we drop the extra least scored tokens.", "labels": [], "entities": []}, {"text": "We find that the tokens that have a TextRank score in bottom 50% possess only 16% of partial or full keyphrases in the validation dataset.", "labels": [], "entities": []}, {"text": "Hence dropping them from the processing does not affect the recall much.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9991687536239624}]}, {"text": "The nodes of the graph are single tokens and not complete phrases, therefore all the tokens of multitoken keyphrases are marked as keyphrase during learning ion the graph.", "labels": [], "entities": []}, {"text": "For the node/keyphrase representations, we map our vocabulary to GloVe embeddings using the 2.2M vocabulary sized, 300 dimension vector variant ().", "labels": [], "entities": []}, {"text": "For reference, we observe GloVe covers about 90% of the words overall 3 datasets.", "labels": [], "entities": [{"text": "words overall 3 datasets", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.6120742112398148}]}, {"text": "We then extract various textual features for the candidate keyphrases, including their position of the first occurrence, tf.idf and n-gram count.", "labels": [], "entities": []}, {"text": "These features are appended to the word embedding to obtain a final feature vector representing each node.", "labels": [], "entities": []}, {"text": "Rather than discard them, we choose to append the n-gram features to retain rich lexical signals obtained from the tokens.", "labels": [], "entities": []}, {"text": "The second step is to train the model with the formulated graphs.", "labels": [], "entities": []}, {"text": "We use a 2-layer network with 128 units with ReLU activations for hidden layers, followed by a simple 2-way softmax classification layer (keyphrase, \u00ackeyphrase).", "labels": [], "entities": []}, {"text": "We further employ 8 attention heads at all layers.", "labels": [], "entities": []}, {"text": "We follow Glorot initialization () for all setting initial parameters weights, use a dropout of 0.5, and employ a L2 regularization of 0.001.", "labels": [], "entities": []}, {"text": "We train with Adam optimizer on cross-entropy loss and initial learning rate of 0.01 for 200 epochs using an early stopping strategy with patience set to 20 epochs.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 55, "end_pos": 76, "type": "METRIC", "confidence": 0.7572249174118042}]}, {"text": "In both evaluation and training, as gold standard keyphrases have multiple tokens, we use each token of the gold keyphrase as the true label for each token.", "labels": [], "entities": []}, {"text": "This step reconstructs the multi-token keyphrase from the probability scores as generated by the Glocal model.", "labels": [], "entities": []}, {"text": "This formation step then requires a re-ranking (calculating R(p)) of the resultant phrase as:  The initial rank of each candidate token is in this case equal to the probability of the keyphrase, i.e., \u2200w i \u2208 p, r(w i ) = p keyphrase (w i ).", "labels": [], "entities": []}, {"text": "We also constrain the process such that the actual word sequence must appear in the original text.", "labels": [], "entities": []}, {"text": "An important note that we strictly do not normalize the \u03b2 i scores; the re-ranking process and the preservation of raw scores work in tandem.", "labels": [], "entities": []}, {"text": "Topologically, such graphs generated from textual data often have a few dense neighborhoods and many sparse ones, resulting in significant raw score differences that can benefit from scaling down the feature appropriately.", "labels": [], "entities": []}, {"text": "If normalization is done in each neighborhood (as done for \u03b1 i ), it will scale up individual nodes in a sparse neighborhood and suppress nodes in a dense neighborhood, the reverse of the intended operation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents in our keyphrase ex- traction datasets. '*' denotes that the dataset does not  have an official split; results are based on random splits.", "labels": [], "entities": []}, {"text": " Table 1. We divide the training data in  80 : 20 fraction for train and validation splits. Our  complete pipeline comprises the following steps:", "labels": [], "entities": [{"text": "validation", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.9431930184364319}]}, {"text": " Table 2: Main comparative system evaluations on keyphrase extraction. All figures are F 1 @K.  \u2020 uses only ab- stract, rest all models are trained on full-text,  *  shows significant improvement over strongest baseline CopyRNN.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7913072109222412}]}, {"text": " Table 3: Summary of the fine-grained F 1 @15 on Se- mEval. We compare with other state-of-the-art Se- mEval systems using text-only feature-based models.", "labels": [], "entities": [{"text": "fine-grained F 1", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.7821065584818522}]}]}