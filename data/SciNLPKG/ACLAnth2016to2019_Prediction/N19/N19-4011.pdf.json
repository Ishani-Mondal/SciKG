{"title": [{"text": "ChatEval: A Tool for Chatbot Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "Open-domain dialog systems (i.e., chatbots) are difficult to evaluate.", "labels": [], "entities": []}, {"text": "The current best practice for analyzing and comparing these dialog systems is the use of human judgments.", "labels": [], "entities": []}, {"text": "However , the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments.", "labels": [], "entities": []}, {"text": "We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems.", "labels": [], "entities": []}, {"text": "Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work.", "labels": [], "entities": []}, {"text": "The evaluation code is open-source to ensure standardization and transparency.", "labels": [], "entities": []}, {"text": "In addition, we introduce open-source baseline models and evaluation datasets.", "labels": [], "entities": []}, {"text": "ChatEval can be found at https://chateval.org.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reproducibility and model assessment for opendomain dialog systems is challenging, as many small variations in the training setup or evaluation technique can result in significant differences in perceived model performance.", "labels": [], "entities": []}, {"text": "While reproducibility is problematic for NLP in general, this is especially true for dialog systems due to the lack of automatic metrics.", "labels": [], "entities": []}, {"text": "In addition, as the field has grown, it has become increasingly fragmented inhuman evaluation methodologies.", "labels": [], "entities": []}, {"text": "Papers often focus on novel methods, but insufficient attention is paid to ensuring that datasets and evaluation remain consistent and reproducible.", "labels": [], "entities": []}, {"text": "For example, while human evaluation of chatbot quality is extremely common, few papers publish the set of prompts used for this evaluation, and almost no papers release their  learned model parameters.", "labels": [], "entities": []}, {"text": "Because of this, papers tend to evaluate their methodological improvement against a sequence-to-sequence (Seq2Seq) baseline) rather than against each other.", "labels": [], "entities": []}, {"text": "Seq2Seq was first proposed for dialog generation by in a system they called the Neural Conversational Model (NCM).", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9177155196666718}]}, {"text": "Due to the NCM being closed-source, nearly all papers compare against their own reimplementations of the model, which can vary widely in performance.", "labels": [], "entities": []}, {"text": "Indeed, we found no model, neither among those we trained nor those available online, that matched the performance of the original NCM, as evaluated by humans.", "labels": [], "entities": []}, {"text": "Another issue is that human evaluation experiments, which are currently the gold standard for model evaluation, are equally fragmented, with almost no two papers by different authors adopting the same evaluation dataset or experimental procedure.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7135758697986603}]}, {"text": "To address these concerns, we have built ChatEval, a scientific framework for evaluating chatbots.", "labels": [], "entities": []}, {"text": "ChatEval consists of two main components: (1) an open-source codebase for conducting automatic and human evaluation of chatbots in a standardized way, and (2) a web portal for accessing model code, trained parameters, and evaluation results, which grows with participation.", "labels": [], "entities": []}, {"text": "In addition, ChatEval includes newly created and curated evaluation datasets with both human annotated and automated baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose using the dataset collected by the dialogue breakdown detection (DBDC) task) as a standard benchmark.", "labels": [], "entities": [{"text": "dialogue breakdown detection (DBDC) task", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.869970977306366}]}, {"text": "The DBDC dataset was created by presenting participants with a short paragraph of context and then asking them to converse with three possible chatbots: TickTock (, Iris 8 , and Conversational Intelligence Challenge 9 . Participants knew that they were speaking with a chatbot, and the conversations reflect this.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8333521783351898}]}, {"text": "We randomly selected 200 human utterances from this dataset, after manually filtering out utterances which were too ambiguous or short to be easily answerable.", "labels": [], "entities": []}, {"text": "As the DBDC dataset does not contain any humanhuman dialog, we collected reference human responses to each utterance.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9760939180850983}]}, {"text": "For compatibility with prior work, we publish random subsets of 200 query-response pairs from the test sets of Twitter and OpenSubtitles.", "labels": [], "entities": []}, {"text": "We also make available the list of 200 prompts used as the evaluation set by in their analysis of the NCM's performance.", "labels": [], "entities": []}, {"text": "We believe that the DBDC dataset best represents the kind of conversations we would expect a user to have with a text-based conversational agent.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9270676076412201}]}, {"text": "The datasets used for chatbot evaluation ought to reflect the goal of the chatbot.", "labels": [], "entities": []}, {"text": "For example, it only makes sense to evaluate a model trained on Twitter using a test set derived from Twitter if the chatbot's aim is to be skilled at responding to Tweets.", "labels": [], "entities": []}, {"text": "With the DBDC dataset, we emphasize the goal of engaging in text-based interactions with users who know they are speaking with a chatbot.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9489103555679321}]}, {"text": "Overfitting One important feature of ChatEval is the ease of adding new evaluation datasets.", "labels": [], "entities": []}, {"text": "In order to assure that researchers are not overfitting to any evaluation set, the ChatEval team will take top performing models and also apply them to other datasets.", "labels": [], "entities": []}, {"text": "New evaluation datasets can be added upon request from the ChatEval team.", "labels": [], "entities": []}, {"text": "We plan to add both the prompts as well as the model responses from as well as.", "labels": [], "entities": []}, {"text": "Finally, we have added the ability to interact with baseline models using FlowAI.", "labels": [], "entities": []}, {"text": "The ChatEval evaluation toolkit is used to evaluate submitted models.", "labels": [], "entities": []}, {"text": "It consists of an automatic evaluation and a human evaluation component.", "labels": [], "entities": []}, {"text": "\u2022 Lexical diversity (distinct-n), the number of unique n-grams in the model's responses divided by the total number of generated tokens ().", "labels": [], "entities": []}, {"text": "\u2022 Average cosine-similarity between the mean of the word embeddings of a generated response and ground-truth response ().", "labels": [], "entities": []}, {"text": "\u2022 Sentence average BLEU-2 score ().", "labels": [], "entities": [{"text": "Sentence", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9478616714477539}, {"text": "BLEU-2 score", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9468884766101837}]}, {"text": "\u2022 Response perplexity, measured using the likelihood that the model predicts the correct response ().", "labels": [], "entities": []}, {"text": "11 Our system is easily extensible to support other evaluation metrics.", "labels": [], "entities": []}, {"text": "Human Evaluation A/B comparison tests consist of showing the evaluator a prompt and two possible responses from models which are being compared.", "labels": [], "entities": []}, {"text": "The prompt can consist of a single utterance or a series of utterances.", "labels": [], "entities": []}, {"text": "The user picks the better response or specifies a tie.", "labels": [], "entities": []}, {"text": "When both model responses are exactly the same, a tie is automatically recorded.", "labels": [], "entities": [{"text": "tie", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9125961065292358}]}, {"text": "The instructions seen by AMT workers are shown in.", "labels": [], "entities": [{"text": "AMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8724601864814758}]}, {"text": "The evaluation prompts are split into blocks (currently defaulted to 10).", "labels": [], "entities": []}, {"text": "Crowd workers are paid $0.01 per single evaluation.", "labels": [], "entities": []}, {"text": "We used three evaluators per prompt, so, if there are 200 prompt/response pairs, we have 600 ratings, and the net cost of the experiment is $7.20 after fees.", "labels": [], "entities": []}, {"text": "On the submission form, we ask researchers to pay for the cost of the AMT experiment.", "labels": [], "entities": [{"text": "AMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9476461410522461}]}, {"text": "The overall inter-annotator agreement (IAA) varies depending on the vagueness of the prompt as well as the similarity of the models.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 12, "end_pos": 43, "type": "METRIC", "confidence": 0.8838105440139771}]}, {"text": "Out of 18 different experiments run, we found that IAA, as measured by Cohen's weighted kappa, varies between .2 to .54 if we include tie choices.", "labels": [], "entities": [{"text": "IAA", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9985474944114685}]}, {"text": "The low IAA is similar to the findings of who also found low interannotator agreement.", "labels": [], "entities": [{"text": "IAA", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9953461289405823}]}, {"text": "Unfortunately, there are occasionally bad workers, which we automatically remove from our results.", "labels": [], "entities": []}, {"text": "In order to identify such workers, we examine the worker against the other annotators.", "labels": [], "entities": []}, {"text": "For analysis of relative performance between models in ChatEval, we use item response theory (IRT) to select prompts as well as test statistical significance.", "labels": [], "entities": [{"text": "item response theory (IRT", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.5019328534603119}]}, {"text": "IRT is the basis for almost all psychometric studies.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8640893697738647}]}, {"text": "We follow the work of, who used head-to-head pairwise testing to compare machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7149441242218018}]}, {"text": "However, we further this work by also examining the discriminative power of prompts.", "labels": [], "entities": []}, {"text": "For instance \"my name is david . what is my name ?\" from the NCM evaluation dataset has been shown to have low discriminative power, whereas, \"tell me something about your parents ?\" is useful to distinguish between the relative performance of models.", "labels": [], "entities": [{"text": "NCM evaluation dataset", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.953362782796224}]}], "tableCaptions": []}