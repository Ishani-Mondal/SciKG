{"title": [{"text": "Repurposing Entailment for Multi-Hop Question Answering Tasks", "labels": [], "entities": [{"text": "Repurposing Entailment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.893994003534317}, {"text": "Multi-Hop Question Answering Tasks", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7102668955922127}]}], "abstractContent": [{"text": "Question Answering (QA) naturally reduces to an entailment problem, namely, verifying whether some text entails the answer to a question.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8992843151092529}]}, {"text": "However, for multi-hop QA tasks, which require reasoning with multiple sentences, it remains unclear how best to utilize entailment models pre-trained on large scale datasets such as SNLI, which are based on sentence pairs.", "labels": [], "entities": []}, {"text": "We introduce Multee, a general architecture that can effectively use entailment models for multi-hop QA tasks.", "labels": [], "entities": []}, {"text": "Multee uses (i) a local module that helps locate important sentences, thereby avoiding distracting information, and (ii) a global module that aggregates information by effectively incorporating importance weights.", "labels": [], "entities": [{"text": "Multee", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8911879658699036}]}, {"text": "Importantly, we show that both modules can use entailment functions pre-trained on a large scale NLI datasets.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.7834368646144867}]}, {"text": "We evaluate performance on MultiRC and OpenBookQA, two multihop QA datasets.", "labels": [], "entities": [{"text": "MultiRC", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9067415595054626}, {"text": "OpenBookQA", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.8982660174369812}]}, {"text": "When using an entailment function pre-trained on NLI datasets, Multee outperforms QA models trained only on the target QA datasets and the OpenAI transformer models.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8642181158065796}]}], "introductionContent": [{"text": "How can we effectively use textual entailment models for question answering?", "labels": [], "entities": [{"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8462178707122803}]}, {"text": "Previous attempts at this have resulted in limited success (;).", "labels": [], "entities": []}, {"text": "With recent large scale entailment datasets) pushing entailment models to high accuracies (, we re-visit this challenge and propose a novel method for repurposing neural entailment models for QA.", "labels": [], "entities": []}, {"text": "A key difficulty in using entailment models for QA turns out to be the mismatch between the inputs to the two tasks: large-scale entailment datasets are typically framed at a sentence level, whereas question answering requires verifying whether multiple sentences, taken together as a premise, entail a hypothesis.", "labels": [], "entities": [{"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.7327164709568024}]}, {"text": "There are two straightforward ways to address this mismatch: (1) aggregate independent entailment decisions over each premise sentence, or (2) make a single entailment decision after concatenating all premise sentences.", "labels": [], "entities": []}, {"text": "Neither approach is fully satisfactory.", "labels": [], "entities": []}, {"text": "To understand why, consider the set of premises in, which entail the hypothesis H c . Specifically, the combined information in P 1 and P 3 entails H c , which corresponds to the correct answer Cambridge.", "labels": [], "entities": []}, {"text": "On one hand, aggregating independent decisions will fail because no individual premise entails H C . On the other hand, simply concatenating premises to form a single paragraph will fail because distracting information in P 2 and P 4 can muddle useful information in P 1 and P 3.", "labels": [], "entities": []}, {"text": "An effective approach, therefore, must recognize relevant sentences (i.e., avoid distracting ones) and compose their sentence-level information.", "labels": [], "entities": []}, {"text": "Our solution to this challenge is based on the observation that a sentence-level entailment function can be re-purposed for both recognizing relevant sentences, and for computing sentence-level representations.", "labels": [], "entities": []}, {"text": "Both tasks require comparing in-formation in a pair of texts, but the objectives of the comparison are different.", "labels": [], "entities": []}, {"text": "This means we can take an entailment function that is trained for basic entailment (i.e., comparing information in texts), and adapt it to work for both recognizing relevance and computing representations.", "labels": [], "entities": []}, {"text": "Thus, this architecture allows us to incorporate advances in entailment architectures and to leverage pretrained models obtained using large scale entailment datasets.", "labels": [], "entities": []}, {"text": "To this end, we propose a general architecture that uses a (pre-trained) entailment function f e for multi-sentence QA.", "labels": [], "entities": []}, {"text": "Given a hypothesis statement H qa representing a candidate answer, and the set of premise sentences {P i }, our proposed architecture uses the same function f e for two components: (a) a sentence relevance module that scores each P i based on its potential relevance to H qa , with the goal of weeding out distractors; and (b) a relevance-weighted aggregator that combines entailment information from multiple P i . Thus, we build effective entailment aware representations of larger contexts (i.e., multiple sentences) from those of small contexts (i.e., individual sentences).", "labels": [], "entities": []}, {"text": "The main strength of our approach is that, unlike standard attention mechanisms, the aggregator module uses the attention scores from the relevance module at multiple levels of abstractions (e.g., multiple layers of a neural network) within f e , using join operations that compose representations at each level.", "labels": [], "entities": []}, {"text": "We refer to this multilevel aggregation of textual entailment representations as Multee (pronounced multi).", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: We evaluate Multee on two datasets, OpenBookQA ( and MultiRC (, both of which are specifically designed to test reasoning over multiple sentences.", "labels": [], "entities": [{"text": "OpenBookQA", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9151028990745544}]}, {"text": "MultiRC is paragraph-based multiple-choice QA dataset derived from varying topics where the questions are answerable based on information from the paragraph.", "labels": [], "entities": []}, {"text": "In MultiRC, each question can have more than one correct answer choice, and so it can be viewed as a binary classification task (one prediction per answer choice), with 4,848 / 4,583 examples in Dev/Test sets.", "labels": [], "entities": []}, {"text": "OpenBookQA, on the other hand, has multiple-choice science questions with exactly one correct answer choice and no associated paragraph.", "labels": [], "entities": [{"text": "OpenBookQA", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9726181626319885}]}, {"text": "As a result, this dataset requires the relevant facts to be retrieved from auxiliary resources including the open book of facts released with the paper and other sources such as WordNet and ConceptNet Preprocessing: For each question and answer choice, we create an answer hypothesis statement using a modified version of the script used in SciTail (  construction.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.9792860150337219}]}, {"text": "We wrote a handful of rules to better convert the question and answer to a hypothesis.", "labels": [], "entities": []}, {"text": "We also mark the span of answer in the hypothesis with special begin and end tokens, @@@answer and answer@@@ respectively 3 . For MultiRC, we also apply an offthe-shelf coreference resolution model 4 and replace the mentions when they resolve to pronouns occurring in a different sentence . For OpenBookQA, we use the exact same retrieval as released by the authors of OpenBookQA 6 and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.", "labels": [], "entities": [{"text": "OpenBook", "start_pos": 394, "end_pos": 402, "type": "DATASET", "confidence": 0.9673395752906799}, {"text": "WordNet", "start_pos": 407, "end_pos": 414, "type": "DATASET", "confidence": 0.542652428150177}]}], "tableCaptions": [{"text": " Table 1: Comparison of  Multee with other systems. Starred (*) results are based on contemporaneous system.  Results marked (-) are not available. RS is Reading Strategies, KER is Knowledge Enhanced Reader, OFT is  OpenAI FineTuned Transformer.", "labels": [], "entities": [{"text": "Multee", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8461688160896301}, {"text": "RS", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.9530505537986755}, {"text": "KER", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9254291653633118}, {"text": "OFT", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.5656316876411438}, {"text": "OpenAI FineTuned Transformer", "start_pos": 216, "end_pos": 244, "type": "DATASET", "confidence": 0.8610668778419495}]}, {"text": " Table 2: Relevance Model Ablation of  Multee.  \u03b1 i : without relevance weights, \u03b1 i : with relevance  weights respectively, \u03b1 i + supervise: with supervised  relevance weights. Test results on OpenBookQA and  Dev results on MultiRC.", "labels": [], "entities": [{"text": "OpenBookQA", "start_pos": 194, "end_pos": 204, "type": "DATASET", "confidence": 0.9547334313392639}, {"text": "MultiRC", "start_pos": 225, "end_pos": 232, "type": "DATASET", "confidence": 0.9271573424339294}]}, {"text": " Table 3: Aggregator Level Ablation of  Multee. On  MultiRC, Multee uses relevance supervision but not  on OpenBookQA because of unavailibility. Test results  on OpenBookQA and Dev results on MultiRC.", "labels": [], "entities": [{"text": "Aggregator Level Ablation of", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7832910865545273}, {"text": "Multee", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.5732537508010864}, {"text": "OpenBookQA", "start_pos": 162, "end_pos": 172, "type": "DATASET", "confidence": 0.9573580622673035}]}, {"text": " Table 4: Effect (on test data) of pre-training the entail- ment model used in Multee.", "labels": [], "entities": [{"text": "Multee", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.7551030516624451}]}, {"text": " Table 5: Pre-training ablations of black-box entailment  baselines for OpenBookQA (test) and MultiRC (dev).", "labels": [], "entities": []}]}