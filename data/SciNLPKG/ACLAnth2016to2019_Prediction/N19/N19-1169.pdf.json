{"title": [{"text": "Unifying Human and Statistical Evaluation for Natural Language Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "How can we measure whether a natural language generation system produces both high quality and diverse outputs?", "labels": [], "entities": []}, {"text": "Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set.", "labels": [], "entities": []}, {"text": "On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human-or machine-generated.", "labels": [], "entities": []}, {"text": "We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE.", "labels": [], "entities": [{"text": "error rate", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9362343549728394}, {"text": "HUSE", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.6790250539779663}]}, {"text": "On summarization and chitchat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity.", "labels": [], "entities": [{"text": "summarization", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9706057906150818}]}], "introductionContent": [{"text": "Generating text is a core part of many NLP tasks such as image captioning (), opendomain dialogue ( , story generation, and summarization (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7066411674022675}, {"text": "story generation", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7664052546024323}, {"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9889559149742126}]}, {"text": "However, proper evaluation of natural language generation has proven difficult (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6990252137184143}]}, {"text": "A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, open-ended tasks like dialogue or story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7125129103660583}]}, {"text": "Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity.", "labels": [], "entities": []}, {"text": "As an example, for language modeling, a model that directly plagiarizes sentences from the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7031721621751785}]}, {"text": "On the other hand, statistical evaluation-i.e., perplexity on a reference test set-captures diversity, as it ensures a model must assign reasonable probability to novel sentences, but perplexity provides an inadequate measure of quality (.", "labels": [], "entities": []}, {"text": "For example, modifying a perfect model by removing its ability to generate even a single test sentence results in infinite perplexity even though the model is still near-perfect.", "labels": [], "entities": []}, {"text": "Automatic metrics such as BLEU () and ROUGE () capture quality better than perplexity but still correlate poorly with human evaluation and fail to capture diversity (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9985431432723999}, {"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9940112233161926}]}, {"text": "Existing approaches to combining statistical and human evaluation have been ad-hoc, leading to misleading performance measures.", "labels": [], "entities": []}, {"text": "A common approach is to measure diversity through the perplexity of a probabilistic model and quality through human evaluation on beam-searched out-puts.", "labels": [], "entities": []}, {"text": "This gives the illusion that a single model is high-quality and diverse, while the reality is that it shows we can have either a diverse model (when sampling from the distribution used to compute perplexity) or a high-quality model (when beamsearching).", "labels": [], "entities": []}, {"text": "In this paper, we define the idealized evaluation metric as twice the error of the optimal discriminator for classifying sentences as coming from the reference distribution or the model (Section 2).", "labels": [], "entities": []}, {"text": "If a model generates gibberish (low quality), the optimal discriminator can classify these accurately as coming from the model.", "labels": [], "entities": []}, {"text": "If the reference distribution contains sentences the model cannot generate (low diversity), the optimal discriminator can classify these accurately as coming from the reference.", "labels": [], "entities": []}, {"text": "Unfortunately, the optimal discriminator is unavailable.", "labels": [], "entities": []}, {"text": "Human discriminators cannot capture diversity effectively, and learned discriminatorse.g., from a Generative Adversarial Network () or one trained on human judgments (-are too unreliable to use for rigorous evaluation.", "labels": [], "entities": []}, {"text": "Our key result (Section 3) is based on the observation that the optimal classifier depends only on two numbers: the probability of a sentence under the model and the probability under the reference distribution.", "labels": [], "entities": []}, {"text": "The former can be computed directly from the model, and we show that the latter can be well-approximated by human judgment scores.", "labels": [], "entities": []}, {"text": "The resulting two-dimensional space is illustrated in.", "labels": [], "entities": []}, {"text": "We apply a simple k-nearest neighbor classifier in this space and define Human Unified with Statistical Evaluation (HUSE) as twice the leave-one-out error of this classifier.", "labels": [], "entities": [{"text": "Statistical Evaluation (HUSE)", "start_pos": 92, "end_pos": 121, "type": "METRIC", "confidence": 0.7617382347583771}]}, {"text": "We apply HUSE to four natural language generation tasks (Section 5): language modeling, chitchat dialogue, story generation, and summarization.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7195310195287069}, {"text": "language modeling", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7409465312957764}, {"text": "story generation", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.7878162562847137}, {"text": "summarization", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.9871189594268799}]}, {"text": "First, we show that human evaluation alone is insufficient to discriminate model generations from the references, leading to inflated estimates of model performance.", "labels": [], "entities": []}, {"text": "In contrast, HUSE is able to reveal deficiencies of current models.", "labels": [], "entities": [{"text": "HUSE", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.6203933954238892}]}, {"text": "We also show that common techniques for improving sample quality such as annealing actually increase distinguishability between the model and reference due to losses in diversity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use HUSE to evaluate three different types of single-sentence natural language generation tasks: (i) unconditional and high entropy (language modeling); (ii) conditional and high entropy (story generation, chit-chat dialogue); and (iii) conditional and low entropy (summarization).", "labels": [], "entities": [{"text": "single-sentence natural language generation", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.709242433309555}, {"text": "story generation", "start_pos": 191, "end_pos": 207, "type": "TASK", "confidence": 0.7351548075675964}]}, {"text": "We show that HUSE provides a direct and interpretable measure of diversity on high-entropy tasks, while also serving as a useful model diagnostic on lowentropy ones.", "labels": [], "entities": []}, {"text": "The four tasks along with the datasets and models are as follows: \u2022 Summarization: Giganews story to headline dataset and the pre-trained model from.", "labels": [], "entities": [{"text": "headline dataset", "start_pos": 101, "end_pos": 117, "type": "DATASET", "confidence": 0.7025991827249527}]}, {"text": "The dataset consists of 3.8 million news story-headline pairs.", "labels": [], "entities": []}, {"text": "Examples from this dataset are shown in Table 2.", "labels": [], "entities": []}, {"text": "\u2022  Annealing can trade-off between diversity and quality but cannot easily increase the underlying model performance (HUSE).", "labels": [], "entities": [{"text": "HUSE)", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9463220238685608}]}, {"text": "\u2022 Language modeling: One billion word benchmark pre-trained language model from . The task consists of generating a single sentence from the one billion word newswire text distribution.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7147074490785599}]}, {"text": "\u2022 Chit-chat dialogue: Two-turn chit-chat dialogue dataset consisting of 37.3 million comment-response pairs from Reddit (Appendix A.4).", "labels": [], "entities": []}, {"text": "Comments are generally short (5-15 tokens) and cover a single topic (e.g. given \"wow how did i not notice that\", the response is \"you were focusing on other things its understandable\").", "labels": [], "entities": []}, {"text": "We train a convolutional model using fairseq).", "labels": [], "entities": []}, {"text": "For all the tasks, we train neural models and evaluate their diversity-quality tradeoffs as we change the decoding scheme for generation.", "labels": [], "entities": []}, {"text": "Our primary evaluation concerns diversity trade-offs involving temperature annealing which is a generation technique applicable to any probabilistic model that generates words sequentially.", "labels": [], "entities": []}, {"text": "In temperature annealed models, we sample a word w proportional top 1/t (w) where p is the model probability of w given previous words and t is the temperature parameter.", "labels": [], "entities": []}, {"text": "We excluded beam search since it qualitatively behaves similarly to temperature annealing with low temperatures and HUSE \u2248 0 due to beam search being extremely Score Summarization Story generation Chit-chat dialogue LM t = 1.0 t = 0.7 t = 1.0 Retrieval t = 1.0 t = 0.7 t = 1.0   under diverse.", "labels": [], "entities": [{"text": "beam search", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8014297187328339}, {"text": "HUSE", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.99326092004776}, {"text": "Summarization Story generation Chit-chat dialogue LM", "start_pos": 166, "end_pos": 218, "type": "TASK", "confidence": 0.7761428505182266}]}, {"text": "As a non-neural baseline, we also consider retrieval based models based on Apache solr on a few tasks.", "labels": [], "entities": []}, {"text": "For this approach, we retrieve the single most relevant response from the training set using the BM25 similarity metric on inputs.", "labels": [], "entities": [{"text": "BM25 similarity metric", "start_pos": 97, "end_pos": 119, "type": "METRIC", "confidence": 0.7178943951924642}]}, {"text": "Such models are known to perform well in tasks with complex outputs such as program generation) and style transfer ( . For cost reasons, we did not measure certain combinations of task and generation mechanisms.", "labels": [], "entities": [{"text": "program generation", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7054529041051865}, {"text": "style transfer", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7189512103796005}]}, {"text": "We did not measure retrieval for chit-chat dialogue, as we observed its outputs were lower quality than a low-temperature neural model.", "labels": [], "entities": []}, {"text": "We also did not anneal language models, as the generation quality from the language model was already high, and our goal was to show that they achieved high HUSE.", "labels": [], "entities": [{"text": "HUSE", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9946234226226807}]}, {"text": "Our set of measurements, while not comprehensive, generally covers the available qualitydiversity tradeoffs for conditional tasks.", "labels": [], "entities": []}, {"text": "Finally, we collect human judgments HJ(x, y) as per Section 4.1 where we query 20 Amazon Mechanical Turk crowdworkers for typicality ratings on 100 reference and 100 model sentences.", "labels": [], "entities": []}, {"text": "Since our models generate UNK (unknown and out-of-vocabulary) tokens, we instructed crowdworkers to treat UNK tokens as rare, but appropriate words for the context.", "labels": [], "entities": []}, {"text": "We use a subset of Reddit comments from 2006-2018 scraped from https://pushshift.io/.", "labels": [], "entities": []}, {"text": "We construct a dictionary containing the 10,000 most popular words and preprocess the dataset by removing deleted posts, out-of-vocabulary tokens, profanity, comments with less than 10 upvotes, and comments with over 400 tokens.", "labels": [], "entities": []}], "tableCaptions": []}