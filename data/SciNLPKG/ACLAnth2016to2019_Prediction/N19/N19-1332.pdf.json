{"title": [{"text": "Relation Discovery with Out-of-Relation Knowledge Base as Supervision", "labels": [], "entities": [{"text": "Relation Discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9858396053314209}]}], "abstractContent": [{"text": "Unsupervised relation discovery aims to discover new relations from a given text corpus without annotated data.", "labels": [], "entities": [{"text": "Unsupervised relation discovery", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6438571015993754}]}, {"text": "However, it does not consider existing human annotated knowledge bases even when they are relevant to the relations to be discovered.", "labels": [], "entities": []}, {"text": "In this paper , we study the problem of how to use out-of-relation knowledge bases to supervise the discovery of unseen relations, where out-of-relation means that relations to discover from the text corpus and those in knowledge bases are not overlapped.", "labels": [], "entities": []}, {"text": "We construct a set of constraints between entity pairs based on the knowledge base embedding and then incorporate constraints into the relation discovery by a variational auto-encoder based algorithm.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7147233486175537}]}, {"text": "Experiments show that our new approach can improve the state-of-the-art relation discovery performance by a large margin.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8280623257160187}]}], "introductionContent": [{"text": "Relation extraction has been widely used for many applications, such as knowledge graph construction (), information retrieval (, and question answering).", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9746994376182556}, {"text": "knowledge graph construction", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6820345520973206}, {"text": "information retrieval", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.8790260553359985}, {"text": "question answering", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.8993447422981262}]}, {"text": "Traditional supervised approaches require direct annotation on sentences with a relatively small number of relations (.", "labels": [], "entities": []}, {"text": "With the development of large-scale knowledge bases (KBs) such as Freebase (, relation extraction has been extended to larger scales comparable to KBs using the distant supervision (.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9824644923210144}, {"text": "relation extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.9268471598625183}]}, {"text": "However, when the training corpus does not support the annotated relations showing in the KB, such approach could fail to find sufficient training examples.", "labels": [], "entities": []}, {"text": "Distant supervision assumption can be violated by up to 31% for some relations when aligning to NYT corpus (.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9380112290382385}]}, {"text": "More importantly, either traditional supervised learning or distantly supervised learning cannot discover new relations unseen in the training phase.", "labels": [], "entities": []}, {"text": "Unsupervised relation discovery tries to overcome the shortcomings of supervised or distantly supervised learning approaches.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8716112077236176}]}, {"text": "Existing approaches either extract surface or syntactic patterns from sentences and use relation expressions as predicates (which result in many noisy relations) (, or cluster the relation expressions based on the extracted triplets to form relation clusters.", "labels": [], "entities": []}, {"text": "However, these approaches do not use existing highquality and large-scale KBs when they are relevant to the relations to be discovered.", "labels": [], "entities": []}, {"text": "In this paper, we consider anew relation discovery problem where both the training corpus for relation clustering and a KB are available, but the relations in the training corpus and those in the KB are not overlapped.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8773768544197083}, {"text": "relation clustering", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.74708491563797}]}, {"text": "As shown in, in the KB, we have entities Pink Floyd, Animals, etc., with some existing relations notable work and has member in the KB.", "labels": [], "entities": [{"text": "KB", "start_pos": 132, "end_pos": 134, "type": "DATASET", "confidence": 0.92657470703125}]}, {"text": "However, when doing relation discovery, we can only get supporting sentences that suggest new relations based on and influenced by.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.913959264755249}]}, {"text": "This is a common and practical problem since predicates in KBs are limited to the annotator defined relations while the real relations in the world are always open and creative.", "labels": [], "entities": []}, {"text": "It is challenging when there is no overlapped relation between target relation clusters and the KB because in this case the KB is not a direct supervision.", "labels": [], "entities": []}, {"text": "But if target relation clusters and the KB share some entities, we can use the shared entities as abridge to introduce indirect supervision for the relation discovery problem.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8392690420150757}]}, {"text": "Specifically, we build constraints between pairs of tuples based on the KB.", "labels": [], "entities": []}, {"text": "For example, in, when we cluster the based on relation, we can evaluate the similarity between the tuple (Animals, Animal Farm) and the tuple (Amused to Death, Amusing Ourselves to Death) based on the KB.", "labels": [], "entities": []}, {"text": "If the KB tells us these two pairs of tuples are close to each other, then we put a constraint to force our relation clustering algorithm to group them together.", "labels": [], "entities": []}, {"text": "We use the discrete-state variational autoencoder (DVAE) framework as our base relation discovery model since this framework is flexible to incorporate different features and currently the state-of-the-art.", "labels": [], "entities": []}, {"text": "We use KB embedding () to obtain entity embeddings in the KB and use entity embeddings to evaluate the similarity between a pair of tuples.", "labels": [], "entities": []}, {"text": "Then constraints are constructed and incorporated into the DVAE framework in away inspired by the must-link and cannot-link based constrained clustering ().", "labels": [], "entities": []}, {"text": "We show that with no overlapped relations between the KB and the training corpus, we can improve the relation discovery by a large margin.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7855482399463654}]}, {"text": "Our contributions are summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We study anew prevalent but challenging task of relation discovery where the training corpus and the KB have no overlapped relation.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8943086862564087}]}, {"text": "\u2022 We propose anew kind of indirect supervision to relation discovery which is built based on pairwise constraints between two tuples.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.884148120880127}]}, {"text": "\u2022 We show promising results using existing relation discovery datasets to demonstrate the effectiveness of our proposed learning algorithm for the new relation discovery task.", "labels": [], "entities": [{"text": "relation discovery task", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.849876324335734}]}, {"text": "The code we used to train and evaluate our models is available at https://github.com/ HKUST-KnowComp/RE-RegDVAE.", "labels": [], "entities": [{"text": "HKUST-KnowComp", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9009525775909424}, {"text": "RE-RegDVAE", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.7442747950553894}]}], "datasetContent": [{"text": "In this section, we show the experimental results.", "labels": [], "entities": []}, {"text": "We evaluate our model in the context of unsupervised relation discovery and compare to the baseline model, DVAE) which is the current state-of-the-art of relation discovery.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7387112379074097}, {"text": "relation discovery", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8855980038642883}]}, {"text": "Distant supervision assumes that the relations should be aligned between the KB and the training text corpus, which is not available in our setting.", "labels": [], "entities": []}, {"text": "We tested our model on three different subsets of New York Times corpus (NYT)  \u2022 The first one is widely used in unsupervised settings, which was developed by Yao et al.", "labels": [], "entities": [{"text": "New York Times corpus (NYT)", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.7861829485212054}]}, {"text": "\u2022 The second and third ones are usually applied by supervised models.", "labels": [], "entities": []}, {"text": "So when they generated the data, they tended to focus on relations with more supporting sentences.", "labels": [], "entities": []}, {"text": "The second one was developed by.", "labels": [], "entities": []}, {"text": "The data is built by aligning Wikidata (Vrande\u010di\u00b4Vrande\u010di\u00b4c, 2012) relations with NYT corpus, as a result of 99 possible relations.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9678671061992645}]}, {"text": "It is built to contain more updated facts and richer structures of relations, e.g., a larger number of relation/relation paths.", "labels": [], "entities": []}, {"text": "We use this dataset to amplify the effects coming from relation paths in KB, as the data was used to train a path-based relation extraction model.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7387813627719879}]}, {"text": "\u2022 The third one was developed by and has also been used by.", "labels": [], "entities": []}, {"text": "This dataset was generated by aligning Freebase () relations with NYT in, and with 52 possible relations.", "labels": [], "entities": [{"text": "NYT", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9747374653816223}]}, {"text": "We use this data to test the clustering result with a narrow relation domain.", "labels": [], "entities": []}, {"text": "We align these datasets against FB15K, which is a randomly sampled subset of Freebase developed by.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9207630157470703}, {"text": "Freebase", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9315244555473328}]}, {"text": "For each of the datasets above, we holdout the triplets in FB15K that contains relations in corresponding text data, so that we ensure that KB cannot give any direct supervision on any relation labels.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9854560494422913}]}, {"text": "We then discard named  As the scoring function, we use the B 3 F 1 () which has also been used by our baseline, and Normalized Mutual Information (NMI) () metrics.", "labels": [], "entities": [{"text": "B 3 F 1", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9669352173805237}]}, {"text": "Both are standard measures for evaluating clustering tasks.", "labels": [], "entities": [{"text": "clustering tasks", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8728945553302765}]}, {"text": "We first report our results on NYT122 using different regularization and prediction settings, as this dataset was used by our baseline model DVAE.", "labels": [], "entities": [{"text": "NYT122", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9562883377075195}, {"text": "DVAE", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.8598595857620239}]}, {"text": "Note that both encoder and decoder components can make relation predictions.", "labels": [], "entities": []}, {"text": "In fact, the way of using encoder q(r|x, \u03c8) for each sentence is straightforward.", "labels": [], "entities": []}, {"text": "Then based on the encoder, we predict relation on the basis of single occurrence of entity pair.", "labels": [], "entities": []}, {"text": "When using the decoder, we need to re-normalize p(e i |r, e \u2212i , \u03b8) as p(r|e 1 , e 2 , \u03b8) to make predictions.", "labels": [], "entities": []}, {"text": "Based on the decoder, we make predictions for each unique entity pair.", "labels": [], "entities": []}, {"text": "As a consequence, our constraints can be imposed on both encoder and decoder.", "labels": [], "entities": []}, {"text": "The way of computing decoder probability distribution is the same as making predictions.", "labels": [], "entities": []}, {"text": "So in this experiment, we report both results.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "From the table, we can see that regularization with Euclidean distance performs the best compared to KL and JS.", "labels": [], "entities": [{"text": "regularization", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9676414728164673}, {"text": "JS", "start_pos": 108, "end_pos": 110, "type": "DATASET", "confidence": 0.8195537328720093}]}, {"text": "Moreover, the regularization over encoder is better than the regularization over decoder.", "labels": [], "entities": []}, {"text": "This maybe because the way that we put constraints only over sampled sentences in a batch may hurt the regularization of decoder, since sampled unique pairs maybe less than sample sentences.", "labels": [], "entities": []}, {"text": "If we look at results comparing original DVAE prediction based on the encoder and the decoder, both result in similar F1 and NMI numbers.", "labels": [], "entities": [{"text": "DVAE prediction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6997178196907043}, {"text": "F1", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.998974084854126}]}, {"text": "Thus, we can only conclude that currently in the way we do sampling, constraining over encoder is a better choice.", "labels": [], "entities": []}, {"text": "We also compare our algorithm on the three datasets with different baseline settings.", "labels": [], "entities": []}, {"text": "In order to evaluate our model rigorously, besides the original DVAE model, we compare two additional augmented baseline models with the same hyper-parameter setting: DVAE with TransE embeddings appended to encoder input features (DVAE+E) and DVAE    with decoder entity vectors replaced by pre-trained KB embeddings (DVAE+D).", "labels": [], "entities": []}, {"text": "For our method, we report RegDVAE with the best setting where we use Euclidean distance based constraints to regularize the encoder.", "labels": [], "entities": [{"text": "RegDVAE", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.8985593914985657}]}, {"text": "Moreover, we report a setting with fixed embeddings in the decoder as the ones obtained from TransE (RegDVAE+D).", "labels": [], "entities": []}, {"text": "This also makes sense since even though the TransE embeddings are not trained with the observation of the same relations as the text corpus, the embeddings already contain much semantic information about entities.", "labels": [], "entities": []}, {"text": "Then by fixing the embeddings of entities in the decoder, we can significantly reduce the number of parameters that need to be trained.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "As we can see that, RegDVAE+D can outperform the original DVAE by 8\u223c23 points on F1.", "labels": [], "entities": [{"text": "RegDVAE+D", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.6966107686360677}, {"text": "F1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9936380982398987}]}, {"text": "DVAE+D is also good but may fail when there area lot of out-of-sample entities in the training corpus.", "labels": [], "entities": [{"text": "DVAE+D", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5310821930567423}]}, {"text": "We have three hyper-parameters in our algorithm: \u03b1 0 for the regularization of encoder entropy, \u03b2 for the regularization with our constraints, and \u03b3 for the threshold of KB based cosine similarities.", "labels": [], "entities": []}, {"text": "Here, we test \u03b2 and \u03b3, since the sensitivity result of \u03b1 0 is the same as the original DVAE work.", "labels": [], "entities": [{"text": "sensitivity", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9927403330802917}]}, {"text": "The sensitivity of \u03b2 is shown in.", "labels": [], "entities": [{"text": "sensitivity", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9626609086990356}]}, {"text": "The results are good in a wide range from \u03b2 = 0.5 to \u03b2 = 2.", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9819579124450684}]}, {"text": "The sensitivity of \u03b3 is shown in.", "labels": [], "entities": []}, {"text": "It reveals some interesting patterns.", "labels": [], "entities": []}, {"text": "At the beginning when \u03b3 is small, it hurts the performance.", "labels": [], "entities": []}, {"text": "After \u03b3 getting greater than 0.7, it improves the performance, which means that only very similar relations indicated by KB embeddings are useful relations as constraints.", "labels": [], "entities": []}, {"text": "In addition, \u03b3 = 1 (meaning only finding identical relations) is worse than \u03b3 = 0.9, which means we indeed find some relations in our KB so that different triplets will be constrained.", "labels": [], "entities": []}, {"text": "Although we assume that there is no overlapped relation between the KB and the training text corpus, in practice, we may find a lot of applications that the relations are partially observed in KB.", "labels": [], "entities": []}, {"text": "Thus, we also test a setting when the KB has different proportions of overlapped relations with training text corpus.", "labels": [], "entities": []}, {"text": "In this case, we train different KB embeddings for different percentages of overlapped relations, and then apply the embeddings into the constraints.", "labels": [], "entities": []}, {"text": "The results are shown in(c).", "labels": [], "entities": []}, {"text": "As we can see, in general, more overlapped relations will result in better performance.", "labels": [], "entities": []}, {"text": "The best number can be better than the number without overlapped relation by about two points.", "labels": [], "entities": []}, {"text": "This again verifies that the KB embedding is very robust and represent the semantic meanings of entities even with part of the  relations observed ().", "labels": [], "entities": []}, {"text": "We also show some examples of entity pair similarities in.", "labels": [], "entities": []}, {"text": "From the Table we can see that our target relation cluster is /location/contained by.", "labels": [], "entities": []}, {"text": "In the first example, the similarity between entity pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences.", "labels": [], "entities": []}, {"text": "The same constraint is applied in the second example, although there's no direct connection between (Brazil, Latin America), (Argentina, Latin America).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of datasets. # facts in the text corpus  is the number of sentences with relation labels.", "labels": [], "entities": []}, {"text": " Table 3: Comparison results on NYT122 with different prediction and regularization strategies (using encoder or  decoder).", "labels": [], "entities": [{"text": "NYT122", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9291530251502991}]}, {"text": " Table 5: Examples for relation: /location/contained by.", "labels": [], "entities": []}]}