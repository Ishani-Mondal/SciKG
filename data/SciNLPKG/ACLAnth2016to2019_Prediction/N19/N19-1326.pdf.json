{"title": [], "abstractContent": [{"text": "In this paper we present a method to learn word embeddings that are resilient to misspellings.", "labels": [], "entities": []}, {"text": "Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns.", "labels": [], "entities": []}, {"text": "In our method, misspellings of each word are embedded close to their correct variants.", "labels": [], "entities": []}, {"text": "We train these embeddings on anew dataset we are releasing publicly.", "labels": [], "entities": []}, {"text": "Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings constitute a building block of many practical applications across NLP and related disciplines.", "labels": [], "entities": []}, {"text": "Techniques such as Word2Vec ( and) have been extensively used in practice.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.952019989490509}]}, {"text": "One of their drawbacks, however, is that they cannot provide embeddings for words that have not been observed at training time, i.e. Out-OfVocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "In real-world tasks, the input text is often generated by people and misspellings, a common source of OOV words, are frequent (e.g. () report that misspellings appear in up to 15% of web search queries).", "labels": [], "entities": []}, {"text": "As a consequence, the quality of downstream applications of word embeddings in real-world scenarios diminishes.", "labels": [], "entities": []}, {"text": "Simply allowing the inclusion of misspellings into corpora and vocabularies in existing methodologies might not provide satisfactory results.", "labels": [], "entities": []}, {"text": "The sparsity of misspellings would most likely prevent * This work was carried out when the author was working as an employee at Facebook London.", "labels": [], "entities": [{"text": "Facebook London", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8759667575359344}]}, {"text": "their embeddings from demonstrating any interesting properties.", "labels": [], "entities": []}, {"text": "Trying to balance the representation of misspellings with the representation of correctly spelled variants in training data by artificially introducing misspelled variants for every word in the corpus would on the other hand cause up to an exponential growth in the size of the training data, making training of the models infeasible.", "labels": [], "entities": []}, {"text": "To address this deficiency, we propose Misspelling Oblivious (word) Embeddings (MOE), anew model combining FastText () with a supervised task which embeds misspellings close to their correct variants.", "labels": [], "entities": [{"text": "Misspelling Oblivious (word) Embeddings (MOE)", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.6314646535449557}]}, {"text": "We carryout experiments on well established tasks and on their variants adapted to the misspellings problem.", "labels": [], "entities": []}, {"text": "We also propose new methods of evaluating embeddings specifically designed to capture their quality on misspelled words.", "labels": [], "entities": []}, {"text": "We train MOE embeddings on anew dataset we are releasing publicly.", "labels": [], "entities": []}, {"text": "Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.", "labels": [], "entities": []}, {"text": "Summarizing, we propose the following contributions: \u2022 a novel problem and a non-trivial solution to building word embeddings resistant to misspellings; \u2022 a novel evaluation method specifically suitable for evaluating the effectiveness of MOE; \u2022 a dataset of misspellings 1 to train MOE.", "labels": [], "entities": [{"text": "MOE", "start_pos": 239, "end_pos": 242, "type": "TASK", "confidence": 0.7932145595550537}]}, {"text": "The reminder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of the word embeddings literature.", "labels": [], "entities": []}, {"text": "In Section 3.1 we introduce Word2Vec and FastText models.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9583026766777039}]}, {"text": "We introduce the MOE model in Section 3.2.", "labels": [], "entities": [{"text": "MOE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.5031343698501587}]}, {"text": "Section 4 contains the descriptions of datasets we trained on and section 5 contains the description of experiments we conducted and their results.", "labels": [], "entities": []}, {"text": "In Section 6 we present our conclusions and plans for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experimental setup used for training our models and the experiments we conducted.", "labels": [], "entities": []}, {"text": "We use FastText 5 as a baseline for comparison since it can generate embeddings for OOV words which makes it potentially suitable for dealing with misspellings.", "labels": [], "entities": []}, {"text": "We train the baseline model using the default hyperparameters provided by the authors.", "labels": [], "entities": []}, {"text": "We consider character n-grams of lengths between m = 3 and M = 6, and we use 5 negative samples for each positive sample.", "labels": [], "entities": []}, {"text": "Training MOE requires optimizing two loss functions L F T and L SC jointly.", "labels": [], "entities": [{"text": "MOE", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9407491087913513}]}, {"text": "For optimizing L F T , we use the same parameters as in the baseline.", "labels": [], "entities": []}, {"text": "Additionally, to optimize L SC , we experiment with 5 negative samples per positive sample.", "labels": [], "entities": [{"text": "L SC", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.5352306962013245}]}, {"text": "We sweep over a range of values for the coefficient combining the two losses: \u03b1 \u2208 {0.01, 0.05, 0.1, 0.5, 0.25, 0.5, 0.75, 0.95, 0.99}.", "labels": [], "entities": []}, {"text": "Both FastText and MOE are trained using Stochastic Gradient Descent with a linearly decaying learning rate for 5 epochs to learn vectors with 300 dimensions.", "labels": [], "entities": [{"text": "MOE", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8418930172920227}]}, {"text": "We evaluate the performance of MOE on the following tasks: (intrinsic) Word Similarity, Word Analogy and Neighborhood Validity; (extrinsic) POS Tagging of English sentences.", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.6604696065187454}, {"text": "Word Analogy", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.6219584792852402}, {"text": "POS Tagging of English sentences", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.8042840838432312}]}, {"text": "We report the overlap between the misspellings seen at training time and misspellings present in tests in.", "labels": [], "entities": []}, {"text": "We evaluate MOE on two classic intrinsic tasks, namely Word Similarity and Word Analogy and representing the degree of similarity between w a and w b as perceived by human judges.", "labels": [], "entities": [{"text": "MOE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9714946746826172}, {"text": "Word Similarity", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.6979994922876358}, {"text": "Word Analogy", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.6979076415300369}]}, {"text": "In order to evaluate how resilient our method is to spelling errors, for each pair of words (w a , w b ) in the dataset, we provide a respective pair of misspellings (m a , m b ).", "labels": [], "entities": []}, {"text": "The misspellings are mined from search query logs of a real-world online search service.", "labels": [], "entities": []}, {"text": "When desired misspellings are not available in the logs, we synthetically generate them using the same script we used to generate the set M (see Section 4 for details).", "labels": [], "entities": []}, {"text": "We create 3 misspelled variants of both WS353 and RW datasets.", "labels": [], "entities": [{"text": "WS353", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9366986751556396}, {"text": "RW datasets", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8728806376457214}]}, {"text": "In each variant we limit the ratio between the edit distance of the word and the misspelling d e (w i , mi ) and the length of the word by a constant r, where r \u2208 {0.125, 0.250, 0.375}, with r = 0 representing the original dataset.", "labels": [], "entities": []}, {"text": "More precisely for each r we look fora misspelling which satisfies the following condition d e (w i , mi ) = r * len(w i ).", "labels": [], "entities": []}, {"text": "Effectively, if a word is too short to satisfy the condition, we preserve the original word (then w i = mi ).", "labels": [], "entities": []}, {"text": "Histograms in show the actual distribution of edit distances and lengths of words.", "labels": [], "entities": []}, {"text": "As expected, edit distance increases steeply with the increase of r value.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.7658393383026123}]}, {"text": "Edit distances are higher for the RW dataset since in average the length of words in RW is higher than on average length of words in WS353.", "labels": [], "entities": [{"text": "RW dataset", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.875309020280838}, {"text": "WS353", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.9747330546379089}]}, {"text": "Also, we observe that for r = 0.125, a significant portion of the words is not changed.", "labels": [], "entities": []}, {"text": "We conduct experiments for different values of the hyperparameter \u03b1 which sets the trade-off between L F T and L SC , i.e. the importance assigned to semantic loss and misspelling loss.", "labels": [], "entities": []}, {"text": "In the experiments, results corresponding to \u03b1 = 0 represents our baseline, FastText, since for \u03b1 = 0 the loss We measure the Spearman's rank correlation between the distance of an input pair of words and the human judgment score both for the original and the misspelled pair.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 126, "end_pos": 153, "type": "METRIC", "confidence": 0.6969854459166527}]}, {"text": "demonstrates the results of the word similarity task on the WS353 dataset.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7365768253803253}, {"text": "WS353 dataset", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9929153323173523}]}, {"text": "We observe that MOE is improving over FastText for WS353 variants with r = 0.25, and r = 0.375, and degrading performance when r = 0, and r = 0.125, where the majority of the words is not changed (see for the edit distance distribution).", "labels": [], "entities": [{"text": "MOE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.6902982592582703}]}, {"text": "As we expected, larger values of \u03b1, corresponding to more attention given to misspellings during training, result in improvements for highly misspelled datasets.", "labels": [], "entities": []}, {"text": "For the RW dataset), we observe that for all the values of r, MOE improves over the FastText baseline when we set \u03b1 = 0.05.", "labels": [], "entities": [{"text": "RW dataset", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9351037442684174}, {"text": "MOE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9986206293106079}, {"text": "FastText baseline", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.8622867465019226}]}, {"text": "More specifically, when r \u2208 {0, 0.125} and when \u03b1 <\u2248 0.1, the proposed method improves over the baseline.", "labels": [], "entities": []}, {"text": "When the amount of misspellings is higher, i.e., r \u2208 {0.25, 0.375}, MOE improves the results over the baseline for all of the \u03b1 values.", "labels": [], "entities": [{"text": "MOE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9553202390670776}]}, {"text": "These results suggest that FastText maybe a good baseline for dealing with low edit distance misspellings, however our model is better at capturing semantic relationships on higher edit distance misspellings.", "labels": [], "entities": []}, {"text": "This is inline with our hypothesis presented in Word Analogy.", "labels": [], "entities": [{"text": "Word Analogy", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.725781112909317}]}, {"text": "In addition to the word similarity, we also test the performance of MOE on the popular word analogy task introduced by).", "labels": [], "entities": [{"text": "MOE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.555652916431427}, {"text": "word analogy task", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7728997270266215}]}, {"text": "This task attempts to measure how good the embeddings model is at preserving relationships between words.", "labels": [], "entities": []}, {"text": "A single test sample from the word analogy dataset consists of four words A, B, C, D, forming two pairs -A, B and C, D, remaining in analogous relationships (\"A is to B like C is to D\").", "labels": [], "entities": []}, {"text": "There are two types of relationships: (i) syntactic, related to the structure of words; and (ii) semantic, related to their meanings.", "labels": [], "entities": []}, {"text": "banana, bananas, cat, cats is an example of a syntactic test sample.", "labels": [], "entities": []}, {"text": "In both pairs the fact that the second word is a plural version of the first constitutes a relationship between them.", "labels": [], "entities": []}, {"text": "Athens, Greece, Berlin, Germany is an example of a semantic test sample.", "labels": [], "entities": []}, {"text": "The relationship which is being tested in this case is that between the capital of a country and the country itself.", "labels": [], "entities": []}, {"text": "In addition to analyzing the canonical variant of the word analogies test, we also introduce a modification which is suitable specifically to the misspellings use-case.", "labels": [], "entities": []}, {"text": "Given a line A, B, C, D from the original analogies dataset, we misspell the first pair of words, obtaining a line A , B , C, D, where A is a misspelling of A and B is a misspelling of B.", "labels": [], "entities": []}, {"text": "We want to test if the misspelled pair A , B preserves the relationship of the pair C, D.", "labels": [], "entities": []}, {"text": "When generating misspellings we use a procedure similar to the one used for word similarities.", "labels": [], "entities": []}, {"text": "We create one variant of the misspelled dataset, constraining the edit distance tor = 0.25.", "labels": [], "entities": [{"text": "edit distance tor", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.8633127808570862}]}, {"text": "Experimental results for the canonical version of the word analogy task, presented in, show that MOE performs worse than FastText on the semantic analogy task.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7952305277188619}, {"text": "MOE", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.6628692746162415}, {"text": "semantic analogy task", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.7690990070501963}]}, {"text": "On the other hand, MOE performs better than the baseline on the syntactic analogies task.", "labels": [], "entities": [{"text": "MOE", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.5590384006500244}]}, {"text": "The results for the misspelled variant of the task show that, the overall performance of both the baseline and MOE is worse than on the canonical variant.", "labels": [], "entities": [{"text": "MOE", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.8505818843841553}]}, {"text": "For low values of \u03b1 \u2208 {0.01, 0.05}, MOE outperforms the baseline on the semantic task, achieving an over 67% better score than FastText for \u03b1 = 0.01.", "labels": [], "entities": [{"text": "MOE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.6469749808311462}]}, {"text": "MOE outperforms the baseline on the syntactic task for all tested values of \u03b1, improving by over 80% for \u03b1 = 0.75.", "labels": [], "entities": [{"text": "MOE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9535108208656311}]}, {"text": "For \u03b1 = 0.01, which achieved the best semantic result, the improvement on the syntactic task is over 33%.", "labels": [], "entities": []}, {"text": "The trends that we observe both in the canonical and the misspelled variant of the word analogies task seem to validate our choice of the loss function for the MOE model.", "labels": [], "entities": []}, {"text": "It is clear that the FastText component of the loss is indispensable to learn the semantic relationships between words.", "labels": [], "entities": []}, {"text": "In fact, it is the only component of the loss function which attempts to learn these relationships.", "labels": [], "entities": []}, {"text": "Therefore, decreasing it's importance (by increasing the value of \u03b1) is reflected by a decay in the semantic analogies score.", "labels": [], "entities": []}, {"text": "The spellcorrection component of the loss function, on the other hand, leverages the relationship between correctly spelled words and their misspellings.", "labels": [], "entities": []}, {"text": "As aside effect, it also adds additional subword information into the model.", "labels": [], "entities": []}, {"text": "This explains our good performance on the syntactic analogies task.", "labels": [], "entities": []}, {"text": "As our results on the misspelled variant of the task show, we improve over the baseline in understanding analogies on the misspelled words, which was one of the design principles for MOE.", "labels": [], "entities": [{"text": "MOE", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.5768834948539734}]}, {"text": "One of the explicit objectives of MOE is to embed misspellings close to their correct variants in the vector space.", "labels": [], "entities": [{"text": "MOE", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8986194133758545}]}, {"text": "In order to validate this hypothesis, we check wherein the neighborhood of a misspelling the correct word is situated.", "labels": [], "entities": []}, {"text": "Formally, fora pair (w m , we ) of a misspelling and its correction, we pick k nearest neighbors of the misspelling w min the embedding space using cosine similarity as a distance metric.", "labels": [], "entities": [{"text": "correction", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9533523321151733}]}, {"text": "We then evaluate the position of the correct word we within the neighborhood of w musing two metrics: \u2022 We use MRR () to score the neighborhood of the embeddings of misspellings (we assign a score of 0 if the correct word is not present).", "labels": [], "entities": [{"text": "MRR", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9973762035369873}]}, {"text": "\u2022 We also compute the neighborhood coverage defined as the percentage of misspellings for which the neighborhood contains the correct version.", "labels": [], "entities": []}, {"text": "The test set contains 5, 910 pairs (w m , we ) sampled from a collection of data coming from a realworld online service . shows experimen-6 www.facebook.com tal results for Neighbor Validity task.", "labels": [], "entities": [{"text": "Neighbor Validity task", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7667220830917358}]}, {"text": "We remind that \u03b1 = 0 denotes the FastText baseline.", "labels": [], "entities": [{"text": "FastText baseline", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.8439042270183563}]}, {"text": "The test results confirm our hypothesis.", "labels": [], "entities": []}, {"text": "We observe that MRR increases when more importance is given to the L SC component of the loss for any size of the neighborhood k \u2208 {5, 10, 50, 100}.", "labels": [], "entities": [{"text": "MRR", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9755306839942932}]}, {"text": "A similar trend can be observed for the neighborhood coverage task.", "labels": [], "entities": [{"text": "neighborhood coverage task", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7967749436696371}]}, {"text": "We conclude that, on average, we're more likely to surface the correction using MOE than with FastText.", "labels": [], "entities": [{"text": "MOE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.4721250832080841}]}, {"text": "What is more, whenever we are able to surface the correct version of a misspelled word, its position in the ranking is higher for MOE than for the FastText baseline.", "labels": [], "entities": [{"text": "MOE", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.7070541977882385}, {"text": "FastText baseline", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.8761084675788879}]}, {"text": "Finally, we evaluate MOE on a Part-of-Speech (POS) tagging task . To assess the impact of misspellings we artificially inject misspellings in the dataset.", "labels": [], "entities": [{"text": "MOE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8459230661392212}, {"text": "Part-of-Speech (POS) tagging task", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6980922917524973}]}, {"text": "We train MOE on three different dataset variants: a non-misspelled dataset, to verify that MOE does not jeopardize the performance on correct words; a dataset where 10% of words contain a misspelling, to simulate a realistic environment where some of the words are misspelled; and finally on a dataset where 100% of words contain misspellings, to simulate a highly distorted environment.", "labels": [], "entities": []}, {"text": "We use a state-of-the-art POS tagger consisting of a Conditional Random Fields (CRF) model where embeddings of the words in a sentence constitute observations and the tags to assign constitute the latent variables.", "labels": [], "entities": []}, {"text": "This model adds a dependency on both layers of a Bi-LSTM component to the tag variables in the CRF.", "labels": [], "entities": []}, {"text": "We evaluate the F1 score of the system for the three dataset variants we describe above.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9827415347099304}]}, {"text": "We test two different representations as input to the CRF: FastText (our baseline), and MOE embeddings.", "labels": [], "entities": []}, {"text": "Our results are reported in Table 2.", "labels": [], "entities": []}, {"text": "We make the following observations based on the results of our experiments.", "labels": [], "entities": []}, {"text": "Firstly, in the two extreme cases of the 100% misspelled test and correct training and the correct test and 100% misspelled training, MOE improves the F1 by 2 and 3.5 points respectively with respect to the FastText baseline.", "labels": [], "entities": [{"text": "correct", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9359295964241028}, {"text": "correct test", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.9457232356071472}, {"text": "MOE", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.8356839418411255}, {"text": "F1", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.997511625289917}, {"text": "FastText baseline", "start_pos": 207, "end_pos": 224, "type": "DATASET", "confidence": 0.9325897395610809}]}, {"text": "When the test data is 100% misspelled, MOE always beats the baseline by up to 2.3 points of F1.", "labels": [], "entities": [{"text": "MOE", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9941270351409912}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.998823344707489}]}, {"text": "Also, in this case the loss in F1 with respect to the case where both the training and the test are   correct is much less then when the training data does not contain misspellings.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9994044303894043}]}, {"text": "To be remarked is the F1 score difference in the more realistic case consisting of training data that is 10% misspelled.", "labels": [], "entities": [{"text": "F1 score difference", "start_pos": 22, "end_pos": 41, "type": "METRIC", "confidence": 0.9838625391324362}]}, {"text": "In this case MOE attains a sensitive improvement of 2.3% points of F1.", "labels": [], "entities": [{"text": "MOE", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.46855637431144714}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9997391104698181}]}, {"text": "Finally, MOE does not reduce the effectiveness of the CRF POS Tagger with respect to the FastText baseline when neither the training nor the test set are misspelled.", "labels": [], "entities": [{"text": "MOE", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.5239259600639343}, {"text": "CRF POS Tagger", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.5071877936522166}, {"text": "FastText baseline", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.8503484725952148}]}, {"text": "All in all, we have shown that MOE does not affect the effectiveness of the POS Tagger in the case of correctly misspelled words and improves sensitively the quality of the POS tagger on misspellings.", "labels": [], "entities": [{"text": "MOE", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9693248271942139}]}], "tableCaptions": [{"text": " Table 1: Percentages of test misspellings unobserved  at training time per test set. The r parameter indicates  variants of respective word similarity test sets.", "labels": [], "entities": []}, {"text": " Table 2: Performance on POS tagging task for UPOS tags using CRF. The models were trained on 100 epochs  with an early stop (small difference on validation error) mechanism enabled. Considering F1 score, we evaluate  on 2 variants of test data: Original (correctly spelled) on the right hand side of the table and 100% misspelled on  the left hand side.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7937309145927429}, {"text": "early stop (small difference on validation error)", "start_pos": 114, "end_pos": 163, "type": "METRIC", "confidence": 0.8024780286682976}, {"text": "F1 score", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9787597358226776}]}]}