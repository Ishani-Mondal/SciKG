{"title": [{"text": "Iterative Search for Weakly Supervised Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.6499480903148651}]}], "abstractContent": [{"text": "Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of logical forms, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer.", "labels": [], "entities": []}, {"text": "We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones.", "labels": [], "entities": []}, {"text": "This training scheme lets us iteratively train models that provide guidance to subsequent ones to search for logical forms of increasing complexity, thus dealing with the problem of spuriousness.", "labels": [], "entities": []}, {"text": "We evaluate these techniques on two hard datasets: WIKITABLEQUESTIONS (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outper-forms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.", "labels": [], "entities": [{"text": "Cornell Natural Language Visual Reasoning (NLVR", "start_pos": 80, "end_pos": 127, "type": "TASK", "confidence": 0.6852110496589116}]}], "introductionContent": [{"text": "Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8209790289402008}]}, {"text": "These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation.", "labels": [], "entities": []}, {"text": "This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (, program synthesis ( and building natural language interfaces (.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 112, "end_pos": 142, "type": "TASK", "confidence": 0.6740811665852865}, {"text": "question answering", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.9045077860355377}, {"text": "program synthesis", "start_pos": 182, "end_pos": 199, "type": "TASK", "confidence": 0.8140696287155151}]}, {"text": "Recent work has focused on training semantic parses via weak supervision from denotations alone (.", "labels": [], "entities": [{"text": "training semantic parses", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6825092534224192}]}, {"text": "This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (), and not assuming full supervision lets us be agnostic about the logical form language.", "labels": [], "entities": []}, {"text": "The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task.", "labels": [], "entities": [{"text": "open-domain semantic parsing tasks", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.7048498839139938}]}, {"text": "However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms, inter alia) but also dealing with spurious logical forms that evaluate to the correct denotation while not being semantically equivalent to the utterance.", "labels": [], "entities": []}, {"text": "For example, if the denotations are binary, 50% of all syntactically valid logical forms evaluate to the correct answer, regardless of their semantics.", "labels": [], "entities": []}, {"text": "This problem renders the training signal extremely noisy, making it hard for the model to learn anything without some additional guidance during search.", "labels": [], "entities": []}, {"text": "We introduce two innovations to improve learning from denotations.", "labels": [], "entities": []}, {"text": "Firstly, we propose an iterative search procedure for gradually increasing the complexity of candidate logical forms for each training instance, leading to better training data and better parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 188, "end_pos": 195, "type": "TASK", "confidence": 0.9458591341972351}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9132223725318909}]}, {"text": "This procedure is implemented via training our model with two interleaving objectives, one that involves searching for logical forms of limited complexity during training (online search), and another that maximizes the marginal likelihood of retrieved logical forms.", "labels": [], "entities": []}, {"text": "Second, we include a notion of coverage over the question in the search step to guide the training algorithm towards logical forms that not only evaluate to the correct denotation, but also have some connection to the words in the utterance.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of these two techniques on two difficult reasoning tasks: WIK-ITABLEQUESTIONS(WTQ)), an open domain task with significant lexical variation, and Cornell Natural Language Visual Reasoning (NLVR) (), a closed domain task with binary denotations, and thus far less supervision.", "labels": [], "entities": [{"text": "Cornell Natural Language Visual Reasoning (NLVR)", "start_pos": 178, "end_pos": 226, "type": "TASK", "confidence": 0.6900207176804543}]}, {"text": "We show that: 1) interleaving online search and MML over retrieved logical forms ( \u00a74) is a more effective training algorithm than each of those objectives alone; 2) coverage guidance during search ( \u00a73) is helpful for dealing with weak supervision, more so in the case of NLVR where the supervision is weaker; 3) a combination of the two techniques yields 44.3% test accuracy on WTQ, outperforming the previous best single model in a comparable setting, and 82.9% test accuracy on NLVR, outperforming the best prior model, which also relies on greater supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 368, "end_pos": 376, "type": "METRIC", "confidence": 0.9115881323814392}, {"text": "WTQ", "start_pos": 380, "end_pos": 383, "type": "DATASET", "confidence": 0.8663740754127502}, {"text": "accuracy", "start_pos": 470, "end_pos": 478, "type": "METRIC", "confidence": 0.9034907221794128}, {"text": "NLVR", "start_pos": 482, "end_pos": 486, "type": "DATASET", "confidence": 0.9436751008033752}]}], "datasetContent": [{"text": "We will now describe the two datasets we use in this work to evaluate our methods -Cornell NLVR and WIKITABLEQUESTIONS.", "labels": [], "entities": [{"text": "Cornell NLVR", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9563354551792145}, {"text": "WIKITABLEQUESTIONS", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.8155171871185303}]}, {"text": "We evaluate both our contributions on NLVR and WIKITABLEQUESTIONS.", "labels": [], "entities": [{"text": "NLVR", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.9589799642562866}, {"text": "WIKITABLEQUESTIONS", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.909089982509613}]}, {"text": "NLVR We use the standard train-dev-test split for NLVR, containing 12409, 988 and 989 sentence-image pairs respectively.", "labels": [], "entities": [{"text": "NLVR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9662306904792786}, {"text": "NLVR", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9548439979553223}]}, {"text": "NLVR contains most of the sentences occurring in multiple worlds (with an average of 3.9 worlds per sentence).", "labels": [], "entities": [{"text": "NLVR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8957481384277344}]}, {"text": "We set the word embedding and action embedding sizes to 50, and the hidden layer size of both the encoder and the decoder to 30.", "labels": [], "entities": []}, {"text": "We initialized all the parameters, including the word and action embeddings using Glorot uniform initialization (.", "labels": [], "entities": []}, {"text": "We found that using pretrained word representations did not help.", "labels": [], "entities": []}, {"text": "We added a dropout () of 0.2 on the outputs of the encoder and the decoder and before predicting the next action, set the beam size to 10 both during training and attest time, and trained the model using ADAM) with a learning rate of 0.001.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.7297413945198059}]}, {"text": "All the hyperparameters are tuned on the validation set.", "labels": [], "entities": []}, {"text": "WIKITABLEQUESTIONS This dataset comes with five different cross-validation folds of training data, each containing a different 80/20 split for training and development.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS This dataset", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.8198404510815939}]}, {"text": "We first show results aggregated from all five folds in \u00a76.3, and then show results from controlled experiments on fold 1.", "labels": [], "entities": []}, {"text": "We replicated the model presented in, and only changed the training algorithm and the language used.", "labels": [], "entities": []}, {"text": "We used abeam size of 20 for MBR during training and decoding, and 10 for MML during decoding, and trained the model using Stochastic Gradient Descent () with a learning rate of 0.1, all of which are tuned on the validation sets.", "labels": [], "entities": [{"text": "MBR", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.5572055578231812}]}, {"text": "Specifics of iterative search For our iterative search algorithm, we obtain an initial set of candidate logical forms in both domains by exhaustively searching to a depth of 10 2 . During search we retrieve the logical forms that lead to the correct denotations in all the corresponding worlds, and sort them based on their coverage cost using the coverage lexicon described in \u00a75.4, and choose the top-k 3 . At each iteration of the search step in our iterative training algorithm, we increase the maximum depth of our search with a step-size of 2, finding more complex logical forms and covering a larger proportion of the training data.", "labels": [], "entities": []}, {"text": "While exhaustive search is prohibitively expensive beyond a fixed number of steps, our training process that uses beam search based approximation can go deeper.", "labels": [], "entities": []}, {"text": "Implementation We implemented our model and training algorithms within the AllenNLP () toolkit.", "labels": [], "entities": [{"text": "AllenNLP () toolkit", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.9319421847661337}]}, {"text": "The code and models are publicly available at https://github.com/allenai/ iterative-search-semparse.", "labels": [], "entities": []}, {"text": "compares the performance of a single model trained using Iterative Search, with that of previously published single models.", "labels": [], "entities": []}, {"text": "We excluded ensemble models since there are differences in the way ensembles are built for this task in previous work, either in terms of size or how the individual models were chosen.", "labels": [], "entities": []}, {"text": "We show both best and aver-  age (over 5 folds) single model performance from (Memory Augmented Policy Optimization).", "labels": [], "entities": [{"text": "Memory Augmented Policy Optimization", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.6114384904503822}]}, {"text": "The best model was chosen based on performance on the development set.", "labels": [], "entities": []}, {"text": "Our single model performances are computed in the same way.", "labels": [], "entities": []}, {"text": "Note that also use a lexicon similar to ours to prune the seed set of logical forms used to initialize their memory buffer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of single model performances of  Iterative Search with previously reported single model  performances", "labels": [], "entities": []}, {"text": " Table 2: Comparison of iterative search with static  MML, iterative MML, and the previous best result  from (Liang et al., 2018), all trained on the official split  1 of WIKITABLEQUESTIONS and tested on the official  test set.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS", "start_pos": 171, "end_pos": 189, "type": "DATASET", "confidence": 0.7387785315513611}, {"text": "official  test set", "start_pos": 208, "end_pos": 226, "type": "DATASET", "confidence": 0.7805747588475546}]}, {"text": " Table 3: Comparison of our approach with previously published approaches. We show accuracy and consistency  on the development set, and public (Test-P) and hidden (Test-H) test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9993423819541931}, {"text": "consistency", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9751474261283875}]}, {"text": " Table 4: Effect of coverage guidance on NLVR parsers  trained with and without initialization from an MML  model. Metrics shown are accuracy and consistency on  the public test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9994217157363892}, {"text": "consistency", "start_pos": 146, "end_pos": 157, "type": "METRIC", "confidence": 0.9978135824203491}]}, {"text": " Table 5: Effect of iterative search (S) and maximization  (M) on NLVR. % cov. is the percentage of training data  for which the S step retrieves consistent logical forms.", "labels": [], "entities": [{"text": "maximization  (M)", "start_pos": 45, "end_pos": 62, "type": "METRIC", "confidence": 0.9263041913509369}]}, {"text": " Table 6: Iterative search on WIKITABLEQUESTIONS.  M and S refer to Maximization and Search steps.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.8718090057373047}, {"text": "Maximization", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9788835048675537}]}]}