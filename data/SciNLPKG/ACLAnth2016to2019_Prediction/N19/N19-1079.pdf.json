{"title": [{"text": "Better Modeling of Incomplete Annotations for Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6526989738146464}]}], "abstractContent": [{"text": "Supervised approaches to named entity recognition (NER) are largely developed based on the assumption that the training data is fully annotated with named entity information.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.8022497594356537}]}, {"text": "However, in practice, annotated data can often be imperfect with one typical issue being the training data may contain incomplete annotations.", "labels": [], "entities": []}, {"text": "We highlight several pitfalls associated with learning under such a setup in the context of NER and identify limitations associated with existing approaches, proposing a novel yet easy-to-implement approach for recognizing named entities with incomplete data annotations.", "labels": [], "entities": [{"text": "NER", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.893741250038147}]}, {"text": "We demonstrate the effectiveness of our approach through extensive experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER)) as one of the most fundamental tasks within natural language processing (NLP) has received significant attention.", "labels": [], "entities": [{"text": "Named entity recognition (NER))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8262251516183218}, {"text": "natural language processing (NLP)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.7364324728647867}]}, {"text": "Most existing approaches to NER focused on a supervised setup, where fully annotated named entity information is assumed to be available during the training phase.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.993981659412384}]}, {"text": "However, in practice, obtaining high-quality annotations can be a very laborious and expensive process (.", "labels": [], "entities": []}, {"text": "One of the common issues with data annotations is there maybe incomplete annotations.", "labels": [], "entities": []}, {"text": "shows an example sentence with two named entities \"John Lloyd Jones\" and \"BBC radio\" of type PER (person) and ORG (organization), respectively.", "labels": [], "entities": [{"text": "John Lloyd Jones", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9338928659756979}, {"text": "BBC radio", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9159160852432251}, {"text": "PER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9723671078681946}, {"text": "ORG", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9735527038574219}]}, {"text": "Following the standard BIOES tagging scheme, the corresponding gold label sequence is shown below the sentence.", "labels": [], "entities": [{"text": "BIOES tagging", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7321780622005463}]}, {"text": "When the data annotations are incomplete, certain labels Our code and data are available at http://statnlp.", "labels": [], "entities": []}, {"text": "org/research/ie.: An example sentence with gold named entity annotations and different assumptions (i.e., A.1 to A.3) on available labels.", "labels": [], "entities": []}, {"text": "\"-\" represents a missing label.", "labels": [], "entities": []}, {"text": "maybe missing from the label sequence.", "labels": [], "entities": []}, {"text": "Properly defining the task is important, and we argue there are two possible potential pitfalls associated with modeling incomplete annotations, especially for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 164, "end_pos": 172, "type": "TASK", "confidence": 0.9152578115463257}]}, {"text": "Several previous approaches assume the incomplete annotations can be obtained by simply removing either word-level labels) or span-level labels).", "labels": [], "entities": []}, {"text": "As shown in, under both assumptions (i.e., A.1 and A.2), there will be words annotated with O labels.", "labels": [], "entities": [{"text": "A.1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.7023740410804749}]}, {"text": "The former approach may even lead to sub-entity level annotations (e.g., \"radio\" is annotated as part of an entity).", "labels": [], "entities": []}, {"text": "However, we argue such assumptions can be largely unrealistic.", "labels": [], "entities": []}, {"text": "In practice, annotators are typically instructed to annotate named entities for complete word spans only ().", "labels": [], "entities": []}, {"text": "Thus, sub-entity level annotations or O labels 2 should not be assumed to be avail-2 Why should the O labels be assumed unavailable?", "labels": [], "entities": []}, {"text": "This is because the annotators typically do not actively specify the O labels when working on annotations.", "labels": [], "entities": []}, {"text": "If the annotator chooses not to annotate a word, it could either mean it is not part of any entity, or the word is actually part of an entity but the annotator neglected it in the annotation process (therefore we have incomplete annotations).", "labels": [], "entities": []}, {"text": "However, we note that assigning the O label to a word would precisely indicate it is strictly not part of any entity, which is not desirable.: Graphical illustrations on different assumptions on unavailable labels, where the entity \"John Lloyd Jones\" of type PER is labeled but \"BBC radio\" of type ORG is missing.", "labels": [], "entities": [{"text": "John Lloyd Jones\"", "start_pos": 233, "end_pos": 250, "type": "DATASET", "confidence": 0.8286296129226685}, {"text": "BBC radio\"", "start_pos": 279, "end_pos": 289, "type": "DATASET", "confidence": 0.9712537129720052}]}, {"text": "Each path refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity).", "labels": [], "entities": []}, {"text": "Therefore such approaches are making sub-optimal assumptions on the available labels.", "labels": [], "entities": []}, {"text": "When the proper assumptions on the available labels are made, one can typically model the missing labels as latent variables and train a latentvariable conditional random fields model).", "labels": [], "entities": []}, {"text": "One such approach is presented in.", "labels": [], "entities": []}, {"text": "Their work focused on the citation parsing 3 (i.e., sequence labeling) task which does not suffer from the above issue as no O label is involved.", "labels": [], "entities": [{"text": "citation parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9214819669723511}, {"text": "sequence labeling)", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7671543459097544}]}, {"text": "However, though the approach was shown effective in the citation parsing task, we found its effectiveness does not transfer to the NER task even in the absence of the above available labels issue.", "labels": [], "entities": [{"text": "citation parsing task", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.9209709366162618}, {"text": "NER task", "start_pos": 131, "end_pos": 139, "type": "TASK", "confidence": 0.8368707001209259}]}, {"text": "As we would highlight later, the reason is related to the undesirable assumptions on the unavailable labels.", "labels": [], "entities": []}, {"text": "In this work, we tackle the incomplete annotation problem when building an NER system, under a more realistic yet more challenging scenario.", "labels": [], "entities": []}, {"text": "We present a novel, effective, yet easy-to-implement approach, and conduct extensive experiments on various datasets and show our approach significantly outperforms several previous approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two standard NER datasets - Spanish datasets that consist of news articles.", "labels": [], "entities": [{"text": "NER datasets - Spanish datasets", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.839509129524231}]}], "tableCaptions": [{"text": " Table 1: Data statistics for the datasets.", "labels": [], "entities": []}]}