{"title": [{"text": "A BERT-based Universal Model for Both Within-and Cross-sentence Clinical Temporal Relation Extraction", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9824498891830444}, {"text": "Within-and Cross-sentence Clinical Temporal Relation Extraction", "start_pos": 38, "end_pos": 101, "type": "TASK", "confidence": 0.6542294224103292}]}], "abstractContent": [{"text": "Classic methods for clinical temporal relation extraction focus on relational candidates within a sentence.", "labels": [], "entities": [{"text": "clinical temporal relation extraction", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6561072319746017}]}, {"text": "On the other hand, breakthrough Bidirectional Encoder Representations from Transformers (BERT) are trained on large quantities of arbitrary spans of contiguous text instead of sentences.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT", "start_pos": 32, "end_pos": 93, "type": "TASK", "confidence": 0.6849601141044072}]}, {"text": "In this study, we aim to build a sentence-agnostic framework for the task of CONTAINS temporal relation extraction.", "labels": [], "entities": [{"text": "CONTAINS temporal relation extraction", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.8739536255598068}]}, {"text": "We establish anew state-of-the-art result for the task, 0.684F for in-domain (0.055-point improvement) and 0.565F for cross-domain (0.018-point improvement), by fine-tuning BERT and pre-training domain-specific BERT models on sentence-agnostic temporal relation instances with WordPiece-compatible encodings, and augmenting the labeled data with automatically generated \"sil-ver\" instances.", "labels": [], "entities": [{"text": "BERT", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.7850673794746399}]}], "introductionContent": [{"text": "The release of BERT ( has substantially advanced the state-of-the-art in several sentence-level, inter-sentence-level, and tokenlevel tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9555787444114685}]}, {"text": "BERT is trained on very large unlabeled corpora to achieve good generalizability.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9257763624191284}]}, {"text": "Instead of relying on a recurrent neural network, BERT uses a transformer architecture to better capture long distance dependencies.", "labels": [], "entities": [{"text": "BERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.7015445828437805}]}, {"text": "BERT is able to make predictions that go beyond natural sentence boundaries, because it is trained on fragments of contiguous text that typically span multiple sentences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8511176705360413}]}, {"text": "These advantages of BERT motivate us to apply it to a traditionally sentence-level task -temporal relation extraction from clinical text.", "labels": [], "entities": [{"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9763540029525757}, {"text": "sentence-level task -temporal relation extraction from clinical text", "start_pos": 68, "end_pos": 136, "type": "TASK", "confidence": 0.7191123929288652}]}, {"text": "The identification of temporal relations in the clinical narrative can lead to accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with a variety of clinical applications such as question answering), clinical outcomes prediction (, and recognition of temporal patterns and timelines ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 250, "end_pos": 268, "type": "TASK", "confidence": 0.8971786201000214}, {"text": "clinical outcomes prediction", "start_pos": 271, "end_pos": 299, "type": "TASK", "confidence": 0.7131542960802714}, {"text": "recognition of temporal patterns and timelines", "start_pos": 307, "end_pos": 353, "type": "TASK", "confidence": 0.7975950340429941}]}, {"text": "However, the labeled instances for this clinical information extraction task are limited, so neural models trained from scratch may not be able to learn complex linguistic phenomena.", "labels": [], "entities": [{"text": "clinical information extraction task", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7512618899345398}]}, {"text": "Pre-trained models like BERT could potentially provide rich representations as they are trained on massive data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.7576171159744263}]}, {"text": "Classic models for clinical temporal relation extraction have framed the task within a sentence (, making them susceptible to sentence detection errors.", "labels": [], "entities": [{"text": "clinical temporal relation extraction", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.6271542757749557}, {"text": "sentence detection", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7066228240728378}]}, {"text": "Using BERT, on the other hand, eliminates this sensitivity to sentence boundary errors.", "labels": [], "entities": [{"text": "BERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9971154928207397}]}, {"text": "The key contributions of this paper are: (1) introducing BERT to the challenging task of clinical temporal relation extraction and evaluating its performance on a widely used testbed (THYME corpus;), (2) developing a universal processing mechanism based on a fixed, sentenceboundary agnostic window of contiguous tokens, (3) pre-training BERT on MIMIC-III (Medical Information Mart for Intensive Care) dataset and comparing its performance to BERT and its biomedical adaptation BioBERT (), (4) augmenting the labeled set with automatically generated instances from unlabeled data, and (5) evaluating models for in-and cross-domain tasks on the THYME corpus.", "labels": [], "entities": [{"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.7043406963348389}, {"text": "clinical temporal relation extraction", "start_pos": 89, "end_pos": 126, "type": "TASK", "confidence": 0.601184107363224}, {"text": "THYME corpus", "start_pos": 184, "end_pos": 196, "type": "DATASET", "confidence": 0.7670665979385376}, {"text": "BERT", "start_pos": 338, "end_pos": 342, "type": "METRIC", "confidence": 0.8674365282058716}, {"text": "BERT", "start_pos": 443, "end_pos": 447, "type": "METRIC", "confidence": 0.7831767201423645}, {"text": "BioBERT", "start_pos": 478, "end_pos": 485, "type": "METRIC", "confidence": 0.6607455015182495}, {"text": "THYME corpus", "start_pos": 644, "end_pos": 656, "type": "DATASET", "confidence": 0.8440317809581757}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Model performance of CONTAINS relation  on colon cancer test set. T: using non-XML tags; S:  adding high confidence positive silver instances.", "labels": [], "entities": [{"text": "T", "start_pos": 76, "end_pos": 77, "type": "METRIC", "confidence": 0.9699408411979675}]}, {"text": " Table 2: Model performance of CONTAINS relation on  brain cancer test set.", "labels": [], "entities": [{"text": "brain cancer test set", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.6769317612051964}]}, {"text": " Table 3: Within-vs. cross-sentence results on colon  cancer development set.", "labels": [], "entities": []}]}