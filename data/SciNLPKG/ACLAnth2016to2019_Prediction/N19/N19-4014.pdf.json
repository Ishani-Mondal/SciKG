{"title": [{"text": "FAKTA: An Automatic End-to-End Fact Checking System", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8521777987480164}, {"text": "End-to-End Fact Checking", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.5571412344773611}]}], "abstractContent": [{"text": "We present FAKTA which is a unified framework that integrates various components of a fact checking process: document retrieval from media sources with various types of reliability , stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis.", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8024742007255554}, {"text": "document retrieval from media sources", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.8110695958137513}, {"text": "stance detection of documents", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.8304575085639954}, {"text": "evidence extraction", "start_pos": 243, "end_pos": 262, "type": "TASK", "confidence": 0.820202648639679}, {"text": "linguistic analysis", "start_pos": 268, "end_pos": 287, "type": "TASK", "confidence": 0.7009048759937286}]}, {"text": "FAKTA predicts the factu-ality of given claims and provides evidence at the document and sentence level to explain its predictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid increase of fake news in social media and its negative influence on people and public opinion (, various organizations are now performing manual fact checking on suspicious claims.", "labels": [], "entities": []}, {"text": "However, manual factchecking is a time consuming and challenging process.", "labels": [], "entities": []}, {"text": "As an alternative, researchers are investigating automatic fact checking which is a multi-step process and involves: (i) retrieving potentially relevant documents fora given claim (, (ii) checking the reliability of the media sources from which documents are retrieved, (iii) predicting the stance of each document with respect to the claim (, and finally (iv) predicting factuality of given claims (.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.70705346763134}, {"text": "predicting factuality of given claims", "start_pos": 361, "end_pos": 398, "type": "TASK", "confidence": 0.8221455335617065}]}, {"text": "While previous works separately investigated individual components of the fact checking process, in this work, we present a unified framework titled FAKTA that integrates these components to not only predict the factuality of given claims, but also provide evidence at the document and sentence level to explain its predictions.", "labels": [], "entities": [{"text": "fact checking process", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.8883178234100342}, {"text": "FAKTA", "start_pos": 149, "end_pos": 154, "type": "DATASET", "confidence": 0.6469510793685913}]}, {"text": "To the best of our knowledge, FAKTA is the only system that offers such a capability.", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.6306854486465454}]}, {"text": "illustrates the general architecture of FAKTA.", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.7956752181053162}]}, {"text": "The system is accessible via a Web browser and has two sides: client and server.", "labels": [], "entities": []}, {"text": "When a user at the client side submits a textual claim for fact checking, the server handles the request by first passing it into the document retrieval component to retrieve a list of top-K relevant documents (see Section 2.1) from four types of sources: Wikipedia, highly-reliable, mixed reliability and low reliability mainstream media (see Section 2.2).", "labels": [], "entities": []}, {"text": "The retrieved documents are passed to the re-ranking model to refine the retrieval result (see Section 2.1).", "labels": [], "entities": []}, {"text": "Then, the stance detection component detects the stance/perspective of each relevant document with respect to the claim, typically modeled using labels such as agree, disagree and discuss.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8336917161941528}]}, {"text": "This component further provides rationales at the sentence level for explaining model predictions (see Section 2.3).", "labels": [], "entities": []}, {"text": "Each document is also passed to the linguistic analysis component to analyze the language of the document using different linguistic lexicons (see Section 2.4).", "labels": [], "entities": []}, {"text": "Finally, the aggregation component combines the predictions of stance detection for all the relevant documents and makes a final decision about the factuality of the claim (see Section 2.5).", "labels": [], "entities": [{"text": "stance detection", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.8898352384567261}]}, {"text": "We describe the components below.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Fact Extraction and VERification (FEVER) dataset) to evaluate our system.", "labels": [], "entities": [{"text": "Fact Extraction", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.6198046803474426}, {"text": "VERification (FEVER) dataset", "start_pos": 31, "end_pos": 59, "type": "METRIC", "confidence": 0.703003442287445}]}, {"text": "In FEVER, each claim is assigned to its relevant Wikipedia documents with agree/disagree stances to the claim, and claims are labeled as supported (SUP, i.e. factually true), refuted (REF, i.e. factually false), and not enough information (NEI, i.e., there is not any relevant document for the claim in Wikipedia).", "labels": [], "entities": [{"text": "FEVER", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.4854474663734436}, {"text": "REF", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9522346258163452}]}, {"text": "The data includes a total of 145K claims, with around 80K, 30K and 35K SUP, REF and NEI labels respectively.", "labels": [], "entities": [{"text": "REF", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9746471643447876}]}, {"text": "Document Retrieval: shows results for document retrieval.", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7423192858695984}, {"text": "document retrieval", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7140053063631058}]}, {"text": "We use various search and ranking algorithms that measure the similarity between each input claim as query and Web documents.", "labels": [], "entities": []}, {"text": "Lines 1-11 in the table show the results when we use Lucene to index and search the data corpus with the following retrieval models: BM25 () with different settings for its hyperparameter (Lines 9-11).", "labels": [], "entities": [{"text": "Lucene", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9762575030326843}, {"text": "BM25", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.7997575998306274}]}, {"text": "According to the resulting performance at different ranks {1-20}, we select the ranking algorithm DFR z (Lucene DF R Z ) as our retrieval model.", "labels": [], "entities": [{"text": "Lucene DF R Z", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.8001842796802521}]}, {"text": "In addition, Lines 12-13 show the results when claims are converted to queries as explained in Section 2.1.", "labels": [], "entities": []}, {"text": "The results (Lines 5 and 12) show that Lucene performance decreases with query generation.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.8522397875785828}, {"text": "query generation", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7656811773777008}]}, {"text": "This might be because the resulting queries become more abstract than their corresponding claims which may introduce some noise to the intended meaning of claims.", "labels": [], "entities": []}, {"text": "However, Lines 14-15 show that our re-ranking model, explained in Section 2.1, can improve both Lucene and Google results.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.7985232472419739}]}, {"text": "FAKTA Full Pipeline: The complete pipeline consists of document retrieval and re-ranking model (Section 2.1), stance detection and rationale extraction 5 (Section 2.3) and aggregation model (Section 2.5).", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8804226517677307}, {"text": "stance detection and rationale extraction", "start_pos": 110, "end_pos": 151, "type": "TASK", "confidence": 0.7555127620697022}]}, {"text": "shows the results for the full pipeline.", "labels": [], "entities": []}, {"text": "Lines 1-3 show the results for all three SUP, REF, and NEI labels (3lbl) and Ran-", "labels": [], "entities": [{"text": "REF", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8139244318008423}]}], "tableCaptions": [{"text": " Table 1: Results of document retrieval on FEVER.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7447186410427094}, {"text": "FEVER", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.6174211502075195}]}, {"text": " Table 2: FAKTA full pipeline Results on FEVER.", "labels": [], "entities": [{"text": "FAKTA", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.534026563167572}, {"text": "FEVER", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9366173148155212}]}]}