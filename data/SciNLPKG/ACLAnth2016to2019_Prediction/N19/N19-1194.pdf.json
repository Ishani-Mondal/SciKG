{"title": [{"text": "VQD: Visual Query Detection in Natural Scenes", "labels": [], "entities": [{"text": "VQD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7729125022888184}, {"text": "Visual Query Detection in Natural Scenes", "start_pos": 5, "end_pos": 45, "type": "TASK", "confidence": 0.7273553162813187}]}], "abstractContent": [{"text": "We propose Visual Query Detection (VQD), anew visual grounding task.", "labels": [], "entities": [{"text": "Visual Query Detection (VQD)", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.740510568022728}]}, {"text": "In VQD, a system is guided by natural language to localize a variable number of objects in an image.", "labels": [], "entities": []}, {"text": "VQD is related to visual referring expression recognition , where the task is to localize only one object.", "labels": [], "entities": [{"text": "visual referring expression recognition", "start_pos": 18, "end_pos": 57, "type": "TASK", "confidence": 0.6599706783890724}]}, {"text": "We describe the first dataset for VQD and we propose baseline algorithms that demonstrate the difficulty of the task compared to referring expression recognition.", "labels": [], "entities": [{"text": "referring expression recognition", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.7972516616185507}]}], "introductionContent": [{"text": "In computer vision, object detection is the task of identifying all objects from a specific closed-set of pre-defined classes by putting a bounding box around each object present in an image, e.g., in the widely used COCO dataset there are 80 object categories and an algorithm must put a box around all instances of each object present in an image ().", "labels": [], "entities": [{"text": "object detection", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7577516734600067}, {"text": "COCO dataset", "start_pos": 217, "end_pos": 229, "type": "DATASET", "confidence": 0.8668919503688812}]}, {"text": "Recent deep learning based models have significantly advanced the state-of-the-art in object detection); however, many applications demand more nuanced detection of objects with specific attributes or objects in relation to each other.", "labels": [], "entities": [{"text": "object detection", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8133966326713562}]}, {"text": "Here, we study goal-directed object detection, where the set of possible valid objects is far greater than in the typical object detection problem.", "labels": [], "entities": [{"text": "object detection", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7195251882076263}, {"text": "object detection", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7244224697351456}]}, {"text": "Specifically, we introduce the Visual Query Detection (VQD) task (see).", "labels": [], "entities": [{"text": "Visual Query Detection (VQD) task", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7606960705348423}]}, {"text": "In VQD, a system is given a query in natural language and an image and it must produce 0-N boxes that satisfy that query.", "labels": [], "entities": []}, {"text": "VQD has numerous applications, ranging from image retrieval to robotics.", "labels": [], "entities": [{"text": "VQD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9493184685707092}, {"text": "image retrieval", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7900068759918213}]}, {"text": "VQD is related to the visual referring expression recognition (RER) task); however, in RER every image has only a single correct box.", "labels": [], "entities": [{"text": "referring expression recognition (RER) task", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.7961222231388092}]}, {"text": "In contrast, in VQD there could be no valid outputs fora query or multiple valid outputs, making the task both harder and more useful.", "labels": [], "entities": []}, {"text": "As discussed later, existing RER datasets have: Unlike VQD, object detection cannot deal with attributes and relations.", "labels": [], "entities": [{"text": "RER datasets", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.8117862641811371}, {"text": "object detection", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8064511716365814}]}, {"text": "In VQA, often algorithms produce the right answers due to dataset bias without 'looking' at relevant image regions.", "labels": [], "entities": []}, {"text": "RER datasets have short and often ambiguous prompts, and by having only a single box as an output, they make it easier to exploit dataset biases.", "labels": [], "entities": [{"text": "RER datasets", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7199529111385345}]}, {"text": "VQD requires goal-directed object detection and outputting a variable number of boxes that answer a query.", "labels": [], "entities": [{"text": "VQD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9242731332778931}]}, {"text": "multiple annotation problems and have significant language bias problems.", "labels": [], "entities": []}, {"text": "VQD is also related to Visual Question Answering (VQA), where the task is to answer questions about images in natural language ().", "labels": [], "entities": [{"text": "VQD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8198500871658325}, {"text": "Visual Question Answering (VQA)", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7800246079762777}]}, {"text": "The key difference is that in VQD the algorithm must generate image bounding boxes that satisfy the query, making it less prone to the forms of bias that plague VQA datasets.", "labels": [], "entities": [{"text": "VQA datasets", "start_pos": 161, "end_pos": 173, "type": "DATASET", "confidence": 0.9205755889415741}]}, {"text": "We make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We describe the first dataset for VQD, which will be publicly released.", "labels": [], "entities": [{"text": "VQD", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8425323963165283}]}, {"text": "2. We evaluate multiple baselines on our VQD dataset.", "labels": [], "entities": [{"text": "VQD dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9366266429424286}]}], "datasetContent": [{"text": "Our experiments are designed to probe the behavior of models on VQD compared to RER datasets.", "labels": [], "entities": [{"text": "VQD", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8704571723937988}, {"text": "RER datasets", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7790130376815796}]}, {"text": "To facilitate this, we created a variant of our VQDv1 dataset that had only a single correct bounding box.", "labels": [], "entities": [{"text": "VQDv1 dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9815420806407928}]}, {"text": "To evaluate performance for the RER and '1 Obj' version of the VQDv1 dataset, systems only output a single bounding box during test time, so the Precision@1 metric is used.", "labels": [], "entities": [{"text": "RER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.836631178855896}, {"text": "VQDv1 dataset", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9828516840934753}, {"text": "Precision@1 metric", "start_pos": 145, "end_pos": 163, "type": "METRIC", "confidence": 0.9307133555412292}]}, {"text": "For the '0-N Obj' version of the VQDv1 dataset, we use the standard PASCAL VOC metric AP IoU =.50 from object detection, which calculates the average precision across the dataset using an intersection over union (IoU) greater than 0.5 criteria for matching with the ground truth boxes.", "labels": [], "entities": [{"text": "VQDv1 dataset", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9863438904285431}, {"text": "PASCAL VOC metric AP IoU", "start_pos": 68, "end_pos": 92, "type": "METRIC", "confidence": 0.6237355172634125}, {"text": "object detection", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.7234438061714172}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9609848856925964}]}], "tableCaptions": [{"text": " Table 1: VQDv1 Query Types", "labels": [], "entities": [{"text": "VQDv1", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8600316643714905}]}, {"text": " Table 2: VQDv1 compared to RER datasets.", "labels": [], "entities": [{"text": "VQDv1", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7286080121994019}, {"text": "RER datasets", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7294566333293915}]}, {"text": " Table 3: Results on RER datasets and two versions of our VQD dataset. The '1 Obj' version is trained and evaluated  on queries with only a single box, analogous to RER, and the 0-N version contains the entire VQD dataset. All  models use the same object proposals and visual features.", "labels": [], "entities": [{"text": "RER datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.6880475133657455}, {"text": "VQD dataset", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9253682196140289}, {"text": "VQD dataset", "start_pos": 210, "end_pos": 221, "type": "DATASET", "confidence": 0.9545252919197083}]}]}