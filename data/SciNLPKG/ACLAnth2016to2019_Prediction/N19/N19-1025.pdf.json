{"title": [{"text": "Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling", "labels": [], "entities": [{"text": "Text Modeling", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.7464302182197571}]}], "abstractContent": [{"text": "Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7567476332187653}, {"text": "text generation", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7584379315376282}]}, {"text": "These models often face a difficult optimization problem, also known as the Kullback-Leibler (KL) term vanishing issue , where the posterior easily collapses to the prior, and the model will ignore latent codes in generative tasks.", "labels": [], "entities": []}, {"text": "To address this problem, we introduce an improved Wasserstein Varia-tional Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.7837092578411102}]}, {"text": "The RNF transforms a latent variable into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the non-informative prior.", "labels": [], "entities": []}, {"text": "The Wasserstein objective minimizes the distance between the marginal distribution and the prior directly, and therefore does not force the posterior to match the prior.", "labels": [], "entities": []}, {"text": "Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performances in tasks such as language modeling, likelihood approximation, and text generation.", "labels": [], "entities": [{"text": "KL vanishing", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.8167446553707123}, {"text": "language modeling", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7452413141727448}, {"text": "likelihood approximation", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.7859522700309753}, {"text": "text generation", "start_pos": 181, "end_pos": 196, "type": "TASK", "confidence": 0.782846212387085}]}, {"text": "Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Variational Autocoder (VAE) is a probabilistic generative model shown to be successful over a wide range of tasks such as image generation (, dialogue generation (), transfer learning (, and classification ().", "labels": [], "entities": [{"text": "image generation", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7598101794719696}, {"text": "dialogue generation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7263752073049545}, {"text": "transfer learning", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.9414510726928711}]}, {"text": "The encoderdecoder architecture of VAE allows it to learn a continuous space of latent representations from high-dimensional data input and makes sampling procedure from such latent space very straightforward.", "labels": [], "entities": [{"text": "VAE", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7941485643386841}]}, {"text": "Recent studies also show that VAE learns meaningful representations that encode non-trivial information from input ().", "labels": [], "entities": [{"text": "VAE", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9215701818466187}]}, {"text": "Applications of VAE in tasks of Natural Language Processing () is not as successful as those in Computer Vision.", "labels": [], "entities": [{"text": "VAE", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7000606656074524}, {"text": "Computer Vision", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7070243805646896}]}, {"text": "With longshort-term-memory network (LSTM) (Hochreiter and) used as encoderdecoder model, the recurrent variational autoencoder () is the first approach that applies VAE to language modeling tasks.", "labels": [], "entities": [{"text": "VAE", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.8643660545349121}, {"text": "language modeling tasks", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7450329462687174}]}, {"text": "They observe that LSTM decoder in VAE often generates texts without making use of latent representations, rendering the learned codes as useless.", "labels": [], "entities": [{"text": "VAE", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8436915278434753}]}, {"text": "This phenomenon is caused by an optimization problem called KL-divergence vanishing when training VAE for text data, where the KL-divergence term in VAE objective collapses to zero.", "labels": [], "entities": []}, {"text": "This makes the learned representations meaningless as zero KL-divergence indicates that the latent codes are independent of input texts.", "labels": [], "entities": []}, {"text": "Many recent studies are proposed to address this key issue.; use convolutional neural network as decoder architecture to limit the expressiveness of decoder model.;) seek to learn different latent space and modify the learning objective.", "labels": [], "entities": []}, {"text": "And, even though not designed to tackle KL vanishing at the beginning, recent studies on Normalizing Flows () learn meaningful latent space as it helps to transform an over-simplified latent distribution into more flexible distributions.", "labels": [], "entities": [{"text": "KL vanishing", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.8873203992843628}]}, {"text": "In this paper, we propose anew type of flow, called Riemannian Normalizing Flow (RNF), together with the recently developed Wasserstein objective (, to ensure VAE models more robust against the KL vanishing problem.", "labels": [], "entities": []}, {"text": "As further explained in later sections, the Wasserstein objective helps to alleviate KL vanishing as it only minimizes the distance between latent marginal distribution and the prior.", "labels": [], "entities": [{"text": "KL vanishing", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.6836316734552383}]}, {"text": "Moreover, we suspect that the problem also comes from the over-simplified prior assumption about latent space.", "labels": [], "entities": []}, {"text": "In most cases, the prior is assumed to be a standard Gaussian, and the posterior is assumed to be a diagonal Gaussian for computational efficiency.", "labels": [], "entities": []}, {"text": "These assumptions, however, are not suitable to encode intrinsic characteristics of input into latent codes as in reality the latent space is likely to be far more complex than a diagonal Gaussian.", "labels": [], "entities": []}, {"text": "The RNF model we proposed in this paper thus helps the situation by encouraging the model to learn a latent space that encodes some geometric properties of input space with a well-defined geometric metric called Riemannian metric tensor.", "labels": [], "entities": []}, {"text": "This renders the KL vanishing problem as impossible since a latent distribution that respects input space geometry would only collapse to a standard Gaussian when the input also follows a standard Gaussian, which is never the case for texts and sentences datasets.", "labels": [], "entities": []}, {"text": "We then empirically evaluate our RNF Variational Wasserstein Autoencoder on standard language modeling datasets and show that our model has achieved state-of-the-art performances.", "labels": [], "entities": []}, {"text": "Our major contributions can be summarized as the following: \u2022 We propose Riemannian Normalizing Flow, anew type of flow that uses the Riemannian metric to encourage latent codes to respect geometric characteristics of input space.", "labels": [], "entities": []}, {"text": "\u2022 We introduce anew Wasserstein objective for text modeling, which alleviates KL divergence term vanishing issue, and makes the computation of normalizing flow easier.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.8871493339538574}, {"text": "KL divergence term vanishing", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.8315306901931763}]}, {"text": "\u2022 Empirical studies show that our model produces state-of-the-art results in language modeling and is able to generate meaningful text sentences that are more diverse.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7148455381393433}]}], "datasetContent": [{"text": "In this section, we investigate WAE's performance with Riemannian Normalizing Flow over language and text modeling.", "labels": [], "entities": [{"text": "WAE", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9935571551322937}]}, {"text": "We use Penn Treebank (), Yelp 13 reviews (, as in (, and Yahoo Answers used in () to follow and compare with prior studies.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9949241280555725}, {"text": "Yelp 13 reviews", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.7754663030306498}]}, {"text": "We limit the maximum length of a sample from all datasets to 200 words.", "labels": [], "entities": []}, {"text": "The datasets statistics is shown in.", "labels": [], "entities": []}, {"text": "For each model, we set the maximum vocabulary size to 20K and the maximum length of input to 200 across all data sets.", "labels": [], "entities": []}, {"text": "Following Bowman et al., we use one-layer undirectional  LSTM for both encoder-decoder models with hidden size 200.", "labels": [], "entities": []}, {"text": "Latent codes dimension is set to 32 for all models.", "labels": [], "entities": [{"text": "Latent codes dimension", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.7271909713745117}]}, {"text": "We share Word Embeddings of size 200.", "labels": [], "entities": []}, {"text": "For stochastic encoders, both M LP \u00b5 and M LP \u03c3 are two layer fully-connected networks with hidden size 200 and a batch normalizing output layer.", "labels": [], "entities": []}, {"text": "We use Adam () with learning rate set to 10 \u22123 to train all models.", "labels": [], "entities": []}, {"text": "Dropout is used and is set to 0.2.", "labels": [], "entities": [{"text": "Dropout", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8376938700675964}]}, {"text": "We train all models for 48 epochs, each of which consists of 2K steps.", "labels": [], "entities": []}, {"text": "For models other than WAE, KL-annealing is applied and is scheduled from 0 to 1 at the 21st epoch.", "labels": [], "entities": [{"text": "KL-annealing", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9836527109146118}]}, {"text": "For vmf-VAE (, we set the word embedding dimension to be 512 and the hidden units to 1024 for Yahoo, and set both of them to 200 for PTB and Yelp.", "labels": [], "entities": [{"text": "PTB", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.959267258644104}, {"text": "Yelp", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.6944726705551147}]}, {"text": "The temperature \u03ba is set to 80 and is kept constant during training.", "labels": [], "entities": []}, {"text": "For all WAE models, we add a small KL divergence term to control the posterior distribution.", "labels": [], "entities": [{"text": "WAE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9272379875183105}]}, {"text": "We found that if we only use RNF with MMD as the distance metric, then the posterior may diverge from the prior such that no reasonable samples can be generated from a standard Gaussian variable.", "labels": [], "entities": []}, {"text": "Hence, for all data sets, we schedule the KL divergence weight \u03b1 from 0 to 0.8, and the weight of the MMD term is set as \u03bb = 10 \u2212 \u03b1. \u03b2 k of RBF is set to 10 for all models.", "labels": [], "entities": [{"text": "KL divergence weight \u03b1", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.7853527218103409}]}, {"text": "For RNF, we use pretrained standard VAE models to gather the clusters ck , k = 1, ..., K, of latent codes, where we set the number of clusters to be 20.", "labels": [], "entities": [{"text": "RNF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9341464638710022}]}, {"text": "We use three normalizing flow for all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language Modeling results on PTB, YAHOO and YELP 13 Reviews. ** are results gathered from (Yang  et al., 2017; Xiao et al., 2018; He et al., 2019). Negative log-likelihood (NLL) is approximated by its lower bound,  where the number in parentheses indicates KL-divergence. NF stands for the standard planar normalizing flow  without Riemannian curvature.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7216146886348724}, {"text": "PTB", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9187330603599548}, {"text": "Negative log-likelihood (NLL)", "start_pos": 158, "end_pos": 187, "type": "METRIC", "confidence": 0.6614697754383088}]}, {"text": " Table 2: Language Modeling using WAE-RNF. We report NLL, PPL, Sum of Log Jacobian, and KL divergence  between q(z |x) and p(z ).", "labels": [], "entities": [{"text": "Sum", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9513252377510071}]}, {"text": " Table 3: Datasets statistics; The numbers reflect size  of each dataset. Vocab is the vocabulary size.", "labels": [], "entities": []}]}