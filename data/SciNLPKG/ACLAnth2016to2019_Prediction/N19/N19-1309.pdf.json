{"title": [{"text": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6905074914296468}]}], "abstractContent": [{"text": "Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade.", "labels": [], "entities": [{"text": "Open Information Extraction (OpenIE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7589782178401947}]}, {"text": "However, this research has largely ignored the vast quantity of facts available in semi-structured webpages.", "labels": [], "entities": []}, {"text": "In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it.", "labels": [], "entities": []}, {"text": "We also introduce a labeled evaluation dataset to motivate research in this area.", "labels": [], "entities": []}, {"text": "Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7892119288444519}]}, {"text": "We then use this training data to learn a classifier for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8921530246734619}]}, {"text": "Experimental results of this method on our new benchmark dataset obtained a precision of over 70%.", "labels": [], "entities": [{"text": "benchmark dataset", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.6652634590864182}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9991934895515442}]}, {"text": "A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge extraction is the problem of extracting (subject, predicate, object) triples from unstructured or semi-structured data, where the subject and object are entities and the predicate indicates the relationship between them.", "labels": [], "entities": [{"text": "Knowledge extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7544071972370148}]}, {"text": "In conventional information extraction (which we call \"ClosedIE\"), a closed set of potential predicates and their semantics are pre-defined in an ontology.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.728424072265625}]}, {"text": "Open Information Extraction (OpenIE) is an alternative approach that has no pre-defined ontology and instead represents the predicate with a string extracted from the source data.", "labels": [], "entities": [{"text": "Open Information Extraction (OpenIE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.736793984969457}]}, {"text": "These extractions can capture a much vaster array of semantic relationships than ClosedIE and have been used to support many downstream use-cases, including questionanswering, ontology discovery, embedding generation, fact checking, and summarization.", "labels": [], "entities": [{"text": "ontology discovery", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7721028029918671}, {"text": "embedding generation", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.7555662393569946}, {"text": "fact checking", "start_pos": 218, "end_pos": 231, "type": "TASK", "confidence": 0.8694190382957458}, {"text": "summarization", "start_pos": 237, "end_pos": 250, "type": "TASK", "confidence": 0.9866839051246643}]}, {"text": "Previous OpenIE work has concentrated on raw text, with the aim to extract \"open\" triples from natural language sentences (, with another line of work focused on extracting from webtables (.", "labels": [], "entities": []}, {"text": "Semi-structured websites (e.g. IMDb) contain many pages displaying information in stand-alone fields in relatively consistent locations on each page, with entities and the relationship between them indicated via formatting features such as section headers and lists of key-value pairs.", "labels": [], "entities": []}, {"text": "Figure 1 shows an example page and the triples it conveys.", "labels": [], "entities": []}, {"text": "Semi-structured websites have recently been shown to be a rich target for IE; the Knowledge Vault large-scale web extraction experiment, which extracted from semi-structured websites, natural language text, webtables, and Semantic Web annotations, found that semi-structured websites contributed 75% of total extracted facts and 94% of high-confidence extractions (); the Ceres system showed that one can automatically extract from semi-structured sites with a precision over 90% using distant supervision ().", "labels": [], "entities": [{"text": "IE", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9965408444404602}, {"text": "Knowledge Vault large-scale web extraction", "start_pos": 82, "end_pos": 124, "type": "TASK", "confidence": 0.4997915387153625}, {"text": "precision", "start_pos": 461, "end_pos": 470, "type": "METRIC", "confidence": 0.9848945736885071}]}, {"text": "These works, however, all build on the tradition of semi-structured ClosedIE techniques ().", "labels": [], "entities": []}, {"text": "Interestingly, we are not aware of any work that applies OpenIE on semi-structured sources, despite the great potential to identify new relationships and new knowledge triples.", "labels": [], "entities": []}, {"text": "We investigated 8 movie websites from the Structured Web Data Extraction (SWDE) corpus) and found that the IMDb ontology can cover only 7% of semantically unique predicates on these sites.", "labels": [], "entities": [{"text": "Structured Web Data Extraction (SWDE) corpus", "start_pos": 42, "end_pos": 86, "type": "DATASET", "confidence": 0.7026915736496449}]}, {"text": "The major challenges that distinguish natural language text and semi-structured data are the basic unit and the inherent structure of the data.", "labels": [], "entities": []}, {"text": "In natural language text, each sentence is a unit; it typically consists of a subject, a verb, and an object, which corresponds naturally to the subject, predicate, and object of a knowledge triple.", "labels": [], "entities": []}, {"text": "Similarly, in webtables, each table is a unit; its rows, columns, and cells also naturally correspond to subjects, predicates, and objects in triples.", "labels": [], "entities": []}, {"text": "In the semi-structured setting, the basic unit is the webpage, which may contain hundreds or thousands of entity mentions.", "labels": [], "entities": []}, {"text": "There is no fixed layout between the subject entity, object entity, and their relation, which maybe far apart on the webpage.", "labels": [], "entities": []}, {"text": "For example, contains object strings that are below, to the left, and to the right of their corresponding predicates; even trickier, for object string \"Uma Thurman\", the correct predicate string \"Cast\" is much farther away than the incorrect one \"Crew\".", "labels": [], "entities": []}, {"text": "Despite the challenges, semistructured pages do provide inherent visual structure to help distinguish the subject of a page, and (predicate, object) pairs for the subject.", "labels": [], "entities": []}, {"text": "In this paper we answer the following question: Given semistructured webpages in a website, how can we tell which field contains the subject, and which fields contain the (predicate, object) pairs for the subject through any visual or DOM-structured clue?", "labels": [], "entities": []}, {"text": "This paper makes three contributions.", "labels": [], "entities": []}, {"text": "Our first contribution is to formally define anew problem of OpenIE from semi-structured websites (Section 2).", "labels": [], "entities": []}, {"text": "We created a benchmark 1 for this problem by enhancing the SWDE corpus; our benchmark contains a high accuracy and coverage set of ground truth extractions for 21 websites spanning three domains, comprising 855,748 labels across 27,641 pages (Section 4).", "labels": [], "entities": [{"text": "SWDE corpus", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.8016608655452728}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9988077878952026}, {"text": "coverage", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9217307567596436}]}, {"text": "Our second contribution is OpenCeres, a solution for OpenIE on semi-structured websites (Section 3).", "labels": [], "entities": []}, {"text": "Our solution is novel in three aspects.", "labels": [], "entities": []}, {"text": "First, whereas ClosedIE techniques on semi-structured data focus on extracting objects forgiven predicates, we also identify predicate strings on the website that represent the relations.", "labels": [], "entities": []}, {"text": "Second, while ClosedIE techniques can only learn extraction patterns for predicates where there exists seed knowledge, we identify unseen predicates by applying semi-supervised label propagation.", "labels": [], "entities": []}, {"text": "Third, whereas most existing extraction techniques on semi-structured sites leverage only DOM patterns as evidence, we use visual aspects of the webpage  for distant supervision and label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 182, "end_pos": 199, "type": "TASK", "confidence": 0.7624000608921051}]}, {"text": "Our final contribution is a comprehensive evaluation on our new benchmark dataset and online websites (Section 5).", "labels": [], "entities": []}, {"text": "Our proposed method obtained an F1 of 0.68, significantly higher than baseline systems, while extracting 7 times as many predicates as were present in the original ontology.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9996991157531738}]}, {"text": "In addition, we evaluate on a set of 31 movie websites, yielding 1.17 million extractions at a precision of 0.70.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9904696941375732}]}, {"text": "Our results inspire new directions for improvement as discussed in Section 7, and serve as a good baseline for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: Our primary dataset is the augmented SWDE corpus described in Section 4.", "labels": [], "entities": [{"text": "SWDE corpus", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.7807348668575287}]}, {"text": "In addition, we used the set of 31 5 movie websites (comprising 433,000 webpages) found in CommonCrawl  ing the original SWDE ground truth for websites CollegeBoard and ESPN to create a KB for the University and NBAPlayer domains respectively.", "labels": [], "entities": [{"text": "CommonCrawl", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9467633366584778}]}, {"text": "Implementations: We compared OpenCeres with two baselines.", "labels": [], "entities": []}, {"text": "The three algorithms apply the same method to extract topic subjects but differ in how they extract (predicate, object) pairs.", "labels": [], "entities": []}, {"text": "1. WEIR: Proposed by, the Web Extraction and Integration of Redundant data (WEIR) approach takes as input a set of websites in the same subject domain and makes use of overlap in observed entities across sites to learn extraction rules for predicates.", "labels": [], "entities": [{"text": "Web Extraction and Integration of Redundant data (WEIR)", "start_pos": 26, "end_pos": 81, "type": "TASK", "confidence": 0.6972364991903305}]}, {"text": "The system is unsupervised, though it does require a dictionary of potential page topic entities for the domain to align pages between sites.", "labels": [], "entities": []}, {"text": "WEIR also contains a method for automatically identifying predicate strings for the extraction rules it learned by finding strings that frequently occur nearby extracted objects in the HTML templates of sites in the domain.", "labels": [], "entities": [{"text": "WEIR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8769729733467102}]}, {"text": "2. Colon Baseline: Semi-structured pages frequently represent a (predicate, object) pair via a set of adjacent DOM nodes, with the predicate string ending in a colon and the object string either to its right or below it.", "labels": [], "entities": []}, {"text": "This baseline starts with the (predicate, object) candidate pairs generated in Section 3.1, identifies those where the predicate field ends with a colon, and extracts them as a predicate along with their closest candidate object either to their right or below.", "labels": [], "entities": []}, {"text": "3. OpenCeres: This implements our system exactly as described in Section 3, using the generated training data to train a Ceres extractor.", "labels": [], "entities": []}, {"text": "In addition, to understand the uper bound of OpenCeres, we implemented two versions using ground truth data for training seeds: 4.", "labels": [], "entities": []}, {"text": "OpenCeres-Gold: This implements our system, but skips the label propagation step and replaces noisy seed labels (Section 3.2) with samples from ground truth triples.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7415885627269745}]}, {"text": "We sampled 25% of triples for each predicate, so this method is essentially ClosedIE Ceres with incomplete but clean training labels, giving an upper bound on the system's performance when no errors are introduced during training data generation and label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 250, "end_pos": 267, "type": "TASK", "confidence": 0.7332859039306641}]}], "tableCaptions": [{"text": " Table 1. We enhanced SWDE in two ways.  First, SWDE on average contains 4,480 triples for  3 predicates from these 21 websites, whereas we  have an average of 41K triples for 36 predicates.  The number of predicates per website ranges from  5 to 272 (Hollywood features very fine-grained re- lationships like \"Assistant Art Director\"). Second,  when multiple predicate strings may apply on the  webpage, we list all of them in order of specificity.  Taking", "labels": [], "entities": []}, {"text": " Table 1: Statistics of the augmented SWDE dataset.", "labels": [], "entities": [{"text": "SWDE dataset", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8132517635822296}]}, {"text": " Table 2: Extraction precision and recall (lenient) on  SWDE domains. OpenCeres on average improves over  baseline by 36% on precision and by 88% on recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.920331597328186}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994356036186218}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9990586638450623}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.998120129108429}]}, {"text": " Table 3: Detailed results of OpenCeres using lenient  scoring, with strict scoring results shown in parenthe- ses.", "labels": [], "entities": [{"text": "OpenCeres", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9351888298988342}]}, {"text": " Table 4: Average number of candidate pairs produced  by considering k-nearest higher ranking text fields for  each candidate object on the SWDE-Movie dataset,  along with recall over true pairs.", "labels": [], "entities": [{"text": "SWDE-Movie dataset", "start_pos": 140, "end_pos": 158, "type": "DATASET", "confidence": 0.9408739805221558}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9986997842788696}]}]}