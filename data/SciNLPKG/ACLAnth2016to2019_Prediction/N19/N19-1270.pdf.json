{"title": [{"text": "Improving Machine Reading Comprehension with General Reading Strategies", "labels": [], "entities": [{"text": "Improving Machine Reading Comprehension", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9026703089475632}]}], "abstractContent": [{"text": "Reading strategies have been shown to improve comprehension levels, especially for readers lacking adequate prior knowledge.", "labels": [], "entities": []}, {"text": "Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a deep language model via pre-training.", "labels": [], "entities": [{"text": "knowledge accumulation", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7259625196456909}]}, {"text": "Inspired by reading strategies identified in cognitive science, and given limited computational resources-just a pre-trained model and a fixed number of training instances-we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner.", "labels": [], "entities": [{"text": "non-extractive machine reading comprehension (MRC)", "start_pos": 225, "end_pos": 275, "type": "TASK", "confidence": 0.6769423314503261}, {"text": "BACK", "start_pos": 281, "end_pos": 285, "type": "METRIC", "confidence": 0.9896482825279236}, {"text": "FORTH READING", "start_pos": 290, "end_pos": 303, "type": "METRIC", "confidence": 0.8253792524337769}, {"text": "SELF-ASSESSMENT", "start_pos": 528, "end_pos": 543, "type": "METRIC", "confidence": 0.9795507192611694}]}, {"text": "By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8% absolute increase inaccuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies.", "labels": [], "entities": [{"text": "MRC dataset RACE", "start_pos": 142, "end_pos": 158, "type": "DATASET", "confidence": 0.8410436511039734}]}, {"text": "We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2% in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC).", "labels": [], "entities": [{"text": "MRC task", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.8948715925216675}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9911062121391296}, {"text": "MultiRC", "start_pos": 311, "end_pos": 318, "type": "DATASET", "confidence": 0.8550419807434082}]}, {"text": "These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of * This work was done when K.", "labels": [], "entities": []}, {"text": "S. was an intern at the Tencent AI Lab, Bellevue, WA.", "labels": [], "entities": []}, {"text": "our fine-tuned models that incorporate these strategies.", "labels": [], "entities": []}, {"text": "Core code is available at https: //github.com/nlpdata/strategy/.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For most of the hyperparameters, we follow the work of.", "labels": [], "entities": []}, {"text": "We use the same preprocessing procedure and the released pretrained transformer.", "labels": [], "entities": []}, {"text": "We generate 119k instances based on the reference documents from the training and development set of RACE (, with n q = 10, n s = 3, n c = 4, and n t = 4 (Section 3.4).", "labels": [], "entities": [{"text": "RACE", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8192639946937561}]}, {"text": "We first fine-tune the original pretrained model on these automatically generated instances with 1 training epoch (data flow 1 boxed in).", "labels": [], "entities": []}, {"text": "We then fine-tune the model on a large-scale general domain MRC dataset RACE with 5 training epochs (data flow 2 boxed in.", "labels": [], "entities": [{"text": "MRC dataset RACE", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.8165765603383383}]}, {"text": "Finally, we fine-tune the resulting model on one of the aforementioned six out-of-domain MRC datasets (at max 10 epochs).", "labels": [], "entities": [{"text": "MRC datasets", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8376452028751373}]}, {"text": "See data flow 3 boxed in.", "labels": [], "entities": []}, {"text": "When we fine-tune the model on different datasets, we set the batch size to 8, language model weight \u03bb to 2.", "labels": [], "entities": []}, {"text": "We ensemble models by averaging logits after the linear layer.", "labels": [], "entities": []}, {"text": "For strategy highlighting (Section 3.3), the contentword POS tagset T = {NN, NNP, NNPS, NNS, VB, VBD, VBG, VBN, VBP, VBZ, JJ, JJR, JJS, RB, RBR, RBS, CD, FW}, and we randomly initialize + and \u2212 .  In, we first report the accuracy of the state-of-the-art models (MMN and original finetuned GPT) and Amazon Turkers (Human performance).", "labels": [], "entities": [{"text": "strategy highlighting", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8017683327198029}, {"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9993488192558289}]}, {"text": "We then report the performance of our implemented fine-tuned GPT baselines and our models (GPT+Strategies).", "labels": [], "entities": [{"text": "GPT baselines", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.829724133014679}]}, {"text": "Results are shown on the RACE dataset () and its two subtasks: RACE-M collected from middle school exams and RACE-H collected from high school exams.", "labels": [], "entities": [{"text": "RACE dataset", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8144608438014984}, {"text": "RACE-M", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.7572298049926758}]}, {"text": "Our single and ensemble models outperform previous state-of-the-art (i.e., GPT and GPT (9\u00d7)) by a large margin (63.8% vs. 59.0%; 66.7% vs. 60.6%).", "labels": [], "entities": [{"text": "GPT", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.6452116966247559}, {"text": "GPT", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.4904784560203552}]}, {"text": "The two single-model strategiesself-assessment and highlighting -improve over the single-model fine-tuned GPT baseline (58.7%) by 1.7% and 4.5%, respectively.", "labels": [], "entities": []}, {"text": "Using the back and forth reading strategy, which involves two models, gives a 3.0% improvement inaccuracy compared to the ensemble of two original finetuned GPTs (59.6%).", "labels": [], "entities": []}, {"text": "Strategy combination further boosts the performance.", "labels": [], "entities": []}, {"text": "By combining selfassessment and highlighting, our single model achieves a 5.1% improvement inaccuracy over the fine-tuned GPT baseline (63.8% vs. 58.7%).", "labels": [], "entities": []}, {"text": "We apply all the strategies by ensembling two such single models that read an input sequence in either the original or the reverse order, leading to a 5.8% improvement inaccuracy over the ensemble of two original fine-tuned GPTs (65.4% vs. 59.6%).", "labels": [], "entities": []}, {"text": "To further analyze performance, we roughly divide the question types into five categories: de-tail (facts and details), inference (reasoning ability), main (main idea or purpose of a document), attitude (author's attitude toward a topic or tone/source of a document), and vocabulary (vocabulary questions) ( and annotate all the instances of the RACE development set.", "labels": [], "entities": [{"text": "RACE development", "start_pos": 346, "end_pos": 362, "type": "TASK", "confidence": 0.8087576329708099}]}, {"text": "As shown in, compared to the fine-tuned GPT baseline, our single-model strategies (SA and HL) consistently improve the results across all categories.", "labels": [], "entities": [{"text": "GPT baseline", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7746458649635315}, {"text": "SA", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9467039108276367}]}, {"text": "Compared to other strategies, highlighting is likely to lead to bigger gains for most question types.", "labels": [], "entities": []}, {"text": "Compared to human performance, there is still a considerable room for improvements, especially on RACE-M.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 98, "end_pos": 104, "type": "TASK", "confidence": 0.6669682860374451}]}, {"text": "We take a close look at the instances from the RACE-M development set that all our implementations fail to answer correctly.", "labels": [], "entities": [{"text": "RACE-M development set", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.815153956413269}]}, {"text": "We notice that 82.0% of them require one or multiple types of world knowledge (e.g., negation resolution, commonsense, paraphrase, and mathematical/logic knowledge (), especially when correct answer options are not explicitly mentioned in the reference document.", "labels": [], "entities": [{"text": "negation resolution", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.9169719219207764}]}, {"text": "For example, we need the knowledgethe type of thing that is written by a writer can probably be a book -to answer the question \"follow your heart is a \" from the context \"Follow your heart by Andrew Matthews, an Australian writer, tells us that making our dreams real is life's biggest challenge\".", "labels": [], "entities": []}, {"text": "Besides, 19.7% of these failed instances require coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.9153434932231903}]}, {"text": "It might be promising to leverage coreference resolvers to connect nonadjacent relevant sentences.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9089322984218597}]}, {"text": "Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing ( , which require a large amount of training data or limited by the number of training instances (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.787044107913971}]}, {"text": "In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document.", "labels": [], "entities": [{"text": "question and answer options) generation", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.64126588900884}]}], "tableCaptions": [{"text": " Table 1: Statistics of multiple-choice machine reading comprehension datasets. Some values come from Reddy  et al. (2018), Ko\u010disk`Ko\u010disk`y et al. (2018), and Lai et al. (2017) (crowd.: crowdsourcing;  \u2020 : regarding each sentence/claim  as a document (Clark et al., 2018); : correct answer options that are not text snippets from reference documents).", "labels": [], "entities": []}, {"text": " Table 2: Accuracy (%) on the test set of RACE (#:  number of (ensemble) models; SA: Self-Assessment;  HL: Highlighting; BF: Back and Forth Reading; : our  implementation).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976062774658203}, {"text": "RACE", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.8873352408409119}, {"text": "BF", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9563045501708984}]}, {"text": " Table 3: Performance (%) on the test sets of ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, and ROCSto- ries and the development set of MultiRC (Acc.: Accuracy; F1 m : macro-average F1; F1 a : micro-average F1;  \u2020 :  using the joint exact match accuracy (i.e., EM 0 reported by the official evaluation (Khashabi et al., 2018))). RACE  is used as the source task for all our implementations.", "labels": [], "entities": [{"text": "ARC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9438634514808655}, {"text": "MultiRC", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.857446014881134}, {"text": "Acc.", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9905468225479126}, {"text": "Accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9540480971336365}, {"text": "F1 m : macro-average F1; F1 a : micro-average F1", "start_pos": 162, "end_pos": 210, "type": "METRIC", "confidence": 0.7209104733033613}, {"text": "exact match accuracy", "start_pos": 234, "end_pos": 254, "type": "METRIC", "confidence": 0.7822966774304708}, {"text": "EM 0", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9696609079837799}, {"text": "RACE", "start_pos": 330, "end_pos": 334, "type": "TASK", "confidence": 0.9305058717727661}]}]}