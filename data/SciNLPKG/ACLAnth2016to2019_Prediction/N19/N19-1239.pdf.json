{"title": [{"text": "Open Information Extraction from Question-Answer Pairs", "labels": [], "entities": [{"text": "Open Information Extraction from Question-Answer Pairs", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7769339879353842}]}], "abstractContent": [{"text": "Open Information Extraction (OPENIE) extracts meaningful structured tuples from free-form text.", "labels": [], "entities": [{"text": "Open Information Extraction (OPENIE) extracts meaningful structured tuples from free-form text", "start_pos": 0, "end_pos": 94, "type": "TASK", "confidence": 0.8143820808484004}]}, {"text": "Most previous work on OPENIE considers extracting data from one sentence at a time.", "labels": [], "entities": [{"text": "OPENIE", "start_pos": 22, "end_pos": 28, "type": "TASK", "confidence": 0.7149688601493835}]}, {"text": "We describe NEURON, a system for extracting tuples from question-answer pairs.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.8153519034385681}, {"text": "extracting tuples from question-answer pairs", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.7839684963226319}]}, {"text": "Since real questions and answers often contain precisely the information that users care about, such information is particularly desirable to extend a knowledge base with.", "labels": [], "entities": []}, {"text": "First, an answer text is often hard to understand without knowing the question, and second, relevant information can span multiple sentences.", "labels": [], "entities": []}, {"text": "To address these, NEURON formulates extraction as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts.", "labels": [], "entities": [{"text": "NEURON formulates extraction", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.8189043998718262}]}, {"text": "We describe experiments on two real-world datasets that demonstrate that NEURON can find a significant number of new and interesting facts to extend a knowledge base compared to state-of-the-art OPENIE methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (OPENIE) ( ) is the problem of extracting structured data from a text corpus, without knowing a priori which relations will be extracted.", "labels": [], "entities": [{"text": "Open Information Extraction (OPENIE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7491166144609451}]}, {"text": "It is one of the primary technologies used in building knowledge bases (KBs) that, in turn, power question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7856700122356415}]}, {"text": "The vast majority of previous work on OPENIE extracts structured information (e.g., triples) from individual sentences.", "labels": [], "entities": [{"text": "OPENIE extracts structured information (e.g., triples) from individual sentences", "start_pos": 38, "end_pos": 118, "type": "TASK", "confidence": 0.7485817223787308}]}, {"text": "This paper addresses the problem of extracting structured data from conversational questionanswer (CQA) data.", "labels": [], "entities": [{"text": "extracting structured data from conversational questionanswer (CQA)", "start_pos": 36, "end_pos": 103, "type": "TASK", "confidence": 0.7489245004124112}]}, {"text": "Often, CQA data contains precisely the knowledge that users care about.", "labels": [], "entities": []}, {"text": "As such, this data offers a goal-directed method for extending existing knowledge bases.", "labels": [], "entities": []}, {"text": "Consider, for example, a KB about a hotel that is used to power its website and/or a conversational interface for hotel guests.", "labels": [], "entities": []}, {"text": "The KB provides information about the hotel's services: complimentary breakfast, free wifi, spa.", "labels": [], "entities": [{"text": "The KB", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8281427323818207}]}, {"text": "However, it may not include information about the menu/times for the breakfast, credentials for the wifi, or the cancellation policy fora spa appointment at the hotel.", "labels": [], "entities": []}, {"text": "Given the wide range of information that maybe of interest to guests, it is not clear how to extend the KB in the most effective way.", "labels": [], "entities": [{"text": "the KB", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.8644649684429169}]}, {"text": "However, the conversational logs, which many hotels keep, contain the actual questions from guests, and can therefore be used as a resource for extending the KB.", "labels": [], "entities": []}, {"text": "Following examples illustrate the kind of data we aim to extract: As can be seen from these examples, harvesting facts from CQA data presents significant challenges.", "labels": [], "entities": []}, {"text": "In particular, the system must interpret information collectively between the questions and answers.", "labels": [], "entities": []}, {"text": "In this case, it must realize that 'third floor' refers to the location of the 'gym' and that 6:00am refers to the opening time of the pool.", "labels": [], "entities": []}, {"text": "OPENIE systems that operate over individual sentences ignore the discourse and context in a QA pair.", "labels": [], "entities": []}, {"text": "Without knowing the question, they either fail to or incorrectly interpret the answer.", "labels": [], "entities": []}, {"text": "This paper describes NEURON, an end-to-end system for extracting information from CQA data.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.5711724162101746}]}, {"text": "We cast OPENIE from CQA as a multi-source sequence-to-sequence generation problem to explicitly model both the question and answer in a QA pair.", "labels": [], "entities": [{"text": "OPENIE", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9136771559715271}]}, {"text": "We propose a multi-encoder, constrained-decoder framework that uses two encoders to encode each of the question and answer to an internal representation.", "labels": [], "entities": []}, {"text": "The two representations are then used by a decoder to generate an output sequence corresponding to an extracted tuple.", "labels": [], "entities": []}, {"text": "For example, the output sequence of Example 2 is: arg 1 pool /arg 1 rel open /relarg 2 6:00am daily /arg 2 While encoder-decoder frameworks have been used extensively for machine translation and summarization, there are two key technical challenges in extending them for information extraction from CQA data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8273045718669891}, {"text": "summarization", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.9042972922325134}, {"text": "information extraction from CQA", "start_pos": 271, "end_pos": 302, "type": "TASK", "confidence": 0.8303827792406082}]}, {"text": "First, it is vital for the translation model to learn constraints such as, arguments and relations are sub-spans from the input sequence, output sequence must have a valid syntax (e.g., arg 1 must precede rel).", "labels": [], "entities": []}, {"text": "These and other constraints can be integrated as hard constraints in the decoder.", "labels": [], "entities": []}, {"text": "Second, the model must recognize auxiliary information that is irrelevant to the KB.", "labels": [], "entities": []}, {"text": "For example, in the hotel application, NEURON must learn to discard greetings in the data.", "labels": [], "entities": []}, {"text": "Since existing facts in the KB are representative of the domain of the KB, this prior knowledge can be incorporated as soft constraints in the decoder to rank various output sequences based on their relevance.", "labels": [], "entities": []}, {"text": "Our contributions are summarized below: \u2022 We develop NEURON, a system for extracting information from CQA data.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.5920196175575256}, {"text": "CQA data", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.6655640155076981}]}, {"text": "NEURON is a novel multi-encoder constrained-decoder method that explicitly models both the question and the answer of a QA pair.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8107246160507202}]}, {"text": "It incorporates vocabulary and syntax as hard constraints and prior knowledge as soft constraints in the decoder.", "labels": [], "entities": []}, {"text": "\u2022 We conduct comprehensive experiments on two real-world CQA datasets.", "labels": [], "entities": [{"text": "CQA datasets", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8906944990158081}]}, {"text": "Our experimental results show that the use of hard and soft constraints improves the extraction accuracy and NEURON achieves the highest accuracy in extracting tuples from QA pairs compared with state-of-the-art sentence-based models, with a relative improvement as high as 13.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9856925010681152}, {"text": "NEURON", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.6676849126815796}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9983144998550415}]}, {"text": "NEU-RON's higher accuracy and ability to discover 15-25% tuples that are not extracted by stateof-the-art models make it suitable as a tuple extraction tool for KB extension.", "labels": [], "entities": [{"text": "NEU-RON", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.901316225528717}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994227886199951}, {"text": "tuple extraction", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7132436335086823}, {"text": "KB extension", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.8122299313545227}]}, {"text": "\u2022 We present a case study to demonstrate how a KB can be extended iteratively using tuples extracted using In each iteration, only relevant tuples are included in the KB.", "labels": [], "entities": []}, {"text": "In turn, the extended KB is used to improve relevance scoring for subsequent iterations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of NEURON on two CQA datasets.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.44854918122291565}, {"text": "CQA datasets", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9764925837516785}]}, {"text": "In our analysis, we find that integrating hard and soft constraints in the decoder improved the extraction performance irrespective of the number of encoders used.", "labels": [], "entities": []}, {"text": "Also, 15-25% of the tuples extracted by NEURON were not extracted by state-of-the-art sentence-based methods.", "labels": [], "entities": []}, {"text": "ConciergeQA is a real-world internal corpus of 33,158 QA pairs collected via a multi-channel communication platform for guests and hotel staff.", "labels": [], "entities": []}, {"text": "Questions (answers) are always made by guests (staff).", "labels": [], "entities": []}, {"text": "An utterance has 36 tokens on average, and there are 25k unique tokens in the dataset.", "labels": [], "entities": []}, {"text": "A QA utterance has 2.8 sentences on average, with the question utterance having 1.02 sentences on average and answer utterance having 1.78 sentences on average.", "labels": [], "entities": [{"text": "QA utterance", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.7596282958984375}]}, {"text": "AmazonQA () is a public dataset with 314,264 QA pairs about electronic products on amazon.com.", "labels": [], "entities": [{"text": "AmazonQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9460740685462952}]}, {"text": "The dataset contains longer and more diverse utterances than the ConciergeQA dataset: utterances have an average of 45 tokens and the vocabulary has more than 50k unique tokens.", "labels": [], "entities": [{"text": "ConciergeQA dataset", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9327119588851929}]}, {"text": "A QA utterance has 3.5 sentences on average.", "labels": [], "entities": [{"text": "QA utterance", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.8192286789417267}]}, {"text": "The question utterances had 1.5 sentences on average and the answer having 2 sentences.", "labels": [], "entities": []}, {"text": "For training NEURON, we bootstrapped a large number of high-quality training examples using a state-of-the-art OPENIE system.", "labels": [], "entities": [{"text": "training NEURON", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5406068861484528}, {"text": "OPENIE", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.7960873246192932}]}, {"text": "Such bootstrapping has been shown to be effective in information extraction tasks ().", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8965769211451212}]}, {"text": "The StanfordIE ( system is used to extract tuples from QA pairs for training examples.", "labels": [], "entities": [{"text": "StanfordIE", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8589220643043518}]}, {"text": "To further obtain high-quality tuples, we filtered out tuples that occur too infrequently (< 5) or too frequently (> 100).", "labels": [], "entities": []}, {"text": "For each tuple in the set, we retrieved all QA pairs that contain all the content words of the tuple and included them in the training set.", "labels": [], "entities": []}, {"text": "This helps create a training set encapsulating the multiplicity of ways in which tuples are expressed across QA pairs.", "labels": [], "entities": []}, {"text": "We randomly sampled 100 QA pairs from our bootstrapping set and found 74 of them supported the corresponding tuples.", "labels": [], "entities": []}, {"text": "We find this quality of bootstrapped dataset satisfactory, since the seed tuples  for bootstrapping could be noisy as they were generated by another OPENIE system.", "labels": [], "entities": []}, {"text": "Our bootstrapped dataset included training instances where a tuple matched (a) tokens in the questions exclusively, (b) tokens in the answers exclusively, (c) tokens from both questions and answers.", "labels": [], "entities": []}, {"text": "shows the distribution of the various types of training instances.", "labels": [], "entities": []}, {"text": "Less than 40% (30%) of ground truth tuples for ConciergeQA (AmazonQA) exclusively appear in the questions or answers.", "labels": [], "entities": []}, {"text": "Also, 22.1% (37.2%) of ground truth tuples for ConciergeQA (AmazonQA) are extracted from the combination of questions and answers.", "labels": [], "entities": []}, {"text": "These numbers support our motivation of extracting tuples from QA pairs.", "labels": [], "entities": []}, {"text": "We used standard techniques to construct training/dev/test splits so that QA pairs in the three sets are disjoint.", "labels": [], "entities": []}, {"text": "shows the details of the various subsets.", "labels": [], "entities": []}, {"text": "The BILSTM-CRF model showed extremely low  sequences (32-39%) could be converted to a tuple.", "labels": [], "entities": [{"text": "BILSTM-CRF", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9787302613258362}]}, {"text": "Most tagged sequences had multiple relations and arguments, indicating that it is difficult to learn how to tag a sequence corresponding to a tuple.", "labels": [], "entities": []}, {"text": "The model only learns how to best predict tags for each token in the sequence, and does not take into account the long-range dependencies to previously predicted tags.", "labels": [], "entities": []}, {"text": "This is still an open problem and is outside the scope of this paper.", "labels": [], "entities": []}, {"text": "show the performance of NEU-RALOPENIE and NEURON on the two CQA datasets.", "labels": [], "entities": [{"text": "NEU-RALOPENIE", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7513415217399597}, {"text": "NEURON", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.5562906861305237}, {"text": "CQA datasets", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9794265925884247}]}, {"text": "NEURON achieves higher precision on both the datasets.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5024191737174988}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9992530941963196}]}, {"text": "This is because NEURALOPENIE uses a single encoder to interpret the question and answer in the same vector space, which leads to lower performance.", "labels": [], "entities": [{"text": "NEURALOPENIE", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.4913216531276703}]}, {"text": "Furthermore, concatenating the question and answer makes the input sequence too long for the decoder to capture long-distance dependencies in history (.", "labels": [], "entities": []}, {"text": "Despite the attention mechanism, the model ignores past alignment information.", "labels": [], "entities": []}, {"text": "This makes it less effective than the dual-encoder model used in NEURON.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.733013391494751}]}, {"text": "The tables also show that incorporating taskspecific hard constraints helps further improve the overall precision and recall, regardless of the methods and the datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9993844032287598}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9993725419044495}]}, {"text": "Re-ranking the tuples based on the soft constraints derived from the existing KB further improves the performance of both methods in ConciergeQA and NEURALOPE-NIE in AmazonQA.", "labels": [], "entities": [{"text": "AmazonQA", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.9554164409637451}]}, {"text": "The existing KB also helps boost the likelihood of a correct candidate tuple sequence that was otherwise scored to be less likely.", "labels": [], "entities": [{"text": "KB", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.832956075668335}]}, {"text": "Lastly, we found that NEURON has significant relative coverage; it discovered significant additional, unique tuples missed by NEURALOPENIE.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.7976087927818298}, {"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.896104097366333}, {"text": "NEURALOPENIE", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.8853403925895691}]}, {"text": "shows a slight decrease in performance for NEURON after soft constraints are added.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.4321739971637726}]}, {"text": "This is likely caused by the lower quality KE model due to the larger vocabulary in AmazonQA.: Precision (P), Recall (R), and Relative Coverage (RC) results on AmazonQA dataset.", "labels": [], "entities": [{"text": "AmazonQA.", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.9702584147453308}, {"text": "Precision (P)", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9575559496879578}, {"text": "Recall (R)", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9597102105617523}, {"text": "Relative Coverage (RC)", "start_pos": 126, "end_pos": 148, "type": "METRIC", "confidence": 0.9139403820037841}, {"text": "AmazonQA dataset", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.9923204183578491}]}, {"text": "This is likely because the NEURALOPENIE model, at this stage, still had a larger margin for improvement.", "labels": [], "entities": [{"text": "NEURALOPENIE", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.40500760078430176}]}, {"text": "We note however that learning the best KE model is not the focus of this work.", "labels": [], "entities": []}, {"text": "AmazonQA is a more challenging dataset than ConciergeQA: longer utterances (avg.", "labels": [], "entities": [{"text": "AmazonQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.942327618598938}]}, {"text": "45 tokens vs. 36 tokens) and richer vocabulary (> 50k unique tokens vs. < 25k unique tokens).", "labels": [], "entities": []}, {"text": "This is reflected in lower precision and recall values of both the systems on the AmazonQA dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993639588356018}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9992881417274475}, {"text": "AmazonQA dataset", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.9947667717933655}]}, {"text": "While the performance of end-to-end extraction systems depends on the complexity and diversity of the dataset, incorporating hard and soft constraints alleviates the problem to some extent.", "labels": [], "entities": []}, {"text": "End-to-end extraction systems tend to outperform rule-based systems on extraction from CQA datasets.", "labels": [], "entities": [{"text": "End-to-end extraction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.639922097325325}, {"text": "CQA datasets", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8434625566005707}]}, {"text": "We observed that training data for ConciergeQA had a large number (> 750k) dependency-based pattern rules, of which < 5% matched more than 5 QA pairs.", "labels": [], "entities": []}, {"text": "The set of rules is too large, diverse and sparse to train an accurate rule-based extractor.", "labels": [], "entities": []}, {"text": "Even though our training data was generated by bootstrapping from a rule-based extractor StanfordIE, we found only 51.5% (30.7%) of correct tuples from NEURON exactly matched the tuples from StanfordIE in ConciergeQA (AmazonQA).", "labels": [], "entities": [{"text": "StanfordIE", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.9176783561706543}]}, {"text": "This indicates that NEURON combined information from question and answer, otherwise not accessible to sentence-wise extractors.", "labels": [], "entities": []}, {"text": "As an evidence, we found 11.4% (6.1%) of tuples were extracted from answers, 16.8% (5.0%) from questions, while 79.6% (82.5%) combined information from questions and answers in ConciergeQA (AmazonQA).", "labels": [], "entities": [{"text": "ConciergeQA (AmazonQA)", "start_pos": 177, "end_pos": 199, "type": "DATASET", "confidence": 0.7596798241138458}]}, {"text": "Multiple Encoders: Our motivation to use different encoders for questions and answers is based on the assumption that they use different vocabulary and semantics.", "labels": [], "entities": []}, {"text": "We found that there were 8k (72k) unique words in questions, 18k (114k) unique words in answers, and the Jaccard co- efficient between two vocabulary sets was 0.25 (0.25) in ConciergeQA (AmazonQA), indicating that two sources use significantly different vocabulary.", "labels": [], "entities": [{"text": "Jaccard co- efficient", "start_pos": 105, "end_pos": 126, "type": "METRIC", "confidence": 0.8012107759714127}]}, {"text": "Also, the same word can have different meanings depending on a speaker, and thus such words in the two sources should be embedded differently.", "labels": [], "entities": []}, {"text": "To visualize the embedding vectors of common words in ConciergeQA, we mapped them into 2D space using t-SNE.", "labels": [], "entities": [{"text": "ConciergeQA", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.914594829082489}]}, {"text": "shows that subjective words that represents speakers attitude (e.g., \"ready\", \"guests\", \"time\") had significantly different embeddings in the question and answer encoders.", "labels": [], "entities": []}, {"text": "In contrast, objective words such as menu, or activity (e.g., \"bacon\", \"cruise\", \"weekday\") had similar embeddings although the two encoders do not directly share the embedding parameters.", "labels": [], "entities": []}, {"text": "This indicates that multiple encoders not only capture the different meanings in questions and answers but also retain consistent meanings for words that keep the same meanings in the two sources.", "labels": [], "entities": []}, {"text": "Relevance Scoring: We compared with another NEURON model that uses HolE () for relevance scoring.", "labels": [], "entities": [{"text": "Relevance Scoring", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7158622145652771}, {"text": "HolE", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.957084596157074}, {"text": "relevance scoring", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7361740469932556}]}, {"text": "Both the HolE and TransE models achieved the same precision of 80.7%, with HolE achieving slightly higher recall (+1.4%).", "labels": [], "entities": [{"text": "TransE", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.7301194667816162}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9995597004890442}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9996817111968994}]}, {"text": "This suggests that incorporating relevance scoring in NEURON can robustly improve the extraction accuracy, regardless of the choice of the knowledge embedding method.", "labels": [], "entities": [{"text": "NEURON", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.5799159407615662}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9928736686706543}]}, {"text": "We also estimated the upper-bound precision by evaluating if the correct tuple was included in the top-500 candidates.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9866978526115417}]}, {"text": "The upper-bound precision was 85.0% on ConciergeQA, indicating that there is still room for improvement on incorporating relevance scoring.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9895274639129639}, {"text": "ConciergeQA", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8659908175468445}]}], "tableCaptions": [{"text": " Table 1: Various types of training instances.", "labels": [], "entities": []}, {"text": " Table 3: Precision (P), Recall (R), and Relative Cover- age (RC) results on ConciergeQA.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9470774531364441}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9605792164802551}, {"text": "Relative Cover- age (RC)", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.9704035009656634}]}, {"text": " Table 4: Precision (P), Recall (R), and Relative Cover- age (RC) results on AmazonQA dataset.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.95062555372715}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9614380151033401}, {"text": "Relative Cover- age (RC)", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.9694080523082188}, {"text": "AmazonQA dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9953831136226654}]}, {"text": " Table 5: Examples of successful cases (1 and 2) and  failed cases (3 and 4) from test data.", "labels": [], "entities": []}, {"text": " Table 6: Different errors N and B made by NEURON  (N ) and NEURALOPENIE (B) respectively.", "labels": [], "entities": []}]}