{"title": [{"text": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA", "labels": [], "entities": [{"text": "Visual Navigation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6105022430419922}, {"text": "QA", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.6047109961509705}]}], "abstractContent": [{"text": "We demonstrate the surprising strength of uni-modal baselines in multimodal domains, and make concrete recommendations for best practices in future research.", "labels": [], "entities": []}, {"text": "Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques.", "labels": [], "entities": []}, {"text": "We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to 29% absolute gain in performance over published baselines.", "labels": [], "entities": [{"text": "visual navigation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7125393599271774}]}], "introductionContent": [{"text": "Baselines should capture these regularities so that outperforming them indicates a model is actually solving a task.", "labels": [], "entities": []}, {"text": "In multimodal domains, bias can occur in any subset of the modalities.", "labels": [], "entities": []}, {"text": "To address this, we argue it is not sufficient for researchers to provide random or majority class baselines; instead we recommend presenting results for unimodal models.", "labels": [], "entities": []}, {"text": "We investigate visual navigation and question answering tasks, where agents move through simulated environments using egocentric (first person) vision.", "labels": [], "entities": [{"text": "visual navigation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7902195751667023}, {"text": "question answering tasks", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8344414631525675}]}, {"text": "We find that unimodal ablations (e.g., language only) in these seemingly multimodal tasks can outperform corresponding full models ( \u00a74.1).", "labels": [], "entities": []}, {"text": "This work extends observations made in both the Computer Vision ( and Natural Language ( communities that complex models often perform well by fitting to simple, unintended correlations in the data, bypassing the complex grounding and reasoning that experimenters hoped was necessary for their tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the visual navigation and egocentric question answering tasks, at each timestep an agent receives an observation and produces an action.", "labels": [], "entities": [{"text": "visual navigation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7942655980587006}, {"text": "egocentric question answering", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.5927723546822866}]}, {"text": "Actions can move the agent to anew location or heading   (e.g., turn left), or answer questions (e.g., answer 'brown').", "labels": [], "entities": []}, {"text": "At timestep t, a multimodal model M takes in a visual input Vt and language question or navigation command L to predict the next action at . The navigation models we examine also take in their action from the previous timestep, a t\u22121 , and 'minimally sensed' world information W specifying which actions are available (e.g., that forward is unavailable if the agent is facing a wall).", "labels": [], "entities": []}, {"text": "In each benchmark, M corresponds to the author's released code and training paradigm.", "labels": [], "entities": []}, {"text": "In addition to their full model, we evaluate the role of each input modality by removing those inputs and replacing them with zero vectors.", "labels": [], "entities": []}, {"text": "Formally, we define the full model and three ablations: corresponding to models with access to Action inputs, Vision inputs, and Language inputs.", "labels": [], "entities": []}, {"text": "These ablations preserve the architecture and number of parameters of M by changing only its inputs.", "labels": [], "entities": []}, {"text": "Across all benchmarks, unimodal baselines outperform baseline models used in or derived from the original works.", "labels": [], "entities": []}, {"text": "Navigating unseen environments, these unimodal ablations outperform their corresponding full models on the Matterport (absolute \u2191 2.5% success rate) and EQA (\u2193 0.06m distance to target).", "labels": [], "entities": [{"text": "Matterport", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.9739170074462891}, {"text": "absolute \u2191 2.5% success rate", "start_pos": 119, "end_pos": 147, "type": "METRIC", "confidence": 0.7826153635978699}, {"text": "EQA", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9455602765083313}]}], "tableCaptions": [{"text": " Table 1: Navigation success (Matterport, THOR-Nav)  (%) and remaining distance to target (EQA) (m). Best  unimodal in bold; better than reported baseline;  *  better  than full model.", "labels": [], "entities": [{"text": "Navigation success", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8465556204319}, {"text": "Matterport", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.9410008788108826}, {"text": "THOR-Nav)", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.796836644411087}, {"text": "remaining distance to target (EQA)", "start_pos": 61, "end_pos": 95, "type": "METRIC", "confidence": 0.9129921538489205}]}, {"text": " Table 2: Navigation results for Matterport when trained  using student forcing. Best unimodal in bold; better  than reported baseline.", "labels": [], "entities": [{"text": "Navigation", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8602673411369324}, {"text": "Matterport", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9405696988105774}]}, {"text": " Table 3: Final distances to targets (d T ) and minimum  distance from target achieved along paths (d min ) in  EQA navigation. Best unimodal in bold; better than  reported baseline;  *  better than full model;  \u2020 tied with  full model.", "labels": [], "entities": [{"text": "EQA navigation", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.7290391027927399}]}, {"text": " Table 4: Top-1 QA accuracy. Best unimodal in bold;  better than reported baseline.", "labels": [], "entities": [{"text": "QA", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.6216720938682556}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8897177577018738}]}]}