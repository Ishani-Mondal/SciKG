{"title": [{"text": "BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering", "labels": [], "entities": [{"text": "BAG", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8750790953636169}, {"text": "Multi-hop Reasoning Question Answering", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.783249020576477}]}], "abstractContent": [{"text": "Multi-hop reasoning question answering requires deep comprehension of relationships between various documents and queries.", "labels": [], "entities": [{"text": "Multi-hop reasoning question answering", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8502469509840012}]}, {"text": "We propose a Bi-directional Attention Entity Graph Convolutional Network (BAG), lever-aging relationships between nodes in an entity graph and attention information between a query and the entity graph, to solve this task.", "labels": [], "entities": []}, {"text": "Graph convolutional networks are used to obtain a relation-aware representation of nodes for entity graphs built from documents with multi-level features.", "labels": [], "entities": []}, {"text": "Bidirectional attention is then applied on graphs and queries to generate a query-aware nodes representation, which will be used for the final prediction.", "labels": [], "entities": []}, {"text": "Experimental evaluation shows BAG achieves state-of-the-art accuracy performance on the QAn-garoo WIKIHOP dataset.", "labels": [], "entities": [{"text": "BAG", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.7257554531097412}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993539452552795}, {"text": "QAn-garoo WIKIHOP dataset", "start_pos": 88, "end_pos": 113, "type": "DATASET", "confidence": 0.8172659277915955}]}], "introductionContent": [{"text": "Question Answering (QA) and Machine Comprehension (MC) tasks have drawn significant attention during the past years.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8573421835899353}, {"text": "Machine Comprehension (MC) tasks", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.7233224411805471}]}, {"text": "The proposal of large-scale single-document-based QA/MC datasets, such as SQuAD (, CNN/Daily mail (, makes training available for end-to-end deep neural models, such as BiDAF (,) and SAN (.", "labels": [], "entities": [{"text": "CNN/Daily mail", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.9213486909866333}]}, {"text": "However, gaps still exist between these datasets and real-world applications.", "labels": [], "entities": []}, {"text": "For example, reasoning is constrained to a single paragraph, or even part of it.", "labels": [], "entities": []}, {"text": "Extended work was done to meet practical demand, such as DrQA () answering a SQuAD question based on the whole Wikipedia instead of single paragraph.", "labels": [], "entities": [{"text": "DrQA () answering a SQuAD question based on the whole Wikipedia", "start_pos": 57, "end_pos": 120, "type": "DATASET", "confidence": 0.52855245904489}]}, {"text": "Besides, latest largescale datasets, e.g. TriviaQA ( and NarrativeQA), address this limitation by introducing multiple documents, ensuring reasoning cannot be done within local information.", "labels": [], "entities": []}, {"text": "Although those datasets are fairly challenging, reasoning are within one document.", "labels": [], "entities": []}, {"text": "In many scenarios, we need to comprehend the relationships of entities across documents before answering questions.", "labels": [], "entities": []}, {"text": "Therefore, reading comprehension tasks with multiple hops were proposed to make it available for machine to tackle such problems, e.g. QAngaroo task (.", "labels": [], "entities": []}, {"text": "Each sample in QAngaroo contains multiple supporting documents, and the goal is selecting the correct answer from a set of candidates fora query.", "labels": [], "entities": []}, {"text": "Most queries cannot be answered depending on a single document, and multi-step reasoning chains across documents are needed.", "labels": [], "entities": []}, {"text": "Therefore, it is possible that understanding apart of paragraphs loses effectiveness for multi-hop inference, which posts a huge challenge for previous models.", "labels": [], "entities": []}, {"text": "Some baseline models, e.g. BiDAF (, which are popular for single-document QA, suffer dramatical accuracy decline in this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9948059916496277}]}, {"text": "In this paper, we propose anew graph-based QA model, named Bi-directional Attention Entity Graph convolutional network (BAG).", "labels": [], "entities": []}, {"text": "Documents are transformed into a graph in which nodes are entities and edges are relationships between them.", "labels": [], "entities": []}, {"text": "The graph is then imported into graph convolutional networks (GCNs) to learn relation-aware representation of nodes.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce anew bi-directional attention between the graph and a query with multi-level features to derive the mutual information for final prediction.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that BAG achieves state-of-the-art performance on the WIK-IHOP dataset.", "labels": [], "entities": [{"text": "BAG", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.4901368021965027}, {"text": "WIK-IHOP dataset", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.9794068932533264}]}, {"text": "Ablation test also shows BAG benefits from the bi-directional attention, multi-level features and graph convolutional networks.", "labels": [], "entities": [{"text": "BAG", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8318073153495789}]}, {"text": "Our contributions can be summarized as: \u2022 Applying a bi-directional attention between graphs and queries to learn query-aware representation for reading comprehension.", "labels": [], "entities": []}, {"text": "\u2022 Multi-level features are involved to gain comprehensive relationship representation for graph nodes during processing of GCNs.: Framework of BAG model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used both unmasked and masked versions of the QAngaroo WIKIHOP dataset (Welbl et al., 2018) and followed its basic setting, in which masked version used specific tokens such as MASK1 to replace original candidates tokens in documents.", "labels": [], "entities": [{"text": "QAngaroo WIKIHOP dataset", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.8376333514849345}]}, {"text": "There are 43,738, 5,129 and 2,451 examples in the training set, the development set and the test set respectively, and test set is not public.", "labels": [], "entities": []}, {"text": "In the implementation 1 , we used standard ELMo with a 1024 dimension representation.", "labels": [], "entities": []}, {"text": "Besides, 300-dimension GLoVe pre-trained embeddings from 840B Web crawl data were used as token-level features.", "labels": [], "entities": []}, {"text": "We used spaCy to provide additional 8-dimension NER and POS features.", "labels": [], "entities": []}, {"text": "The dimension of the 1-layer linear network for nodes in multi-level feature module was 512 with tanh as activation function.", "labels": [], "entities": []}, {"text": "A 2-layer Bi-LSTM was employed for queries whose hidden state size is 256.", "labels": [], "entities": []}, {"text": "Then the feature dimension is d = 512 + 8 + 8 = 528.", "labels": [], "entities": []}, {"text": "The GCN layer number L was set as 5.", "labels": [], "entities": []}, {"text": "And the unit number of intermediate layers in output layer was 256.", "labels": [], "entities": []}, {"text": "In addition, the number of nodes and the query length were truncated as 500 and 25 respectively for normalized computation.", "labels": [], "entities": []}, {"text": "Dropout with rate 0.2 was applied before GCN layer.", "labels": [], "entities": [{"text": "GCN layer", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8909493684768677}]}, {"text": "Adam optimizer is employed with initial learning rate 2 \u00d7 10 \u22124 , which will be halved for every 5 epochs, With batch size 32.", "labels": [], "entities": []}, {"text": "It took about 14 hours for 50-epoch training on two GTX1080Ti GPUs using pre-built and pre-processed graph data generated from original corpus, which can significantly decrease the training time.", "labels": [], "entities": []}, {"text": "We consider the following baseline models: FastQA (,, Coref-GRU (, MHQA-GRN (, Entity-GCN.", "labels": [], "entities": []}, {"text": "Former three models are RNN-based models, while coreference relationship is involved in Coref-GRU.", "labels": [], "entities": []}, {"text": "The last two models are graph-based models specially designed for multi-hop QA tasks.", "labels": [], "entities": []}, {"text": "As shown in, we collected three kinds of results.", "labels": [], "entities": []}, {"text": "The dev and test results stand for the original validation and test sets respectively, noting that the test set is not public.", "labels": [], "entities": []}, {"text": "In addition, we divide the original validation set of masked version into two parts evenly, one as a split validation set for tuning model and the other one as a split test set.", "labels": [], "entities": []}, {"text": "The test 1 results are for the split test set.", "labels": [], "entities": [{"text": "split test set", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.7093649705251058}]}, {"text": "Our BAG model achieves state-of-the art per-formance on both unmasked and masked data 2 , with accuracy 69.0% on the test set, which is 1.4% higher in value than previous best model Entity-GCN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9990622401237488}]}, {"text": "It is significant superior than FastQA and BiDAF due to leveraging of relationship information given by the graph and abandoning some distracting context in multiple documents.", "labels": [], "entities": []}, {"text": "Although Coref-GRU extends GRU with coreference relationships, it is still not enough for multi-hop because hop relationships are not limited to coreference, entities with the same strings also existed across documents which can be used for reasoning.", "labels": [], "entities": []}, {"text": "Both MHQA-GRN and Entity-GCN utilize graph networks to resolve relations among entities in documents.", "labels": [], "entities": [{"text": "MHQA-GRN", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.7795019745826721}]}, {"text": "However, the lack of attention and complementary features limits their performance.", "labels": [], "entities": []}, {"text": "Therefore our BAG model achieves the best performance under all data configurations.", "labels": [], "entities": []}, {"text": "It is noticed that BAG only gets a small promotion on masked data.", "labels": [], "entities": [{"text": "BAG", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.6274452805519104}]}, {"text": "We argue that the reason is the attention between masks and queries generating less useful information compared to unmasked ones.", "labels": [], "entities": []}, {"text": "Moreover, ablation experimental results on unmasked version of the WIKIHOP dev set are given in.", "labels": [], "entities": [{"text": "WIKIHOP dev set", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9145562450091044}]}, {"text": "Once we remove the bi-directional attention and put the concatenation of nodes and queries directly into the output layer, it shows significant performance drop with more than 3%, proving the necessity of attention for reasoning in multi-hop QA.", "labels": [], "entities": []}, {"text": "If we use linear-transformationbased single attention a = h n W a f q given in () instead of our bi-directional attention, the accuracy drops with 2%, which means attention bi-directionality also contributes to the performance improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9995654225349426}]}, {"text": "The similar condition will appear if we remove GCN, but use raw nodes as input for the attention layer.", "labels": [], "entities": [{"text": "GCN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7765461802482605}]}, {"text": "In addition, if edge types are no longer considered, which makes R-GCN degraded to vanilla GCN, noticeable accuracy loss about 2% appears.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9994623064994812}]}, {"text": "The absence of multi-level features will also cause degradation.", "labels": [], "entities": []}, {"text": "The removal of semantic-level features causes slight decline on the performance, including NER and POS features.", "labels": [], "entities": []}, {"text": "Further removal of ELMo feature will causes a dramatical drop, which reflects the insufficiency of only using word embeddings as features for nodes and that contextual information is very important.", "labels": [], "entities": []}, {"text": "The paper was written on early Dec. 2018, during that time Entity-GCN is the best public model, and only one anonymous model is better than it.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance of different models on both  masked and unmasked version of WIKIHOP dataset.  ([*] Results reported in original papers, others are ob- tained by official code. [ \u2020] Masked data is not suitable  for coreference parsing. [ \u2021] Some results are missing  due to unavailability of source code.)", "labels": [], "entities": [{"text": "WIKIHOP dataset", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9633825421333313}, {"text": "coreference parsing", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.9546020925045013}]}, {"text": " Table 2: Ablation test results of BAG model on the  unmasked validation set of the WIKIHOP dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.982038676738739}, {"text": "WIKIHOP dataset", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9788294434547424}]}]}