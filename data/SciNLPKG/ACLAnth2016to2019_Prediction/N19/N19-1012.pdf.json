{"title": [{"text": "\"President Vows to Cut Taxes Hair\": Dataset and Analysis of Creative Text Editing for Humorous Headlines", "labels": [], "entities": [{"text": "Creative Text Editing", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.692806601524353}]}], "abstractContent": [{"text": "We introduce, release, and analyze anew dataset, called Humicroedit, for research in computational humor.", "labels": [], "entities": [{"text": "Humicroedit", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9359177350997925}]}, {"text": "Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny.", "labels": [], "entities": []}, {"text": "We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline.", "labels": [], "entities": []}, {"text": "The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous.", "labels": [], "entities": []}, {"text": "We show how the data support classic theories of humor, such as incongruity, superiority , and setup/punchline.", "labels": [], "entities": []}, {"text": "Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humor detection and generation continue to be challenging AI problems.", "labels": [], "entities": [{"text": "Humor detection and generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8565782010555267}]}, {"text": "While there have been some advances in automatic humor recognition, computerized humor generation has seen less progress ().", "labels": [], "entities": [{"text": "automatic humor recognition", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6862275103727976}, {"text": "computerized humor generation", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6625527739524841}]}, {"text": "This is not surprising, given that humor involves in-depth world-knowledge, commonsense, and the ability to perceive relationships across entities and objects at various levels of understanding.", "labels": [], "entities": []}, {"text": "Even humans often fail at being funny or recognizing humor.", "labels": [], "entities": []}, {"text": "A big hindrance to progress on humor research is the scarcity of public datasets.", "labels": [], "entities": [{"text": "humor research", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9245671927928925}]}, {"text": "Further-  more, the existing datasets address specific humor templates, such as funny one-liners) and filling in Mad Libs R. Creating a humor corpus is non-trivial, however, because it requires (i) human annotation, and (ii) a clear definition of humor to achieve good inter-annotator agreement.", "labels": [], "entities": [{"text": "filling in Mad Libs R. Creating a humor", "start_pos": 102, "end_pos": 141, "type": "TASK", "confidence": 0.6290598697960377}]}, {"text": "We introduce Humicroedit, a novel dataset for research in computational humor.", "labels": [], "entities": [{"text": "Humicroedit", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.8946112990379333}]}, {"text": "First, we collect original news headlines from news media posted on Reddit (reddit.com).", "labels": [], "entities": []}, {"text": "Then, we qualify expert annotators from Amazon Mechanical Turk (mturk.com) to (i) generate humor by applying small edits to these headlines, and to (ii) judge the humor in these edits.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (mturk.com", "start_pos": 40, "end_pos": 73, "type": "DATASET", "confidence": 0.9118541479110718}]}, {"text": "Our resulting dataset contains 15,095 edited news headlines and their numerically assessed humor.", "labels": [], "entities": []}, {"text": "Screenshots of our two annotation tasks are shown in, and Table 1 shows some of these annotated headlines.", "labels": [], "entities": []}, {"text": "This new dataset enables various humor tasks, such as: (i) understanding what makes an edited headline funny, (ii) predicting whether an edited headline is funny, (iii) ranking multiple edits of the same headline on a funniness scale, (iv) generating humorous news headlines, and (v) recommending funny headlines personalized to a reader.", "labels": [], "entities": [{"text": "predicting whether an edited headline", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.8170993566513062}]}, {"text": "Our dataset presents several opportunities for computational humor research since: \u2022 Headlines do not have specific templates.", "labels": [], "entities": [{"text": "computational humor research", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8091515302658081}]}, {"text": "\u2022 Headlines contain very few words, but convey a lot of information.", "labels": [], "entities": []}, {"text": "\u2022 A deeper understanding of world-knowledge and common-sense is needed to completely understand what makes a headline funny.", "labels": [], "entities": []}, {"text": "\u2022 Humorous headlines are often generated using several layers of cognition and reasoning.", "labels": [], "entities": [{"text": "Humorous headlines", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.8244293332099915}]}, {"text": "\u2022 Despite us carefully qualifying annotators, their knowledge, preferences, bias and stance towards information presented in headlines influence whether they perceive a potentially funny headline as humorous, offensive, confusing, etc.", "labels": [], "entities": []}, {"text": "The presence of these factors suggests that thorough humor comprehension in our dataset requires the development of NLP tools that are not only robust at pattern recognition but also capable of deeper semantic understanding and reasoning.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7193843722343445}]}, {"text": "As an initial exploration of this proposition, we perform various data analysis against the background of humor theories, and we train and examine classifiers to detect humorous edited headlines in our data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe how we gathered our set of original headlines, directed editors to make them funny, employed graders to assess the level of humor in the modified headlines, and created the Humicroedit dataset.", "labels": [], "entities": [{"text": "Humicroedit dataset", "start_pos": 202, "end_pos": 221, "type": "DATASET", "confidence": 0.9738147258758545}]}], "tableCaptions": [{"text": " Table 1: Some headlines in our dataset and their edits, mean funniness grades, and accuracy of funniness prediction  by LSTM. We acknowledge that some of these are offensive, but we use them for analysis in Section 4.1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.99954754114151}, {"text": "funniness prediction", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6036036163568497}, {"text": "LSTM", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.870930016040802}]}, {"text": " Table 2: Twenty clusters of replacement words with manually determined cluster labels.", "labels": [], "entities": []}, {"text": " Table 3: Edited headlines were judged funnier when  they had a larger proportion of replaceable words.", "labels": [], "entities": []}, {"text": " Table 4: LSTM accuracy for distinct grade bins (upper- bounds are inclusive) on the test set for X = 40.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7481575012207031}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9125747680664062}]}, {"text": " Table 5: Classification accuracy for various funniness- sorted dataset proportions and classifiers/feature sets.  MaxUF is the highest score for the not-funny class,  MinF is the lowest score for the funny class, and we  also provide Krippendorff's \u03b1 for judge agreement.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9206370115280151}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9865002036094666}, {"text": "MaxUF", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.9004324674606323}]}]}