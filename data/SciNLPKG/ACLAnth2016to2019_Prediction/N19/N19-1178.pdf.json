{"title": [], "abstractContent": [{"text": "Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7470652461051941}]}, {"text": "The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems.", "labels": [], "entities": []}, {"text": "We propose to extract dialog structures using a modified VRNN model with discrete latent vectors.", "labels": [], "entities": []}, {"text": "Different from existing HMM-based models, our model is based on variational-autoencoder (VAE).", "labels": [], "entities": [{"text": "variational-autoencoder (VAE", "start_pos": 64, "end_pos": 92, "type": "METRIC", "confidence": 0.7476307153701782}]}, {"text": "Such model is able to capture more dynamics in dialogs beyond the surface forms of the language.", "labels": [], "entities": []}, {"text": "We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data.", "labels": [], "entities": []}, {"text": "We further evaluate the model's effectiveness in a downstream task, the dialog system building task.", "labels": [], "entities": [{"text": "dialog system building task", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.748735100030899}]}, {"text": "Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human dialogs are like well-structured buildings, with words as the bricks, sentences as the floors, and topic transitions as the stairs connecting the whole building.", "labels": [], "entities": []}, {"text": "Therefore, discovering dialog structure is crucial for various areas in computational linguistics, such as dialog system building), discourse analysis (, and dialog summarization (;.", "labels": [], "entities": [{"text": "dialog system building", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.6847622791926066}, {"text": "discourse analysis", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.8077844679355621}, {"text": "dialog summarization", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7908492982387543}]}, {"text": "In domain specific tasks such as restaurant booking, it's common for people to follow atypical conversation flow.", "labels": [], "entities": [{"text": "restaurant booking", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7327716797590256}]}, {"text": "Current dialog systems require human experts to design the dialog structure, which is time consuming and sometimes insufficient to satisfy various customer needs.", "labels": [], "entities": []}, {"text": "Therefore, it's of great importance to automatically discover dialog structure from existing human-human conversations and incorporate it into the dialog system design.", "labels": [], "entities": []}, {"text": "However, modeling human conversation is not easy for machines.", "labels": [], "entities": [{"text": "modeling human conversation", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7824660539627075}]}, {"text": "Some previous work rely on human annotations to learn dialog structures in supervised learning settings.", "labels": [], "entities": []}, {"text": "But since human labeling is expensive and hard to obtain, such method is constrained by the small size of training examples, and by the limited number of application domains.", "labels": [], "entities": [{"text": "human labeling", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5794019550085068}]}, {"text": "Moreover, structure annotations on human conversation can be subjective, which makes it hard to reach inter-rater agreements.", "labels": [], "entities": []}, {"text": "Therefore, we propose an unsupervised method to infer the latent dialog structure since unsupervised methods do not require annotated dialog corpus.", "labels": [], "entities": []}, {"text": "Limited previous work has studied unsupervised methods to model the latent dialog structure.", "labels": [], "entities": []}, {"text": "Most of the previous methods use the hidden Markov model to capture the temporal dependency within human dialogs).", "labels": [], "entities": []}, {"text": "We propose to adopt anew type of models, the variational recurrent neural network (VRNN, a recurrent version of the VAE) (, and infer the latent dialog structure with variational inference.", "labels": [], "entities": []}, {"text": "VRNN is suitable for modeling sequential information.", "labels": [], "entities": [{"text": "VRNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8943079710006714}]}, {"text": "Compared to the simpler HMM models, VRNN also has the flexibility to model highly non-linear dynamics () inhuman dialogs.", "labels": [], "entities": [{"text": "VRNN", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.680194616317749}]}, {"text": "Our basic approach assumes that the dialog structure is composed of a sequence of latent states.", "labels": [], "entities": []}, {"text": "Each conversational exchange (a pair of user and system utterances at time t) belongs to a latent state, which has causal effect on the future latent states and the words the conversants produce.", "labels": [], "entities": []}, {"text": "Because discrete latent states are more interpretable than continuous ones, we combine VRNN with Gumbel-Softmax ( to obtain discrete latent vectors to represent the latent states.", "labels": [], "entities": [{"text": "VRNN", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7381263375282288}]}, {"text": "A common way to represent the dialog structure both visually and numerically is to construct a transition probability table among latent states.", "labels": [], "entities": []}, {"text": "The idea of transition table inspires us to develop two model variants to model the dependency between states indirectly and directly.", "labels": [], "entities": []}, {"text": "Once we obtain such a human-readable dialog structure, we can use it to facilitate many downstream tasks, such as dialog system training.", "labels": [], "entities": [{"text": "dialog system training", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.7727940082550049}]}, {"text": "The motivation is that the dialog structure contains important information on the flow of the conversation; if the automatic dialog system can mimic the behaviour in human-human dialogs, it can interact with users in a more natural and user-friendly way.", "labels": [], "entities": []}, {"text": "Therefore, we propose to integrate the dialog structure information into the reward design of the reinforcement learning (RL).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6838678836822509}]}, {"text": "Experiments show that the model with the proposed reward functions converges faster to a better success rate.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the proposed method on the CamRest676 corpus, which was released and collected by.", "labels": [], "entities": [{"text": "CamRest676 corpus", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.9947669804096222}]}, {"text": "The task is to help users find restaurants in Cambridge, UK.", "labels": [], "entities": [{"text": "Cambridge, UK", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.7881837288538615}]}, {"text": "While this task is highly similar to DSTC2, we choose this dataset instead of DSTC2 because it is relatively clean and comes with good entity extraction methods.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8916963934898376}, {"text": "DSTC2", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9517538547515869}, {"text": "entity extraction", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.713564082980156}]}, {"text": "There area total of 676 dialogs in this dataset with three information slots (food, price range and area) and three request table slots (address, phone and postcode).", "labels": [], "entities": []}, {"text": "We also evaluate our model on another dataset of simulated conversations, proposed in.", "labels": [], "entities": []}, {"text": "The task is to help users get the weather report in a certain place at a specific time.", "labels": [], "entities": []}, {"text": "The dialog system is controlled by a fixed structure and hand-set probabilities.", "labels": [], "entities": []}, {"text": "Therefore, learning the dialog structure of this dataset might be easier.", "labels": [], "entities": []}, {"text": "We assume each latent vector in the VAE emits one conversational exchange, including one user utterance and the corresponding system response at time t, and each conversational exchange corresponds to one latent vector, following.", "labels": [], "entities": [{"text": "VAE", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8415987491607666}]}, {"text": "We use LSTM) with 200-400 units for the RNNs, and a fully-connected network for all the \u03d5 (\u00b7) \u03c4 with a dropout rate of 0.4.", "labels": [], "entities": []}, {"text": "Additionally, we use trainable 300-dimension word embeddings initialized by Google word2vec ().", "labels": [], "entities": []}, {"text": "The maximum utterance word length is 40 and the maximum dialog length is 10.", "labels": [], "entities": []}, {"text": "We set the \u03bb for the bow-loss to be 0.1. 80% of the entire dataset are used for training, 10% for validation and 10% for testing.", "labels": [], "entities": []}, {"text": "Parameters mentioned are selected based on the performance of the validation set.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9388484358787537}, {"text": "validation", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.9579634070396423}]}, {"text": "The evaluation of unsupervised methods has always been a challenge.", "labels": [], "entities": []}, {"text": "We first compare our models with a simple K-means clustering algorithm to show its context sensitivity.", "labels": [], "entities": []}, {"text": "Then we compare our models with traditional HMM methods both qualitatively and quantitatively.", "labels": [], "entities": [{"text": "HMM", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9367522597312927}]}, {"text": "Finally, we compare the three proposed model variants.", "labels": [], "entities": []}, {"text": "The qualitative evaluation involves generating dialog structures with different models, and the quantitative evaluations involves calculating the likelihood on a held-out test set under a specific model, which measures the model's predictive power.", "labels": [], "entities": []}], "tableCaptions": []}