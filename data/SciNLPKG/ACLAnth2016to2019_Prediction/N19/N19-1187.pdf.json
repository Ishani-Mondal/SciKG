{"title": [{"text": "Learning to Stop in Structured Prediction for Neural Machine Translation", "labels": [], "entities": [{"text": "Learning to Stop in Structured Prediction", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.582321509718895}, {"text": "Neural Machine Translation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7329695224761963}]}], "abstractContent": [{"text": "Beam search optimization (Wiseman and Rush, 2016) resolves many issues in neural machine translation.", "labels": [], "entities": [{"text": "Beam search optimization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9258733987808228}, {"text": "neural machine translation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6703560749689738}]}, {"text": "However, this method lacks principled stopping criteria and does not learn how to stop during training, and the model naturally prefers longer hypotheses during the testing time in practice since they use the raw score instead of the probability-based score.", "labels": [], "entities": []}, {"text": "We propose a novel ranking method which enables an optimal beam search stopping criteria.", "labels": [], "entities": [{"text": "beam search stopping", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.815575917561849}]}, {"text": "We further introduce a struc-tured prediction loss function which penalizes suboptimal finished candidates produced by beam search during training.", "labels": [], "entities": []}, {"text": "Experiments of neural machine translation on both synthetic data and real languages (German\u2192English and Chinese\u2192English) demonstrate our proposed methods lead to better length and BLEU score.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7148646314938863}, {"text": "length", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9995755553245544}, {"text": "BLEU score", "start_pos": 180, "end_pos": 190, "type": "METRIC", "confidence": 0.977173775434494}]}], "introductionContent": [{"text": "Sequence-to-sequence (seq2seq) models based on RNNs (), and selfattention () have achieved great successes in Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.7779458413521448}]}, {"text": "The above family of models encode the source sentence and predict the next word in an autoregressive fashion at each decoding time step.", "labels": [], "entities": []}, {"text": "The classical \"cross-entropy\" training objective of seq2seq models is to maximize the likelihood of each word in the translation reference given the source sentence and all previous words in that reference.", "labels": [], "entities": []}, {"text": "This word-level loss ensures efficient and scalable training of seq2seq models.", "labels": [], "entities": []}, {"text": "However, this word-level training objective suffers from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch  2015a;).", "labels": [], "entities": []}, {"text": "In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy.", "labels": [], "entities": [{"text": "beam search", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.839737594127655}]}, {"text": "Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods, training with alternative references.", "labels": [], "entities": []}, {"text": "Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO).", "labels": [], "entities": [{"text": "Beam Search Optimization (BSO)", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.829594890276591}]}, {"text": "While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in which we present with details below.", "labels": [], "entities": [{"text": "BSO", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.7639548182487488}, {"text": "German-toEnglish translation", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6072336137294769}]}, {"text": "BSO relies on unnormalized raw scores instead of locally-normalized probabilities to get rid of the label bias problem.", "labels": [], "entities": [{"text": "BSO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5576536059379578}]}, {"text": "However, since the raw score can be either positive or negative, the optimal stopping criteria ) no longer holds, e.g., one extra decoding step would increase the entire unfinished hypothesis's model score when we have positive word score.", "labels": [], "entities": []}, {"text": "This leads to two consequences: we do not know when to stop the beam search and it could return overlength translations () or underlength translations in practice.", "labels": [], "entities": []}, {"text": "As shown in, the BLEU score of BSO drops significantly when beam size gets larger as a result of overlong translations (as evidenced by length ratios larger than 1).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9847199320793152}, {"text": "BSO", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5012065172195435}]}, {"text": "Furthermore, BSO performs poorly (shown in Section 4) on hard translation pairs, e.g., Chinese\u2192English (Zh\u2192En) translation, when the target / source ratio is more diverse.", "labels": [], "entities": [{"text": "BSO", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6433634161949158}, {"text": "Chinese\u2192English (Zh\u2192En) translation", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.595177087518904}]}, {"text": "To overcome the above issues, we propose to use the sigmoid function instead of the raw score at each time step to rank candidates.", "labels": [], "entities": []}, {"text": "In this way, the model still has probability properties to hold optimal stopping criteria without label bias effects.", "labels": [], "entities": []}, {"text": "Moreover, we also encourage the model to generate the hypothesis which is more similar to gold reference in length.", "labels": [], "entities": []}, {"text": "Compared with length rewardbased methods, our model does not need to tune the predicted length and per-word reward.", "labels": [], "entities": []}, {"text": "Experiments on both synthetic and real language translations (De\u2192En and Zh\u2192En) demonstrate significant improvements in BLEU score over strong baselines and other methods.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9738287925720215}]}], "datasetContent": [{"text": "We showcase the performance comparisons over three different datasets.", "labels": [], "entities": []}, {"text": "We implement seq2seq model, BSO and our proposed model based on PyTorch-based OpenNMT ().", "labels": [], "entities": [{"text": "BSO", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9024351239204407}, {"text": "PyTorch-based OpenNMT", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.7813932597637177}]}, {"text": "We use a two-layer bidirectional LSTM as the encoder and a two layer LSTM as the decoder.", "labels": [], "entities": []}, {"text": "We train Seq2seq model for 20 epochs to minimize perplexity on the training dataset, with a batch size of 64, word embedding size of 512, the learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.2.", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 164, "end_pos": 183, "type": "METRIC", "confidence": 0.927966296672821}, {"text": "dropout rate", "start_pos": 195, "end_pos": 207, "type": "METRIC", "confidence": 0.9450287520885468}]}, {"text": "Following, we then train BSO and our model based on the previous Seq2seq model with the learning rate of 0.01 and learning rate decay of 0.75, batch size of 40.", "labels": [], "entities": [{"text": "BSO", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9126493334770203}, {"text": "learning rate", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9274516403675079}, {"text": "learning rate decay", "start_pos": 114, "end_pos": 133, "type": "METRIC", "confidence": 0.9390200773874918}]}, {"text": "Note that our pretrained model is softmaxbased, and we only replace the softmax layer with the sigmoid layer for later training for simplicity.", "labels": [], "entities": []}, {"text": "The performance will have another boost when our pretrained model is sigmoid-based.", "labels": [], "entities": []}, {"text": "We use Adagrad () as the optimizer.", "labels": [], "entities": []}, {"text": "In Zh\u2192En task, we employ BPE () which reduces the source and target language vocabulary sizes to 18k and 10k.", "labels": [], "entities": [{"text": "BPE", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9930009841918945}]}, {"text": "Following BSO, we set the decoding beam size smaller than the training beam size by 1.", "labels": [], "entities": [{"text": "BSO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9200822710990906}]}, {"text": "shows the statistics of source sentence length and the ratio between target and source sentences.", "labels": [], "entities": []}, {"text": "The synthetic dataset is a simple translation task which generates target sentences from this grammar: {a \u2192 x, b \u2192 xx, c \u2192 xx x, d \u2192 xx xx, e \u2192 xx xx x}.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics of source sentence length and  the ratio between target and source sentences. \u03c3 is stan- dard deviation.  *  shows statistics of cleaned test set.", "labels": [], "entities": [{"text": "stan- dard deviation", "start_pos": 112, "end_pos": 132, "type": "METRIC", "confidence": 0.8488160520792007}]}, {"text": " Table 2: BLEU and length ratio on the De\u2192En valida- tion set.  \u2020 indicates our own implementation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992488026618958}, {"text": "De\u2192En valida- tion set", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.6193890741893223}]}, {"text": " Table 3: Ablation study on the De\u2192En validation set  with training beam size b = 6.", "labels": [], "entities": [{"text": "De\u2192En validation set", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.625860619544983}]}, {"text": " Table 4: BLEU and length ratio on the De\u2192En test  set.  \u2020 indicates our own implementation.  \u2021 results from  (", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999251663684845}, {"text": "De\u2192En test  set", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.6822065591812134}]}, {"text": " Table 5: BLEU and length ratio of models on Zh\u2192En  validation set.  \u2020 indicates our own implementation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995864033699036}, {"text": "length ratio", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9486006200313568}]}, {"text": " Table 6: BLEU and length ratio of models on Zh\u2192En  test set.  \u2020 indicates our own implementation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995717406272888}, {"text": "length ratio", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9355437159538269}, {"text": "Zh\u2192En  test set", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.7226923167705536}]}]}