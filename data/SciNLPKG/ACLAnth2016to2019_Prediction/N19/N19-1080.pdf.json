{"title": [], "abstractContent": [{"text": "The goal of event detection (ED) is to detect the occurrences of events and categorize them.", "labels": [], "entities": [{"text": "event detection (ED)", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.865202260017395}]}, {"text": "Previous work solved this task by recognizing and classifying event triggers, which is defined as the word or phrase that most clearly expresses an event occurrence.", "labels": [], "entities": []}, {"text": "As a consequence , existing approaches required both annotated triggers and event types in training data.", "labels": [], "entities": []}, {"text": "However, triggers are nonessential to event detection, and it is time-consuming for annotators to pick out the \"most clear-ly\" word from a given sentence, especially from along sentence.", "labels": [], "entities": [{"text": "event detection", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7603293061256409}]}, {"text": "The expensive annotation of training corpus limits the application of existing approaches.", "labels": [], "entities": []}, {"text": "To reduce manual effort, we explore detecting events without triggers.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel framework dubbed as Type-aware Bias Neural Network with Attention Mechanisms (TBN-NAM), which encodes the representation of a sentence based on target event types.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness.", "labels": [], "entities": []}, {"text": "Remarkably, the proposed approach even achieves competitive performances compared with state-of-the-arts that used annotated triggers.", "labels": [], "entities": []}], "introductionContent": [{"text": "This work tackles the task of event detection (ED), whose goal is to detect the occurrences of predefined events and categorize them.", "labels": [], "entities": [{"text": "event detection (ED)", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8620126008987427}]}, {"text": "For example, consider the following sentence \"In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.\", an ideal event detection system should recognize two events, Death and Attack(suppose that both Death and Attack are in the predefined event set) . Previous work typically solved this task by recognizing and classifying event triggers.", "labels": [], "entities": [{"text": "event detection", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.7438527643680573}]}, {"text": "According to ACE (Automatic Context Extraction) event evaluation program, event trigger is defined as the word or phrase that most clearly expresses an event occurrence.", "labels": [], "entities": [{"text": "Automatic Context Extraction) event evaluation", "start_pos": 18, "end_pos": 64, "type": "TASK", "confidence": 0.7784373909235001}]}, {"text": "Take the following sentence as an example: S: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.", "labels": [], "entities": [{"text": "Palestine Hotel", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.9531516134738922}]}, {"text": "\"died\" is the trigger word of Death event, and \"fired\" is the trigger word of Attack event.", "labels": [], "entities": []}, {"text": "The majority of existing approaches modeled this task as word classification, which predicted whether each word in a given sentence is an event trigger and what type of event it triggered.", "labels": [], "entities": [{"text": "word classification", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7473904490470886}]}, {"text": "As a consequence, these approaches required both annotated triggers and event types for training.", "labels": [], "entities": []}, {"text": "However, event triggers are nonessential to this task.", "labels": [], "entities": []}, {"text": "Remind that the goal of event detection is to recognize and categorize events, thus triggers could be viewed as intermediate results of this task.", "labels": [], "entities": [{"text": "event detection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7248471230268478}]}, {"text": "Furthermore, it is time-consuming for annotators to pick out the \"most clearly\" word from a given sentence, especially from along sentence, which limits the application of existing ED approaches.", "labels": [], "entities": []}, {"text": "To reduce manual effort, we explore detecting events without triggers.", "labels": [], "entities": []}, {"text": "In this study, the only annotated information of each sentence is the types of events occurred in it.", "labels": [], "entities": []}, {"text": "Consider the aforementioned example S again, its annotation is {Death, Attack}.", "labels": [], "entities": [{"text": "Death", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9182339310646057}]}, {"text": "On the contrast, previous work also required an annotated trigger for each event, which means the annotated information of S is {Death:died, Attack:fired} in previous work.", "labels": [], "entities": []}, {"text": "Without event triggers, it is intuitive to model this task via text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.6862271577119827}]}, {"text": "However, there are two challenges: (1) Multi-label problem: each sentence may contain arbitrary number of events, which means it could have zero or multiple target labels.", "labels": [], "entities": []}, {"text": "In machine learning, this problem is called multi-label problem.", "labels": [], "entities": []}, {"text": "(2) Trigger absence problem: previous work illustrated that trigger words play important roles in event detection.", "labels": [], "entities": [{"text": "Trigger absence", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8883072435855865}, {"text": "event detection", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7788375616073608}]}, {"text": "It is challenging to model this information without annotated triggers.", "labels": [], "entities": []}, {"text": "To solve the first challenge, we transform multilabel classification to multiple binary classification problems.", "labels": [], "entities": [{"text": "multilabel classification", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7111857086420059}]}, {"text": "Specifically, a given sentence s attached each pre-defined event type t forms an instance, which is expected to be labeled with 0 or 1 according to whether s contains an event of type t.", "labels": [], "entities": []}, {"text": "For example, suppose there totally are 3 predefined types of events(denoted by t 1 , t 2 and t 3 ), and sentence s contains two events of type t 1 and t 3 , then it could be transformed to the following three instances: In this paradigm, sentences that convey multiple events will yield multiple positive pairs, thus the multi-label problem could be well solved.", "labels": [], "entities": []}, {"text": "Furthermore, each type of events are usually triggered by a set of specific words, which are called event trigger words.", "labels": [], "entities": []}, {"text": "For example, Death events are usually triggered by \"die\", \"passed away\", \"gone\", etc.", "labels": [], "entities": []}, {"text": "Therefore, event trigger words are important clues to this task.", "labels": [], "entities": []}, {"text": "Since existing work explicitly exploited annotated trigger words in their approaches, they can directly model this observation.", "labels": [], "entities": []}, {"text": "However, in our case, annotated triggers are unavailable.", "labels": [], "entities": []}, {"text": "To model this information, we propose a simple but effective model, called Type-aware Bias Neural Network with Attention Mechanisms (TBNNAM).", "labels": [], "entities": []}, {"text": "illustrates the framework of TBNNAM.", "labels": [], "entities": [{"text": "TBNNAM", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.6089763045310974}]}, {"text": "The input is consisted of two parts: a tokenized sentence with NER tags and a target event type.", "labels": [], "entities": []}, {"text": "The output o is expected to be 1 if the given sentence conveys an event of the target type, otherwise 0 (the output should be 1 for the example given in.", "labels": [], "entities": []}, {"text": "Specifically, given a sentence, the proposed model first transforms the input tokens into embeddings, and applies an LSTM layer to calculate a context-dependent representation for each token.", "labels": [], "entities": []}, {"text": "Then it computes an attention vector, \u03b1, based on the target event type, where the trigger word is expected to obtain higher score.", "labels": [], "entities": []}, {"text": "Finally, the sentence representation s att is calculated based on \u03b1.", "labels": [], "entities": []}, {"text": "Here, s att is expected to focus on local information (trigger word).", "labels": [], "entities": []}, {"text": "To capture global information, the final output, o, is also connected to the last LSTM units, which encodes the global information of the input sentence.", "labels": [], "entities": []}, {"text": "Furthermore, to reinforce the influence of positive samples, we devise a bias objective function in our model . We call our model \"type-aware\" because the representation of a sentence, s att , is calculated based on the target event type.", "labels": [], "entities": []}, {"text": "We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 1 . The results illustrate that our approach outperforms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers.", "labels": [], "entities": [{"text": "benchmark dataset ACE2005 1", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.7577239125967026}]}, {"text": "We publish our code for further study by the NLP community.", "labels": [], "entities": []}, {"text": "In summary, the main contributions of this work are: (1) To the best of our knowledge, this is the first work that focuses on detecting events without triggers.", "labels": [], "entities": [{"text": "detecting events without triggers", "start_pos": 126, "end_pos": 159, "type": "TASK", "confidence": 0.857067808508873}]}, {"text": "Compared with existing approaches, the proposed method requires less manual annotations.", "labels": [], "entities": []}, {"text": "(2) Without triggers, this task encounters two challenges: multi-label problem and trigger absence problem.", "labels": [], "entities": []}, {"text": "We propose a simple but effective model, which even achieves competitive results compared with approaches that using annotated triggers.", "labels": [], "entities": []}, {"text": "(3) Since this is the first work on detecting events without triggers, we implement a series of baseline models for this task, and systematically evaluate and analyze them.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we introduce the dataset, evaluation metrics and the settings of hyper parameters.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on ACE 2005 dataset.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9889140129089355}]}, {"text": "Following the evaluation of previous work, we randomly selected 30 articles from different genres as the development set, and subsequently conducted a blind test on a separate set of 40 ACE 2005 newswire documents.", "labels": [], "entities": [{"text": "ACE 2005 newswire documents", "start_pos": 186, "end_pos": 213, "type": "DATASET", "confidence": 0.9576043784618378}]}, {"text": "We used the remaining 529 articles as our training set.", "labels": [], "entities": []}, {"text": "This work focuses on detecting events without triggers.", "labels": [], "entities": [{"text": "detecting events without triggers", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.8577555567026138}]}, {"text": "Therefore, we remove trigger annotations from the corpus.", "labels": [], "entities": []}, {"text": "Specifically, we employ Stanford CoreNLP Toolkit to split each document into sentences, and assign each sentence with a set of labels according to the original annotations in ACE 2005 corpus.", "labels": [], "entities": [{"text": "Stanford CoreNLP Toolkit", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.85437540213267}, {"text": "ACE 2005 corpus", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.9582964181900024}]}, {"text": "If a sentence does not contain any event, we assign it with a special label, NA.", "labels": [], "entities": []}, {"text": "If a sentence contains multiple events of the same type (less than 3% in ACE corpus), we only keep one label for each type.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.889691561460495}]}, {"text": "shows several samples of the our corpus.", "labels": [], "entities": []}, {"text": "sentence labels They got married in 1985.", "labels": [], "entities": []}, {"text": "{Marry} They got married in 1985, and divorced 3 years latter.", "labels": [], "entities": []}, {"text": "{Marry, Divorce} They are very happy everyday.", "labels": [], "entities": [{"text": "Marry, Divorce}", "start_pos": 1, "end_pos": 16, "type": "TASK", "confidence": 0.8087824136018753}]}, {"text": "{NA}: Examples of instances in our corpus (without event trigger annotations).", "labels": [], "entities": []}, {"text": "Following previous work (, we use precision (P), recall (R) and F 1 -measure (F 1 ) to evaluate the results.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9419326037168503}, {"text": "recall (R)", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9358014315366745}, {"text": "F 1 -measure (F 1 )", "start_pos": 64, "end_pos": 83, "type": "METRIC", "confidence": 0.9615494906902313}]}, {"text": "Precision: the proportion of correctly predicted events in total predicted events.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9917984008789062}]}, {"text": "Recall: the proportion of correctly predicted events in total gold events of the dataset.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9838322401046753}]}, {"text": "F 1 -measure: 2\u00d7P \u00d7R P +R", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9401534199714661}]}], "tableCaptions": [{"text": " Table 3: Experimental results on ACE 2005 corpus.  Methods with name MC-* are based on multi-class  classification, and methods with name BC-* are based  on binary classification.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.971106747786204}]}, {"text": " Table 4: Experimental results on ACE 2005 corpus.  Methods in the first group are baseline systems. Meth- ods in the second group are the proposed approaches.  Methods in the last group are state-of-the-art ED sys- tems.  \u2020 requiring annotated triggers,  \u2021 using external  data", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9737622539202372}]}, {"text": " Table 5: Results of systems without/with bias term in  loss function, where *\\Bias do not use bias term.", "labels": [], "entities": []}]}