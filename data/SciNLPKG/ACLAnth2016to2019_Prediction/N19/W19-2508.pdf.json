{"title": [{"text": "A framework for streamlined statistical prediction using topic models", "labels": [], "entities": [{"text": "statistical prediction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7873889207839966}]}], "abstractContent": [{"text": "In the Humanities and Social Sciences, there is increasing interest in approaches to information extraction, prediction, intelligent linkage , and dimension reduction applicable to large text corpora.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7574169039726257}, {"text": "dimension reduction", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6936188191175461}]}, {"text": "With approaches in these fields being grounded in traditional statistical techniques, the need arises for frameworks whereby advanced NLP techniques such as topic modelling maybe incorporated within classical methodologies.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.8169935941696167}]}, {"text": "This paper provides a classical, supervised, statistical learning framework for prediction from text, using topic models as a data reduction method and the topics themselves as predictors, alongside typical statistical tools for predictive modelling.", "labels": [], "entities": [{"text": "predictive modelling", "start_pos": 229, "end_pos": 249, "type": "TASK", "confidence": 0.9286220669746399}]}, {"text": "We apply this framework in a Social Sciences context (applied animal behaviour) as well as a Humanities context (narrative analysis) as examples of this framework.", "labels": [], "entities": []}, {"text": "The results show that topic regression models perform comparably to their much less efficient equivalents that use individual words as pre-dictors.", "labels": [], "entities": [{"text": "topic regression", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7005679905414581}]}], "introductionContent": [{"text": "For the past 20 years, topic models have been used as a means of dimension reduction on text data, in order to ascertain underlying themes, or 'topics', from documents.", "labels": [], "entities": []}, {"text": "These probabilistic models have frequently been applied to machine learning problems, such as web spam filtering (, database sorting () and trend detection (.", "labels": [], "entities": [{"text": "web spam filtering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.654735267162323}, {"text": "database sorting", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7734023630619049}, {"text": "trend detection", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.7040165662765503}]}, {"text": "This paper develops a methodology for incorporating topic models into traditional statistical regression frameworks, such as those used in the Social Sciences and Humanities, to make predictions.", "labels": [], "entities": []}, {"text": "Statistical regression is a supervised method, however it should be noted the majority of topic models are themselves unsupervised.", "labels": [], "entities": [{"text": "Statistical regression", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.790459394454956}]}, {"text": "When using text data for prediction, we are often confronted with the problem of condensing the data into a manageable form, which still retains the necessary information contained in the text.", "labels": [], "entities": [{"text": "prediction", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9617746472358704}]}, {"text": "Methods such as using individual words as predictors, or n-grams, while conceptually quite simple, have a tendency to be extremely computationally expensive (with tens of thousands of predictors in a model).", "labels": [], "entities": []}, {"text": "Except on extremely large corpora, this inevitably leads to overfitting.", "labels": [], "entities": []}, {"text": "As such, methods that allow text to be summarised by a handful of (semantically meaningful) predictors, like topic models, gives a means to use large amounts of text data more effectively within a supervised predictive context.", "labels": [], "entities": []}, {"text": "This paper outlines a statistical framework for predictive topic modelling in a regression context.", "labels": [], "entities": [{"text": "predictive topic modelling", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.9125133951505026}]}, {"text": "First, we discuss the implementation of a relatively simple (and widely used) topic model, latent Dirichlet allocation (LDA) (, as a preprocessing step in a regression model.", "labels": [], "entities": []}, {"text": "We then compare this model to an equivalent topic model that incorporates supervised learning, supervised LDA (sLDA).", "labels": [], "entities": []}, {"text": "Using topic models in a predictive framework necessitates estimating topic proportions for new documents, however retraining the LDA model to find these is computationally expensive.", "labels": [], "entities": []}, {"text": "Hence we derive an efficient likelihood-based method for estimating topic proportions for previously unseen documents, without the need to retrain.", "labels": [], "entities": []}, {"text": "Given these two models hold the 'bag of words' assumption (i.e., they assume independence between words in a document), we also investigate the effect of introducing language structure to the model through the hidden Markov topic model (HMTM).", "labels": [], "entities": []}, {"text": "The implementation of these three topic models as a dimension reduction step fora regression model provides a framework for the implementation of further topic models, dependent on the needs of the corpus and response in question.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Table of the percentage of hard classifications  of storylines for each left-out scene in the corpus that  are correct, alongside the Brier score, for each model.", "labels": [], "entities": [{"text": "Brier score", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.8631112277507782}]}]}