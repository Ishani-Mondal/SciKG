{"title": [{"text": "Single Document Summarization as Tree Induction", "labels": [], "entities": [{"text": "Single Document Summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7750678062438965}]}], "abstractContent": [{"text": "In this paper we conceptualize single-document extractive summarization as a tree induction problem.", "labels": [], "entities": [{"text": "single-document extractive summarization", "start_pos": 31, "end_pos": 71, "type": "TASK", "confidence": 0.7145869135856628}]}, {"text": "In contrast to previous approaches (Marcu, 1999; Yoshida et al., 2014) which have relied on linguistically motivated document representations to generate summaries, our model induces a multi-root dependency tree while predicting the output summary.", "labels": [], "entities": []}, {"text": "Each root node in the tree is a summary sentence, and the subtrees attached to it are sentences whose content relates to or explains the summary sentence.", "labels": [], "entities": []}, {"text": "We design anew iterative refinement algorithm: it induces the trees through repeatedly refining the structures predicted by previous iterations.", "labels": [], "entities": []}, {"text": "We demonstrate experimentally on two benchmark datasets that our summarizer 1 performs competitively against state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information.", "labels": [], "entities": [{"text": "Single-document summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7324682772159576}]}, {"text": "The task has received much attention in the natural language processing community due to its potential for various information access applications.", "labels": [], "entities": []}, {"text": "Examples include tools which digest textual content (e.g., news, social media, reviews), answer questions, or provide recommendations.", "labels": [], "entities": []}, {"text": "Of the many summarization paradigms that have been identified over the years (see and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention.", "labels": [], "entities": []}, {"text": "In abstractive summarization, various text rewriting operations generate summaries using words or phrases that were not in the original text, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.5175845921039581}]}, {"text": "Recent Our code is publicly available at https://github.", "labels": [], "entities": []}, {"text": "com/nlpyang/SUMO.", "labels": [], "entities": []}, {"text": "approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural network architectures (.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.7962112724781036}]}, {"text": "The idea is to predict a label for each sentence specifying whether it should be included in the summary.", "labels": [], "entities": []}, {"text": "Existing systems mostly rely on recurrent neural networks (Hochreiter and) to model the document and obtain a vector representation for each sentence.", "labels": [], "entities": []}, {"text": "Intersentential relations are captured in a sequential manner, without taking the structure of the document into account, although the latter has been shown to correlate with what readers perceive as important in a text.", "labels": [], "entities": []}, {"text": "Another problem in neural-based extractive models is the lack of interpretability.", "labels": [], "entities": []}, {"text": "While capable of identifying summary sentences, these models are notable to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate).", "labels": [], "entities": []}, {"text": "The summarization literature offers examples of models which exploit the structure of the underlying document, inspired by existing theories of discourse such as Rhetorical Structure Theory (RST;.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST", "start_pos": 162, "end_pos": 194, "type": "TASK", "confidence": 0.7064648032188415}]}, {"text": "Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora ().", "labels": [], "entities": []}, {"text": "For instance, argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones).", "labels": [], "entities": []}, {"text": "Other work () extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming.", "labels": [], "entities": []}, {"text": "summarize product reviews; their system aggregates RST trees rep-", "labels": [], "entities": [{"text": "RST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.7994529604911804}]}], "datasetContent": [{"text": "In this section we present our experimental setup, describe the summarization datasets we used, discuss implementation details, our evaluation protocol, and analyze our results.", "labels": [], "entities": []}, {"text": "Calculate unnormalized edge scores: Calculate marginal probabilities: Update sentence representations: Calculate final unnormalized root and edge scores: Calculate final root and edge probabilities: 12 Function k-Hop-Propagation(e, s, k): , we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later).", "labels": [], "entities": []}, {"text": "We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset.", "labels": [], "entities": []}, {"text": "The Both datasets contain abstractive gold summaries, which are not readily suited to training extractive summarization models.", "labels": [], "entities": []}, {"text": "A greedy algorithm similar to was used to generate an oracle summary for each document.", "labels": [], "entities": []}, {"text": "The algorithm explores different combinations of sentences and generates an oracle consisting of multiple sentences which maximize the ROUGE score with the gold summary.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9794881939888}]}, {"text": "We assigned label 1 to sentences selected in the oracle summary and 0 otherwise and trained SUMO on this data.", "labels": [], "entities": [{"text": "SUMO", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8837174773216248}]}, {"text": "We evaluated summarization quality using ROUGE F 1).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9688739776611328}]}, {"text": "We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.8871678113937378}, {"text": "ROUGE-2", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9098016023635864}, {"text": "ROUGE-L", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.7884482145309448}]}, {"text": "We evaluated two variants of SUMO, with one and three structured-attention layers.", "labels": [], "entities": []}, {"text": "We compared against a baseline which simply selects the first three sentences in each document (LEAD-3) and several incarnations of the basic Transformer model introduced in Section 2.1.", "labels": [], "entities": [{"text": "LEAD-3", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9779118299484253}]}, {"text": "These include a Transformer without document-level self-attention and two variants with document-level self attention instantiated with one and three layers.", "labels": [], "entities": []}, {"text": "Several stateof-the-art models are also included in, both extractive and abstractive.", "labels": [], "entities": []}, {"text": "REFRESH) is an extractive summarization system trained by globally optimizing the ROUGE metric with reinforcement learning.", "labels": [], "entities": [{"text": "REFRESH", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8895248174667358}, {"text": "extractive summarization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.5590489506721497}, {"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9274650812149048}]}, {"text": "The system of Marcu is another extractive summarizer based on RST parsing.", "labels": [], "entities": [{"text": "Marcu", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.9668706059455872}, {"text": "RST parsing", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.8128676116466522}]}, {"text": "It uses discourse structures and RST's notion of nuclearity to score document sentences in terms of their importance and selects the most important ones as the summary.", "labels": [], "entities": [{"text": "RST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9180670380592346}]}, {"text": "Our re-implementation of used the parser of to obtain RST trees.", "labels": [], "entities": []}, {"text": "develop a summarization system which integrates a compression model that enforces grammaticality and coherence.", "labels": [], "entities": []}, {"text": "present an abstractive summarization system based on an encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "system is state-of-the-art in abstractive summarization using multiple agents to represent the document as well a hierarchical attention mechanism over the agents for decoding.", "labels": [], "entities": []}, {"text": "As far as SUMO is concerned, we observe that it outperforms a simple Transformer model without any document attention as well as variants with document attention.", "labels": [], "entities": []}, {"text": "SUMO with three layers of structured attention overall performs best, confirming our hypothesis that document-level structure is beneficial for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9887675642967224}]}, {"text": "The results in also reveal that SUMO and all Transformer-based models with document attention (doc-att) outperform LEAD-3 across metrics.", "labels": [], "entities": []}, {"text": "SUMO (3-layer) is competitive or better than stateof-the-art approaches.", "labels": [], "entities": []}, {"text": "Examples of system output are shown in.", "labels": [], "entities": []}, {"text": "Finally, we should point out that SUMO is superior to Marcu (1999) even though the latter employs linguistically informed document representations.", "labels": [], "entities": []}, {"text": "In addition to automatic evaluation, we also assessed system performance by eliciting human judgments.", "labels": [], "entities": []}, {"text": "Our first evaluation quantified the degree to which summarization models retain key information from the document following a question-answering (QA) paradigm).", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9700000286102295}]}, {"text": "We created a set of questions based on the gold summary under the assumption that it highlights the most important document content.", "labels": [], "entities": []}, {"text": "We then examined whether participants were able to answer these questions by reading system summaries alone without access to the article.", "labels": [], "entities": []}, {"text": "The more questions a system can answer, the better it is at summarizing the document as a whole.", "labels": [], "entities": [{"text": "summarizing the document", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8788804411888123}]}, {"text": "We randomly selected 20 documents from the CNN/DailyMail and NYT datasets, respectively and wrote multiple question-answer pairs for each gold summary.", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8881931304931641}, {"text": "NYT datasets", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8429166674613953}]}, {"text": "We created 71 questions in total varying from two to six questions per gold summary.", "labels": [], "entities": []}, {"text": "We asked participants to read the summary and answer all associated questions as best they could without access to the original document or the gold summary.", "labels": [], "entities": []}, {"text": "Examples of questions and their answers are given in.", "labels": [], "entities": []}, {"text": "We adopted the same scoring mechanism used in  with a score of one, partially correct answers with a score of 0.5, and zero otherwise.", "labels": [], "entities": []}, {"text": "Answers were elicited using Amazon's Mechanical Turk platform.", "labels": [], "entities": []}, {"text": "Participants evaluated summaries produced by the LEAD-3 baseline, our 3-layered SUMO model and multiple state-of-the-art systems.", "labels": [], "entities": [{"text": "LEAD-3 baseline", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.7675260603427887}]}, {"text": "We elicited 5 responses per summary.", "labels": [], "entities": []}, {"text": "Overall, we observe there is room for improvement since no system comes close to the extractive oracle, indicating that improved sentence selection would bring further performance gains to extractive approaches.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.6949885487556458}]}, {"text": "Between-systems differences are all statistically significant (using a one-way ANOVA with posthoc Tukey HSD tests; p < 0.01) with the exception of LEAD-3 and See et al.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.961150050163269}, {"text": "LEAD-3", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.7328426241874695}]}, {"text": "Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria: Informativeness , Fluency, and Succinctness.", "labels": [], "entities": [{"text": "Informativeness", "start_pos": 154, "end_pos": 169, "type": "METRIC", "confidence": 0.9910195469856262}, {"text": "Fluency", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9945040941238403}, {"text": "Succinctness", "start_pos": 185, "end_pos": 197, "type": "METRIC", "confidence": 0.9964447617530823}]}, {"text": "The study was conducted on the Amazon Mechanical Turk platform using Best-Worst Scaling (), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk platform", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.929402232170105}]}, {"text": "Participants were presented with a document and CNN+DM NYT P H EA P H EA Parser 24.8 8.9 -18.7 10.6 -SUMO (1-layer) 69.0 2.9 23.1 54.7 3.6 20.6 SUMO (3-layer) 52.7 3.7 25.3 45.1 6.2 21.6 Left Branching --21.4 --21.3 Right Branching --7.3 --6.7: Descriptive statistics Projectivity(%), Height and EdgeAgreement(%) for dependency trees produced by our model and the RST discourse parser of.", "labels": [], "entities": [{"text": "CNN+DM NYT P H EA P H EA Parser 24.8", "start_pos": 48, "end_pos": 84, "type": "DATASET", "confidence": 0.9043821940819422}, {"text": "Height", "start_pos": 285, "end_pos": 291, "type": "METRIC", "confidence": 0.9634480476379395}, {"text": "RST discourse parser", "start_pos": 364, "end_pos": 384, "type": "TASK", "confidence": 0.6419074535369873}]}, {"text": "Results are shown on the CNN/DailyMail and NYT test sets.", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8873658577601115}, {"text": "NYT test sets", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9063733220100403}]}, {"text": "summaries generated from 3 out of 7 systems and were asked to decide which summary was better and which one was worse, taking into account the criteria mentioned above.", "labels": [], "entities": []}, {"text": "We used the same 20 documents from each dataset as in our QA evaluation and elicited 5 responses per comparison.", "labels": [], "entities": []}, {"text": "The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst.", "labels": [], "entities": []}, {"text": "Ratings range from -1 (worst) to 1 (best).", "labels": [], "entities": [{"text": "Ratings", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8990247845649719}]}, {"text": "As shown in Table 2 (Rank column), participants overwhelming prefer the extractive oracle summaries followed by.", "labels": [], "entities": []}, {"text": "Abstractive systems () perform relatively poorly in this evaluation; we suspect that humans are less forgiving to fluency errors and slightly incoherent summaries.", "labels": [], "entities": []}, {"text": "Interestingly, gold summaries fare worse than the oracle and extractive systems.", "labels": [], "entities": []}, {"text": "Albeit fluent, gold summaries naturally contain less detail compared to oracle-based ones; on virtue of being abstracts, they are written in a telegraphic style, often in conversational language while participants prefer the more lucid style of the extracts.", "labels": [], "entities": []}, {"text": "All pairwise comparisons among systems are statistically significant (using a one-way ANOVA with post-hoc Tukey HSD tests; p < 0.", "labels": [], "entities": []}, {"text": "To gain further insight into the structures learned by SUMO, we inspected the trees it produces.", "labels": [], "entities": []}, {"text": "Specifically, we used the Chu-Liu-Edmonds algorithm ( to extract the maximum spanning tree from the attention scores.", "labels": [], "entities": []}, {"text": "We report various statistics on the characteristics of the induced trees across datasets in.", "labels": [], "entities": []}, {"text": "We also examine the trees learned from different SUMO variants (with different numbers of iterations) in order to establish whether the iterative process yields better structures.", "labels": [], "entities": []}, {"text": "Specifically, we compared the dependency trees obtained from our model to those produced by a discourse parser () trained on a corpus which combines annotations from the RST treebank () and the Penn Treebank (.", "labels": [], "entities": [{"text": "RST treebank", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.9071072638034821}, {"text": "Penn Treebank", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.9932639598846436}]}, {"text": "Unlike traditional RST discourse parsers, which first segment a document into Elementary Discourse Units (EDUs) and then build a discourse tree with the EDUs 2 as leaves, parse a document into an RST tree along with its syntax subtrees without segmenting it into EDUs.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.9397457043329874}]}, {"text": "The outputs of their parser are ideally suited for comparison with our model, since we only care about document-level structures, and ignore the subtrees within sentence boundaries.", "labels": [], "entities": []}, {"text": "We converted the constituency RST trees obtained from the discourse parser into dependency trees using.", "labels": [], "entities": []}, {"text": "As can be seen in, the dependency structures induced by SUMO are simpler compared to those obtained from the discourse parser.", "labels": [], "entities": []}, {"text": "Our trees are generally shallower, almost half of them are projective.", "labels": [], "entities": []}, {"text": "We also calculated the percentage of head-dependency edges that are identical between learned trees and parser generated ones.", "labels": [], "entities": []}, {"text": "Although SUMO is not exposed to any annotated trees during training, a number of edges agree with the outputs of the discourse parser.", "labels": [], "entities": []}, {"text": "Moreover, we observe that the iterative process involving multiple structured attention layers helps generate better discourse trees.", "labels": [], "entities": []}, {"text": "We also compare SUMO trees against a left-and right-branching baseline, where the document is trivially parsed into a left-and right-branching tree forming a chain-like structure.", "labels": [], "entities": []}, {"text": "As shown in, SUMO outperforms these baselines (with the exception of the onelayered model on NYT).", "labels": [], "entities": [{"text": "NYT", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9230064749717712}]}, {"text": "We should also point out that the edge agreement between SUMO generated trees and left/right branching trees is low (around 30% on both datasets), indicating that the trees we learn are different from a simple chain.", "labels": [], "entities": []}, {"text": "In 2001, the Taliban wiped out 1700 years of history in a matter of seconds, by blowing up ancient Buddha statues in central Afghanistan with dynamite.", "labels": [], "entities": []}, {"text": "They proceeded to do so after an attempt at bringing down the 175-foot tall sculptures with anti-aircraft artillery had failed.", "labels": [], "entities": []}, {"text": "Sadly, the event was just the first in a series of atrocities that have robbed the world of some of its most prized cultural heritage.", "labels": [], "entities": []}, {"text": "The Road Home, the Louisiana grant program for homeowners who lost their houses to hurricanes Katrina and Rita, is expected to cost far more than the $7.5 billion provided by the Federal Government, in part because many more families have applied than officials had anticipated.", "labels": [], "entities": []}, {"text": "As a result, Louisiana officials on Tuesday night set a July 31 deadline for applicants, who can receive up to $150,000 to repair or rebuild their houses.", "labels": [], "entities": []}, {"text": "With the cutoff date, the State hopes to be able to figure out how much more money it needs to pay for the program.", "labels": [], "entities": []}, {"text": "The Taliban wiped out 1700 years of history in a matter of seconds.", "labels": [], "entities": []}, {"text": "The thought of losing apiece of our collective history is a bleak one.", "labels": [], "entities": []}, {"text": "But if loss can't be avoided, technology can lend a hand.", "labels": [], "entities": []}, {"text": "Louisiana grant program for homeowners who lost their houses to hurricanes Katrina and Rita is expected to cost far more than $7.5 billion provided by federal government.", "labels": [], "entities": []}, {"text": "Louisiana officials set July 31 deadline for applicants, who can receive up to $150,000 to repair or rebuild their houses.", "labels": [], "entities": []}, {"text": "Sadly, the event was just the first in a series of atrocities that have robbed the world of some of its most prized cultural heritage.", "labels": [], "entities": []}, {"text": "But historical architecture is also under threat from calamities which might well escape our control, such as earthquakes and climate change.", "labels": [], "entities": []}, {"text": "The thought of losing apiece of our collective history is a bleak one.", "labels": [], "entities": []}, {"text": "The Road Home, the Louisiana grant program for homeowners who lost their houses to hurricanes Katrina and Rita, is expected to cost far more than the $7.5 billion provided by the federal government, in part because many more families have applied than officials had anticipated.", "labels": [], "entities": []}, {"text": "With the cutoff date, the State hopes to be able to figure out how much more money it needs to pay for the program.", "labels": [], "entities": []}, {"text": "The shortfall is projected to be $2.9 billion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set results on the CNN/DailyMail and NYT datasets using ROUGE F 1 (R-1 and R-2 are shorthands  for unigram and bigram overlap, R-L is the longest common subsequence.", "labels": [], "entities": [{"text": "CNN/DailyMail and NYT datasets", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.8467657566070557}, {"text": "ROUGE F 1", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.896501878897349}]}, {"text": " Table 2: System ranking according to human judg- ments on summary quality and QA-based evaluation.", "labels": [], "entities": []}]}