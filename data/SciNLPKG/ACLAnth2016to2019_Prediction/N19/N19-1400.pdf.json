{"title": [{"text": "Conversation Initiation by Diverse News Contents Introduction", "labels": [], "entities": []}], "abstractContent": [{"text": "In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting.", "labels": [], "entities": []}, {"text": "However, most existing conversation systems cannot play this role.", "labels": [], "entities": []}, {"text": "Previous studies on conversation systems assume that the user always initiates conversation , and have placed emphasis on how to respond to the given user's utterance.", "labels": [], "entities": []}, {"text": "As a result, existing conversation systems become passive.", "labels": [], "entities": []}, {"text": "Namely they continue waiting until being spoken to by the users.", "labels": [], "entities": []}, {"text": "In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation.", "labels": [], "entities": []}, {"text": "Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings.", "labels": [], "entities": []}, {"text": "To this end, we propose to generate initial utterance by summarizing and chatting about news articles , which provide fresh and various contents everyday.", "labels": [], "entities": [{"text": "summarizing and chatting about news articles", "start_pos": 57, "end_pos": 101, "type": "TASK", "confidence": 0.7860305706659952}]}, {"text": "To address the lack of the training data for this task, we constructed a novel large-scale dataset through crowd-sourcing.", "labels": [], "entities": []}, {"text": "We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities).", "labels": [], "entities": []}, {"text": "We present several approaches to conversation initiation including information retrieval based and generation based models.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8405804932117462}]}, {"text": "Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversation 1 systems are becoming increasingly important as a means to facilitate human-computer: Conversation initiation task.", "labels": [], "entities": [{"text": "Conversation initiation task", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.8141242663065592}]}, {"text": "The system in this example is given a news post about \"iPhone\" and generates an initial utterance for chatting about it. communication.", "labels": [], "entities": []}, {"text": "However, most of the studies on conversation systems have been based on the assumption that a human always initiates conversation.", "labels": [], "entities": []}, {"text": "As a result, the systems are designed to be passive, meaning that they keep waiting until they are spoken to by the human and will never speak to the human proactively.", "labels": [], "entities": []}, {"text": "For example, popular encoder-decoder models are designed to respond to input utterances provided by humans, and it is difficult for them to proactively initiate the conversation.", "labels": [], "entities": []}, {"text": "Although some systems are able to initiate conversations, they basically adopt template-based generation methods and thus lack diversity.", "labels": [], "entities": []}, {"text": "This paper investigates generating the very first utterance in a conversation.", "labels": [], "entities": []}, {"text": "We feel strongly that conversation systems should not always be passive; sometimes, they have to proactively initiate the conversation to enable more natural conversation.", "labels": [], "entities": []}, {"text": "In addition, it is crucial to be able to initiate conversation in various ways in actual applications, since systems that initiate a conversation by always saying \"Let's talk about something\" or \"Hello\" are inherently boring.", "labels": [], "entities": []}, {"text": "We propose a task setting in which the system initiates a conversation by talking about a news topic.", "labels": [], "entities": []}, {"text": "In this task, the system is provided with a news post to talk about and uses it to generate the initial utterance of the conversation).", "labels": [], "entities": []}, {"text": "This task is referred to as conversation initiation in this paper.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9000138342380524}]}, {"text": "We have two primary reasons for using news posts.", "labels": [], "entities": []}, {"text": "First, sharing and exchanging opinions about the latest news with friends is common in our daily conversations () (e.g, asking something like \"What do you think about today's news on Trump?\").", "labels": [], "entities": [{"text": "sharing and exchanging opinions about the latest news with friends", "start_pos": 7, "end_pos": 73, "type": "TASK", "confidence": 0.6036662012338638}]}, {"text": "Second, and more importantly, this task setting allows us to proactively generate diverse utterances to initiate conversations by simply using the latest news posts, which include a wide variety of content published daily.", "labels": [], "entities": []}, {"text": "We created a large-scale dataset for training and evaluating conversation initiation models through a crowd-sourcing service.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7736549377441406}]}, {"text": "The crowd-sourcing workers were presented with news posts collected from Twitter and asked to create utterances to initiate a conversation about the post.", "labels": [], "entities": []}, {"text": "The resulting dataset will be released to facilitate future studies at the time of publication.", "labels": [], "entities": []}, {"text": "We developed several neural models, including retrieval-based and generation-based ones, to empirically compare their performances.", "labels": [], "entities": []}, {"text": "We also compared the proposed models against baselines that utilize automatically constructed training dataset to investigate the effectiveness of our dataset.", "labels": [], "entities": []}, {"text": "Both automatic and manual evaluation were used to assess not only the quality but also the diversity of the generated initial utterances.", "labels": [], "entities": []}, {"text": "The results indicate that the proposed models successfully generated initial utterances for the given news posts, and significantly outperformed the baseline models.", "labels": [], "entities": []}, {"text": "Our contributions are the following: \u2022 We investigate the task of conversation initiation, which has been largely overlooked in previous studies.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8908416628837585}]}, {"text": "\u2022 We construct and release a large-scale dataset for conversation initiation.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8661754429340363}]}, {"text": "\u2022 We develop several neural models and empirically compare their effectiveness on our dataset.", "labels": [], "entities": []}, {"text": "2 Related work 2.1 Non-task-oriented Conversation System There are many existing studies on non-taskoriented conversation systems.", "labels": [], "entities": []}, {"text": "Research started with rule-based methods and gradually shifted to statistical approaches (, and many follow-up studies have since been undertaken to improve the quality of the generated responses ().", "labels": [], "entities": []}, {"text": "However, the task of conversation initiation has been largely absent in these studies.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.9301118552684784}]}], "datasetContent": [{"text": "In this section, we explain how we constructed the dataset for the task of conversation initiation.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.8697251975536346}]}, {"text": "We then analyze the constructed dataset to provide insights into its effectiveness.", "labels": [], "entities": []}, {"text": "We empirically evaluate the performance of the proposed methods on the constructed dataset.", "labels": [], "entities": []}, {"text": "We divided the 104,960 items of data (news post and initial utterance pairs) into 90,000, 10,000, and 4,960 for training data, development data, and test data, respectively.", "labels": [], "entities": []}, {"text": "Input news posts that appear in the training data were removed from the test data.", "labels": [], "entities": []}, {"text": "Consequently, 4,776 data were used as the final test data.", "labels": [], "entities": []}, {"text": "To train the baseline model, we collected 277,813 tweets and their corresponding replies from six major Japanese news accounts 6 on Twitter.", "labels": [], "entities": []}, {"text": "We then divided those pairs into 260,000 and 17,813 for training data and development data for the baselines.", "labels": [], "entities": []}, {"text": "We performed tokenization using a Japanese morphological analyzer, MeCab, 7 with IPAdic dictionary, 8 and then removed usernames, URLs, and hashtags.", "labels": [], "entities": []}, {"text": "We used OpenNMT-py ( for building the models described in Section 4.", "labels": [], "entities": []}, {"text": "Their hyperparameter settings are given in.", "labels": [], "entities": []}, {"text": "We used GloVe ( to learn 300-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "We trained word embedding using a Japanese Wikipedia dump released on February 22nd, 2018.", "labels": [], "entities": [{"text": "Japanese Wikipedia dump released on February 22nd", "start_pos": 34, "end_pos": 83, "type": "DATASET", "confidence": 0.8855096016611371}]}, {"text": "These embeddings were used for acquiring news post embeddings, as described in Section 4.1.", "labels": [], "entities": []}, {"text": "As discussed in Section 3.2, since the initial utterance can be divided into separate parts that have different properties, we evaluated each part separately to examine the generated initial utterances.", "labels": [], "entities": []}, {"text": "We automatically divided the generated sentences and reference sentences into summary parts and chit-chat parts, as explained in Section 3.2.", "labels": [], "entities": []}, {"text": "We used ROUGE-1, ROUGE-2, and ROUGE-L () for evaluating the summary part (denoted as R-1, R-2, and R-L, respectively) and BLEU () for evaluating the chit-chat part.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9628679752349854}, {"text": "ROUGE-2", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9362984895706177}, {"text": "ROUGE-L", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9815915822982788}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9990032315254211}]}, {"text": "We use different metrics for each part because ROUGE is often used for summarization tasks while BLEU is used for conversational tasks.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9855090975761414}, {"text": "summarization tasks", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.9236755967140198}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.998015284538269}]}, {"text": "Since these automatic metrics are insufficient for evaluation (, we also perform a manual evaluation in Section 5.4.", "labels": [], "entities": []}, {"text": "To evaluate diversity, we calculate the proportion of distinct unigrams, bigrams, and sentences) in the generated initial utterances ().", "labels": [], "entities": []}, {"text": "lists the results of the summary part.", "labels": [], "entities": []}, {"text": "The baseline method that outputs the first sentence of an input news post achieved higher ROUGE scores than the proposed methods.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9974129796028137}]}, {"text": "This does not necessarily mean that the proposed methods are poor because even the SOTA summarization system exceeds such a baseline by only a small margin ().", "labels": [], "entities": [{"text": "SOTA summarization", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7837860882282257}]}, {"text": "Also, our task has a requirement to convert sentences into colloquial expressions, and the ROUGE metric cannot capture such a subtle difference.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9710728526115417}]}, {"text": "We perform a deeper investigation into the quality of the generated initial utterance in the next section.", "labels": [], "entities": []}, {"text": "Regarding the diversity, almost all of the generated initial utterances are distinct, as shown in. shows the result of the chit-chat part.", "labels": [], "entities": []}, {"text": "The proposed methods outperformed the baselines in terms of BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9813218116760254}]}, {"text": "Although the baselines use two times as much training data as the proposed methods, their scores were quite low.", "labels": [], "entities": []}, {"text": "This demonstrates the quality of our dataset.", "labels": [], "entities": []}, {"text": "The score of Separate (IR) was relatively low among the proposed methods, presumably because the chitchat parts retrieved from the training data do not always match the content of the input news post.", "labels": [], "entities": [{"text": "score of Separate (IR)", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.8610775371392568}]}, {"text": "We also see that all the BLEU scores of the models are quite lower than ROUGE scores in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9996926784515381}, {"text": "ROUGE", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9980663657188416}]}, {"text": "In general, both summarization and chat generation tasks often use automatic evaluation metrics to evaluate generated sentences, their scores tend to be much lower in the chat generation task.", "labels": [], "entities": [{"text": "summarization and chat generation", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.6356105506420135}]}, {"text": "This is because the answer sentences (utterances) of the chat generation task have more diverse candidates than other generation tasks such as machine translation and summarization ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7665226757526398}, {"text": "summarization", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.8989114165306091}]}, {"text": "We also examine the diversity of the chit-chat part in.", "labels": [], "entities": []}, {"text": "Although the diversity of the IR-based methods was high, their BLEU scores deteriorated considerably.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9993732571601868}]}, {"text": "Among the generation-based methods, although Separate (Gen+MMI) achieved the highest BLEU score, it lacked diversity.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.983507364988327}]}, {"text": "In contrast, Joint achieved a reasonable BLEU score while maintaining diversity to some extent.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9821501672267914}]}, {"text": "Although diversity of utterances can be quantified automatically, ROUGE and BLEU scores do not always follow human intuition (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9958083629608154}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9972936511039734}]}, {"text": "Therefore, we evaluate the generated initial utterances manually.", "labels": [], "entities": []}, {"text": "We picked the three proposed models with good performance in the automatic evaluation along with one baseline for this manual evaluation.", "labels": [], "entities": []}, {"text": "300 posts were sampled as the input news posts, and the outputs of the four methods were manually evaluated from two perspectives: 1) Naturalness: Does the utterance naturally initiate conversation? and 2) Coherency: Is the content of the utterance coherent with the given news post?", "labels": [], "entities": []}, {"text": "We recruited crowd workers to score each utterance on a 4-point scale (Agree, Slightly Agree, Slightly Disagree, Disagree).", "labels": [], "entities": [{"text": "Agree", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9937587976455688}, {"text": "Agree", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.48936712741851807}]}, {"text": "show the results of the manual evaluation for Naturalness and Coherency of the generated initial utterances.", "labels": [], "entities": []}, {"text": "The proposed methods excluding Separate (IR) outperformed Baseline (Gen+MMI) in both perspectives and achieved reasonable scores compared to human upperbound.", "labels": [], "entities": []}, {"text": "The scores of Separate (IR) are quite low because the retrieval result does not follow the input news post in many cases.", "labels": [], "entities": [{"text": "Separate (IR)", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.7380673587322235}]}, {"text": "This reveals that although those sentences have high diversity, their quality is poor as initial utterances.", "labels": [], "entities": []}, {"text": "Although Baseline (Gen+MMI) achieved high ROUGE scores in, its style is not colloquial.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9960641264915466}]}, {"text": "Thus, workers felt odd and lowered their scores.", "labels": [], "entities": []}, {"text": "In conclusion, it is better to use the generation-based methods for conversation initiation.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8768337965011597}]}, {"text": "We also evaluated Dullness: Is the given utterance dull or boring?", "labels": [], "entities": [{"text": "Dullness", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.7812027931213379}]}, {"text": "We used 15 manually created boilerplate utterances (e.g., Hello., How are you?, Let's talk with me.) rather than Baseline (Gen+MMI) to confirm the effectiveness of utilizing news contents as the initial utterances.", "labels": [], "entities": []}, {"text": "show the results of the manual evaluation for Dullness of the generated initial utterances.", "labels": [], "entities": [{"text": "Dullness", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9973530769348145}]}, {"text": "We see that compared to our proposed methods, the score of the boilerplate baseline is quite high.", "labels": [], "entities": []}, {"text": "This indicates that using boilerplate utterances for conversation initiation often bores users and possibly leads to early abandonment of the conversation.", "labels": [], "entities": [{"text": "conversation initiation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7644992470741272}]}, {"text": "To determine the statistical significance of our results, we performed Wilcoxon signed-rank tests with Bonferroni correction).", "labels": [], "entities": []}, {"text": "In, for all combinations except Baseline (Gen+MMI) vs. Separate (IR) and Separate (Gen+MMI) vs. Joint, there were significant differences (p-value < 0.005 (corrected)) in both perspectives.", "labels": [], "entities": []}, {"text": "Similarly, in, there were statistically significant differences for all combinations except Separate (IR) vs. Separate (Gen+MMI) and Separate (Gen+MMI) vs. Joint.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution over sampled dialogue acts and example initial utterances. Italics are chit-chat parts.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the dataset. First and second  columns show the average numbers per utterance.", "labels": [], "entities": []}, {"text": " Table 3: Hyperparameter settings for training encoder- decoder models.", "labels": [], "entities": []}, {"text": " Table 4: Results of summary part generation.", "labels": [], "entities": [{"text": "summary part generation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.5829795002937317}]}, {"text": " Table 5: Results of chit-chat part generation.", "labels": [], "entities": [{"text": "chit-chat part generation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6999833285808563}]}, {"text": " Table 6: Results of evaluating Naturalness and Co- herency of the generated utterances by the manual  evaluation (higer is better).", "labels": [], "entities": []}, {"text": " Table 7: Results of evaluating Dullness of the gener- ated utterances by the manual evaluation (lower is bet- ter). Boilerplate uses manually created boilerplate ut- terances.", "labels": [], "entities": [{"text": "Dullness", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9959314465522766}]}]}