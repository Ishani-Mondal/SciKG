{"title": [{"text": "BAM: A combination of deep and shallow models for German Dialect Identification", "labels": [], "entities": [{"text": "BAM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5539912581443787}, {"text": "German Dialect Identification", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6942512591679891}]}], "abstractContent": [{"text": "In this paper, we present a machine learning approach for the German Dialect Identification (GDI) Closed Shared Task of the DSL 2019 Challenge.", "labels": [], "entities": [{"text": "German Dialect Identification (GDI) Closed Shared Task", "start_pos": 62, "end_pos": 116, "type": "TASK", "confidence": 0.7496754659546746}, {"text": "DSL 2019 Challenge", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.7592270771662394}]}, {"text": "The proposed approach combines deep and shallow models, by applying a voting scheme on the outputs resulted from a Character-level Convolutional Neural Networks (Char-CNN), a Long Short-Term Memory (LSTM) network, and a model based on String Kernels.", "labels": [], "entities": []}, {"text": "The first model used is the Char-CNN model that merges multiple con-volutions computed with kernels of different sizes.", "labels": [], "entities": []}, {"text": "The second model is the LSTM network which applies a global max pooling over the returned sequences overtime.", "labels": [], "entities": []}, {"text": "Both models pass the activation maps to two fully-connected layers.", "labels": [], "entities": []}, {"text": "The final model is based on String Kernels, computed on character p-grams extracted from speech transcripts.", "labels": [], "entities": []}, {"text": "The model combines two blended kernel functions, one is the presence bits kernel, and the other is the intersection kernel.", "labels": [], "entities": []}, {"text": "The empirical results obtained in the shared task prove that the approach can achieve good results.", "labels": [], "entities": []}, {"text": "The system proposed in this paper obtained the fourth place with a macro-F 1 score of 62.55%.", "labels": [], "entities": [{"text": "macro-F 1 score", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.8304716149965922}]}], "introductionContent": [{"text": "Being at its third edition, the 2019 VarDial Evaluation Campaign () includes two shared tasks on dialect identification which proves that researchers are still interested in this challenging NLP task.", "labels": [], "entities": [{"text": "VarDial Evaluation Campaign", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.633690615495046}, {"text": "dialect identification", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7825765013694763}]}, {"text": "For example, in the 2018 GDI Shared Task (), a system () that uses a series of language models based on character n-grams achieves state-of-the-art with a macro-F 1 score near 69%, in a 4-way classification setting.", "labels": [], "entities": [{"text": "macro-F 1 score", "start_pos": 155, "end_pos": 170, "type": "METRIC", "confidence": 0.725936750570933}]}, {"text": "For the 2019 GDI Shared Task, the organizers have included audio features together with speech transcripts, and also provided a word-level normalization for each transcript.", "labels": [], "entities": []}, {"text": "For solving this task, we propose a combination of deep and shallow models, by applying a voting scheme on the outputs resulted from a Character-level Convolutional Neural Networks (Char-CNN), a Long Short-Term Memory (LSTM) network, and a model based on String Kernels.", "labels": [], "entities": []}, {"text": "In the 2019 GDI Shared Task, the participants had to discriminate between four German dialects, in a 4-way classification setting.", "labels": [], "entities": [{"text": "GDI Shared Task", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.6448385318120321}]}, {"text": "A number of 6 participants have submitted their results, and the model proposed in this paper obtained 4 th place with an accuracy of 62.95%, and macro-F 1 score of 62.55%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9996215105056763}, {"text": "macro-F 1 score", "start_pos": 146, "end_pos": 161, "type": "METRIC", "confidence": 0.7857624987761179}]}, {"text": "The best scoring system that we submitted for the GDI Shared Task is an ensemble that combines both deep and shallow models.", "labels": [], "entities": [{"text": "GDI Shared Task", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.5097232858339945}]}, {"text": "The system uses features from two deep models, CharCNNs and LSTMs, and also from a shallow model that combines several kernels using multiple kernel learning.", "labels": [], "entities": []}, {"text": "The Char-CNN model merges convolutions computed with kernels of different sizes to learn a first group of features.", "labels": [], "entities": []}, {"text": "The LSTM network learns the second group of features by applying a global max pooling over the returned sequences overtime.", "labels": [], "entities": []}, {"text": "For the String Kernel model, we combined two kernel functions.", "labels": [], "entities": []}, {"text": "The first kernel used is the p-grams presence bits kernel 1 which takes into account only the presence of p-grams instead of their frequencies.", "labels": [], "entities": []}, {"text": "The second kernel is the histogram intersection kernel 2 , which was first used in a text mining task by ().", "labels": [], "entities": [{"text": "histogram intersection", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7446937561035156}, {"text": "text mining task", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.8375007510185242}]}, {"text": "This kernel functions proved useful in previous dialect identification shared tasks (.", "labels": [], "entities": [{"text": "dialect identification shared tasks", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.8000302463769913}]}, {"text": "There are two steps in the learning process.", "labels": [], "entities": []}, {"text": "In the first step, the deep models are trained individually using the Adam optimization algorithm ().", "labels": [], "entities": []}, {"text": "In the second step, the string kernel model is learned by applying Kernel Ridge Regression (KRR)).", "labels": [], "entities": [{"text": "Kernel Ridge Regression (KRR))", "start_pos": 67, "end_pos": 97, "type": "METRIC", "confidence": 0.8033622652292252}]}, {"text": "Finally, a voting schema is applied to obtain the final class fora test sample.", "labels": [], "entities": []}, {"text": "Before deciding the final system, we tuned each model for the task.", "labels": [], "entities": []}, {"text": "First of all, we tuned the string kernels model by trying out p-grams of various lengths, including blended variants of string kernels as well.", "labels": [], "entities": []}, {"text": "Besides blended variants, we evaluated individual kernels, and also various kernel combinations.", "labels": [], "entities": []}, {"text": "Second of all, we tuned the Char-CNN model, by trying out various convolution lengths, number of filters and depths.", "labels": [], "entities": []}, {"text": "Finally, we tuned the LSTM model by seeking the best number of output units.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.7118200063705444}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Work related to German dialect identification, models based on Character-Level Convolutional Neural Networks, Long Short-Term Memory Networks, and methods based on string kernels is presented in Section 2.", "labels": [], "entities": [{"text": "German dialect identification", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6783145666122437}]}, {"text": "Section 3 presents Char-CNNs, LSTMs and the string kernel models used in this approach.", "labels": [], "entities": []}, {"text": "In this section, we also present the ensemble model.", "labels": [], "entities": []}, {"text": "Details about the German dialect identification experiments are provided in Section 4.", "labels": [], "entities": [{"text": "German dialect identification", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.5952215393384298}]}, {"text": "Finally, we draw the conclusion in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on the test set of the 2019 GDI Shared  Task (closed training) of the method described in this  paper.", "labels": [], "entities": [{"text": "GDI Shared  Task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.5779261986414591}]}]}