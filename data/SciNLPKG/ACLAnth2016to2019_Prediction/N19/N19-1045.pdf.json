{"title": [{"text": "Aligning Vector-spaces with Noisy Supervised Lexicons", "labels": [], "entities": [{"text": "Aligning Vector-spaces", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8483045995235443}]}], "abstractContent": [{"text": "The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP.", "labels": [], "entities": []}, {"text": "Current solutions assume that the lexicon which defines the alignment pairs is noise-free.", "labels": [], "entities": []}, {"text": "We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process.", "labels": [], "entities": []}, {"text": "We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9992874264717102}]}, {"text": "We propose a model that accounts for noisy pairs.", "labels": [], "entities": []}, {"text": "This is achieved by introducing a generative model with a compatible iterative EM algorithm.", "labels": [], "entities": []}, {"text": "The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our proposed algorithm on two alignment problems: bilingual word embedding translation , and mapping between diachronic embedding spaces for recovering the semantic shifts of words across time periods.", "labels": [], "entities": [{"text": "bilingual word embedding translation", "start_pos": 86, "end_pos": 122, "type": "TASK", "confidence": 0.6843906790018082}, {"text": "recovering the semantic shifts of words across time periods", "start_pos": 177, "end_pos": 236, "type": "TASK", "confidence": 0.7327563928233253}]}], "introductionContent": [{"text": "We consider the problem of mapping between points in different vector spaces.", "labels": [], "entities": []}, {"text": "This problem has prominent applications in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8067354758580526}]}, {"text": "Some examples are creating bilingual word lexicons (, machine translation, hypernym generation (, diachronic embeddings alignment) and domain adaptation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7858642637729645}, {"text": "hypernym generation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6812928020954132}, {"text": "domain adaptation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7450011372566223}]}, {"text": "In all these examples one is given word embeddings in two different vector spaces, and needs to learn a mapping from one to the other.", "labels": [], "entities": []}, {"text": "The problem is traditionally posed as a supervised learning problem, in which we are given two sets of vectors (e.g.: word-vectors in Italian and in English) and a lexicon mapping the points between the two sets (known word-translation pairs).", "labels": [], "entities": []}, {"text": "Our goal is to learn a mapping that will correctly map the vectors in one space (e.g.: English word embeddings) to their known corresponding vectors in the other (e.g.: Italian word embeddings).", "labels": [], "entities": []}, {"text": "The mapping will then be used to translate vectors for which the correspondence is unknown.", "labels": [], "entities": []}, {"text": "This setup was popularized by.", "labels": [], "entities": []}, {"text": "The supervised setup assumes a perfect lexicon.", "labels": [], "entities": []}, {"text": "Here, we consider what happens in the presence of training noise, where some of the lexicon's entries are incorrect in the sense that they don't reflect an optimal correspondence between the word vectors.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and  En\u2192Es improvements are significant at p < 0.05 according to ANOVA on the different runs.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.6185702681541443}]}]}