{"title": [{"text": "Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Bi-Directional Differentiable Input Reconstruction", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6124442219734192}, {"text": "Low-Resource Neural Machine Translation", "start_pos": 55, "end_pos": 94, "type": "TASK", "confidence": 0.6541885435581207}]}], "abstractContent": [{"text": "We aim to better exploit the limited amounts of parallel text available in low-resource settings by introducing a differentiable reconstruction loss for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 153, "end_pos": 185, "type": "TASK", "confidence": 0.8232560654481252}]}, {"text": "This loss compares original inputs to reconstructed inputs, obtained by back-translating translation hypotheses into the input language.", "labels": [], "entities": []}, {"text": "We leverage differentiable sampling and bi-directional NMT to train models end-to-end, without introducing additional parameters.", "labels": [], "entities": []}, {"text": "This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9947262406349182}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8329026500384012}]}, {"text": "Past work has addressed this problem by leveraging monolingual data or multilingual parallel data ().", "labels": [], "entities": []}, {"text": "We hypothesize that the traditional training can be complemented by better leveraging limited training data.", "labels": [], "entities": []}, {"text": "To this end, we propose anew training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples.", "labels": [], "entities": []}, {"text": "Input reconstruction is motivated by the idea of round-trip translation.", "labels": [], "entities": [{"text": "Input reconstruction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8739525675773621}, {"text": "round-trip translation", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6463043689727783}]}, {"text": "Suppose sentence f is translated forward toe using model \u03b8 f e and then translated back t\u00f4 f using model \u03b8 ef , then e is more likely to be a good translation if the distance between\u02c6fbetween\u02c6 between\u02c6f and f is small.", "labels": [], "entities": []}, {"text": "Prior work applied round-trip translation to monolingual examples and sampled the intermediate translation e from a K-best list generated by model \u03b8 f e using beam search (.", "labels": [], "entities": [{"text": "round-trip translation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.6728265583515167}]}, {"text": "However, beam search is not differentiable which prevents back-propagating reconstruction errors to \u03b8 f e . As a result, reinforcement learning algorithms, or independent updates to \u03b8 f e and \u03b8 ef were required.", "labels": [], "entities": [{"text": "beam search", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9117611646652222}]}, {"text": "In this paper, we focus on the problem of making input reconstruction differentiable to simplify training.", "labels": [], "entities": [{"text": "input reconstruction differentiable", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.8016829689343771}]}, {"text": "In past work, addressed this issue by reconstructing source sentences from the decoder's hidden states.", "labels": [], "entities": []}, {"text": "However, this reconstruction task can be artificially easy if hidden states over-memorize the input.", "labels": [], "entities": []}, {"text": "This approach also requires a separate auxiliary reconstructor, which introduces additional parameters.", "labels": [], "entities": []}, {"text": "We propose instead to combine benefits from differentiable sampling and bi-directional NMT to obtain a compact model that can be trained endto-end with back-propagation.", "labels": [], "entities": []}, {"text": "Specifically, \u2022 Translations are sampled using the StraightThrough Gumbel Softmax (STGS) estimator (, which allows back-propagating reconstruction errors.", "labels": [], "entities": [{"text": "Translations", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9683406352996826}, {"text": "StraightThrough Gumbel Softmax (STGS) estimator", "start_pos": 51, "end_pos": 98, "type": "METRIC", "confidence": 0.628458878823689}]}, {"text": "\u2022 Our approach builds on the bi-directional NMT model (, which improves low-resource translation by jointly modeling translation in both directions (e.g., Swahili \u2194 English).", "labels": [], "entities": []}, {"text": "A single bi-directional model is used as a translator and a reconstructor (i.e. \u03b8 ef = \u03b8 f e ) without introducing more parameters.", "labels": [], "entities": []}, {"text": "Experiments show that our approach outperforms reconstruction from hidden states.", "labels": [], "entities": []}, {"text": "It achieves consistent improvements across various low-resource language pairs and directions, showing its effectiveness in making better use of limited parallel data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Experiments are conducted on four low- resource language pairs, in both translation directions.", "labels": [], "entities": []}, {"text": " Table 1. We report case-insensitive  BLEU with the WMT standard '13a' tokenization  using SacreBLEU (Post, 2018).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9791361689567566}, {"text": "WMT standard '13a' tokenization", "start_pos": 52, "end_pos": 83, "type": "DATASET", "confidence": 0.906900962193807}]}, {"text": " Table 2: BLEU scores on eight translation directions. The numbers before and after '\u00b1' are the mean and standard  deviation over five randomly seeded models. Our proposed methods (\u03b2 = 0/0.5) achieve small but consistent  improvements. \u2206BLEU scores are in bold if mean\u2212std is above zero while in red if the mean is below zero.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974537491798401}, {"text": "BLEU", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.9987842440605164}]}]}