{"title": [{"text": "A Partially Rule-Based Approach to AMR Generation", "labels": [], "entities": [{"text": "AMR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9602734446525574}]}], "abstractContent": [{"text": "This paper presents anew approach to generating English text from Abstract Meaning Representation (AMR).", "labels": [], "entities": [{"text": "generating English text from Abstract Meaning Representation (AMR)", "start_pos": 37, "end_pos": 103, "type": "TASK", "confidence": 0.7712198495864868}]}, {"text": "In contrast to the neural and statistical MT approaches used in other AMR generation systems, this one is largely rule-based, supplemented only by a language model and simple statistical linearization models , allowing for more control over the output.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.7859026193618774}, {"text": "AMR generation", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.9415741264820099}]}, {"text": "We also address the difficulties of automatically evaluating AMR generation systems and the problems with BLEU for this task.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.9202367663383484}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9966744184494019}]}, {"text": "We compare automatic metrics to human evaluations and show that while METEOR and TER arguably reflect human judgments better than BLEU, further research into suitable evaluation metrics is needed.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9603347778320312}, {"text": "TER arguably", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.8985500335693359}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.997941792011261}]}], "introductionContent": [{"text": "Abstract Meaning Representation, or AMR, is a representation of a sentence as a rooted, labeled graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.857268492380778}]}, {"text": "It provides a representation of the sentence's semantics while abstracting away from morphosyntactic details such as tense, number, word order, and part of speech (.", "labels": [], "entities": []}, {"text": "Because of these abstractions, it can be very difficult to generate from an AMR back to a fluent English sentence which preserves the original meaning.", "labels": [], "entities": []}, {"text": "It is also difficult to accurately evaluate the quality of generation results, since there is typically only one reference sentence available to compare results to, but one of the basic principles of AMR is the fact that the same AMR is used to represent many possible sentences; for example, \"he described her as a genius\", \"his description of her: genius\", and \"she was a genius, according to his description\" would all correspond to the same AMR (.", "labels": [], "entities": []}, {"text": "The following represents an AMR graph for the sentence \"A key European arms control treaty must be maintained.\": This example demonstrates several of the challenges faced by an AMR generation system.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 177, "end_pos": 191, "type": "TASK", "confidence": 0.8751035928726196}]}, {"text": "These include properly addressing constructions that do not correspond closely to the words in the reference, such as the use of the frame obligate-01 to express 'must' and the specific construction used for named entities such as 'Europe', as well as word order and the passive construction 'be maintained'.", "labels": [], "entities": []}, {"text": "In fact, this system successfully addresses some but not all of these challenges, producing the output \"Must maintain Europe key arms control treaty .\" While most previous work in AMR generation has used statistical and neural techniques, the current work approaches the task with a combination of rules and statistical methods; the rules are intended to constrain possibilities, particularly the possible realizations of concepts and which information from the AMR is expressed.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 180, "end_pos": 194, "type": "TASK", "confidence": 0.9873039126396179}]}, {"text": "This allows for greater control over the output; even if the overall results do not score as well, on average, as those of other approaches, this approach has the potential to minimize the chances of significant adequacy errors such as omission of key information or addition of information not contained in the AMR, which are possible in machine-learning-based systems.", "labels": [], "entities": []}, {"text": "Another advantage of a partially rule-based approach is that it can work without large amounts of AMR-annotated data; it could thus be adapted to anew language or an altered AMR scheme in situations where there is insufficient data fora machinelearning-based system to achieve satisfactory performance.", "labels": [], "entities": []}, {"text": "2 Related Work 2.1 AMR Generation Systems introduced the first AMR generator (JAMR), which transforms an AMR graph into a tree before using a weighted tree-to-string transducer to generate the string.", "labels": [], "entities": []}, {"text": "While most of its rules are automatically extracted, these are supplemented with handwritten rules for some phenomena including dates, conjunctions, and some special concepts.", "labels": [], "entities": []}, {"text": "An ablation experiment showed that these handwritten rules contributed significantly to the results.", "labels": [], "entities": []}, {"text": "AMR generation was included as a shared task at.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7101927846670151}]}, {"text": "The winner of the task, as determined by human judgments, was the RIGOTRIO system (, which uses handcrafted rules to convert AMRs to Abstract Syntax Trees, which are then realized as strings using existing Grammatical Framework resources.", "labels": [], "entities": [{"text": "RIGOTRIO", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.858747124671936}]}, {"text": "However, this approach has limited coverage, and is only used for about 12% of sentences, while the system defaults to using JAMR for other sentences.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.7747690677642822}]}, {"text": "Other submissions to the shared task included FORGe, which uses graph transducers (; Sheffield, which treats AMR generation as an inverse of transition-based parsing, transforming the AMR graph into a syntactic dependency tree before realizing it as a sentence; and ISI, which uses phrasebased machine translation (PBMT) methods).", "labels": [], "entities": [{"text": "FORGe", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8136415481567383}, {"text": "AMR generation", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.9273923337459564}, {"text": "phrasebased machine translation (PBMT)", "start_pos": 282, "end_pos": 320, "type": "TASK", "confidence": 0.7601872632900873}]}, {"text": "Beyond the shared task, other work in AMR generation has approached the task as a Traveling Salesman Problem (, with synchronous node replacement grammar (, and using a transition-based approach to transform the AMR into a syntactic dependency tree.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9872962534427643}]}, {"text": "Castro compare the effect of different types of preprocessing on the performance of AMR generation systems based on PBMT and NMT.", "labels": [], "entities": []}, {"text": "The best results to date have been obtained with neural methods, which excel when the small amount of manually-annotated training data is augmented with millions of unlabeled sentences which have been automatically parsed.", "labels": [], "entities": []}, {"text": "first used this approach to train a sequenceto-sequence model, and and later adapted it to a graph-to-sequence model.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most previous work in AMR generation has reported results exclusively using BLEU scores, with the original sentence as the only reference.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9865070879459381}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9981856942176819}]}, {"text": "A notable exception is the five systems included in the SemEval-2017 shared task, which were additionally compared by human judgments.", "labels": [], "entities": [{"text": "SemEval-2017 shared task", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.686921755472819}]}, {"text": "The human evaluations were shown not to correlate well with BLEU scores, raising questions about the suitability of the metric for this task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.999113142490387}]}, {"text": "In particular, BLEU as used for AMR generation is intuitively inappropriate because it strictly measures similarity to one reference sentence, while by design, a single AMR can correspond to many different English sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9982396364212036}, {"text": "AMR generation", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9884958565235138}]}, {"text": "Thus, BLEU is in practice more of a measure of how closely a system can replicate the exact wording used in the original sentence than of how adequately and fluently it expresses the meaning of the AMR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9992984533309937}]}, {"text": "Ideally, evaluation of AMR generation would be performed using human judgments or task-based evaluations; unfortunately, however, it is sometimes necessary to rely on the practicality of automatic metrics.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9923087358474731}]}, {"text": "We thus follow Castro in reporting two additional automatic metrics alongside BLEU, which may provide slightly more insight into system performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9966679215431213}]}, {"text": "The first is METEOR, which has been shown to correlate more strongly with human judgments of machine translation quality than BLEU does.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9753236770629883}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7142503559589386}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9951242804527283}]}, {"text": "It is a particularly appealing alternative to BLEU for AMR generation because, instead of only giving credit to exact word matches, METEOR also allows matching based on stems, synonyms, and paraphrases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.990994393825531}, {"text": "AMR generation", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9671530723571777}, {"text": "METEOR", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.8669371604919434}]}, {"text": "This mitigates the issues associated with having a single reference sentence in AMR, because it does not penalize systems as harshly for not correctly guessing the forms of morphological and syntactic variants that are usually not specified within the AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.7171977162361145}, {"text": "AMR", "start_pos": 252, "end_pos": 255, "type": "DATASET", "confidence": 0.7768540382385254}]}, {"text": "The final evaluation metric used is Translation Edit Rate (TER), which has been shown to require only one reference sentence in order to correlate as well with human judgments for ma-chine translation as BLEU does with four references ().", "labels": [], "entities": [{"text": "Translation Edit Rate (TER)", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.8443856338659922}, {"text": "BLEU", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.9643717408180237}]}, {"text": "This robustness against lack of extra references makes it, too, likely to be better suited to the AMR generation task than BLEU is.", "labels": [], "entities": [{"text": "AMR generation task", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.9486573139826456}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9898321628570557}]}, {"text": "These metrics were all designed for evaluation of machine translation; it may also be useful in the future to explore evaluating AMR generation with metrics from other NLG-related tasks, such as referenceless measures developed for grammatical error correction (e.g.).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7487879693508148}, {"text": "AMR generation", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.9905327260494232}, {"text": "grammatical error correction", "start_pos": 232, "end_pos": 260, "type": "TASK", "confidence": 0.6869019865989685}]}, {"text": "First, several variations on this system with different hyperparameters were tested on the 1368 AMRs in the dev data.", "labels": [], "entities": [{"text": "1368 AMRs in the dev data", "start_pos": 91, "end_pos": 116, "type": "DATASET", "confidence": 0.6767264753580093}]}, {"text": "As discussed in 3.4, the linearization model contains two separate components, each of which is optional, resulting in four different linearization configurations.", "labels": [], "entities": []}, {"text": "The pruning parameter k was tested at values of 5, 10, and 100.", "labels": [], "entities": []}, {"text": "In total, 12 different versions of the system were tested on dev data.", "labels": [], "entities": []}, {"text": "Based on these results, an optimal version of the system was chosen, which was then evaluated quantitatively on test data with automatic metrics.", "labels": [], "entities": []}, {"text": "This system's output on dev data was also evaluated qualitatively by reviewing a small sample of its sentences, and quantitatively by comparing the number of tokens and frequency of parts of speech to those of the references.", "labels": [], "entities": []}, {"text": "As discussed in 2.2, AMR Generation is usually evaluated only with BLEU, but one shared task obtained human judgments of five systems which were shown not to correlate well with BLEU scores.", "labels": [], "entities": [{"text": "AMR Generation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9666334390640259}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9979060888290405}, {"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9961101412773132}]}, {"text": "We tested the system outputs from this task with BLEU 2 as well as METEOR and TER 4 to determine whether these metrics would correspond more closely to human judgments; results are presented and discussed in 5.5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9986928105354309}, {"text": "METEOR", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9808695316314697}, {"text": "TER 4", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9752496480941772}]}, {"text": "shows the performance of each variation of the system on the dev data.", "labels": [], "entities": []}, {"text": "In the SemEval shared task, only BLEU scores were originally reported alongside measures of human rankings.", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8859598636627197}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990226030349731}]}, {"text": "To explore how well each of the three automatic metrics used in this paper correlate with human judgments, we tested the output of all the systems that participated in the task with each of these metrics.", "labels": [], "entities": []}, {"text": "shows these new results, alongside the results of the human rankings.", "labels": [], "entities": []}, {"text": "All four measures agree that RIGOTRIO and CMU are the best two systems out of the task participants, but METEOR and TER both agree with humans in rating RIGOTRIO highest, while CMU obtains a higher BLEU score.", "labels": [], "entities": [{"text": "RIGOTRIO", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8414528369903564}, {"text": "METEOR", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9659383893013}, {"text": "TER", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9961129426956177}, {"text": "BLEU score", "start_pos": 198, "end_pos": 208, "type": "METRIC", "confidence": 0.9796185195446014}]}, {"text": "This provides some evidence for the claim that METEOR and TER maybe better suited than BLEU to evaluating AMR generation, especially when it comes to distinguishing among stronger systems.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9704445600509644}, {"text": "TER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9978613257408142}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9972805976867676}, {"text": "AMR generation", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.9161639213562012}]}, {"text": "However, none of the metrics fully match the ranking given by humansin particular, while humans considered FORGe the third-best system, all of the automatic metrics rank it lower.", "labels": [], "entities": [{"text": "FORGe", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9916571378707886}]}, {"text": "Thus, while these alternatives maybe an improvement over BLEU, more research is necessary to determine a more accurate way to auto-: Comparison of evaluation metrics to Trueskill (measure of human rankings) for shared task data and systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9863823652267456}, {"text": "Trueskill", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9362135529518127}]}, {"text": "This system's performance on the same data according to automatic metrics is provided for comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System performance on dev data with vari- ous hyperparameter combinations. '+P' is used to des- ignate systems using the pair-order model and '-P' to  designate those without it; similarly, '+C' and '-C' are  used to designate whether or not the coreness model  was used. In the '-P-C' configuration, baseline lin- earization is used. The best score for each metric is  shown in bold. The shaded row represents the configu- ration of the system selected for further evaluation.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of test scores achieved by this system and others.", "labels": [], "entities": []}, {"text": " Table 4: Description and example for each of the four  quality categories.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of counts of part-of-speech tags  in system output vs. references.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of evaluation metrics to Trueskill  (measure of human rankings) for shared task data and  systems. This system's performance on the same data  according to automatic metrics is provided for compar- ison.", "labels": [], "entities": [{"text": "Trueskill", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.808610737323761}]}]}