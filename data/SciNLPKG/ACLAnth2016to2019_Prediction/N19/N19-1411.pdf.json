{"title": [{"text": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "The variational autoencoder (VAE) imposes a probabilistic distribution (typically Gaus-sian) on the latent space and penalizes the Kullback-Leibler (KL) divergence between the posterior and prior.", "labels": [], "entities": [{"text": "variational autoencoder (VAE", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.7016525715589523}]}, {"text": "In NLP, VAEs are extremely difficult to train due to the problem of KL collapsing to zero.", "labels": [], "entities": [{"text": "VAEs", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.94486403465271}]}, {"text": "One has to implement various heuristics such as KL weight anneal-ing and word dropout in a carefully engineered manner to successfully train a VAE for text.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use the Wasser-stein autoencoder (WAE) for probabilistic sentence generation, where the encoder could be either stochastic or deterministic.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7438732087612152}]}, {"text": "We show theoretically and empirically that, in the original WAE, the stochastically encoded Gaus-sian distribution tends to become a Dirac-delta function, and we propose a variant of WAE that encourages the stochasticity of the en-coder.", "labels": [], "entities": []}, {"text": "Experimental results show that the latent space learned by WAE exhibits properties of continuity and smoothness as in VAEs, while simultaneously achieving much higher BLEU scores for sentence reconstruction.", "labels": [], "entities": [{"text": "continuity", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9950680732727051}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.999423623085022}, {"text": "sentence reconstruction", "start_pos": 183, "end_pos": 206, "type": "TASK", "confidence": 0.7557242512702942}]}], "introductionContent": [{"text": "Natural language sentence generation in the deep learning regime typically uses a recurrent neural network (RNN) to predict the most probable next word given previous words (.", "labels": [], "entities": [{"text": "Natural language sentence generation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6627272665500641}]}, {"text": "Such RNN architecture can be further conditioned on some source information, for example, an input sentence, resulting in a sequence-to-sequence (Seq2Seq) model.", "labels": [], "entities": []}, {"text": "Traditionally, sentence generation is accomplished in a deterministic fashion, i.e., the model uses a deterministic neural network to encode an input sentence to some hidden representations, from which it then decodes an output sentence using another deterministic neural network.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7329947650432587}]}, {"text": "propose to use the variational autoencoder to map an input sentence to a probabilistic continuous latent space.", "labels": [], "entities": []}, {"text": "VAE makes it possible to generate sentences from a distribution, which is desired in various applications.", "labels": [], "entities": [{"text": "VAE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7100533246994019}]}, {"text": "in an open-domain dialog system, the information of an utterance and its response is not necessarily a one-to-one mapping, and multiple plausible responses could be suitable fora given input.", "labels": [], "entities": []}, {"text": "Probabilistic sentence generation makes the dialog system more diversified and more meaningful (.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7429336905479431}]}, {"text": "Besides, probabilistic modeling of the hidden representations serves as away of posterior regularization (, facilitating interpolation () and manipulation of the latent representation (.", "labels": [], "entities": []}, {"text": "However, training VAEs in NLP is more difficult than the image domain ().", "labels": [], "entities": []}, {"text": "The VAE training involves a reconstruction loss and a Kullback-Leibler (KL) divergence between the posterior and prior of the latent space.", "labels": [], "entities": [{"text": "VAE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5368677377700806}, {"text": "reconstruction loss", "start_pos": 28, "end_pos": 47, "type": "METRIC", "confidence": 0.7346015274524689}]}, {"text": "In NLP, the KL term tends to vanish to zero during training, leading to an ineffective latent space.", "labels": [], "entities": []}, {"text": "Previous work has proposed various engineering tricks to alleviate this problem, including KL annealing and word dropout.", "labels": [], "entities": []}, {"text": "In this paper, we address the difficulty of training VAE sentence generators by using a Wasserstein autoencoder.", "labels": [], "entities": [{"text": "VAE sentence generators", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8541121482849121}]}, {"text": "WAE modifies VAE in that it requires the integration of the posterior to be close to its prior, where the closeness is measured with empirical samples drawn from the distributions.", "labels": [], "entities": [{"text": "VAE", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.541636049747467}]}, {"text": "In this way, the encoder could be either stochastic or deterministic, but the model still retains probabilistic properties.", "labels": [], "entities": []}, {"text": "Moreover, we show both theoretically and empirically that the stochastic Gaussian encoder in the original form tends to be a Dirac-delta function.", "labels": [], "entities": []}, {"text": "We thus propose a WAE variant that encourages the encoder's stochasticity by penalizing an auxiliary KL term.", "labels": [], "entities": [{"text": "WAE", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8482125401496887}]}, {"text": "Experiments show that the sentences generated by WAE exhibit properties of continuity and smoothness as in VAE, while achieving a much higher reconstruction performance.", "labels": [], "entities": [{"text": "continuity", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9967638254165649}, {"text": "VAE", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.5460217595100403}]}, {"text": "Our proposed variant further encourages the stochasticity of the encoder.", "labels": [], "entities": []}, {"text": "More importantly, WAE is robust to hyperparameters and much easier to train, without the need for KL annealing or word dropout as in VAE.", "labels": [], "entities": [{"text": "WAE", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9041692018508911}, {"text": "VAE", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.6683279871940613}]}, {"text": "Ina dialog system, we demonstrate that WAEs are capable of generating better quality and more diverse sentences than VAE.", "labels": [], "entities": [{"text": "VAE", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.7672673463821411}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of SNLI-style sentence generation,  where WAE is compared with DAE and VAE. D and  S refer to the deterministic and stochastic encoders, re- spectively. \u2191/\u2193 The larger/lower, the better. For En- tropy and AvgLen, the closer to corpus statistics, the  better (indicated by the \u2192 arrow).", "labels": [], "entities": [{"text": "SNLI-style sentence generation", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.9154854218165079}, {"text": "VAE", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9747227430343628}]}, {"text": " Table 2: Results on dialog generation, where  VED/WED hyperparameters for each model were cho- sen by Table 1.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8992378413677216}]}]}