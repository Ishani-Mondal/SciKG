{"title": [{"text": "Selective Attention for Context-aware Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6686902542908987}]}], "abstractContent": [{"text": "Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation fora full document.", "labels": [], "entities": [{"text": "NMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.76673424243927}]}, {"text": "Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents.", "labels": [], "entities": []}, {"text": "To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences.", "labels": [], "entities": []}, {"text": "We also propose single-level attention approaches based on sentence or word-level information in the context.", "labels": [], "entities": []}, {"text": "The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context.", "labels": [], "entities": []}, {"text": "Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outper-forms context-agnostic baselines but also surpasses context-aware baselines inmost cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation has grown immensely in the past few years, from the simplistic RNNbased encoder-decoder models to the state-of-the-art Transformer architecture ().", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8374840418497721}]}, {"text": "Most of these models rely on the attention mechanism as a major component, which involves focusing on different parts of a sequence to compute new representations, and has proven to be quite effective in improving the translation quality (.", "labels": [], "entities": []}, {"text": "However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence * * Work initiated during an internship at Unbabel.", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9648986458778381}]}, {"text": "basis, thus ignoring the long-range dependencies which maybe useful when it comes to translating discourse phenomena.", "labels": [], "entities": [{"text": "translating discourse phenomena", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.8637566169102987}]}, {"text": "More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years.", "labels": [], "entities": [{"text": "NMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7107440829277039}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9473923444747925}]}, {"text": "Most of these focus on using a few previous sentences as context and neglect the rest of the document.", "labels": [], "entities": []}, {"text": "Only one existing work has endeavoured to consider the full document context , thus proposing a more generalised approach to document-level NMT.", "labels": [], "entities": []}, {"text": "However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated).", "labels": [], "entities": []}, {"text": "A more recent work ( proposes to use a hierarchical attention network (HAN) () to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet, it uses a limited number of past source and target sentences as context and is not scalable to entire document.", "labels": [], "entities": []}, {"text": "In this work, we propose a selective attention approach to first selectively focus on relevant sentences in the global document-context and then attend to key words in those sentences, while ignoring the rest.", "labels": [], "entities": []}, {"text": "Towards this goal, we use sparse attention, enabling an efficient and scalable use of the context.", "labels": [], "entities": []}, {"text": "The intuition behind this is the way humans translate a sentence containing ambiguous words.", "labels": [], "entities": []}, {"text": "They may look for sentences in the whole document which contain similar words and just focus on those for the translation.", "labels": [], "entities": []}, {"text": "This attention, which we call Hierarchical Attention, is computed dynamically for each query word.", "labels": [], "entities": []}, {"text": "Furthermore, we propose a Flat Attention approach which is based on either sentence or word-level information in the context.", "labels": [], "entities": [{"text": "Flat Attention", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8095172047615051}]}, {"text": "We integrate the document-level context representation, produced from these attention modules, into the encoder or decoder of the Transformer model depending on whether we consider monolingual (source-side) or bilingual (both source and target-side) context.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (i) we propose a novel and efficient top-down approach to hierarchical attention for context-aware NMT, (ii) we compare variants of selective attention with both context-agnostic and context-aware baselines, and (iii) we run experiments in both online (only past context) and offline (both past and future context) settings on three English-German datasets.", "labels": [], "entities": [{"text": "English-German datasets", "start_pos": 367, "end_pos": 390, "type": "DATASET", "confidence": 0.7104359269142151}]}, {"text": "Experiments show that our approach improves upon the Transformer by an overall +1.34, +2.06 and +1.18 BLEU for TED Talks, News-Commentary and Europarl, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9975084066390991}, {"text": "Europarl", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.9384127855300903}]}, {"text": "It also outperforms two recent context-aware baselines () in majority of the cases.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU and Meteor scores for variants of our model and two context-agnostic baselines for offline docu- ment MT. bold: Best performance. All reported results for our model are significantly better than both baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999197781085968}, {"text": "offline docu- ment MT", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.5452171444892884}]}, {"text": " Table 3: BLEU and Meteor scores for variants of our model and three baselines for online document MT. bold:  Best performance. , \u2666, \u2663: Statistically significantly better than our implementations of Zhang et al. (2018),  Miculicich et al. (2018), or both. All reported results for our model are significantly better than the Transformer.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990320205688477}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.858821451663971}]}, {"text": " Table 4: Accuracy on contrastive test set with regard to  antecedent distance (in sentences) on TED Talks. An- tecedent distance 0 means the pronoun occurs in the  same sentence as the antecedent.", "labels": [], "entities": []}, {"text": " Table 5: Model complexity for Encoder Context inte- gration models (News-Commentary).", "labels": [], "entities": []}]}