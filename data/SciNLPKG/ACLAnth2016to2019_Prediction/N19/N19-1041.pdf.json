{"title": [{"text": "ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems", "labels": [], "entities": [{"text": "ReWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7740684747695923}, {"text": "Regularization of Neural Machine Translation", "start_pos": 37, "end_pos": 81, "type": "TASK", "confidence": 0.8801116108894348}]}], "abstractContent": [{"text": "Regularization of neural machine translation is still a significant problem, especially in low-resource settings.", "labels": [], "entities": [{"text": "Regularization of neural machine translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7476494789123536}]}, {"text": "To mollify this problem , we propose regressing word embeddings (ReWE) as anew regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value).", "labels": [], "entities": [{"text": "regressing word embeddings (ReWE)", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.6330133080482483}]}, {"text": "Such a joint training allows the proposed system to learn the distributional properties represented by the word embeddings, empirically improving the generalization to unseen sentences.", "labels": [], "entities": []}, {"text": "Experiments over three translation datasets have showed a consistent improvement over a strong baseline, ranging between 0.91 and 2.54 BLEU points, and also a marked improvement over a state-of-the-art system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9979813694953918}]}], "introductionContent": [{"text": "The last few years have witnessed remarkable improvements in the performance of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8624390363693237}]}, {"text": "These improvements are strongly linked to the development of neural machine translation (NMT): based on encoderdecoder architectures (also known as seq2seq), NMT can use recurrent neural networks (RNNs), convolutional neural networks (CNNs)) or transformers ( to learn how to map a sentence from the source language to an adequate translation in the target language.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.8697998424371084}]}, {"text": "In addition, attention mechanisms ( help soft-align the encoded source words with the predictions, further improving the translation.", "labels": [], "entities": []}, {"text": "NMT systems are usually trained via maximum likelihood estimation (MLE).", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE", "start_pos": 36, "end_pos": 70, "type": "METRIC", "confidence": 0.7182641804218293}]}, {"text": "However, as * * The author has changed affiliation to Microsoft after the completion of this work.", "labels": [], "entities": []}, {"text": "His new email is: Ehsan.ZareBorzeshi@microsoft.com The proposed regularizer: the hidden vector in the decoder, s j , transits through two paths: 1) a linear and a softmax layers that output vector v j (vocab dim) which is used for predicting the target word as usual, and 2) a two-layer network (ReWE) that outputs a vector, e j , of word embedding size (word emb dim).", "labels": [], "entities": [{"text": "predicting the target word", "start_pos": 231, "end_pos": 257, "type": "TASK", "confidence": 0.8302014768123627}]}, {"text": "During training, e j is used in a regressive loss with the ground-truth embedding.", "labels": [], "entities": []}, {"text": "pointed out by, MLE suffers from two obvious limitations: the first is that it treats all the predictions other than the ground truth as equally incorrect.", "labels": [], "entities": [{"text": "MLE", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9042544960975647}]}, {"text": "As a consequence, synonyms and semantically-similar words -which are often regarded as highly interchangeable with the ground truth -are completely ignored during training.", "labels": [], "entities": []}, {"text": "The second limitation is that MLEtrained systems suffer from \"exposure bias\" and do not generalize well over the large output space of translations.", "labels": [], "entities": []}, {"text": "Owing to these limitations, NMT systems still struggle to outperform other traditional MT approaches when the amount of supervised data is limited (.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9757206439971924}]}, {"text": "In this paper, we propose a novel regularization technique for NMT aimed to influence model learning with contextual properties.", "labels": [], "entities": []}, {"text": "The technique -nicknamed ReWE from \"regressing word embedding\" -consists of modifying a conventional seq2seq decoder to jointly learn to a) predict the next word in the translation (categorical value), as usual, and b) regress its word embedding (numerical value).", "labels": [], "entities": []}, {"text": "Both predictions are incorporated in the training objective, combining standard MLE with a continuous loss function based on word embeddings.", "labels": [], "entities": [{"text": "MLE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.4289538264274597}]}, {"text": "The rationale is to encourage the system to learn to co-predict the next word together with its context (by means of the word embedding representation), in the hope of achieving improved generalization.", "labels": [], "entities": []}, {"text": "At inference time, the system operates as a standard NMT system, retaining the categorical prediction and ignoring the predicted embedding.", "labels": [], "entities": []}, {"text": "We qualify our proposal as a regularization technique since, like any other regularizers, it only aims to influence the model's training, while leaving the inference unchanged.", "labels": [], "entities": []}, {"text": "We have evaluated the proposed system over three translation datasets of different size, namely English-French (en-fr), CzechEnglish (cs-en), and Basque-English (eu-en).", "labels": [], "entities": []}, {"text": "In each case, ReWE has significantly outperformed its baseline, with a marked improvement of up to 2.54 BLEU points for eu-en, and consistently outperformed a state-of-the-art system).", "labels": [], "entities": [{"text": "ReWE", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.6486184597015381}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9991496801376343}]}], "datasetContent": [{"text": "We have developed our models building upon the OpenNMT toolkit ( 3 . For training, we have used the same settings as.", "labels": [], "entities": []}, {"text": "We have also explored the use of sub-word units learned with byte pair encoding (BPE) ().", "labels": [], "entities": [{"text": "byte pair encoding (BPE)", "start_pos": 61, "end_pos": 85, "type": "METRIC", "confidence": 0.6249274760484695}]}, {"text": "All the preprocessing steps, hyperparameter values and training parameters are described in detail in the supplementary material to ease reproducibility of our results.", "labels": [], "entities": []}, {"text": "We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . lists the datasets and their main features.", "labels": [], "entities": [{"text": "ACL Conference on Machine Translation (WMT16)", "start_pos": 85, "end_pos": 130, "type": "TASK", "confidence": 0.6372632309794426}, {"text": "International Workshop on Spoken Language Translation (IWSLT16)", "start_pos": 144, "end_pos": 207, "type": "TASK", "confidence": 0.6275299323929681}]}, {"text": "Despite having nearly 90,000 parallel sentences, the eu-en dataset only contains 2,000 human-translated sentences; the others are translations of Wikipedia page titles and localization files.", "labels": [], "entities": [{"text": "eu-en dataset", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.7889550626277924}]}, {"text": "Therefore, we regard the eu-en dataset as very low-resource.", "labels": [], "entities": [{"text": "eu-en dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.7996490597724915}]}, {"text": "In addition to the seq2seq baseline, we have compared our results with those recently reported by Denkowski and Neubig for non-ensemble models.", "labels": [], "entities": []}, {"text": "For all models, we report the BLEU scores (, with the addition of selected comparative examples.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9991914629936218}]}, {"text": "Two contrastive experiments are also added in supplementary notes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores over the test sets. Average of 10 models independently trained with different seeds.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986969828605652}]}]}