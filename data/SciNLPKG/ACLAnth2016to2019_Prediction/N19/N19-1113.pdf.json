{"title": [{"text": "Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction", "labels": [], "entities": [{"text": "Mutual Information Maximization", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7673467000325521}]}], "abstractContent": [{"text": "We address part-of-speech (POS) induction by maximizing the mutual information between the induced label and its context.", "labels": [], "entities": [{"text": "part-of-speech (POS) induction", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6625216007232666}]}, {"text": "We focus on two training objectives that are amenable to stochastic gradient descent (SGD): a novel generalization of the classical Brown clustering objective and a recently proposed varia-tional lower bound.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7607935269673666}]}, {"text": "While both objectives are subject to noise in gradient updates, we show through analysis and experiments that the vari-ational lower bound is robust whereas the generalized Brown objective is vulnerable.", "labels": [], "entities": []}, {"text": "We obtain strong performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context.", "labels": [], "entities": []}], "introductionContent": [{"text": "We consider information theoretic objectives for POS induction, an important unsupervised learning problem in computational linguistics ().", "labels": [], "entities": [{"text": "POS induction", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9573209583759308}]}, {"text": "The idea is to make the induced label syntactically informative by maximizing its mutual information with respect to local context.", "labels": [], "entities": []}, {"text": "Mutual information has long been a workhorse in the development of NLP techniques, for instance the classical Brown clustering algorithm.", "labels": [], "entities": []}, {"text": "But its role in today's deep learning paradigm is less clear and a subject of active investigation (.", "labels": [], "entities": []}, {"text": "We focus on fully differentiable objectives that can be plugged into an automatic differentiation system and efficiently optimized by SGD.", "labels": [], "entities": []}, {"text": "Specifically, we investigate two training objectives.", "labels": [], "entities": []}, {"text": "The first is a novel generalization of the Brown clustering objective obtained by relaxing the hard clustering constraint.", "labels": [], "entities": []}, {"text": "The second is a recently proposed variational lower bound on mutual information.", "labels": [], "entities": []}, {"text": "A main challenge in optimizing these objectives is the difficulty of stochastic optimization.", "labels": [], "entities": [{"text": "stochastic optimization", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7302377820014954}]}, {"text": "Each objective involves entropy estimation which is a nonlinear function of all data and does not decompose over individual instances.", "labels": [], "entities": [{"text": "entropy estimation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.75028857588768}]}, {"text": "This makes the gradients estimated on minibatches inconsistent with the true gradient estimated from the entire dataset.", "labels": [], "entities": []}, {"text": "To our surprise, in practice we are able to optimize the variational objective effectively but not the generalized Brown objective.", "labels": [], "entities": []}, {"text": "We analyze the estimated gradients and show that the inconsistency error is only logarithmic in the former but linear in the latter.", "labels": [], "entities": []}, {"text": "We validate our approach on POS induction by attaining strong performance on a multitude of datasets and languages.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9782257080078125}]}, {"text": "Our simple architecture that encodes morphology and context reaches up to 80.1 many-to-one accuracy on the 45-tag Penn WSJ dataset and achieves 4.7% absolute improvement to the previous best result on the universal treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9947939515113831}, {"text": "Penn WSJ dataset", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9112147291501363}]}, {"text": "Unlike previous works, our model does not rely on computationally expensive structured inference or hand-crafted features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the effectiveness of our training objectives on the task of POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9747684001922607}]}, {"text": "The goal of this task is to induce the correct POS tag fora given word in context.", "labels": [], "entities": []}, {"text": "As typical in unsupervised tasks, evaluating the quality of induced labels is challenging; see \u2022 We use many-to-one accuracy as a primary evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9605994820594788}]}, {"text": "That is, we map each induced label to the most frequently coinciding ground-truth POS tag in the annotated data and report the resulting accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9987596273422241}]}, {"text": "We also use the V-measure ( when comparing with CRF autoencoders to be consistent with reported results ().", "labels": [], "entities": [{"text": "V-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.949395477771759}]}, {"text": "\u2022 We use the number of ground-truth POS tags as the value of m (i.e., number of labels to induce).", "labels": [], "entities": []}, {"text": "This is a data-dependent quantity, for instance 45 in the Penn WSJ and 12 in the universal treebank.", "labels": [], "entities": [{"text": "Penn WSJ", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9850532710552216}]}, {"text": "Fixing the number of tags this way obviates many evaluation issues.", "labels": [], "entities": []}, {"text": "\u2022 Model-specific hyperparameters are tuned on the English Penn WSJ dataset.", "labels": [], "entities": [{"text": "English Penn WSJ dataset", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.9573971629142761}]}, {"text": "This configuration is then fixed and used for all other datasets: 10 languages in the universal treebank 2 and 7 languages from CoNLL-X and CoNLL 2007.", "labels": [], "entities": [{"text": "CoNLL-X", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9223265647888184}, {"text": "CoNLL 2007", "start_pos": 140, "end_pos": 150, "type": "DATASET", "confidence": 0.863998532295227}]}], "tableCaptions": [{"text": " Table 1: Many-to-one accuracy on the 45-tag Penn  WSJ with the best hyperparameter configurations. The  average accuracy over 10 random restarts is reported  and the standard deviation is given in parentheses (ex- cept for deterministic methods).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9919375777244568}, {"text": "45-tag Penn  WSJ", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.6363096634546915}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9943299293518066}]}, {"text": " Table 2: Ablation of the best model on Penn WSJ.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9962232112884521}, {"text": "Penn WSJ", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9606200754642487}]}, {"text": " Table 3: Nearest neighbors of \"made\" under GloVe em- beddings (840B.300d, within PTB vocab).", "labels": [], "entities": [{"text": "PTB vocab", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.8996061086654663}]}, {"text": " Table 4: Many-to-one accuracy on the 12-tag universal treebank dataset. We use the same setting in Table 1. All  models use a fixed hyperparameter configuration optimized on the 45-tag Penn WSJ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9926377534866333}, {"text": "universal treebank dataset", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.7038824061552683}, {"text": "Penn WSJ", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.9330603778362274}]}, {"text": " Table 5: Comparison with the reported results with CRF autoencoders in many-to-one accuracy (M2O) and the  V-measure (VM).", "labels": [], "entities": [{"text": "accuracy (M2O)", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.8831032961606979}, {"text": "V-measure (VM)", "start_pos": 108, "end_pos": 122, "type": "METRIC", "confidence": 0.871117040514946}]}]}