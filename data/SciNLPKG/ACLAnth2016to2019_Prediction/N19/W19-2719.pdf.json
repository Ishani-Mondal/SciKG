{"title": [{"text": "Using Rhetorical Structure Theory to Assess Discourse Coherence for Non-native Spontaneous Speech", "labels": [], "entities": []}], "abstractContent": [{"text": "This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for non-native speakers.", "labels": [], "entities": []}, {"text": "Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts; however, limited research has been conducted to date on RST annotation and parsing of spoken language , in particular, non-native spontaneous speech.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8574811518192291}, {"text": "analysis of discourse organization of written texts", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.7518675242151532}, {"text": "RST annotation and parsing of spoken language", "start_pos": 173, "end_pos": 218, "type": "TASK", "confidence": 0.750290355512074}]}, {"text": "Due to the fact that the measurement of discourse coherence is typically a key metric inhuman scoring rubrics for assessments of spoken language, we conducted research to obtain RST annotations on non-native spoken responses from a standardized assessment of academic English proficiency.", "labels": [], "entities": [{"text": "RST", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9570877552032471}]}, {"text": "Subsequently , automatic parsers were trained on these annotations to process non-native spontaneous speech.", "labels": [], "entities": []}, {"text": "Finally, a set of features were extracted from automatically generated RST trees to evaluate the discourse structure of non-native spontaneous speech, which were then employed to further improve the validity of an automated speech scoring system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The spread of English as the main global language for education and commerce is continuing, and there is a strong interest in developing assessment systems that can automatically score spontaneous speech from non-native speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners).", "labels": [], "entities": [{"text": "reliability", "start_pos": 294, "end_pos": 305, "type": "METRIC", "confidence": 0.9606713056564331}]}, {"text": "Various features related to different aspects of speaking proficiency have been explored, such as features for pronunciation, prosody, and fluency (), as well as features for vocabulary, grammar, and content (.", "labels": [], "entities": []}, {"text": "Discourse coherence, which refers to how well a text or speech is organized to convey information, is an important aspect of communicative competence, as is reflected inhuman scoring rubrics for assessments of non-native English).", "labels": [], "entities": []}, {"text": "However, discourse-level features have rarely been investigated in the context of automated speech scoring.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6250726083914439}]}, {"text": "In order to address this deficiency, this study aims to explore effective means to automate the analysis of discourse and the measurement of coherence in non-native spoken responses, thereby improving the validity of an automated scoring system.", "labels": [], "entities": [{"text": "validity", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9790017604827881}]}, {"text": "Rhetorical Structure Theory (RST) () is one of the most influential approaches for document-level discourse analysis.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.843987246354421}, {"text": "document-level discourse analysis", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.7211125294367472}]}, {"text": "It can represent a document's discourse structure using a hierarchical tree in which nodes are recursively linked with rhetorical relations and labeled with nucleus or satellite tags to depict the importance of the child nodes in a relation.", "labels": [], "entities": []}, {"text": "In our previous study (), RST-based discourse annotations were obtained on a corpus of 600 spontaneous spoken responses provided by non-native English speakers in the context of an English speaking proficiency assessment.", "labels": [], "entities": [{"text": "RST-based discourse annotations", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.9272946715354919}]}, {"text": "In this paper, we continued this line of research, and made further contributions as follows: \u2022 A larger annotated corpus consisting of 1440 non-native spontaneous spoken responses was obtained using an annotation scheme based on the RST framework.", "labels": [], "entities": [{"text": "RST framework", "start_pos": 234, "end_pos": 247, "type": "DATASET", "confidence": 0.6850773096084595}]}, {"text": "In addition to the previously annotated 600 responses (), annotations on additional 840 responses were obtained to enlarge the data set that can be used to train an automatic RST parser.", "labels": [], "entities": [{"text": "RST parser", "start_pos": 175, "end_pos": 185, "type": "TASK", "confidence": 0.9089012444019318}]}, {"text": "When comparing the annotations from two independent human experts on 120 responses, the resulting micro-averaged F1 scores on the three different levels of span, nuclearity, and relation 1 are 86.8%, 72.2%, and 58.2%, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.987301230430603}, {"text": "span", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9727708697319031}]}, {"text": "\u2022 Based on all these manual annotations, automatic RST parsers were trained and evaluated.", "labels": [], "entities": [{"text": "RST parsers", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.949765533208847}]}, {"text": "When comparing the automatically generated trees with double annotations from each of the two human experts separately, the F1 scores on the three levels of span, nuclearity, and relation are 76.1%/77.0%, 57.6%/59.7%, and 42.6%/44.4%, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9994332194328308}, {"text": "span", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9745175242424011}]}, {"text": "\u2022 A set of RST-based features were introduced to measure the discourse structure of nonnative spontaneous speech, where 1) an automatic speech recognizer (ASR) was used to transcribe the speech into text; 2) the aforementioned automatic parsers were applied to build RST trees based on the ASR output; 3) a set of features extracted from the automatic trees were explored, and the results show that these discourse features can predict holistic proficiency scores with an accuracy of 55.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 472, "end_pos": 480, "type": "METRIC", "confidence": 0.9979643821716309}]}, {"text": "Finally, these features were used in combination with other types of features to enhance the validity of an automated speech scoring system.", "labels": [], "entities": [{"text": "validity", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9854941368103027}]}], "datasetContent": [{"text": "For comparison, we trained three different parsers on both RST DT and RST SS: (a) RST SS: using the training set from the corpus of non-native spontaneous speech, where 49 double-annotated responses were used as the development set; (b) RST DT: using the training set from the RST Dis-  course Treebank, where 40 samples from the training set were separated as the development set; and (c) RST SS + RST DT: using the training sets from both RST SS and RST DT, where the development set is the same one used in (a).", "labels": [], "entities": [{"text": "RST SS", "start_pos": 82, "end_pos": 88, "type": "TASK", "confidence": 0.8356715738773346}, {"text": "RST Dis-  course Treebank", "start_pos": 277, "end_pos": 302, "type": "DATASET", "confidence": 0.5625431358814239}]}, {"text": "These three parsers were evaluated on the same test set from RST SS, where the gold standard EDU segmentations were used.", "labels": [], "entities": [{"text": "RST SS", "start_pos": 61, "end_pos": 67, "type": "TASK", "confidence": 0.4907994419336319}]}, {"text": "As shown in, the parser trained on RST SS outperformed the one trained on RST DT, especially on the relation level, i.e., 41.2%/43.1% vs. 35.0%/36.5%.", "labels": [], "entities": [{"text": "RST SS", "start_pos": 35, "end_pos": 41, "type": "TASK", "confidence": 0.5807757377624512}, {"text": "RST DT", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.7107339203357697}]}, {"text": "By combining both data corpora, the F1 scores can further be improved.", "labels": [], "entities": [{"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9993942975997925}]}, {"text": "Furthermore, besides using gold standard EDU segmentations, we also applied the automatic EDU segmenter within the parser to generate segmentations and then build the RST trees upon them.", "labels": [], "entities": []}, {"text": "The evaluation results showed that F1 scores of all three parsers were greatly reduced through this transition.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9814840257167816}]}, {"text": "For example, they were decreased to 53.0%/53.6% on span, 40.4%/41.9% on nuclearity, and 29.3%/31.1% on relation for parser (a) trained on RST SS.", "labels": [], "entities": [{"text": "span", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.963246762752533}, {"text": "RST SS", "start_pos": 138, "end_pos": 144, "type": "TASK", "confidence": 0.6897733211517334}]}, {"text": "Therefore, the improvement of EDU segmentations is also a research focus of our future work.", "labels": [], "entities": [{"text": "EDU segmentations", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8152878284454346}]}, {"text": "In the following section on discourse modeling for spontaneous speech, parser (a), which was trained on RST SS and using automatic EDU segmentations, was employed for discourse modeling.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7047331482172012}, {"text": "RST SS", "start_pos": 104, "end_pos": 110, "type": "TASK", "confidence": 0.7578020989894867}, {"text": "discourse modeling", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7947215437889099}]}, {"text": "The task is to build effective classification models, referred to as \"scoring models\", which can automatically predict the holistic proficiency scores by measuring the different aspects of non-native speaking proficiency, including pronunciation, prosody, fluency, vocabulary, grammar, and, in particular, discourse in spontaneous speech.", "labels": [], "entities": []}, {"text": "In order to obtain credible evaluation results, this study collected a large data set from the operational TOEFL iBT assessment to conduct this experiment, which includes 17,194 speakers who responded to all the six test questions as described in Section 3.1.", "labels": [], "entities": [{"text": "TOEFL iBT assessment", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.8585798541704813}]}, {"text": "The holistic proficiency scores were provided during the operational test, but more specific discourse coherence scores were not available for this large data set.", "labels": [], "entities": []}, {"text": "The whole data set was partitioned into two sets: one containing 12,194 speakers (73,164 responses) as the training set to build the scoring models, and the other one containing 5,000 speakers (30,000 responses) to test the model performance.", "labels": [], "entities": []}, {"text": "The baseline scoring model was built with approximately 130 automatic features extracted from the SpeechRater system, which can measure the pronunciation, prosody, fluency, rhythm, vocabulary, and grammar of spontaneous speech.", "labels": [], "entities": []}, {"text": "All SpeechRater features were extracted either directly from the speech signal or from the output of a Kaldi-based automatic speech recognizer) with a word error rate of 20.9% on an independent evaluation set with non-native spontaneous speech from the TOEFL iBT speaking test.", "labels": [], "entities": [{"text": "Kaldi-based automatic speech recognizer", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.5716824010014534}, {"text": "word error rate", "start_pos": 151, "end_pos": 166, "type": "METRIC", "confidence": 0.8145591815312704}, {"text": "TOEFL iBT speaking test", "start_pos": 253, "end_pos": 276, "type": "DATASET", "confidence": 0.9241364449262619}]}, {"text": "Based on the automatic speech recognition output (without punctuations and capitalization) generated by SpeechRater, the automatic parsers developed in section 4.1 were applied to extract RST trees.", "labels": [], "entities": [{"text": "RST trees", "start_pos": 188, "end_pos": 197, "type": "TASK", "confidence": 0.8287234306335449}]}, {"text": "Afterwards, the RST-based features were automatically obtained.", "labels": [], "entities": []}, {"text": "Therefore, in this process, no manual transcriptions or manual annotations were involved.", "labels": [], "entities": []}, {"text": "Furthermore, the RST-based discourse features can be combined with the baseline features to extend the ability of SpeechRater to assess the discourse structure of non-native spontaneous speech.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average numbers of EDUs and word tokens  (and their standard deviations) appearing in the RST  Discourse Treebank (RST DT) and the annotated cor- pus of non-native spontaneous speech (RST SS).", "labels": [], "entities": [{"text": "RST  Discourse Treebank (RST DT)", "start_pos": 100, "end_pos": 132, "type": "DATASET", "confidence": 0.8460841008595058}, {"text": "non-native spontaneous speech (RST SS)", "start_pos": 163, "end_pos": 201, "type": "TASK", "confidence": 0.6612049681799752}]}, {"text": " Table 2: Top 10 relations appearing in the training  set of the annotated corpus of spontaneous speech  (RST SS). The percentages of each relation appear- ing in both RST SS and the RST Discourse Treebank  (RST DT) are listed for comparison.", "labels": [], "entities": [{"text": "RST Discourse Treebank  (RST DT)", "start_pos": 183, "end_pos": 215, "type": "DATASET", "confidence": 0.8981848018510001}]}, {"text": " Table 3: Discourse parsing performance in terms of F1  scores (%) on three levels of Span, Nuclearity, and Re- lation. Human agreements are also listed for compari- son. Within each cell, two micro F1 scores according to  the gold standards from each of two human annotators  are both reported.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8119477033615112}, {"text": "F1  scores", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9837019145488739}, {"text": "Re- lation", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9522000948588053}, {"text": "F1", "start_pos": 199, "end_pos": 201, "type": "METRIC", "confidence": 0.7237312197685242}]}, {"text": " Table 4: Pearson correlation coefficients (r) of dis- course features with both the holistic proficiency scores  as well as the discourse coherence scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9321838319301605}]}, {"text": " Table 5: Pearson correlation coefficients (r) of dis- course features with both the holistic proficiency  scores. RST SS indicates using the parser trained with  the annotations on speech data during the feature gen- eration, and RST SS + RST DT indicates using the  parser trained with both the annotations on speech data  and the RST Discourse Treebank.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9562485814094543}, {"text": "RST SS", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9016247689723969}, {"text": "RST Discourse Treebank", "start_pos": 333, "end_pos": 355, "type": "DATASET", "confidence": 0.8603777289390564}]}, {"text": " Table 6: Performance of the automatic scoring mod- els to predict holistic proficiency scores. The baseline  system was built with 131 SpeechRater features, and  the automatically generated 8 RST-based features were  appended to measure the discourse structure.", "labels": [], "entities": []}]}