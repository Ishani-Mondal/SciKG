{"title": [{"text": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them", "labels": [], "entities": [{"text": "Systematic Gender Biases in Word Embeddings", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.7122151553630829}]}], "abstractContent": [{"text": "Word embeddings are widely used in NLP fora vast range of tasks.", "labels": [], "entities": []}, {"text": "It was shown that word embeddings derived from text corpora reflect gender biases in society.", "labels": [], "entities": []}, {"text": "This phenomenon is pervasive and consistent across different word embedding models, causing serious concern.", "labels": [], "entities": []}, {"text": "Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word em-beddings, demonstrating convincing results.", "labels": [], "entities": []}, {"text": "However, we argue that this removal is superficial.", "labels": [], "entities": []}, {"text": "While the bias is indeed substantially reduced according to the provided bias definition , the actual effect is mostly hiding the bias, not removing it.", "labels": [], "entities": []}, {"text": "The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debi-ased embeddings, and can be recovered from them.", "labels": [], "entities": []}, {"text": "We present a series of experiments to support this claim, for two debiasing methods.", "labels": [], "entities": []}, {"text": "We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.", "labels": [], "entities": [{"text": "bias removal", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.7324092537164688}]}], "introductionContent": [{"text": "Word embeddings have become an important component in many NLP models and are widely used fora vast range of downstream tasks.", "labels": [], "entities": []}, {"text": "However, these word representations have been proven to reflect social biases (e.g. race and gender) that naturally occur in the data used to train them (.", "labels": [], "entities": []}, {"text": "In this paper we focus on gender bias.", "labels": [], "entities": [{"text": "gender bias", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6889784038066864}]}, {"text": "Gender bias was demonstrated to be consistent and pervasive across different word embeddings.", "labels": [], "entities": []}, {"text": "show that using word embeddings for simple analogies surfaces many gender stereotypes.", "labels": [], "entities": []}, {"text": "For example, the word embedding they use (word2vec embedding trained on the Google News dataset 1 () an-1 https://code.google.com/archive/p/word2vec/ swer the analogy \"man is to computer programmer as woman is to x\" with \"x = homemaker\".", "labels": [], "entities": [{"text": "Google News dataset 1", "start_pos": 76, "end_pos": 97, "type": "DATASET", "confidence": 0.8543463796377182}]}, {"text": "further demonstrate association between female/male names and groups of words stereotypically assigned to females/males (e.g. arts vs. science).", "labels": [], "entities": []}, {"text": "In addition, they demonstrate that word embeddings reflect actual gender gaps in reality by showing the correlation between the gender association of occupation words and labor-force participation data.", "labels": [], "entities": []}, {"text": "Recently, some work has been done to reduce the gender bias in word embeddings, both as a post-processing step () and as part of the training procedure (.", "labels": [], "entities": []}, {"text": "Both works substantially reduce the bias with respect to the same definition: the projection on the gender direction (i.e. \u2212 \u2192 he \u2212 \u2212\u2192 she), introduced in the former.", "labels": [], "entities": []}, {"text": "They also show that performance on word similarity tasks is not hurt.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.800090084473292}]}, {"text": "We argue that current debiasing methods, which lean on the above definition for gender bias and directly target it, are mostly hiding the bias rather than removing it.", "labels": [], "entities": []}, {"text": "We show that even when drastically reducing the gender bias according to this definition, it is still reflected in the geometry of the representation of \"gender-neutral\" words, and a lot of the bias information can be recovered.", "labels": [], "entities": []}], "datasetContent": [{"text": "We refer to the word embeddings of the previous works as HARD-DEBIASED () and GN-GLOVE (gender-neutral GloVe) ().", "labels": [], "entities": [{"text": "HARD-DEBIASED", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9754847288131714}, {"text": "GN-GLOVE", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8502759337425232}]}, {"text": "For each debiased word embedding we quantify the hidden bias with respect to the biased version.", "labels": [], "entities": []}, {"text": "For HARD-DEBIASED we compare to the embeddings before applying the debiasing procedure.", "labels": [], "entities": [{"text": "HARD-DEBIASED", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.73695307970047}]}, {"text": "For GN-GLOVE we compare to embedding trained with standard GloVe on the same corpus.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, we follow Bolukbasi et al.", "labels": [], "entities": []}, {"text": "(2016b) and use a reduced version of the vocabulary for both word embeddings: we take the most frequent 50,000 words and phrases and remove words with upper-case letters, digits, or punctuation, and words longer than 20 characters.", "labels": [], "entities": []}, {"text": "In addition, to avoid quantifying the bias of words that are inherently gendered (e.g. mother, father, queen), we remove from each vocabulary the respective set of gendered words as pre-defined in each work.", "labels": [], "entities": []}, {"text": "This yeilds a vocabulary of 26,189 words for HARD-DEBIASED and of 47,698 words for GN-GLOVE.", "labels": [], "entities": [{"text": "HARD-DEBIASED", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.7005824446678162}, {"text": "GN-GLOVE", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8295199871063232}]}, {"text": "As explained in Section 2 and according to the definition in previous works, we compute the bias of a word by taking its projection on the gender direction: \u2212 \u2192 he \u2212 \u2212\u2192 she.", "labels": [], "entities": []}, {"text": "In order to quantify the association between sets of words, we follow and use their Word Embedding Association Test (WEAT): consider two sets of target words (e.g., male and female professions) and two sets of attribute words (e.g., male and female names).", "labels": [], "entities": [{"text": "Word Embedding Association Test (WEAT)", "start_pos": 84, "end_pos": 122, "type": "METRIC", "confidence": 0.6469634175300598}]}, {"text": "A permutation test estimates the probability that a random permutation of the target words would produce equal or greater similarities to the attribute sets.", "labels": [], "entities": []}, {"text": "Male-and female-biased words cluster together We take the most biased words in the vocabulary according to the original bias (500 male-(a) Clustering for HARD-DEBIASED embedding, before (left hand-side) and after (right hand-side) debiasing.", "labels": [], "entities": []}, {"text": "(b) Clustering for GN-GLOVE embedding, before (left handside) and after (right hand-side) debiasing.", "labels": [], "entities": []}, {"text": "Figure 1: Clustering the 1,000 most biased words, before and after debiasing, for both models.", "labels": [], "entities": []}, {"text": "biased and 500 female-biased 9 ), and cluster them into two clusters using k-means.", "labels": [], "entities": []}, {"text": "For the HARD-DEBIASED embedding, the clusters align with gender with an accuracy of 92.5% (according to the original bias of each word), compared to an accuracy of 99.9% with the original biased version.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9982339143753052}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9964113831520081}]}, {"text": "For the GN-GLOVE embedding, we get an accuracy of 85.6%, compared to an accuracy of 100% with the biased version.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996362924575806}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9990412592887878}]}, {"text": "These results suggest that indeed much of the bias information is still embedded in the representation after debiasing.", "labels": [], "entities": []}, {"text": "shows the tSNE projection of the vectors before and after debiasing, for both models.", "labels": [], "entities": []}, {"text": "Bias-by-projection correlates to bias-byneighbours This clustering of gendered words indicates that while we cannot directly \"observe\" the bias (i.e. the word \"nurse\" will no longer be closer to explicitly marked feminine words) the bias is still manifested by the word being close to socially-marked feminine words, for example \"nurse\" being close to \"receptionist\", \"caregiver\" and \"teacher\".", "labels": [], "entities": [{"text": "Bias-by-projection", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9038150906562805}]}, {"text": "This suggests anew mechanism for measuring bias: the percentage of male/female socially-biased words among the k nearest neighbors of the target word.", "labels": [], "entities": []}, {"text": "We measure the correlation of this new bias  Professions We consider the list of professions used in and in light of the neighbours-based bias definition.", "labels": [], "entities": []}, {"text": "plots the professions, with axis X being the original bias and axis Y being the number of male neighbors, before and after debiasing.", "labels": [], "entities": []}, {"text": "For both methods, there is a clear correlation between the two variables.", "labels": [], "entities": []}, {"text": "We observe a Pearson correlation of 0.606 (compared to a correlation of 0.747 when checking neighbors according to the biased version) for HARD-DEBIASED and 0.792 (compared to 0.820) for GN-GLOVE.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.9789701700210571}, {"text": "HARD-DEBIASED", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.7793511748313904}, {"text": "GN-GLOVE", "start_pos": 187, "end_pos": 195, "type": "DATASET", "confidence": 0.899605393409729}]}, {"text": "All these correlations are significant with p-values < 1 \u00d7 10 \u221230 .", "labels": [], "entities": []}], "tableCaptions": []}