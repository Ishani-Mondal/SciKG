{"title": [{"text": "One Size Does Not Fit All: Comparing NMT Representations of Different Granularities", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has shown that contextualized word representations derived from neural machine translation area viable alternative to such from simple word predictions tasks.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7718907395998637}]}, {"text": "This is because the internal understanding that needs to be builtin order to be able to translate from one language to another is much more comprehensive.", "labels": [], "entities": []}, {"text": "Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies , and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used.", "labels": [], "entities": []}, {"text": "Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics.", "labels": [], "entities": []}, {"text": "We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen the revolution of deep neural networks and the subsequent rise of representation learning based on network-internal activations.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.9170101583003998}]}, {"text": "Such representations have been shown useful when addressing various problems from fields such as image recognition, speech recognition (, and natural language processing (NLP) ().", "labels": [], "entities": [{"text": "image recognition", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7611020505428314}, {"text": "speech recognition", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.775076299905777}]}, {"text": "The central idea is that the internal representations trained to solve an NLP task could be useful for other tasks as well.", "labels": [], "entities": []}, {"text": "For example, word embeddings learned fora simple word prediction task in context, word2vec-style (), have now become almost obligatory in state-of-the-art NLP models.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7820766766866049}]}, {"text": "One issue with such word embeddings is that the resulting representation is context-independent.", "labels": [], "entities": []}, {"text": "Recently, it has been shown that huge performance gains can be achieved by contextualizing the representations, so that the same word could have a different embedding in different contexts.", "labels": [], "entities": []}, {"text": "This is best achieved by changing the auxiliary task.", "labels": [], "entities": []}, {"text": "For example,) learns contextualized word embeddings from language modeling (LM) using long short-term memory networks (LSTMs).", "labels": [], "entities": []}, {"text": "It has been further argued that complex auxiliary tasks such as neural machine translation (NMT) are better tailored for representation learning, as the internal understanding of the input language that needs to be built by the network to be able to translate from one language to another needs to be much more comprehensive compared to what would be needed fora simple word prediction task.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.8470910886923472}, {"text": "representation learning", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.9336254596710205}, {"text": "word prediction task", "start_pos": 370, "end_pos": 390, "type": "TASK", "confidence": 0.7955343127250671}]}, {"text": "This idea is implemented in the seq2seq-based CoVe model (.", "labels": [], "entities": []}, {"text": "More recently, the BERT model ( proposed to use representation from another NMT model, the Transformer, while optimizing for two LM-related auxiliary tasks: (i) masked language model and (ii) next sentence prediction.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9896029829978943}, {"text": "next sentence prediction", "start_pos": 192, "end_pos": 216, "type": "TASK", "confidence": 0.6483656366666158}]}, {"text": "Another important aspect of representation learning is the basic unit the model operates on.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9701136350631714}]}, {"text": "In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words ( . This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g.,), GloVe () and FastText () offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases.", "labels": [], "entities": []}, {"text": "The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences.", "labels": [], "entities": [{"text": "byte-pair encoding (BPE)", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7616178631782532}]}, {"text": "A less popular solution is to use characters as the basic unit (, and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation).", "labels": [], "entities": []}, {"text": "The impact of using different units of representation in NMT models has been studied in previous work (, among others), but the focus has been exclusively on the quality of the resulting translation output.", "labels": [], "entities": []}, {"text": "However, it remains unclear what input and output units should be chosen if we are primarily interested in representation learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.892555832862854}]}, {"text": "Here, we aim at bridging this gap by evaluating the quality of NMT-derived embeddings originating from units of different granularity when used for modeling morphology, syntax, and semantics (as opposed to end tasks such as sentiment analysis and question answering).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.9267393946647644}, {"text": "question answering", "start_pos": 247, "end_pos": 265, "type": "TASK", "confidence": 0.7405679374933243}]}, {"text": "Our contributions are as follows: \u2022 We study the impact of using words vs. characters vs. BPE units vs. morphological segments on the quality of representations learned by NMT models when used to model morphology, syntax, and semantics.", "labels": [], "entities": []}, {"text": "\u2022 We further study the robustness of these representations with respect to noise.", "labels": [], "entities": []}, {"text": "We found that while representations derived from morphological segments are better for modeling non-local syntactic and semantic dependencies, character-based ones are superior for morphology and are also more robust to noise.", "labels": [], "entities": []}, {"text": "There is also value in combining different representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data and Languages We trained NMT systems for four language pairs: German-English, CzechEnglish, Russian-English, and English-German, using data made available through two popular machine translation campaigns, namely, WMT (Bojar et al., 2017) and IWSLT (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 219, "end_pos": 222, "type": "DATASET", "confidence": 0.8283483982086182}, {"text": "IWSLT", "start_pos": 248, "end_pos": 253, "type": "DATASET", "confidence": 0.6463834047317505}]}, {"text": "We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU ().", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9310159087181091}, {"text": "NEWS", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9731783866882324}, {"text": "TED training datasets", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.8995717167854309}, {"text": "TED test sets (testsets-11-13", "start_pos": 116, "end_pos": 145, "type": "DATASET", "confidence": 0.9122408509254456}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9984155893325806}]}, {"text": "We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9591925740242004}, {"text": "TED testsets", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.7796788513660431}]}, {"text": "We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora.", "labels": [], "entities": []}, {"text": "See for details about the datasets.", "labels": [], "entities": []}, {"text": "Taggers We used RDRPOST () to annotate data for the classifier.", "labels": [], "entities": [{"text": "RDRPOST", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7902966737747192}]}, {"text": "For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (, which were made available by for more detailed statistics about the train/dev/test datasets we used.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8449506163597107}, {"text": "Groningen Parallel Meaning Bank", "start_pos": 72, "end_pos": 103, "type": "DATASET", "confidence": 0.729849174618721}]}, {"text": "MT Systems and Classifiers We used seq2seq-attn to train a two-layer encoder-decoder NMT model based on LSTM representation with attention (Hochreiter and Schmidhuber, 1997) with a bidirectional encoder and a unidirectional decoder.", "labels": [], "entities": []}, {"text": "We used 500 dimensions for both word embeddings and LSTM states.", "labels": [], "entities": []}, {"text": "We trained the systems with SGD for 20 epochs and we used the final model, i.e., the one with the lowest loss on the development dataset, to generate features for the classifier.", "labels": [], "entities": []}, {"text": "We trained our neural machine translation models in both *-to-English and English-to-* translation directions, and we analyzed the representations from both the encoder and the decoder.", "labels": [], "entities": []}, {"text": "In order to analyze the representations derived from the encoder side, we fixed the decoder side with BPEbased embeddings, and we trained the source side with word/BPE/Morfessor/character units.", "labels": [], "entities": []}, {"text": "Similarly, when analyzing the representations from the decoder side, we trained the encoder representation with BPE units, and we varied the decoder side using word/BPE/char units.", "labels": [], "entities": [{"text": "BPE", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9900117516517639}]}, {"text": "Our motivation for this setup is that we wanted to analyze the encoder/decoder side representations in isolation, keeping the other half of the network (i.e., the decoder/encoder) static across different settings.", "labels": [], "entities": []}, {"text": "6 In our experiments, we used 50k BPE operations and we limited the vocabulary of all systems to 50k.", "labels": [], "entities": []}, {"text": "Moreover, we trained the word, BPE, Morfessor, and character-based systems with maximum sentence lengths of 80, 100, 100, and 400 units, respectively.", "labels": [], "entities": [{"text": "BPE", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.7595441937446594}]}, {"text": "For the classification tasks, we used a logistic regression classifier whose input is either the hidden states in the case of the word-based models, or the Last or the Average representations in the case of character-and subword-based models.", "labels": [], "entities": []}, {"text": "Since for the bidirectional encoder we concatenate forward and backward states from all layers, this yields 2,000/1,000 dimensions when classifying using the representations from the encoder/decoder: 500 dimensions\u00d72 layers\u00d72 directions (1 for the decoder, as it is uni-directional).", "labels": [], "entities": []}, {"text": "In all cases, we trained the logistic regression classifier for ten epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics about NMT and classifier training  data for English (en), German (de), Russian (ru), and  Czech (cs). Here, CV stands for cross-validation.", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8664504289627075}]}, {"text": " Table 4: Classifier accuracy for the representations  generated by aggregating subword (sub) or character  (char) representations using either the average or the  last LSTM state for each word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9894146919250488}]}, {"text": " Table 5: BLEU scores across language pairs. \"s \u2192 t\"  means source and target units; morf = Morfessor.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9970476031303406}]}, {"text": " Table 6: OOV rate for the MT and the classifier testsets.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9805663824081421}, {"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.8791314363479614}]}, {"text": " Table 7: Classification accuracy for combined rep- resentations for morphological and semantic tagging.  Here, W/B/C stand for word/BPE/character units.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9903222918510437}, {"text": "semantic tagging", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6940895915031433}]}]}