{"title": [{"text": "Text Classification with Few Examples using Controlled Generalization", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6819431632757187}]}], "abstractContent": [{"text": "Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems.", "labels": [], "entities": [{"text": "text classification", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8159615397453308}]}, {"text": "This means classifiers must generalize from limited evidence , but the manner and extent of generalization is task dependent.", "labels": [], "entities": []}, {"text": "Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones.", "labels": [], "entities": []}, {"text": "Unfortunately, this squishes many components of meaning into highly restricted capacity.", "labels": [], "entities": []}, {"text": "Our alternative begins with sparse pre-trained representations derived from unla-beled parsed corpora; based on the available training data, we select features that offers the relevant generalizations.", "labels": [], "entities": []}, {"text": "This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods.", "labels": [], "entities": []}, {"text": "By further pairing this network with a convolu-tional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network).", "labels": [], "entities": [{"text": "text classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8521834015846252}]}, {"text": "Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7644914984703064}]}, {"text": "Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class.", "labels": [], "entities": []}, {"text": "Labels can be obtained cheaply from crowd workers for some languages, but there area nearly unlimited number of bespoke, challenging text classification problems that crop up in practical settings (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7191082090139389}]}, {"text": "Obtaining representative labeled examples for classification problems with many labels, like taxonomies, is especially challenging.", "labels": [], "entities": []}, {"text": "Text classification is abroad but useful term and covers classification based on topic, on sentiment, and even social status.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7272438555955887}]}, {"text": "As Systemic Functional Linguists such as point out, language carries many kinds of meanings.", "labels": [], "entities": []}, {"text": "For example, words such as ambrosial and delish inform us not just of the domain of the text (food) and sentiment, but perhaps also of the age of the speaker.", "labels": [], "entities": []}, {"text": "Text classification problems differ on the dimensions they distinguish along and thus in the words that help in identifying the class.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7927015423774719}]}, {"text": "As show, classifiers mostly focus on sub-lexicons; they memorize patterns instead of extending more general knowledge about language to a particular task.", "labels": [], "entities": []}, {"text": "When there is low lexical overlap between training and test data, accuracy drops as much as 23.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994434714317322}]}, {"text": "When training data is limited, most meaning-carrying terms are never seen in training, and the sub-lexicons correspondingly poorer.", "labels": [], "entities": []}, {"text": "Classifiers must generalize from available training data, possibly exploiting external knowledge, including representations derived from raw texts.", "labels": [], "entities": []}, {"text": "For small training sizes, this requires moving beyond sub-lexicons.", "labels": [], "entities": []}, {"text": "Existing strategies for low data scenarios include treating labels as informative) and using labelspecific lexicons, but neither is competitive when labeled data is plentiful.", "labels": [], "entities": []}, {"text": "Instead, we seek classifiers that adapt to both low and high data scenarios.", "labels": [], "entities": []}, {"text": "People exploit parallelism among examples for generalization; Hofstadter and 1.1 Kampuchea says rice crop in 1986 increased . .", "labels": [], "entities": [{"text": "generalization", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.9652290940284729}]}, {"text": "1.2 U.S. sugar policy may self-destruct . .", "labels": [], "entities": []}, {"text": "2.2 Life on Mars 1.3 EC denies maize exports reserved for the U.S..", "labels": [], "entities": [{"text": "2.2 Life on Mars 1.3 EC", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.44189197321732837}]}, {"text": "2.3 Single launch space stations 1.4 U.S. corn, sorghum payments 50-50 cash/certs.", "labels": [], "entities": []}, {"text": "2.4 Astronauts-what does weighlessness feel like?", "labels": [], "entities": []}, {"text": "1.5 Canada corn decision unjustified.", "labels": [], "entities": [{"text": "Canada corn decision", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7317519783973694}]}, {"text": "2.5 Satellite around Pluto mission?", "labels": [], "entities": []}, {"text": "Consider, which displays five examples from a single class for two tasks.", "labels": [], "entities": []}, {"text": "Bolded terms for each task are clearly related, and to a person, suggest abstractions that help relate other terms to the task.", "labels": [], "entities": []}, {"text": "This helps with disambiguation: that the word Pluto is the planet and not Disney's character is inferred not just by withinexample evidence (e.g. mission) but also by crossexample presence of Mars and astronauts.", "labels": [], "entities": []}, {"text": "Cross-example analysis also reveals the amount of generalization warranted.", "labels": [], "entities": []}, {"text": "For a word associated with a label, word embeddings give us neighbors, which often are associated with that label.", "labels": [], "entities": []}, {"text": "What they do not tell us is the extent this associatedwith-same-label phenomenon holds; that depends on the granularity of the classes.", "labels": [], "entities": []}, {"text": "Cross-example analysis is required to determine how neighbors at various distances are distributed among labels in the training data.", "labels": [], "entities": []}, {"text": "This should allow us to include barley and peaches as evidence fora class like Agriculture but only barley for Grains.", "labels": [], "entities": []}, {"text": "Most existing systems ignore cross-example parallelism and thus miss out on a strong classification signal.", "labels": [], "entities": []}, {"text": "We introduce a flexible method for controlled generalization that selects syntactosemantic features from sparse representations constructed by Category Builder (.", "labels": [], "entities": []}, {"text": "Starting with sparse representations of words and their contexts, a tuning algorithm selects features with the relevant kinds and appropriate amounts of generalization, making use of parallelism among examples.", "labels": [], "entities": []}, {"text": "This produces taskspecific dense embeddings for new texts that can be easily incorporated into classifiers.", "labels": [], "entities": []}, {"text": "Our simplest model, CBC (Category Builder Classifier), is a feed-forward network that uses only CB embeddings to represent a document.", "labels": [], "entities": []}, {"text": "For small amounts of training data, this simple model dramatically outperforms both CNNs and BERT  the CNN that concatenates their pre-prediction layers and adds an additional layer.", "labels": [], "entities": [{"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9967817068099976}]}, {"text": "By training this model with a scheduled block dropout () that gradually introduces the CBC sub-network, we obtain the benefits of CBC in low data scenarios while obtaining parity with CNNs when plentiful data is available.", "labels": [], "entities": [{"text": "CBC", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8537916541099548}]}, {"text": "BERT still dominates when all data is available, suggesting that further combinations or ensembles are likely to improve matters further.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9844359159469604}]}], "datasetContent": [{"text": "Our primary goal is to study classifier performance with limited data.", "labels": [], "entities": []}, {"text": "To that end, we obtain learning curves on four standard text classification datasets) based on evaluating predictions on the full test sets.", "labels": [], "entities": []}, {"text": "At each sample size, we produce multiple samples and run several text classification methods multiple times, measuring the following: \u2022 Macro-F1 score.", "labels": [], "entities": [{"text": "text classification", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7072071433067322}]}, {"text": "Macro-F1 measures support for all classes better than accuracy, especially with imbalanced class distributions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9992563128471375}]}, {"text": "\u2022 Recall for the rarest class.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9038116931915283}]}, {"text": "Many measures like F1 and accuracy often mask performance on infrequent but high impact classes, such as detecting toxicity (Waseem and Hovy, 2016)) \u2022 Degenerate solutions.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9995755553245544}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996004700660706}, {"text": "detecting toxicity", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8517057299613953}]}, {"text": "Complex classifiers with millions of parameters sometimes produce degenerate classifiers when provided very few training examples; as a result, they can skip some output classes entirely.", "labels": [], "entities": []}, {"text": "The datasets we chose for evaluation, while all multi-class, form a diverse set in terms of the number of classes and kinds of cohesion among examples in a single class.", "labels": [], "entities": []}, {"text": "The former clearly affects training data needs, while the latter informs appropriate generalization.", "labels": [], "entities": []}, {"text": "\u2022 20 Newsgroups 20Newsgroups (20NG) contains documents from 20 different newsgroups with about 1000 messages from each.", "labels": [], "entities": []}, {"text": "We randomly split the documents into an 80-10-10 train-dev-test split.", "labels": [], "entities": []}, {"text": "The classes are evenly balanced.", "labels": [], "entities": []}, {"text": "The Reuters21578 dataset contains Reuters Newswire articles.", "labels": [], "entities": [{"text": "Reuters21578 dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9752485156059265}, {"text": "Reuters Newswire articles", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.8808207710584005}]}, {"text": "Following several authors (, for example), we use only the eight most frequent labels.", "labels": [], "entities": []}, {"text": "We begin with a given 80/10/10 split.", "labels": [], "entities": []}, {"text": "Given that we focused on single-label classification, we removed items associated with two or more of the top eight labels (about 3% of examples).", "labels": [], "entities": [{"text": "single-label classification", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6866464167833328}]}, {"text": "Of the 6888 training examples, 3128 are labeled earn, while only 228 examples are of class interest and only 128 are ship.", "labels": [], "entities": []}, {"text": "\u2022 Wiki Comments Personal Attack.", "labels": [], "entities": [{"text": "Wiki Comments Personal Attack", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.5745873525738716}]}, {"text": "The Wikipedia Detox project collected over 100k discussion comments from English Wikipedia and annotated them for presence of personal attack ().", "labels": [], "entities": []}, {"text": "We randomly select 10k, 2k, and 2k items as train/dev/test.", "labels": [], "entities": []}, {"text": "\u2022 Spam The SMS Spam Collection v.1 has SMS labeled messages that were collected for mobile phone spam research ().", "labels": [], "entities": []}, {"text": "Each of the 5574 messages is labeled as spam or ham.", "labels": [], "entities": []}, {"text": "Our primary goal is to improve generalization for low-data scenarios, but we also want our methods to remain competitive on full data.", "labels": [], "entities": [{"text": "generalization", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9583870768547058}]}, {"text": "We compare different models across learning curves of increasing the training set sizes.", "labels": [], "entities": []}, {"text": "We use training data sizes of 40, 80, . .", "labels": [], "entities": []}, {"text": ", 5120 as well as the entire available training data.", "labels": [], "entities": []}, {"text": "For each training size, we produce three independent samples 3 https://github.com/google/categorybuilder by uniformly sampling the training data and training each model three times at each size.", "labels": [], "entities": []}, {"text": "The final value reported is the average of all nine runs.", "labels": [], "entities": []}, {"text": "All models are implemented in Tensorflow.", "labels": [], "entities": []}, {"text": "Batch sizes are between 5 and 64 depending on training size.", "labels": [], "entities": []}, {"text": "Training stops after there is no macro-F1 improvement on development data for 1000 steps.", "labels": [], "entities": []}, {"text": "For evaluation, we focus primarily on macro-F1 and recall of the rarest class.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9992997646331787}]}, {"text": "The recall on the rarest class is especially important for imbalanced classification problems.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9992276430130005}]}, {"text": "For such problems, a model can obtain high accuracy by strongly preferring the majority class, but we seek models that effectively identify minority class labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9979666471481323}]}, {"text": "(This is especially important for active learning scenarios, where we expect the CB-vectors to help within future.) shows learning curves giving macro-F1 scores and rarest class recall for all four datasets.", "labels": [], "entities": [{"text": "rarest class recall", "start_pos": 165, "end_pos": 184, "type": "METRIC", "confidence": 0.7563537359237671}]}, {"text": "When very limited training data is available, the simple CBC model generally outperforms the CNN and BERT, except for the Spam dataset.", "labels": [], "entities": [{"text": "CNN", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.7758470177650452}, {"text": "BERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9948461055755615}, {"text": "Spam dataset", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.9415760040283203}]}, {"text": "The more powerful models eventually surpass CBC; however, the CBCNN model provides consistent strong performance at all dataset sizes by combining the generalization of CBC with the general efficacy of CNNs.", "labels": [], "entities": [{"text": "CBC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.7961044907569885}]}, {"text": "Importantly, CBCNN provides massive error reductions with low data for 20NG and R8 (tasks with many labels).'s left half gives results for all models when using only 320 training examples.", "labels": [], "entities": [{"text": "CBCNN", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8265427350997925}]}, {"text": "For 20NG, CNN's macro-F1 is just 43.9, whereas CBC and CBCNN achieve 61.7 and 62.4-the same as CNN performance with four times as much data.", "labels": [], "entities": [{"text": "CNN", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8644608855247498}, {"text": "CBC", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8308501839637756}, {"text": "CBCNN", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8701269030570984}]}, {"text": "These models outperform CNN on R8 as well, reaching 83.7 vs CNN's 74.1, and also on the Wiki-attack dataset, achieving 80.6 vs CNNs 74.0.", "labels": [], "entities": [{"text": "R8", "start_pos": 31, "end_pos": 33, "type": "DATASET", "confidence": 0.7941023111343384}, {"text": "CNN", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.95720374584198}, {"text": "Wiki-attack dataset", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.9658360779285431}]}, {"text": "BERT fails to produce a solution for the two datasets with >2 labels, but does produce the best result for Spam-indicating an opportunity to more fully explore BERT's parameter settings for low data scenarios and to fruitfully combine CBC with BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9544975161552429}, {"text": "BERT", "start_pos": 244, "end_pos": 248, "type": "METRIC", "confidence": 0.8192592859268188}]}], "tableCaptions": [{"text": " Table 1: Left: examples from the Reuters Grains class, showing semantic type cohesion (kinds of crops). Right:  post headers from the sci.space newsgroup in 20 Newsgroups, showing topical cohesion (astronomical terms).  Bolded terms are to draw the reader's attention to parallels among examples.", "labels": [], "entities": [{"text": "Reuters Grains class", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9122768044471741}]}, {"text": " Table 5: Macro-F1 scores on all data sets when using 320 training examples (left) and when using all available  training data (right). k is the number of classes. The CBCNN model provides the strongest overall performance  across all data sizes. (Note that BERT produces degenerate solutions for the >2 class problems with 320 examples.)", "labels": [], "entities": [{"text": "BERT", "start_pos": 258, "end_pos": 262, "type": "METRIC", "confidence": 0.923529326915741}]}, {"text": " Table 6: Minimum training size at which a non- degenerate model was produced in any of 9 runs. With  more classes, more data is needed by CNN and BERT  to produce acceptable models. k is number of classes.", "labels": [], "entities": [{"text": "Minimum training size", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8309466044108073}, {"text": "CNN", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.8813730478286743}, {"text": "BERT", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9175421595573425}]}]}