{"title": [{"text": "Better Word Embeddings by Disentangling Contextual n-Gram Information", "labels": [], "entities": []}], "abstractContent": [{"text": "Pre-trained word vectors are ubiquitous in Natural Language Processing applications.", "labels": [], "entities": []}, {"text": "In this paper, we show how training word em-beddings jointly with bigram and even trigram embeddings, results in improved unigram em-beddings.", "labels": [], "entities": []}, {"text": "We claim that training word embed-dings along with higher n-gram embeddings helps in the removal of the contextual information from the unigrams, resulting in better stand-alone word embeddings.", "labels": [], "entities": []}, {"text": "We empirically show the validity of our hypothesis by outper-forming other competing word representation models by a significant margin on a wide variety of tasks.", "labels": [], "entities": []}, {"text": "We make our models publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed word representations are essential building blocks of modern NLP systems.", "labels": [], "entities": [{"text": "Distributed word representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6790494918823242}]}, {"text": "Used as features in downstream applications, they often enhance generalization of models trained on a limited amount of data.", "labels": [], "entities": []}, {"text": "They do so by capturing relevant distributional information about words from large volumes of unlabeled text.", "labels": [], "entities": []}, {"text": "Efficient methods to learn word vectors have been introduced in the past, most of them based on the distributional hypothesis of;: \"a word is characterized by the company it keeps\".", "labels": [], "entities": []}, {"text": "While a standard approach relies on global corpus statistics () formulated as a matrix factorization using mean square reconstruction loss, other widely used methods are the bilinear word2vec architectures introduced by: While skip-gram aims to predict nearby words from a given word, CBOW predicts a target word from its set of context words.", "labels": [], "entities": []}, {"text": "Recently, significant improvements in the quality of the word embeddings were obtained by * indicates equal contribution augmenting word-context pairs with sub-word information in the form of character n-grams, especially for morphologically rich languages.", "labels": [], "entities": []}, {"text": "Nevertheless, to the best of our knowledge, no method has been introduced leveraging collocations of words with higher order word n-grams such as bigrams or trigrams as well as character n-grams together.", "labels": [], "entities": []}, {"text": "In this paper, we show how using higher order word n-grams along with unigrams during training can significantly improve the quality of obtained word embeddings.", "labels": [], "entities": []}, {"text": "The addition furthermore helps to disentangle contextual information present in the training data from the unigrams and results in overall better distributed word representations.", "labels": [], "entities": []}, {"text": "To validate our claim, we train two modifications of CBOW augmented with word-n-gram information during training.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.7852931022644043}]}, {"text": "One is a recent sentence embedding method,, which we repurpose to obtain word vectors.", "labels": [], "entities": []}, {"text": "The second method we propose is a modification of CBOW enriched with character ngram information ) that we again augment with word n-gram information.", "labels": [], "entities": []}, {"text": "In both cases, we compare the resulting vectors with the most widely used word embedding methods on word similarity and analogy tasks and show significant quality improvements.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.7991016626358032}]}, {"text": "The code used to train the models presented in this paper as well as the models themselves are made available to the public 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our model, we use six datasets covering pair-wise word-similarity tasks and two datasets covering word-analogy tasks.", "labels": [], "entities": []}, {"text": "Word-similarity tasks consist of word pairs along with their human annotated similarity scores.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our models on pair-wise wordsimilarity tasks, we use WordSim353 (353 wordpairs) (   To calculate the similarity between two words, we use the cosine similarity between their word representations.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.9222701787948608}]}, {"text": "The similarity scores then, are compared to the human ratings using Spearman's \u03c1 (Spearman, 1904) correlation scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9857818484306335}, {"text": "Spearman's \u03c1 (Spearman, 1904) correlation", "start_pos": 68, "end_pos": 109, "type": "METRIC", "confidence": 0.6576673918300204}]}, {"text": "Word analogy tasks pose analogy relations of the form \"x is toy as x is toy \", where y is hidden and must be guessed from the dataset vocabulary.", "labels": [], "entities": [{"text": "Word analogy", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7071171700954437}]}, {"text": "We use the MSR () and the Google () analogy datasets.", "labels": [], "entities": [{"text": "MSR", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9127861857414246}, {"text": "Google () analogy datasets", "start_pos": 26, "end_pos": 52, "type": "DATASET", "confidence": 0.8348802775144577}]}, {"text": "The MSR dataset contains 8000 syntactic analogy quadruplets while the Google set has 8869 semantic and 10675 syntactic relations.", "labels": [], "entities": [{"text": "MSR dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9587563276290894}, {"text": "Google set", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9292377233505249}]}, {"text": "To calculate the missing word in the relation, we use the 3CosMul method (: where \u03b5 = 0.0001 is used to prevent division by 0 and V is the dataset vocabulary.", "labels": [], "entities": []}, {"text": "We remove all the out of vocabulary words and are left with 6946 syntactic relations for the MSR dataset and 1959 word-pairs for the Rare Words dataset.", "labels": [], "entities": [{"text": "MSR dataset", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.9806524515151978}, {"text": "Rare Words dataset", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.8474945624669393}]}, {"text": "All other datasets do not have any out of vocabulary words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Impact of using word n-grams: Models are compared using Spearman correlation measures for word  similarity tasks and accuracy for word analogy tasks. Top performances on each dataset are shown in bold. An  underline shows the best model(s) restricted to each architecture type. The abbreviations uni., bi., and tri. stand  for unigrams, bigrams, and trigrams respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9984843134880066}, {"text": "word analogy", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.7398028671741486}]}, {"text": " Table 2: Improvement over existing methods: Models are compared using Spearman correlation measures for word  similarity tasks and accuracy for word analogy tasks. Top performance(s) on each dataset is(are) shown in bold.  The abbreviations uni., bi., and tri. stand for unigrams, bigrams, and trigrams respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.998285710811615}, {"text": "word analogy tasks", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.8040352662404379}]}, {"text": " Table 3: Training parameters for all non-GloVe models", "labels": [], "entities": [{"text": "Training", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.937677800655365}]}]}