{"title": [{"text": "PAWS: Paraphrase Adversaries from Word Scrambling", "labels": [], "entities": [{"text": "Paraphrase Adversaries from Word Scrambling", "start_pos": 6, "end_pos": 49, "type": "TASK", "confidence": 0.7404227137565613}]}], "abstractContent": [{"text": "Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.7916170060634613}]}, {"text": "Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York.", "labels": [], "entities": []}, {"text": "This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), anew dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap.", "labels": [], "entities": [{"text": "Paraphrase Adversaries from Word Scrambling)", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.5825572113196055}]}, {"text": "Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7002445161342621}]}, {"text": "State-of-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9979180693626404}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9978273510932922}]}, {"text": "In contrast, models that do not capture non-local contextual information fail even with PAWS training examples.", "labels": [], "entities": []}, {"text": "As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pair-wise comparisons.", "labels": [], "entities": [{"text": "PAWS", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.6035501956939697}]}], "introductionContent": [{"text": "Word order and syntactic structure have a large impact on sentence meaning.", "labels": [], "entities": [{"text": "sentence meaning", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7036232799291611}]}, {"text": "Even small perturbation in word order can completely change interpretation.", "labels": [], "entities": []}, {"text": "Consider the following related sentences.", "labels": [], "entities": []}, {"text": "(1) Flights from New York to Florida.", "labels": [], "entities": []}, {"text": "(2) Flights to Florida from NYC.", "labels": [], "entities": []}, {"text": "(3) Flights from Florida to New York.", "labels": [], "entities": []}, {"text": "All three have high bag-of-words (BOW) overlap.", "labels": [], "entities": [{"text": "bag-of-words (BOW) overlap", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.7394372940063476}]}, {"text": "However, (2) is a paraphrase of (1), while (3) has a very different meaning from (1).", "labels": [], "entities": []}, {"text": "Existing datasets lack non-paraphrase pairs like (1) and (3).", "labels": [], "entities": []}, {"text": "The Quora Question Pairs (QQP) corpus contains 400k real world pairs, but its negative examples are drawn primarily from related questions.", "labels": [], "entities": [{"text": "Quora Question Pairs (QQP) corpus", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.5653093414647239}]}, {"text": "Few have high word overlap, and of the \u223c1,000 pairs with the same BOW, only 20% are not paraphrases.", "labels": [], "entities": [{"text": "BOW", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.7838121056556702}]}, {"text": "This provides insufficient representative examples to evaluate models' performance on this problem, and there are too few examples for models to learn the importance of word order.", "labels": [], "entities": []}, {"text": "shows that models trained on QQP are inclined to mark any sentence pairs with high word overlap as paraphrases despite clear clashes in meaning.", "labels": [], "entities": []}, {"text": "Models trained or evaluated with only this data may not perform well on real world tasks where such sensitivity is important.", "labels": [], "entities": []}, {"text": "To address this, we introduce a workflow (outlined in) for generating pairs of sentences that have high word overlap, but which are balanced with respect to whether they are paraphrases or not.", "labels": [], "entities": []}, {"text": "Using this process, we create PAWS (Paraphrase Adversaries from Word Scrambling), a dataset constructed from sentences in Quora and That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "labels": [], "entities": [{"text": "Paraphrase Adversaries from Word Scrambling)", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.5494610816240311}]}, {"text": "Examples are generated from controlled language models and back translation, and given five human ratings each in both phases.", "labels": [], "entities": []}, {"text": "A final rule recombines annotated examples and balances the labels.", "labels": [], "entities": []}, {"text": "Our final PAWS dataset will be released publicly with 108,463 pairs at https: //g.co/dataset/paws.", "labels": [], "entities": [{"text": "PAWS dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9193969368934631}]}, {"text": "We show that existing state-of-the-art models fail miserably on PAWS when trained on existing resources, but some perform well when given PAWS training examples.", "labels": [], "entities": []}, {"text": "BERT fine-tuned on QQP achieves over 90% accuracy on QQP, but only 33% accuracy on PAWS data in the same domain.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9475993514060974}, {"text": "QQP", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8458002805709839}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992054104804993}, {"text": "QQP", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9348793625831604}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9990384578704834}, {"text": "PAWS data", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8436309099197388}]}, {"text": "However, the accuracy on PAWS boosts to 85% by including 12k PAWS training pairs (without reducing QQP performance).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999733030796051}, {"text": "QQP", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8735916614532471}]}, {"text": "Table 1 also shows that the new model is able to correctly classify challenging pairs.", "labels": [], "entities": []}, {"text": "Annotation scale is also important: our learning curves show strong models like BERT improve with tens of thousands of training examples.", "labels": [], "entities": [{"text": "Annotation scale", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9723797738552094}, {"text": "BERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9967681169509888}]}, {"text": "Our experimental results also demonstrate that PAWS effectively measures sensitivity of models to word order and structure.", "labels": [], "entities": [{"text": "PAWS", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.8365521430969238}]}, {"text": "Unlike BERT, a simple BOW model fails to learn from PAWS training examples, demonstrating its weakness at capturing non-local contextual information.", "labels": [], "entities": [{"text": "BERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9836223721504211}]}, {"text": "Our experiments show that the gains from PAWS examples correlate with the complexity of models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the example generation strategies described in Section 3 combined with human paraphrase an-  We start by producing swapped examples from both QQP and Wikipedia.", "labels": [], "entities": [{"text": "example generation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7357623875141144}, {"text": "QQP", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.955352783203125}]}, {"text": "Both sources contain naturally occurring sentences covering many topics.", "labels": [], "entities": []}, {"text": "On both corpora only about 3% of candidates are selected for further processing-the rest are filtered because there is no valid generation candidate that satisfies all swapping constraints or because the language model score of the best candidate is below the threshold.", "labels": [], "entities": []}, {"text": "The remaining pairs (16,280 for QQP and 50k for Wikipedia) are passed to human review.", "labels": [], "entities": [{"text": "QQP", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9080973863601685}, {"text": "Wikipedia", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9525808691978455}]}, {"text": "Sentence correction The examples generated using both of our strategies are generally of high quality, but they still need to be checked with respect to grammar and coherence.", "labels": [], "entities": [{"text": "Sentence correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9063765108585358}]}, {"text": "Annotators evaluate each generated sentence without seeing its source sentence.", "labels": [], "entities": []}, {"text": "The sentence is accepted as is, fixed, or rejected.", "labels": [], "entities": []}, {"text": "shows the number of pairs of each action on each domain.", "labels": [], "entities": []}, {"text": "Most of fixes are minor grammar corrections like a apple\u2192an apple.", "labels": [], "entities": []}, {"text": "Accepted and fixed sentences are then passed to the next stage for paraphrase annotation.", "labels": [], "entities": [{"text": "paraphrase annotation", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.872987687587738}]}, {"text": "Overall 88% of generated examples passed the human correction phase on both domains.", "labels": [], "entities": [{"text": "human correction", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.6749397069215775}]}, {"text": "Paraphrase identification Sentence pairs are presented to five annotators, each of which gives a binary judgment as to whether they are paraphrases or not.", "labels": [], "entities": [{"text": "Paraphrase identification Sentence pairs", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8998652696609497}]}, {"text": "We choose binary judgments to make our dataset have the same label schema as the QQP corpus.", "labels": [], "entities": [{"text": "QQP corpus", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9308382570743561}]}, {"text": "shows aggregated annotation statistics on both domains, including the number of paraphrase (positive) and nonparaphrase (negative) pairs and human agreement, which is the percentage ratio of agreement between each individual label and the majority vote of five labels on each example pair.", "labels": [], "entities": []}, {"text": "Overall, human agreement is high on both Quora (92.0%) and Wikipedia (94.7%) and each label only takes about 24 seconds.", "labels": [], "entities": [{"text": "agreement", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.944139838218689}, {"text": "Quora", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9013781547546387}, {"text": "Wikipedia", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9586921334266663}]}, {"text": "As such, answers are usually straightforward to human raters.", "labels": [], "entities": []}, {"text": "To ensure the data is comprised of clearly paraphrase or non-paraphrase pairs, only examples with four or five raters agreeing are kept.", "labels": [], "entities": []}, {"text": "An example of low agreement is Why is the 20th-century music so different from the 21st music?", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9022777676582336}]}, {"text": "Why is the 21st century music so different from the 20th century music?, where three out of five raters gave negative labels on this pair.", "labels": [], "entities": []}, {"text": "The bottom block of shows the final number of pairs after this filtering, and human agreement further goes up to over 95%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.8399388790130615}]}, {"text": "Finally, source and generated sentences are randomly flipped to mask their provenance.", "labels": [], "entities": []}, {"text": "The swapping strategy generally produces nonparaphrase examples-67% for QQP and 88% for Wikipedia.", "labels": [], "entities": [{"text": "QQP", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9218698143959045}, {"text": "Wikipedia", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.9525787830352783}]}, {"text": "Because (a) the label imbalance is less pronounced for QQP and (b) NMT models perform poorly on Quora questions due to domain mismatch, we only apply the back translation strategy to Wikipedia pairs.", "labels": [], "entities": []}, {"text": "Doing so creates 26,897 candidate example pairs after filtering.", "labels": [], "entities": []}, {"text": "As before, each pair is rated by five annotators on the paraphrase identification task.", "labels": [], "entities": [{"text": "paraphrase identification task", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.9237248698870341}]}, {"text": "most of the examples (94.9%) are paraphrases (as expected), with high human agreement (94.8%).", "labels": [], "entities": [{"text": "agreement", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.8667875528335571}]}, {"text": "Finally, we expand the pairs using the the rules described in Section 3.2.", "labels": [], "entities": []}, {"text": "provides counts for each split in the final PAWS datasets.", "labels": [], "entities": [{"text": "counts", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9721412658691406}, {"text": "PAWS datasets", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.965011715888977}]}, {"text": "The training portion of PAWS QQP is a subset of the QQP training set; however, PAWS QQP 's development set is a subset of both QQP's development and test sets because there are only 677 pairs.", "labels": [], "entities": [{"text": "PAWS QQP", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9256387948989868}, {"text": "QQP training set", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9490211804707845}]}, {"text": "PAWS Wiki randomly draws 8,000 pairs for each of its development and test sets and takes the rest as its training set, with no overlap of source sentences across sets.", "labels": [], "entities": [{"text": "PAWS Wiki", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9359280169010162}]}, {"text": "Finally, any trivial pairs with identical sentences from development and test sets are removed.", "labels": [], "entities": []}, {"text": "The final PAWS QQP has a total of 12,665 pairs (443k tokens), where 31.3% of them have positive labels (paraphrases).", "labels": [], "entities": [{"text": "PAWS QQP", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9157240092754364}]}, {"text": "PAWS Wiki has a total of 65,401 pairs (2.8m tokens), where 44.2% of them are paraphrases.", "labels": [], "entities": [{"text": "PAWS Wiki", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9745769202709198}]}, {"text": "Note that we have human annotations on 43k pairs generated by the word swapping method on Wikipedia, but 30k of them have no back translation counterparts and therefore they are not included in our final PAWS Wiki dataset.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.7168531864881516}, {"text": "PAWS Wiki dataset", "start_pos": 204, "end_pos": 221, "type": "DATASET", "confidence": 0.9397848645846049}]}, {"text": "Nevertheless, they are high-quality pairs with manual labels, so we include them as an auxiliary training set (PAWS Wiki-Swap in), and empirically show its impact in Section 6.", "labels": [], "entities": [{"text": "PAWS Wiki-Swap", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.8827525079250336}]}, {"text": "Unlabeled PAWS Wiki In addition to the fully labeled PAWS Wiki dataset, we also construct an unlabeled PAWS Wiki set at large scale.", "labels": [], "entities": [{"text": "PAWS Wiki dataset", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9348737001419067}]}, {"text": "The idea is to simply treat all pairs from word swapping as nonparaphrases and all pairs from back translation as paraphrase, and construct the dataset in the same way as labeled PAWS Wiki . The result is a total of 656k pairs with silver labels.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7055333703756332}, {"text": "PAWS Wiki", "start_pos": 179, "end_pos": 188, "type": "DATASET", "confidence": 0.8995240330696106}]}, {"text": "We show empirically NMT generates fluent output.", "labels": [], "entities": []}, {"text": "Such trivial examples exist because annotators sometimes fix a swapped sentence back to its source.", "labels": [], "entities": []}, {"text": "We keep such examples in the training set (about 8% of the corpus) because otherwise a trained model would actually predict low similarity scores to identical pairs.", "labels": [], "entities": []}, {"text": "the impact of using this silver set in pre-training in Section 6.", "labels": [], "entities": []}, {"text": "We seek to understand how well models trained on standard datasets perform on PAWS pairs and to see which models are most able to learn from PAWS pairs.", "labels": [], "entities": []}, {"text": "A strong model should improve significantly on PAWS when trained on PAWS pairs without diminishing performance on existing datasets like QQP.", "labels": [], "entities": [{"text": "QQP", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.9272176027297974}]}, {"text": "Overall, both DIIN and BERT prove remarkably able to adapt to PAWS pairs and perform well on both PAWS QQP and PAWS Wiki while the other models prove far less capable.", "labels": [], "entities": [{"text": "DIIN", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.6963838934898376}, {"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9960871934890747}, {"text": "PAWS QQP", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.8419391810894012}, {"text": "PAWS Wiki", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.8494088649749756}]}, {"text": "We use two metrics: classification accuracy and area-under-curve (AUC) scores of precision-recall curves.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9150553345680237}, {"text": "area-under-curve (AUC) scores", "start_pos": 48, "end_pos": 77, "type": "METRIC", "confidence": 0.8762147784233093}, {"text": "precision-recall", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.911213219165802}]}, {"text": "For all classification models, 0.5 is the threshold used to compute accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9963533878326416}]}, {"text": "We report results on testing sets for QQP and PAWS Wiki , and on the development set for PAWS QQP (which has no test set).", "labels": [], "entities": [{"text": "QQP", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.944094717502594}, {"text": "PAWS Wiki", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9555241167545319}, {"text": "PAWS QQP", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8988716304302216}]}, {"text": "For BERT, we use the implementation provided by the authors and apply their default fine-tuning configuration.", "labels": [], "entities": [{"text": "BERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9036952257156372}]}, {"text": "We use the provided BERT BASE pre-trained model instead of BERT LARGE due to GPU memory limitations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9975624084472656}, {"text": "BASE", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8060001730918884}, {"text": "BERT LARGE", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.901547372341156}]}, {"text": "For all other models, we use our own (re-)implementations that matched reported performance on QQP.", "labels": [], "entities": [{"text": "QQP", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9217635989189148}]}, {"text": "We use 300 dimensional GloVe embeddings () to represent words and fix them during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Detailed counts for examples created via the  swapping strategy, followed by human filtering and  paraphrase judgments.", "labels": [], "entities": []}, {"text": " Table 5: Counts of experimental split for each PAWS  dataset. The final column gives the proportion of para- phrase (positive) pairs. There are 108,463 PAWS pairs  in total.", "labels": [], "entities": [{"text": "PAWS  dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8347336649894714}]}, {"text": " Table 7: Accuracy (%) of classification and AUC scores (%) of precision-recall curves on Quora Question Pairs  (QQP) testing set and our PAWS QQP development set. QQP\u2192PAWS QQP indicates that models are trained on  QQP and evaluated on PAWS QQP . Other columns are defined in a similar way. QQP+PAWS QQP is a simple  concatenation of the two training sets. Boldface numbers indicate the best accuracy for each testing scenario.  Numbers in parentheses indicate absolute gains from adding PAWS QQP training data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980216026306152}, {"text": "AUC scores", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9777076840400696}, {"text": "precision-recall", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.9889135956764221}, {"text": "Quora Question Pairs  (QQP) testing set", "start_pos": 90, "end_pos": 129, "type": "DATASET", "confidence": 0.5961081795394421}, {"text": "PAWS QQP development set", "start_pos": 138, "end_pos": 162, "type": "DATASET", "confidence": 0.748386800289154}, {"text": "accuracy", "start_pos": 392, "end_pos": 400, "type": "METRIC", "confidence": 0.998406708240509}, {"text": "PAWS QQP training data", "start_pos": 488, "end_pos": 510, "type": "DATASET", "confidence": 0.6227017641067505}]}, {"text": " Table 8: Accuracy (%) and AUC scores (%) of  different models on PAWS Wiki testing set. Super- vised models are trained on human-labeled data only,  while Pretrain+Fine-tune models are first trained on  noisy unlabeled PAWS Wiki data and then fine-tuned on  human-labeled data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992703795433044}, {"text": "AUC", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9996449947357178}, {"text": "PAWS Wiki testing set", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.9404416978359222}, {"text": "PAWS Wiki data", "start_pos": 220, "end_pos": 234, "type": "DATASET", "confidence": 0.9300937056541443}]}, {"text": " Table 9: AUC scores (%) when training DIIN models  on different sets of training data. Boldface numbers  indicate the best accuracy for each testing set.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9816545248031616}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9991432428359985}]}]}