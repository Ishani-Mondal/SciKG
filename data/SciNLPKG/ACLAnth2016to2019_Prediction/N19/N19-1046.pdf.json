{"title": [{"text": "Understanding and Improving Hidden Representation for Neural Machine Translation", "labels": [], "entities": [{"text": "Improving Hidden Representation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.8140111366907755}, {"text": "Neural Machine Translation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7265103061993917}]}], "abstractContent": [{"text": "Multilayer architectures are currently the gold standard for large-scale neural machine translation.", "labels": [], "entities": [{"text": "large-scale neural machine translation", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.6203869506716728}]}, {"text": "Existing works have explored some methods for understanding the hidden representations , however, they have not sought to improve the translation quality rationally according to their understanding.", "labels": [], "entities": []}, {"text": "Towards understanding for performance improvement, we first artificially construct a sequence of nested relative tasks and measure the feature generalization ability of the learned hidden representation over these tasks.", "labels": [], "entities": []}, {"text": "Based on our understanding, we then propose to regularize the layer-wise representations with all tree-induced tasks.", "labels": [], "entities": []}, {"text": "To overcome the computational bottleneck resulting from the large number of regularization terms, we design efficient approximation methods by selecting a few coarse-to-fine tasks for regularization.", "labels": [], "entities": []}, {"text": "Extensive experiments on two widely-used datasets demonstrate the proposed methods only lead to small extra overheads in training but no additional overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9973948001861572}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has witnessed great successes in recent years (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8797937631607056}]}, {"text": "Current state-of-the-art (SOTA) NMT models are mainly constructed by a stacked neural architecture consisting of multiple hidden layers from bottom-up, where a classifier is built upon the topmost layer to solve the target task of translation (.", "labels": [], "entities": []}, {"text": "Most works tend to focus on the translation performance of the classifier defined on the topmost layer, however, they do not deeply understand the learned representations of hidden layers. and attempt * Conghui Zhu is the corresponding author.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9594404101371765}]}, {"text": "to understand the hidden representations through the lens of a few linguistic tasks, while and propose appealing visualization approaches to understand NMT models including the representation of hidden layers.", "labels": [], "entities": []}, {"text": "However, employing the analyses to motivate new methods for better translation, the ultimate goal of understanding NMT, is not achieved in these works.", "labels": [], "entities": [{"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9605259299278259}]}, {"text": "In our paper, we aim at understanding the hidden representation of NMT from an alternative viewpoint, and particularly we propose simple yet effective methods to improve the translation performance based on our understanding.", "labels": [], "entities": [{"text": "translation", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.9571052193641663}]}, {"text": "We start from a fundamental question: what are the characteristics of the hidden representation for better translation modeling?", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.9555768966674805}]}, {"text": "Inspired by the lessons from transfer learning (), we propose to empirically verify the argument: good hidden representation fora target task should be able to generalize well across any similar tasks.", "labels": [], "entities": []}, {"text": "Unlike and who employ one or two linguistic tasks involving human annotated data to evaluate the feature generalization ability of the hidden representation, which might make understanding bias to a specific task, we instead construct a nested sequence of many relative tasks with entailment structure induced by a hierarchical clustering tree over the output label space (target vocabulary).", "labels": [], "entities": []}, {"text": "Each task is defined as predicting the cluster of the next token according to a given source sentence and its translation prefix.", "labels": [], "entities": []}, {"text": "Similar to, and, we measure the feature generalization ability of the hidden representation regarding each task.", "labels": [], "entities": []}, {"text": "Our observations are ( \u00a72): \u2022 The hidden representations learned by NMT indeed has decent feature generalization ability for the tree-induced relative tasks compared to the randomly initialized NMT model and a strong baseline with lexical features.", "labels": [], "entities": []}, {"text": "\u2022 The hidden representations from the higher layers generalize better across tasks than those from the lower layers.", "labels": [], "entities": []}, {"text": "And more similar tasks have closer performances.", "labels": [], "entities": []}, {"text": "Based on the above findings, we decide to regularize and improve the hidden representations of NMT for better predictive performances regarding those relative tasks, in hope of achieving improved performance in terms of the target translation task.", "labels": [], "entities": []}, {"text": "One natural solution is to feed all relative tasks to every hidden layer of the NMT decoder under the framework of multi-task learning.", "labels": [], "entities": []}, {"text": "This may make the full coverage of the potential regularization effect.", "labels": [], "entities": []}, {"text": "Unfortunately, this vanilla method is inefficient in training because there are more than one hundred task-layer combinations.", "labels": [], "entities": []}, {"text": "1 Based on the second finding, to approximate the vanilla method, we instead feed a single relative task to each hidden layer as a regularization auxiliary in a coarseto-fine manner ( \u00a73.1).", "labels": [], "entities": []}, {"text": "Furthermore, we design another regularization criterion to encourage predictive decision consistency between a pair of adjacent hidden layers, which leads to better approximated regularization effect ( \u00a73.2).", "labels": [], "entities": []}, {"text": "Our method is simple to implement and efficient for training and testing.", "labels": [], "entities": []}, {"text": "illustrates the representation regularization framework.", "labels": [], "entities": []}, {"text": "To summarize, our contributions are as follows: \u2022 We propose an approach to understand hidden representation of multilayer NMT by There are about 22 tasks that we have constructed and 6 layers in SOTA NMT models measuring their feature generalization ability across relative tasks constructed by a hierarchical clustering tree.", "labels": [], "entities": []}, {"text": "\u2022 We propose two simple yet effective methods to regularize the hidden representation.", "labels": [], "entities": []}, {"text": "These two methods serve as trade-offs between regularization coverage and efficiency with respect to the tree-induced tasks.", "labels": [], "entities": []}, {"text": "\u2022 We conduct experiments on two widely used datasets and obtain consistent improvements (up to +1.3 BLEU) over the current SOTA Transformer () model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9972798824310303}]}], "datasetContent": [{"text": "In the following, we conduct several quantitative experiments to demonstrate the advantages of our proposed two regularization methods over the baseline.", "labels": [], "entities": []}, {"text": "Note that, since we need to guarantee that the decoded sequence has the same length with the reference for one-by-one token comparison, the following experiments are all conducted with teacher forcing and greedy decoding.", "labels": [], "entities": []}, {"text": "In this appendix, we describe the detailed experiment information: the construction of the hierar-  chical clustering tree (), the model configuration, and the training details.", "labels": [], "entities": []}, {"text": "We construct two hierarchical clustering trees for the two target languages, English (Zh\u21d2En) and German (En\u21d2De) in our experiments, with Percy Liang's Brown Clustering algorithm implementation.", "labels": [], "entities": []}, {"text": "In both languages, we set the number of clusters to 5000 (a hyperparameter in the algorithm, c = 5000), that is, we will obtain trees with 5000 leaves.", "labels": [], "entities": []}, {"text": "We set c to 5000 since the total vocabulary of the two languages are around 30k, and 5000 clusters will get a token coverage of 6 (30k/5000=6) for each leave if the clusters are balanced.", "labels": [], "entities": []}, {"text": "By using left-branching introduced in Appendix A, we can finally get two trees with every depths as a relative task.", "labels": [], "entities": []}, {"text": "The statistics of each relative task's cardinality for each tree are shown in(a) and (b) respectively.", "labels": [], "entities": []}, {"text": "For selecting the depths in a coarse-to-fine manner, we follow the heuristics mentioned in Section 3.1, that is, we select depths which are diverse enough so as to have better coverage of all the relative tasks.", "labels": [], "entities": []}, {"text": "Specifically, we follow a quotient between two adjacent selected tasks of 5, and select from the task which has the cardinality of 5000, then we select tasks of cardinalities around 1000, 200, 40 respectively.", "labels": [], "entities": []}, {"text": "For the Zh\u21d2En dataset, we select tasks at depths 5, 8, 11, 20 with cardinalities 32, 208, 955, 5000; for the En\u21d2De dataset, we select tasks at depths 5, 7, 10, 21 with cardinalities 32, 127, 878, 5000.", "labels": [], "entities": [{"text": "Zh\u21d2En dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.5736770033836365}, {"text": "En\u21d2De dataset", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.5294888392090797}]}, {"text": "The model configuration strictly follows that of the base model in with word embedding size of 512, hidden size of 512, feedforward projection size of 2048, layer and head number of 6 and 8.", "labels": [], "entities": []}, {"text": "The dropout rates of the embedding, attention and residual block are all set to 0.1.", "labels": [], "entities": [{"text": "dropout rates", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9595126509666443}]}, {"text": "All architectural settings are in accordance with the base model of.", "labels": [], "entities": []}, {"text": "We use Adam () as the default optimizer with an initial learning rate of 0.0001.", "labels": [], "entities": []}, {"text": "Training batch size is set to 8192 \u00d7 4 tokens per batch, that is, we use data parallelism with 4 P40 or M40 GPUs and 8192 tokens per GPU.", "labels": [], "entities": []}, {"text": "We train the Zh\u21d2En models from scratch for about 240k iterations about 2 weeks on 4 M40 GPUs for both the baseline and the 3 regularization variants.", "labels": [], "entities": []}, {"text": "For the En\u21d2De models, we first attempt to train all the methods from scratch up to 200k iterations, but do not see significant improvement on BLEU score (around 0.6 points on test).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9825258851051331}]}, {"text": "So we use the pretrained baseline to initialize our proposed methods, and further train them for about 200k iterations, which results in the reported improvement in Section 4.5.", "labels": [], "entities": []}, {"text": "As suggested by the reviewers, we conduct independent layer regularization by imposing a relative task with cardinality of 5000 on layer 2 to 5 with the six layer Transformer.", "labels": [], "entities": [{"text": "independent layer regularization", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.5745193262894949}]}, {"text": "The performances are demonstrated in.", "labels": [], "entities": []}, {"text": "It seems that independent layer regularization can as well brings about descent improvements over the baseline.", "labels": [], "entities": [{"text": "independent layer regularization", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.5611982544263204}]}, {"text": "And regularizing layer 2 of the decoder performs better than our HR method.", "labels": [], "entities": [{"text": "regularizing layer", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8869213163852692}]}, {"text": "It is surprising that lower layers are more urgent to be regularized than higher layers.", "labels": [], "entities": []}, {"text": "This phenomenon may raise the question that how on earth the intermediate layer representations help with final prediction.", "labels": [], "entities": [{"text": "final prediction", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.4887234717607498}]}, {"text": "One hypothesis maybe drawn from our paper is that: in baseline, the coherence among the layer-wise representations maybe weak so that some non-linear transformations from lower layer may not lead to essential predictive power of the final layer representation.", "labels": [], "entities": []}, {"text": "And by using KL divergence to externally constrain their decision consistency may take better advantage of lower layers.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7339192628860474}]}, {"text": "As one of the reviewer pointed out,) with base model's hyperparameters achieves 28.90 on WMT14 En\u21d2De newstest14, with 1/6 enc-dec parameters (not considering embeddings).", "labels": [], "entities": [{"text": "WMT14", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.823955774307251}]}, {"text": "Its architectural inductive bias is motivated from iterative refinement of the layer-wise representation so the decoder at each time step builds an RNN like reasoning process to refine upon previous layer's representation.", "labels": [], "entities": []}, {"text": "We think this might be a more effective inductive bias in ResNet architecture which is adopted by Transformer, since provides evidence that ResNet does iterative representation inference (residual as refined quantity).", "labels": [], "entities": []}, {"text": "Future directions may include relating the dimension reduced representations ( to the coarse-to-fine structural bias, or experiments on Universal Transformer architecture to probe its learned representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Train and test efficiency comparison, mea- sured by steps/s (second) and sentences/s respectively.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.8433262705802917}]}, {"text": " Table 2: BLEU comparison on the LDC dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986366629600525}, {"text": "LDC dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9570635557174683}]}, {"text": " Table 3: BLEU comparison on the WMT14 dataset.  Here MT13 and MT14 denote newstest2013 and new- stest2014, which are used as development and test set  respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983123540878296}, {"text": "WMT14 dataset", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9804428815841675}, {"text": "MT13", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.6988552808761597}, {"text": "MT14", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7106333374977112}]}, {"text": " Table 4: BLEU comparison on the LDC dataset with independently regularized layers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998138427734375}, {"text": "LDC dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9074545800685883}]}, {"text": " Table 4. It seems  that independent layer regularization can as well  brings about descent improvements over the base- line. And regularizing layer 2 of the decoder per- forms better than our HR method. It is surpris-", "labels": [], "entities": []}]}