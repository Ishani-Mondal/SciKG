{"title": [{"text": "On Knowledge Distillation from Complex Networks for Response Prediction", "labels": [], "entities": [{"text": "Knowledge Distillation from Complex Networks", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.8186560869216919}, {"text": "Response Prediction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8591679632663727}]}], "abstractContent": [{"text": "Recent advances in Question Answering have lead to the development of very complex models which compute rich representations for query and documents by capturing all pair-wise interactions between query and document words.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8048877120018005}]}, {"text": "This makes these models expensive in space and time, and in practice one has to restrict the length of the documents that can be fed to these models.", "labels": [], "entities": []}, {"text": "Such models have also been recently employed for the task of predicting dialog responses from available background documents (e.g., Holl-E dataset).", "labels": [], "entities": [{"text": "predicting dialog responses from available background documents", "start_pos": 61, "end_pos": 124, "type": "TASK", "confidence": 0.7901932426861354}, {"text": "Holl-E dataset", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.9256002306938171}]}, {"text": "However, here the documents are longer, thereby rendering these complex models infeasible except in select restricted settings.", "labels": [], "entities": []}, {"text": "In order to overcome this, we use standard simple models which do not capture all pairwise interactions, but learn to emulate certain characteristics of a complex teacher network.", "labels": [], "entities": []}, {"text": "Specifically, we first investigate the conicity of representations learned by a complex model and observe that it is significantly lower than that of simpler models.", "labels": [], "entities": []}, {"text": "Based on this insight, we modify the simple architecture to mimic this characteristic.", "labels": [], "entities": []}, {"text": "We go further by using knowledge distillation approaches , where the simple model acts as a student and learns to match the output from the complex teacher network.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7213400453329086}]}, {"text": "We experiment with the Holl-E dialog data set and show that by mimicking characteristics and matching outputs from a teacher, even a simple network can give improved performance.", "labels": [], "entities": [{"text": "Holl-E dialog data set", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.8977564424276352}]}], "introductionContent": [{"text": "The advent of large scale datasets for QA has lead to the development of increasing complex neural models with specialized components for (i) encoding the query (ii) encoding the document(s) (iii) capturing interactions between document and query words and (iv) generating/extracting the correct answer span from the given document (.", "labels": [], "entities": []}, {"text": "While these models give state-of-the-art performance on a variety of datasets, they have very high space and time complexity.", "labels": [], "entities": []}, {"text": "This is a concern, and in practice, it is often the case that one has to resort to restricting the maximum length of the input document such that the model can run with reasonable resources (say, a single 12GB Tesla K80 GPU).", "labels": [], "entities": []}, {"text": "Such complex span prediction models are also being adapted for other NLP tasks such as dialog response prediction (, which is the focus of this work.", "labels": [], "entities": [{"text": "dialog response prediction", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.8446733951568604}]}, {"text": "In particular, we refer to the Holl-E dataset where the task is to extract the next response from a document which is relevant to the conversation (see).", "labels": [], "entities": [{"text": "Holl-E dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9088371098041534}]}, {"text": "This setup is very similar to QA wherein the input is {context, document} as opposed to {query, document} and the correct response span needs to be extracted from the given document.", "labels": [], "entities": []}, {"text": "Given this similarity, it is natural to adopt existing QA models () for this task.", "labels": [], "entities": []}, {"text": "However, the documents in Holl-E dataset are longer, and the authors specifically report that they were unable to run these models when the entire document was given as input.", "labels": [], "entities": [{"text": "Holl-E dataset", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.9506027102470398}]}, {"text": "Hence, they report results only in constrained oracle settings where the document is trimmed such that the response still lies in the shortened document.", "labels": [], "entities": []}, {"text": "The above situation suggests that there is clearly a trade-off needed.", "labels": [], "entities": []}, {"text": "On one hand, we want to harness the power of these complex models to achieve better performance and on the other hand we want to be able to run them with reasonable compute resources without arbitrarily trimming the input document.", "labels": [], "entities": []}, {"text": "This trade-off situation naturally leads to the following question: Is it possible to build a simple model, with low memory and compute requirements, that copy desirable characteristics from complex models?", "labels": [], "entities": []}, {"text": "To answer this, we start with a relatively simple model with very basic components for encoding query, document and capturing interactions.", "labels": [], "entities": []}, {"text": "Once these interactions are cap-Source Doc: ...comes in.", "labels": [], "entities": []}, {"text": "As soon as the door is open, the Bride's fist crashes into Vernita's face.", "labels": [], "entities": []}, {"text": "A savage fight follows, first with fists, then with knives....", "labels": [], "entities": []}, {"text": "At this point Vernita is introduced as a member of the Deadly Vipers, codename Copperhead.", "labels": [], "entities": []}, {"text": "Sample Conversation: Prober (S1): Which is your favourite character in this?", "labels": [], "entities": [{"text": "Prober", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.8990402221679688}]}, {"text": "Responder (S2): My favorite character was Copperhead because she was kicking butt.", "labels": [], "entities": [{"text": "Copperhead", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9095327854156494}]}, {"text": "Prober (S3): Oh my goodness I agree, because the fight with Vernita was the best in the whole movie.", "labels": [], "entities": []}, {"text": "Responder (S4): It's starts off action packed because as soon as the door is open, the Bride's fist crashes into Vernita's face.", "labels": [], "entities": []}, {"text": "A savage fight follows, first with fists, then with knives.", "labels": [], "entities": []}, {"text": "Prober (S5): And it gets better when we find out they are both assassins.", "labels": [], "entities": [{"text": "Prober", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.946478545665741}]}, {"text": "Responder (S6): And a group of them, Vernita is introduced as a member of the Deadly Vipers, codename Copperhead.", "labels": [], "entities": []}, {"text": "tured, the model computes a final representation which is then fed to a decoder to predict the correct span in the document.", "labels": [], "entities": []}, {"text": "This recipe is very similar to), QANeT () but the main difference is that these models use much more complex encoder and interaction components to arrive at the final representation.", "labels": [], "entities": []}, {"text": "As expected, the performance of this model is poor when compared to BiDAF, QANeT.", "labels": [], "entities": []}, {"text": "The aim now is to improve the performance of this model by carefully analysing or learning from complex models.", "labels": [], "entities": []}, {"text": "Given that the complex model differs in the manner in which the final representation is computed, one hypothesis is that it learns richer final representations than the simple model.", "labels": [], "entities": []}, {"text": "Indeed, on investigation, we found that the final representations learned by complex models are diverse for different inputs (context, document pairs) as compared to the simple model.", "labels": [], "entities": []}, {"text": "Based on this insight, we propose a modification to the simple model which increases the diversity of the embeddings, thereby improving the performance.", "labels": [], "entities": []}, {"text": "While this insight obtained by manual investigation is useful, there is clearly scope for learning by exploring other characteristics of the model.", "labels": [], "entities": []}, {"text": "One principled way of doing this is to use knowledge distillation) where the simple model acts as a student and learns to mimic the probability distributions predicted by a teacher.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6863875091075897}]}, {"text": "In other words, instead of simply maximizing the log likelihood of the training data, the simple model now gets additional signals from the teacher which act as hints while training.", "labels": [], "entities": []}, {"text": "Our experiments, using the Holl-E dataset show that by (i) improving the conicity () of the representations learned by the simple model and (ii) mimicking the outputs of the complex teacher model the simple model can give improved performance with fewer compute and memory requirements.", "labels": [], "entities": [{"text": "Holl-E dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.900348961353302}]}, {"text": "In particular, when compared to a standalone simple model the student model shows an improvement of 3.4% (compare SAM-mul-train (LD) and SAM-add-topk (LD) entries in and respectively).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the setup used for our experiments and discuss the results.", "labels": [], "entities": []}, {"text": "We perform experiments using the Holl-E conversation dataset () which contains crowdsourced conversations from the movie domain.", "labels": [], "entities": [{"text": "Holl-E conversation dataset", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8964004119237264}]}, {"text": "Every conversation in this dataset is associated with background knowledge comprising of plot details (from Wikipedia), reviews and comments (from Reddit).", "labels": [], "entities": []}, {"text": "Every alternate utterance in the conversation is generated by copying and/or modifying sentences from this unstructured background knowledge.", "labels": [], "entities": []}, {"text": "We refer the reader again to fora sample from this dataset.", "labels": [], "entities": []}, {"text": "We use the same train, test and validation splits as provided by the authors of the original paper (.", "labels": [], "entities": []}, {"text": "For each chat in the training data, the authors construct training triplets of the form {document, context, response} where the number of train, test and validations triplets are 34486, 4388 and 4318 respectively.", "labels": [], "entities": []}, {"text": "The context contains (i) the query (the prober's most recent utterance) and (ii) the history (past 2 utterances by the prober and the responder) as described earlier.", "labels": [], "entities": []}, {"text": "The task then is to train a model which can predict the response given the document and the context.", "labels": [], "entities": []}, {"text": "At test time, the model is shown document, context and predicts the response.", "labels": [], "entities": []}, {"text": "As mentioned earlier, the authors of Holl-E found that BiDAF and QANeT run into memory issues when evaluated on their dataset.", "labels": [], "entities": []}, {"text": "Hence, they propose two setups (i) long document (LD) setup and (ii) short document (SD) setup.", "labels": [], "entities": []}, {"text": "In the long document setup, the authors do not trim the document from which the response needs to be predicted.", "labels": [], "entities": []}, {"text": "In the short document setup, the authors trim the document to 256 words such that the span containing the response is contained in the trimmed document.", "labels": [], "entities": []}, {"text": "This enables them to evaluate BiDAF and QANeT on the trimmed document.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.6798707842826843}, {"text": "QANeT", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.6773936152458191}]}, {"text": "We also report experiments using both the LD and SD setup.", "labels": [], "entities": []}, {"text": "As mentioned above complex models (BiDAF and QANeT) face memory issues on training set with long documents.", "labels": [], "entities": []}, {"text": "So for all situations where we need predictions from complex models for long documents, we use a BiDAF/QANeT model trained on short document examples, and the prediction on the long document is made by splitting the long documents into chunks and feeding it to the trained BiDAF/QANeT model.", "labels": [], "entities": []}, {"text": "The final predicted span is the largest scoring span across all chunks.", "labels": [], "entities": []}, {"text": "For all models, we considered the following hyperparameters and tuned them using the validation set.", "labels": [], "entities": []}, {"text": "We tried batch sizes of 32 and 64 and the following GRU sizes: 64, 100, 128.", "labels": [], "entities": []}, {"text": "We experimented with 1, 2 and 3 layers of GRU.", "labels": [], "entities": []}, {"text": "We used pre-trained publicly available Glove word embeddings 1 of 100 dimensions.", "labels": [], "entities": []}, {"text": "The best performance SAM, SD SAM, LD BiDAF, SD BiDAF, LD QANeT, SD QANeT, LD Memory 540MB 1   was with the batch size of 32, 2 layers of GRU with hidden size 64.", "labels": [], "entities": []}, {"text": "We used Adam () optimizer with initial learning rate set to 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999.", "labels": [], "entities": []}, {"text": "We performed L2 weight decay with decay rate set to 0.001.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance (F1 Scores) of  different baseline models on SD (short  document) and LD (long document) test  set.", "labels": [], "entities": [{"text": "F1 Scores)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9728963176409403}]}, {"text": " Table 3: F1 Scores for different variants of simple  attention model on long documents test set.", "labels": [], "entities": [{"text": "F1 Scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9821555316448212}, {"text": "long documents test set", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.7232535779476166}]}, {"text": " Table 4: F1 Scores for different variants of simple  attention model on short document test set.", "labels": [], "entities": [{"text": "F1 Scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9826906323432922}, {"text": "short document test set", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.6838772222399712}]}, {"text": " Table 5: Comparison of parameters (in Million) and  training time (seconds) per epoch for different models.", "labels": [], "entities": [{"text": "training time (seconds)", "start_pos": 53, "end_pos": 76, "type": "METRIC", "confidence": 0.9275251269340515}]}, {"text": " Table 6: Comparison of conicity between variants of  simple-attention model and complex models.", "labels": [], "entities": []}]}