{"title": [{"text": "AUTOSEM: Automatic Task Selection and Mixing in Multi-Task Learning", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.4698579013347626}, {"text": "Automatic Task Selection and Mixing", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.6269014656543732}]}], "abstractContent": [{"text": "Multi-task learning (MTL) has achieved success over a wide range of problems, where the goal is to improve the performance of a primary task using a set of relevant auxiliary tasks.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8819113612174988}]}, {"text": "However, when the usefulness of the auxiliary tasks w.r.t. the primary task is not known a priori, the success of MTL models depends on the correct choice of these auxiliary tasks and also a balanced mixing ratio of these tasks during alternate training.", "labels": [], "entities": [{"text": "MTL", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9304121732711792}]}, {"text": "These two problems could be resolved via manual intuition or hyper-parameter tuning overall com-binatorial task choices, but this introduces in-ductive bias or is not scalable when the number of candidate auxiliary tasks is very large.", "labels": [], "entities": []}, {"text": "To address these issues, we present AUTOSEM, a two-stage MTL pipeline, where the first stage automatically selects the most useful auxiliary tasks via a Beta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage learns the training mixing ratio of these selected auxiliary tasks via a Gaussian Process based Bayesian optimization framework.", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8430818319320679}]}, {"text": "We conduct several MTL experiments on the GLUE language understanding tasks, and show that our AUTOSEM framework can successfully find relevant auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on several primary tasks.", "labels": [], "entities": [{"text": "GLUE language understanding tasks", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.6921115964651108}]}, {"text": "Finally, we present ablations for each stage of AUTOSEM and analyze the learned auxiliary task choices.", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.8405280709266663}]}], "introductionContent": [{"text": "Multi-task Learning (MTL)) is an inductive transfer mechanism which leverages information from related tasks to improve the primary model's generalization performance.", "labels": [], "entities": [{"text": "Multi-task Learning (MTL))", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7529103577136993}]}, {"text": "It achieves this goal by training multiple tasks in parallel while sharing representations, where the training signals from the auxiliary tasks can help improve the performance of the primary task.", "labels": [], "entities": []}, {"text": "Multi-task learning has been applied to a wide range of natural language processing problems (.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.790842205286026}]}, {"text": "Despite its impressive performance, the design of a multitask learning system is non-trivial.", "labels": [], "entities": []}, {"text": "In the context of improving the primary task's performance using knowledge from other auxiliary tasks (), two major challenges include selecting the most relevant auxiliary tasks and also learning the balanced mixing ratio for synergized training of these tasks.", "labels": [], "entities": []}, {"text": "One can achieve this via manual intuition or hyper-parameter tuning overall combinatorial task choices, but this introduces human inductive bias or is not scalable when the number of candidate auxiliary tasks is considerable.", "labels": [], "entities": []}, {"text": "To this end, we present AUTOSEM, a two-stage Bayesian optimization pipeline to this problem.", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8292127251625061}]}, {"text": "In our AUTOSEM framework 1 , the first stage addresses automatic task selection from a pool of auxiliary tasks.", "labels": [], "entities": []}, {"text": "For this, we use a non-stationary multi-armed bandit controller (MAB)) that dynamically alternates among task choices within the training loop, and eventually returns estimates of the utility of each task w.r.t. the primary task.", "labels": [], "entities": []}, {"text": "We model the utility of each task as a Beta distribution, whose expected value can be interpreted as the probability of each task making a non-negative contribution to the training performance of the primary task.", "labels": [], "entities": []}, {"text": "Further, we model the observations as Bernoulli variables so that the posterior distribution is also Beta-distributed.", "labels": [], "entities": []}, {"text": "We use Thompson sampling () to trade off exploitation and exploration.", "labels": [], "entities": []}, {"text": "The second stage then takes the auxiliary tasks selected in the first stage and automatically learns the training mixing ratio of these tasks, through the framework of Bayesian optimization, by modeling the performance of each mixing ratio as a sample from a Gaussian Process (GP) to sequentially search for the optimal values).", "labels": [], "entities": []}, {"text": "For the covariance function in the GP, we use the Matern kernel which is parameterized by a smoothness hyperparameter so as to control the level of differentiability of the samples from GP.", "labels": [], "entities": []}, {"text": "Further, following, we use a portfolio of optimistic and improvementbased policies as acquisition functions for selecting the next sample point from the GP search space.", "labels": [], "entities": []}, {"text": "We conduct several experiments on the GLUE natural language understanding benchmark (, where we choose each of RTE, MRPC, QNLI, CoLA, and SST-2 as the primary task, and treat the rest of the classification tasks from the GLUE benchmark as candidate auxiliary tasks.", "labels": [], "entities": [{"text": "GLUE natural language understanding", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.6140941604971886}, {"text": "RTE", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.6495247483253479}, {"text": "MRPC", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.542772114276886}, {"text": "GLUE benchmark", "start_pos": 221, "end_pos": 235, "type": "DATASET", "confidence": 0.9142106771469116}]}, {"text": "Results show that our AUTOSEM framework can successfully find useful auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on top of strong baselines for several primary tasks, e.g., 5.2% improvement on QNLI, 4.7% improvement on RTE, and 2.8%/0.8% improvement on MRPC.", "labels": [], "entities": [{"text": "QNLI", "start_pos": 250, "end_pos": 254, "type": "DATASET", "confidence": 0.7779794931411743}, {"text": "MRPC", "start_pos": 310, "end_pos": 314, "type": "DATASET", "confidence": 0.7368859648704529}]}, {"text": "We also ablate the usefulness of our two stages of auxiliary task selection and automatic mixing ratio learning.", "labels": [], "entities": [{"text": "auxiliary task selection", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6460679272810618}]}, {"text": "The first ablation removes the task selection stage and instead directly performs the second GP mixing ratio learning stage on all auxiliary tasks.", "labels": [], "entities": [{"text": "GP mixing ratio learning stage", "start_pos": 93, "end_pos": 123, "type": "METRIC", "confidence": 0.8962221503257751}]}, {"text": "The second ablation performs the task selection stage (with multi-armed bandit) but replaces the second stage Gaussian Process with manual tuning on the selected tasks.", "labels": [], "entities": [{"text": "task selection", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7145039737224579}]}, {"text": "Our 2-stage model performs better than both these ablations, showing that both of our stages are crucial.", "labels": [], "entities": []}, {"text": "Further, we also discuss the learned auxiliary task choices in terms of their intuitive relevance w.r.t. the corresponding primary task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: We evaluate our models on several datasets from the GLUE benchmark (: RTE, QNLI, MRPC, SST-2, and CoLA.", "labels": [], "entities": [{"text": "GLUE benchmark", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.737869918346405}, {"text": "RTE", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6507617831230164}, {"text": "QNLI", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.745101273059845}, {"text": "MRPC", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.6446544528007507}]}, {"text": "For all these datasets, we use the standard splits provided by: Test GLUE results of previous work, our baseline, and our AUTOSEM MTL framework.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9269137382507324}, {"text": "AUTOSEM MTL framework", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.7472194234530131}]}, {"text": "We report accuracy and F1 for MRPC, Matthews correlation for CoLA, and accuracy for all others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999821126461029}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997567534446716}, {"text": "MRPC", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.45535895228385925}, {"text": "Matthews correlation", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.9662615954875946}, {"text": "CoLA", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.7087149024009705}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.999663233757019}]}, {"text": "Training Details: We use pre-trained ELMo 4 to obtain sentence representations as inputs to our model (, and the Gaussian Process implementation is based on Scikit-Optimize 5 , and we adopt most of the default configurations.", "labels": [], "entities": []}, {"text": "We use accuracy as the validation criterion for all tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9993688464164734}]}, {"text": "For all of our experiments except QNLI and SST-2, we apply early stopping on the validation performance plateau.", "labels": [], "entities": [{"text": "QNLI", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9249080419540405}, {"text": "early stopping", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.9380897879600525}, {"text": "validation", "start_pos": 81, "end_pos": 91, "type": "TASK", "confidence": 0.9709621667861938}]}, {"text": "The set of candidate auxiliary tasks consists of all 2-sentence classification tasks when the primary task is a classification of two sentences, whereas it consists of all two-sentence and single-sentence classification tasks when the primary task is a classification of a single sentence.", "labels": [], "entities": []}, {"text": "Since the utility estimates from the multi-armed bandit controller are noisy, we choose the top two tasks based on expected task utility estimates, and include additional tasks if their utility estimate is above 0.5.", "labels": [], "entities": []}, {"text": "All the results reported are the aggregate of the same experiment with two runs (with different random seeds) unless explicitly mentioned.", "labels": [], "entities": []}, {"text": "We use a two-layer LSTM-RNN with hidden size of 1024 for RTE and 512 for the rest of the models, and use Adam Optimizer ().", "labels": [], "entities": []}, {"text": "The prior parameters of each task in stage-1 are set to be \u03b1 0 = 1, \u03b2 0 = 1, which are commonly used in other literature.", "labels": [], "entities": []}, {"text": "For stage-1, the bandit controller iteratively selects batches of data from different tasks during training to learn the approximate importance of each auxiliary task ().", "labels": [], "entities": []}, {"text": "In stage-2 (Gaussian Process), we sequentially draw samples of mixing ratios and evaluate each sample after full training (.", "labels": [], "entities": []}, {"text": "Without much tuning, we used approximately 200 rounds for the stage-1 bandit-based approach, where each round consist of approximately 10 mini-batches of optimization.", "labels": [], "entities": []}, {"text": "For stage-2, we experimented with 15 and 20 as the number of samples to draw and found that 15 samples for MRPC and 20 samples for the rest of the tasks work well.", "labels": [], "entities": [{"text": "MRPC", "start_pos": 107, "end_pos": 111, "type": "TASK", "confidence": 0.5292220115661621}]}, {"text": "This brings the total computational cost for our two-stage pipeline to be approximately (15+1)x and (20+1)x, where x represents the time taken to run the baseline model for the given task.", "labels": [], "entities": []}, {"text": "This is significantly more efficient than a grid-search based manually-tuned mixing ratio setup (which would scale exponentially with the number of tasks).", "labels": [], "entities": []}, {"text": "shows the results of our baseline and previous works ().", "labels": [], "entities": []}, {"text": "We can see that our single-task baseline models achieve stronger performance on almost all tasks in comparison to previous work's single-task models.", "labels": [], "entities": []}, {"text": "Next, we present the performance of our AUTOSEM framework on top of these strong baselines.", "labels": [], "entities": [{"text": "AUTOSEM framework", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.6877017766237259}]}, {"text": "also presents the performance of our AU-TOSEM framework-based MTL models.", "labels": [], "entities": [{"text": "AU-TOSEM framework-based MTL", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.7769876917203268}]}, {"text": "As can be seen, our MTL models improve significantly (see for standard deviations) upon their corresponding single-task baselines for all tasks, and achieve strong improvements as compared to the fairly-comparable 9 multi-task results of previous work ().", "labels": [], "entities": [{"text": "MTL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9417446255683899}]}, {"text": "10 During the task Note that we do not report previous works which finetune large external language models for the task (e.g., OpenAI-GPT and BERT), because they are not fairly comparable w.r.t. our models.", "labels": [], "entities": [{"text": "OpenAI-GPT", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.921337366104126}, {"text": "BERT", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9768713116645813}]}, {"text": "Similarly, we report the nonattention based best GLUE models (i.e., BiLSTM+ELMo) fora fair comparison to our non-attention baseline.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.8668003678321838}, {"text": "BiLSTM+ELMo", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.6564272244771322}]}, {"text": "Our approach should ideally scale to large pre-training/fine-tuning models like BERT, given appropriate compute resources.", "labels": [], "entities": [{"text": "BERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9603135585784912}]}, {"text": "Note that even though the performance improvement gaps of (MTL vs. baseline) and our improvements (AUTOSEM vs. our improved baseline) are similar, these are inherently two different setups.", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9249799251556396}]}, {"text": "MTL is based on a 'one model for all' setup (, whereas our approach in-selection stage of our AUTOSEM framework, we observe that MultiNLI is chosen as one of the auxiliary tasks in all of our MTL models.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5693872570991516}, {"text": "AUTOSEM framework", "start_pos": 94, "end_pos": 111, "type": "DATASET", "confidence": 0.9011272192001343}]}, {"text": "This is intuitive given that MultiNLI contains multiple genres covering diverse aspects of the complexity of language (.", "labels": [], "entities": []}, {"text": "Also, we observe that WNLI is sometimes chosen in the task selection stage; however, it is always dropped (mixing ratio of zero) by the Gaussian Process controller, showing that it is not beneficial to use WNLI as an auxiliary task (intuitive, given its small size).", "labels": [], "entities": []}, {"text": "Next, we discuss the improvements on each of the primary tasks and the corresponding auxiliary tasks selected by AUTOSEM framework.", "labels": [], "entities": [{"text": "AUTOSEM framework", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.8683039546012878}]}, {"text": "RTE: Our AUTOSEM approach achieves stronger results w.r.t. the baseline on RTE (58.7 vs. 54.0).", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4990360736846924}, {"text": "AUTOSEM", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.7256895899772644}, {"text": "RTE", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.6065444946289062}]}, {"text": "During our task selection stage, we found out that QQP and MultiNLI tasks are important for RTE as auxiliary tasks.", "labels": [], "entities": [{"text": "QQP", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8604822158813477}, {"text": "RTE", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9470357894897461}]}, {"text": "For the second stage of automatic mixing ratio learning via Gaussian Process, the model learns that a mixing ratio of 1:5:5 works best to improve the primary task (RTE) using related auxiliary tasks of QQP and MultiNLI.", "labels": [], "entities": []}, {"text": "MRPC: AUTOSEM here performs much better than the baseline on MRPC (78.5/84.5 vs. 75.7/83.7).", "labels": [], "entities": [{"text": "MRPC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9467300772666931}, {"text": "AUTOSEM", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.961133599281311}, {"text": "MRPC", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.874625027179718}]}, {"text": "During our task selection stage, we found out that RTE and MultiNLI tasks are important for MRPC as auxiliary tasks.", "labels": [], "entities": [{"text": "MRPC", "start_pos": 92, "end_pos": 96, "type": "TASK", "confidence": 0.9635154008865356}]}, {"text": "In the second stage, AUTOSEM learned a mixing ratio of 9:1:4 for these three tasks (MRPC:RTE:MultiNLI).", "labels": [], "entities": [{"text": "AUTOSEM", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.5876829028129578}, {"text": "MRPC:RTE:MultiNLI", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.6091938972473144}]}, {"text": "QNLI: Again, we achieve substantial improvements with AUTOSEM w.r.t. baseline on QNLI (79.2 vs. 74.0).", "labels": [], "entities": [{"text": "QNLI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9672414064407349}, {"text": "AUTOSEM", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.8341894149780273}, {"text": "QNLI", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9336153864860535}]}, {"text": "Our task selection stage learned that WNLI and MultiNLI tasks are best as auxiliary tasks for QNLI.", "labels": [], "entities": [{"text": "WNLI", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8045058846473694}, {"text": "MultiNLI", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.7617508769035339}]}, {"text": "We found that the Gaussian Process further drops WNLI by setting its mixing ratio to zero, and returns 20:0:5 as the best mixing ratio for QNLI:WNLI:MultiNLI.", "labels": [], "entities": [{"text": "WNLI", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8404867053031921}, {"text": "MultiNLI", "start_pos": 149, "end_pos": 157, "type": "DATASET", "confidence": 0.5250030159950256}]}, {"text": "CoLA: We also observe a strong performance improvement on CoLA with our AUTOSEM model w.r.t. our baseline (32.9 vs. 30.8).", "labels": [], "entities": [{"text": "CoLA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7896999716758728}, {"text": "AUTOSEM", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.82576984167099}]}, {"text": "During our task selection stage, we found out that MultiNLI and WNLI tasks are important for CoLA as auxiliary tasks.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.865215003490448}, {"text": "WNLI", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7112272381782532}, {"text": "CoLA", "start_pos": 93, "end_pos": 97, "type": "TASK", "confidence": 0.9260204434394836}]}, {"text": "In the second stage, GP learns to drop WNLI, and found the mixing ratio of 20:5:0 for CoLA:MultiNLI:WNLI.", "labels": [], "entities": [{"text": "WNLI", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.802217423915863}, {"text": "MultiNLI:WNLI", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.625884473323822}]}, {"text": "SST-2: Here also our AUTOSEM approach performs better than the baseline (91.8 vs. 91.3).", "labels": [], "entities": [{"text": "SST-2", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.453816682100296}, {"text": "AUTOSEM", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.5268921256065369}]}, {"text": "The task selection stage chooses MultiNLI, MRPC, terpretably chooses the 2-3 tasks that are most beneficial for the given primary task.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.7983759045600891}, {"text": "MRPC", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.6323322653770447}]}, {"text": "4 for comparison of training speeds for these two setups.  and WNLI as auxiliary tasks and the stage-2 Gaussian Process model drops MRPC and WNLI by setting their mixing ratio to zero (learns ratio of 13:5:0:0 for SST-2:MultiNLI:MRPC:WNLI).", "labels": [], "entities": [{"text": "WNLI", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9421980977058411}, {"text": "WNLI", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.864301860332489}, {"text": "WNLI", "start_pos": 234, "end_pos": 238, "type": "DATASET", "confidence": 0.7829618453979492}]}], "tableCaptions": [{"text": " Table 1: Test GLUE results of previous work, our baseline, and our AUTOSEM MTL framework. We report  accuracy and F1 for MRPC, Matthews correlation for CoLA, and accuracy for all others.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9942266941070557}, {"text": "AUTOSEM MTL framework", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.6717643439769745}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9996753931045532}, {"text": "F1", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9995260238647461}, {"text": "MRPC", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.4571400582790375}, {"text": "Matthews correlation", "start_pos": 128, "end_pos": 148, "type": "METRIC", "confidence": 0.95535808801651}, {"text": "CoLA", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.7025024890899658}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9994357228279114}]}, {"text": " Table 2: Ablation results on the two stages of our AU- TOSEM framework on MRPC.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9853731989860535}, {"text": "AU- TOSEM framework", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.7317829877138138}, {"text": "MRPC", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.5696306228637695}]}, {"text": " Table 3: Validation-set performance mean and standard  deviation (based on three runs) of our baselines and  Multi-task models in accuracy.", "labels": [], "entities": [{"text": "Validation-set performance mean", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.7904506127039591}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9995482563972473}]}]}