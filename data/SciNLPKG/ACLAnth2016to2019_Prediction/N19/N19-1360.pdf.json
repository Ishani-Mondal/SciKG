{"title": [{"text": "Context-Dependent Semantic Parsing over Temporally Structured Data", "labels": [], "entities": [{"text": "Context-Dependent Semantic Parsing", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6992286741733551}]}], "abstractContent": [{"text": "We describe anew semantic parsing setting that allows users to query the system using both natural language questions and actions within a graphical user interface.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.748503565788269}]}, {"text": "Multiple time series belonging to an entity of interest are stored in a database and the user interacts with the system to obtain a better understanding of the entity's state and behavior, entailing sequences of actions and questions whose answers may depend on previous factual or nav-igational interactions.", "labels": [], "entities": []}, {"text": "We design an LSTM-based encoder-decoder architecture that models context dependency through copying mechanisms and multiple levels of attention over inputs and previous outputs.", "labels": [], "entities": []}, {"text": "When trained to predict tokens using supervised learning, the proposed architecture substantially outper-forms standard sequence generation baselines.", "labels": [], "entities": []}, {"text": "Training the architecture using policy gradient leads to further improvements in performance, reaching a sequence-level accuracy of 88.7% on artificial data and 74.8% on real data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8804823160171509}]}], "introductionContent": [], "datasetContent": [{"text": "To train and evaluate semantic parsing approaches, we created two datasets of sequential interactions: a dataset of real interactions (Section 3.1) and a much larger dataset of artificial interactions (Section 3.2).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7320001423358917}]}, {"text": "All models are implemented in Tensorflow using dropout to deal with overfitting.", "labels": [], "entities": []}, {"text": "For both datasets, 10% of the data is put aside for validation.", "labels": [], "entities": []}, {"text": "After tuning on the artificial validation data, the feedforward neural networks dropout rate was set to 0.5 and the LSTM units dropout rate was set to 0.3.", "labels": [], "entities": [{"text": "LSTM units dropout rate", "start_pos": 116, "end_pos": 139, "type": "METRIC", "confidence": 0.7767142802476883}]}, {"text": "The word embeddings had dimensionality of 64 and were initialized at random.", "labels": [], "entities": []}, {"text": "Optimization is performed with the Adam algorithm.", "labels": [], "entities": []}, {"text": "For each dataset, we use five-fold cross evaluation, where the data is partitioned into five folds, one fold is used for testing and the other folds for training.", "labels": [], "entities": []}, {"text": "The process is repeated five times to obtain test results on all folds.", "labels": [], "entities": []}, {"text": "We use an early-stop strategy on the validation set.", "labels": [], "entities": []}, {"text": "The number of gradient updates is typically more than 20,000.", "labels": [], "entities": []}, {"text": "All the experiments are performed on a single NVIDIA GTX1080 GPU.", "labels": [], "entities": [{"text": "NVIDIA GTX1080 GPU", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7776280641555786}]}, {"text": "The models are trained and evaluated on the artificial interactions first.", "labels": [], "entities": []}, {"text": "To evaluate on real interactions, the models are pre-trained on the entire artificial dataset and then fine-tuned using real interactions.", "labels": [], "entities": []}, {"text": "SPAAC-RL is pre-trained with MLE loss to provide more efficient policy exploration.", "labels": [], "entities": [{"text": "SPAAC-RL", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8614538311958313}, {"text": "MLE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9304445385932922}, {"text": "policy exploration", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7190826088190079}]}, {"text": "We use sequence level accuracy as evaluation metric for all models: a generated sequence is considered correct if and only if all the generated tokens match the ground truth tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.6742753386497498}]}, {"text": "We report experimental evaluations of the proposed models SPAAC-MLE and SPAAC-RL and baseline models SeqGen, SeqGen+Att2In on the", "labels": [], "entities": [{"text": "SPAAC-MLE", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9090439677238464}, {"text": "SPAAC-RL", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.8897995948791504}]}], "tableCaptions": [{"text": " Table 3: Sequence-level accuracy on the 2 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9864474534988403}]}]}