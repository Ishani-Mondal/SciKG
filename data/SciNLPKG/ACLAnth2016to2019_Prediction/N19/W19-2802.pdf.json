{"title": [{"text": "Neural Coreference Resolution with Limited Lexical Context and Explicit Mention Detection for Oral French", "labels": [], "entities": [{"text": "Neural Coreference Resolution", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8701427578926086}]}], "abstractContent": [{"text": "We propose an end-to-end coreference resolution system obtained by adapting neural models that have recently improved the state-of-the-art on the OntoNotes benchmark to make them applicable to other paradigms for this task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8883298337459564}, {"text": "OntoNotes benchmark", "start_pos": 146, "end_pos": 165, "type": "DATASET", "confidence": 0.8538409173488617}]}, {"text": "We report the performances of our system on ANCOR, a corpus of transcribed oral French-for which it constitutes anew baseline with proper evaluation.", "labels": [], "entities": [{"text": "ANCOR", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.5423523187637329}]}], "introductionContent": [{"text": "In the last few years, coreference resolution systems based on artificial neural networks architectures have received much attention by tremendously improving upon the previous state-of-the-art.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9589749276638031}]}, {"text": "In particular, the system introduced by K. and refined in (K.) have proved that relatively high scores could be achieved without relying on rich features and preprocessing pipelines.", "labels": [], "entities": []}, {"text": "However, these results were obtained in the paradigm of the CoNLL-2012 shared task) and it is not self-evident that they are generalisable to other datasets, other domains and other languages.", "labels": [], "entities": []}, {"text": "For instance, the choice in to not include singleton mentions in the CoNLL-2012 dataset is quite uncommon and might rightfully be suspected to affect the evaluation of coreference resolution architectures (see for instance the comparisons made by).", "labels": [], "entities": [{"text": "CoNLL-2012 dataset", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.9704296886920929}, {"text": "coreference resolution", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.9288640320301056}]}, {"text": "In this work, we present an adaptation of K.'s system (henceforth E2EC 1 ) to make it more suitable to other paradigms.", "labels": [], "entities": []}, {"text": "We evaluate our system on ANCOR () -a corpus of transcribed oral French.", "labels": [], "entities": [{"text": "ANCOR", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8080139756202698}]}], "datasetContent": [{"text": "Following the recommendations of Recasens (2010, p. 122) and Salmon-Alt et al.", "labels": [], "entities": []}, {"text": "(2004) we evaluate our system separately on the two subtasks that it performs.", "labels": [], "entities": []}, {"text": "For mention detection, we report the usual Precision, Recall and F-score detection metrics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9982519745826721}, {"text": "Recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9468385577201843}, {"text": "F-score detection", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.9007098078727722}]}, {"text": "For coreference resolution, we use the CoNLL-2012 metrics () including BLANC.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9745298624038696}, {"text": "CoNLL-2012", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.8168765902519226}, {"text": "BLANC", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9648674726486206}]}, {"text": "This is a standard evaluation procedure for coreference resolution systems -as seen for example in the CRAC18 shared task ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9636704921722412}]}, {"text": "It also allows us to compare our system with other works on ANCOR ( and to assess the actual capabilities of our antecedent scoring module by avoiding the noise caused by the inevitable mention detection errors.", "labels": [], "entities": [{"text": "ANCOR", "start_pos": 60, "end_pos": 65, "type": "TASK", "confidence": 0.47873613238334656}]}], "tableCaptions": []}