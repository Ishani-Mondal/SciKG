{"title": [{"text": "Show Some Love to Your -grams: A Bit of Progress and Stronger -gram Language Modeling Baselines", "labels": [], "entities": []}], "abstractContent": [{"text": "In recent years neural language models (LMs) have set state-of-the-art performance for several benchmarking datasets.", "labels": [], "entities": []}, {"text": "While the reasons for their success and their computational demand are well-documented, a comparison between neural models and more recent developments in-gram models is neglected.", "labels": [], "entities": []}, {"text": "In this paper, we examine the recent progress in-gram literature, running experiments on 50 languages covering all morphological language families.", "labels": [], "entities": []}, {"text": "Experimental results illustrate that a simple extension of Modified Kneser-Ney outperforms an LSTM language model on 42 languages while a word-level Bayesian-gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018a) on 8 languages.", "labels": [], "entities": []}, {"text": "Further experiments on larger Eu-roparl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper-gram models:-gram training is up to 15, 000\u00d7 quicker.", "labels": [], "entities": []}, {"text": "Our experiments illustrate that standalone-gram models lend themselves as natural choices for resource-lean or morphologically rich languages , while the recent progress has significantly improved their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9974960684776306}]}], "introductionContent": [{"text": "Statistical language models (LMs) are the pivot for several natural language processing tasks where a model trained on a text corpus is required to assign a probability to a given sequence 1 2 ....", "labels": [], "entities": [{"text": "Statistical language models (LMs)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5987085203329722}]}, {"text": "This probability indicates how likely is for 1 to belong to the corpus and is decomposed into conditional probabilities of words given their preceding contexts as ( ).", "labels": [], "entities": []}, {"text": "Several smoothing techniques address the statistical sparsity issue for computing the conditional probabilities), while others avoided the above approximation with unbounded hierarchical nonparametric Bayesian frameworks.", "labels": [], "entities": []}, {"text": "Alternatively, neural LMs compute ( \u22121 1 ) via recurrent neural units which, in theory, are capable of encoding an unbounded context \u22121", "labels": [], "entities": []}], "datasetContent": [{"text": "As our main large-scale experiment we use a typologically diverse set of 50 languages.", "labels": [], "entities": []}, {"text": "These LM datasets cover many languages which are challenging in terms of data size, as well as the type-token ratio.", "labels": [], "entities": []}, {"text": "Ina less challenging setup, we experiment on 3 languages with larger training data from the Europarl () to compare the two classes of LMs based on perplexity reduction and training time.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.9865831732749939}]}, {"text": "For full data statistics, actual sampling of languages and data curation see.", "labels": [], "entities": []}, {"text": "The common practice of setting a frequency threshold and mapping training data unigrams (( = 1)-gram) to an UNK token degrades the performance of -gram LMs by discarding a range of discount parameters: e.g., using threshold (< 3) results in 1 (1), (1) = 0, both included in Eq. and Eq.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 108, "end_pos": 117, "type": "DATASET", "confidence": 0.9247047305107117}, {"text": "Eq.", "start_pos": 274, "end_pos": 277, "type": "DATASET", "confidence": 0.9255786538124084}, {"text": "Eq", "start_pos": 282, "end_pos": 284, "type": "DATASET", "confidence": 0.9288211464881897}]}, {"text": "(4), increases the average perplexity score of 5-gram KN in our experiments by 11%.", "labels": [], "entities": [{"text": "perplexity score", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.906373530626297}]}, {"text": "Motivated by its significance, we base our comparison on reported results by: they deal with the task in the full vocabulary setting) with word-level predictions, and follow a relatively comparable treatment of unseen words with both -gram and neural LM families (although not identical) attest time without enforcing any threshold over the training data.", "labels": [], "entities": []}, {"text": "The benchmarked neural LMs include three models with word-level predictions: a standard LSTM (), a Char-CNN-LSTM (denoted as CNN)) which incorporates character-level information in the input, and the Attract-Preserve model (denoted as AP)) which injects further subword-level information.", "labels": [], "entities": []}, {"text": "All benchmarked -gram LMs are 5-grams, with the exception of BKN which is an -gram model trained via 5 samples 4 following the recipe of . GKN results are based on = 5, tuned on a development set.", "labels": [], "entities": [{"text": "BKN", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.48362040519714355}]}, {"text": "The main results on the 50-languages benchmark are summarized in Table 2.", "labels": [], "entities": []}, {"text": "The results for the more recent -gram LMs indicate that these -gram models are highly competitive with neural LMs in this challenging resource-lean setup.", "labels": [], "entities": []}, {"text": "For instance, for 26/50 languages all -gram models outperform a regular LSTM, while GKN and BKN extend the lead to 42/50 Marginal improvements achieved with more sampling.", "labels": [], "entities": [{"text": "GKN", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8519442081451416}, {"text": "BKN", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8887939453125}, {"text": "Marginal", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8320341110229492}]}, {"text": "The size of each dataset is 40K sentences, which is at the level of the standard Penn Treebank dataset often used for LM evaluation in English (.   and 48/50 languages, respectively.", "labels": [], "entities": [{"text": "Penn Treebank dataset", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.9936444163322449}, {"text": "LM evaluation", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.8919301927089691}]}, {"text": "For certain morphologically rich languages (e.g., Tamil, Mongolian, Hebrew), the -gram LMs are able to outscore even character-aware neural LMs.", "labels": [], "entities": []}, {"text": "On average we observe -grams succeed, especially the BKN model, for introflexive and agglutinative languages which are known to have productive morphological systems: as reported in, they have the higher OOV ratio compared to the other language families.", "labels": [], "entities": [{"text": "BKN", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.5608422756195068}, {"text": "OOV ratio", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9896961450576782}]}, {"text": "Overall, the best performing -gram model, BKN, outperforms both the LSTM (42% reduction in perplexity) and CNN models (3% reduction in perplexity), while falling behind AP by 8%.", "labels": [], "entities": [{"text": "BKN", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6827433705329895}, {"text": "AP", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.8816261291503906}]}, {"text": "These results highlight that -gram models can serve as strong baselines for such morphologically rich languages with high OOV rates and type-totoken ratios, which are especially problematic in scarce data setups.", "labels": [], "entities": [{"text": "OOV rates", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9614441990852356}]}, {"text": "Additionally, they suggest that more sophisticated -gram variants such as GKN or BKN should be used to provide adequate comparison points with -gram LMs than the commonly used KN or MKN.", "labels": [], "entities": [{"text": "GKN", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.874714732170105}, {"text": "BKN", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.3575032651424408}]}, {"text": "As expected, experiments on 10\u00d7 larger Europarl datasets for 3 languages show that neural models outperform -gram models in less challenging data-intensive scenarios.", "labels": [], "entities": [{"text": "Europarl datasets", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9908480942249298}]}, {"text": "However, training on large datasets comes at the expense of training efficiency for neural models: e.g., according to training non-Bayesian -grams is around 15,000\u00d7 quicker than training neural models.", "labels": [], "entities": []}, {"text": "We leave a full-fledged investigation on the relation of training corpus size and efficacy of -gram vs. neural LM training for future work.", "labels": [], "entities": []}, {"text": "In addition, motivated by these preliminary insights, we advocate investing further efforts in future work into coupling the ideas behind -gram and neural LMs towards improved language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7098188996315002}]}], "tableCaptions": [{"text": " Table 2: Data Statistics and Perplexity scores. OOV de- notes the percentage of unseen words at test time. For  detailed data stats see (Gerz et al., 2018a). Suit symbols  denote morphological types: Isolating, Fusional,", "labels": [], "entities": [{"text": "OOV", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9695796370506287}]}]}