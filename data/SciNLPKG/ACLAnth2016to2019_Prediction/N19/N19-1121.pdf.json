{"title": [{"text": "Consistency by Agreement in Zero-shot Neural Machine Translation", "labels": [], "entities": [{"text": "Zero-shot Neural Machine Translation", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.610197015106678}]}], "abstractContent": [{"text": "Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest.", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7050914168357849}]}, {"text": "In this paper, we focus on zero-shot generalization-a challenging setup that tests models on translation directions they have not been optimized for at training time.", "labels": [], "entities": [{"text": "zero-shot generalization-a", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8079966902732849}]}, {"text": "To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages.", "labels": [], "entities": []}, {"text": "We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.", "labels": [], "entities": [{"text": "IWSLT17", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.8722269535064697}, {"text": "UN corpus", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.923850953578949}, {"text": "Europarl", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9265033006668091}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9949532151222229}]}], "introductionContent": [{"text": "Machine translation (MT) has made remarkable advances with the advent of deep learning approaches ().", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9080402016639709}]}, {"text": "The progress was largely driven by the encoder-decoder framework) and typically supplemented with an attention mechanism ().", "labels": [], "entities": []}, {"text": "Compared to the traditional phrase-based systems, neural machine translation (NMT) requires large amounts of data in order to reach high performance (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.8349892695744833}]}, {"text": "Using NMT in a multilingual setting exacerbates the problem by the fact that given k languages * Work done at Google.", "labels": [], "entities": []}, {"text": "At training time, given English-French (En \u2194 Fr) and English-German (En \u2194 De) parallel sentences, the model not only is trained to translate between the pair but also to agree on translations into a third language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate agreement-based training against baselines from the literature on three public datasets that have multi-parallel evaluation data that allows assessing zero-shot performance.", "labels": [], "entities": []}, {"text": "We report results in terms of the BLEU score () that was computed using mteval-v13a.perl.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9771615862846375}]}, {"text": "Additional details on the hyperparameters can be found in Appendix A.4.", "labels": [], "entities": [{"text": "Appendix A.4", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.8716416954994202}]}, {"text": "We use a smaller version of the GNMT architecture () in all our experiments: 512-dimensional embeddings (separate for source and target sides), 2 bidirectional LSTM layers of 512 units each for encoding, and GNMT-style, 4-layer, 512-unit LSMT decoder with residual connections from the 2nd layer onward.", "labels": [], "entities": []}, {"text": "We trained the above model using the standard method of and using our proposed agreement-based training (Algorithm 1).", "labels": [], "entities": []}, {"text": "In both cases, the model was optimized using Adafactor) on a machine with 4 P100 GPUs for up to 500K steps, with early stopping on the dev set.", "labels": [], "entities": []}, {"text": "We focus our evaluation mainly on zero-shot performance of the following methods: (a) Basic, which stands for directly evaluating a multilingual GNMT model after standard training ().", "labels": [], "entities": [{"text": "Basic", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9913570880889893}]}, {"text": "(b) Pivot, which performs pivoting-based inference using a multilingual GNMT model (after standard training); often regarded as gold-standard.", "labels": [], "entities": []}, {"text": "(c) Agree, which applies a multilingual GNMT model trained with agreement losses directly to zero-shot directions.", "labels": [], "entities": [{"text": "Agree", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9908022880554199}]}, {"text": "To ensure a fair comparison in terms of model capacity, all the techniques above use the same multilingual GNMT architecture described in the previous section.", "labels": [], "entities": []}, {"text": "All other results provided in the tables are as reported in the literature.", "labels": [], "entities": []}, {"text": "All our methods were implemented using TensorFlow ( on top of tensor2tensor library (.", "labels": [], "entities": []}, {"text": "Our code will be made publicly available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on UNCorpus-1.", "labels": [], "entities": [{"text": "UNCorpus-1", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.977033257484436}]}, {"text": " Table 2: Results on UNCorpus-2.", "labels": [], "entities": [{"text": "UNCorpus-2", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.978692352771759}]}, {"text": " Table 3: Zero-shot results on Europarl. Note that Soft and", "labels": [], "entities": [{"text": "Europarl", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9886099100112915}]}, {"text": " Table 4: Results on the official IWSLT17 multilingual task.", "labels": [], "entities": [{"text": "IWSLT17 multilingual task", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5505201319853464}]}, {"text": " Table 5: Results on our proposed IWSLT17 .", "labels": [], "entities": [{"text": "IWSLT17", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.7862080335617065}]}]}