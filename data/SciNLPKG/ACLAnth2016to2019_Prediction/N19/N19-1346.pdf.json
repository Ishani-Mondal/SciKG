{"title": [], "abstractContent": [{"text": "This paper introduces anew task-Chinese address parsing-the task of mapping Chi-nese addresses into semantically meaningful chunks.", "labels": [], "entities": []}, {"text": "While it is possible to model this problem using a conventional sequence labelling approach, our observation is that there exist complex dependencies between labels that cannot be readily captured by a simple linear-chain structure.", "labels": [], "entities": []}, {"text": "We investigate neural structured prediction models with latent variables to capture such rich structural information within Chinese addresses.", "labels": [], "entities": []}, {"text": "We create and publicly release anew dataset consisting of 15,000 Chinese addresses, and conduct extensive experiments on the dataset to investigate the model effectiveness and robustness.", "labels": [], "entities": []}, {"text": "We release our code and data at http:// statnlp.org/research/sp.", "labels": [], "entities": []}], "introductionContent": [{"text": "Addresses play an important role in modern society.", "labels": [], "entities": []}, {"text": "They are typically used as identifiers to locations and entities in the world that can be used to facilitate various social activities, such as business correspondences, meetings and events.", "labels": [], "entities": []}, {"text": "Recent research efforts show that systems that perform address parsing, the task of automatically parsing addresses into semantically meaningful structures, can be useful for tasks such as building e-commerce or product recommendation systems ().", "labels": [], "entities": [{"text": "address parsing", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7681038081645966}]}, {"text": "Due to historical reasons, the English addresses come with a standardized format, mostly written in order from most specific to most general.", "labels": [], "entities": []}, {"text": "Meaningful chunks in an English address are also separated by punctuation or the new-line symbols.", "labels": [], "entities": []}, {"text": "Such characteristics make parsing English addresses a relatively easy task.", "labels": [], "entities": [{"text": "parsing English addresses", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.9173897703488668}]}, {"text": "Unlike English addresses, Chinese addresses are typically written in the form of a consecutive sequence of Chinese characters (possibly intermixed with digits and English letters).", "labels": [], "entities": []}, {"text": "presents two example Chinese addresses and their desired output structures after parsingchunks annotated with their labels indicating semantics (such as province, road, etc).", "labels": [], "entities": []}, {"text": "The Chinese addressing system is also different from that of English.", "labels": [], "entities": []}, {"text": "Though it is generally believed that the system uses the opposite ordering -starting from most general (e.g., province) and ending with most specific (e.g., room no.), in practice it can be observed that the format is far less rigorous than expected.", "labels": [], "entities": []}, {"text": "The lack of rigor also leads to other issues -the addresses may come with incomplete, redundant or even inaccurate information, as we can see from the second example listed in.", "labels": [], "entities": []}, {"text": "Such unique challenges make the design of an effective Chinese address parser non-trivial.", "labels": [], "entities": [{"text": "Chinese address parser", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6060220797856649}]}, {"text": "Parsing a Chinese address into semantically meaningful structures can be regarded as a special type of chunking task, where we need to perform address-specific Chinese word segmentation) while assigning a semantic label to each chunk.", "labels": [], "entities": [{"text": "Parsing a Chinese address into semantically meaningful structures", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.8564770147204399}]}, {"text": "However, existing models designed for chunking may not be readily applicable in this task.", "labels": [], "entities": [{"text": "chunking", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9784592390060425}]}, {"text": "Our observations show that there area few characteristics associated with the task.", "labels": [], "entities": []}, {"text": "We found that while generally there exists certain ordering information among the chunks of different labels in the addresses, such ordering information is better preserved among the chunks that appear at the beginning of the addresses.", "labels": [], "entities": []}, {"text": "For the chunks appearing towards the end of the addresses, chunks of different types often appear in more flexible order.", "labels": [], "entities": []}, {"text": "On top of the above observations, we propose a specific model based on neural networks for the task of Chinese address parsing.", "labels": [], "entities": [{"text": "Chinese address parsing", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.722821036974589}]}, {"text": "The model is able to encode the regular patterns among chunks that appear at the beginning of a Chinese address, while flexibly capturing the irregular patterns and rich dependencies among the chunks of different types that appear towards the end of the address.", "labels": [], "entities": []}, {"text": "This is achieved by designing a novel structured representation integrating both a linear structure and a latent-variable tree structure.", "labels": [], "entities": []}, {"text": "Our main contributions in this work can be summarized as follows: \u2022 We create and publicly release anew corpus consisting of 15K Chinese address entries fully annotated with chunk boundaries and address labels.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first and largest annotated Chinese address corpus.", "labels": [], "entities": [{"text": "Chinese address corpus", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.5942666927973429}]}, {"text": "\u2022 We introduce a novel neural approach to Chinese address parsing with latent variables to flexibly capture both prior ordering information and rich dependencies among labels.", "labels": [], "entities": [{"text": "Chinese address parsing", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7650892734527588}]}, {"text": "\u2022 Through extensive experiments, we demonstrate the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "The experimental results show that our approach outperforms several baselines significantly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We call our model Address Parser with Latent Trees (APLT).", "labels": [], "entities": []}, {"text": "We conducted experiments based on different settings of the sp values, leading to many model variants.", "labels": [], "entities": []}, {"text": "We describe baselines, model hyperparameters as well as evaluation metrics in this section.", "labels": [], "entities": []}, {"text": "Baselines To understand the effectiveness of our models, we build the following baselines: \u2022 CRF is the standard first-order linear CRF model () with discrete features for sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.6826325953006744}]}, {"text": "\u2022 sCRF is based on the standard semi-Markov CRF () with discrete features 5 . \u2022 LSTM is the standard bi-directional LSTM model for sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.6601066092650095}]}, {"text": "\u2022 LSTM-CRF is proposed by which is the state-of-the-art for many sequence labeling tasks \u2022 LSTM-sCRF is based on segmental recurrent neural network ( which is the neural network version of semi-Markov CRF ().", "labels": [], "entities": []}, {"text": "\u2022 TP is a transition-based parser for chunking based on, which makes use of the stack LSTM ( to encode the representation of the stack.", "labels": [], "entities": []}, {"text": "Hyperparameters We conducted all the experiments based on our Chinese Address corpus.", "labels": [], "entities": [{"text": "Chinese Address corpus", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9442394375801086}]}, {"text": "We pre-trained Chinese character embeddings based on the Chinese Gigaword corpus (), using the skip-gram model with hierarchical softmax implemented within the word2vec toolkit () where we set the sample rate to 10 \u22125 and embedding size to 100.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.7810189723968506}]}, {"text": "We use a 2-layer LSTM (for both directions) with a hidden dimension of 200.", "labels": [], "entities": []}, {"text": "For optimization, we adopt the Adam ( optimizer to optimize the model with batch size 1 and dropout rate 0.4.", "labels": [], "entities": []}, {"text": "We randomly replace the low frequency words with the UNK token and normalize all numbers by replacing each digit (including Chinese characters representing numbers from 0-9) to 0.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9359975159168243}]}, {"text": "We train our model fora maximal of 30 epochs and select the model parameters based on the F 1 score after each epoch on the development set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9782362182935079}]}, {"text": "The selected model is then applied to the test set for evaluation.", "labels": [], "entities": []}, {"text": "Our model, as well as the baseline neural models, are implemented using DyNet ().", "labels": [], "entities": []}, {"text": "All the neural weights are initialized following the default initialization method used in DyNet.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We use the standard evaluation metrics from the CoNLL-2000 shared task), reporting precision (P.), recall (R.) and F 1 percentage scores.", "labels": [], "entities": [{"text": "CoNLL-2000 shared task", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.8271587491035461}, {"text": "reporting precision (P.)", "start_pos": 92, "end_pos": 116, "type": "METRIC", "confidence": 0.9045595526695251}, {"text": "recall (R.)", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9566616863012314}, {"text": "F 1 percentage scores", "start_pos": 134, "end_pos": 155, "type": "METRIC", "confidence": 0.9774903059005737}]}], "tableCaptions": [{"text": " Table 1: Statistics of different labels in our Chinese Address corpus.", "labels": [], "entities": [{"text": "Chinese Address corpus", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.8765784502029419}]}, {"text": " Table 3: F 1 score comparison on test data for each label  among 4 models as well as the percentage of each label  in the gold data.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9949237704277039}]}, {"text": " Table 4: Results for different chunk lengths.", "labels": [], "entities": []}, {"text": " Table 5: Accuracy on test data for the new chunks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998077392578125}]}]}