{"title": [{"text": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems", "labels": [], "entities": [{"text": "Visually Attacking and Shielding NLP", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.718705952167511}]}], "abstractContent": [{"text": "Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., \"!d10t\") or as a writing style (\"1337\" in \"leet speak\"), among other scenarios.", "labels": [], "entities": []}, {"text": "We consider this as anew type of ad-versarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate.", "labels": [], "entities": []}, {"text": "We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%.", "labels": [], "entities": []}, {"text": "We then explore three shielding methods-visual character embeddings, adversarial training, and rule-based recovery-which substantially improve the robustness of the models.", "labels": [], "entities": [{"text": "rule-based recovery-which", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.7745090126991272}]}, {"text": "However , the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.", "labels": [], "entities": []}], "introductionContent": [{"text": "For humans, visual similarity can play a decisive role for assessing the meaning of characters.", "labels": [], "entities": []}, {"text": "Some evidence for these are: the frequent swapping of similar looking characters in Internet slang or abusive comments, creative trademark logos, and attack scenarios such as domain name spoofing (see examples in.", "labels": [], "entities": []}, {"text": "Recently, some NLP systems have exploited visual features to capture visual relationships among characters in compositional writing systems such as Chinese or Korean (.", "labels": [], "entities": []}, {"text": "However, in more general cases, current neural NLP systems have no built-in notion of visual character similarity.", "labels": [], "entities": []}, {"text": "Rather, they either treat characters as discrete units forming a word or they represent characters by randomly initialized embeddings and update them during training-typically in order to generate a character-based word representation that is robust to morphological variation or spelling mistakes.", "labels": [], "entities": []}, {"text": "Intriguingly, this marked distinction between human and machine processing can be exploited as a blind spot of NLP systems.", "labels": [], "entities": []}, {"text": "For example, spammers might send malicious emails or post toxic comments to online discussion forums) by visually 'perturbing' the input text in such away that it is still easily recoverable by humans.", "labels": [], "entities": []}, {"text": "The issue of exposing and addressing the weaknesses of deep learning models to adversarial inputs, i.e., perturbed versions of original input samples, has recently received considerable attention.", "labels": [], "entities": []}, {"text": "For instance, showed that small perturbations in the pixels of an image can mislead a neural classifier to predict an incorrect label for the image.", "labels": [], "entities": []}, {"text": "In NLP, Jia and Liang (2017) inserted grammatically correct but semantically irrelevant paragraphs to stories to fool neural reading comprehension models.", "labels": [], "entities": []}, {"text": "showed significant drops in the performance of neural models for question answering when using simple paraphrases of the original questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8172386288642883}]}, {"text": "Unlike previous NLP attack scenarios, visual attacks, i.e., the exchange of characters in the input with visually similar alternatives, have the following 'advantages': 1) They do not require any linguistic knowledge beyond the character level, making the attacks straightforwardly applicable across languages, domains, and tasks.", "labels": [], "entities": []}, {"text": "2) They are al-legedly less damaging to human perception and understanding than, e.g., syntax errors or the insertion of negations ().", "labels": [], "entities": []}, {"text": "3) They do not require knowledge of the attacked model's parameters or loss function (.", "labels": [], "entities": []}, {"text": "In this work, we investigate to what extent recent state-of-the-art (SOTA) deep learning models are sensitive to visual attacks and explore various shielding techniques.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We introduce VIPER, a Visual Perturber that randomly replaces characters in the input with their visual nearest neighbors in a visual embedding space.", "labels": [], "entities": [{"text": "VIPER", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9035188555717468}]}, {"text": "\u2022 We show that the performance of SOTA deep learning models substantially drops for various NLP tasks when attacked by VIPER.", "labels": [], "entities": [{"text": "SOTA deep learning", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8483354449272156}]}, {"text": "On individual tasks (e.g., Chunking) and attack scenarios, our observed drops are up to 82%.", "labels": [], "entities": [{"text": "Chunking)", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.8078125417232513}]}, {"text": "\u2022 We show that, in contrast to NLP systems, humans are only mildly or not at all affected by visual perturbations.", "labels": [], "entities": []}, {"text": "\u2022 We explore three methods to shield from visual attacks, viz., visual character embeddings, adversarial training (, and rule-based recovery.", "labels": [], "entities": [{"text": "rule-based recovery", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7855551242828369}]}, {"text": "We quantify to which degree and in which circumstances these are helpful.", "labels": [], "entities": []}, {"text": "We point out that integrating visual knowledge with deep learning systems, as our visual character embeddings do, aims to make NLP models behave more like humans by taking cues directly from sensory information such as vision.", "labels": [], "entities": []}], "datasetContent": [{"text": "We asked 6 human annotators, university employees and students with native or near-native English language skills, to recover the original underlying English sentences given some perturbed text (data taken from the POS tagging and Chunking tasks, see).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 215, "end_pos": 226, "type": "TASK", "confidence": 0.6973533183336258}]}, {"text": "We considered different conditions: For each condition, we used 60-120 sentences, whereat most 20 sentences of one condition were given to an annotator.", "labels": [], "entities": []}, {"text": "Examples of selected conditions are shown in.", "labels": [], "entities": []}, {"text": "Our rationale for including this recovery task is to test robustness of human perception under (our) visual perturbations.", "labels": [], "entities": []}, {"text": "We focus on recovery instead of an extrinsic task such as POS because the latter would have required expert/trained annotators.", "labels": [], "entities": []}, {"text": "We evaluate by measuring the normalized edit distance between the recovered sentence and the underlying original, averaged overall sequence pairs and all human annotators.", "labels": [], "entities": []}, {"text": "We normalize by the maximum lengths of the two sequences.", "labels": [], "entities": []}, {"text": "In our case, this metric can be interpreted as the fraction of characters that have been, on average, wrongly recovered by human annotators.", "labels": [], "entities": []}, {"text": "We refer to the metric as \"error rate\".", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9794239103794098}]}, {"text": "In easy, there is almost no difference between perturbation levels p = 0.4 and p = 0.8, so we merge the two conditions.", "labels": [], "entities": []}, {"text": "Humans make copy mistakes even when the input is not perturbed, as evidenced by a positive nearest neighbors are largely random and there are far more CJK characters in our subset of Unicode.", "labels": [], "entities": []}, {"text": "Such mistakes are typically misspellings or the wrong type of quotation marks (\" vs. \").", "labels": [], "entities": []}, {"text": "We observe a slightly higher error rate in easy than in clean.", "labels": [], "entities": [{"text": "error rate", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9776518940925598}]}, {"text": "However, on average 75% of all sentences are (exactly) correctly recovered in easy while this number is lower (72.5%) in clean.", "labels": [], "entities": []}, {"text": "By chance, clean contains fewer sentences with quotation marks than easy, for which a copy mistake was more likely.", "labels": [], "entities": []}, {"text": "This may explain easy's higher error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9831810295581818}]}, {"text": "We now evaluate the capabilities of SOTA neural network models to deal with visual attacks in four extrinsic evaluation tasks described in \u00a75.1 and illustrated in.", "labels": [], "entities": []}, {"text": "Hyperparameters of all our models are given in \u00a7A.2.", "labels": [], "entities": []}, {"text": "We first examine the robustness of all architectures to visual perturbations in \u00a75.2 and then evaluate different shielding approaches in \u00a75.3.", "labels": [], "entities": []}, {"text": "To analyze the differences between VELMo and SELMo, we investigate whether the models learn similar word embeddings fora clean sentence and its visually perturbed counterpart.", "labels": [], "entities": []}, {"text": "We compare sentence embeddings which we obtain by averaging over the SELMo or VELMo word embeddings of a sentence (clean or perturbed).", "labels": [], "entities": [{"text": "SELMo", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.694646418094635}]}, {"text": "Setup Given a sentence embedding \u03c3 of a clean sentence and an embedding \u03c3 \u2032 of its visually perturbed counterpart, obtained by either averaging over VELMo (indicated by subscript v) or SELMo (indicated by subscript s) word embeddings, we test if the condition is met (with cos being the cosine similarity).", "labels": [], "entities": [{"text": "VELMo", "start_pos": 149, "end_pos": 154, "type": "DATASET", "confidence": 0.6350620985031128}, {"text": "SELMo", "start_pos": 185, "end_pos": 190, "type": "METRIC", "confidence": 0.8599962592124939}]}, {"text": "For our experiments, we randomly sample 1000 sentences from the Toxic Comments dataset (see \u00a75.1) and perturb them with VIPER(p, CES) where CES \u2208 {ICES, DCES}.", "labels": [], "entities": [{"text": "Toxic Comments dataset", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.6054026981194814}, {"text": "VIPER", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.837967038154602}, {"text": "CES", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.6913900971412659}, {"text": "CES", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.8766198754310608}]}, {"text": "We then count the number N of cases in which the above condition is met with regards to the chosen CES and the value of p, and report the ratio R = N/1000.", "labels": [], "entities": [{"text": "CES", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8338090181350708}]}], "tableCaptions": [{"text": " Table 2: Ten nearest neighbors in our different character spaces. 'SELMo' refers to the nearest neighbors of the  trained character embeddings in SELMo.", "labels": [], "entities": []}, {"text": " Table 3: Examples of perturbed sentences and underlying originals.", "labels": [], "entities": []}, {"text": " Table 4: NLP tasks considered in this work, along with (perturbed) examples and data split statistics.", "labels": [], "entities": []}, {"text": " Table 5: Two examples of toxic/non-toxic comments that show the effects of the different shielding methods. We  report the averaged sum over the six toxicity classes, e.g., 4.00 is equal to a positive example in four classes. p = 0.1.", "labels": [], "entities": []}, {"text": " Table 4. We report edit distance between de- sired pronunciations and predicted pronunciations  as metric. We report the edit distance averaged  across all 1k test strings, averaged over 5 random  initializations of all weight matrices.", "labels": [], "entities": []}, {"text": " Table 6: Results of the intrinsic evaluation where we  compare clean sentences to their perturbed counterparts  as well as randomly chosen sentences. The numbers  show the ratio of cases where clean sentences are more  similar to their perturbed counterparts than the ran- domly chosen sentences.", "labels": [], "entities": []}]}