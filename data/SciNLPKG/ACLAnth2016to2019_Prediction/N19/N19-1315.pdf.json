{"title": [{"text": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction", "labels": [], "entities": [{"text": "Accelerated", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9828621745109558}, {"text": "Sentence Generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.9394724667072296}, {"text": "Vocabulary Prediction", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.6481545269489288}]}], "abstractContent": [{"text": "A major obstacle in reinforcement learning-based sentence generation is the large action space whose size is equal to the vocabulary size of the target-side language.", "labels": [], "entities": [{"text": "reinforcement learning-based sentence generation", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.6999442726373672}]}, {"text": "To improve the efficiency of reinforcement learning, we present a novel approach for reducing the action space based on dynamic vocabulary prediction.", "labels": [], "entities": [{"text": "dynamic vocabulary prediction", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.6433189908663431}]}, {"text": "Our method first predicts a fixed-size small vocabulary for each input to generate its target sentence.", "labels": [], "entities": []}, {"text": "The input-specific vocabularies are then used at supervised and reinforcement learning steps, and also attest time.", "labels": [], "entities": []}, {"text": "In our experiments on six machine translation and two image captioning datasets, our method achieves faster reinforcement learning (\u223c2.7x faster) with less GPU memory (\u223c2.3x less) than the full-vocabulary counterpart.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7082828432321548}]}, {"text": "We also show that our method more effectively receives rewards with fewer iterations of supervised pre-training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence generation with neural networks plays a key role in many language processing tasks, including machine translation (), image captioning (, and abstractive summarization ().", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9432170689105988}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8164439499378204}, {"text": "image captioning", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.7565961480140686}, {"text": "abstractive summarization", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.5707958340644836}]}, {"text": "The most common approach for learning the sentence generation models is maximizing the likelihood of the model on the gold-standard target sentences.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7065753191709518}]}, {"text": "Recently, approaches based on reinforcement learning have attracted increasing attention to reduce the gap between training and test situations and to directly incorporate taskspecific and more flexible evaluation metrics such as BLEU scores () into optimization (.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 230, "end_pos": 241, "type": "METRIC", "confidence": 0.9773699045181274}]}, {"text": "While reinforcement learning-based sentence generation is appealing, it is often too computa-tionally demanding to be used with large training data.", "labels": [], "entities": [{"text": "reinforcement learning-based sentence generation", "start_pos": 6, "end_pos": 54, "type": "TASK", "confidence": 0.7270614206790924}]}, {"text": "In reinforcement learning for sentence generation, selecting an action corresponds to selecting a word in the vocabulary V . The number of possible actions at each time step is thus equal to the vocabulary size, which often exceeds tens of thousands.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7393853962421417}]}, {"text": "Among such a large set of possible actions, at most N actions are selected if the length of the generated sentence is N , where we can assume N |V |.", "labels": [], "entities": []}, {"text": "In other words, most of the possible actions are not selected, and the large action space slows down reinforcement learning and consumes a large amount of GPU memory.", "labels": [], "entities": []}, {"text": "In this paper, we propose to accelerate reinforcement learning by reducing the large action space.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9325438141822815}]}, {"text": "The reduction of action space is achieved by predicting a small vocabulary for each source input.", "labels": [], "entities": []}, {"text": "Our method first constructs the small inputspecific vocabulary by selecting K (\u2264 1000) relevant words, and then the small vocabulary is used at both training and test time.", "labels": [], "entities": []}, {"text": "Our experiments on six machine translation and two image captioning datasets show that our method enables faster reinforcement learning with less GPU memory than the standard full softmax method, without degrading the accuracy of the sentence generation tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7033358514308929}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9970394968986511}, {"text": "sentence generation tasks", "start_pos": 234, "end_pos": 259, "type": "TASK", "confidence": 0.7727998892466227}]}, {"text": "Our method also works faster attest time, especially on CPUs.", "labels": [], "entities": []}, {"text": "The implementation of our method is available at https: //github.com/hassyGo/NLG-RL.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe our experimental settings, and the details can be found in the supplemental material.", "labels": [], "entities": []}, {"text": "We used machine translation datasets of four different language pairs: English-to-German (En-De), English-to-Japanese (En-Ja), Englishto-Vietnamese (En-Vi), and Chinese-to-Japanese (Ch-Ja).", "labels": [], "entities": []}, {"text": "For image captioning, we used two datasets: MS COCO () and Flickr8K.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8259771168231964}, {"text": "MS COCO", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.7356924116611481}, {"text": "Flickr8K", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.6717876195907593}]}, {"text": "summarizes the statistics of the training datasets, where the number of training examples (\"Size\"), the target vocabulary size (|V |), and the maximum length of the target sentences (max(N )) are shown.", "labels": [], "entities": []}, {"text": "For the machine translation datasets, we manually set max(N ) and omitted training examples which violate the constraints.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7059160470962524}]}], "tableCaptions": [{"text": " Table 1: Statistics of the training datasets.", "labels": [], "entities": [{"text": "training datasets", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.6823591589927673}]}, {"text": " Table 2: BLEU scores for the development splits of the six datasets. \"Small softmax\" corresponds to our method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992644190788269}]}, {"text": " Table 3: BLEU scores for the development split of the  En-Ja (2M) and En-Ja (2M, SW) datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989939332008362}, {"text": "En-Ja (2M, SW) datasets", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.6462146895272392}]}, {"text": " Table 4: BLEU scores for the En-Ja test split, where  we use the En-Ja (2M, SW) dataset. The 95% confi- dence interval by bootstrap resampling", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993990659713745}, {"text": "En-Ja test split", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.7890768150488535}, {"text": "En-Ja (2M, SW) dataset", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.5980792556490216}]}, {"text": " Table 5: Training time, and maximum memory consumption on our GPU devices for the text generation models.  For the full softmax baseline on the En-Ja (2M) experiments, the mini-batch splitting strategy (described in Sec- tion 4.4) is applied. CE: Cross-Entropy, Small: Small softmax (our proposed method), Full: Full softmax (the  baseline).", "labels": [], "entities": [{"text": "CE", "start_pos": 244, "end_pos": 246, "type": "METRIC", "confidence": 0.9754098057746887}]}]}