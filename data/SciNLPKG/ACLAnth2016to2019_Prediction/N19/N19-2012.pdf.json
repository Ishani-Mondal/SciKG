{"title": [{"text": "Active Learning for New Domains in Natural Language Understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore active learning (AL) for improving the accuracy of new domains in a natural language understanding (NLU) system.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7047802448272705}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9946795105934143}]}, {"text": "We propose an algorithm called Majority-CRF that uses an ensemble of classification models to guide the selection of relevant utterances, as well as a sequence labeling model to help prioritize informative examples.", "labels": [], "entities": []}, {"text": "Experiments with three domains show that Majority-CRF achieves 6.6%-9% relative error rate reduction compared to random sampling with the same annotation budget, and statistically significant improvements compared to other AL approaches.", "labels": [], "entities": [{"text": "relative error rate reduction", "start_pos": 71, "end_pos": 100, "type": "METRIC", "confidence": 0.8592252731323242}]}, {"text": "Additionally, case studies with human-in-the-loop AL on six new domains show 4.6%-9% improvement on an existing NLU system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Intelligent voice assistants (IVA) such as Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana, are becoming increasingly popular.", "labels": [], "entities": []}, {"text": "For IVA, natural language understanding (NLU) is a main component, in conjunction with automatic speech recognition (ASR) and dialog management (DM).", "labels": [], "entities": [{"text": "IVA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9810738563537598}, {"text": "natural language understanding (NLU)", "start_pos": 9, "end_pos": 45, "type": "TASK", "confidence": 0.7902807394663492}, {"text": "automatic speech recognition (ASR)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.7855677207310995}, {"text": "dialog management (DM)", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8158734500408172}]}, {"text": "ASR converts user's speech to text.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9170923829078674}]}, {"text": "Then, the text is passed to NLU for classifying the action or \"intent\" that the user wants to invoke (e.g., PlayMusicIntent, TurnOnIntent, BuyItemIntent) and recognizing namedentities (e.g., Artist, Genre, City).", "labels": [], "entities": [{"text": "BuyItemIntent", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.897062361240387}]}, {"text": "Based on the NLU output, DM decides the appropriate response, which could be starting a song playback or turning off lights.", "labels": [], "entities": []}, {"text": "NLU systems for IVA support functionality in a wide range of domains, such as music, weather, and traffic.", "labels": [], "entities": [{"text": "IVA", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9545107483863831}]}, {"text": "Also, an important requirement is the ability to add support for new domains.", "labels": [], "entities": []}, {"text": "The NLU models for Intent Classification (IC) and Named Entity Recognition (NER) use machine learning to recognize variation in natural language.", "labels": [], "entities": [{"text": "Intent Classification (IC)", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8391116857528687}, {"text": "Named Entity Recognition (NER)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7675731480121613}]}, {"text": "Diverse, annotated training data collected from IVA users, or \"annotated live utterances,\" are essential for these models to achieve good performance.", "labels": [], "entities": []}, {"text": "As such, new domains frequently exhibit suboptimal performance due to alack of annotated live utterances.", "labels": [], "entities": []}, {"text": "While an initial training dataset can be bootstrapped using grammar generated utterances and crowdsourced collection (Amazon Mechanical Turk), the performance that can be achieved using these approaches is limited because of the unexpected discrepancies between anticipated and live usage.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk)", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9054413139820099}]}, {"text": "Thus, a mechanism is required to select live utterances to be manually annotated for enriching the training dataset.", "labels": [], "entities": []}, {"text": "Random sampling is a common method for selecting live utterances for annotation.", "labels": [], "entities": [{"text": "Random sampling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7312746942043304}]}, {"text": "However, in an IVA setting with many users, the number of available live utterances is vast.", "labels": [], "entities": []}, {"text": "Meanwhile, due to the high cost of manual annotation, only a small percentage of utterances can be annotated.", "labels": [], "entities": []}, {"text": "As such, in a random sample of live data, the number of utterances relevant to new domains maybe small.", "labels": [], "entities": []}, {"text": "Moreover, those utterances may not be informative, where informative utterances are those that, if annotated and added to the training data, reduce the error rates of the NLU system.", "labels": [], "entities": [{"text": "error rates", "start_pos": 152, "end_pos": 163, "type": "METRIC", "confidence": 0.9561543762683868}]}, {"text": "Thus, for new domains, we want a sampling procedure which selects utterances that are both relevant and informative.", "labels": [], "entities": []}, {"text": "Active learning (AL) refers to machine learning methods that can interact with the sampling procedure and guide the selection of data for annotation.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7985807895660401}]}, {"text": "In this work, we explore using AL for live utterance selection for new domains in NLU.", "labels": [], "entities": [{"text": "live utterance selection", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6407904128233591}]}, {"text": "Authors have successfully applied AL techniques to NLU systems with little annotated data overall).", "labels": [], "entities": []}, {"text": "The difference with our work is that, to the best of our knowledge, there is little published AL research that focuses on data selection explicitly targeting new domains.", "labels": [], "entities": []}, {"text": "We compare the efficacy of leastconfidence ( and query-by-committee () AL for new domains.", "labels": [], "entities": []}, {"text": "Moreover, we propose an AL algorithm called Majority-CRF, designed to improve both IC and NER of an NLU system.", "labels": [], "entities": [{"text": "IC", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.7062274813652039}]}, {"text": "Majority-CRF uses an ensemble of classification models to guide the selection of relevant utterances, as well as a sequence labeling model to help prioritize informative examples.", "labels": [], "entities": []}, {"text": "Simulation experiments on three different new domains show that Majority-CRF achieves 6.6%-9% relative improvements in-domain compared to random sampling, as well as significant improvements compared to other active learning approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Slot Error Rate (SER) (), including the intent as slot, to evaluate the overall predictive performance of the NLU models.", "labels": [], "entities": [{"text": "Slot Error Rate (SER)", "start_pos": 7, "end_pos": 28, "type": "METRIC", "confidence": 0.799005980292956}, {"text": "slot", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9877555966377258}]}, {"text": "SER as the ratio of the number of slot prediction errors to the total number of reference slots.", "labels": [], "entities": [{"text": "SER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9933951497077942}, {"text": "slot prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6221489310264587}]}, {"text": "Errors are insertions, substitutions and deletions.", "labels": [], "entities": []}, {"text": "We treat the intent misclassifications as substitution errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Simulation experimental results with 36K annotation budget. \u2206SER is % relative reduction is SER  compared to the initial model: Books SER 30.59, Local Search SER 39.09, Cinema SER 38.71. Higher \u2206SER is  better. The best result is in bold, and the second best is underlined. The i = 1 means selection in a single iteration,  otherwise if not specified selection is in six iterations (i = 6). Overall #Utt shows the remaining from the 36K  selected after removing the sentence fragments and out-of-domain utterances. Both target and non-target domains  IC and NER models are re-retrained with the new data.", "labels": [], "entities": [{"text": "SER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9550982713699341}, {"text": "SER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9908930063247681}, {"text": "SER", "start_pos": 205, "end_pos": 208, "type": "METRIC", "confidence": 0.9763765931129456}]}, {"text": " Table 4: AL with human annotator results. \u2206SER is %  relative gain compared to the existing model. Higher is  better.", "labels": [], "entities": [{"text": "AL", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.7987971305847168}, {"text": "SER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9987102746963501}]}]}