{"title": [{"text": "Augmenting word2vec with Latent Dirichlet Allocation within a Clinical Application", "labels": [], "entities": [{"text": "Augmenting word2vec", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7645718455314636}]}], "abstractContent": [{"text": "This paper presents three hybrid models that directly combine latent Dirichlet allocation and word embedding for distinguishing between speakers with and without Alzheimer's disease from transcripts of picture descriptions.", "labels": [], "entities": []}, {"text": "Two of our models get F-scores over the current state-of-the-art using automatic methods on the DementiaBank dataset.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9973167777061462}, {"text": "DementiaBank dataset", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9516335129737854}]}], "introductionContent": [{"text": "Word embedding projects words into a lowerdimensional latent space that captures semantic and morphological information.", "labels": [], "entities": []}, {"text": "Separately but related, the task of topic modelling also discovers latent semantic structures or topics in a corpus.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7998360693454742}]}, {"text": "Latent Dirichlet allocation (LDA) uses bag-of-words statistics to infer topics in an unsupervised manner.", "labels": [], "entities": [{"text": "Latent Dirichlet allocation (LDA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7481170147657394}]}, {"text": "LDA considers each document to be a probability distribution over hidden topics, and each topic is a probability distribution overall words in the vocabulary, both with Dirichlet priors.", "labels": [], "entities": [{"text": "LDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7779407501220703}]}, {"text": "The inferred probabilities over learned latent topics of a given document (i.e., topic vectors) can be used along with a discriminative classifier, as in the work by, but other approaches such as TF-IDF () easily outperform this model, like in the case of the Reuters-21578 corpus (.", "labels": [], "entities": [{"text": "Reuters-21578 corpus", "start_pos": 260, "end_pos": 280, "type": "DATASET", "confidence": 0.9681835472583771}]}, {"text": "Here, we hypothesize that creating a hybrid of LDA and word2vec () models will produce discriminative features.", "labels": [], "entities": []}, {"text": "We introduce three new variants of hybrid LDA-word2vec models, and investigate the effect of dropping the first component after principal component analysis (PCA).", "labels": [], "entities": []}, {"text": "These models can bethought of as extending the conglomeration of topical embedding models.", "labels": [], "entities": []}, {"text": "We incorporate topical information into our word2vec models by using the final state of the topic-word distribution in the LDA model during training.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Wisconsin Longitudinal Study (WLS) is a normative dataset where residents of Wisconsin perform the Cookie Theft picture description task).", "labels": [], "entities": [{"text": "Wisconsin Longitudinal Study (WLS)", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.8349389632542928}, {"text": "Cookie Theft picture description task", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.7231956481933594}]}, {"text": "The audio excerpts from the 2011 survey (N = 1,366) were converted to text using the Kaldi open source automatic speech recognition (ASR) engine), specifically using a bi-directional LSTM trained to the Fisher data set ().", "labels": [], "entities": [{"text": "Kaldi open source automatic speech recognition (ASR)", "start_pos": 85, "end_pos": 137, "type": "TASK", "confidence": 0.6774121026198069}, {"text": "Fisher data set", "start_pos": 203, "end_pos": 218, "type": "DATASET", "confidence": 0.8662458658218384}]}, {"text": "DementiaBank (DB) is part of the TalkBank project).", "labels": [], "entities": []}, {"text": "Each participant was assigned to either the 'Dementia' group (N = 167) or the 'Control' group (N = 97).", "labels": [], "entities": []}, {"text": "We use 240 samples from those in the 'Dementia': DB test-data distribution group, and 233 from those in the 'Control' group.", "labels": [], "entities": [{"text": "Dementia': DB test-data distribution group", "start_pos": 38, "end_pos": 80, "type": "DATASET", "confidence": 0.6070324395384107}]}, {"text": "Each speech sample was recorded and manually transcribed at the word level following the CHAT protocol.", "labels": [], "entities": []}, {"text": "We use a 5\u2212fold group cross-validation (CV) to split this dataset while ensuring that a particular participant does not occur in both the train and test splits.", "labels": [], "entities": []}, {"text": "presents the distribution of Control and Dementia groups in the test split for each fold.", "labels": [], "entities": []}, {"text": "WLS is used to train our LDA, word2vec and hybrid models that are then used to generate feature vectors on the DB dataset.", "labels": [], "entities": [{"text": "DB dataset", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8873604834079742}]}, {"text": "The feature vectors on the train set are used to train a discriminative classifier (e.g., SVM), that is then used to do the AD/CT binary classification on the feature vectors of the test set.", "labels": [], "entities": [{"text": "AD/CT binary classification", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.4318993330001831}]}, {"text": "During training we filter out spaCy's () list of stop words from our datasets.", "labels": [], "entities": []}, {"text": "For our LDA models trained on ASR transcripts, we remove the and tokens generated by Kaldi, as well as the um and uh tokens, as this improved downstream model performance.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.8536441922187805}]}, {"text": "Using the the 25-topic topic-induced word2vec, we consider other discriminative classifiers.", "labels": [], "entities": []}, {"text": "As seen in, the linear SVM model gives the best accuracy of 77.5%, though all other models perform similarly, with accuracies upwards of 70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999610960483551}, {"text": "accuracies", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9742223620414734}]}, {"text": "There is no statistically significant difference between using an SVM vs. a LR (p = 0.569) or a gradient boosting classifier (p = 0.094).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: DB test-data distribution", "labels": [], "entities": []}, {"text": " Table 2: DB Classification results (Average 5-Fold F-scores): Part 1", "labels": [], "entities": [{"text": "DB Classification", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.916351318359375}, {"text": "Average 5-Fold F-scores)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7782650142908096}]}, {"text": " Table 3: DB Classification results (Average 5-Fold F-scores): Part 2", "labels": [], "entities": [{"text": "DB Classification", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9012744128704071}, {"text": "Average 5-Fold F-scores)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7820522785186768}]}, {"text": " Table 4: DB: Discriminative Classifiers on Topic- induced LDA-25 model", "labels": [], "entities": []}]}