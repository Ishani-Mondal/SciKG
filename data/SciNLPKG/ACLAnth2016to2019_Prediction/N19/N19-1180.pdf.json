{"title": [{"text": "Hierarchical User and Item Representation with Three-Tier Attention for Recommendation", "labels": [], "entities": [{"text": "Hierarchical User and Item Representation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5558133661746979}, {"text": "Recommendation", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8778004050254822}]}], "abstractContent": [{"text": "Utilizing reviews to learn user and item representations is useful for recommender systems.", "labels": [], "entities": []}, {"text": "Existing methods usually merge all reviews from the same user or for the same item into along document.", "labels": [], "entities": []}, {"text": "However, different reviews, sentences and even words usually have different informativeness for modeling users and items.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hierarchical user and item representation model with three-tier attention to learn user and item representations from reviews for recommendation.", "labels": [], "entities": []}, {"text": "Our model contains three major components, i.e., a sentence encoder to learn sentence representations from words, a review encoder to learn review representations from sentences, and a user/item encoder to learn user/item representations from reviews.", "labels": [], "entities": []}, {"text": "In addition, we incorporate a three-tier attention network in our model to select important words, sentences and reviews.", "labels": [], "entities": []}, {"text": "Besides, we combine the user and item representations learned from the reviews with user and item embeddings based on IDs as the final representations to capture the latent factors of individual users and items.", "labels": [], "entities": []}, {"text": "Extensive experiments on four benchmark datasets validate the effectiveness of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning accurate user and item representations is very important for recommender systems).", "labels": [], "entities": []}, {"text": "Many of existing recommendation methods learn user and item representations based on the ratings that users gave to items.", "labels": [], "entities": []}, {"text": "For example, proposed a matrix factorization method based on SVD to learn latent representations of users and items from the rating matrix between users and items.", "labels": [], "entities": []}, {"text": "However, since the numbers of users and items in online platforms are usually huge, and the rating matrix between users and items is usually very sparse, it is quite diffi-\uff0a\uff0a\uff0a\uff0a\u5408 Defrag and cleanup, then you have a great laptop!", "labels": [], "entities": []}, {"text": "July 4, 2018 Style: Laptop Only Verified Purchase I bought this laptop yesterday.", "labels": [], "entities": [{"text": "Style", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9838516116142273}]}, {"text": "This is a great laptop if you immediately run maintenance checks on it (defrag, disk cleanup} and remove a little bloatware.", "labels": [], "entities": []}, {"text": "It is not a laptop to game with, but as a working/school laptop, you're getting a great bang for your buck.", "labels": [], "entities": []}, {"text": "Only giving it four stars just because of the above mentioned things I did afterward, but by no means is this a ho \u300crible laptop.", "labels": [], "entities": []}, {"text": "cult for those rating based recommendation methods to learn accurate user and item representations (.", "labels": [], "entities": []}, {"text": "Luckily, in many online platforms such as Amazon and IMDB, there are rich reviews written by the users to express their opinions on items.", "labels": [], "entities": []}, {"text": "These reviews can provide rich information of items.", "labels": [], "entities": []}, {"text": "For example, if sentences like \"bad battery life\" and \"battery capacity is low\" frequently appear in the reviews of a smartphone, then we can infer the performance of this item in battery life is not good.", "labels": [], "entities": []}, {"text": "The reviews also contain rich information of users.", "labels": [], "entities": []}, {"text": "For example, if a user frequently mentions \"the price is too high\" and \"very expensive\" in his/her reviews for different items, then we can infer this user maybe sensitive to price.", "labels": [], "entities": []}, {"text": "Thus, these reviews can help enhance the learning of user and item representations especially when ratings are sparse, which is beneficial for improving the performance of recommender systems (.", "labels": [], "entities": []}, {"text": "Utilizing reviews to learn user and item representations for recommendation has attracted increasing attentions (.", "labels": [], "entities": []}, {"text": "For example, proposed a DeepCoNN method to learn the representations of users and items from reviews using convolutional neural networks (CNN), and achieved huge improvement in recommendation performance.", "labels": [], "entities": []}, {"text": "These methods usually concatenate the reviews from the same user or the same item into along document.", "labels": [], "entities": []}, {"text": "However, different reviews usually have different informativeness in representing users and items.", "labels": [], "entities": []}, {"text": "For example, in the first review is much more informative than the second one.", "labels": [], "entities": []}, {"text": "Distinguishing informative reviews from noisy ones can help learn more accurate user and item representations.", "labels": [], "entities": []}, {"text": "In addition, different sentences in the same review may also have different informativeness.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"it is not a laptop to game with\" contains more important information than \"I bought this laptop yesterday\".", "labels": [], "entities": []}, {"text": "Besides, different words in the same sentence may also have different importance.", "labels": [], "entities": []}, {"text": "For example, in \"this is a great laptop if you ...\" the word \"great\" is more important than \"you\" in modeling this item.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hierarchical user and item representation model with three-tier attention (HUITA) to learn informative user and item representations from reviews for recommendation.", "labels": [], "entities": [{"text": "three-tier attention (HUITA)", "start_pos": 81, "end_pos": 109, "type": "METRIC", "confidence": 0.6297235131263733}]}, {"text": "In our approach, the hierarchical user and item representation model contains three major components, i.e., a sentence encoder to learn sentence representations from words, a review encoder to learn review representations from sentences, and a user/item encoder to learn user/item representations from the all reviews posted by this user or for this item.", "labels": [], "entities": []}, {"text": "In addition, we propose to incorporate a three-tier attention network into our model to select important words, sentences and reviews to learn more informative user and item representations.", "labels": [], "entities": []}, {"text": "Besides, we combine the user and item representations learned from the reviews with the user and item embeddings based on their IDs as the final representations to capture the latent factors of each individual users and items.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on four benchmark datasets.", "labels": [], "entities": []}, {"text": "The results show our approach can effectively improve the performance of recommendation and outperform many baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on four widely used benchmark datasets in different domains to evaluate the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "Following (, we used three datasets from the Amazon collection 1, i.e., Toys and Games, Kindle Store, and Movies and TV.", "labels": [], "entities": [{"text": "Amazon collection", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.9299487173557281}, {"text": "Kindle Store", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9100305140018463}]}, {"text": "Another dataset is from Yelp Challenge 2017 2 (denoted as Yelp 2017), which is a large-scale restaurant review dataset.", "labels": [], "entities": [{"text": "Yelp Challenge 2017 2", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.9414161145687103}, {"text": "Yelp 2017)", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.8414683739344279}]}, {"text": "Following (, we only kept the users and items which have at least 5 reviews.", "labels": [], "entities": []}, {"text": "The detailed statistics of the four datasets are summarized in.", "labels": [], "entities": []}, {"text": "The ratings in these datasets are in.", "labels": [], "entities": []}, {"text": "In our experiments, the dimension of word embeddings was set to 300.", "labels": [], "entities": []}, {"text": "We used the pre-trained Google embedding () to initialize the word embedding matrix.", "labels": [], "entities": []}, {"text": "The wordlevel CNN has 200 filters and their window size is 3.", "labels": [], "entities": [{"text": "wordlevel CNN", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7801201641559601}]}, {"text": "The sentence-level CNN has 100 filters with window size of 3.", "labels": [], "entities": []}, {"text": "We applied dropout strategy () to each layer of our model to mitigate overfitting.", "labels": [], "entities": []}, {"text": "The dropout rate was set to 0.2.", "labels": [], "entities": []}, {"text": "Adam () was used as the optimization algorithm.", "labels": [], "entities": []}, {"text": "The batch size was set to 20.", "labels": [], "entities": []}, {"text": "We randomly selected 80% of the user-item pairs in each dataset for training, 10% for validation and 10% for test.", "labels": [], "entities": []}, {"text": "All the hyperparameters were selected according to the validation set.", "labels": [], "entities": []}, {"text": "We independently repeated each experiment for 5 times and reported the average performance in Root Mean Square Error (RMSE).", "labels": [], "entities": [{"text": "Root Mean Square Error (RMSE)", "start_pos": 94, "end_pos": 123, "type": "METRIC", "confidence": 0.8925834553582328}]}, {"text": "We evaluate the performance of our approach by comparing it with several baseline methods.", "labels": [], "entities": []}, {"text": "The methods to be compared include: \u2022 PMF: Probabilistic Matrix Factorization, which models users and items based on   ratings via matrix factorization).", "labels": [], "entities": [{"text": "PMF", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.7758228182792664}]}, {"text": "\u2022 NMF: Non-negative Matrix Factorization for recommendation based on rating scores ().", "labels": [], "entities": [{"text": "NMF", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.4631175398826599}]}, {"text": "\u2022 SVD++: The recommendation method based on rating matrix via SVD and similarities between items).", "labels": [], "entities": []}, {"text": "\u2022 HFT: Hidden Factor as Topic (HFT), a method to combine reviews with ratings via LDA).", "labels": [], "entities": [{"text": "HFT", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.6342734694480896}]}, {"text": "\u2022 DeepCoNN: Deep Cooperative Neural Networks, a neural method to jointly model users and items from their reviews via CNN ().", "labels": [], "entities": []}, {"text": "\u2022 Attn+CNN: Attention-based CNN, which uses both CNN and attention over word embeddings to learn user and item representation from reviews ().", "labels": [], "entities": []}, {"text": "\u2022 NARRE: Neural Attentional Rating Regression with Review-level Explanations, which uses attention mechanism to model the informativeness of reviews for recommendation ().", "labels": [], "entities": []}, {"text": "\u2022 HUITA: our proposed hierarchical user and item representation approach with three-tier attention for recommendation with reviews.", "labels": [], "entities": [{"text": "HUITA", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.782242476940155}]}, {"text": "In, we show a simple comparison of different methods in terms of the information considered in each method.", "labels": [], "entities": []}, {"text": "Traditional recommendation methods such as PMF, NMF and SVD are solely based on rating scores, and other methods HFT, DeepCoNN, Attn+CNN, NARRE and HUITA can exploit both rating scores and reviews for recommendation.", "labels": [], "entities": [{"text": "HFT", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9239367246627808}, {"text": "DeepCoNN", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.8915151357650757}, {"text": "Attn+CNN", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.7182701826095581}]}, {"text": "Among the latter methods, HFT is based on topic models and cannot capture the contexts and orders of words.", "labels": [], "entities": []}, {"text": "DeepCoNN and Attn+CNN simply concatenate reviews into along document, and cannot model the informativeness of different reviews.", "labels": [], "entities": [{"text": "DeepCoNN", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9693049192428589}, {"text": "Attn+CNN", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9040995438893636}]}, {"text": "Although NARRE can model review helpfulness via attention, it simply merges all sentences in a review together, and does not model the informativeness of different sentences and words.", "labels": [], "entities": []}, {"text": "Different from these methods, our HUITA approach learns user and item representations from reviews in a hierarchical manner, and uses a three-tier attention network to select and attend to important words, sentences and reviews.", "labels": [], "entities": []}, {"text": "The results of different methods are shown in.", "labels": [], "entities": []}, {"text": "We have several observations from the results.", "labels": [], "entities": []}, {"text": "First, the methods which exploit reviews (i.e., HFT, DeepCoNN, Attn+CNN, NARRE and HUITA) usually perform better than the methods only based on rating scores (i.e., PMF, NMF and SVD++).", "labels": [], "entities": [{"text": "HFT", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9619388580322266}, {"text": "DeepCoNN", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8690605759620667}, {"text": "Attn+CNN", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7569347222646078}]}, {"text": "It validates reviews can provide rich information of user preferences and item properties, and is important to learn informative user and item representations and can benefit recommendation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of datasets used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Information used in different methods. *Only word attention is modeled.", "labels": [], "entities": [{"text": "word attention", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7067808359861374}]}, {"text": " Table 3: RMSE scores of different methods on different datasets. Lower RMSE score means better performance.", "labels": [], "entities": [{"text": "RMSE score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9739708602428436}]}, {"text": " Table 4: The effectiveness of different levels of attentions. The evaluation metric is RMSE.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9906331300735474}]}]}