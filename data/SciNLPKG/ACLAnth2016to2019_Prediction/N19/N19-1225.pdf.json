{"title": [{"text": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets", "labels": [], "entities": []}], "abstractContent": [{"text": "Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks.", "labels": [], "entities": []}, {"text": "While model performance on these challenge datasets is significantly lower compared to the original benchmark , it is unclear what particular weaknesses they reveal.", "labels": [], "entities": []}, {"text": "For example, a challenge dataset maybe difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set.", "labels": [], "entities": []}, {"text": "We introduce inoculation by fine-tuning, anew analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphor-ical pathogen) and assessing how well they can adapt.", "labels": [], "entities": []}, {"text": "We apply our method to analyze the NLI \"stress tests\" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017).", "labels": [], "entities": [{"text": "Adversarial SQuAD dataset", "start_pos": 82, "end_pos": 107, "type": "DATASET", "confidence": 0.5739816824595133}]}, {"text": "We show that after slight exposure, some of these datasets are no longer challenging , while others remain difficult.", "labels": [], "entities": []}, {"text": "Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared.", "labels": [], "entities": []}, {"text": "A recent pattern involves challenges to benchmarks: 1 manipulations to input data that result in severe degradation of system performance, but not human performance.", "labels": [], "entities": []}, {"text": "These challenges have been used as evidence that current systems are brittle Often referred to as \"adversarial datasets\" or \"attacks\".", "labels": [], "entities": []}, {"text": "Figure 1: An illustration of the standard challenge evaluation procedure (e.g., and our proposed analysis method.", "labels": [], "entities": []}, {"text": "\"Original\" refers to the a standard dataset (e.g., SQuAD) and \"Challenge\" refers to the challenge dataset (e.g., Adversarial SQuAD).", "labels": [], "entities": []}, {"text": "Outcomes are discussed in Section 2.", "labels": [], "entities": []}, {"text": "For instance, generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI () and SNLI (.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9239718914031982}]}, {"text": "Similarly, built an adversarial evaluation dataset for reading comprehension based on SQuAD ().", "labels": [], "entities": []}, {"text": "What should we conclude when a system fails on a challenge dataset?", "labels": [], "entities": []}, {"text": "In some cases, a challenge might exploit blind spots in the design of the original dataset (dataset weakness).", "labels": [], "entities": []}, {"text": "In others, the challenge might expose an inherent inability of a particular model family to handle certain natural language phenomena (model weakness).", "labels": [], "entities": []}, {"text": "These are, of course, not mutually exclusive.", "labels": [], "entities": []}, {"text": "We introduce inoculation by fine-tuning, anew method for analyzing the effects of challenge datasets ().", "labels": [], "entities": []}, {"text": "Given a model trained on the original dataset, we expose it to a small number of examples from the challenge dataset, allowing learning to continue.", "labels": [], "entities": []}, {"text": "To the extent that the weakness lies with the original dataset, then the inoculated model will perform well on both the original and challenge held-out data (Outcome 1 in.", "labels": [], "entities": []}, {"text": "If the weakness lies with the model, then inoculation will prove ineffective and the model's performance will remain unchanged (Outcome 2).", "labels": [], "entities": [{"text": "Outcome", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.6433507800102234}]}, {"text": "Inoculation can also decrease a model's performance on the original dataset (Outcome 3).", "labels": [], "entities": []}, {"text": "This case is not as clear as the first two, and could result from systematic differences between the original and challenge datasets, due to, e.g., predictive artifacts in either dataset (.", "labels": [], "entities": []}, {"text": "We apply our method to analyze six challenge datasets: the word overlap, negation, spelling errors, length mismatch and numerical reasoning NLI challenge datasets proposed by, as well as the Adversarial SQuAD reading comprehension challenge dataset.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.6407840698957443}, {"text": "NLI challenge datasets", "start_pos": 140, "end_pos": 162, "type": "DATASET", "confidence": 0.6136014958222707}, {"text": "Adversarial SQuAD reading comprehension challenge dataset", "start_pos": 191, "end_pos": 248, "type": "DATASET", "confidence": 0.6242714921633402}]}, {"text": "We analyze NLI datasets with the ESIM () and the decomposable attention () models, and reading comprehension with the BiDAF () and the QANet () models.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7083302736282349}, {"text": "BiDAF", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.8590818047523499}]}, {"text": "By fine-tuning on, in some cases, as few as 100 examples, both NLI models are able to recover almost the entire performance gap on both the word overlap and negation challenge datasets (Outcome 1).", "labels": [], "entities": []}, {"text": "In contrast, both models struggle to adapt to the spelling error and length mismatch challenge datasets (Outcome 2).", "labels": [], "entities": [{"text": "spelling error", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.743015706539154}]}, {"text": "On the numerical reasoning challenge dataset, both models close all of the gap using a small number of samples, but at the expense of performance on the original dataset (Outcome 3).", "labels": [], "entities": []}, {"text": "For Adversarial SQuAD, BiDAF closes 60% of the gap with minimal fine-tuning, but suffers a 7% decrease in original test set performance (Outcome 3).", "labels": [], "entities": [{"text": "Adversarial SQuAD", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.605420395731926}, {"text": "BiDAF", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9937531352043152}, {"text": "Outcome 3)", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.823454221089681}]}, {"text": "Our proposed analysis is broadly applicable, easy to perform, and task-agnostic.", "labels": [], "entities": []}, {"text": "By gaining a better understanding of how challenge datasets stress models, we can better tease apart limitations of datasets and limitations of models.", "labels": [], "entities": []}, {"text": "Inoculation evokes the idea that treatable diseases have different implications (for society and for the patient) than untreatable ones.", "labels": [], "entities": []}, {"text": "We differentiate the abstract process of inoculation from our way of executing it (fine-tuning) since it is easy to imagine alternative ways to inoculate a model.", "labels": [], "entities": []}], "datasetContent": [{"text": "To demonstrate the utility of our method, we apply it to analyze the NLI stress tests (Naik et al.,  We briefly describe the analyzed datasets, but refer readers to the original publications for details.", "labels": [], "entities": []}, {"text": "The word overlap challenge dataset is designed to exploit models' sensitivity to high lexical overlap in the premise and hypothesis by appending the tautology \"and true is true\" to the hypothesis.", "labels": [], "entities": []}, {"text": "The negation challenge dataset is based on the observation that negation words (e.g., \"no\", \"not\") cause the model to classify neutral or entailed statements as contradiction.", "labels": [], "entities": []}, {"text": "In this dataset, the tautology \"and false is not true\" is appended to the hypothesis sentence.", "labels": [], "entities": []}, {"text": "The spelling errors challenge dataset is designed to evaluate model robustness to noisy data in the form of misspellings.", "labels": [], "entities": []}, {"text": "The length mismatch challenge dataset is designed to exploit models' inability to handle examples with much longer premises than hypotheses.", "labels": [], "entities": []}, {"text": "In this dataset, the tautology \"and true is true\" is appended five times to the end of the premise.", "labels": [], "entities": []}, {"text": "Lastly, the numerical reasoning challenge dataset is designed to test models' ability to perform algebraic calculations, by introducing premise-hypothesis pairs containing numerical expressions.", "labels": [], "entities": []}, {"text": "Generating challenge training sets When varying the size of the challenge dataset train split used for fine-tuning, we subsample inclusively.", "labels": [], "entities": []}, {"text": "For example, the dataset used for fine-tuning on 5 examples is a subset of the dataset used for fine-tuning on 100 examples, which is a subset of the dataset used for fine-tuning on 1000 examples.", "labels": [], "entities": []}, {"text": "The word overlap, negation, spelling errors and length mismatch NLI challenge datasets, as well as Adversarial SQuAD, include splits for training and evaluation.", "labels": [], "entities": [{"text": "negation", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.9461838006973267}, {"text": "length mismatch NLI challenge datasets", "start_pos": 48, "end_pos": 86, "type": "DATASET", "confidence": 0.6320565938949585}]}, {"text": "To generate the datasets used for fine-tuning, we subsample 1000 random examples from each of the challenge dataset train splits.", "labels": [], "entities": [{"text": "fine-tuning", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9621647000312805}]}, {"text": "The evaluation splits are used as-is.", "labels": [], "entities": []}, {"text": "The numerical reasoning NLI challenge dataset is unsplit.", "labels": [], "entities": [{"text": "numerical reasoning NLI challenge dataset", "start_pos": 4, "end_pos": 45, "type": "DATASET", "confidence": 0.7098406314849853}]}, {"text": "As a result, we generate the datasets used for fine-tuning by subsampling 1000 random examples from the entirety of the challenge dataset, and use the remaining examples for evaluation.", "labels": [], "entities": []}, {"text": "Experimental details To train the ESIM model of, the decomposable attention model of, the BiDAF model of, and the QANet model of, we use the implementations in AllenNLP ().", "labels": [], "entities": [{"text": "AllenNLP", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9769206047058105}]}, {"text": "The models are trained with the same hyperparameters as described in their respective papers.", "labels": [], "entities": []}, {"text": "For each training dataset size, we tune the learning rate on the original development set accuracy; the learning rate is halved whenever validation performance (F 1 for SQuAD, accuracy for NLI) does not improve, and we employ early stopping with a patience of 5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9698742032051086}, {"text": "F 1", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.976786881685257}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9980922341346741}, {"text": "patience", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9685064554214478}]}, {"text": "This ensures that we are not implicitly using additional challenge dataset examples.", "labels": [], "entities": []}, {"text": "For each model and amount of challenge dataset examples used for fine-tuning, the reported challenge dataset performance is the performance of the learning rate configuration that yields the best challenge dataset performance.", "labels": [], "entities": []}, {"text": "We leave all other hyperparameters (such as the batch size and choice of optimizer) unchanged from the model's original training procedure.", "labels": [], "entities": []}, {"text": "For the Adversarial SQuAD experiments, we experiment with learning rates of 0.00001, 0.0001, (e) Performance of the ESIM and DA models (where the mismatched development set was used during training to control learning rate scheduling and early stopping) after fine-tuning on a variable number of numerical reasoning challenge dataset examples.", "labels": [], "entities": []}], "tableCaptions": []}