{"title": [{"text": "Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Sequence-to-sequence models for open-domain dialogue generation tend to favor generic, uninformative responses.", "labels": [], "entities": [{"text": "open-domain dialogue generation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.7080976168314616}]}, {"text": "Past work has focused on word frequency-based approaches to improving specificity, such as penalizing responses with only common words.", "labels": [], "entities": []}, {"text": "In this work, we examine whether specificity is solely a frequency-related notion and find that more linguistically-driven speci-ficity measures are better suited to improving response informativeness.", "labels": [], "entities": []}, {"text": "However, we find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics.", "labels": [], "entities": []}, {"text": "We rerank our model's outputs using externally-trained classifiers targeting each of these identified factors.", "labels": [], "entities": []}, {"text": "Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the pioneering work in machine translation (, sequence-tosequence (SEQ2SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7423998415470123}, {"text": "open-domain dialogue generation", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7172906597455343}, {"text": "single-turn generation", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.6998763531446457}]}, {"text": "However, SEQ2SEQ methods are known to favor universal responses, e.g., \"I don't know what you are talking about\" ().", "labels": [], "entities": []}, {"text": "These responses tend to be \"safe\" responses to many input queries, yet they usually fail to provide useful information.", "labels": [], "entities": []}, {"text": "One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (; . However, past work in sentence specificity-the \"quality of belonging or relating uniquely to a particular subject\" 1 -has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure ( and discourse context.", "labels": [], "entities": [{"text": "sentence specificity-the \"quality of belonging or relating uniquely to a particular subject\"", "start_pos": 239, "end_pos": 331, "type": "TASK", "confidence": 0.781399803502219}]}, {"text": "Frequency-based specificity also does not exactly capture \"the amount of information\" as an information-theoretic concept.", "labels": [], "entities": []}, {"text": "Hence, in dialogue generation, we can potentially make progress by incorporating more linguistically driven measures of specificity, as opposed to relying solely on frequency.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8212311863899231}]}, {"text": "We present a sequence-to-sequence dialogue model that factors out specificity and explicitly conditions on it when generating a response.", "labels": [], "entities": []}, {"text": "The decoder takes as input categorized values of several specificity metrics, embeds them, and uses them at each stage of decoding.", "labels": [], "entities": []}, {"text": "During training, the model can learn to associate different specificity levels with different types of responses.", "labels": [], "entities": []}, {"text": "At test time, we set the specificity level to its maximum value to force specific responses, which we found to be most beneficial.", "labels": [], "entities": []}, {"text": "We integrate linguistic (, information-theoretic, and frequency-based specificity metrics to better understand their roles in guiding response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7005094438791275}]}, {"text": "The second component of our model is designed to make the more specific responses more semantically plausible.", "labels": [], "entities": []}, {"text": "In particular, we found that forcing a SEQ2SEQ model to be more specific exposes problems with plausibility as illustrated in Table 1.", "labels": [], "entities": []}, {"text": "As sentences become more specific and contain more information, intra-response consistency i understand.", "labels": [], "entities": []}, {"text": "i am not sure if i can afford a babysitter, i am a millionaire Wrong connective i am an animal phobic, but i do not like animals Wrong pronoun my mom was asocial worker, he was an osteopath.", "labels": [], "entities": []}, {"text": "i work at anon profit organization that sells the holocaust.", "labels": [], "entities": []}, {"text": "Repeating my favorite food is italian, but i also love italian food, especially italian food.: Examples of different types of implausible responses on the PersonaChat dataset generated from our system that maximizes specificity only.", "labels": [], "entities": [{"text": "PersonaChat dataset generated", "start_pos": 155, "end_pos": 184, "type": "DATASET", "confidence": 0.8691165248552958}]}, {"text": "problems become evident, making the overall response implausible or unreasonable in real life.", "labels": [], "entities": []}, {"text": "Our inspection discovered that \u223c30% of specific responses suffer from a range of problems from semantic incompatibility to flawed discourse.", "labels": [], "entities": []}, {"text": "To improve the plausibility of responses, we propose a reranking method based on four external classifiers, each targeting a separate aspect of linguistic plausibility.", "labels": [], "entities": []}, {"text": "These classifiers are learned on synthetically generated examples, and attest time their responses are used to rerank proposed responses and mitigate the targeted issues.", "labels": [], "entities": []}, {"text": "Using both automatic and human evaluation, we find that linguistic-based specificity is more suitable than frequency-based specificity for generating informative and topically relevant responses, and learning from different types of specificity metrics leads to further improvement.", "labels": [], "entities": []}, {"text": "Our plausibility reranking method not only successfully improved the semantic plausibility of responses, but also improved their informativeness, relevance, and grammaticality.", "labels": [], "entities": []}, {"text": "Our system is available at https://git.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic evaluation of dialogue generation systems is a known challenge.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.796487033367157}]}, {"text": "Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (), ROUGE), ME-TEOR () and perplexity have poor correlations with human judgment ( or are modeldependent (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9986030459403992}, {"text": "ROUGE", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9910216927528381}, {"text": "ME-TEOR", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9023940563201904}]}, {"text": "Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present.", "labels": [], "entities": []}, {"text": "We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (, which evaluates response diversity.", "labels": [], "entities": []}, {"text": "They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (; (3) cosine similarity between input and response representations, which captures coherence.", "labels": [], "entities": []}, {"text": "We follow standards from prior work for human evaluation ().", "labels": [], "entities": []}, {"text": "We select 250 prompt-response pairs, and asked 5 judges from MechanicalTurk to rate the responses for each prompt.", "labels": [], "entities": [{"text": "MechanicalTurk", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9702045917510986}]}, {"text": "We evaluate whether the responses are informative () and on topic with the prompt, on a scale of 1-5.", "labels": [], "entities": []}, {"text": "In addition, we evaluate plausibility by asking judges whether they think the given response sentence without the prompt can reasonably be uttered, following instructions from.", "labels": [], "entities": []}, {"text": "The percentage of plausible ratings are reported.", "labels": [], "entities": []}, {"text": "Data We use two datasets in this work: (1) OpenSubtitles (, a collection of movie subtitles widely used in open-domain dialogue generation.", "labels": [], "entities": [{"text": "open-domain dialogue generation", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.6679604848225912}]}, {"text": "We sample 4,173,678 pairs for training and 5,000 pairs for testing from the movie subtitles dataset.", "labels": [], "entities": [{"text": "movie subtitles dataset", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.6211064954598745}]}, {"text": "Following, we remove all pairs with responses shorter than 5 words to improve the quality of the generated responses.", "labels": [], "entities": []}, {"text": "(2) PersonaChat (), a chit-chat dataset collected via crowdsourcing.", "labels": [], "entities": []}, {"text": "This is a multi-turn dataset, but we only consider single turn generation in this work.", "labels": [], "entities": []}, {"text": "We don't use the personas and false candidate replies.", "labels": [], "entities": []}, {"text": "There are 122,458 prompt-response pairs for training and 14,602 pairs for testing.", "labels": [], "entities": []}, {"text": "For validation, for reasons described in Section 5.1, we opt for human evaluation of overall response quality on a validation set of 60 prompt-response pairs from PersonaChat.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9686424136161804}]}, {"text": "Settings We use LSTMs with hidden layers of size 500, Adam optimizer ( with learning rate 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999, dropout rate 0.2 for both training and testing, metric embedding dimension 300 and 5 training epochs.", "labels": [], "entities": []}, {"text": "We train randomly initialized word embeddings of size 500 for the dialog model and use 300 dimentional GloVe () embeddings for reranking classifiers.", "labels": [], "entities": []}, {"text": "We generate 15 candidates for reranking per input sentence.", "labels": [], "entities": []}, {"text": "To train the 4 reranking classifiers, we use 375,996 positive sentences on Opensubtitles and 110,221 on PersonaChat.", "labels": [], "entities": []}, {"text": "We generate one negative sentence per word or phrase in the positive sentences.", "labels": [], "entities": []}, {"text": "Since specificity is the focus of this study, during testing, we use the embedding of the highest specificity level (5) for NIWF and the linguistically informed specificity predictor.", "labels": [], "entities": [{"text": "NIWF", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8098390698432922}]}, {"text": "For PPW, we observe that the perplexity of generated sentences does not increase beyond the median level (3) during development, hence we use the median level.", "labels": [], "entities": []}, {"text": "For comparison, we also report results when all metric levels are set to be the median (level 3).: Results for the base SEQ2SEQ model and each component.", "labels": [], "entities": []}, {"text": "(*) denotes significant improvement (p < 0.05 with paired bootstrap resampling) over our three baselines and benchmarks (Seq2seq, MMI-Anti, Zhang) according to paired bootstrap resampling.", "labels": [], "entities": []}, {"text": "Our full reranking model performed the best in informativeness and topic relevance.", "labels": [], "entities": []}, {"text": "A drop in plausibility is unavoidable as responses become more specific; the reranking model mitigates this.", "labels": [], "entities": []}, {"text": "Prompt (PersonaChat) thank you . also , the food is amazing . fries are the best thing ever . Seq2seq yes it is . do you have a favorite color ? mine is blue . Ours(Spec) yes , i am a vegetarian so i eat a lot of meat and bread puffs . Ours(Spec+Plaus) i love italian food , but i am a vegan so i eat a lot of curries MMI-Anti i agree . i am a vegetarian . Zhang i agree . i love to eat and eat .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the base SEQ2SEQ model and each component. (*) denotes significant improvement (p < 0.05  with paired bootstrap resampling) over our three baselines and benchmarks (Seq2seq, MMI-Anti, Zhang) according  to paired bootstrap resampling. Our full reranking model performed the best in informativeness and topic relevance.  A drop in plausibility is unavoidable as responses become more specific; the reranking model mitigates this.", "labels": [], "entities": []}, {"text": " Table 4: Effect of excluding each specificity metric on  PersonaChat. Delta against Ours(Spec) are included  in parenthesis and (*) denotes significant delta (p <  0.05). Excluding linguistically informed specificity led  to the greatest drop in informativeness and the slightest  increase in topic relevance.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of different reranking meth- ods on PersonaChat: training a single classifier, us- ing max/mean posterior from four classifiers, and us- ing CoLA. (*) denotes significant improvement over  Ours(Spec) (p < 0.05). Learning multiple classifiers  from synthetic data is the most effective.", "labels": [], "entities": [{"text": "CoLA", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.959200918674469}]}, {"text": " Table 6: Percentage of sentences judged grammatical  on OpenSubtitles.", "labels": [], "entities": []}]}