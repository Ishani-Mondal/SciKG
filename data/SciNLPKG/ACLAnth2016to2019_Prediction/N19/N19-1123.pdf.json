{"title": [{"text": "Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models", "labels": [], "entities": [{"text": "Rethinking Action Spaces", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9015337228775024}]}], "abstractContent": [{"text": "Defining action spaces for conversational agents and optimizing their decision-making process with reinforcement learning is an enduring challenge.", "labels": [], "entities": []}, {"text": "Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces.", "labels": [], "entities": []}, {"text": "Both have their own limitations.", "labels": [], "entities": []}, {"text": "This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as latent variables and develops unsupervised methods in order to induce its own action space from the data.", "labels": [], "entities": []}, {"text": "Comprehensive experiments are conducted examining both continuous and discrete action types and two different optimization methods based on stochastic variational inference.", "labels": [], "entities": []}, {"text": "Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs.", "labels": [], "entities": [{"text": "DealOrNoDeal", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.9016952514648438}]}, {"text": "Our detailed analysis also provides insights about various latent variable approaches for policy learning and can serve as a foundation for developing better latent actions in future research.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.7589788436889648}]}], "introductionContent": [{"text": "Optimizing dialog strategies in multi-turn dialog models is the cornerstone of building dialog systems that more efficiently solve real-world challenges, e.g. providing information), winning negotiations (, improving engagement () etc.", "labels": [], "entities": []}, {"text": "A classic solution employs reinforcement learning (RL) to learn a dialog policy that models the optimal action distribution conditioned on the dialog state (.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6758044481277465}]}, {"text": "However, since there are infinite human language possibilities, an enduring challenge has been to define what the action space is.", "labels": [], "entities": []}, {"text": "For traditional modular systems, the action space is defined by hand-crafted semantic representations such as dialog acts and slotvalues () and the goal is to obtain a dialog policy that chooses the best hand-crafted action at each dialog turn.", "labels": [], "entities": []}, {"text": "But it is limited because it can only handle simple domains whose entire action space can be captured by hand-crafted representations.", "labels": [], "entities": []}, {"text": "This cripples a system's ability to handle conversations in complex domains.", "labels": [], "entities": []}, {"text": "Conversely, end-to-end (E2E) dialog systems have removed this limit by directly learning a response generation model conditioned on the dialog context using neural networks (.", "labels": [], "entities": []}, {"text": "To apply RL to E2E systems, the action space is typically defined as the entire vocabulary; every response output word is considered to bean action selection step (), which we denote as the word-level RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9602622389793396}]}, {"text": "Word-level RL, however, has been shown to have several major limitations in learning dialog strategies.", "labels": [], "entities": [{"text": "Word-level RL", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.4105539321899414}]}, {"text": "The foremost one is that direct application of word-level RL leads to degenerate behavior: the response decoder deviates from human language and generates utterances that are incomprehensible (.", "labels": [], "entities": []}, {"text": "A second issue is that since a multi-turn dialog can easily span hundreds of words, word-level RL suffers from credit assignment over along horizon, leading to slow and suboptimal convergence (.", "labels": [], "entities": []}, {"text": "This paper proposes Latent Action Reinforcement Learning (LaRL), a novel framework that overcomes the limitations of word-level RL for E2E dialog models, marrying the benefits of a traditional modular approach in an unsupervised manner.", "labels": [], "entities": [{"text": "Latent Action Reinforcement Learning (LaRL)", "start_pos": 20, "end_pos": 63, "type": "TASK", "confidence": 0.7231742909976414}]}, {"text": "The key idea is to develop E2E models that can invent their own discourse-level ac-tions.", "labels": [], "entities": []}, {"text": "These actions must be expressive enough to capture response semantics in complex domains (i.e. have the capacity to represent a large number of actions), thus decoupling the discourselevel decision-making process from natural language generation.", "labels": [], "entities": []}, {"text": "Then any RL technique can be applied to this induced action space in the place of word-level output.", "labels": [], "entities": []}, {"text": "We propose a flexible latent variable dialog framework and investigate several approaches to inducing latent action space from natural conversational data.", "labels": [], "entities": []}, {"text": "We further propose (1) a novel training objective that outperforms the typical evidence lower bound used in dialog generation and (2) an attention mechanism for integrating discrete latent variables in the decoder to better model long responses.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.8528085350990295}]}, {"text": "We test this on two datasets, DealOrNoDeal ( and MultiWoz (), to answer two key questions: what are the advantages of LaRL over Word-level RL and what effective methods can induce this latent action space.", "labels": [], "entities": [{"text": "DealOrNoDeal", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8664782047271729}]}, {"text": "Results show that LaRL is significantly more effective than word-level RL for learning dialog policies and it does not lead to incomprehensible language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7813555300235748}]}, {"text": "Our models achieve 18.2% absolute improvement over the previous stateof-the-art on MultiWoz and discover novel and diverse negotiation strategies on DealOrNoDeal.", "labels": [], "entities": []}, {"text": "Besides strong empirical improvement, our model analysis reveals novel insights, e.g. it is crucial to reduce the exposure bias in the latent action space and discrete latent actions are more suitable than continuous ones to serve as action spaces for RL dialog agents.", "labels": [], "entities": [{"text": "RL dialog agents", "start_pos": 252, "end_pos": 268, "type": "TASK", "confidence": 0.8808749914169312}]}], "datasetContent": [{"text": "It is challenging to quantify the performance of RL-based neural generation systems because it is possible fora model to achieve high task reward and yet not generate human language ().", "labels": [], "entities": [{"text": "RL-based neural generation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8656371831893921}]}, {"text": "Therefore, we propose a novel measure, the Language Constrained Reward (LCR) curve as an additional robust measure.", "labels": [], "entities": [{"text": "Language Constrained Reward (LCR) curve", "start_pos": 43, "end_pos": 82, "type": "METRIC", "confidence": 0.7759149287428174}]}, {"text": "The basic idea is to use an ROC-style curve to visualize the tradeoff between achieving higher reward and being faithful to human language.", "labels": [], "entities": []}, {"text": "Specifically, at each checkpoint i over the course of RL training, we record two measures: (1) the PPL of a given model on the test data pi = PPL(\u03b8 i ) and (2) this model's average cumulative task reward in the test environment R ti . After RL training is complete, we create a 2D plot where the x-axis is the maximum PPL allowed, and the y-axis is the best achievable reward within the PPL budget in the testing environments: As a result, a perfect model should lie in the upper left corner whereas a model that sacrifices language quality for higher reward will lie in the lower right corner.", "labels": [], "entities": [{"text": "RL training", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9171929955482483}]}, {"text": "Our results will show that the LCR curve is an informative and robust measure for model comparison.", "labels": [], "entities": []}, {"text": "the RL training step, we set RL:SL=off for all latent action models, while the baseline word-level RL models are free to tune RL:SL for best performance.", "labels": [], "entities": []}, {"text": "For latent variable models, their perplexity is estimated via Monte Carlo p(x|c) \u2248 E p(z|c) [p(x|z)p(z|c)].", "labels": [], "entities": []}, {"text": "For the sake of clarity, this section only compares the best performing latent action models to the best performing word-level models and focuses on the differences between them.", "labels": [], "entities": []}, {"text": "A detailed comparison of the 6 latent space configurations is addressed in Section 7.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on DealOrNoDeal. Diversity is mea- sured by the number of unique responses the model  used in all scenarios from the test data.", "labels": [], "entities": []}, {"text": " Table 3: Main results on MultiWoz test set. RL models  are chosen based on performance on the validation set.", "labels": [], "entities": [{"text": "MultiWoz test set", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.821906586488088}]}, {"text": " Table 5: Comparison of 6 model variants with only su- pervised learning training.", "labels": [], "entities": []}, {"text": " Table 6: Best rewards in test environments on  DealOrNoDeal with various \u03b2.", "labels": [], "entities": [{"text": "DealOrNoDeal", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9272822737693787}]}, {"text": " Table 7: Training details for DealOrNoDeal experi- ments. Attn GRU refers to (Yang et al., 2016)", "labels": [], "entities": [{"text": "Attn GRU", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.5690242201089859}]}, {"text": " Table 8: Training details for MultiWoz experiments", "labels": [], "entities": []}, {"text": " Table 9: Example dialogs between baseline with the  user model. Agent is trained with word-level policy  gradient and the user is a supervised pre-trained model.", "labels": [], "entities": []}]}