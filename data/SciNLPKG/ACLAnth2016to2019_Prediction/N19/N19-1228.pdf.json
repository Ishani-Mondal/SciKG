{"title": [{"text": "Sequential Attention with Keyword Mask Model for Community-based Question Answering", "labels": [], "entities": [{"text": "Sequential Attention", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9030382037162781}, {"text": "Question Answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6833204627037048}]}], "abstractContent": [{"text": "In Community-based Question Answering system(CQA), Answer Selection(AS) is a critical task, which focuses on finding a suitable answer within a list of candidate answers.", "labels": [], "entities": [{"text": "Question Answering system(CQA)", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.8008864025274912}, {"text": "Answer Selection(AS)", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8227484583854675}]}, {"text": "For neural network models, the key issue is how to model the representations of QA text pairs and calculate the interactions between them.", "labels": [], "entities": []}, {"text": "We propose a Sequential Attention with Keyword Mask model(SAKM) for CQA to imitate human reading behavior.", "labels": [], "entities": []}, {"text": "Question and answer text regard each other as context within keyword-mask attention when encoding the representations, and repeat multiple times(hops) in a sequential style.", "labels": [], "entities": []}, {"text": "So the QA pairs capture features and information from both question text and answer text, interacting and improving vector representations iteratively through hops.", "labels": [], "entities": []}, {"text": "The flexibility of the model allows to extract meaningful keywords from the sentences and enhance diverse mutual information.", "labels": [], "entities": []}, {"text": "We perform on answer selection tasks and multi-level answer ranking tasks.", "labels": [], "entities": [{"text": "answer selection tasks", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9198383887608846}, {"text": "answer ranking tasks", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8008637030919393}]}, {"text": "Experiment results demonstrate the superiority of our proposed model on community-based QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.7105859220027924}]}], "introductionContent": [{"text": "Answering selection(AS) is one of the most fundamental challenges in community-based question answering(CQA) services.", "labels": [], "entities": [{"text": "Answering selection(AS)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9737642049789429}, {"text": "question answering(CQA)", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.8297532796859741}]}, {"text": "Given a question and a list of candidate answers, its aim is to choose the most matching one to the question.", "labels": [], "entities": []}, {"text": "During this process of matching questions and answer candidates, how to encode the question and answer(QA) into meaningful and semantic representations impacts on the results directly.", "labels": [], "entities": []}, {"text": "Earlier conventional statistic methods are normally based on feature engineering and resource toolkits.", "labels": [], "entities": []}, {"text": "Though these methods are easy in implementation, they require extra efforts and handcrafted features.", "labels": [], "entities": []}, {"text": "Recently, with the development of neural network, deep learning based models attract much attention in various tasks().", "labels": [], "entities": []}, {"text": "In question answering field, the convolutional neural networks(CNNs)( and recurrent neural networks(RNNs)( are widely employed to convert the question and answer text into vectors and define a feed-forward multi-layer perceptron to compute the interactions between them.", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8777060806751251}]}, {"text": "These models construct sentences in an end-toend fashion with less manual involvement.", "labels": [], "entities": []}, {"text": "To capture fine-grained features, on the one hand, some works are concerned with matching QA pairs relationship in a more complex and diverse way, e.g., CNTN( and MV-LSTM().", "labels": [], "entities": []}, {"text": "On the other hand, latent representation models aim to jointly learn lexical and semantic information from QA sentences and influence the vector generation directly, e.g., attention mechanism(.", "labels": [], "entities": []}, {"text": "Attention mechanism learns attention weights of each words pairs between QA sentences.", "labels": [], "entities": []}, {"text": "Afterwards it can calculate the weighted sum of hidden states overall time steps.", "labels": [], "entities": []}, {"text": "This approach has shown promising results, while challenges still exist.", "labels": [], "entities": []}, {"text": "For example, questions and answers in CQA services are generally long sentences, as such it is still difficult to compress all information into a fixed-length vector.", "labels": [], "entities": []}, {"text": "To solve this problem, proposes co-attention view which brings improvement, and  further proposes a two-step attention to build dynamic question vectors based on various answer words.", "labels": [], "entities": []}, {"text": "These kinds of methods usually require more parameters to learn representations.", "labels": [], "entities": []}, {"text": "More importantly, when computing attention weights, every words in QA pairs are involved.", "labels": [], "entities": []}, {"text": "This word-to-word pattern takes meaningless noise into consideration, such as informal language usage or text irrelevant to the question.", "labels": [], "entities": []}, {"text": "To alleviate this problem, proposes a context-aligned model to align phrase in QA relying on overlapped words and Stanford Core NLP tools . Inspired by coattention, we extend it to sequential style to learn better representations and try to extract useful keywords with less parameters and resource toolkits.", "labels": [], "entities": [{"text": "Stanford Core NLP", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.8386533260345459}]}, {"text": "In this work, we propose a Sequential Attention with Keyword Mask(SAKM) model for answer selection task.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.938414454460144}]}, {"text": "We encode sentences similar to human reading behavior.", "labels": [], "entities": []}, {"text": "When generating the question, our model refers to the answer and combines the mutual information.", "labels": [], "entities": []}, {"text": "It is the same processing for producing answer representations.", "labels": [], "entities": []}, {"text": "So when encoding a question, answer text is used as context and vice versa.", "labels": [], "entities": []}, {"text": "We term this co-attention view as one \"hop\".", "labels": [], "entities": []}, {"text": "Afterwards we repeat this process several times(hops) in a sequential style.", "labels": [], "entities": []}, {"text": "As such QA pairs review each other recurrently to remind of mutual information and refine the sentence representations to be more precise across hops.", "labels": [], "entities": []}, {"text": "Besides, the Keyword Mask modifies the attention mechanism such that the attention is computed over keywords instead of all words in the QA pair.", "labels": [], "entities": []}, {"text": "So only keywords in the long context are extracted at each time step.", "labels": [], "entities": []}, {"text": "The contributions in this paper are three folders: 1) We extend attention mechanism to sequential structure, so the question and answer review each other recurrently to improve the sentence representations.", "labels": [], "entities": []}, {"text": "2) Different from standard soft attention, we propose sequential attention with keyword mask(SAKM) model.", "labels": [], "entities": []}, {"text": "Besides, our model focuses on the significant words and filters other meaningless data.", "labels": [], "entities": []}, {"text": "3) We analyse the proposed SAKM model not only on classical answer selection tasks and but also multi-level answer ranking tasks.", "labels": [], "entities": [{"text": "answer selection tasks", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8577036062876383}, {"text": "answer ranking tasks", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7456237276395162}]}, {"text": "Experiment results show that our model tends to encode more rich semantic representations with less parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we test the proposed model on classical answer selection task and also multi-level answer ranking task to validate the model's effectiveness 2 .", "labels": [], "entities": [{"text": "answer selection", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.8227978348731995}]}], "tableCaptions": [{"text": " Table 2: Experiment results on YahooCQA. SA is the  sequential attention baseline without keyword-mask  operation. SAKM is our sequential attention with key- word mask model.", "labels": [], "entities": [{"text": "YahooCQA", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9624414443969727}]}, {"text": " Table 3: Experiment results on ZhihuCQA.", "labels": [], "entities": []}, {"text": " Table 5: Experiment results on YahooCQA using vari- ous representations. SAKM hop 1 denotes that SAKM  model tests with the sentence representations of the first  hop.", "labels": [], "entities": [{"text": "YahooCQA", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9581719040870667}]}]}