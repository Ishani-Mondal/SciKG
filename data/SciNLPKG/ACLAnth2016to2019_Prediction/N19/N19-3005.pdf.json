{"title": [{"text": "Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling", "labels": [], "entities": [{"text": "Low-Resource Sequence Labeling", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.6468789974848429}]}], "abstractContent": [{"text": "In this paper, we address the problem of effectively self-training neural networks in a low-resource setting.", "labels": [], "entities": []}, {"text": "Self-training is frequently used to automatically increase the amount of training data.", "labels": [], "entities": []}, {"text": "However, in a low-resource scenario , it is less effective due to unreliable annotations created using self-labeling of unlabeled data.", "labels": [], "entities": []}, {"text": "We propose to combine self-training with noise handling on the self-labeled data.", "labels": [], "entities": []}, {"text": "Directly estimating noise on the combined clean training set and self-labeled data can lead to corruption of the clean data and hence, performs worse.", "labels": [], "entities": []}, {"text": "Thus, we propose the Clean and Noisy Label Neural Network which trains on clean and noisy self-labeled data simultaneously by explicitly modelling clean and noisy labels separately.", "labels": [], "entities": []}, {"text": "In our experiments on Chunking and NER, this approach performs more robustly than the baselines.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9210011959075928}]}, {"text": "Complementary to this explicit approach, noise can also be handled implicitly with the help of an auxiliary learning task.", "labels": [], "entities": []}, {"text": "To such a complementary approach, our method is more beneficial than other baseline methods and together provides the best performance overall.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many low-resource languages or domains, only small amounts of labeled data exist.", "labels": [], "entities": []}, {"text": "Raw or unlabeled data, on the other hand, is usually available even in these scenarios.", "labels": [], "entities": []}, {"text": "Automatic annotation or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like humangenerated lexica which might not be available in a low-resource context.", "labels": [], "entities": []}, {"text": "Self-training is a popular technique to automatically label additional text.", "labels": [], "entities": []}, {"text": "There, a classifier is trained on a small amount of labeled data and then used to obtain labels for \u00a7 This work was started while the authors were at Saarland University.", "labels": [], "entities": []}, {"text": "However, this can lead to unreliable or noisy labels on the additional data which impede the learning process (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on overcoming this slowdown of self-training.", "labels": [], "entities": []}, {"text": "Hence, we propose to apply noisereduction techniques during self-training to clean the self-labeled data and learn effectively in a lowresource scenario.", "labels": [], "entities": []}, {"text": "Inspired by the improvements shown by the Noisy Label Neural Network), we can directly apply NLNN to the combined set of the existing clean data and the noisy self-labeled data.", "labels": [], "entities": []}, {"text": "However, such an application can be detrimental to the learning process (Section 6).", "labels": [], "entities": []}, {"text": "Thus, we introduce the Clean and Noisy Label Neural Network (CNLNN) that treats the clean and noisy data separately while training on them simultaneously (Section 3).", "labels": [], "entities": []}, {"text": "This approach leads to two advantages over NLNN (Section 6 and 7) when evaluating on two sequence-labeling tasks, Chunking and Named Entity Recognition.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6141255100568136}]}, {"text": "Firstly, when adding noisy data, CNLNN is robust showing consistent improvements over the regular neural network, whereas NLNN can lead to degradation in performance.", "labels": [], "entities": []}, {"text": "Secondly, when combining with an indirect-noise handling technique, i.e. with an auxiliary target in a multi-task fashion, CNLNN complements better than NLNN in the multi-task setup and overall leads to the best performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate CNLNN and other methods on a Chunking and a Named Entity Recognition (NER) task with F 1 -score as the metric in each case.", "labels": [], "entities": [{"text": "Chunking and a Named Entity Recognition (NER) task", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.6804481267929077}, {"text": "F 1 -score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9725405722856522}]}, {"text": "For Chunking, we use the same data splits as (S\u00f8gaard and Goldberg, 2016) based on the English Penn Treebank dataset).", "labels": [], "entities": [{"text": "English Penn Treebank dataset", "start_pos": 87, "end_pos": 116, "type": "DATASET", "confidence": 0.8400560021400452}]}, {"text": "For NER, the data splits of the English CoNLL 2003 task are used).", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7209634780883789}, {"text": "English CoNLL 2003 task", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.8283451199531555}]}, {"text": "Note that in our NER setup, we evaluate using BIO-2 labels, so F 1 -scores reported below might not be comparable to prior work.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9425753951072693}, {"text": "F 1 -scores", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9701574593782425}]}, {"text": "To mimic a low resource setting, we limit each training set to the first 10k tokens.", "labels": [], "entities": []}, {"text": "The development sets are randomly chosen sentences from the original training set restricted to 1k tokens.", "labels": [], "entities": []}, {"text": "The test sets remain unchanged.", "labels": [], "entities": []}, {"text": "For the rest of the training data, the original labels are removed and the words are automatically labeled using the baseline model.", "labels": [], "entities": []}, {"text": "We add variable amounts of this automatically-annotated data for self-training in our experiments.", "labels": [], "entities": []}], "tableCaptions": []}