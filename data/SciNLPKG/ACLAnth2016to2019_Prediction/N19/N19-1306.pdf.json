{"title": [{"text": "Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks", "labels": [], "entities": [{"text": "Long-tail Relation Extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6512537797292074}]}], "abstractContent": [{"text": "We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings.", "labels": [], "entities": [{"text": "distance supervised relation extraction", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.6216392666101456}]}, {"text": "Here, the challenge is to learn accurate \"few-shot\" models for classes existing at the tail of the class distribution, for which little data is available.", "labels": [], "entities": []}, {"text": "Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail.", "labels": [], "entities": []}, {"text": "First, we propose to leverage implicit relational knowledge among class labels from knowledge graph em-beddings and learn explicit relational knowledge using graph convolution networks.", "labels": [], "entities": []}, {"text": "Second , we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8503216207027435}]}, {"text": "We demonstrate our results fora large-scale benchmark dataset which show that our approach significantly outperforms other base-lines, especially for long-tail relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction (RE) is an important task in information extraction, aiming to extract the relation between two given entities based on their related context.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8849610805511474}, {"text": "information extraction", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.857316792011261}]}, {"text": "Due to the capability of extracting textual information and benefiting many NLP applications (e.g., information retrieval, dialog generation, and question answering), RE appeals to many researchers.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.8050641715526581}, {"text": "dialog generation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8299325406551361}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8934210240840912}, {"text": "RE", "start_pos": 167, "end_pos": 169, "type": "TASK", "confidence": 0.9676541090011597}]}, {"text": "Conventional supervised models have been widely explored in this task (); however, their performance heavily depends on the scale and quality of training data.", "labels": [], "entities": []}, {"text": "\u2020 Alibaba-Zhejiang University Frontier Technology Research Center To construct large-scale data,) proposed a novel distant supervision (DS) mechanism to automatically label training instances by aligning existing knowledge graphs (KGs) with text.", "labels": [], "entities": [{"text": "Alibaba-Zhejiang University Frontier Technology Research Center", "start_pos": 2, "end_pos": 65, "type": "DATASET", "confidence": 0.9464005927244822}]}, {"text": "DS enables RE models to work on large-scale training corpora and has thus become a primary approach for RE recently (.", "labels": [], "entities": [{"text": "RE", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9544141292572021}, {"text": "RE", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9898634552955627}]}, {"text": "Although these DS models achieve promising results on common relations, their performance still degrades dramatically when there are only a few training instances for some relations.", "labels": [], "entities": []}, {"text": "Empirically, DS can automatically annotate adequate amounts of training data; however, this data usually only covers a limited part of the relations.", "labels": [], "entities": []}, {"text": "Many relations are long-tail and still suffer from data deficiency.", "labels": [], "entities": []}, {"text": "Current DS models ignore the problem of long-tail relations, which makes it challenging to extract comprehensive information from plain text.", "labels": [], "entities": []}, {"text": "Long-tail relations are important and cannot be ignored.", "labels": [], "entities": []}, {"text": "Nearly 70% of the relations are longtail in the widely used New York Times (NYT) dataset 1 () as shown in.", "labels": [], "entities": [{"text": "New York Times (NYT) dataset 1", "start_pos": 60, "end_pos": 90, "type": "DATASET", "confidence": 0.7510075867176056}]}, {"text": "Therefore, it is crucial for mod-els to be able to extract relations with limited numbers of training instances.", "labels": [], "entities": []}, {"text": "Dealing with long tails is very difficult as few training examples are available.", "labels": [], "entities": []}, {"text": "Therefore, it is natural to transfer knowledge from data-rich and semantically similar head classes to data-poor tail classes ( . For example, the long-tail relation /people/deceased person/place of burial and head relation /people/deceased person/place of death are in the same branch /people/deceased person/* as shown in.", "labels": [], "entities": [{"text": "head relation /people/deceased person/place of death", "start_pos": 210, "end_pos": 262, "type": "TASK", "confidence": 0.6394889056682587}]}, {"text": "They are semantically similar, and it is beneficial to leverage head relational knowledge and transfer it to the long-tail relation, thus enhancing general performance.", "labels": [], "entities": []}, {"text": "In other words, long-tail relations of one entity tuple can have class ties with head relations, which can be leveraged to enhance RE for narrowing potential search spaces and reducing uncertainties between relations when predicting unknown relations.", "labels": [], "entities": [{"text": "RE", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.6789506077766418}]}, {"text": "If one pair of entities contains /people/deceased person/place of death, there is a high probability that it will contain /people/deceased person/place of burial.", "labels": [], "entities": []}, {"text": "If we can incorporate the relational knowledge between two relations, extracting head relations will provide evidence for the prediction of long-tail relations.", "labels": [], "entities": []}, {"text": "However, there exist two problems: (1) Learning relational knowledge: Semantically similar classes may contain more relational information that will boost transfer, whereas irrelevant classes (e.g., /location/location/contains and /people/family/country) usually contain less relational information that may result in negative transfer.", "labels": [], "entities": []}, {"text": "(2) Leveraging relational knowledge: Integrating relational knowledge to existing RE models is challenging.", "labels": [], "entities": [{"text": "Leveraging relational knowledge", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8421699206034342}]}, {"text": "To address the problem of learning relational knowledge, as shown in (, we use class embeddings to represent relation classes and utilize KG embeddings and graph convolution networks (GCNs) to extract implicit and explicit relational knowledge.", "labels": [], "entities": []}, {"text": "Specifically, previous studies (  have shown that the embeddings of semantically similar relations are located near each other in the latent space.", "labels": [], "entities": []}, {"text": "For instance, the relation /people/person/place lived and /people/person/nationality are more relevant, whereas the relation /people/person/profession has less correlation with the former two relations.", "labels": [], "entities": []}, {"text": "Thus, it, whose filmmaking collaboration with james ivory created a genre of films with visually sumptuous settings that told literate tales of individuals trying to adapt to shifting societal values , died yesterday in a hospital , an actor with hundreds of television , movie and theatrical credits to his name , died on saturday in . the night the news hit that had committed suicide at his home in , colo.", "labels": [], "entities": []}, {"text": ", i drove to my office and read a few of the letters we had exchanged over the years . is natural to leverage this knowledge from KGs.", "labels": [], "entities": []}, {"text": "However, because there are many one-to-multiple relations in KGs, the relevant information for each class maybe scattered.", "labels": [], "entities": []}, {"text": "In other words, there may not be enough relational signal between classes.", "labels": [], "entities": []}, {"text": "Therefore, we utilize GCNs to learn explicit relational knowledge.", "labels": [], "entities": []}, {"text": "To address the problem of leveraging relational knowledge, we first use convolution neural networks ( to encode sentences; then introduce coarse-to-fine knowledgeaware attention mechanism for combining relational knowledge with encoded sentences into bag representation vectors.", "labels": [], "entities": []}, {"text": "The relational knowledge not only provides more information for relation prediction but also provides a better reference message for the attention module to raise the performance of long-tail classes.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.9076841175556183}]}, {"text": "Our experimental results on the NYT dataset show that: (1) our model is effective compared to baselines especially for long-tail relations; (2) leveraging relational knowledge enhances RE and our model is efficient in learning relational knowledge via GCNs.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9680670499801636}, {"text": "RE", "start_pos": 185, "end_pos": 187, "type": "METRIC", "confidence": 0.8444890379905701}]}], "datasetContent": [{"text": "We evaluate our models on the NYT dataset developed by (, which has been widely used in recent studies (.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8587053418159485}]}, {"text": "The dataset has 53 relations including the N A relation, which indicates that the relations of instances are not available.", "labels": [], "entities": []}, {"text": "The training set has 522611 sentences, 281270 entity pairs, and 18252 relational facts.", "labels": [], "entities": []}, {"text": "In the test set, there are 172448 sentences, 96678 entity pairs, and 1950 relational facts.", "labels": [], "entities": []}, {"text": "In both training and test set, we truncate sentences with more than 120 words into 120 words.", "labels": [], "entities": []}, {"text": "We evaluate all models in the held-out evaluation.", "labels": [], "entities": []}, {"text": "It evaluates models by comparing the relational facts discovered from the test articles with those in Freebase and provides an approximate measure of precision without human evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9991317391395569}]}, {"text": "For evaluation, we draw precision-recall curves for all models.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9990972280502319}]}, {"text": "To further verify the effect of our model for long-tails, we follow previous studies) to report the Precision@N results.", "labels": [], "entities": [{"text": "Precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.993586003780365}]}, {"text": "The dataset and baseline code can be found on Github 3 .  To evaluate the performance of our proposed model, we compare the precision-recall curves of our model with various previous RE models.", "labels": [], "entities": [{"text": "Github 3", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.8902581930160522}, {"text": "precision-recall", "start_pos": 124, "end_pos": 140, "type": "METRIC", "confidence": 0.9925988912582397}]}, {"text": "The evaluation results are shown in and.", "labels": [], "entities": []}, {"text": "We report the results of neural architectures including CNN and PCNN with various attention based methods: +KATT denotes our approach, +HATT is the hierarchical attention method), +ATT is the plain selective attention method over instances (, +ATT+ADV is the denoising attention method by adding a small adversarial perturbation to instance embeddings (, and +ATT+SL is the attention-based model using soft-labeling method to mitigate the side effect of the wrong labeling problem at entitypair level ( ).", "labels": [], "entities": []}, {"text": "We also compare our method with feature-based models, including), MultiR (Hoffmann et al., 2011) and MIML ().", "labels": [], "entities": []}, {"text": "As shown in both figures, our approach achieves the best results among all attention-based models.", "labels": [], "entities": []}, {"text": "Even when compared with PCNN+HATT, PCNN+ATT+ADV, and PCNN+ATT+SL, which adopt sophisticated denoising schemes and extra information, our model is still more advantageous.", "labels": [], "entities": []}, {"text": "This indicates that our method can take advantage of the rich correlations between relations through KGs and GCNs, which improve the performance.", "labels": [], "entities": []}, {"text": "We believe the performance of our model can be further improved by adopting additional mechanisms like adversarial training, and reinforcement learning, which will be part of our future work.", "labels": [], "entities": []}, {"text": "To further demonstrate the improvements in performance for long-tail relations, following the study by) we extract a subset of the test dataset in which all the relations have  fewer than 100/200 training instances.", "labels": [], "entities": []}, {"text": "We employ the Hits@K metric for evaluation.", "labels": [], "entities": []}, {"text": "For each entity pair, the evaluation requires its corresponding golden relation in the first K candidate relations recommended by the models.", "labels": [], "entities": []}, {"text": "Because it is difficult for the existing models to extract long-tail relations, we select K from {10,15,20}.", "labels": [], "entities": []}, {"text": "We report the macro average Hits@K accuracies for these subsets because the micro-average score generally overlooks the influences of long-tails.", "labels": [], "entities": [{"text": "macro average Hits@K accuracies", "start_pos": 14, "end_pos": 45, "type": "METRIC", "confidence": 0.804403136173884}]}, {"text": "From the results shown in, we observe that for both CNN and PCNN models, our model outperforms the plain attention model and the HATT model.", "labels": [], "entities": []}, {"text": "Although our KATT method has achieved better results for long-tail relations as compared to both plain ATT method and HATT method, the results of all these methods are still far from satisfactory.", "labels": [], "entities": []}, {"text": "This indicates that distantly supervised RE models still suffer from the long-tail relation problem, which may require additional schemes and extra information to solve this problem in the future.", "labels": [], "entities": [{"text": "RE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.931003749370575}]}], "tableCaptions": [{"text": " Table 1: Accuracy (%) of Hits@K on relations with  training instances fewer than 100/200.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989830851554871}]}, {"text": " Table 2: Results of ablation study with PCNN.", "labels": [], "entities": [{"text": "PCNN", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9076625108718872}]}]}