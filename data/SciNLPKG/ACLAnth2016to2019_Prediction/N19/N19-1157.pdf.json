{"title": [{"text": "Quantifying the morphosyntactic content of Brown Clusters", "labels": [], "entities": []}], "abstractContent": [{"text": "Brown and Exchange word clusters have long been successfully used as word representations in Natural Language Processing (NLP) systems.", "labels": [], "entities": []}, {"text": "Their success has been attributed to their seeming ability to represent both semantic and syntactic information.", "labels": [], "entities": []}, {"text": "Using corpora representing several language families, we test the hypothesis that Brown and Exchange word clusters are highly effective at encoding mor-phosyntactic information.", "labels": [], "entities": []}, {"text": "Our experiments show that word clusters are highly capable of distinguishing Parts of Speech.", "labels": [], "entities": [{"text": "distinguishing Parts of Speech", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8527898043394089}]}, {"text": "We show that increases in Average Mutual Information, the clustering algorithms' optimization goal, are highly correlated with improvements in encoding of morphosyntactic information.", "labels": [], "entities": [{"text": "Average Mutual Information", "start_pos": 26, "end_pos": 52, "type": "METRIC", "confidence": 0.8515425523122152}]}, {"text": "Our results provide empirical evidence that downstream NLP systems addressing tasks dependent on morphosyntactic information can benefit from word cluster features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributionally generated word classes (often referred to as word clusters) are hard clusters, containing all word types observed in a corpus, allocated to clusters based on contextual information observed in the corpus.", "labels": [], "entities": [{"text": "Distributionally generated word classes (often referred to as word clusters) are hard clusters, containing all word types observed in a corpus, allocated to clusters based on contextual information observed in the corpus", "start_pos": 0, "end_pos": 220, "type": "Description", "confidence": 0.7663041626413664}]}, {"text": "They have found wide use in Natural Language Processing (NLP) systems as an alternative to word embeddings such as word2vec (.", "labels": [], "entities": []}, {"text": "Word clusters differentiate themselves from word embeddings by requiring estimation of many fewer parameters, and by their ability to derive qualitative representations from smaller corpora ().", "labels": [], "entities": []}, {"text": "Brown Clusters () area wellknown approach based on hard, hierarchical, distributionally derived groups of word types observed in a corpus of unstructured text, with Average Mutual Information (AMI) as the optimization goal.", "labels": [], "entities": [{"text": "Average Mutual Information (AMI)", "start_pos": 165, "end_pos": 197, "type": "METRIC", "confidence": 0.8888451755046844}]}, {"text": "Exchange Clusters are an alternative approach obtained by applying the Exchange Algorithm ( to the same optimization goal.", "labels": [], "entities": []}, {"text": "Unlike Brown, Exchange outputs a flat clustering, with no hierarchy (.", "labels": [], "entities": []}, {"text": "When only the bottom of the hierarchy is used, like in this paper, Exchange and Brown clusters are interchangeable.", "labels": [], "entities": []}, {"text": "Both Brown and Exchange clusters have been used as word representations for various Natural Language Processing tasks such as Part of Speech tagging in clean and noisy text, dependency parsing (), Chinese Word Segmentation (, and Named Entity Recognition ().", "labels": [], "entities": [{"text": "Part of Speech tagging", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8252929598093033}, {"text": "dependency parsing", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.8559031784534454}, {"text": "Chinese Word Segmentation", "start_pos": 197, "end_pos": 222, "type": "TASK", "confidence": 0.6252659956614176}, {"text": "Named Entity Recognition", "start_pos": 230, "end_pos": 254, "type": "TASK", "confidence": 0.6158579488595327}]}, {"text": "Word clusters distinguish themselves from word embedding models by their ability to learn from little data (; for example, in cases like (), word clusters outperform other kinds of representations, including word embeddings.", "labels": [], "entities": []}, {"text": "In the literature, it is often observed that word clusters seem to encode a considerable amount of morphosyntactic and semantic knowledge.", "labels": [], "entities": []}, {"text": "However, it has not yet been studied to which extent such knowledge is encoded, as previous work on Brown and Exchange clusters focuses mostly on algorithmic improvements and on applications to different NLP tasks.", "labels": [], "entities": []}, {"text": "In this work, we present a principled study of the morphosyntactic information encoded in flat word clusters induced exclusively from classbased language models via Brown Clustering and Exchange algorithm.", "labels": [], "entities": []}, {"text": "In particular, we focus on how well these approaches derive clusters that represent Parts of Speech as a measure of the morphosyntactic information encoded.", "labels": [], "entities": []}, {"text": "We find that Brown and Exchange clusters are highly effective at representing morphosyntactic information, even when hyper-parameters are set such that they match only the number of Parts of Speech, thereby grouping into relatively few word clusters only.", "labels": [], "entities": []}, {"text": "Our results provide empirical evidence for the observed performance gains when including Brown and Exchange word clusters as features in NLP systems that rely on morphosyntactic information.", "labels": [], "entities": []}, {"text": "Furthermore, we find that there is a strong correlation between the optimization goal of Brown clustering and the Exchange Algorithm (i.e., Average Mutual Information), and performance at Parts of Speech separation, which again confirms the appropriateness of choosing AMI in word clustering for morphosyntactic information.", "labels": [], "entities": [{"text": "Parts of Speech separation", "start_pos": 188, "end_pos": 214, "type": "TASK", "confidence": 0.6300620138645172}, {"text": "word clustering", "start_pos": 276, "end_pos": 291, "type": "TASK", "confidence": 0.7049926072359085}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview of data sets", "labels": [], "entities": [{"text": "Overview of data sets", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.702410601079464}]}, {"text": " Table 2: Correlation between Average Mutual Infor- mation and PoS purity of the clustering resulted from  Exchange with k = 18. Words with frequency < 5  have been filtered. p < 0.01 for all coefficients.", "labels": [], "entities": [{"text": "Average Mutual Infor- mation", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.8643536925315857}, {"text": "PoS purity", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.8859605193138123}]}, {"text": " Table 3: Correlation between Average Mutual Infor- mation and AdjMI of the clustering resulted from Ex- change with k = 18. Words with frequency < 5 have  been filtered. p < 0.01 for all coefficients.", "labels": [], "entities": [{"text": "Average Mutual Infor- mation", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.9153500318527221}, {"text": "AdjMI", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9716209769248962}, {"text": "Ex- change", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.970930278301239}]}, {"text": " Table 4: Percentage of vocabulary with multiple PoS  tags. Values are calculated relative to the vocabulary  remaining after application of threshold.", "labels": [], "entities": []}]}