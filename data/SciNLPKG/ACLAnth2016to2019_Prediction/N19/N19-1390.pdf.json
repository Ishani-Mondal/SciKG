{"title": [{"text": "Combining Discourse Markers and Cross-lingual Embeddings for Synonym-Antonym Classification", "labels": [], "entities": []}], "abstractContent": [{"text": "It is well-known that distributional semantic approaches have difficulty in distinguishing between synonyms and antonyms (Grefen-stette, 1992; Pad\u00f3 and Lapata, 2003).", "labels": [], "entities": []}, {"text": "Recent work has shown that supervision available in English for this task (e.g., lexical resources) can be transferred to other languages via cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "However, this kind of transfer misses monolingual distributional information available in a target language, such as contrast relations that are indicative of antonymy (e.g., hot\u2026while\u2026cold).", "labels": [], "entities": []}, {"text": "In this work, we improve the transfer by exploiting monolingual information, expressed in the form of co-occurrences with discourse markers that convey contrast.", "labels": [], "entities": []}, {"text": "Our approach makes use of less than a dozen markers, which can easily be obtained for many languages.", "labels": [], "entities": []}, {"text": "Compared to a baseline using only cross-lingual embed-dings, we show absolute improvements of 4-10% F 1-score in Vietnamese and Hindi.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9853447079658508}]}], "introductionContent": [{"text": "Recent work has shown that monolingual word embeddings in different languages can be aligned in an unsupervised manner).", "labels": [], "entities": []}, {"text": "The resulting cross-lingual embeddings can be used to share supervision for lexical classification tasks across languages, when annotated data is not available in one language.", "labels": [], "entities": []}, {"text": "For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (.", "labels": [], "entities": []}, {"text": "However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language.", "labels": [], "entities": []}, {"text": "In this paper, we consider one lexical classification task, namely the distinction between synonyms and antonyms, which is important co-occurrences with discourse markers Figure 1: Supervision for distinguishing antonyms from synonyms can be derived using discourse markers.", "labels": [], "entities": []}, {"text": "Here, antonyms available in English (denoted by solid edge between hot and cold) are translated to German via cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "Using co-occurrences with discourse markers indicative of antonymy (shown in box), we can identify pairs of words in the n-best translations (clouds) that are also antonymous (e.g., dashed edge between kalt and hei\u00df).", "labels": [], "entities": []}, {"text": "for downstream applications such as contradiction detection ( and machine translation).", "labels": [], "entities": [{"text": "contradiction detection", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8187492787837982}, {"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8073495030403137}]}, {"text": "To facilitate better transfer, we propose to use monolingual information in the form of word co-occurrences in contrast relations, in addition to cross-lingual embeddings (see).", "labels": [], "entities": []}, {"text": "In particular, we utilize the fact that discourse markers conveying contrast are more likely to be surrounded by antonyms than synonyms (e.g., hot\u2026while\u2026cold), as shown by Roth and Schulte im.", "labels": [], "entities": []}, {"text": "Our analysis reveals that (1) fine-grained semantic information that is required to distinguish synonyms from antonyms is insufficiently preserved cross-lingually in word embeddings, but (2) such information can be recovered (at least partially) by relying on linguistic intuitions about contrast relations in discourse.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments test the performance of an offthe-shelf system for lexical relation classification under different cross-lingual settings.", "labels": [], "entities": [{"text": "lexical relation classification", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.655247151851654}]}, {"text": "In particular, we evaluate the performance of unsupervised cross-lingual word embeddings and assess the benefits of translation and re-ranking, taking into account word co-occurrences in contrast relations.", "labels": [], "entities": [{"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9753550887107849}]}, {"text": "For our experiments, we use four languages: English, German, Hindi and Vietnamese.", "labels": [], "entities": []}, {"text": "The antonymy and synonymy dataset by is used for training and estimating an upper bound in English.", "labels": [], "entities": []}, {"text": "For crosslingual development and hyper-parameter selection, we use the German dataset by.", "labels": [], "entities": [{"text": "hyper-parameter selection", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7824033498764038}, {"text": "German dataset", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9553104341030121}]}, {"text": "Specifically, we select the number of hidden layers {0, 1}, learning rate {0.001,\u2026,0.1}, dropout rate {0.0, 0.5}, and whether to include information on paths between words {yes, no}.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9197728633880615}]}, {"text": "As examples of under-resourced languages, we evaluate on the Vietnamese dataset ViCon by and on anew Hindi dataset, which we crawled from hindi2dictionary.com.", "labels": [], "entities": [{"text": "Vietnamese dataset ViCon", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8742586175600687}, {"text": "Hindi dataset", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.7112403213977814}]}, {"text": "Note that annotated data in these languages is held out exclusively for testing.", "labels": [], "entities": []}, {"text": "The task addressed here is to distinguish synonymous from antonymous words.", "labels": [], "entities": []}, {"text": "Accordingly, we consider only word pairs for training and testing that are marked as antonyms or synonyms, even if the original dataset also contains unrelated words or other relations.", "labels": [], "entities": []}, {"text": "Statistics of all considered word pairs, the ratio between synonyms and antonyms, as well as sizes of the text corpora used in our experiments are given in.", "labels": [], "entities": []}, {"text": "We test our proposed approach against two baselines: NoTrans and BestTrans.", "labels": [], "entities": [{"text": "NoTrans", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9232607483863831}, {"text": "BestTrans", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.8935952186584473}]}, {"text": "NoTrans simply uses the training data in English and allows us to test in how far semantic properties related to the distinction between synonymy and antonymy are preserved cross-lingually.", "labels": [], "entities": [{"text": "NoTrans", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8800964951515198}]}, {"text": "BestTrans uses 1 st best translations of the English training data in a target language, as described in Section 3.1.", "labels": [], "entities": [{"text": "BestTrans", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9351441860198975}]}, {"text": "Our own approach, henceforth called LingTrans, is based on n-best translations and exploits word co-occurrences in contrast relations for reranking, as described in Section 3.2.", "labels": [], "entities": []}, {"text": "All three models take cross-lingual word embeddings as input.", "labels": [], "entities": []}, {"text": "We created these as follows.", "labels": [], "entities": []}, {"text": "First, the unsupervised morphological analyzer of is used to lemmatize the monolingual corpora for the respective languages-the German Wikipedia, the Vietnamese portion of the LORELEI pack (, and the HindEnCorp corpus ().", "labels": [], "entities": [{"text": "LORELEI", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.8849024772644043}, {"text": "HindEnCorp corpus", "start_pos": 200, "end_pos": 217, "type": "DATASET", "confidence": 0.9697853922843933}]}, {"text": "This step ensures that we can compare the (stems of) LexNet-crosslingual   word tokens in text to those of the word forms that actually appear in the synonymy/antonymy datasets.", "labels": [], "entities": [{"text": "LexNet-crosslingual   word tokens", "start_pos": 53, "end_pos": 86, "type": "DATASET", "confidence": 0.8748817443847656}]}, {"text": "Note that, while we use an unsupervised morphological analyzer, a stemmer can also be used, if available for that language.", "labels": [], "entities": []}, {"text": "Next, we created monolingual embeddings for each language using fastText ().", "labels": [], "entities": []}, {"text": "Finally, we applied the unsupervised variant of vecmap () to compute alignments and cross-lingual mappings.", "labels": [], "entities": []}, {"text": "The word and discourse marker co-occurrence counts required for our approach are computed on the same monolingual corpora used for training monolingual embeddings.", "labels": [], "entities": []}, {"text": "shows macro-averaged F 1 -scores for crosslingual synonymy/antonymy distinction (top part) as well as monolingual results in English for comparison (bottom part).", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9423429220914841}]}, {"text": "The monolingual results show that word embeddings provide appropriate information for classification inmost instances, achieving an F 1 -score of 70.1.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9888520836830139}]}, {"text": "The comparatively low cross-lingual results by NoTrans indicate that aligning and mapping embedding spaces does not preserve all semantic properties relevant to the distinction between synonymy and antonymy.", "labels": [], "entities": [{"text": "NoTrans", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9158870577812195}]}, {"text": "The use of first-best translations in BestTrans alleviates this issue partially and improves F 1 by up to 4.6 points.", "labels": [], "entities": [{"text": "BestTrans", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9168573021888733}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.987544447183609}]}, {"text": "Our intuition regarding discourse cues in LingTrans leads to further improvements of up to 9.9 additional points in F 1 and considerably closes the gap between monolingual and cross-lingual results (56.8\u221264.2 vs. 70.1 F 1 ).", "labels": [], "entities": [{"text": "F 1", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9625851511955261}]}, {"text": "Based on an approximate randomization test over the respective test items), we find the improvements of our proposed approach LingTrans over NoTrans to be significant in Vietnamese (p<0.01), German and Hindi (p<0.1).", "labels": [], "entities": []}, {"text": "In contrast, there is no significant difference in performance between BestTrans and NoTrans, confirming that both translation and reranking are required to achieve consistent gains.", "labels": [], "entities": [{"text": "BestTrans", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.955880343914032}, {"text": "NoTrans", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8504336476325989}, {"text": "translation", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.944908857345581}]}], "tableCaptions": [{"text": " Table 1: Data statistics of the classification datasets as  well as the corpora used to create word representations.  Note that the data used here consist of only synonyms  and antonyms (approximately in equal proportions).", "labels": [], "entities": []}, {"text": " Table 2: Macro-averaged F 1 -scores for crosslingual  synonymy/antonymy distinction in German (de), Hindi  (hi), and Vietnamese (vi). Significant differences  from NoTrans are marked by asterisk(s) (* p<0.1,  ** p<0.01). Monolingual results in English (en) are  only shown for comparison.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9397676885128021}, {"text": "crosslingual  synonymy/antonymy distinction", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.6506091058254242}, {"text": "NoTrans", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.8949604034423828}]}]}