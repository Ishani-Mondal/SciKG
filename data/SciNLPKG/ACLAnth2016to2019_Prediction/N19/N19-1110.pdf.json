{"title": [{"text": "Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings", "labels": [], "entities": [{"text": "Cross-Topic Distributional Semantic Representations", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.569627657532692}]}], "abstractContent": [{"text": "In traditional Distributional Semantic Models (DSMs) the multiple senses of a polyse-mous word are conflated into a single vector space representation.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.6064953625202179}]}, {"text": "In this work, we propose a DSM that learns multiple distributional representations of a word based on different topics.", "labels": [], "entities": []}, {"text": "First, a separate DSM is trained for each topic and then each of the topic-based DSMs is aligned to a common vector space.", "labels": [], "entities": []}, {"text": "Our unsupervised mapping approach is motivated by the hypothesis that words preserving their relative distances in different topic semantic sub-spaces constitute robust semantic anchors that define the mappings between them.", "labels": [], "entities": []}, {"text": "Aligned cross-topic representations achieve state-of-the-art results for the task of contextual word similarity.", "labels": [], "entities": [{"text": "contextual word similarity", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6221376160780588}]}, {"text": "Furthermore, evaluation on NLP downstream tasks shows that multiple topic-based embeddings outperform single-prototype models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-level representation learning algorithms adopt the distributional hypothesis, presuming a correlation between the distributional and the semantic relationships of words.", "labels": [], "entities": [{"text": "Word-level representation learning", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7370778222878774}]}, {"text": "Typically, these models encode the contextual information of words into dense feature vectors-often referred to as embeddings-of a k-dimensional space, thus creating a Vector Space Model (VSM) of lexical semantics.", "labels": [], "entities": []}, {"text": "Such embeddings have been successfully applied to various natural language processing applications, including information retrieval (, sentiment analysis (, and machine translation (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.838171511888504}, {"text": "sentiment analysis", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.9401485919952393}, {"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8199255466461182}]}, {"text": "Despite their popularity, traditional DSMs rely solely on models where each word is uniquely represented by one point in the vector space.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.9596455097198486}]}, {"text": "From a * The research was performed when the author was an undergraduate researcher at School of ECE, NTUA in Athens, linguistic perspective, these models cannot capture the distinct meanings of polysemous words (e.g., bank or cancer), resulting in conflated word representations of diverse contextual semantics.", "labels": [], "entities": []}, {"text": "To alleviate this problem, DSMs with multiple representations per word have been proposed in the literature, based on clustering local contexts of individual words).", "labels": [], "entities": []}, {"text": "An alternative way to train multiple representation DSMs is to utilize semantic lexical resources (., based on the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words.", "labels": [], "entities": []}, {"text": "Their model performs well fora variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common semantic space.", "labels": [], "entities": []}, {"text": "The problem of defining transformations between embeddings-trained independently under different corpora-has been previously examined in various works, such as machine translation, induction of historical embeddings and lexical resources enrichment (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.8652889430522919}, {"text": "lexical resources enrichment", "start_pos": 220, "end_pos": 248, "type": "TASK", "confidence": 0.6204062898953756}]}, {"text": "Following this line of research, we induce the creation of multiple cross-topic word embeddings by projecting the semantic representations of topic-based DSMs to a unified semantic space.", "labels": [], "entities": []}, {"text": "We investigate different ways to perform the mappings from the topic sub-spaces to the unified semantic space, and propose a completely unsupervised approach to extract semantic anchors that define those mappings.", "labels": [], "entities": []}, {"text": "Furthermore, we claim that polysemous words change their meaning in different topic domains; this is reflected in rela-tive shifts of their distributional representations in different topic-based DSMs.", "labels": [], "entities": []}, {"text": "On the other hand, semantic anchors should have consistent semantic relationships regardless of the domain they reside in.", "labels": [], "entities": []}, {"text": "Hence, their distributions of similarity values should also be similar across different domains.", "labels": [], "entities": []}, {"text": "Finally, we apply a smoothing technique to each word's set of topic embeddings, resulting in representations with fine-grained semantics.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first time that mappings between semantic spaces are applied to the problem of learning multiple embeddings for polysemous words.", "labels": [], "entities": []}, {"text": "Our multi-topic word representations are evaluated on the contextual semantic similarity task and yield state-of-the-art performance compared to other unsupervised multi-prototype word embedding approaches.", "labels": [], "entities": []}, {"text": "We further perform experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word representations consistently provide higher performance than single-prototype word embedding models.", "labels": [], "entities": [{"text": "text classification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.806244969367981}, {"text": "paraphrase identification", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.8933857083320618}]}, {"text": "The code of the present work is publicly available 1 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance comparison between differ- ent state-of-the-art approaches on SCWS, in terms of  Spearman's correlation. UTDSM refers to the pro- jected cross-topic representation, UTDSM Random  refers to the case when random words served as an- chors and GMM (c) corresponds to GMM smoothing  with c components.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results of multi-class text classifi- cation.", "labels": [], "entities": []}, {"text": " Table 2. We observe that our model per- forms better than the baseline across all metrics  for both averaging approaches (AvgC D , Avg D ),  while the usage of dominant topics appears to have  lower performance (MaxC D ). Specifically, we get  an improvement of 2 \u2212 2.5% on topic-based aver- age and 0.5 \u2212 1% on simple average combination  compared to using Global-DSM.", "labels": [], "entities": [{"text": "Avg D )", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.940406084060669}, {"text": "Global-DSM", "start_pos": 359, "end_pos": 369, "type": "DATASET", "confidence": 0.9389463663101196}]}, {"text": " Table 3: Evaluation results on paraphrase detection  task.", "labels": [], "entities": [{"text": "paraphrase detection  task", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.923902690410614}]}]}