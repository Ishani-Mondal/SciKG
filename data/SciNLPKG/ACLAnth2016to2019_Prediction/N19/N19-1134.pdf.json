{"title": [{"text": "Adaptation of Hierarchical Structured Models for Speech Act Recognition in Asynchronous Conversation", "labels": [], "entities": [{"text": "Speech Act Recognition", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6208977301915487}]}], "abstractContent": [{"text": "We address the problem of speech act recognition (SAR) in asynchronous conversations (forums, emails).", "labels": [], "entities": [{"text": "speech act recognition (SAR) in asynchronous conversations (forums, emails)", "start_pos": 26, "end_pos": 101, "type": "TASK", "confidence": 0.8159080445766449}]}, {"text": "Unlike synchronous conversations (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective SAR model.", "labels": [], "entities": [{"text": "SAR", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9859550595283508}]}, {"text": "In this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains.", "labels": [], "entities": []}, {"text": "We carryout our research in three main steps.", "labels": [], "entities": []}, {"text": "First, we introduce a neural architecture based on hierarchical LSTMs and conditional random fields (CRF) for SAR, and show that our method outperforms existing methods when trained on in-domain data only.", "labels": [], "entities": [{"text": "SAR", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9785529375076294}]}, {"text": "Second, we improve our initial SAR models by semi-supervised learning in the form of pretrained word embeddings learned from a large unla-beled conversational corpus.", "labels": [], "entities": [{"text": "SAR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9874927997589111}]}, {"text": "Finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly mod-eling the distributional shift in two domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the ever-increasing popularity of Internet and mobile technologies, communication media like emails and forums have become an integral part of people's daily life where they discuss events, issues and experiences.", "labels": [], "entities": []}, {"text": "Participants interact with each other asynchronously in these media by writing at different times, generating a type of conversational discourse that is different from synchronous conversations such as meeting and phone conversations ().", "labels": [], "entities": []}, {"text": "In the course of the interactions, the participants perform certain communicative acts like asking questions, requesting information, or suggesting something, which are known as speech acts (Austin, * All authors contibuted equally.", "labels": [], "entities": []}, {"text": "C1:hoping to do the XinJiang Tibet Highway.", "labels": [], "entities": []}, {"text": "Am hoping to hire a 4-wheel drive.", "labels": [], "entities": [{"text": "Am", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.934640645980835}]}, {"text": "I know the roads are bad and I would need an experienced driver and guide -any recommendations?", "labels": [], "entities": []}, {"text": "C2:I never done this routine,however i been to Xinjiang twice,in my opinion the local people not friendly, not safe to do this.", "labels": [], "entities": []}, {"text": "I still have relative stay in Xinjiang, however don't know what they can offer for help...", "labels": [], "entities": []}, {"text": "C3:I'm not sure if travelling overland from Xinjiang to Tibet is officially legal yet.", "labels": [], "entities": []}, {"text": "You might want to post your question on the NorthEast Asia branch of Lonely Planet's ThornTree forum for more (useful) answers.", "labels": [], "entities": [{"text": "NorthEast Asia branch of Lonely Planet's ThornTree forum", "start_pos": 44, "end_pos": 100, "type": "DATASET", "confidence": 0.9212048980924818}]}, {"text": "[Suggestion] C4:a frend and i are trying this route as well, we will likely be in urumuqi and northern part of xinjiang from 8th apr to end apr; looking at doing the xin jiang tibet highway from end apr.", "labels": [], "entities": []}, {"text": "(truncated) contact meat if you want to hookup for possible transport sharing cheers.", "labels": [], "entities": []}, {"text": "Figure 1: Example of speech acts in a forum thread.", "labels": [], "entities": []}, {"text": "For example, consider the forum conversation in.", "labels": [], "entities": []}, {"text": "The participant who posted the initial comment C 1 , describes his situation and asks a couple of questions.", "labels": [], "entities": []}, {"text": "Other participants respond to the initial post with more information and provide suggestions.", "labels": [], "entities": []}, {"text": "In this process, the participants get into a conversation by taking turns, each of which consists of one or more speech acts.", "labels": [], "entities": []}, {"text": "Speech act recognition (SAR) is an important step towards deep conversational analysis, and can benefit many downstream applications.", "labels": [], "entities": [{"text": "Speech act recognition (SAR)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7727427979310354}, {"text": "deep conversational analysis", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6139740645885468}]}, {"text": "Availability of large labeled datasets such as the Switchboard-DAMSL (SWBD) () and the Meeting Recorder Dialog Act (MRDA) () corpora has fostered research in data-driven SAR methods in synchronous domains.", "labels": [], "entities": [{"text": "Meeting Recorder Dialog Act (MRDA)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.6342910400458744}, {"text": "SAR", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.9476872086524963}]}, {"text": "However, such large corpora are not available in the asynchronous domains, and many of the existing (small-sized) ones use task-specific tagsets as opposed to a standard one.", "labels": [], "entities": []}, {"text": "The unavailability of large annotated datasets with standard tagsets is one of the main reasons for SAR not getting much attention in asynchronous domains, and it is often quite expensive to annotate such datasets for each domain of interest.", "labels": [], "entities": [{"text": "SAR", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9764063954353333}]}, {"text": "SAR methods proposed before the neural 'tsunami', e.g.,, used mostly bag-of-ngram representation (e.g., unigram, bigram) of a sentence, and most of these methods disregard conversational dependencies (discourse structure) between sentences.", "labels": [], "entities": []}, {"text": "Recently, proposed a neural-CRF framework for SAR in forum conversations.", "labels": [], "entities": [{"text": "SAR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9899521470069885}]}, {"text": "In their approach, a bi-LSTM (trained on the SAR task) first encodes the sentences separately into task-specific embeddings, which are then used in a separate CRF model to capture the conversational dependencies between sentences.", "labels": [], "entities": []}, {"text": "They also use labeled data from the MRDA meeting corpus, without which their LSTMs perform worse than simple feed-forward networks.", "labels": [], "entities": [{"text": "MRDA meeting corpus", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8167612353960673}]}, {"text": "Although their method attempts to model sentence structure (using LSTM) and conversational dependencies (using CRF), the approach has several limitations.", "labels": [], "entities": []}, {"text": "First, the LSTM-CRF framework was disjoint, and thus cannot be trained end-to-end.", "labels": [], "entities": []}, {"text": "Second, when using the MRDA meeting data, their method simply concatenates it with the target domain data assuming they have the same distribution.", "labels": [], "entities": [{"text": "MRDA meeting data", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.7710511088371277}]}, {"text": "However, asynchronous domains (forum, email) differ from synchronous (MRDA) in their underlying conversational structure, in style (spoken vs. written), and in vocabulary usage (meetings on some focused agenda vs. conversations on any topic of interests in a public forum).", "labels": [], "entities": []}, {"text": "Therefore, we hypothesize that to make the best use of labeled data from synchronous domains, one needs to model the shift in domains.", "labels": [], "entities": []}, {"text": "In this work, we advance the state-of-the-art of SAR in asynchronous conversations in three main steps.", "labels": [], "entities": [{"text": "SAR", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.987285315990448}]}, {"text": "First, we introduce an end-to-end neural architecture based on a hierarchical LSTM encoder with a Softmax or CRF output layer.", "labels": [], "entities": []}, {"text": "Second, we improve our initial SAR model by semisupervised learning in the form of word embeddings learned from a large unlabeled conversational corpus.", "labels": [], "entities": [{"text": "SAR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9884231686592102}]}, {"text": "Most importantly, we adapt our hierarchical LSTM encoder using domain adversarial training ( to leverage the labeled data from synchronous domains by explicitly modeling the shift in the two domains.", "labels": [], "entities": []}, {"text": "We evaluate our models on three different asynchronous datasets containing forum and email conversations, and on the MRDA meeting corpus.", "labels": [], "entities": [{"text": "MRDA meeting corpus", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.8518013954162598}]}, {"text": "Our main findings are: (i) the hierarchical LSTMs outperform existing methods when trained on in-domain data for both synchronous and asynchronous domains, setting anew stateof-the-art; (ii) conversational word embeddings yield significant improvements over off-the-shelf ones; and (iii) domain adversarial training improves the results by inducing domain-invariant features.", "labels": [], "entities": []}, {"text": "The source code, the conversational word embeddings, and the datasets are available at https://ntunlpsg.github.io/ demo/project/speech-act/.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned, asynchronous domains lack large corpora that are annotated with a standard speech act tagset.", "labels": [], "entities": []}, {"text": "annotated sentences in TripAdvisor (TA) forum threads with the standard 12 act types defined in MRDA.", "labels": [], "entities": []}, {"text": "They also remapped the BC3 email corpus ( according to these tags.", "labels": [], "entities": [{"text": "BC3 email corpus", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.9318162004152933}]}, {"text": "Subsequent studies ( We use these three asynchronous datasets in our experiments.", "labels": [], "entities": []}, {"text": "For our experiments on synchronous domains, we use the MRDA meeting corpus that was also used in related studies.", "labels": [], "entities": [{"text": "MRDA meeting corpus", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8572885394096375}]}, {"text": "show some basic statistics of the datasets and the tag distributions.", "labels": [], "entities": []}, {"text": "Note that the tagset used by us and other related studies in asynchronous (written) conversation is different from the one used in synchronous spoken conversations ().", "labels": [], "entities": []}, {"text": "The later tagset contains acts like backchannel, filter and disruption that are more specific to speech.", "labels": [], "entities": []}, {"text": "The train-dev-test splits of the asynchronous datasets are done uniformly at random at the conversation level.", "labels": [], "entities": []}, {"text": "Since the asynchronous datasets are quite small in size, to have a reliable test set, we create the train:test splits with an equal number of conversations.", "labels": [], "entities": []}, {"text": "also created conversation level datasets to train and test their CRF models.", "labels": [], "entities": []}, {"text": "Their test sets however contain only 20% of the conversations, providing only 5 conversations for QC3 and BC3, and 20 for TA.", "labels": [], "entities": [{"text": "QC3", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8861370086669922}]}, {"text": "Our experiments on these small test sets showed unstable results for all the models.", "labels": [], "entities": []}, {"text": "Therefore, we use a larger test set (50%), and we report more general results on the whole corpus based on 2-fold cross-validation, where the second fold was    created by interchanging the train and test splits in.", "labels": [], "entities": []}, {"text": "The same development set was used to tune the hyperparameters of the models for experiments on each fold.", "labels": [], "entities": []}, {"text": "For experiments on MRDA, we use the same train:test:dev split as in (; Joty and Hoque, 2016).", "labels": [], "entities": [{"text": "MRDA", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9263372421264648}]}, {"text": "We followed similar preprocessing steps as; specifically: normalize all characters to lowercase, spell out digits and URLs, and tokenize the texts using TweetNLP (  ison, we use accuracy and macro-F 1 . Like other related studies, we consider macro-F 1 as the main metric (more appropriate when class distributions are imbalanced), and select our model based on the best F 1 on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9992893934249878}]}, {"text": "Due to space limitations, we report only macro-F 1 here.", "labels": [], "entities": []}, {"text": "Please refer to the Appendix for the accuracy numbers.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9909398555755615}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996170997619629}]}, {"text": "We first evaluate our base models on in-domain datasets by comparing with state-of-the-art models.", "labels": [], "entities": []}, {"text": "In the next subsection, we evaluate our adaptation method in the three adaptation scenarios.", "labels": [], "entities": []}, {"text": "To validate the efficacy of our model, we compare it with two baselines: a Support Vector Machine (SVM) and a feed-forward network (FFN).", "labels": [], "entities": []}, {"text": "In one setting, we use the concatenated word vectors as the input sentence representation, while in another, we use the pretrained skipthought vectors ().", "labels": [], "entities": []}, {"text": "We also compare our models with the bi-LSTM (B-LSTM) model of Joty and Hoque (2016) and the stacked LSTM (S-LSTM) of.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer () with a learning rate of 0.001, and use dropout to avoid over-fitting.", "labels": [], "entities": []}, {"text": "We use the Xavier initializer to initialize the weights, and uniform U(\u22120.05, 0.05) to initialize the word vectors randomly.", "labels": [], "entities": []}, {"text": "For pretrained word embeddings, we experiment with off-theshelf embeddings that come with Glove as well as with our conversational word embeddings.", "labels": [], "entities": []}, {"text": "For both random and pretrained initialization, we finetune our word embeddings on the SAR task.", "labels": [], "entities": [{"text": "SAR task", "start_pos": 86, "end_pos": 94, "type": "TASK", "confidence": 0.9027876257896423}]}, {"text": "We construct sequences from the chronological order of the sentences in a conversation.", "labels": [], "entities": []}, {"text": "Since MRDA conversations are much longer compared to those in asynchronous domains sentences in), we split the MRDA conversations into smaller parts containing a maximum of 100 sentences.", "labels": [], "entities": [{"text": "MRDA conversations", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8447923064231873}, {"text": "MRDA conversations", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.6672773063182831}]}, {"text": "The number of epochs and batch size were fixed to 30 and 5 (conversations), respectively.", "labels": [], "entities": []}, {"text": "We ran each experiment 5 times, each time with a different random seed, and report the average of the (2-fold\u00d75=10) runs along with the standard deviation.", "labels": [], "entities": []}, {"text": "Recently, show that the main source of variability in results for neural models come from the random seed, and the author has recommended to report the distribution of results from a range of seeds.", "labels": [], "entities": []}, {"text": "We present the results in.", "labels": [], "entities": []}, {"text": "From the first block of results, we notice that both SVM and FFN baselines perform poorly compared to other models that tune the word embeddings and learn the sentence representation on the SAR task.", "labels": [], "entities": [{"text": "SAR task", "start_pos": 190, "end_pos": 198, "type": "TASK", "confidence": 0.8992415368556976}]}, {"text": "The second block contains five LSTM variants: (i) B-LSTM rand , referring to bi-LSTM with random initialization; (ii) B-LSTM gl , referring to bi-LSTM initialized with off-the-shelf Glove embeddings; (iii) B-GRU c-gl , referring to bidirectional Gated Recurrent Unit () initialized with our conversational Glove; (iv) B-LSTM c-gl , referring to bi-LSTM initialized with conversational Glove, and (v) S-LSTM c-gl , referring to a 2-layer stacked LSTM with conversational Glove.", "labels": [], "entities": []}, {"text": "From the results, we can make the following conclusions.", "labels": [], "entities": []}, {"text": "First, B-LSTM rand overfits extremely on the asynchronous datasets, giving the worst results among the LSTMs.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9132605791091919}]}, {"text": "Second, pretrained vectors help to achieve better results, however, compared to the off-the-shelf vectors, our conversational word vectors yield much higher F 1 , especially, in the asynchronous datasets that are smaller in size (5 -11% absolute gains).", "labels": [], "entities": [{"text": "F 1", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9971126019954681}]}, {"text": "This demonstrates that pretrained word embeddings provide an effective method to perform semi-supervised learning, when they are learned from relevant datasets.", "labels": [], "entities": []}, {"text": "The last block shows the results of our models.", "labels": [], "entities": []}, {"text": "It is evident that both H-LSTM and H-LSTM-CRF outperform other models in all the datasets except QC3 where the difference is very small.", "labels": [], "entities": [{"text": "QC3", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9703031182289124}]}, {"text": "They also give the best F 1 reported so far on MRDA, outperforming the B-LSTM models of and S-LSTM model of.", "labels": [], "entities": [{"text": "F 1", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9908660650253296}, {"text": "MRDA", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.5977063179016113}]}, {"text": "When we compare the two models, we notice that H-LSTM outperforms H-LSTM-CRF in all the datasets.", "labels": [], "entities": []}, {"text": "A reason for this could be that the contextual dependency is already captured by the upper LSTM layer and the data maybe too small for the CRF to offer anything more.", "labels": [], "entities": []}, {"text": "We compare our adversarial adaptation method with three baseline methods: Transfer, Merge and Fine-tune.", "labels": [], "entities": [{"text": "Transfer", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.9557929635047913}, {"text": "Merge", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.8566064238548279}, {"text": "Fine-tune", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9496335387229919}]}, {"text": "Transfer models are trained on the source (MRDA) and tested on the target (QC3, TA, BC3).", "labels": [], "entities": []}, {"text": "Our adversarial unsupervised adaptation method is comparable to the transfer method as they use labeled data only from the source domain.", "labels": [], "entities": []}, {"text": "In Merge, models are trained on the concatenated training set of source and target datasets.", "labels": [], "entities": []}, {"text": "Fine-tune is a widely used adaptation method for neural models (.", "labels": [], "entities": []}, {"text": "In this method, we first train a model on the source domain until convergence, then we finetune it on the target by training it further on the target dataset.", "labels": [], "entities": []}, {"text": "Both merge and fine-tune are comparable to our semi-supervised/supervised adaptation as these methods use labeled data from the target domain.", "labels": [], "entities": []}, {"text": "For semi-supervised experiments, we take smaller subsets (e.g., 25%, 50%, and 75% of the labeled data) from the target domain.", "labels": [], "entities": []}, {"text": "We also compare our method with Neural SCL (, which is another domain adaption method in the neural framework.", "labels": [], "entities": [{"text": "Neural SCL", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.5932956039905548}]}, {"text": "We used the implementation made available by the authors.", "labels": [], "entities": []}, {"text": "For training our adaptation models, we use SGD (Algorithm 1 in the Appendix) with a momentum term of 0.9 and a dynamic learning rate as suggested by.", "labels": [], "entities": []}, {"text": "The adaptation results are shown in Table 6.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9509573578834534}]}, {"text": "We observe that without any labeled data from the target (Unsup.", "labels": [], "entities": []}, {"text": "adap), our adversarial adapted models (Adv-H-LSTM, Adv-H-LSTM-CRF) perform worse than the transfer baseline in all three datasets.", "labels": [], "entities": []}, {"text": "In this case, since the out-ofdomain labeled dataset (MRDA) is much larger, it overwhelms the model inducing features that are not relevant for the task in the target domain.", "labels": [], "entities": []}, {"text": "However, when we provide the models with some labeled in-domain examples in the semisupervised (50%) setting, we observe about 11% absolute gains in QC3 and BC3 over the corresponding Merge baselines, and 7 -8% gains over the corresponding Fine-tune baselines.", "labels": [], "entities": [{"text": "Merge baselines", "start_pos": 184, "end_pos": 199, "type": "DATASET", "confidence": 0.8759939074516296}]}, {"text": "As we add more target labels (100%), performance of our adapted models (Sup.", "labels": [], "entities": []}, {"text": "adap) improve further, yielding sizable improvements (\u223c 3% absolute) over the corresponding baselines in all datasets.", "labels": [], "entities": []}, {"text": "Also notice that our adversarial adaptation outperforms Merge and Fine-tune methods for all models overall datasets, showing its effectiveness.", "labels": [], "entities": [{"text": "Fine-tune", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.918712317943573}]}, {"text": "presents the F 1 scores of our adapted models with varying amount of labeled data in the target domain.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9690042535463969}]}, {"text": "We notice that the largest improvements for all three datasets come from the first 25% of the target labels.", "labels": [], "entities": []}, {"text": "The gains from the second quartile are also relatively higher than the last two quartiles for TA and BC3.", "labels": [], "entities": []}, {"text": "Another interesting observation is that H-LSTM-CRF performs better in unsupervised and semi-supervised settings (i.e., with less target labels).", "labels": [], "entities": []}, {"text": "In other words, H-LSTM-CRF adapts better than H-LSTM with small target datasets by exploiting the tag dependencies in the source.", "labels": [], "entities": []}, {"text": "As we include more labeled data from the target, H-LSTM catches up with H-LSTM-CRF.", "labels": [], "entities": []}, {"text": "Surprisingly, Neural SCL performs the worst.", "labels": [], "entities": [{"text": "Neural SCL", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.6348124593496323}]}, {"text": "We suspect this is due to the mismatches between pivot features of the source and target domains.", "labels": [], "entities": []}, {"text": "If we compare our adaptation results with the in-domain results in, we notice that using the same amount of labeled data in the target, our supervised adaptation gives 3-4% gains across the datasets.", "labels": [], "entities": []}, {"text": "Our semi-supervised adaptation using half of the target labels (50%) also outperforms the in-domain models that use all the target labels.", "labels": [], "entities": []}, {"text": "To further analyze the cases where our adapted models make a difference, shows the confusion matrices for the adapted H-LSTM and the non-adapted H-LSTM on the concatenated testsets of QC3, TA, and BC3.", "labels": [], "entities": [{"text": "QC3", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9521028399467468}]}, {"text": "In general, our classifiers get confused between Response and Statement, and between Suggestion and Statement the most.", "labels": [], "entities": [{"text": "Suggestion and Statement", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.7317314942677816}]}, {"text": "We noticed similar phenomena in the human annotations, where annotators had difficulties with these three acts.", "labels": [], "entities": []}, {"text": "It is however noticeable that the adapted H-LSTM is less affected by class imbalance, and it can detect the Suggestion and Polite acts more correctly than the non-adapted one.", "labels": [], "entities": [{"text": "Suggestion", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.6222304701805115}]}], "tableCaptions": [{"text": " Table 1: Basic statistics about our corpora.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of speech acts in our corpora.", "labels": [], "entities": [{"text": "Distribution of speech acts", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8739802837371826}]}, {"text": " Table 3: Train, dev. and test sets for the datasets. Num- bers in parentheses indicate the number of sentences.", "labels": [], "entities": [{"text": "Num- bers", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8591242829958597}]}, {"text": " Table 4: Datasets and their statistics used for training  the conversational word embeddings.", "labels": [], "entities": []}, {"text": " Table 5: Macro-F 1 scores for in-domain training.", "labels": [], "entities": []}]}