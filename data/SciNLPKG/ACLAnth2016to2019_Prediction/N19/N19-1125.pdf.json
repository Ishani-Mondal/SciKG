{"title": [{"text": "Jointly Optimizing Diversity and Relevance in Neural Response Generation", "labels": [], "entities": [{"text": "Neural Response Generation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.8006306290626526}]}], "abstractContent": [{"text": "Although recent neural conversation models have shown great potential, they often generate bland and generic responses.", "labels": [], "entities": []}, {"text": "While various approaches have been explored to diversify the output of the conversation model, the improvement often comes at the cost of decreased relevance (Zhang et al., 2018).", "labels": [], "entities": [{"text": "relevance", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9706699252128601}]}, {"text": "In this paper, we propose a SPACEFUSION model to jointly optimize diversity and relevance that essentially fuses the latent space of a sequence-to-sequence model and that of an autoen-coder model by leveraging novel regulariza-tion terms.", "labels": [], "entities": []}, {"text": "As a result, our approach induces a latent space in which the distance and direction from the predicted response vector roughly match the relevance and diversity, respectively.", "labels": [], "entities": []}, {"text": "This property also lends itself well to an intuitive visualization of the latent space.", "labels": [], "entities": []}, {"text": "Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong base-lines in both diversity and relevance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of neural response generation is advancing rapidly both in terms of research and commercial applications (.", "labels": [], "entities": [{"text": "neural response generation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7100026706854502}]}, {"text": "Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses ().", "labels": [], "entities": []}, {"text": "encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context.", "labels": [], "entities": []}, {"text": "However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam.", "labels": [], "entities": [{"text": "beam search", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8830379545688629}]}, {"text": "As a result, re-ranking can be extremely An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: \"I would love to play this game\".", "labels": [], "entities": []}, {"text": "[Context] Anyone want to start this game?", "labels": [], "entities": []}, {"text": "Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model.", "labels": [], "entities": []}, {"text": "Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively.", "labels": [], "entities": []}, {"text": "Based on the example in. time-consuming, raising difficulties for real-time applications.", "labels": [], "entities": []}, {"text": "This highlights the need to improve the diversity of candidates before re-ranking, and the need to optimize for diversity during training rather than just at the decoding stage.", "labels": [], "entities": []}, {"text": "While various approaches have been explored to diversify the output of conversation models, the improvement often comes at the cost of decreased response relevance along other dimensions.", "labels": [], "entities": []}, {"text": "For instance, present an approach to enhancing diversity by mapping diverse responses to a probability distribution using a conditional variational autoencoder (CVAE).", "labels": [], "entities": []}, {"text": "Despite the improved response diversity, this approach reduces response relevance as measured against the baseline.", "labels": [], "entities": []}, {"text": "One possible reason for this diversityrelevance trade-off is that such probabilistic approaches are not explicitly encouraged to induce a disentangled representation in latent space for controlling diversity and relevance independently.", "labels": [], "entities": []}, {"text": "Consider a Gaussian distribution, which is widely used for CVAE.", "labels": [], "entities": []}, {"text": "A Gaussian distribution naturally brings frequent responses near its mean, and the resulting responses are often generic and boring.", "labels": [], "entities": []}, {"text": "To generate diverse and interesting responses, one needs to sample a little distance from the mean.", "labels": [], "entities": []}, {"text": "But doing so naturally leads to infrequent and thus even irrelevant responses.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel geometrical approach that explicitly encourages a structured latent space in which the distance and direction from a predicted response vector roughly match the relevance and diversity, respectively, as illustrated in.", "labels": [], "entities": []}, {"text": "To induce such a latent space, we leverage two different models: 1) a S2S model, producing the predicted response vector (the black dot at the center in), and 2) an autoencoder (AE) model, yielding the vectors for potential responses (the colored dots).", "labels": [], "entities": []}, {"text": "In order to make the S2S and AE share the same latent space (the cloud), we use the same decoder for both and train them jointly end-to-end with novel regularization terms.", "labels": [], "entities": []}, {"text": "As this fuses the two latent spaces, we refer to our model as SPACEFUSION.", "labels": [], "entities": [{"text": "SPACEFUSION", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9030371904373169}]}, {"text": "Regularization is necessary because only sharing the decoder, as in (, does not necessarily align the latent spaces obtained by S2S and AE respectively or impose a disentangled structure onto the space.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9438015222549438}]}, {"text": "We introduce two regularization terms to tackle this issue.", "labels": [], "entities": []}, {"text": "1) interpolation term: we encourage a smooth semantic transition along the path between the predicted response vector and each target response vector (arrowed lines in).", "labels": [], "entities": []}, {"text": "This term effectively prevents semantically different responses from aligning in the same direction, essentially scattering them over different directions.", "labels": [], "entities": []}, {"text": "2) fusion term: we want the vectors from the two models to be distributed in a homogeneous manner, rather than forming two separate clusters () that can potentially make sampling non-trivial.", "labels": [], "entities": []}, {"text": "With the resulting latent space, we can control relevance and diversity by respectively adjusting distance and direction from a predicted response vector, without sacrificing each other greatly.", "labels": [], "entities": []}, {"text": "Our approach also lends itself well to the intuitive visualization of latent space.", "labels": [], "entities": []}, {"text": "Since our model allows us to geometrically find not only the predicted response vector but also the target response vector as in, we can visually interpret the structure of latent space and identify major issues thereof.", "labels": [], "entities": []}, {"text": "We devote Section 5.1 to show comprehensive examples for visualization-based analysis.", "labels": [], "entities": []}, {"text": "Automatic and human evaluations demonstrate that the proposed approach improves both the diversity and relevance of the responses, compared to strong baselines on two datasets with one-tomany context-response mapping.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the following datasets.", "labels": [], "entities": []}, {"text": "Some of their key features are presented in.", "labels": [], "entities": []}, {"text": "Switchboard: We use the version offered by, which is an extension of the original version by. collected multiple references for the test set using information retrieval (IR) techniques followed by human filtering, and randomly split the data into 2316/60/62 conversations for Although we use greedy decoding in this work, other decoding techniques, such as beam search, can be applied.", "labels": [], "entities": [{"text": "beam search", "start_pos": 357, "end_pos": 368, "type": "TASK", "confidence": 0.83103346824646}]}, {"text": "We let each system generate 100 hypotheses {h j } for each context xi in the test dataset.", "labels": [], "entities": []}, {"text": "Assuming xi has N r,i references, we pick the top N r,i distinct hypotheses ranked by log p(h j |x i )+\u03bb|h j |.", "labels": [], "entities": []}, {"text": "Similar to (, we takes |h j | into consideration, as BLEU is sensitive to length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9987655878067017}, {"text": "length", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9518246054649353}]}, {"text": "For fair comparison, \u03bb is tuned such that the average hypothesis length becomes roughly the same for all systems and approaches the average length of the references.", "labels": [], "entities": []}, {"text": "The automatic evaluation results are reported in.", "labels": [], "entities": []}, {"text": "On both datasets, the proposed system consistently outperforms the baselines by a large margin in Precision, Recall, and F1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9989867806434631}, {"text": "Recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9922075867652893}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9990987777709961}]}, {"text": "Examples of system outputs and human references can be found in  We randomly sampled 500 contexts from the Reddit test dataset and picked the top 1 hypothesis generated for each context ranked by log p(h j |x i ) + \u03bb|h j |.", "labels": [], "entities": [{"text": "Reddit test dataset", "start_pos": 107, "end_pos": 126, "type": "DATASET", "confidence": 0.9383829236030579}]}, {"text": "As in the automatic evaluation, we tuned \u03bb such that all systems have roughly context Everything about this movie is awesome!", "labels": [], "entities": []}, {"text": "SPACE \u2022 I love this movie.", "labels": [], "entities": [{"text": "SPACE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7896194458007812}]}, {"text": "FUSION \u2022 It's so awesome!!!", "labels": [], "entities": [{"text": "FUSION", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7731948494911194}]}, {"text": "I have no idea how to watch this movie.", "labels": [], "entities": []}, {"text": "I can't wait for the trailer.", "labels": [], "entities": []}, {"text": "\u2022 I don't think i'm a fan of the movie.", "labels": [], "entities": []}, {"text": "\u2022 I would love to see this.", "labels": [], "entities": []}, {"text": "\u2022 I want to watch this movie.", "labels": [], "entities": []}, {"text": "CVAE \u2022 Smartphones of the best games!.", "labels": [], "entities": [{"text": "CVAE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9219284057617188}]}, {"text": "+BOW \u2022 I'm in the same boat!", "labels": [], "entities": [{"text": "BOW", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9965208768844604}, {"text": "I", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9764714241027832}]}, {"text": "I feel the same way about this \u2022 I don't know why but can't tell if the New York will be the only one.", "labels": [], "entities": []}, {"text": "\u2022 Caves would never say yes, but I'd love to know.", "labels": [], "entities": []}, {"text": "\u2022 I know where this movie is.", "labels": [], "entities": []}, {"text": "MTask \u2022 This is so funny.", "labels": [], "entities": [{"text": "MTask", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9050890803337097}]}, {"text": "\u2022 I think I saw this.", "labels": [], "entities": []}, {"text": "\u2022 I don't know how many people do you.", "labels": [], "entities": []}, {"text": "\u2022 I remember watching was was disappointed.", "labels": [], "entities": []}, {"text": "S2S+ \u2022 Whoa man, this is amazing!!!", "labels": [], "entities": []}, {"text": "sampling \u2022 Man, I love Tom Cruise and I love the show.", "labels": [], "entities": []}, {"text": "\u2022 Is this a bill on the right?", "labels": [], "entities": [{"text": "a bill on the right", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.6982429087162018}]}, {"text": "\u2022 More like samurai jack.", "labels": [], "entities": []}, {"text": "\u2022 I weep for the hivemind.", "labels": [], "entities": []}, {"text": "human \u2022 I love this movie.", "labels": [], "entities": []}, {"text": "\u2022 My favorite Muppet movie by far.", "labels": [], "entities": []}, {"text": "\u2022 Now you have got the song into my head.", "labels": [], "entities": []}, {"text": "the same average hypothesis length.", "labels": [], "entities": []}, {"text": "We also randomly select one reference for each context and compare them with the systems (labeled \"human\" in As illustrated in: Performance of each model on human evaluation.", "labels": [], "entities": []}, {"text": "The highest score, except human, in each row is in bold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Key features of the datasets.", "labels": [], "entities": []}, {"text": " Table 3: Semantic interpolation with and without regularization. Results decoded from z interp .", "labels": [], "entities": []}, {"text": " Table 4. On both datasets, the proposed system  consistently outperforms the baselines by a large  margin in Precision, Recall, and F1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9988905787467957}, {"text": "Recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9918070435523987}, {"text": "F1", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9996175765991211}]}, {"text": " Table 4: Performance of each model on automatic mea- sures. The highest score in each row is in bold for each  dataset. Note that our BLEU scores are normalized to  [0, 100].", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9984251260757446}]}, {"text": " Table 7: Performance of each model on human evalu- ation. The highest score, except human, in each row is  in bold.", "labels": [], "entities": []}]}