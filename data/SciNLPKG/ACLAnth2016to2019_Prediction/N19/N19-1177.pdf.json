{"title": [{"text": "A Streamlined Method for Sourcing Discourse-level Argumentation Annotations from the Crowd", "labels": [], "entities": [{"text": "Sourcing Discourse-level Argumentation Annotations", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.7484158426523209}]}], "abstractContent": [{"text": "The study of argumentation and the development of argument mining tools depends on the availability of annotated data, which is challenging to obtain in sufficient quantity and quality.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7808757722377777}]}, {"text": "We present a method that breaks down a popular but relatively complex discourse-level argument annotation scheme into a simpler, iterative procedure that can be applied even by untrained annotators.", "labels": [], "entities": []}, {"text": "We apply this method in a crowdsourcing setup and report on the reliability of the annotations obtained.", "labels": [], "entities": []}, {"text": "The source code fora tool implementing our annotation method, as well as the sample data we obtained (4909 gold-standard annotations across 982 documents), are freely released to the research community.", "labels": [], "entities": []}, {"text": "These are intended to serve the needs of qualitative research into argumen-tation, as well as of data-driven approaches to argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.8262531459331512}]}], "introductionContent": [{"text": "Empirical study of argumentation requires examples drawn from authentic, human-authored text.", "labels": [], "entities": []}, {"text": "Likewise, the applications of computational argumentation, such as argument mining, can require significant amounts of argument-annotated data to achieve reasonable performance.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7594142556190491}]}, {"text": "However, this data can be challenging to obtain in sufficient quantity and quality, particularly for discourse-level argumentation.", "labels": [], "entities": []}, {"text": "This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., the need to tag relationships between annotated elements), and context weighting (i.e., the amount of context around markable units that needs to be considered)).", "labels": [], "entities": []}, {"text": "Successfully applying such schemes typically requires expensive and laborious work by expert-trained annotators.", "labels": [], "entities": []}, {"text": "In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme.", "labels": [], "entities": []}, {"text": "This scheme has been widely cited and used in argumentation studies (e.g.,, and while it is fairly coarse-grained, it is expensive to apply to new texts.", "labels": [], "entities": []}, {"text": "Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme.", "labels": [], "entities": []}, {"text": "We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators.", "labels": [], "entities": []}, {"text": "We find that agreement between the two groups increases sublinearly with the number of crowd annotators, achieving up to \u03b1 U = 0.52 when using ten crowd workers.", "labels": [], "entities": [{"text": "agreement", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9579734802246094}, {"text": "\u03b1 U", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.6612789928913116}]}, {"text": "We release not only our sample data set (consisting of 4909 gold-standard argument component and argument relation annotations over 982 product reviews), but also the source code for the annotation tool itself, which will allow others to produce their own quantity-and quality-controlled annotated data sets.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}