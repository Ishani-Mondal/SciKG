{"title": [{"text": "Multi-modal Discriminative Model for Vision-and-Language Navigation", "labels": [], "entities": []}], "abstractContent": [{"text": "Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals.", "labels": [], "entities": [{"text": "Vision-and-Language Navigation (VLN)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8331870615482331}]}, {"text": "Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback.", "labels": [], "entities": []}, {"text": "Generalization ability is limited by the amount of human annotated data.", "labels": [], "entities": [{"text": "Generalization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9643446803092957}]}, {"text": "In particular, paired vision-language sequence data is expensive to collect.", "labels": [], "entities": []}, {"text": "We develop a discriminator that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment.", "labels": [], "entities": []}, {"text": "Our study reveals that only a small fraction of the high-quality augmented data from Fried et al.", "labels": [], "entities": []}, {"text": "(2018), as scored by our discriminator, is useful for training VLN agents with similar performance on previously unseen environments.", "labels": [], "entities": []}, {"text": "We also show that a VLN agent warm-started with pre-trained components from the discriminator outperforms the benchmark success rates of 35.5 by 10% relative measure on previously unseen environments.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is an increased research interest in the problems containing multiple modalities ().", "labels": [], "entities": []}, {"text": "The models trained on such problems learn similar representations for related concepts in different modalities.", "labels": [], "entities": []}, {"text": "Model components can be pretrained on datasets with individual modalities, the final system must be trained (or fine-tuned) on task-specific datasets.", "labels": [], "entities": []}, {"text": "In this paper, we focus on vision-and-language navigation (VLN), which involves understanding * Authors contributed equally.", "labels": [], "entities": [{"text": "vision-and-language navigation (VLN)", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.8271705687046051}]}, {"text": "visual-spatial relations as described in instructions written in natural language.", "labels": [], "entities": []}, {"text": "In the past, VLN datasets were built on virtual environments, with being perhaps the most prominent example.", "labels": [], "entities": []}, {"text": "More recently, challenging photo-realistic datasets containing instructions for paths in real-world environments have been released (.", "labels": [], "entities": []}, {"text": "Such datasets require annotations by people who follow and describe paths in the environment.", "labels": [], "entities": []}, {"text": "Because the task is quite involved-especially when the paths are longerobtaining human labeled examples at scale is challenging.", "labels": [], "entities": []}, {"text": "For instance, the Touchdown dataset () has only 9,326 examples of the complete task.", "labels": [], "entities": [{"text": "Touchdown dataset", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.9818627834320068}]}, {"text": "Others, such as  and side-step this problem by using formulaic instructions provided by mapping applications.", "labels": [], "entities": []}, {"text": "This makes it easy to get instructions at scale.", "labels": [], "entities": []}, {"text": "However, since these are not natural language instructions, they lack the quasi-regularity, diversity, richness and errors inherent in how people give directions.", "labels": [], "entities": []}, {"text": "More importantly, they lack the more interesting connections between language and the visual scenes encountered on a path, such as head over the train tracks, hang aright just pasta cluster of palm trees and stop by the redbrick town home with a flag over its door.", "labels": [], "entities": []}, {"text": "In general, the performance of trained neural models is highly dependent on the amount of available training data.", "labels": [], "entities": []}, {"text": "Since human-annotated data is expensive to collect, it is imperative to maximally exploit existing resources to train models that can be used to improve the navigation agents.", "labels": [], "entities": []}, {"text": "For instance, to extend the Room-to-Room (R2R) dataset), created an augmented set of instructions for randomly generated paths in the same underlying environment.", "labels": [], "entities": []}, {"text": "These instructions were generated by a speaker model that was trained on the available human-annotated instructions in R2R.", "labels": [], "entities": []}, {"text": "Using this augmented data improved the navigation models in the original paper as well as later models such as.", "labels": [], "entities": [{"text": "navigation", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.9375578165054321}]}, {"text": "However, our own inspection of the generated instructions revealed that many have little connection between the instructions and the path they were meant to describe, raising questions about what models can and should learn from noisy, automatically generated instructions.", "labels": [], "entities": []}, {"text": "We instead pursue another, high precision strategy for augmenting the data.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9786893725395203}]}, {"text": "Having access to an environment provides opportunities for creating instruction-path pairs for modeling alignments.", "labels": [], "entities": []}, {"text": "In particular, given a path and a navigation instruction created by a person, it is easy to create incorrect paths by creating permutations of the original path.", "labels": [], "entities": []}, {"text": "For example, we can hold the instructions fixed, but reverse or shuffle the sequence of perceptual inputs, or sample random paths, including those that share the start or end points of the original one.", "labels": [], "entities": []}, {"text": "Crucially, given the diversity and relative uniqueness of the properties of different rooms and the trajectories of different paths, it is highly unlikely that the original instruction will correspond well to the mined negative paths.", "labels": [], "entities": []}, {"text": "This negative path mining strategy stands in stark contrast with approaches that create new instructions.", "labels": [], "entities": [{"text": "path mining", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8104042708873749}]}, {"text": "Though they cannot be used to directly train navigation agents, negative paths can instead be used to train discriminative models that can assess the fit of an instruction and a path.", "labels": [], "entities": []}, {"text": "As such, they can be used to judge the quality of machinegenerated extensions to VLN datasets and possibly reject bad instruction-path pairs.", "labels": [], "entities": []}, {"text": "More importantly, the components of discriminative models can be used for initializing navigation models themselves and thus allow them to make more effective use of the limited positive paths available.", "labels": [], "entities": [{"text": "initializing navigation", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.8612720966339111}]}, {"text": "We present four main contributions.", "labels": [], "entities": []}, {"text": "First, we propose a discriminator model) that can predict how well a given instruction explains the paired path.", "labels": [], "entities": []}, {"text": "We list several cheap negative sampling techniques to make the discriminator more robust.", "labels": [], "entities": []}, {"text": "Second, we show that only a small portion of the augmented data in are high fidelity.", "labels": [], "entities": []}, {"text": "Including just a small fraction of them in training is sufficient for reaping most of the gains afforded by the full augmentation set: using just the top 1% augmented data samples, as scored by the discriminator, is sufficient to generalize to previously unseen environments.", "labels": [], "entities": []}, {"text": "Third, we train the discriminator using alignment-based similarity metric that enables the model to align same concepts in the language and visual modalities.", "labels": [], "entities": []}, {"text": "We provide a qualitative assessment of the alignment learned by the model.", "labels": [], "entities": []}, {"text": "Finally, we show that a navigation agent, when initialized with components of fully-trained discriminator, outperforms the existing benchmark on success rate by over 10% relative measure on previously unseen environments.", "labels": [], "entities": [{"text": "navigation agent", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.9052585065364838}]}], "datasetContent": [{"text": "Room-to-Room (R2R) is a visually-grounded natural language navigation dataset in photo-realistic environments ().", "labels": [], "entities": []}, {"text": "Each environment is defined by a graph where nodes are locations with egocentric panoramic images and edges define valid connections for agent navigation.", "labels": [], "entities": [{"text": "agent navigation", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7254038006067276}]}, {"text": "The navigation dataset consists of language instructions paired with reference paths, where each path is defined by a sequence of graph nodes.", "labels": [], "entities": []}, {"text": "The data collection process is based on sampling pairs of start/end nodes and defining the shortest path between the two.", "labels": [], "entities": []}, {"text": "Furthermore the collection process ensures no paths are shorter than 5m and must be between 4 to 6 edges.", "labels": [], "entities": []}, {"text": "Each sampled path is associated with 3 natural language instructions collected from Amazon Mechanical Turk with an average length of 29 tokens from a vocabulary of 3.1k tokens.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9545678893725077}]}, {"text": "Apart from the training set, the dataset includes two validation sets and a test set.", "labels": [], "entities": []}, {"text": "One of the validation sets includes new instructions on environments overlapping with the training set (Validation Seen), and the other is entirely disjoint from the training set (Validation Unseen).", "labels": [], "entities": []}, {"text": "Several metrics are commonly used to evaluate agents' ability to follow navigation instructions.", "labels": [], "entities": [{"text": "follow navigation instructions", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6763662298520406}]}, {"text": "Path Length (PL) measures the total length of the predicted path, where the optimal value is the length of the reference path.", "labels": [], "entities": [{"text": "Path Length (PL)", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8093546271324158}]}, {"text": "Navigation Error (NE) measures the distance between the last nodes in the predicted path and the reference path.", "labels": [], "entities": [{"text": "Navigation Error (NE)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.9168297052383423}]}, {"text": "Success Rate (SR) measures how often the last node in the predicted path is within some threshold distance d th of the last node in the reference path.", "labels": [], "entities": [{"text": "Success Rate (SR)", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.977897047996521}]}, {"text": "More recently, proposed the Success weighted by Path Length (SPL) measure that also considers whether the success criteria was met (i.e., whether the last node in the predicted path is within some threshold d th of the reference path) and the normalized path length.", "labels": [], "entities": [{"text": "Success weighted by Path Length (SPL) measure", "start_pos": 28, "end_pos": 73, "type": "METRIC", "confidence": 0.7662366098827786}]}, {"text": "Agents should minimize NE and maximize SR and SPL.", "labels": [], "entities": [{"text": "NE", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9977766871452332}, {"text": "SR", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9948245286941528}]}], "tableCaptions": [{"text": " Table 3: Results on R2R validation unseen paths (U) and seen paths (S) when trained only with small fraction of  Fried-Augmented ordered by discriminator scores. For Random Full study, examples are sampled uniformly  over entire dataset. For Random Top/Bottom study, examples are sampled from top/bottom 40% of ordered dataset.  SPL and SR are reported as percentages and NE and PL in meters.", "labels": [], "entities": [{"text": "R2R validation unseen paths", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8188396841287613}, {"text": "SR", "start_pos": 338, "end_pos": 340, "type": "METRIC", "confidence": 0.937828540802002}, {"text": "NE", "start_pos": 373, "end_pos": 375, "type": "METRIC", "confidence": 0.9857189059257507}]}, {"text": " Table 4: Results 3 on R2R validation unseen (U) and validation seen (S) paths when trained with full training set  and selected fraction of Fried-Augmented. SPL and SR are reported as percentages and NE and PL in meters.", "labels": [], "entities": [{"text": "SR", "start_pos": 166, "end_pos": 168, "type": "METRIC", "confidence": 0.9462692737579346}, {"text": "NE", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9853184223175049}]}, {"text": " Table 5: Results on R2R validation unseen (U) and validation seen (S) paths after initializing navigation agent's  instruction and visual encoders with discriminator.", "labels": [], "entities": []}]}