{"title": [], "abstractContent": [{"text": "Existing computational models to understand hate speech typically frame the problem as a simple classification task, bypassing the understanding of hate symbols (e.g., 14 words, kigy) and their secret connotations.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel task of deciphering hate symbols.", "labels": [], "entities": [{"text": "deciphering hate symbols", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7934003869692484}]}, {"text": "To do this, we leverage the Urban Dictionary and collected anew, symbol-rich Twitter corpus of hate speech.", "labels": [], "entities": [{"text": "Urban Dictionary", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8888307511806488}]}, {"text": "We investigate neural network latent context models for deciphering hate symbols.", "labels": [], "entities": []}, {"text": "More specifically, we study Sequence-to-Sequence models and show how they are able to crack the ciphers based on context.", "labels": [], "entities": []}, {"text": "Furthermore, we propose a novel Variational Decipher and show how it can generalize better to unseen hate symbols in a more challenging testing setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Federal Bureau of Investigation of United States 1 reported over 6,000 criminal incidents motivated by bias against race, ethnicity, ancestry, religion, sexual orientation, disability, gender, and gender identity during 2016.", "labels": [], "entities": []}, {"text": "The most recent 2016 report shows an alarming 4.6% increase, compared with 2015 data 2 . In addition to these reported cases, thousands of Internet users, including celebrities, are forced out of social media due to abuse, hate speech, cyberbullying, and online threats.", "labels": [], "entities": []}, {"text": "While such social media data is abundantly available, the broad question we are asking is-What can machine learning and natural language processing do to help and prevent online hate speech?", "labels": [], "entities": []}, {"text": "The vast quantity of hate speech on social media can be analyzed to study online abuse.", "labels": [], "entities": []}, {"text": "In recent years, there has been a growing trend of developing computational models of hate speech.", "labels": [], "entities": []}, {"text": "However, the majority of the prior studies focus solely on modeling hate speech as a binary or multiclass classification task (.", "labels": [], "entities": []}, {"text": "While developing new features for hate speech detection certainly has merits, we believe that understanding hate speech requires us to design computational models that can decipher hate symbols that are commonly used by hate groups.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8140238523483276}]}, {"text": "shows an example usage of hate symbols in an otherwise seemingly harmless tweet that promotes hate.", "labels": [], "entities": []}, {"text": "For example, Aryan Warrior is a longstanding racist prison gang based in the Nevada prison system.", "labels": [], "entities": []}, {"text": "WPWW is the acronym for White Pride World Wide.", "labels": [], "entities": [{"text": "WPWW", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.934637725353241}, {"text": "White Pride World Wide", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.7372460886836052}]}, {"text": "The hate symbols 1488 and 2316 are more implicit.", "labels": [], "entities": []}, {"text": "14 symbolizes the 14 words: \"WE MUST SECURE THE EXISTENCE OF OUR PEOPLE AND A FUTURE FOR WHITE CHILDREN\", spoken by members of the Order neo-Nazi movement.", "labels": [], "entities": [{"text": "WE MUST SECURE THE EXISTENCE OF OUR PEOPLE", "start_pos": 29, "end_pos": 71, "type": "METRIC", "confidence": 0.7427052706480026}, {"text": "FUTURE FOR WHITE CHILDREN", "start_pos": 78, "end_pos": 103, "type": "METRIC", "confidence": 0.845178484916687}]}, {"text": "H is the 8th letter of the alphabet, so 88=HH=Heil Hitler.", "labels": [], "entities": []}, {"text": "Similarly, Wis the 23rd and P is the 16th letter of the alphabet, so 2316=WP=White Power.", "labels": [], "entities": []}, {"text": "In this work, we propose the first models for deciphering hate symbols.", "labels": [], "entities": [{"text": "deciphering hate symbols", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8125947515169779}]}, {"text": "We investigate two families of neural network approaches: the Sequenceto-Sequence models) and a novel Variational Decipher based on the Conditional Variational Autoencoders (.", "labels": [], "entities": []}, {"text": "We show how these neural network models are able to guess the meaning of hate symbols based on context embeddings and even generalize to unseen hate symbols during testing.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: \u2022 We propose a novel task of learning to decipher hate symbols, which moves beyond the standard formulation of hate speech classification settings.", "labels": [], "entities": [{"text": "hate speech classification", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.6714432736237844}]}, {"text": "\u2022 We introduce anew, symbol-rich tweet dataset for developing computational models of hate speech analysis, leveraging the Urban Dictionary.", "labels": [], "entities": [{"text": "hate speech analysis", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.7511834700902303}, {"text": "Urban Dictionary", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8344162404537201}]}, {"text": "\u2022 We investigate a sequence-to-sequence neural network model and show how it is able to encode context and crack the hate symbols.", "labels": [], "entities": []}, {"text": "We also introduce a novel Variational Decipher, which generalizes better in a more challenging setting.", "labels": [], "entities": []}, {"text": "In the next section, we outline related work in text normalization, machine translation, conditional variational autoencoders, and hate speech analysis.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7863504588603973}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8403925895690918}, {"text": "hate speech analysis", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.766802966594696}]}, {"text": "In Section 3, we introduce our new dataset for deciphering hate speech.", "labels": [], "entities": [{"text": "deciphering hate speech", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7939658562342325}]}, {"text": "Next, in Section 4, we describe the design of two neural network models for the decipherment problem.", "labels": [], "entities": []}, {"text": "Quantitative and qualitative experimental results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the dataset we collected for hate symbol decipherment.", "labels": [], "entities": [{"text": "hate symbol decipherment", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8075061440467834}]}, {"text": "We use the dataset collected as described in section 3 for training and testing.", "labels": [], "entities": []}, {"text": "We randomly selected 2,440 tuples for testing and use the remaining 16,227 tuples for training.", "labels": [], "entities": []}, {"text": "Note that there are no overlapping hate symbols between the training dataset U and the testing dataset D.", "labels": [], "entities": [{"text": "training dataset U", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.7412691513697306}]}, {"text": "We randomly initialize network parameters \u03d5, \u03b1, \u03b2; 3: for epoch = 1, E do 4: for (tweet, symbol, def inition) in U do 5: get embeddings u, sw, sc, d * ; 6: compute x, c and h with RNN encoders; 7: compute \u00b5, \u03a3 with the posterior network; 8: compute \u00b5 , \u03a3 with the prior network; 9: compute KL-divergence loss LKL; 10: sample z = reparameterize(\u00b5, \u03a3); 11: initialize the decoder state e0 = c; 12: LREC = 0; 13: fort = 1, M do 14: compute attention weights wt; 15: compute ot, et and p(dt|z, u, s); if for (tweet, symbol, def inition) in V do 29: get embeddings u, sw, sc; 30: compute c and h with RNN encoders; 31: compute \u00b5 , \u03a3 with the prior network; 32: sample z = reparameterize(\u00b5 , \u03a3 ); 33: initialize the decoder state e0 = c; 34: compute attention weights w; 36: compute ot, et and p(dt|z, u, s); if dt==EOS then 39: break; 40: end if 41: end for 42: end for 43: end function symbol wigwog and Wig Wog have the same definition but one is in the training dataset, the other is in the first testing dataset.", "labels": [], "entities": [{"text": "LREC", "start_pos": 396, "end_pos": 400, "type": "METRIC", "confidence": 0.9878280162811279}, {"text": "break", "start_pos": 823, "end_pos": 828, "type": "METRIC", "confidence": 0.9725947976112366}]}, {"text": "We assume that such types of hate symbols share similar surface forms or similar tweet contexts.", "labels": [], "entities": []}, {"text": "Therefore, the first testing dataset D sis to evaluate how well the model captures the semantic similarities among the tweet contexts in different examples or the similarities among different surface forms of a hate symbol.", "labels": [], "entities": []}, {"text": "Deciphering the hate symbols in the second testing dataset D dis more challenging.", "labels": [], "entities": []}, {"text": "Both the unseen hate symbols and definitions require the model to have the ability to accurately capture the semantic information in the tweet context and then make a reasonable prediction.", "labels": [], "entities": []}, {"text": "For the Seq2Seq model, we use negative loglikelihood loss for training.", "labels": [], "entities": []}, {"text": "Both models are optimized using Adam optimizer (.", "labels": [], "entities": []}, {"text": "The hyper-parameters of two models are exactly the same.", "labels": [], "entities": []}, {"text": "We set the maximum generation length M = 50.", "labels": [], "entities": []}, {"text": "The hidden size of the encoders is 64.", "labels": [], "entities": []}, {"text": "The size of the word embedding is 200 and that of character embedding is 100.", "labels": [], "entities": []}, {"text": "The word embeddings and character embeddings are randomly initialized.", "labels": [], "entities": []}, {"text": "Each model is trained for 50 epochs.", "labels": [], "entities": []}, {"text": "We report the deciphering results of two models on three testing datasets D, D sand D d .  Quantitative Results: We use equally weighted BLEU score for up to 4-grams (), ROUGE-L () and METEOR () to evaluate the decipherment results.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9815233647823334}, {"text": "ROUGE-L", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.9959024786949158}, {"text": "METEOR", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9965229034423828}]}, {"text": "The results are shown in Table 1..", "labels": [], "entities": []}, {"text": "The gap between the performance of the Seq2Seq model on D sand D dis much larger than that between the performance of the Variational Decipher on these two datasets.", "labels": [], "entities": []}, {"text": "Human Evaluation: We employed crowdsourced workers to evaluate the deciphering results of two models.", "labels": [], "entities": []}, {"text": "We randomly sampled 100 items of deciphering results from D sand another   In each choice question, the workers are given the hate symbol, the referential definition, the original tweet and two machine-generated plain texts from the Seq2Seq model and Variational Decipher.", "labels": [], "entities": []}, {"text": "Workers are asked to select the more reasonable of the two results.", "labels": [], "entities": []}, {"text": "In each choice question, the order of the results from the two models is permuted.", "labels": [], "entities": []}, {"text": "Ties are permitted for answers.", "labels": [], "entities": [{"text": "Ties", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9674646258354187}]}, {"text": "We batch five items in one assignment and insert an artificial item with two identical outputs as a sanity check.", "labels": [], "entities": []}, {"text": "The workers who fail to choose \"tie\" for that item are rejected from our test.", "labels": [], "entities": []}, {"text": "The human evaluation results are shown in, which coincide with the results in and.", "labels": [], "entities": []}, {"text": "Discussion: When deciphering the hate symbols that have the same definitions as in the training dataset, the model can rely more on the surface forms of hate symbols than the tweet context to make a prediction because usually the hate symbols that share the same definitions also have similar surface forms.", "labels": [], "entities": []}, {"text": "However, when it comes to the hate symbols with unseen definitions, simply relying on the surface forms cannot lead to a reasonable deciphering result.", "labels": [], "entities": []}, {"text": "Instead, the model should learn the relationships between the con- text information and the definition of the symbol.", "labels": [], "entities": []}, {"text": "Therefore, the different performances of two models on the two testing datasets D sand D d indicate that the Seq2Seq model is better at capturing the similarities among different surface forms of a hate symbol, while the Variational Decipher is better at capturing the semantic relationship between the tweet context and the hate symbol.", "labels": [], "entities": []}, {"text": "The Sequence-to-Sequence model tries to capture such kinds of relationships by compressing all the context information into a fixed length vector, so its deciphering strategy is actually behavior cloning.", "labels": [], "entities": []}, {"text": "On the other hand, the Variational Decipher captures such relationships by explicitly modeling the posterior and likelihood distributions.", "labels": [], "entities": []}, {"text": "The modeled distributions provide higher-level semantic information compared to the compressed context, which allows the Variational Decipher to generalize better to the symbols with unseen definitions.", "labels": [], "entities": []}, {"text": "This explains why the gap between the performance of the Seq2Seq model on two datasets is larger.", "labels": [], "entities": []}, {"text": "shows some example errors of the deciphering results of our Seq2Seq model and Variational Decipher.", "labels": [], "entities": []}, {"text": "One problem with the deciphering results is that the generated sentences have poor grammatical structure, as shown in.", "labels": [], "entities": []}, {"text": "This is mainly because the size of our dataset is small, and the models need a much larger corpus to learn the grammar.", "labels": [], "entities": []}, {"text": "We anticipate that the generation performance will be improved with a larger dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The BLEU, ROUGE-L and METEOR scores  on testing datasets. VD refers to the Variational Deci- pher. D is the entire testing dataset. D s is the first part  of D and D d is the second part. The better results are  in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9994081258773804}, {"text": "ROUGE-L", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9933464527130127}, {"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9950240254402161}, {"text": "VD", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.8926714658737183}]}, {"text": " Table 2: The results of human evaluation on two sepa- rate testing datasets D s and D d .", "labels": [], "entities": []}]}