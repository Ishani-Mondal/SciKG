{"title": [{"text": "Neural Semi-Markov Conditional Random Fields for Robust Character-Based Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Robust Character-Based Part-of-Speech Tagging", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.5499147176742554}]}], "abstractContent": [{"text": "Character-level models of tokens have been shown to be effective at dealing with within-token noise and out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "However, they often still rely on correct token boundaries.", "labels": [], "entities": []}, {"text": "In this paper, we propose to eliminate the need for tokenizers with an end-to-end character-level semi-Markov conditional random field.", "labels": [], "entities": []}, {"text": "It uses neural networks for its character and segment representations.", "labels": [], "entities": []}, {"text": "We demonstrate its effectiveness in multilingual settings and when token boundaries are noisy: It matches state-of-the-art part-of-speech tag-gers for various languages and significantly outperforms them on a noisy English version of a benchmark dataset.", "labels": [], "entities": []}, {"text": "Our code and the noisy dataset are publicly available at http: //cistern.cis.lmu.de/semiCRF.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, character-based neural networks (NNs) gained popularity for different tasks, ranging from text classification ( and language modeling) to machine translation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7967702746391296}, {"text": "language modeling", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.6895189881324768}, {"text": "machine translation", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.8025707006454468}]}, {"text": "Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (.", "labels": [], "entities": []}, {"text": "However, most character-level models still rely on tokenization and use characters only for creating more robust token representations.", "labels": [], "entities": []}, {"text": "This leads to high performance on well-formatted text or text with misspellings () but ties the performance to the quality of the tokenizer.", "labels": [], "entities": []}, {"text": "While humans are very robust to noise caused by insertion of spaces (e.g., \"car nival\") or deletion of spaces (\"deeplearning\"), this can cause severe underperformance of machine learning models.", "labels": [], "entities": []}, {"text": "Similar challenges arise for languages with difficult tokenization, such as Chinese or Vietnamese.", "labels": [], "entities": []}, {"text": "For text with difficult or noisy tokenization, more robust models are needed.", "labels": [], "entities": []}, {"text": "In order to address this challenge, we propose a model that does not require any tokenization.", "labels": [], "entities": []}, {"text": "It is based on semi-Markov conditional random fields (semi-CRFs)) which jointly learn to segment (tokenize) and label the input (e.g., characters).", "labels": [], "entities": []}, {"text": "To represent the character segments, we compare different NN approaches.", "labels": [], "entities": []}, {"text": "In our experiments, we address part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6091567814350128}]}, {"text": "However, our model is generally applicable to other sequence-tagging tasks as well since it does not require any task-specific hand-crafted features.", "labels": [], "entities": []}, {"text": "Our model achieves stateof-the-art results on the Universal Dependencies dataset (.", "labels": [], "entities": [{"text": "Universal Dependencies dataset", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.832094649473826}]}, {"text": "To demonstrate its effectiveness, we evaluate it not only on English but also on languages with inherently difficult tokenization, namely Chinese, Japanese and Vietnamese.", "labels": [], "entities": []}, {"text": "We further analyze the robustness of our model against difficult tokenization by randomly corrupting the tokenization of the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.7824593484401703}]}, {"text": "Our model significantly outperforms state-of-theart token-based models in this analysis.", "labels": [], "entities": []}, {"text": "Our contributions are: 1) We present a truly end-to-end character-level sequence tagger that does not rely on any tokenization and achieves state-of-the-art results across languages.", "labels": [], "entities": [{"text": "character-level sequence tagger", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6763654152552286}]}, {"text": "2) We show its robustness against noise caused by corrupted tokenization, further establishing the importance of character-level models as a promising research direction.", "labels": [], "entities": []}, {"text": "3) For future research, our code and the noisy version of the dataset are publicly available at http://cistern.cis.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our implementation is in PyTorch (.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9244599938392639}]}, {"text": "Hyperparameters are tuned on the development set.", "labels": [], "entities": []}, {"text": "We use mini-batch gradient descent with a batch size of) as the optimizer.", "labels": [], "entities": []}, {"text": "The learning rate is 1e-3, the coefficients for computing running averages of the gradient and its square are 0.9 and 0.999, respectively.", "labels": [], "entities": []}, {"text": "A term of 1e-8 is added to the denominator for numerical stability.", "labels": [], "entities": []}, {"text": "We use character embeddings of size 60 and three stacked biLSTM layers with 100 hidden units for each direction.", "labels": [], "entities": []}, {"text": "For the semi-CRF, we set the maximum segment length to L = 23 as tokens of bigger length are rarely seen in the training sets.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we apply dropout with a probability of 0.25 on each layer including the input.", "labels": [], "entities": []}, {"text": "For input dropout, we randomly replace a character embedding with a zero vector, similar to.", "labels": [], "entities": []}, {"text": "This avoids overfitting to local character patterns.", "labels": [], "entities": []}, {"text": "Moreover, we employ early stopping on the development set with a minimum of 20 training epochs.", "labels": [], "entities": []}, {"text": "We run our experiments on a gpu which speeds up the training compared to multiple cpu cores considerably.", "labels": [], "entities": []}, {"text": "We assume that it especially benefits from parallelizing the computation of each level of the grConv pyramid..", "labels": [], "entities": []}, {"text": "Results on English (UD v1.2).", "labels": [], "entities": [{"text": "UD v1.2)", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.7091733614603678}]}, {"text": "provides our results on UD v1.2, categorizing the models into token-level ( w) and character-only models ( c).", "labels": [], "entities": []}, {"text": "While most pure character-level models cannot ensure consistent labels for each character of a token, our semi-CRF outputs correct segments inmost cases (tokenization F 1 is 98.69%, see), and ensures a single label for all characters of a segment.", "labels": [], "entities": []}, {"text": "Our model achieves the best results among all character-level models and comparable results to the word-level model MarMot.", "labels": [], "entities": [{"text": "MarMot", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.8732398152351379}]}, {"text": "In addition, we assess the impact of two components of our model: the space feature (see Section 2.1) and grConv (see Section 2.2.1).", "labels": [], "entities": []}, {"text": "shows that the performance of our model decreases when ablating the space feature, confirming that information about spaces plays a valuable role for English.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of grConv for segment representations, we replace it with a Segmental Recurrent Neural Network (SRNN) (  this to the different way of feature creation: While grConv hierarchically combines context-enhanced n-grams, SRNN constructs segments in a sequential order.", "labels": [], "entities": []}, {"text": "The latter maybe less suited for compositional segments like \"airport\".", "labels": [], "entities": []}, {"text": "We compare to the top performing models for EN, JA, VI, ZH from the CoNLL 2017 shared task: UDPipe 1.,, FBAML (Qian and Liu, 2017), TRL (, and IMS.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.8500893861055374}, {"text": "FBAML", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.8044281601905823}]}, {"text": "Multilingual Results (UD v2.0).", "labels": [], "entities": []}, {"text": "While for each language another shared task system performs best, our system performs consistently well across languages (best or second-best except for EN), leading to the best average scores for both tokenization and POS tagging.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 202, "end_pos": 214, "type": "TASK", "confidence": 0.9718636274337769}, {"text": "POS tagging", "start_pos": 219, "end_pos": 230, "type": "TASK", "confidence": 0.7902772426605225}]}, {"text": "Moreover, it matches the state of the art for Chinese (ZH) and Vietnamese (VI), two languages with very different characteristics in tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 133, "end_pos": 145, "type": "TASK", "confidence": 0.9655869603157043}]}], "tableCaptions": [{"text": " Table 1: POS tag accuracy on UD v1.2 (EN).  '-' denotes that the model does not use this input.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8481375575065613}]}, {"text": " Table 2: Tokenization and joint token-POS F 1 on UD v2.0. Best scores are in bold, second-best are underlined.", "labels": [], "entities": [{"text": "UD v2.0", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8768486380577087}]}, {"text": " Table 3: Noisy dataset statistics (three different noise  levels).", "labels": [], "entities": []}, {"text": " Table 4: Tokenization F 1 , joint token-POS F 1 and (re- laxed) POS tag accuracies on noisy version of UD v1.2.", "labels": [], "entities": [{"text": "joint token-POS F 1", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.7264035865664482}]}]}