{"title": [{"text": "Alignment over Heterogeneous Embeddings for Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7401508986949921}]}], "abstractContent": [{"text": "We propose a simple, fast, and mostly-unsupervised approach for non-factoid question answering (QA) called Alignment over Heterogeneous Embeddings (AHE).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8174795687198639}, {"text": "Alignment over Heterogeneous Embeddings (AHE)", "start_pos": 107, "end_pos": 152, "type": "TASK", "confidence": 0.5545422009059361}]}, {"text": "AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency of the corresponding ques-tion/answer term.", "labels": [], "entities": [{"text": "AHE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7347903847694397}]}, {"text": "AHE's similarity function operates over embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised component.", "labels": [], "entities": [{"text": "AHE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8200723528862}, {"text": "FLAIR", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.9440957307815552}, {"text": "BERT", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9306879639625549}]}, {"text": "Despite its simplicity and lack of supervision, AHE obtains anew state-of-the-art performance on the \"Easy\" partition of the AI2 Reasoning Challenge (ARC) dataset (64.6% accuracy), top-two performance on the \"Challenge\" partition of ARC (34.1%), and top-three performance on the WikiQA dataset (74.08% MRR), out-performing many other complex, supervised approaches.", "labels": [], "entities": [{"text": "AHE", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7357131242752075}, {"text": "AI2 Reasoning Challenge (ARC) dataset", "start_pos": 125, "end_pos": 162, "type": "DATASET", "confidence": 0.5392045506409237}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9979971051216125}, {"text": "WikiQA dataset", "start_pos": 279, "end_pos": 293, "type": "DATASET", "confidence": 0.9706500172615051}, {"text": "MRR", "start_pos": 302, "end_pos": 305, "type": "METRIC", "confidence": 0.9958364963531494}]}, {"text": "Our error analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information.", "labels": [], "entities": []}, {"text": "We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation, which further improves the performance of un-supervised AHE 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The \"deep learning tsunami\" has had a major impact on important natural language processing (NLP) applications such as question answering (QA).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.8942943215370178}]}, {"text": "Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks; 1 Code: https://github.com/vikas95/AHE Question -Which sequence of energy transformations occurs after a battery-operated flashlight is turned on?", "labels": [], "entities": [{"text": "QA", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9713202118873596}, {"text": "AHE", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9946460723876953}]}, {"text": "1. electrical \u2192 light \u2192 chemical 2.", "labels": [], "entities": []}, {"text": "electrical \u2192chemical \u2192 light 3.", "labels": [], "entities": []}, {"text": "chemical \u2192 light \u2192 electrical 4.", "labels": [], "entities": []}, {"text": "chemical \u2192 electrical \u2192 light Supporting paragraph(s): \"a chemical cell converts chemical energy into electrical energy; a flashlight chemical energy to light energy\": A multiple-choice question from the ARC dataset with the correct answer in bold font.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 204, "end_pos": 215, "type": "DATASET", "confidence": 0.9706756174564362}]}, {"text": "This question is answered correctly by our alignment method that relies on contextualized word embeddings that capture the correct sequence, and cannot be answered correctly when relying on uncontextualized embeddings., inter alia).", "labels": [], "entities": []}, {"text": "However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches.", "labels": [], "entities": []}, {"text": "For instance, alignment approaches have received considerably less interest recently, despite their initial successes (.", "labels": [], "entities": [{"text": "alignment", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.983606219291687}]}, {"text": "While a few recent efforts have adapted these alignment methods to operate over word representations, they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe ().", "labels": [], "entities": []}, {"text": "In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations.", "labels": [], "entities": [{"text": "BoW", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.8823384642601013}]}, {"text": "For example, shows an example of a question from AI2's Reasoning Challenge (ARC) dataset ), which is not answered correctly by a state-of-theart BoW alignment method (, but is correctly answered by our alignment approach when operating over Bidirectional Encoder Representations from Transformers (BERT) embeddings).", "labels": [], "entities": [{"text": "AI2's Reasoning Challenge (ARC) dataset", "start_pos": 49, "end_pos": 88, "type": "DATASET", "confidence": 0.6136494129896164}]}, {"text": "We propose a simple, fast, and mostlyunsupervised approach for non-factoid QA called Alignment over Heterogeneous Embeddings (AHE).", "labels": [], "entities": [{"text": "Alignment over Heterogeneous Embeddings (AHE)", "start_pos": 85, "end_pos": 130, "type": "TASK", "confidence": 0.5915247755391257}]}, {"text": "AHE uses an off-the-shelf information retrieval (IR) component to retrieve likely supporting paragraphs from a knowledge base (KB) given a question and candidate answer.", "labels": [], "entities": [{"text": "AHE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6887935996055603}, {"text": "information retrieval (IR)", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7149084091186524}]}, {"text": "Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term.", "labels": [], "entities": [{"text": "AHE", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9257624745368958}, {"text": "inverse document frequency (IDF)", "start_pos": 170, "end_pos": 202, "type": "METRIC", "confidence": 0.8346305092175802}]}, {"text": "AHE's overall alignment score is the sum of the IDF weighted scores of each of the question/answer term.", "labels": [], "entities": [{"text": "AHE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7125917077064514}, {"text": "alignment score", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.8098017573356628}, {"text": "IDF weighted scores", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.9560174743334452}]}, {"text": "Importantly, AHE's alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT), and sentence (InferSent), where the latter is the only supervised component in the proposed approach.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9377545118331909}, {"text": "BERT", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.7355112433433533}]}, {"text": "The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier.", "labels": [], "entities": []}, {"text": "The contributions of our work are the following: 1.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, including character, word, and sentence.", "labels": [], "entities": []}, {"text": "2. We obtain (near) state-of-the-art results (top three or higher) on three QA datasets: WikiQA () (74.08 mean reciprocal rank), ARC the Challenge partition (34.1% precision at 1 (P@1)) and ARC Easy (64.6 P@1).", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8587599396705627}, {"text": "WikiQA", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.8998405933380127}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9895607233047485}, {"text": "ARC", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.7876020669937134}, {"text": "Easy", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.6196873188018799}]}, {"text": "Our approach outperforms information retrieval methods, other unsupervised alignment approaches, and many supervised, neural approaches, despite the fact that it is mostly unsupervised and much simpler.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7811351716518402}]}, {"text": "Importantly, unlike many neural approaches, our results are robust across several datasets.", "labels": [], "entities": []}, {"text": "Minimally, these results indicate that the work proposed here should be considered as anew, strong baseline for the task.", "labels": [], "entities": []}, {"text": "3. Our analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information.", "labels": [], "entities": []}, {"text": "We highlight this complementarity with an oracle system that chooses the correct answer when it is proposed by any of the AHE's representations, which achieves 68% P@1 on ARC Challenge, 86% on ARC Easy, and 93.7% mean average precision (MAP) on WikiQA.", "labels": [], "entities": [{"text": "AHE", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.9255648255348206}, {"text": "P@1", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.95234747727712}, {"text": "ARC Challenge", "start_pos": 171, "end_pos": 184, "type": "DATASET", "confidence": 0.8735036551952362}, {"text": "ARC Easy", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.7214732766151428}, {"text": "mean average precision (MAP)", "start_pos": 213, "end_pos": 241, "type": "METRIC", "confidence": 0.9116827150185903}, {"text": "WikiQA", "start_pos": 245, "end_pos": 251, "type": "DATASET", "confidence": 0.941990077495575}]}, {"text": "We exploit this complementarity with a simple meta-classifier that learns when and how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on the ARC dataset, measured using precision at 1 (P@1), on both the Easy and Challenge  partitions. Italic font indicates which AHE components are supervised, e.g., InferSent is the InferSent model  trained on ARC data; AHE is the AHE variant that uses the supervised meta-classifier ensemble. Line 8a shows  performance of alignment over the original InferSent embeddings (trained on NLI datasets); line 8b shows perfor- mance when using InferSent embeddings trained on ARC training data. The \"minimal\" supervision configurations  (lines 20 and 21) include the supervised InferSent, but use the unsupervised NoisyOr strategy for aggregation.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.7637074291706085}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9744411110877991}, {"text": "NLI datasets", "start_pos": 408, "end_pos": 420, "type": "DATASET", "confidence": 0.8009734153747559}, {"text": "ARC training data", "start_pos": 494, "end_pos": 511, "type": "DATASET", "confidence": 0.6585314075152079}]}, {"text": " Table 2: Performance on the WikiQA dataset, measured using mean average precision (MAP) and mean reciprocal  rank (MRR). Italic font indicates which AHE components are supervised, e.g., InferSent is the InferSent model  trained on WikiQA data; AHE is the AHE variant that uses the supervised meta-classifier ensemble. The \"minimal\"  supervision configurations (lines 23 and 24) include the supervised InferSent, but use the unsupervised NoisyOr  strategy for aggregation.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9195397198200226}, {"text": "mean average precision (MAP)", "start_pos": 60, "end_pos": 88, "type": "METRIC", "confidence": 0.9017278750737509}, {"text": "mean reciprocal  rank (MRR)", "start_pos": 93, "end_pos": 120, "type": "METRIC", "confidence": 0.8580355743567148}]}, {"text": " Table 3: Performance of two neural QA methods, BiLSTM Max-out and BiMPM, when trained/tested across  datasets. The first value in each cell corresponds to BiLSTM Max-out, and the second to BiMPM. The last row  contains the best unsupervised performance of AHE, which was not trained on any of these three datasets.", "labels": [], "entities": [{"text": "BiMPM", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8282096982002258}, {"text": "AHE", "start_pos": 257, "end_pos": 260, "type": "METRIC", "confidence": 0.9308599233627319}]}]}