{"title": [{"text": "On the Importance of Distinguishing Word Meaning Representations: A Case Study on Reverse Dictionary Mapping", "labels": [], "entities": [{"text": "Importance of Distinguishing Word Meaning Representations", "start_pos": 7, "end_pos": 64, "type": "TASK", "confidence": 0.7116483251253763}, {"text": "Reverse Dictionary Mapping", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7660507559776306}]}], "abstractContent": [{"text": "Meaning conflation deficiency is one of the main limiting factors of word representations which, given their widespread use at the core of many NLP systems, can lead to inaccurate semantic understanding of the input text and inevitably hamper the performance.", "labels": [], "entities": [{"text": "Meaning conflation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7211871147155762}]}, {"text": "Sense representations target this problem.", "labels": [], "entities": []}, {"text": "However , their potential impact has rarely been investigated in downstream NLP applications.", "labels": [], "entities": []}, {"text": "Through a set of experiments on a state-of-the-art reverse dictionary system based on neu-ral networks, we show that a simple adjustment aimed at addressing the meaning con-flation deficiency can lead to substantial improvements.", "labels": [], "entities": []}, {"text": "1 Meaning Conflation Deficiency Words are often the most fine-grained meaning bearing components of NLP systems.", "labels": [], "entities": [{"text": "1 Meaning Conflation Deficiency Words", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6137061893939972}]}, {"text": "As a standard practise, particularly for neural models, the input text is treated as a sequence of words and each word in the sequence is represent with a dense distributional representation (word embedding).", "labels": [], "entities": []}, {"text": "Importantly, this setting ignores the fact that a word can be polysemous, i.e., it can take multiple (possibly unrelated) meanings.", "labels": [], "entities": []}, {"text": "Representing a word with all its possible meanings as a single point (vector) in the embedding space, the so-called meaning conflation deficiency (Camacho-Collados and Pilehvar, 2018), can hinder system's semantic effectiveness.", "labels": [], "entities": []}, {"text": "To address this deficiency, many techniques have been put forward over the past few years, the most prominent of which is sense representation or multi-prototype embedding (Sch\u00fctze, 1998; Reisinger and Mooney, 2010).", "labels": [], "entities": [{"text": "sense representation", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.696211963891983}]}, {"text": "However, as a general trend, these representations are usually evaluated either on generic benchmarks, such as word similarity, or on sense-centered tasks such as Word Sense Disambiguation, leaving their potential impact on downstream word-based systems unknown.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 163, "end_pos": 188, "type": "TASK", "confidence": 0.7228289445241293}]}, {"text": "In this paper, we provide an analysis to highlight the importance of addressing the meaning conflation deficiency.", "labels": [], "entities": [{"text": "meaning conflation", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7913533747196198}]}, {"text": "Specifically, we show how distinguishing different meanings of a word can facilitate a more accurate semantic understanding of a state-of-the-art reverse dictionary system, reflected by substantial improvements in recall and generalisation power.", "labels": [], "entities": [{"text": "recall", "start_pos": 214, "end_pos": 220, "type": "METRIC", "confidence": 0.9975729584693909}]}, {"text": "2 Reverse Dictionary Reverse dictionary, conceptual dictionary, or concept lookup is the task of returning a word given its description or definition (Brown and McNeill, 1966; Zock and Bilac, 2004).", "labels": [], "entities": []}, {"text": "For example, given \"a crystal of snow\", the system has to return the word snowflake.", "labels": [], "entities": []}, {"text": "The task is closely related to the \"tip of the tongue\" problem where an individual recalls some general features about a word but cannot retrieve that from memory.", "labels": [], "entities": []}, {"text": "Therefore, a reverse dictionary system can be particularly useful to writers and translators when they cannot recall a word in time or are unsure how to express an idea they want to convey.", "labels": [], "entities": []}, {"text": "2.1 Evaluation framework Our experiments are based on the reverse dictionary model of Hill et al.", "labels": [], "entities": []}, {"text": "(2016) which leverages a standard neural architecture in order to map dictionary definitions to representations of the words defined by those definitions.", "labels": [], "entities": []}, {"text": "Specifically, they proposed two neural architectures for mapping the definition of word t to its word embedding e t.", "labels": [], "entities": []}, {"text": "Let D t be the sequence of words in t's definition, i.e., D t = {w 1 , w 2 ,.", "labels": [], "entities": []}, {"text": ".. , w n }, with their corresponding embeddings {v 1 , v 2 ,.", "labels": [], "entities": []}, {"text": ".. , v n }.", "labels": [], "entities": []}, {"text": "The two models differ in the way they process D t.", "labels": [], "entities": []}, {"text": "In the bag-of-words (BoW) model, D t is taken as a bag", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our experiments are based on the reverse dictionary model of which leverages a standard neural architecture in order to map dictionary definitions to representations of the words defined by those definitions.", "labels": [], "entities": []}, {"text": "Specifically, they proposed two neural architectures for mapping the definition of word t to its word embedding e t . Let D t be the sequence of words in t's definition, i.e., D t = {w 1 , w 2 , . .", "labels": [], "entities": []}, {"text": ", w n }, with their corresponding embeddings {v 1 , v 2 , . .", "labels": [], "entities": []}, {"text": "The two models differ in the way they process D t . In the bag-of-words (BoW) model, D t is taken as a bag of words, i.e., the representation of the definition is encoded by adding the word embeddings of all its content words, i.e., n i=1 vi . The model learns, using a fully-connected layer, a matrix for transforming the encoded representation to the target word's embedding e t . The BoW model is not sensitive to the order of words in D t . This might be crucial for an accurate semantic understanding.", "labels": [], "entities": []}, {"text": "The Recurrent Neural Network (RNN) model alleviates this issue by encoding the input sequence using an LSTM architecture).", "labels": [], "entities": []}, {"text": "Similarly to the BoW model, a dense layer maps the encoded representation to the target word's embedding.", "labels": [], "entities": []}, {"text": "In both cases, the goal is to map a given definition to the corresponding target word's embedding e t , computed using Word2vec ( and independently from the training of the main model.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.970085859298706}]}, {"text": "Two cost functions were tested: (1) the cosine distance between the estimated point in the target space (\u02c6 e t ) and e t , and (2) the rank loss which contrast the choice of e t with a random choice fora randomly-selected word from the vocabulary other than t.", "labels": [], "entities": []}, {"text": "The reverse dictionary system takes advantage of a standard architecture which has proven effective in various NLP tasks.", "labels": [], "entities": []}, {"text": "However, similarly to many other word-based models, the system ignores that the same word can have multiple (potentially unrelated) meanings.", "labels": [], "entities": []}, {"text": "In fact, it tries to map multiple definitions, with different semantics, to the same point in the target space.", "labels": [], "entities": []}, {"text": "For instance, the three semantically unrelated definitions of crane: \"lifts and moves heavy objects\", \"large long-necked wading bird\", and \"a small constellation in the southern hemisphere\" will have similar semantic interpretation by the system.", "labels": [], "entities": []}, {"text": "This wordlevel meaning conflation can hamper the ability of the system in learning an accurate mapping function.", "labels": [], "entities": []}, {"text": "In what follows in this paper, we will illustrate how a simple sense level distinction can facilitate a more accurate semantic understanding for the reverse dictionary system, hence leading to significant performance improvements.", "labels": [], "entities": []}, {"text": "We carried out evaluations on the three reverse dictionary datasets created by: WordNet definitions and \"single-sentence descriptions\" written fora set of frequent words (concept mapping).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9345740675926208}]}, {"text": "They proposed two different versions of the WordNet dataset: WN-seen, in which a test instance is already observed during training, and WN-unseen, in which test instances are excluded from the training data.", "labels": [], "entities": [{"text": "WordNet dataset", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9776584506034851}]}, {"text": "The former dataset is targeted at evaluating the ability of the system to recall a previously encoded information.", "labels": [], "entities": []}, {"text": "We experimented with three variants of the reverse dictionary system: the original word-based model and the two proposed sense-based variants, based on WordNet senses and supersenses.", "labels": [], "entities": []}, {"text": "Table 1 reports accuracy performance for four different configurations of the system (BoW and RNN definition composition and cosine and ranking loss; cf. Section 2.1) on the three datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9990353584289551}, {"text": "BoW", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.754664957523346}]}, {"text": "In the last row, we also report results for the unsupervised baseline of which adds the embedding of words in the input definition and finds the nearest embedding in the target space.", "labels": [], "entities": []}, {"text": "Results reported in the highlight that addressing the meaning conflation deficiency in the system has led to significant performance improvements (word vs. sense and supersense settings).", "labels": [], "entities": [{"text": "meaning conflation", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7057886570692062}]}, {"text": "This is observed consistently across all the three datasets and for both sense-based models.", "labels": [], "entities": []}, {"text": "The better semantic understanding of the system is reflected by its better recall of seen test instances (WN-seen) and better generalisation to unseen and out-of-domain data (WN-unseen and concept mapping).", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9983235001564026}]}, {"text": "The absolute top-10 accuracy improvements of the ranking-BoW supersense model over the best corresponding word-based configurations are: 14.4% (WN-seen), 15% (WN-unseen), and 7% (concept mapping).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9988370537757874}]}, {"text": "Among the two proposed systems, supersenses prove to be more effective, suggesting that the fine-grained sense distinctions in WordNet might not be necessary for an accurate reverse dic-tionary mapping, corroborating previous findings).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9601625800132751}]}, {"text": "Our results are also inline with the findings of that the reverse dictionary system performs best with the bag-of-words (BoW) input encoding and the ranking loss.", "labels": [], "entities": []}, {"text": "One of the fundamental differences between the two input encodings lies in their sensitiveness to order: RNNs are sensitive to the order of words in a given sequence whereas permuting words in the sequence does not alter BoW's encoding.", "labels": [], "entities": [{"text": "BoW", "start_pos": 221, "end_pos": 224, "type": "DATASET", "confidence": 0.8700637221336365}]}, {"text": "suggested that it is often possible to retrieve a concept even if the words in its corresponding definition are shuffled.", "labels": [], "entities": []}, {"text": "This can partly explain the strikingly good relative performance of the BoW model.", "labels": [], "entities": [{"text": "BoW", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8906672596931458}]}], "tableCaptions": [{"text": " Table 1: Accuracy performance (@10/100) of the original (word-based) reverse dictionary system and its sense- and supersense-based improvements on different datasets. See Section 2.1 for system configurations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989984631538391}]}]}