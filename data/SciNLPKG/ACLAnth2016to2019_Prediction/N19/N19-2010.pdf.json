{"title": [{"text": "A Case Study on Neural Headline Generation for Editing Support", "labels": [], "entities": [{"text": "Neural Headline Generation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.6717113852500916}]}], "abstractContent": [{"text": "There have been many studies on neural headline generation models trained with a lot of (article, headline) pairs.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7579227089881897}]}, {"text": "However, there are few situations for putting such models into practical use in the real world since news articles typically already have corresponding headlines.", "labels": [], "entities": []}, {"text": "In this paper, we describe a practical use case of neural headline generation in a news aggregator, where dozens of professional editors constantly select important news articles and manually create their headlines, which are much shorter than the original headlines.", "labels": [], "entities": [{"text": "neural headline generation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7077610095342001}]}, {"text": "Specifically, we show how to deploy our model to an editing support tool and report the results of comparing the behavior of the editors before and after the release.", "labels": [], "entities": []}], "introductionContent": [{"text": "A news-aggregator is a website or mobile application that aggregates a large amount of web content, e.g., online newspapers provided by different publishers.", "labels": [], "entities": []}, {"text": "The main purpose of such a service is to help users obtain important news out of vast amounts of information quickly and easily.", "labels": [], "entities": []}, {"text": "Therefore, it is critical to consider how to compactly show news, as well as what type of news to select, to improve service quality.", "labels": [], "entities": []}, {"text": "In fact, the news-aggregator of Yahoo!", "labels": [], "entities": []}, {"text": "JAPAN 1 , the largest Japanese portal site, is supported by dozens of professional editors who constantly select important news articles and manually create their new headlines called short titles, which are much shorter than the original headline, to construct a newstopic list.", "labels": [], "entities": [{"text": "JAPAN 1", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.90671306848526}]}, {"text": "Note that we use the term \"title\" to avoid confusion with the original news headline, although they are similar concepts.", "labels": [], "entities": []}, {"text": "* Both authors contributed equally to this work..", "labels": [], "entities": []}, {"text": "The left figure (a) shows the list of news topics (important news articles), which includes short titles, and the right figure (b) shows the entry page of the first topic in the list, which consists of a headline and lead.", "labels": [], "entities": []}, {"text": "The lead is a short version of the article and can be used by users to decide whether to read the whole article.", "labels": [], "entities": []}, {"text": "The editors' job is to create a short title from news content including the headline and lead.", "labels": [], "entities": []}, {"text": "A short title has two advantages over a normal headline; one is quick understandability of the content and the other is saving display space by using a single line.", "labels": [], "entities": []}, {"text": "This means that short titles can increase a user's chances of reaching interesting articles.", "labels": [], "entities": []}, {"text": "Since the click-through rate of news articles is directly related toad revenue, even a small improvement in short titles has a significant impact on business.", "labels": [], "entities": []}, {"text": "We tackle an automatic-generation task of such short titles fora news aggregator to support the Japanese English translation Short title The prime minister cannot say that there is no surmise.", "labels": [], "entities": []}, {"text": "Headline It cannot be said that there is no \"sontaku (surmise)\" with absolute certainty.", "labels": [], "entities": [{"text": "Headline", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8966475129127502}]}, {"text": "The prime minister Abe said about the problem of \"Kake Gakuen (Kake school)\".", "labels": [], "entities": []}, {"text": "Lead Prime Minister Shinzo Abe said, in an intensive deliberation with the House of Councilors Budget Committee held on the afternoon of the 14th, as an answer to a question about whether bureaucrats surmised to the prime minister regarding the Kake suspicion, \"It is difficult to understand whether there is a sontaku (surmise)\".", "labels": [], "entities": []}, {"text": "He said \"It cannot be said that there was nothing wrong,\" while explaining that \"I do not need to be obsequious\".", "labels": [], "entities": []}, {"text": "An answer to Ichiro Tsukada (LDP).: Short title, headline, and lead in(b) with English versions.", "labels": [], "entities": [{"text": "headline", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8967816829681396}]}, {"text": "Our task is a variant of newsheadline generation, which has been extensively studied, as described in Section 6.", "labels": [], "entities": [{"text": "newsheadline generation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7959870398044586}]}, {"text": "A clear difference between their task and ours is that we need to generate short titles from news content including headlines.", "labels": [], "entities": []}, {"text": "Thus, we formulate our task as an abstractive summarization from multiple information sources, i.e., headlines and leads, based on an encoder-decoder model (Section 2).", "labels": [], "entities": []}, {"text": "There are roughly three approaches for handling multiple information sources.", "labels": [], "entities": []}, {"text": "The first approach is to merge all sources with some weights based on the importance of each source, which can be achieved by a weighted average of the context vectors, as in multimodal summarization (.", "labels": [], "entities": []}, {"text": "This is the most general approach since the other two can also be regarded as special cases of the weighted average.", "labels": [], "entities": []}, {"text": "The second approach is to use one source as the main source and others as secondary ones.", "labels": [], "entities": []}, {"text": "This is effective when the main source can be clearly determined, such as query-focused summarization (, where the target document is main and a query is secondary.", "labels": [], "entities": []}, {"text": "The third approach is to find the salient components of the sources.", "labels": [], "entities": []}, {"text": "This is suitable when there are many sources including less informative ones (redundant sources), such as lengthydocument summarization that outputs a multisentence summary, where each sentence can be regarded as one source.", "labels": [], "entities": []}, {"text": "We addressed an extension of the weighted average approach and compared our proposed model with a multimodal model () from the first approach and a query-based model () from the second approach, as well as the normal encoder-decoder model.", "labels": [], "entities": []}, {"text": "Since we have only two sources (headlines and leads), where the headline source is clearly salient for generating a short title, the third approach can be reduced to the normal encoder-decoder model.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "\u2022 We report on a case study of short-title generation of news articles fora news aggregator as a real-world application of neural headline generation.", "labels": [], "entities": [{"text": "short-title generation of news articles", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.8466439843177795}, {"text": "neural headline generation", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.6975103318691254}]}, {"text": "This study supports previous studies based on the encoder-decoder model from a practical standpoint since most real-world news articles basically already have headlines, which means that there has been little direct application of these previous studies.", "labels": [], "entities": []}, {"text": "\u2022 We propose an encoder-decoder model with multiple encoders for separately encoding news headlines and leads (Section 3).", "labels": [], "entities": []}, {"text": "Our comparative experiments with several baselines involving evaluations done by crowdsourcing workers showed the effectiveness of our model, especially using the \"usefulness\" measure (Section 4).", "labels": [], "entities": []}, {"text": "\u2022 We describe how to deploy our model to an editing support tool and show the results of comparing the editors' behavior before and after releasing the tool (Section 5), which imply that the editors began to refer to generated titles after the release.", "labels": [], "entities": []}, {"text": "late the following conditional likelihood with respect to each pair (x, y) of an input sequence x = x 1 \u00b7 \u00b7 \u00b7 x Sand output sequence y = y 1 \u00b7 \u00b7 \u00b7 y T , where y \u2264t = y 1 \u00b7 \u00b7 \u00b7 y t , and maximize its mean.", "labels": [], "entities": []}, {"text": "The model p(y | x) in Eq. is computed by a combination of two recurrent neural networks (RNNs): an encoder and decoder.", "labels": [], "entities": []}, {"text": "The encoder reads an input sequence x to recognize its content, and the decoder predicts an output sequence y corresponding to the content.", "labels": [], "entities": []}, {"text": "More formally, an encoder calculates a hidden state h s for each element x sin ax by using the state transition function f enc of the encoder: h s = f enc (x s , h s\u22121 ).", "labels": [], "entities": []}, {"text": "Ina similar fashion, a decoder calculates a hidden stat\u00ea ht for each element y tin a y by using the state transition function f dec of the decoder after setting the last hidden state of the encoder as the initial state of the decoder ( Then, a prediction of outputs for each\u02c6heach\u02c6 each\u02c6h t is calculated using the output function g dec with an attention mechanism: where ct is a weighted average of the encoder hidden states {h 1 , \u00b7 \u00b7 \u00b7 , h S }, defined by where at (s) represents a weight of an encoder hidden state h s with respect to a decoder hidden stat\u00eastat\u00ea ht . ct represents a soft alignment (or attention weight) to the source sequence at the target position t, so it is called a context.", "labels": [], "entities": []}], "datasetContent": [{"text": "We prepared a dataset extracted from the newsaggregator of Yahoo!", "labels": [], "entities": [{"text": "newsaggregator of Yahoo!", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.8238328397274017}]}, {"text": "The dataset included 263K (headline, lead, short title) triples, and was split into three parts, i.e., for training (90%), validation (5%), and testing (5%).", "labels": [], "entities": []}, {"text": "We preprocessed them by separating characters for training since our preliminary experiments showed that character-based training clearly performed better than word-based training.", "labels": [], "entities": []}, {"text": "The statistics of our dataset are as follows.", "labels": [], "entities": []}, {"text": "The average lengths of headlines, leads, and short titles are 24.87, 128.49, and 13.05 Japanese characters, respectively.", "labels": [], "entities": []}, {"text": "The dictionary sizes (for characters) of headlines, leads, and short titles are 3618, 4226, and 3156, respectively.", "labels": [], "entities": []}, {"text": "Each news article has only one short title created by a professional editor.", "labels": [], "entities": []}, {"text": "The percentage of short titles equal to their headlines is only 0.13%, while the percentage of extractively solvable instances, in which the characters in each short title are completely matched by those in the corresponding headline, was about 20%.", "labels": [], "entities": []}, {"text": "However, the average edit distance) between short titles and headlines was 23.74.", "labels": [], "entities": [{"text": "edit distance)", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9759358962376913}]}, {"text": "This means that short titles cannot be easily created from headlines.", "labels": [], "entities": []}, {"text": "We conducted two crowdsourcing tasks to separately measure readability and usefulness.", "labels": [], "entities": []}, {"text": "The readability task asked ten workers how readable each short title was on a four-point scale (higher is better), while the usefulness task asked them how useful the short title was compared to the corresponding article.", "labels": [], "entities": []}, {"text": "The score of each generated short title was calculated by averaging the scores collected from the ten workers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Mean scores of readability (r), usefulness (u),  and their average r+u  2 based on crowdsourcing. The  \" \u2020\" mark shows a statistical significance from all three  baselines OpenNMT, MultiModal, and QueryBased on  a one-tailed, paired t-test (p < 0.01).", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 182, "end_pos": 189, "type": "DATASET", "confidence": 0.9225659370422363}]}]}