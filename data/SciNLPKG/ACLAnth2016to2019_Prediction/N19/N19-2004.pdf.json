{"title": [{"text": "Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "labels": [], "entities": [{"text": "Nurse-Patient Conversations on Symptom Monitoring", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.7332724690437317}]}], "abstractContent": [{"text": "Data for human-human spoken dialogues for research and development are currently very limited in quantity, variety, and sources; such data are even scarcer in healthcare.", "labels": [], "entities": []}, {"text": "In this work, we investigate fast prototyping of a dialogue comprehension system by leverag-ing on minimal nurse-to-patient conversations.", "labels": [], "entities": []}, {"text": "We propose a framework inspired by nurse-initiated clinical symptom monitoring conversations to construct a simulated human-human dialogue dataset, embodying linguistic characteristics of spoken interactions like thinking aloud, self-contradiction, and topic drift.", "labels": [], "entities": []}, {"text": "We then adopt an established bidirectional attention pointer network on this simulated dataset, achieving more than 80% F1 score on a held-out test set from real-world nurse-to-patient conversations.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.979054182767868}]}, {"text": "The ability to automatically comprehend conversations in the healthcare domain by exploiting only limited data has implications for improving clinical workflows through red flag symptom detection and triaging capabilities.", "labels": [], "entities": [{"text": "red flag symptom detection", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.6340944468975067}]}, {"text": "We demonstrate the feasibility for efficient and effective extraction , retrieval and comprehension of symptom checking information discussed in multi-turn human-human spoken conversations.", "labels": [], "entities": [{"text": "symptom checking information discussed in multi-turn human-human spoken conversations", "start_pos": 103, "end_pos": 188, "type": "TASK", "confidence": 0.8068519201543596}]}], "introductionContent": [], "datasetContent": [{"text": "We divide the construction of data simulation into two stages.", "labels": [], "entities": []}, {"text": "In Section 4.1, we build templates and expression pools using linguistic analysis followed by manual verification.", "labels": [], "entities": []}, {"text": "In Section 4.2, we present our proposed framework for generating simulated training data.", "labels": [], "entities": []}, {"text": "The templates and framework are verified for logical correctness and clinical soundness.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our linguisticallyinspired simulation approach, the model is trained on the simulated data (see Section 4.2).", "labels": [], "entities": []}, {"text": "We designed 3 evaluation sets: (1) Base Set (1,264 samples) held out from the simulated data.", "labels": [], "entities": []}, {"text": "(2) Augmented Set (1,280 samples) built by adding two out-of-distribution symptoms, with corresponding dialogue contents and queries, to the Base Set (\"bleeding\" and \"cold\", which never appeared in training data).", "labels": [], "entities": []}, {"text": "(3) Real-World Set (944 samples) manually delineated from the the symptom checking portions (approximately 4 hours) of real-world dialogues, and annotated as evaluation samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Linguistic characterization of inquiry- response types and their occurrence frequency from the  seed data in Section 3.2.", "labels": [], "entities": []}, {"text": " Table 2: QA model evaluation results. Each sample is  a simulated multi-turn conversation.", "labels": [], "entities": []}, {"text": " Table 3: Ablation experiments on 100K training size.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9846794009208679}]}]}