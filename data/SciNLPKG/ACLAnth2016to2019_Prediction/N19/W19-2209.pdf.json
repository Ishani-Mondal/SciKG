{"title": [{"text": "Extreme Multi-Label Legal Text Classification: A case study in EU Legislation", "labels": [], "entities": [{"text": "Extreme Multi-Label Legal Text Classification", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.4995388925075531}]}], "abstractContent": [{"text": "We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain.", "labels": [], "entities": [{"text": "Extreme Multi-Label Text Classification (XMTC)", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.6853242303643908}]}, {"text": "We release anew dataset of 57k legislative documents from EUR-LEX, the Euro-pean Union's public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus.", "labels": [], "entities": [{"text": "EUR-LEX", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9030022025108337}, {"text": "Euro-pean Union's public document database", "start_pos": 71, "end_pos": 113, "type": "DATASET", "confidence": 0.9279598693052927}]}, {"text": "The dataset is substantially larger than previous EUR-LEX datasets and suitable for XMTC, few-shot and zero-shot learning.", "labels": [], "entities": [{"text": "EUR-LEX datasets", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9414540529251099}]}, {"text": "Experimenting with several neural classifiers, we show that BIGRUs with self-attention outperform the current multi-label state-of-the-art methods, which employ label-wise attention.", "labels": [], "entities": [{"text": "BIGRUs", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9186007380485535}]}, {"text": "Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance.", "labels": [], "entities": [{"text": "BIGRUs", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9422798752784729}]}], "introductionContent": [{"text": "Extreme multi-label text classification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes).", "labels": [], "entities": [{"text": "multi-label text classification (XMTC)", "start_pos": 8, "end_pos": 46, "type": "TASK", "confidence": 0.7728581577539444}]}, {"text": "Applications include building web directories, labeling scientific publications with concepts from ontologies (, product categorization, categorizing medical examinations (), and indexing legal documents.", "labels": [], "entities": [{"text": "indexing legal documents", "start_pos": 179, "end_pos": 203, "type": "TASK", "confidence": 0.8824975887934366}]}, {"text": "We focus on legal text processing, an emerging NLP field with many applications (), but limited publicly available resources.", "labels": [], "entities": [{"text": "legal text processing", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7960252165794373}]}, {"text": "We release anew dataset, named EURLEX57K, including 57,000 English documents of EU legislation from the EUR-LEX portal.", "labels": [], "entities": [{"text": "EURLEX57K", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8746691346168518}, {"text": "EUR-LEX portal", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8973102271556854}]}, {"text": "All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the Publications Office of the European Union.", "labels": [], "entities": [{"text": "European Vocabulary (EUROVOC)", "start_pos": 54, "end_pos": 83, "type": "DATASET", "confidence": 0.7703361272811889}]}, {"text": "Although EUROVOC contains more than 7,000 concepts, most of them are rarely used in practice.", "labels": [], "entities": [{"text": "EUROVOC", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.9046916961669922}]}, {"text": "Consequently, they are under-represented in EU-RLEX57K, making the dataset also appropriate for few-shot and zero-shot learning.", "labels": [], "entities": [{"text": "EU-RLEX57K", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9442607760429382}]}, {"text": "Experimenting on EURLEX57K, we explore the use of various proposed a CNN similar to that of for XMTC.", "labels": [], "entities": [{"text": "EURLEX57K", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.9586532711982727}]}, {"text": "They reported results on several benchmark datasets, most notably: (), containing news articles; EUR-LEX (, containing legal documents; Amazon-12K, containing product descriptions; and Wiki-30K, containing Wikipedia articles.", "labels": [], "entities": [{"text": "EUR-LEX", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.8350011110305786}, {"text": "Amazon-12K", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.8756101727485657}]}, {"text": "Their proposed method outperformed both tree-based methods (e.g., FASTXML, () and target-embedding methods (e.g., SLEEC),).", "labels": [], "entities": [{"text": "FASTXML", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.5434408187866211}]}], "datasetContent": [{"text": "We implemented all methods in KERAS.", "labels": [], "entities": [{"text": "KERAS", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.534753143787384}]}, {"text": "We used Adam ( with learning rate 1e \u2212 3.", "labels": [], "entities": [{"text": "learning rate 1e \u2212 3", "start_pos": 20, "end_pos": 40, "type": "METRIC", "confidence": 0.9439828872680665}]}, {"text": "Hyper-parameters were tuned on development data using HYPEROPT.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.959621250629425}, {"text": "HYPEROPT", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.5909385681152344}]}, {"text": "We tuned for the following hyper-parameters and ranges: ENC output units {200, 300, 400}, ENC layers {1, 2}, batch size {8, 12, 16}, dropout rate {0.1, 0.2, 0.3, 0.4}, word dropout rate {0.0, 0.01, 0.02}.", "labels": [], "entities": []}, {"text": "For the best hyper-parameter values, we perform five runs and report mean scores on test data.", "labels": [], "entities": []}, {"text": "For statistical significance, we take the run of each method with the best performance on development data, and perform two-tailed approximate randomization tests) on test data.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6997776031494141}]}, {"text": "We used 200-dimensional pre-trained GLOVE embeddings) in all neural methods.", "labels": [], "entities": []}, {"text": "The most common evaluation measures in XMTC are recall (R@K), precision (P @K), and nDCG (nDCG@K) at the top K predicted labels, along with micro-averaged F -1 across all labels.", "labels": [], "entities": [{"text": "recall (R@K)", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9206587572892507}, {"text": "precision (P @K)", "start_pos": 62, "end_pos": 78, "type": "METRIC", "confidence": 0.9445272982120514}, {"text": "F -1", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9366599321365356}]}, {"text": "Measures that macro-average over labels do not consider the number of instances per label, thus being very sensitive to infrequent labels, which are many more than frequent ones.", "labels": [], "entities": []}, {"text": "On the other hand, ranking measures, like R@K, P @K, nDCG@K, are sensitive to the choice of K.", "labels": [], "entities": []}, {"text": "In EURLEX57K the average number of labels per document is 5.07, hence evaluating at K = 5 is a reasonable choice.", "labels": [], "entities": [{"text": "EURLEX57K", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.8647848963737488}]}, {"text": "We note that 99.4% of the dataset's documents have at most 10 gold labels.", "labels": [], "entities": []}, {"text": "While R@K and P @K are commonly used, we question their suitability for XMTC.", "labels": [], "entities": []}, {"text": "R@K leads to unfair penalization of methods when documents have more than K gold labels.", "labels": [], "entities": []}, {"text": "Evaluating at K = 1 fora document with N > 1 gold labels returns at most R@1 = 1 N , unfairly penalizing systems by not allowing them to return N labels.", "labels": [], "entities": []}, {"text": "This is shown in, where the green lines show that R@K decreases as K decreases, because of low scores obtained for documents with more than K labels.", "labels": [], "entities": [{"text": "R@K", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9295404950777689}]}, {"text": "On the other hand, P @K leads to excessive penalization for documents with fewer than K gold labels.", "labels": [], "entities": []}, {"text": "Evaluating at K = 5 fora document with just one gold label returns at most P @5 = 1 5 = 0.20, unfairly penalizing systems that retrieved all the gold labels (in this case, just one).", "labels": [], "entities": []}, {"text": "The red lines of decline as K increases, because the number of documents with fewer than K gold labels increases (recall that the average number of gold labels is 5.07).", "labels": [], "entities": [{"text": "K", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.9660812616348267}]}, {"text": "Similar concerns have led to the introduction of R-Precision and nDCG@K in Information Retrieval (), which we believe are also more appropriate for XMTC.", "labels": [], "entities": [{"text": "R-Precision", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9446715712547302}, {"text": "Information Retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7764140069484711}]}, {"text": "Note, however, that R-Precision requires that the number of gold labels per document is known beforehand, which is not realistic in practical applications.", "labels": [], "entities": []}, {"text": "Therefore we propose R-Precision@K (RP @K) where K is the maximum number of retrieved labels.", "labels": [], "entities": []}, {"text": "Both RP @K and nDCG@K adjust to the number of gold labels per document, without unfairly penalizing systems for documents with  fewer than K or many more than K gold labels.", "labels": [], "entities": []}, {"text": "They are defined as follows: Here N is the number of test documents; Rel(n, k) is 1 if the k-th retrieved label of the n-th test document is correct, otherwise 0; Rn is the number of gold labels of the n-th test document; and Z Kn is a normalization factor to ensure that nDCG@K = 1 for perfect ranking.", "labels": [], "entities": [{"text": "Rel", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9949116110801697}]}, {"text": "In effect, RP @K is a macro-averaged (over test documents) version of P @K, but K is reduced to the number of gold labels Rn of each test document, if K exceeds Rn . shows RP @K for the three best systems.", "labels": [], "entities": []}, {"text": "Unlike P @K, RP @K does not decline sharply as K increases, because it replaces K by Rn (number of gold labels) when K > Rn . For K = 1, RP @K is equivalent to P @K, as confirmed by.", "labels": [], "entities": []}, {"text": "For large values of K that almost always exceed Rn , RP @K asymptotically approaches R@K (macro-averaged over documents), as also confirmed by.: Results on EURLEX57K for all, frequent (> 50 training instances), few-shot (1 to 50 instances), and zeroshot labels.", "labels": [], "entities": [{"text": "EURLEX57K", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.9354594349861145}]}, {"text": "All the differences between the best (bold) and other methods are statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": "methods, while Logistic Regression is also unable to cope with the complexity of XMTC.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7254237234592438}, {"text": "XMTC", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.8552583456039429}]}, {"text": "In Section 2, we referred to the lack of previous experimental comparison between methods relying on label-wise attention and strong generic text classification baselines.", "labels": [], "entities": [{"text": "generic text classification", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.6190344393253326}]}, {"text": "Interestingly, for all, frequent, and even few-shot labels, the generic BIGRU-ATT performs better than CNN-LWAN, which was designed for XMTC.", "labels": [], "entities": [{"text": "BIGRU-ATT", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9915452003479004}, {"text": "XMTC", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.8669367432594299}]}, {"text": "HAN also performs better than CNN-LWAN for all and frequent labels.", "labels": [], "entities": [{"text": "HAN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6441508531570435}]}, {"text": "However, replacing the CNN encoder of CNN-LWAN with a BIGRU (BIGRU-LWAN) leads to the best results overall, with the exception of zero-shot labels, indicating that the main weakness of CNN-LWAN is its vanilla CNN encoder.", "labels": [], "entities": [{"text": "CNN-LWAN", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8685372471809387}, {"text": "BIGRU", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9939800500869751}, {"text": "BIGRU-LWAN", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.8868546485900879}]}], "tableCaptions": [{"text": " Table 1: Statistics of the EUR-LEX dataset.", "labels": [], "entities": [{"text": "EUR-LEX dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.978101909160614}]}, {"text": " Table 2: Results on EURLEX57K for all, frequent (> 50 training instances), few-shot (1 to 50 instances), and zero- shot labels. All the differences between the best (bold) and other methods are statistically significant (p < 0.01).", "labels": [], "entities": [{"text": "EURLEX57K", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9140565991401672}]}]}