{"title": [{"text": "The Lower The Simpler: Simplifying Hierarchical Recurrent Models", "labels": [], "entities": [{"text": "Simpler", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9933155179023743}, {"text": "Simplifying Hierarchical Recurrent", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8090720176696777}]}], "abstractContent": [{"text": "To improve the training efficiency of hierarchical recurrent models without compromising their performance, we propose a strategy named as \"the lower the simpler\", which is to simplify the baseline models by making the lower layers simpler than the upper layers.", "labels": [], "entities": []}, {"text": "We carryout this strategy to simplify two typical hierarchical recurrent models, namely Hierarchical Recurrent Encoder-Decoder (HRED) and R-NET, whose basic building block is GRU.", "labels": [], "entities": []}, {"text": "Specifically, we propose Scalar Gated Unit (SGU), which is a simplified variant of GRU, and use it to replace the GRUs at the middle layers of HRED and R-NET.", "labels": [], "entities": []}, {"text": "Besides, we also use Fixed-size Ordinally-Forgetting Encoding (FOFE), which is an efficient encoding method without any trainable parameter, to replace the GRUs at the bottom layers of HRED and R-NET.", "labels": [], "entities": [{"text": "Fixed-size Ordinally-Forgetting Encoding (FOFE)", "start_pos": 21, "end_pos": 68, "type": "METRIC", "confidence": 0.5768805394570032}]}, {"text": "The experimental results show that the simplified HRED and the simplified R-NET contain significantly less trainable parameters, consume significantly less training time, and achieve slightly better performance than their baseline models.", "labels": [], "entities": [{"text": "HRED", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.464719295501709}]}], "introductionContent": [{"text": "With the advance of various deep learning frameworks, neural network based models proposed for natural language understanding tasks are becoming increasingly complicated.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 95, "end_pos": 131, "type": "TASK", "confidence": 0.7353104799985886}]}, {"text": "To the best of our knowledge, a considerable part of these complicated models are both hierarchical and recurrent.", "labels": [], "entities": []}, {"text": "For example, Hierarchical Recurrent EncoderDecoder (HRED) (, which is a conversational model, is constructed by stacking three layers of GRUs ().", "labels": [], "entities": []}, {"text": "Besides, several well-known Machine Reading Comprehension (MRC) models, such as R-NET ( and FusionNet (, are mainly composed of multiple layers of bidirectional or bidirectional LSTMs (BiLSTMs)).", "labels": [], "entities": [{"text": "Machine Reading Comprehension (MRC)", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.7588256597518921}]}, {"text": "The above hierarchical recurrent models have achieved excellent performance, but training them usually consumes a lot of time and memory, that is because their computational graphs contain a large amount of operators and trainable parameters, which makes their training computationally expensive.", "labels": [], "entities": []}, {"text": "According to, in the training of recurrent neural networks, it is the backward propagation rather than the forward propagation that consumes the majority of the computational resources.", "labels": [], "entities": []}, {"text": "Besides, considering the chain rule in the backward propagation, the complexity of computing gradients fora hierarchical recurrent model increases exponentially from the top layer of the model down to the bottom layer.", "labels": [], "entities": []}, {"text": "Therefore, to improve the training efficiency of hierarchical recurrent models, our strategy is to simplify the baseline models by making the lower layers simpler than the upper layers, which we name as \"the lower the simpler\".", "labels": [], "entities": []}, {"text": "Here \"simpler\" means containing less operators and trainable parameters.", "labels": [], "entities": []}, {"text": "This strategy is guaranteed to work, since it can accelerate the computation of gradients, which is the substance of the backward propagation.", "labels": [], "entities": []}, {"text": "However, there is still a big concern: once the baseline models are simplified, will their performance be compromised?", "labels": [], "entities": []}, {"text": "To address this concern, we carryout our proposed strategy to simplify two typical hierarchical recurrent models, namely HRED and R-NET, whose basic building block is GRU.", "labels": [], "entities": []}, {"text": "Specifically, we propose Scalar Gated Unit (SGU), which is a simplified variant of GRU, and use it to replace the GRUs at the middle layers of HRED and R-NET.", "labels": [], "entities": []}, {"text": "Besides, we also use Fixed-size Ordinally-Forgetting Encoding (FOFE) (, which is an efficient encoding method without any trainable parameter, to replace the GRUs at the bottom layers of HRED and R-NET.", "labels": [], "entities": [{"text": "Fixed-size Ordinally-Forgetting Encoding (FOFE)", "start_pos": 21, "end_pos": 68, "type": "METRIC", "confidence": 0.6001238624254862}]}, {"text": "In the experiments, we separately compare the simplified HRED and the simplified R-NET with their baseline models in terms of both the training efficiency and the performance.", "labels": [], "entities": [{"text": "HRED", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.49767962098121643}]}, {"text": "The experimental results show that the simplified models contain significantly less trainable parameters, consume significantly less training time, and achieve slightly better performance than their baseline models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the simplified HRED with the baseline HRED on two dialogue datasets, namely) and Ubuntu (.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9548275470733643}]}, {"text": "MovieTriples contains over 240, 000 dialogues collected from various movie scripts, with each dialogue consisting of three sentences.", "labels": [], "entities": [{"text": "MovieTriples", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9352524280548096}]}, {"text": "Ubuntu contains over 490, 000 dialogues collected from the Ubuntu chat-logs, with each dialogue consisting of seven sentences on average.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.949871301651001}]}, {"text": "Both MovieTriples and Ubuntu have been randomly partitioned into three parts: a training set (80%), a development set (10%), and a test set (10%).", "labels": [], "entities": []}, {"text": "We compare the simplified R-NET with the baseline R-NET on an MRC dataset, namely SQuAD (.", "labels": [], "entities": [{"text": "MRC dataset", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.8646697998046875}]}, {"text": "SQuAD contains over 100, 000 passage-question pairs with human-generated answer spans, where the passages are collected from Wikipedia, and the answer to each question is guaranteed to be a fragment in the corresponding passage.", "labels": [], "entities": []}, {"text": "Besides, SQuAD has also been randomly partitioned into three parts: a training set (80%), a development set (10%), and a test set (10%).", "labels": [], "entities": []}, {"text": "Both the training set and the development set are publicly available, but the test set is confidential.", "labels": [], "entities": []}, {"text": "For model comparison in the training efficiency, we use the same hardware (i.e., Intel Core i7-6700 CPU and NVIDIA GeForce GTX 1070 GPU) to train both the baseline models and the simplified models.", "labels": [], "entities": []}, {"text": "The experimental results show that our proposed \"the lower the simpler\" strategy improves the training efficiency of both HRED and R-NET without compromising their performance.", "labels": [], "entities": []}, {"text": "On the one hand, as shown in and Table 2, the simplified HRED contains 25%-35% less trainable parameters, consumes over 50% less training time, and achieves slightly better performance than the baseline HRED.", "labels": [], "entities": []}, {"text": "Besides, also shows that appropriately scaling up the model brings better performance but consumes more resource, which implies that the simplified HRED will perform better than the baseline HRED when time or memory is limited.", "labels": [], "entities": []}, {"text": "On the other hand, as shown in, the simplified R-NET contains 13% less trainable parameters, consumes 21% less training time, and achieves slightly better performance than the baseline R-NET.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparing the simplified HRED with the baseline HRED on MovieTriples.", "labels": [], "entities": [{"text": "HRED", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.961925745010376}, {"text": "HRED", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.7758936882019043}, {"text": "MovieTriples", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9456723928451538}]}, {"text": " Table 2: Comparing the simplified HRED with the baseline HRED on Ubuntu.", "labels": [], "entities": [{"text": "HRED", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.7128840684890747}, {"text": "Ubuntu", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.862614095211029}]}, {"text": " Table 3: Comparing the simplified HRED with the baseline HRED on SQuAD.", "labels": [], "entities": [{"text": "HRED", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9401509761810303}, {"text": "SQuAD", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8811451196670532}]}]}