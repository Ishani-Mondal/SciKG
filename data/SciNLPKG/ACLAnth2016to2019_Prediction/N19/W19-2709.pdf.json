{"title": [{"text": "EusDisParser: improving an under-resourced discourse parser with cross-lingual data", "labels": [], "entities": []}], "abstractContent": [{"text": "Development of discourse parsers to annotate the relational discourse structure of a text is crucial for many downstream tasks.", "labels": [], "entities": []}, {"text": "However, most of the existing work focuses on English, assuming a quite large dataset.", "labels": [], "entities": []}, {"text": "Discourse data have been annotated for Basque, but training a system on these data is challenging since the corpus is very small.", "labels": [], "entities": []}, {"text": "In this paper, we create the first parser based on RST for Basque, and we investigate the use of data in another language to improve the performance of a Basque discourse parser.", "labels": [], "entities": [{"text": "Basque discourse parser", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.6078130106131235}]}, {"text": "More precisely, we build a monolingual system using the small set of data available and investigate the use of multilingual word embeddings to train a system for Basque using data annotated for another language.", "labels": [], "entities": []}, {"text": "We found that our approach to building a system limited to the small set of data available for Basque allowed us to get an improvement over previous approaches making use of many data annotated in other languages.", "labels": [], "entities": []}, {"text": "At best, we get 34.78 in F1 for the full discourse structure.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9992591738700867}]}, {"text": "More data annotation is necessary in order to improve the results obtained with these techniques.", "labels": [], "entities": []}, {"text": "We also describe which relations match with the gold standard, in order to understand these results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several theoretical frameworks exist for discourse analysis, and automatic discourse analyzers (ADA) have been developed within each framework, but mostly for English texts: i) under Rhetorical Structure Theory (RST) (: see for example ( ii) under Segmented Discourse Representation Theory (SDRT), as the one developed by iii) or Penn Discourse Treebank (PDTB) style as the one described in ().", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7218470722436905}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 183, "end_pos": 216, "type": "TASK", "confidence": 0.7010252277056376}, {"text": "Segmented Discourse Representation Theory (SDRT)", "start_pos": 248, "end_pos": 296, "type": "TASK", "confidence": 0.8031298773629325}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 330, "end_pos": 360, "type": "DATASET", "confidence": 0.950332780679067}]}, {"text": "Within RST, discourse parsing is done in two steps: (i) Linear discourse segmentation: The text is divided into EDUs (Elementary Discourse Unit) ; (ii) Rhetorical annotation: All the EDUs are linked following tree structure (RS-tree).", "labels": [], "entities": [{"text": "RST", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.948325514793396}, {"text": "discourse parsing", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6804170161485672}, {"text": "Linear discourse segmentation", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6381891767183939}]}, {"text": "proposed to carryout an intermediate phase, between segmentation and rhetorical labelling, the annotation of the central unit.", "labels": [], "entities": [{"text": "rhetorical labelling", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.8235793113708496}]}, {"text": "Although several ADAs exist, researchers have still face important issues: \u2212 ADAs are not easy to test unless an online version exists.", "labels": [], "entities": []}, {"text": "\u2212 Most of them were developed for English or languages with a considerable amount of resources.", "labels": [], "entities": []}, {"text": "\u2212 The evaluation methods do not demonstrate robustness and reliability of the systems.", "labels": [], "entities": []}, {"text": "Moreover, when working on low resourced languages such as Basque, with few resources available, one has to deal with additional difficulties: \u2212 Information obtained from automatic tools (e.g. PoS tags) are often less accurate, or are even sometimes not available.", "labels": [], "entities": []}, {"text": "\u2212 The terminology and discourse markers (or signals) are not standardised since students have developed the domain or topic.", "labels": [], "entities": []}, {"text": "2 \u2212 Even in academic texts, language standards are not known nor established, and there are more writing errors.", "labels": [], "entities": []}, {"text": "\u2212 Finding reliable and third parties annotated corpora is challenging.", "labels": [], "entities": []}, {"text": "Due to these difficulties, the way to get an ADA for some languages was done step by step, follow-ing a partial labelling strategy, such as: 3 focusing on segmentation, as done for French, for Spanish (da ) and for Basque (, or on the detection of centrals units, as done for Basque ( ) and for Spanish ( . Moreover, a system has been developed for identifying nuclearity and intra-sentential relations for Spanish (da , and a rule-based discourse parser exist for Brazilian Portuguese ().", "labels": [], "entities": []}, {"text": "The first versions of these tools were developed mostly following simple techniques (i.e. a rule-based approach) and, later, that results were improved using more complicated techniques, more amount of data or machine learning techniques.", "labels": [], "entities": []}, {"text": "Recently, from a different perspective, using a cross-lingual discourse parsing approach, carried out a discourse parser which includes several languages: English, Basque, Spanish, Portuguese, Dutch and German.", "labels": [], "entities": [{"text": "cross-lingual discourse parsing", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.725629965464274}, {"text": "discourse parser", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7465431392192841}]}, {"text": "For Basque, report at best 29.5% in F1 for the full discourse structure using training data from other languages.", "labels": [], "entities": [{"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9996880292892456}]}, {"text": "However, we want to underline that do not use specific materials (e.g. word embeddings) for Basque, and they do not report results fora system trained on Basque data only.", "labels": [], "entities": []}, {"text": "When experimenting on a low-resourced language (i.e., less that 100 document in total), such as Basque, they only report results with a union of all the training data for the other languages, possibly using some held-out documents to tune the hyper-parameters of their model.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the use of data in another language to improve the performance of a discourse parser for Basque, an under-resourced language.", "labels": [], "entities": []}, {"text": "Moreover, we create and evaluate the first parser for Basque, and investigate the following questions: \u2212 Can we learn from other languages and improve the performance of a parser?", "labels": [], "entities": []}, {"text": "\u2212 What differences emerge between the human and machine annotation?", "labels": [], "entities": []}, {"text": "\u2212 Is the parser confident about same rhetorical relations as humans?", "labels": [], "entities": []}, {"text": "As we mentioned, a limit of this work is that more annotation data is necessary, in order to improve the results of the Basque parser.", "labels": [], "entities": []}, {"text": "All of them can be tested online.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2, Section 3, Section 4 and Section 5 present the system of the Basque discourse parser, the approach and the settings of the system.", "labels": [], "entities": [{"text": "Basque discourse parser", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.6570732494195303}]}, {"text": "Section 6 lays out the evaluation of the results.", "labels": [], "entities": []}, {"text": "Finally, section 7 sets out the conclusions, the limitations and the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report both macro-and micro-average scores, since both have been reported in previous studies, as noted in following the quantitative evaluation mehtod of Marcu (2000).", "labels": [], "entities": []}, {"text": "Discourse annotation and its evaluation is a challenging task (.", "labels": [], "entities": [{"text": "Discourse annotation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.838536411523819}]}, {"text": "To understand what this parser is doing, we followed the evaluation method proposed by , and compare our best systems in order to understand what kind of RS-trees the system is producing.", "labels": [], "entities": []}, {"text": "Note that scores per relation or confusion matrices are rarely given in studies on discourse parsing, while it would allow fora better and deeper comparison of the systems developed.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7208123803138733}]}], "tableCaptions": [{"text": " Table 1: Number of documents (#Doc), words  (#Words), relations (#Rel, originally), labels (#Lab, re- lation and nuclearity) and EDUs (#EDU).", "labels": [], "entities": [{"text": "EDUs", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9591267704963684}]}, {"text": " Table 2: Mono-lingual systems, micro-and macro- averaged F1 scores on the test set. Results reported  from Braud et al. (2017) were obtained in a cross- lingual setting without the use of pre-trained embed- dings.", "labels": [], "entities": [{"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.817135751247406}]}, {"text": " Table 3: Results for the mono-lingual systems built for  the source languages used in the cross-lingual setting.  The systems use the bi-lingual word embeddings built  by Artetxe et al. (2018).", "labels": [], "entities": []}, {"text": " Table 4: Cross-lingual systems evaluated on Basque  using the word embeddings built by Artetxe et al.  (2018), results on the Basque test set. 'Src only':  trained only on source language training data (the  hyper-parameters are optimized using the Basque de- velopment set). 'Src+Tgt': trained on source language  training data + Basque development set (the hyper- parameters are the ones used in the monolingual set- ting).", "labels": [], "entities": [{"text": "Basque test set", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.8125592867533366}]}, {"text": " Table 5: Central Unit reliability", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix of the Basque monolin- gual parser: gold standard in files and parser output in  columns. Agreement in bold", "labels": [], "entities": [{"text": "Basque monolin- gual parser", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.5327863574028016}]}, {"text": " Table 7: Description of gold and automatic label  matching", "labels": [], "entities": []}, {"text": " Table 8: Parser annotation confusion matrix", "labels": [], "entities": [{"text": "Parser annotation confusion", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.572799930969874}]}, {"text": " Table 9: Description of gold and automatic label  matching for Portuguese.", "labels": [], "entities": []}, {"text": " Table 10: Description of gold and automatic label  matching for Basque, using cross-lingual information  from Portuguese", "labels": [], "entities": [{"text": "label  matching", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.6967758387327194}]}]}