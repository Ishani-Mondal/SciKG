{"title": [{"text": "Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition", "labels": [], "entities": [{"text": "Examining Semantic Composition", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.790220320224762}]}], "abstractContent": [{"text": "Bigrams (two-word sequences) hold a special place in semantic composition research since they are the smallest unit formed by composing words.", "labels": [], "entities": [{"text": "semantic composition research", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.8229432900746664}]}, {"text": "A semantic relatedness dataset that includes bigrams will thus be useful in the development of automatic methods of semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7284543514251709}]}, {"text": "However, existing relatedness datasets only include pairs of unigrams (single words).", "labels": [], "entities": []}, {"text": "Further, existing datasets were created using rating scales and thus suffer from limitations such as inconsistent annotations and scale region bias.", "labels": [], "entities": []}, {"text": "In this paper, we describe how we created a large, fine-grained, bigram relatedness dataset (BiRD), using a comparative annotation technique called Best-Worst Scaling.", "labels": [], "entities": []}, {"text": "Each of BiRD's 3,345 English term pairs involves at least one bigram.", "labels": [], "entities": [{"text": "BiRD", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8032180070877075}]}, {"text": "We show that the relatedness scores obtained are highly reliable (split-half reliability r = 0.937).", "labels": [], "entities": [{"text": "split-half reliability r", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.9333341519037882}]}, {"text": "We analyze the data to obtain insights into bigram semantic relatedness.", "labels": [], "entities": [{"text": "bigram semantic relatedness", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.574284185965856}]}, {"text": "Finally, we present benchmark experiments on using the relatedness dataset as a testbed to evaluate simple unsupervised measures of semantic composition.", "labels": [], "entities": []}, {"text": "BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.", "labels": [], "entities": [{"text": "BiRD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7563061714172363}]}], "introductionContent": [{"text": "The term semantic relatedness refers to the extent to which two concepts are close in meaning.", "labels": [], "entities": []}, {"text": "The ability to assess semantic relatedness is central to the use and understanding of language.", "labels": [], "entities": [{"text": "assess semantic relatedness", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.618260016043981}]}, {"text": "Manual ratings of semantic relatedness are useful for: (a) obtaining insights into how humans perceive and use language; and (b) developing and evaluating automatic natural language systems.", "labels": [], "entities": []}, {"text": "Existing datasets of semantic relatedness, such as the one by, only focus on pairs of unigrams (single words).", "labels": [], "entities": []}, {"text": "However, the concept of semantic relatedness applies more generally to any unit of text.", "labels": [], "entities": []}, {"text": "Work in semantic representation explores how best to represent the meanings of words, phrases, and sentences.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7419211268424988}]}, {"text": "Bigrams (two-word sequences) are especially important there since they are the smallest unit formed by composing words.", "labels": [], "entities": []}, {"text": "Thus it would be useful to have large semantic relatedness datasets involving bigrams.", "labels": [], "entities": []}, {"text": "Existing datasets also suffer from shortcomings due to the annotation schemes employed.", "labels": [], "entities": []}, {"text": "Except in the case of a few small but influential datasets, such as those by and, annotations were obtained using rating scales.", "labels": [], "entities": []}, {"text": "(Annotators were asked to give scores for each pair; usually on a discrete 0 to 5 scale.)", "labels": [], "entities": []}, {"text": "Rating scales suffer from significant known limitations, including: inconsistencies in annotations by different annotators, inconsistencies in annotations by the same annotator, scale region bias (annotators often have a bias towards a portion of the scale), and problems associated with a fixed granularity.", "labels": [], "entities": []}, {"text": "Best-Worst Scaling (BWS) is an annotation scheme that addresses these limitations by employing comparative annotations.", "labels": [], "entities": [{"text": "Best-Worst Scaling (BWS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7240957677364349}]}, {"text": "Annotators are given n items at a time (an n-tuple, where n > 1 and commonly n = 4).", "labels": [], "entities": []}, {"text": "They are asked which item is the best (highest in terms of the property of interest) and which is the worst (least in terms of the property of interest).", "labels": [], "entities": []}, {"text": "1 When working on 4-tuples, best-worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six items (i.e., fora 4-tuple with items A, B, C, and D, if A is the best, and Dis the worst, then A > B, A > C, A > D, B > D, and C > D).", "labels": [], "entities": []}, {"text": "It has been empirically shown that annotating 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items).", "labels": [], "entities": []}, {"text": "showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales.", "labels": [], "entities": [{"text": "BWS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9545935392379761}]}, {"text": "In this paper, we describe how we obtained fine-grained human ratings of semantic relatedness for English term pairs involving at least one bigram.", "labels": [], "entities": []}, {"text": "The other term in the pair is either another bigram or a unigram.", "labels": [], "entities": []}, {"text": "We first selected a set of target bigrams AB (A represents the first word in the bigram and B represents the second word).", "labels": [], "entities": []}, {"text": "For each AB, we created several pairs of the form AB-X, where X is a unigram or bigram.", "labels": [], "entities": []}, {"text": "As X's we chose terms from a diverse set of language resources: \u2022 terms that are transpose bigrams BA-where the first word is B and the second word is A (taken from occurrences in Wikipedia); \u2022 terms that are related to AB by traditional semantic relations such as hypernymy, hyponymy, holonymy, meronymy, and synonymy (taken from WordNet); and \u2022 terms that are co-aligned with AB in a parallel corpus (taken from a machine translation phrase table).", "labels": [], "entities": []}, {"text": "The dataset includes 3,345 term pairs corresponding to 410 ABs.", "labels": [], "entities": [{"text": "ABs", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9658914804458618}]}, {"text": "We refer to this dataset as the Bigram Relatedness Dataset (or, BiRD).", "labels": [], "entities": [{"text": "Bigram Relatedness Dataset", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.7125332951545715}]}, {"text": "We use BWS to obtain semantic relatedness by: (1) creating items that are pairs of terms, and (2) prompting four items (pairs) at a time and asking annotators to mark the pair that is most related and the pair that is least related.", "labels": [], "entities": []}, {"text": "Once the annotations are complete, we obtain real-valued scores of semantic relatedness for each pair using simple arithmetic on the counts of how often an item is chosen best and worst.", "labels": [], "entities": []}, {"text": "(Details in Section 3.)", "labels": [], "entities": []}, {"text": "To evaluate the quality of BiRD we determine the consistency of the BWS annotations.", "labels": [], "entities": [{"text": "consistency", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9956061244010925}, {"text": "BWS annotations", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.8342983424663544}]}, {"text": "A commonly used approach to determine consistency in dimensional annotations is to calculate split-half reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.6751664280891418}]}, {"text": "We show that our semantic relatedness annotations have a split-half reliability score of r = 0.937, indicating high reliability, that is, if the annotations were repeated then similar scores and rankings would be obtained.", "labels": [], "entities": [{"text": "split-half reliability score", "start_pos": 57, "end_pos": 85, "type": "METRIC", "confidence": 0.7442635695139567}, {"text": "reliability", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9611700773239136}]}, {"text": "(Details in Section 4.)", "labels": [], "entities": []}, {"text": "We use BiRD to (a) obtain insights into bigram semantic relatedness, and (b) to evaluate automatic semantic composition methods.", "labels": [], "entities": [{"text": "bigram semantic relatedness", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.5254641473293304}]}], "datasetContent": [{"text": "Using BWS: BWS has been used for creating datasets for relational similarity, word-sense disambiguation, word-sentiment intensity, word-emotion intensity, and tweet-emotion intensity.", "labels": [], "entities": [{"text": "BWS", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.7320888042449951}, {"text": "word-sense disambiguation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7424255013465881}]}, {"text": "The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a).", "labels": [], "entities": [{"text": "BWS dataset", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.7592784464359283}, {"text": "NRC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8705510497093201}]}, {"text": "We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS.", "labels": [], "entities": [{"text": "bigram relatedness dataset", "start_pos": 67, "end_pos": 93, "type": "DATASET", "confidence": 0.6764267385005951}, {"text": "BWS", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.869592547416687}]}], "tableCaptions": [{"text": " Table 2: BiRD annotation statistics. SHR = split-half reliability (as measured by Pearson correlation).", "labels": [], "entities": [{"text": "SHR", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9805155992507935}, {"text": "split-half reliability", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.8959626853466034}, {"text": "Pearson correlation", "start_pos": 83, "end_pos": 102, "type": "METRIC", "confidence": 0.9389468729496002}]}, {"text": " Table 3: Average relatedness and standard deviation  (\ud97b\udf59) scores for term pairs from the various sources.", "labels": [], "entities": []}, {"text": " Table 4: Pearson correlations of model predictions with  BiRD relatedness ratings. Highest scores are in bold.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9193952083587646}, {"text": "BiRD relatedness ratings", "start_pos": 58, "end_pos": 82, "type": "METRIC", "confidence": 0.8521658182144165}]}]}