{"title": [{"text": "Incorporating Word Attention into Character-Based Word Segmentation", "labels": [], "entities": [{"text": "Incorporating Word Attention into Character-Based Word Segmentation", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7277252078056335}]}], "abstractContent": [{"text": "Neural network models have been actively applied to word segmentation, especially Chi-nese, because of the ability to minimize the effort in feature engineering.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7322480529546738}]}, {"text": "Typical seg-mentation models are categorized as character-based, for conducting exact inference, or word-based, for utilizing word-level information.", "labels": [], "entities": []}, {"text": "We propose a character-based model utilizing word information to leverage the advantages of both types of models.", "labels": [], "entities": []}, {"text": "Our model learns the importance of multiple candidate words fora character on the basis of an attention mechanism, and makes use of it for seg-mentation decisions.", "labels": [], "entities": []}, {"text": "The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6959346234798431}, {"text": "natural language processing (NLP)", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.8395481606324514}]}, {"text": "In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7539002001285553}, {"text": "feature engineering", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7280830144882202}]}, {"text": "These models are categorized as character-based or word-based.", "labels": [], "entities": []}, {"text": "Wordbased models () directly segment a character sequence into words and can easily achieve the benefits of word-level information.", "labels": [], "entities": []}, {"text": "However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length.", "labels": [], "entities": []}, {"text": "On the other hand, character-based models () treat word segmentation as sequence labeling.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7308274507522583}]}, {"text": "These models typically predict optimal label sequences while considering adjacent labels.", "labels": [], "entities": []}, {"text": "Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics.", "labels": [], "entities": []}, {"text": "In particular, the candidate word information fora character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character.", "labels": [], "entities": []}, {"text": "For example, there are three or four candidate words for characters x 3 , x 4 and x 5 in a sentence x 1:5 in.", "labels": [], "entities": []}, {"text": "A feasible solution to develop a model with both characteristics is to incorporate word information into a character-based framework.", "labels": [], "entities": []}, {"text": "An example of such work is that of.", "labels": [], "entities": []}, {"text": "They concatenated embeddings of a character and candidate words and used it in their convolutional neural network (CNN)-based model.", "labels": [], "entities": []}, {"text": "They treated candidate words equivalently, although the plausibility of a candidate word differs in the context of a target character.", "labels": [], "entities": []}, {"text": "In this paper, we propose a character-based word segmentation model that utilizes word information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7025979161262512}]}, {"text": "Our model is based on a BiLSTM-CRF architecture that has been successfully applied to sequence labeling tasks ().", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7185671726862589}]}, {"text": "Differing from the work of, our model learns and distinguishes the importance of candidate words fora character in a context, by applying an attention mechanism (.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We introduce word information and an attention mechanism into a character-based word segmentation framework, to distinguish and leverage the importance of candidate words Sentence: x1:5 = \u27e8kare wa nihonjin\u27e9 (He is a Japanese.)", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7347365319728851}]}, {"text": "Candidate words {wj} for characters {xi} in the sentence: (NOM) (day) (book) (person) (Japan) (the person) (Japanese) Figure 1: An example of candidate words w 1:8 retrieved from a vocabulary fora sentence x 1:5 . Strings in angle brackets \"\" and in parentheses \"()\" respectively indicate (typical) readings and English translations of words.", "labels": [], "entities": [{"text": "NOM", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8541566133499146}]}, {"text": "The value in each (i, j) represents whether the i-th character is contained by the j-th word (i.e., \u03b4 ij in Eq.).", "labels": [], "entities": []}, {"text": "\u2022 We empirically reveal that accurate attention to proper candidate words leads to correct segmentations.", "labels": [], "entities": []}, {"text": "\u2022 Our model outperforms the state-of-the-art word segmentation models on both Japanese and Chinese datasets.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7508086264133453}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Hyperparameters common between the base- line and the proposed model (top) and specific to the  proposed model (bottom). BiLSTM-C and BiLSTM- WC, respectively, indicate recurrent layers for charac- ter and word-integrated character representation.", "labels": [], "entities": [{"text": "word-integrated character representation", "start_pos": 216, "end_pos": 256, "type": "TASK", "confidence": 0.6230045656363169}]}, {"text": " Table 2: Results of the test sets. The table shows the  mean of F1 scores of three runs for the baseline (BASE)  and the proposed model variants. The symbols  \u2020 and  \u2021  indicate statistical significance at 0.001 level over the  baseline and over the variant without attention, respec- tively.", "labels": [], "entities": [{"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.993105947971344}, {"text": "BASE", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9961789846420288}]}, {"text": " Table 3: Comparison with state-of-the-art character- based (top) and word-based (middle) and other types  of models (bottom) on the test sets. Models marked  with a symbol indicate ones based on linear statistical  algorithms (), ones with additional unlabeled texts (\u2022)  and ones replacing specific characters as preprocessing  (\u2022). The result with  *  is from our run on their released  implementation.", "labels": [], "entities": []}, {"text": " Table 4: Attention accuracy, segmentation accuracy and F1-score on the development sets. The table also shows  segmentation accuracy based on the cases where attention is correct (Acc-CA) and incorrect (Acc-IA), the average  number of candidate words for a character (NoC) and lower and upper bounds of attention accuracy.", "labels": [], "entities": [{"text": "Attention", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9295850396156311}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.5351215600967407}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8988689184188843}, {"text": "F1-score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994939565658569}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.8089008331298828}, {"text": "Acc-CA", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9731084108352661}, {"text": "Acc-IA)", "start_pos": 204, "end_pos": 211, "type": "METRIC", "confidence": 0.8830840587615967}, {"text": "accuracy", "start_pos": 314, "end_pos": 322, "type": "METRIC", "confidence": 0.5724200010299683}]}]}