{"title": [{"text": "Multi-Level Memory for Task Oriented Dialogs", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent end-to-end task oriented dialog systems use memory architectures to incorporate external knowledge in their dialogs.", "labels": [], "entities": []}, {"text": "Current work makes simplifying assumptions about the structure of the knowledge base (such as the use of triples to represent knowledge) and combines dialog utterances (context), as well as, knowledge base (KB) results, as part of the same memory.", "labels": [], "entities": []}, {"text": "This causes an explosion in the memory size, and makes reasoning over memory , harder.", "labels": [], "entities": []}, {"text": "In addition, such a memory design forces hierarchical properties of the data to befit into a triple structure of memory.", "labels": [], "entities": []}, {"text": "This requires the memory reader to learn how to infer relationships across otherwise connected attributes.", "labels": [], "entities": []}, {"text": "In this paper we relax the strong assumptions made by existing architectures and use separate memories for modeling dialog context and KB results.", "labels": [], "entities": []}, {"text": "Instead of using triples to store KB results, we introduce a novel multi-level memory architecture consisting of cells for each query and their corresponding results.", "labels": [], "entities": []}, {"text": "The multi-level memory first addresses queries, followed by results and finally each key-value pair within a result.", "labels": [], "entities": []}, {"text": "We conduct detailed experiments on three publicly available task oriented dialog data sets and we find that our method conclusively outperforms current state-of-the-art models.", "labels": [], "entities": []}, {"text": "We report a 15-25% increase in both entity F1 and BLEU scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.8477266430854797}, {"text": "BLEU scores", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9709534645080566}]}], "introductionContent": [{"text": "Task oriented dialog systems are designed to complete a user specified goal, or service an information request using natural language exchanges.", "labels": [], "entities": []}, {"text": "Unlike open domain end-to-end neural dialog models, task oriented systems rely on external knowledge sources, outside of the current conversation context, to return a response (Henderson * Work done during internship at IBM Research AI et al., 2014a;.", "labels": [], "entities": []}, {"text": "For instance, in the example shown in Table 1, a dialog agent giving tour package recommendations needs to be able to first query an external knowledge source to determine packages that meet a user's requirement, and then respond accordingly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present our experiments using three real world publicly available multi-turn task oriented dia- dialogues developed to study the role of memory in task oriented dialogue systems.", "labels": [], "entities": []}, {"text": "The dataset is set in the domain of booking travel packages which involves flights and hotels.", "labels": [], "entities": []}, {"text": "In contrast to the previous two datasets, this dataset contains dialogs that require the agent to remember all information presented previously as well as support results from multiple queries to the knowledge base.", "labels": [], "entities": []}, {"text": "A user's preferences may change as the dialogue proceeds, and can also refer to previously presented queries (non-sequential dialog).", "labels": [], "entities": []}, {"text": "Thus, to store multiple queries, we require 3 levels in our multi-level memory as compared to 2 levels in the other datasets, since they don't have more than one query.", "labels": [], "entities": []}, {"text": "We do not use the dialogue frame annotations and use only the raw text of the dialogues.", "labels": [], "entities": []}, {"text": "We map ground-truth queries to API calls that are also required to be generated by the model.", "labels": [], "entities": []}, {"text": "Recent work has used this dataset only for frame tracking ( ) and dialogue act prediction (.", "labels": [], "entities": [{"text": "frame tracking", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.8630409836769104}, {"text": "dialogue act prediction", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.791784942150116}]}, {"text": "To the best of our knowledge we are the first to attempt the end-to-end dialog task using this dataset.", "labels": [], "entities": []}, {"text": "We also conducted a blind user study that compared outputs from our model, Mem2Seq and KVRet systems.", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9575188159942627}]}, {"text": "We used 96 randomly selected examples from each test split of Maluuba and CamRest datasets resulting in a total of 192 examples.", "labels": [], "entities": [{"text": "CamRest datasets", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.8796221613883972}]}, {"text": "Our study was split across 8 users who were provided with results fetched from the KB, current dialog context, gold response and the outputs of each of the models.", "labels": [], "entities": [{"text": "KB", "start_pos": 83, "end_pos": 85, "type": "DATASET", "confidence": 0.8081037402153015}]}, {"text": "Model outputs were shuffled in each example and users were asked to score each output between 1 (lowest) to 5 (highest) in terms of its accuracy of information in response and the quality of language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9963850975036621}]}, {"text": "The results of this study are presented in.", "labels": [], "entities": []}, {"text": "We also report the MRR (mean-reciprocal rank) for model preference along with other scores.", "labels": [], "entities": [{"text": "MRR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9984605312347412}, {"text": "mean-reciprocal rank)", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.8891912698745728}]}, {"text": "It can be seen that our model consistently ranks high for both information accuracy and language quality as well as reports a higher MRR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9833716750144958}, {"text": "MRR", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9209782481193542}]}, {"text": "To further understand the quality of model performance, we asked the human evaluators whether their best ranked model output was a useful response.", "labels": [], "entities": []}, {"text": "We saw that the evaluators agreed in 76.04% and 58.33% of the cases for CamRest and Maluuba datasets respectively.", "labels": [], "entities": [{"text": "CamRest", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.980217456817627}, {"text": "Maluuba datasets", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.8221069574356079}]}, {"text": "We observe that the results from human evaluation go hand-in-hand with automatic evaluation and reinforce our claim that separating context, KB memory and using a multilevel representation for the KB memory are useful for improving dialog modeling.", "labels": [], "entities": [{"text": "dialog modeling", "start_pos": 232, "end_pos": 247, "type": "TASK", "confidence": 0.9243867099285126}]}], "tableCaptions": [{"text": " Table 1: A goal oriented dialog based on the Frames  dataset (El Asri et al., 2017) along with an external  knowledge source with each row containing a tour  package.", "labels": [], "entities": [{"text": "Frames  dataset (El Asri et al., 2017)", "start_pos": 46, "end_pos": 84, "type": "DATASET", "confidence": 0.9163317799568176}]}, {"text": " Table 3: Statistics for 3 different datasets.", "labels": [], "entities": []}, {"text": " Table 5: Percentage (%) of category-wise (context vs  KB) ground truth entities correctly captured in gener- ated response. Abbreviation Ctxt denotes context.", "labels": [], "entities": [{"text": "Abbreviation Ctxt", "start_pos": 125, "end_pos": 142, "type": "METRIC", "confidence": 0.9133336246013641}]}, {"text": " Table 6: Model ablation study : Effect of (i) separate memory and (ii) multi-level memory design.", "labels": [], "entities": []}, {"text": " Table 7: Informational accuracy and language quality  scores using a human study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9870536923408508}]}, {"text": " Table 8: Comparing the responses generated by various  models on an example in test set of Maluuba Frames.", "labels": [], "entities": []}]}