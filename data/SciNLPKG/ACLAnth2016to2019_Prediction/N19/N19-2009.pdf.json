{"title": [{"text": "Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce", "labels": [], "entities": [{"text": "Short Product Title Generation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6304935216903687}]}], "abstractContent": [{"text": "Nowadays, more and more customers browse and purchase products in favor of using mobile E-Commerce Apps such as Taobao and Amazon.", "labels": [], "entities": [{"text": "Taobao", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9805770516395569}]}, {"text": "Since merchants are usually inclined to describe redundant and over-informative product titles to attract attentions from customers, it is important to concisely display short product titles on limited screen of mobile phones.", "labels": [], "entities": []}, {"text": "To address this discrepancy, previous studies mainly consider textual information of long product titles and lacks of human-like view during training and evaluation process.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Multi-Modal Gen-erative Adversarial Network (MM-GAN) for short product title generation in E-Commerce, which innovatively incorporates image information and attribute tags from product, as well as textual information from original long titles.", "labels": [], "entities": [{"text": "short product title generation", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6342282444238663}]}, {"text": "MM-GAN poses short title generation as a reinforcement learning process, where the generated titles are evaluated by the discrimi-nator in a human-like view.", "labels": [], "entities": [{"text": "short title generation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6259636878967285}]}, {"text": "Extensive experiments on a large-scale E-Commerce dataset demonstrate that our algorithm outperforms other state-of-the-art methods.", "labels": [], "entities": [{"text": "E-Commerce dataset", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8530001938343048}]}, {"text": "Moreover, we deploy our model into a real-world online E-Commerce environment and effectively boost the performance of click through rate and click conversion rate by 1.66% and 1.87%, respectively .", "labels": [], "entities": []}], "introductionContent": [{"text": "E-commerce companies such as TaoBao and Amazon put many efforts to improve the user experience of their mobile Apps.", "labels": [], "entities": [{"text": "TaoBao", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9669867157936096}]}, {"text": "For the sake of improving retrieval results by search engines, merchants usually write lengthy, over-informative, and sometimes incorrect titles, e.g., the original product title in contains more than 20 Chinese words, which maybe suitable for PCs.", "labels": [], "entities": []}, {"text": "However, these titles are cut down and no more than 10 words can be displayed on a mobile phone with limited screen size varying from 4 to 5.8 inches.", "labels": [], "entities": []}, {"text": "Hence, to properly display products in mobile screen, it is important to produce succinct short titles to preserve important information of original long titles and accurate descriptions of products.", "labels": [], "entities": []}, {"text": "This problem is related to text summarization, which can be categorized into two classes: extractive () and abstractive) methods.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7567154765129089}]}, {"text": "The extractive methods select important words from original titles, while the abstractive methods generate titles by extracting words from original titles or generating new words from data corpus.", "labels": [], "entities": []}, {"text": "They usually approximate such goals by predicting the next word given previous predicted words using maximum likelihood estimation (MLE) objective.", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE) objective", "start_pos": 101, "end_pos": 146, "type": "METRIC", "confidence": 0.7976073324680328}]}, {"text": "Despite their successes to a large extent, they suffer from the issue of exposure bias (.", "labels": [], "entities": []}, {"text": "It may cause the models to behave in undesired ways, e.g., generating repetitive or truncated outputs.", "labels": [], "entities": []}, {"text": "In addition, predicting next word based on previously generated words will make the learned model lack of human-like holistic view of the whole generated short product titles.", "labels": [], "entities": [{"text": "predicting next word", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8649751941363016}]}, {"text": "More recent state-of-the-art methods () treat short product titles generation as a sentence compression task following attention-based extractive mechanism.", "labels": [], "entities": [{"text": "short product titles generation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.7229337096214294}, {"text": "sentence compression task", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.7858900725841522}]}, {"text": "They extract key characteristics mainly from original long product titles.", "labels": [], "entities": []}, {"text": "However, in real E-Commerce scenario, product titles are usually redundant and over-informative, and sometimes even inaccurate, e.g., long titles of a cloth may include both \"\u00e5\u00bb\u00e5|\u00e7\u00e9 (hip-pop|wild)\" and \"ae\u00e8 o |ae\u00b7\u00e5 (artsy|delicate)\" simultaneously.", "labels": [], "entities": []}, {"text": "It is a tough task to generate succinct and accurate short titles merely relying on the original titles.", "labels": [], "entities": []}, {"text": "Therefore, it is insufficient to regard short title generation as traditional text summarization problem in which original text has already contained complete information.", "labels": [], "entities": [{"text": "short title generation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6639114121596018}, {"text": "text summarization", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6937209367752075}]}, {"text": "In this paper, we propose a novel Multi-Modal Generative Adversarial Network, named MM-GAN, to better generate short product titles.", "labels": [], "entities": []}, {"text": "It contains a generator and a discriminator.", "labels": [], "entities": []}, {"text": "The generator generates a short product title based on original long titles, with additional information from the corresponding visual image and attribute tags.", "labels": [], "entities": []}, {"text": "On the other hand, the discriminator tries to distinguish whether the generated short titles are humanproduced or machine-produced in a human-like view.", "labels": [], "entities": []}, {"text": "The task is treated as a reinforcement learning problem, in which the quality of a machinegenerated short product title depends on its ability to fool the discriminator into believing it is generated by human, and output of the discriminator is a reward for the generator to improve generated quality.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: \u2022 In this paper, we focus on a fundamental problem existing in the E-Commerce industry, i.e., generating short product titles for mobile ECommerce Apps.", "labels": [], "entities": []}, {"text": "We formulate the problem as a reinforcement learning task; \u2022 We design a multi-modal generative adversarial network to consider multiple modalities of inputs for better short product titles generation in E-commerce; \u2022 To verify the effectiveness of our proposed model, we deploy it into a mobile Ecommerce App.", "labels": [], "entities": []}, {"text": "Extensive experiments on a large-scale real-world dataset with A/B testing show that our proposed model outperforms state-of-the-art methods.", "labels": [], "entities": [{"text": "A/B testing", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.8434160798788071}]}], "datasetContent": [{"text": "The dataset used in our experiment is crawled from a module named \u6709\u597d\u8d27 (Youhaohuo) of the well-known \u6dd8\u5b9d (TAOBAO) platform in China.", "labels": [], "entities": []}, {"text": "Every product in the dataset includes along product title and a short product title written by professional writers, along with product several high quality visual images and attributes tags, here for each product we use its main image.", "labels": [], "entities": []}, {"text": "This Youhaohuo module includes more than 100 categories of products, we crawled top 7 categories of them in the module, and exclude the products with original long titles shorter than 10 Chinese characters.", "labels": [], "entities": [{"text": "Youhaohuo module", "start_pos": 5, "end_pos": 21, "type": "DATASET", "confidence": 0.933439701795578}]}, {"text": "We further tokenize the original long tittles and short titles into Chinese or English words, e.g. \"skirt\" is a word in English and \u534a\u8eab\u88d9 is a word in Chinese.", "labels": [], "entities": []}, {"text": "shows some details of the dataset.", "labels": [], "entities": []}, {"text": "We randomly select 1.6M samples for training, 0.2M samples for validation, and test our proposed model on 5000 samples.", "labels": [], "entities": []}, {"text": "We compare our proposed model with the following four baselines: (a) Pointer Network (Ptr-Net) ( which is a seq2seq based framework with pointer-generator network copying words from the source text via pointing.", "labels": [], "entities": []}, {"text": "(b) Feature-Enriched-Net (FE-Net) () which is a deep and wide model based on attentive RNN to generate the textual long product titles.", "labels": [], "entities": [{"text": "FE-Net", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.827681303024292}]}, {"text": "(c) Agreement-based MTL (Agree-MTL) ( ) which is a multi-task learning approach to improve product title compression with user searching log data.", "labels": [], "entities": [{"text": "product title compression", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6425793766975403}]}, {"text": "(d) Generative Adversarial Network (GAN) (  which is a generative adversarial method for text generation with only one modality of input.", "labels": [], "entities": [{"text": "text generation", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7457605302333832}]}, {"text": "We first pre-train the multi-modal generative model given humangenerated data via maximum likelihood estimation (MLE), and we transfer the pretrained model weights for the multi-modal encoder and decoder modules.", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE", "start_pos": 82, "end_pos": 116, "type": "METRIC", "confidence": 0.7865580916404724}]}, {"text": "Then we pre-train the discriminator using human-generated data and machine-generated data.", "labels": [], "entities": []}, {"text": "To get training samples for the discriminator, we sample half of data from multi-modal generator and another half from human-generated data.", "labels": [], "entities": []}, {"text": "After that, we perform normal training process based on pre-trained MM-GAN.", "labels": [], "entities": []}, {"text": "Specifically, we create a vocabulary of 35K words for long product titles and short titles, and another vocabulary of 35k for attribute tags in training data with size of 1.6M.", "labels": [], "entities": []}, {"text": "Words appear less than 8 times in the training set are replaced as <UNK>.", "labels": [], "entities": []}, {"text": "We implement a two-layer LSTM with 100 hidden states to encoder attribute tags, and all other LSTMs in our model are two layers with 512 hidden states.", "labels": [], "entities": []}, {"text": "The last 3 layers of the pre-trained VGG16 network are fine tuned based on the products visual images with 7 classes.", "labels": [], "entities": [{"text": "VGG16 network", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9235950112342834}]}, {"text": "The Adam optimizer () is initialized with a learning rate 10 \u22123 . The multi-modal generator and discriminator are pre-trained for 10000 steps, the normal training steps are set to 13000, the batch size is set to 512 for the discriminator and 256 for the generator, the MC search time is set to 7.", "labels": [], "entities": []}, {"text": "To evaluate the quality of generated product short titles, we follow ( and use standard recall-oriented ROUGE metric, which measures the generated quality by counting the overlap of N-grams be-", "labels": [], "entities": [{"text": "recall-oriented ROUGE metric", "start_pos": 88, "end_pos": 116, "type": "METRIC", "confidence": 0.8665641148885092}]}], "tableCaptions": [{"text": " Table 1: Statistics of the crawled dataset. Here all the  lengths are counted by Chinese or English words.", "labels": [], "entities": []}, {"text": " Table 2: ROUGE performance of different models on the test set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9939633011817932}]}]}