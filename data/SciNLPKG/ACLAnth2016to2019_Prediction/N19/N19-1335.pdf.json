{"title": [{"text": "Benchmarking Approximate Inference Methods for Neural Structured Prediction", "labels": [], "entities": [{"text": "Neural Structured Prediction", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8048515121142069}]}], "abstractContent": [{"text": "Exact structured inference with neural network scoring functions is computationally challenging but several methods have been proposed for approximating inference.", "labels": [], "entities": []}, {"text": "One approach is to perform gradient descent with respect to the output structure directly (Belanger and McCallum, 2016).", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.70673568546772}]}, {"text": "Another approach, proposed recently, is to train a neural network (an \"inference network\") to perform inference (Tu and Gimpel, 2018).", "labels": [], "entities": []}, {"text": "In this paper, we compare these two families of inference methods on three sequence labeling datasets.", "labels": [], "entities": []}, {"text": "We choose sequence labeling because it permits us to use exact inference as a benchmark in terms of speed, accuracy, and search error.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7251551449298859}, {"text": "speed", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9792128205299377}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9979427456855774}]}, {"text": "Across datasets, we demonstrate that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent, while also being faster than exact inference at similar accuracy levels.", "labels": [], "entities": [{"text": "accuracy/search error trade-off", "start_pos": 79, "end_pos": 110, "type": "METRIC", "confidence": 0.7554634749889374}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9858286380767822}]}, {"text": "We find further benefit by combining inference networks and gradient descent, using the former to provide a warm start for the latter.", "labels": [], "entities": []}], "introductionContent": [{"text": "Structured prediction models commonly involve complex inference problems for which finding exact solutions is intractable.", "labels": [], "entities": [{"text": "Structured prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8529087901115417}]}, {"text": "There are generally two ways to address this difficulty.", "labels": [], "entities": []}, {"text": "One is to restrict the model family to those for which inference is feasible.", "labels": [], "entities": []}, {"text": "For example, state-ofthe-art methods for sequence labeling use structured energies that decompose into label-pair potentials and then use rich neural network architectures to define the potentials.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.6887899041175842}]}, {"text": "Exact dynamic programming algorithms like the Viterbi algorithm can be used for inference.", "labels": [], "entities": []}, {"text": "The second approach is to retain computationally-intractable scoring functions but then use approximate methods for inference.", "labels": [], "entities": []}, {"text": "For example, some researchers relax the structured output space from a discrete space to a continuous one and then use gradient descent to maximize the score function with respect to the output (.", "labels": [], "entities": []}, {"text": "Another approach is to train a neural network (an \"inference network\") to output a structure in the relaxed space that has high score under the structured scoring function (.", "labels": [], "entities": []}, {"text": "This idea was proposed as an alternative to gradient descent in the context of structured prediction energy networks.", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7514210045337677}]}, {"text": "In this paper, we empirically compare exact inference, gradient descent, and inference networks for three sequence labeling tasks.", "labels": [], "entities": []}, {"text": "We train conditional random fields (CRFs) for sequence labeling with neural networks used to define the potentials.", "labels": [], "entities": []}, {"text": "We choose a scoring function that permits exact inference via Viterbi so that we can benchmark the approximate methods in terms of search error in addition to speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9950705766677856}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9979488253593445}]}, {"text": "We consider three families of neural network architectures to serve as inference networks: convolutional neural networks (CNNs), recurrent neural networks (RNNs), and sequence-to-sequence models with attention (seq2seq;).", "labels": [], "entities": []}, {"text": "We also use multi-task learning while training inference networks, combining the structured scoring function with a local cross entropy loss.", "labels": [], "entities": []}, {"text": "Our empirical findings can be summarized as follows.", "labels": [], "entities": []}, {"text": "Gradient descent works reasonably well for tasks with small label sets and primarily local structure, like part-of-speech tagging.", "labels": [], "entities": [{"text": "Gradient descent", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7547610998153687}, {"text": "part-of-speech tagging", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7032086402177811}]}, {"text": "However, gradient descent struggles on tasks with longdistance dependencies, even with small label set sizes.", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.8118586242198944}]}, {"text": "For tasks with large label set sizes, inference networks and Viterbi perform comparably, with Viterbi taking much longer.", "labels": [], "entities": []}, {"text": "In this regime, it is difficult for gradient descent to find a good solution, even with many iterations.", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.812507301568985}]}, {"text": "In comparing inference network architectures, (1) CNNs are the best choice for tasks with primarily local structure, like part-of-speech tagging; (2) RNNs can handle longer-distance dependencies while still offering high decoding speeds; and (3) seq2seq networks consistently work better than RNNs, but are also the most computationally expensive.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.7506866753101349}]}, {"text": "We also compare search error between gradient descent and inference networks and measure correlations with input likelihood.", "labels": [], "entities": []}, {"text": "We find that inference networks achieve lower search error on instances with higher likelihood (under a pretrained language model), while for gradient descent the correlation between search error and likelihood is closer to zero.", "labels": [], "entities": []}, {"text": "This shows the impact of the use of dataset-based learning of inference networks, i.e., they are more effective at amortizing inference for more common inputs.", "labels": [], "entities": []}, {"text": "Finally, we experiment with two refinements of inference networks.", "labels": [], "entities": []}, {"text": "The first fine-tunes the inference network parameters fora single test example to minimize the energy of its output.", "labels": [], "entities": []}, {"text": "The second uses an inference network to provide a warm start for gradient descent.", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7670061886310577}]}, {"text": "Both lead to reductions in search error and higher accuracies for certain tasks, with the warm start method leading to a better speed/accuracy trade-off.", "labels": [], "entities": [{"text": "error", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.814321756362915}, {"text": "accuracies", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.978222131729126}, {"text": "speed", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9633978009223938}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.8498020768165588}]}], "datasetContent": [{"text": "We perform experiments on three tasks: Twitter part-of-speech tagging (POS), named entity recognition (NER), and CCG supersense tagging (CCG).", "labels": [], "entities": [{"text": "Twitter part-of-speech tagging (POS)", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.7311811496814092}, {"text": "named entity recognition (NER)", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.7517748574415842}, {"text": "CCG supersense tagging (CCG)", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.7276170502106348}]}, {"text": "There is a strong local dependency between neighboring labels because this is a labeled segmentation task.", "labels": [], "entities": []}, {"text": "We use the BIOES tagging scheme, so there are 17 labels.", "labels": [], "entities": [{"text": "BIOES tagging", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.6218357682228088}]}, {"text": "We use 100-dimensional pretrained GloVe () embeddings.", "labels": [], "entities": []}, {"text": "The task is evaluated with micro-averaged F1 score using the conlleval script.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9700922071933746}]}, {"text": "We use the standard splits from CCGbank ().", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9619419574737549}]}, {"text": "We only keep sentences with length less than 50 in the original training data when training the CRF.", "labels": [], "entities": []}, {"text": "The training data contains 1,284 unique labels, but because the label distribution has along tail, we use only the 400 most frequent labels, replacing the others by a special tag * . The percentages of * in train/development/test are 0.25/0.23/0.23%.", "labels": [], "entities": []}, {"text": "When the gold standard tag is * , the prediction is always evaluated as incorrect.", "labels": [], "entities": []}, {"text": "We use the same GloVe embeddings as in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.8629131317138672}]}, {"text": "Because of the compositional nature of supertags, this task has more non-local dependencies.", "labels": [], "entities": []}, {"text": "The task is evaluated with per-token accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9597638845443726}]}], "tableCaptions": [{"text": " Table 1: Test results for all tasks. Inference networks, gradient descent, and Viterbi are all optimizing the BLSTM- CRF energy. Best result per task is in bold.", "labels": [], "entities": [{"text": "BLSTM- CRF energy", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.652825340628624}]}, {"text": " Table 2: Development results for CNNs with two filter  sets (H = 100).", "labels": [], "entities": [{"text": "CNNs", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.9559020400047302}]}, {"text": " Table 3: Speed comparison of inference networks  across tasks and architectures (examples/sec).", "labels": [], "entities": []}, {"text": " Table 4: Test results with BLSTM-CRF+. For local  baseline and inference network architectures, we use  CNN for POS, seq2seq for NER, and BLSTM for CCG.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.8979660272598267}]}, {"text": " Table 6: Test set results of approximate inference methods for three tasks, showing performance metrics (accuracy  and F1) as well as average energy of the output of each method. The inference network architectures in the above  experiments are: CNN for POS, seq2seq for NER, and BLSTM for CCG. N is the number of epochs for GD  inference or instance-tailored fine-tuning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9985275268554688}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9971892237663269}, {"text": "BLSTM", "start_pos": 281, "end_pos": 286, "type": "METRIC", "confidence": 0.9799638390541077}]}]}