{"title": [{"text": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language.", "labels": [], "entities": []}, {"text": "We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration.", "labels": [], "entities": []}, {"text": "This graph is used to prune the action space, enabling more efficient exploration.", "labels": [], "entities": []}, {"text": "The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture.", "labels": [], "entities": []}, {"text": "In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives.", "labels": [], "entities": [{"text": "TextWorld framework", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9035520553588867}]}, {"text": "We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language communication can be used to affect change in the real world.", "labels": [], "entities": []}, {"text": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a steppingstone toward more real-world environments where agents must communicate to understand the state of the world and indirectly affect change in the world.", "labels": [], "entities": []}, {"text": "Text adventure games are also useful for developing and testing reinforcement learning algorithms that must deal with the partial observability of the world (.", "labels": [], "entities": []}, {"text": "In text adventure games, the agent receives an incomplete textual description of the current state of the world.", "labels": [], "entities": []}, {"text": "From this information, and previous interactions with the world, a player must determine the next best action to take to achieve some quest or goal.", "labels": [], "entities": []}, {"text": "The player must then compose a textual description of the action they intend to make and receive textual feedback of the effects of the action.", "labels": [], "entities": []}, {"text": "Formally, a text-based game is a partially observable Markov decision process (POMDP), represented as a 7-tuple of S, T, A, \u2126, O, R, \u03b3 representing the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, observation conditional probabilities, reward function, and the discount factor respectively.", "labels": [], "entities": []}, {"text": "In text-based games, the agent never has access to the true underlying world state and has to reason about how to act in the world based only on the textual observations.", "labels": [], "entities": []}, {"text": "Additionally, the agent's actions must be expressed through natural language commands, ensuring that the action space is combinatorially large.", "labels": [], "entities": []}, {"text": "Thus, text-based games pose a different set of challenges than traditional video games.", "labels": [], "entities": []}, {"text": "Text-based games require a greater understanding of previous context to be able to explore the state-action space more effectively.", "labels": [], "entities": []}, {"text": "Such games have historically proven to be difficult to play for AI agents, and the more complex variants such as Zork still remain firmly out of the reach of existing approaches.", "labels": [], "entities": []}, {"text": "We introduce three contributions to text-based game playing to deal with the combinatorially large state and action spaces.", "labels": [], "entities": []}, {"text": "First, we show that a state representation in the form of a knowledge graph gives us the ability to effectively prune an action space.", "labels": [], "entities": []}, {"text": "A knowledge graph captures the relationships between entities as a directed graph.", "labels": [], "entities": []}, {"text": "The knowledge graph provides a persistent memory of the world overtime and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.", "labels": [], "entities": []}, {"text": "Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value fora stateaction pair.", "labels": [], "entities": []}, {"text": "This architecture leverages recent advances in graph embedding and attention techniques ( to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs.", "labels": [], "entities": []}, {"text": "Finally, we take initial steps toward framing the POMDP as a questionanswering (QA) problem wherein a knowledgegraph can be used to not only prune actions but to answer the question of what action is most appropriate.", "labels": [], "entities": []}, {"text": "Previous work has shown that many NLP tasks can be framed as instances of questionanswering and that we can transfer knowledge between these tasks (.", "labels": [], "entities": []}, {"text": "We show how pre-training certain parts of our KG-DQN network using existing QA methods improves performance and allows knowledge to be transferred from different games.", "labels": [], "entities": []}, {"text": "We provide results on ablative experiments comparing our knowledge-graph based approach approaches to strong baselines.", "labels": [], "entities": []}, {"text": "Results show that incorporating a knowledge-graph into a reinforcement learning agent results in converges to the highest reward more than 40% faster than the best baseline.", "labels": [], "entities": []}, {"text": "With pre-training using a questionanswering paradigm, we achieve this fast convergence rate while also achieving high quality quest solutions as measured by the number of steps required to complete the quests.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments in the TextWorld framework (C\u00f4t\u00e9 et al., 2018) using their \"home\" theme.", "labels": [], "entities": []}, {"text": "TextWorld uses a grammar to randomly generate game worlds and quests with given parameters.", "labels": [], "entities": [{"text": "TextWorld", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9359363913536072}]}, {"text": "Games generated with TextWorld start with a zero-th observation that gives instructions for the quest; we do not allow our agent to access this information.", "labels": [], "entities": []}, {"text": "The TextWorld API also provides a list of admissible actions at each state-the actions that can be performed based on the objects that are present.", "labels": [], "entities": []}, {"text": "We do not allow our agent to access the admissible actions.", "labels": [], "entities": []}, {"text": "We generated two sets of games with different random seeds, representing different game difficulties, which we denote as small and large.", "labels": [], "entities": []}, {"text": "Small games have ten rooms and quests of length five and large games have twenty rooms and quests of length ten.", "labels": [], "entities": []}, {"text": "Statistics on the games are given in.", "labels": [], "entities": []}, {"text": "Quest length refers to the number of actions that the agent is required to perform in order to finish the quest; more actions are typically necessary to move around the environment and find the objects that need to be interacted with.", "labels": [], "entities": []}, {"text": "The branching factor is the size of the action set A for that particular game.", "labels": [], "entities": []}, {"text": "The reward function provided by TextWorld is as follows: +1 for each action taken that moves the agent closer to finishing the quest; -1 for each action taken that extends the minimum number of steps needed to finish the quest from the current stage; 0 for all other situations.", "labels": [], "entities": []}, {"text": "The maximum achievable reward for the small and large sets of games are 5 and 10 respectively.", "labels": [], "entities": []}, {"text": "This allows for  a large amount of variance in quest quality-as measured by steps to complete the quest-that receives maximum reward.", "labels": [], "entities": []}, {"text": "The following procedure for pre-training was done separately for each set of games.", "labels": [], "entities": []}, {"text": "Pre-training of the SB-LSTM within the question-answering architecture is conducted by generating 200 games from the same TextWorld theme.", "labels": [], "entities": []}, {"text": "The QA system was then trained on data from walkthroughs of a randomly-chosen subset of 160 of these generated games, tuned on a dev set of 20 games, and evaluated on the held-out set of 20 games.", "labels": [], "entities": []}, {"text": "provides details on the Exact Match (EM), precision, recall, and F1 scores of the QA system after training for the small and large sets of games.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9581677079200744}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.999237060546875}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9989610910415649}, {"text": "F1 scores", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9792098701000214}]}, {"text": "Precision, recall, and F1 scores are calculated by counting the number of tokens between the predicted answer and ground truth.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9947256445884705}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9961303472518921}, {"text": "F1 scores", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9800125658512115}]}, {"text": "An Exact Match is when the entire predicted answer matches with the ground truth.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 3, "end_pos": 14, "type": "METRIC", "confidence": 0.8843065500259399}]}, {"text": "This score is used to tune the model based on the dev set of games.", "labels": [], "entities": []}, {"text": "A random game was chosen from the test-set of games and used as the environment for the agent to train its deep Q-network on.", "labels": [], "entities": []}, {"text": "Thus, at no time did the QA system seethe final testing game prior to the training of the KG-DQN network.", "labels": [], "entities": [{"text": "KG-DQN network", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9146799147129059}]}, {"text": "We compare our technique to three baselines: \u2022 Random command, which samples from the list of admissible actions returned by the TextWorld simulator at each step.", "labels": [], "entities": []}, {"text": "\u2022 LSTM-DQN, developed by.", "labels": [], "entities": []}, {"text": "\u2022 Bag-of-Words DQN, which uses a bag-ofwords encoding with a multi-layer feed forward network instead of an LSTM.", "labels": [], "entities": [{"text": "Bag-of-Words DQN", "start_pos": 2, "end_pos": 18, "type": "DATASET", "confidence": 0.5321690291166306}]}, {"text": "To achieve the most competitive baselines, we used a randomized grid search to choose the best hyperparameters (e.g., hidden state size, \u03b3, \u03c1, final , update frequency, learning rate, replay buffer size) for the BOW-DQN and LSTM-DQN baselines.", "labels": [], "entities": [{"text": "replay buffer size", "start_pos": 184, "end_pos": 202, "type": "METRIC", "confidence": 0.8224966327349345}, {"text": "BOW-DQN", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.6936030983924866}, {"text": "LSTM-DQN baselines", "start_pos": 224, "end_pos": 242, "type": "DATASET", "confidence": 0.7516220510005951}]}, {"text": "We tested three versions of our KG-DQN: 1.", "labels": [], "entities": []}, {"text": "Un-pruned actions with pre-training Algorithm 1 1 , 2 -greedy learning algorithm for KG-DQN 1: for episode=1 to M do 2: Initialize action dictionary A and graph G0", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generated game details.", "labels": [], "entities": []}]}