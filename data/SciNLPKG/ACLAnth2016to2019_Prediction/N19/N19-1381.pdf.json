{"title": [{"text": "Evaluating Coherence in Dialogue Systems using Entailment", "labels": [], "entities": []}], "abstractContent": [{"text": "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers.", "labels": [], "entities": []}, {"text": "Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9981508851051331}]}, {"text": "Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable.", "labels": [], "entities": []}, {"text": "Moreover, judges tend to evaluate a small number of dialogues , meaning that minor differences in evaluation configuration may lead to dissimilar results.", "labels": [], "entities": []}, {"text": "In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques.", "labels": [], "entities": []}, {"text": "Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, we have witnessed a big success in the capability of computers to seemingly understand natural language text and to generate plausible responses to conversations.", "labels": [], "entities": []}, {"text": "A challenging task of building dialogue systems lies in evaluating the quality of their responses.", "labels": [], "entities": []}, {"text": "Typically, evaluating goal-oriented dialogue systems is done via human-generated judgment like a task completion test or user satisfaction score ().", "labels": [], "entities": []}, {"text": "However, the task of evaluating open-ended dialogue systems is not well defined as there is no * Equal Contribution clear explicit goal for conversations.", "labels": [], "entities": []}, {"text": "Indeed, dialog systems are ultimately created to satisfy the user's need which can be associated with how entertaining and engaging the conversation was.", "labels": [], "entities": []}, {"text": "It is unclear how to define a metric that can account comprehensibly for the semantic meaning of the responses.", "labels": [], "entities": []}, {"text": "Moreover, grasping the underlying meaning of text has always been fraught with difficulties, which are essentially attributed to the complexities and ambiguities in natural language.", "labels": [], "entities": [{"text": "grasping the underlying meaning of text", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.8042066891988119}]}, {"text": "Generally, a good dialogue can be described as an exchange of information that sustain coherence through a train of thoughts and a flow of topics.", "labels": [], "entities": []}, {"text": "Therefore, a plausible way to evaluate open-ended dialogue systems is to measure the consistency of the responses.", "labels": [], "entities": [{"text": "consistency", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.983064591884613}]}, {"text": "For example, a neural dialogue system can respond to the utterance \"Do you like animals?\" by \"Yes, I have three cats\", thereafter replies to \"How many cats do you have\" by \"I don't have cats.\".", "labels": [], "entities": []}, {"text": "Here, we can notice that the dialogue system failed to provide a coherent answer and instead generated an inconsistent response.", "labels": [], "entities": []}, {"text": "In this work, we characterize the consistency of dialogue systems as a natural language inference (NLI) () problem.", "labels": [], "entities": []}, {"text": "In particular, NLI is focused on recognizing whether a hypothesis is inferred from a premise.", "labels": [], "entities": []}, {"text": "In dialogue systems, we cast a generated response as the hypothesis and the conversation history as the premise, projecting thus the automatic evaluation into an NLI task.", "labels": [], "entities": []}, {"text": "In other words, we propose directly calculable approximations of human evaluation grounded on conversational coherence and affordance by using state-of-the-art entailment techniques.", "labels": [], "entities": []}, {"text": "For this purpose, we build a synthesized inference data from conversational corpora.", "labels": [], "entities": []}, {"text": "The intuition behind this choice is motivated by the fact that utterances in a human conversation tend to follow a consistent and coherent flow where each utterance can be inferred from the previous interactions.", "labels": [], "entities": []}, {"text": "We train the state-of-the-art infer-ence models on our conversational inference data and then the learned models are used to evaluate the coherence in a given conversation.", "labels": [], "entities": []}, {"text": "Finally, we fare our proposed evaluation method against existing automated metrics.", "labels": [], "entities": []}, {"text": "The results highlight the capability of inference models to automatically evaluate dialogue coherence.", "labels": [], "entities": []}, {"text": "The source code and the dataset are available at https://github.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we focus on the task of evaluating the next utterance given the conversation history.", "labels": [], "entities": []}, {"text": "We used the following models to generate responses.", "labels": [], "entities": []}, {"text": "These models were trained on the conversational datasets, using optimization, until convergence: \u2022 Seq2Seq with attention mechanism (Bahdanau et al., 2015): predicts the next response given the previous utterance using an encoder-decoder model.", "labels": [], "entities": []}, {"text": "\u2022 HRED ( : extends the Seq2Seq model by adding a context-RNN layer that accounts for contextual information.", "labels": [], "entities": [{"text": "HRED", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.7751507759094238}]}, {"text": "\u2022 TA-Seq2Seq (: extends the Seq2Seq model by biasing the overall distribution towards leveraging topic words in the response.", "labels": [], "entities": [{"text": "TA-Seq2Seq", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.7542926073074341}]}, {"text": "\u2022   and 150 dialogues from OpenSubtitles.", "labels": [], "entities": []}, {"text": "All subjects have informed consent as required from the Ethics Review Board at the University of Alberta.", "labels": [], "entities": []}, {"text": "Due to lack of space, we will omit an exhaustive description of the human evaluation process and refer readers to () as we conducted the same evaluation procedure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of inference models on InferCon- vAI.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9885901808738708}, {"text": "InferCon- vAI", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.6360913515090942}]}]}