{"title": [{"text": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing", "labels": [], "entities": [{"text": "Cross-Lingual Transfer with Order Differences", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.6669293761253356}, {"text": "Dependency Parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7595334053039551}]}], "abstractContent": [{"text": "Different languages might have different word orders.", "labels": [], "entities": []}, {"text": "In this paper, we investigate cross-lingual transfer and posit that an order-agnostic model will perform better when transferring to distant foreign languages.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7800686359405518}]}, {"text": "To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages.", "labels": [], "entities": []}, {"text": "Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures.", "labels": [], "entities": []}, {"text": "The former relies on sequential information while the latter is more flexible at modeling word order.", "labels": [], "entities": []}, {"text": "Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual transfer, which transfers models across languages, has tremendous practical value.", "labels": [], "entities": [{"text": "Cross-lingual transfer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7816739082336426}]}, {"text": "It reduces the requirement of annotated data fora target language and is especially useful when the target language is lack of resources.", "labels": [], "entities": []}, {"text": "Recently, this technique has been applied to many NLP tasks such as text categorization (), tagging (, dependency parsing ( and machine translation (.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7845873832702637}, {"text": "tagging", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9769605398178101}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8291987478733063}, {"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7897379100322723}]}, {"text": "Despite the preliminary success, transferring across languages is challenging as it requires understanding and handling differences between languages at levels of morphology, syntax, and semantics.", "labels": [], "entities": [{"text": "transferring across languages", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8606582085291544}]}, {"text": "It is especially difficult to learn invariant features that can robustly transfer to distant languages.", "labels": [], "entities": []}, {"text": "Prior work on cross-lingual transfer mainly focused on sharing word-level information by leveraging multi-lingual word embeddings.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.8319720029830933}]}, {"text": "However, words are not independent in sentences; their combinations form larger linguistic units, known as context.", "labels": [], "entities": []}, {"text": "Encoding context information is vital for many NLP tasks, and a variety of approaches (e.g., convolutional neural networks and recurrent neural networks) have been proposed to encode context as a high-level feature for downstream tasks.", "labels": [], "entities": []}, {"text": "In this paper, we study how to transfer generic contextual information across languages.", "labels": [], "entities": []}, {"text": "For cross-language transfer, one of the key challenges is the variation in word order among different languages.", "labels": [], "entities": [{"text": "cross-language transfer", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8698728680610657}]}, {"text": "For example, the Verb-Object pattern in English can hardly be found in Japanese.", "labels": [], "entities": []}, {"text": "This challenge should betaken into consideration in model design.", "labels": [], "entities": [{"text": "model design", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7818991839885712}]}, {"text": "RNN is a prevalent family of models for many NLP tasks and has demonstrated compelling performances (.", "labels": [], "entities": []}, {"text": "However, its sequential nature makes it heavily reliant on word order information, which exposes to the risk of encoding language-specific order information that cannot generalize across languages.", "labels": [], "entities": []}, {"text": "We characterize this as the \"order-sensitive\" property.", "labels": [], "entities": []}, {"text": "Another family of models known as \"Transformer\" uses self-attention mechanisms to capture context and was shown to be effective in various NLP tasks.", "labels": [], "entities": []}, {"text": "With modification in position representations, the self-attention mechanism can be more robust than RNNs to the change of word order.", "labels": [], "entities": []}, {"text": "We refer to this as the \"order-free\" property.", "labels": [], "entities": []}, {"text": "In this work, we posit that order-free models have better transferability than order-sensitive models because they less suffer from overfitting  language-specific word order features.", "labels": [], "entities": []}, {"text": "To test our hypothesis, we first quantify language distance in terms of word order typology, and then systematically study the transferability of ordersensitive and order-free neural architectures on cross-lingual dependency parsing.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 200, "end_pos": 232, "type": "TASK", "confidence": 0.6332387228806814}]}, {"text": "We use dependency parsing as a test bed primarily because of the availability of unified annotations across abroad spectrum of languages ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8323585093021393}]}, {"text": "Besides, word order typology is found to influence dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8751706182956696}]}, {"text": "Moreover, parsing is a low-level NLP task () that can benefit many downstream applications.", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.986869752407074}]}, {"text": "We conduct evaluations on 31 languages across abroad spectrum of language families, as shown in.", "labels": [], "entities": []}, {"text": "Our empirical results show that orderfree encoding and decoding models generally perform better than the order-sensitive ones for crosslingual transfer, especially when the source and target languages are distant.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7780589461326599}]}], "datasetContent": [{"text": "In this section, we compare four architectures for cross-lingual transfer dependency parsing with a different combination of order-free and ordersensitive encoder and decoder.", "labels": [], "entities": [{"text": "cross-lingual transfer dependency parsing", "start_pos": 51, "end_pos": 92, "type": "TASK", "confidence": 0.6672906205058098}]}, {"text": "We conduct several detailed analyses showing the pros and cons of both types of models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results (UAS%/LAS%, excluding punctuation) on the test sets. Languages are sorted by the word- ordering distance to English, as shown in the second column. '*' refers to results of delexicalized models, ' \u2020'  means that the best transfer model is statistically significantly better (by paired bootstrap test, p < 0.05) than all  other transfer models. Models are marked with their encoder and decoder order sensitivity, OF denotes order-free  and OS denotes order-sensitive.", "labels": [], "entities": [{"text": "UAS%/LAS%", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.7795385420322418}]}, {"text": " Table 3: Comparisons of different encoders (averaged  results over all languages on the original training sets).", "labels": [], "entities": []}, {"text": " Table 4: Relative frequencies (%) of dependency dis- tances. English differs from the Average at d=1.", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9812914133071899}]}]}