{"title": [{"text": "Evaluating Rewards for Question Generation Models", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.816183477640152}]}], "abstractContent": [{"text": "Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.870065838098526}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.743638813495636}]}, {"text": "Models are trained using teacher forcing to optimise only the one-step-ahead prediction.", "labels": [], "entities": []}, {"text": "However, attest time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (ex-posure bias).", "labels": [], "entities": []}, {"text": "A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch.", "labels": [], "entities": []}, {"text": "We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples.", "labels": [], "entities": []}, {"text": "We confirm that training with policy gradient methods leads to increases in the metrics used as rewards.", "labels": [], "entities": []}, {"text": "We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.", "labels": [], "entities": []}], "introductionContent": [{"text": "Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions (, become more robust to queries (, and to act as automatic tutors.", "labels": [], "entities": []}, {"text": "Recent approaches to question generation have used) models with attention () and a form of copy mechanism).", "labels": [], "entities": [{"text": "question generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7666129469871521}]}, {"text": "Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document ().", "labels": [], "entities": []}, {"text": "There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD (.", "labels": [], "entities": [{"text": "question generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7094158083200455}]}, {"text": "Only a single question is available for each context-answer pair, and models are trained using teacher forcing (.", "labels": [], "entities": []}, {"text": "This lack of diverse training data combined with the one-stepahead training procedure exacerbates the problem of exposure bias (.", "labels": [], "entities": []}, {"text": "The model does not learn how to distribute probability mass over sequences that are valid but different to the ground truth; during inference, the model must predict the whole sequence, and may not be robust to mistakes during decoding.", "labels": [], "entities": []}, {"text": "Recent work has investigated training the models directly on a performance based objective, either by optimising for BLEU score () or other quality metrics (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9861369132995605}]}, {"text": "By decoupling the training procedure from the ground truth data, the model is able to explore the space of possible questions and learn to recover from suboptimal predictions during decoding.", "labels": [], "entities": []}, {"text": "While the metrics used seem to be intuitively good choices, there is an assumption that they are good proxies for question quality which has not yet been confirmed.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "We perform fine tuning using a range of rewards, including a novel adversarial objective that directly estimates the probability that a question was generated or came from the ground truth data.", "labels": [], "entities": [{"text": "fine tuning", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.7669588923454285}]}, {"text": "We show that although fine tuning leads to increases in reward scores, the resulting models perform worse when evaluated by human workers.", "labels": [], "entities": []}, {"text": "We also demonstrate that the generated questions exploit weaknesses in the reward models.", "labels": [], "entities": []}, {"text": "Context although united methodist practices and interpretation of beliefs have evolved overtime , these practices and beliefs can be traced to the writings of the church 's founders , especially john wesley and charles wesley ( anglicans ) , but also philip william otterbein and martin boehm ( united brethren ) , and jacob albright ( evangelical association ) .: Example generated questions for various fine-tuning objectives.", "labels": [], "entities": []}, {"text": "The answer is highlighted in bold.", "labels": [], "entities": []}, {"text": "The model trained on a QA reward has learned to simply point at the answer and exploit the QA model, while the model trained on a language model objective has learned to repeat common phrase templates.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task is to generate a natural language question, conditioned on a document and the location of an answer within that document.", "labels": [], "entities": []}, {"text": "For example, given the input document \"this paper investigates rewards for question generation\" and answer \"question generation\", the model should produce a question such as \"what is investigated in the paper?\"", "labels": [], "entities": [{"text": "question generation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7622672021389008}, {"text": "question generation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.719778299331665}]}, {"text": "We report the negative log-likelihood (NLL) of the test set under the different models, as well as the corpus level BLEU-4 score () of the generated questions compared to the ground truth.", "labels": [], "entities": [{"text": "negative log-likelihood (NLL)", "start_pos": 14, "end_pos": 43, "type": "METRIC", "confidence": 0.7786815285682678}, {"text": "BLEU-4 score", "start_pos": 116, "end_pos": 128, "type": "METRIC", "confidence": 0.9323928654193878}]}, {"text": "We also report the rewards achieved on the  test set, as the QA, LM and discriminator scores.", "labels": [], "entities": [{"text": "QA", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9888421297073364}, {"text": "LM", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.8525847792625427}, {"text": "discriminator", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.8381856679916382}]}, {"text": "For the human evaluation, we follow the standard approach in evaluating machine translation systems (), as used for question generation by.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.6813846677541733}, {"text": "question generation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7667562067508698}]}, {"text": "We ask three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer.", "labels": [], "entities": []}, {"text": "shows the changes in automatic metrics for models fine tuned on various combinations of rewards, compared to the model without tuning.", "labels": [], "entities": []}, {"text": "In all cases, the BLEU score reduces, as the training objective is no longer closely coupled to the training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9861846566200256}]}, {"text": "In general, models achieve better scores on the metrics on which they were fine tuned.", "labels": [], "entities": []}, {"text": "Jointly training on a QA and LM reward results in better LM scores than training on only a LM reward; the LM score did not increase smoothly when used as the sole objective, and we believe the additional QA reward acts as a form of regularisation.", "labels": [], "entities": []}, {"text": "We conclude that fine tuning using policy gradients can be used to attain higher rewards, as expected.", "labels": [], "entities": [{"text": "fine tuning", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.720868319272995}]}, {"text": "shows the human evaluation scores fora subset of the fine tuned models.", "labels": [], "entities": []}, {"text": "The model fine tuned on a QA and LM objective is rated as significantly worse by human annotators, despite achieving higher scores in the automatic metrics.", "labels": [], "entities": []}, {"text": "In other words, the training objective given by these reward sources does not correspond to true question quality, despite them being intuitively good choices.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Changes in automatic evaluation metrics after models were fine tuned on various objectives. QA refers  to the F1 score obtained by a question answering system on the generated questions. LM refers to the perplexity  of generated questions under a separate language model. The discriminator reward refers to the percentage of  generated sequences that fooled the discriminator. Lower LM and NLL scores are better. BLEU scores decreased  in all cases.", "labels": [], "entities": [{"text": "QA", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.938562273979187}, {"text": "F1 score obtained", "start_pos": 120, "end_pos": 137, "type": "METRIC", "confidence": 0.9725726644198099}, {"text": "BLEU", "start_pos": 423, "end_pos": 427, "type": "METRIC", "confidence": 0.9994667172431946}]}, {"text": " Table 3: Summary of human evaluation of selected models", "labels": [], "entities": []}]}