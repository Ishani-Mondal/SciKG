{"title": [{"text": "SEQ 3 : Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora.", "labels": [], "entities": []}, {"text": "We present a sequence-to-sequence-to-sequence autoencoder (SEQ 3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables.", "labels": [], "entities": []}, {"text": "We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.653973658879598}]}, {"text": "Constraining the length of the latent word sequences forces the model to distill important information from the input.", "labels": [], "entities": []}, {"text": "A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable.", "labels": [], "entities": []}, {"text": "Continuous re-laxations enable us to sample from categorical distributions, allowing gradient-based optimization , unlike alternatives that rely on reinforcement learning.", "labels": [], "entities": []}, {"text": "The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.7589451372623444}]}], "introductionContent": [{"text": "Neural sequence-to-sequence models (SEQ2SEQ) perform impressively well in several natural language processing tasks, such as machine translation ( or syntactic constituency parsing ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8494654297828674}, {"text": "syntactic constituency parsing", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.6651520331700643}]}, {"text": "However, they require massive parallel training datasets (.", "labels": [], "entities": []}, {"text": "Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ2SEQ models, mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro-  posed.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.6797241369883219}]}, {"text": "Unsupervised (or semi-supervised) SEQ2SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.9276061952114105}, {"text": "sentence compression", "start_pos": 167, "end_pos": 187, "type": "TASK", "confidence": 0.731022372841835}]}, {"text": "Current models, however, barely reach lead-N baselines, and/or are non-differentiable (, thus relying on reinforcement learning, which is unstable and inefficient.", "labels": [], "entities": []}, {"text": "By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ 3 , that can be trained end-to-end via gradient-based optimization.", "labels": [], "entities": []}, {"text": "SEQ 3 employs differentiable approximations for sampling from categorical distributions, which have been shown to outperform reinforcement learning.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.822696328163147}]}, {"text": "Therefore it is a generic framework which can be easily extended to other tasks, e.g., machine translation and semantic parsing via task-specific losses.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8238619863986969}, {"text": "semantic parsing", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.7172861397266388}]}, {"text": "In this work, as a first step, we apply SEQ to unsupervised abstractive sentence compression.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9018955230712891}, {"text": "abstractive sentence compression", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.6131284038225809}]}, {"text": "SEQ 3 ( \u00a72) comprises two attentional encoderdecoder () pairs (): a compressor C and a reconstructor R.", "labels": [], "entities": []}, {"text": "C ( \u00a72.1) receives an input text x = \u27e8x 1 , . .", "labels": [], "entities": []}, {"text": ", x N \u27e9 of N words, and generates a summary y = \u27e8y 1 , . .", "labels": [], "entities": []}, {"text": ", y M \u27e9 of M words (M<N), y being a latent variable.", "labels": [], "entities": []}, {"text": "Rand C communicate only through the discrete words of the summary y ( \u00a72.2).", "labels": [], "entities": []}, {"text": "R ( \u00a72.3) produces a sequenc\u00ea: More detailed illustration of SEQ 3 . The compressor (C) produces a summary from the input text, and the reconstructor (R) tries to reproduce the input from the summary.", "labels": [], "entities": []}, {"text": "Rand C comprise an attentional encoder-decoder each, and communicate only through the (discrete) words of the summary.", "labels": [], "entities": []}, {"text": "The LM prior incentivizes C to produce human-readable summaries, while topic loss rewards summaries with similar topicindicating words as the input text.", "labels": [], "entities": []}, {"text": "ing to minimize a reconstruction loss L R = (x, \u02c6 x) ( \u00a72.5).", "labels": [], "entities": [{"text": "reconstruction loss L R", "start_pos": 18, "end_pos": 41, "type": "METRIC", "confidence": 0.7284174114465714}]}, {"text": "A pretrained language model acts as a prior on y, introducing an additional loss LP (x, y) that encourages SEQ 3 to produce human-readable summaries.", "labels": [], "entities": []}, {"text": "A third loss L T (x, y) rewards summaries y with similar topic-indicating words as x.", "labels": [], "entities": []}, {"text": "Experiments ( \u00a73) on the Gigaword sentence compression dataset () and the shared tasks) produce promising results.", "labels": [], "entities": [{"text": "Gigaword sentence compression dataset", "start_pos": 25, "end_pos": 62, "type": "DATASET", "confidence": 0.8229476809501648}]}, {"text": "Our contributions are: (1) a fully differentiable sequence-to-sequence-to-sequence (SEQ 3 ) autoencoder that can be trained without parallel data via gradient optimization; (2) an application of SEQ 3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 217, "end_pos": 249, "type": "TASK", "confidence": 0.6558807094891866}, {"text": "abstractive sentence compression", "start_pos": 346, "end_pos": 378, "type": "TASK", "confidence": 0.711154560248057}]}, {"text": "This work is a step towards exploring the potential of SEQ 3 in other tasks, such as machine translation.", "labels": [], "entities": [{"text": "SEQ 3", "start_pos": 55, "end_pos": 60, "type": "TASK", "confidence": 0.8776737153530121}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8351256251335144}]}], "datasetContent": [{"text": "Datasets sentences (sources) of the training pairs from Gigaword to train SEQ 3 ; our model is never exposed to target headlines (summaries) during training or evaluation, i.e., it is completely unsupervised.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9152907133102417}]}, {"text": "Our code is publicly available.", "labels": [], "entities": []}, {"text": "We compare SEQ 3 to other unsupervised sentence compression models.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7456342279911041}]}, {"text": "We note that the extractive model of relies on a pre-trained attention model using at least 500K parallel sentences, which is crucial to mitigate the inefficiency of sampling-based variational inference and REINFORCE.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9179122447967529}]}, {"text": "Therefore it is not comparable, as it is semi-supervised.", "labels": [], "entities": []}, {"text": "The results of the extractive model of Fevry and Phang (2018) are also not comparable, as they were obtained on a different, not publicly available test set.", "labels": [], "entities": []}, {"text": "We note, however, that they report that their system performs worse than the LEAD-8 baseline in ROUGE-2 and ROUGE-L on Gigaword.", "labels": [], "entities": [{"text": "LEAD-8", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9662083387374878}, {"text": "ROUGE-2", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.7539561986923218}, {"text": "Gigaword", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.9464613199234009}]}, {"text": "The only directly comparable unsupervised model is the abstractive 'Pretrained Generator' of.", "labels": [], "entities": []}, {"text": "The version of 'Adversarial REINFORCE' that consider unsupervised is actually weakly supervised, since its discriminator was exposed to the summaries of the same sources the rest of the model was trained on.", "labels": [], "entities": []}, {"text": "As baselines, we use LEAD-8 for Gigaword, which simply selects the first 8 words of the source, and PREFIX for DUC, which includes the first 75 bytes of the source article.", "labels": [], "entities": [{"text": "LEAD-8", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9874905943870544}, {"text": "PREFIX", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.8704475164413452}]}, {"text": "We also compare to supervised abstractive sentence compression methods.", "labels": [], "entities": [{"text": "supervised abstractive sentence compression", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.694885678589344}]}, {"text": "Following previous work, we report the average F1 of ROUGE-1, ROUGE-2, ROUGE-L).", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9994657635688782}, {"text": "ROUGE-1", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9914662837982178}, {"text": "ROUGE-2", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9591968655586243}, {"text": "ROUGE-L", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9875441789627075}]}, {"text": "We implemented SEQ 3 with LSTMs (see Appendix) and during inference we perform greedy-sampling.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8826656341552734}]}, {"text": "This makes sense, since the pretrained LM rewards correct word order.", "labels": [], "entities": []}, {"text": "We also tried removing the topic loss, but the model failed to converge and results were extremely poor.", "labels": [], "entities": []}, {"text": "Topic loss acts as a bootstrap mechanism, biasing the compressor to generate words that maintain the topic of the input text.", "labels": [], "entities": []}, {"text": "This greatly reduces variance due to sampling in early stages of training, alleviating the need to pretrain individual), although it was trained on Gigaword.", "labels": [], "entities": [{"text": "variance", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9903208613395691}, {"text": "Gigaword", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.9724538922309875}]}, {"text": "In DUC-2003, however, it does not surpass the PREFIX baseline.", "labels": [], "entities": [{"text": "DUC-2003", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.9553627967834473}, {"text": "PREFIX baseline", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.8361706137657166}]}, {"text": "Finally, illustrates three randomly sampled outputs of SEQ 3 on Gigaword.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9325064420700073}]}, {"text": "In the first one, SEQ 3 copies several words esp. from the beginning of the input (hence the high ROUGE-L) exhibiting extractive capabilities, though still being adequately abstractive (bold words denote paraphrases).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9972867965698242}]}, {"text": "In the second one, SEQ 3 showcases its true abstractive power by paraphrasing and compressing multi-word expressions to single content words more heavily, still without losing the overall meaning.", "labels": [], "entities": [{"text": "SEQ 3", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.8575611710548401}]}, {"text": "In the last example, SEQ 3 progressively becomes ungrammatical though interestingly retaining some content words from the input.", "labels": [], "entities": [{"text": "SEQ 3", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.7690895199775696}]}], "tableCaptions": [{"text": " Table 1: Average results on the (English) Gigaword dataset for abstractive sentence compression methods.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8184277713298798}, {"text": "abstractive sentence compression", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.6078021029631296}]}, {"text": " Table 2: Averaged results on the DUC-2003 dataset; the  top part reports results of supervised systems.", "labels": [], "entities": [{"text": "DUC-2003 dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9902380108833313}]}, {"text": " Table 3: Averaged results on the DUC-2004 dataset; the  top part reports results of supervised systems.", "labels": [], "entities": [{"text": "DUC-2004 dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.989787757396698}]}]}