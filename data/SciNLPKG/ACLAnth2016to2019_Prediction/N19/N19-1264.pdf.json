{"title": [{"text": "Guiding Extractive Summarization with Question-Answering Rewards", "labels": [], "entities": [{"text": "Guiding Extractive Summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8758676846822103}]}], "abstractContent": [{"text": "Highlighting while reading is a natural behavior for people to track salient content of a document.", "labels": [], "entities": []}, {"text": "It would be desirable to teach an ex-tractive summarizer to do the same.", "labels": [], "entities": []}, {"text": "However, a major obstacle to the development of a supervised summarizer is the lack of ground-truth.", "labels": [], "entities": []}, {"text": "Manual annotation of extraction units is cost-prohibitive, whereas acquiring labels by automatically aligning human abstracts and source documents can yield inferior results.", "labels": [], "entities": []}, {"text": "In this paper we describe a novel framework to guide a supervised, extractive summarization system with question-answering rewards.", "labels": [], "entities": []}, {"text": "We argue that quality summaries should serve as a document surrogate to answer important questions, and such question-answer pairs can be conveniently obtained from human abstracts.", "labels": [], "entities": []}, {"text": "The system learns to promote summaries that are informative, fluent, and perform competitively on question-answering.", "labels": [], "entities": []}, {"text": "Our results compare favorably with those reported by strong sum-marization baselines as evaluated by automatic metrics and human assessors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our increasingly digitized lifestyle calls for summarization techniques to produce short and accurate summaries that can be accessed at anytime.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9897527694702148}]}, {"text": "These summaries should factually adhere to the content of the source text and present the reader with the key points therein.", "labels": [], "entities": []}, {"text": "Although neural abstractive summarization has shown promising results (, these methods can have potential drawbacks.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.7421845098336538}]}, {"text": "It was revealed that abstracts generated by neural systems sometimes alter or falsify objective details, and introduce new meanings not present in the original text.", "labels": [], "entities": []}, {"text": "Reading these abstracts can lead to misinterpretation of the source materials, which is clearly undesirable.", "labels": [], "entities": []}, {"text": "In this work, we focus on extractive summarization, where the summaries are guaranteed (CNN) A judge this week sentenced a former TSA agent to six months in jail for secretly videotaping a female co-worker while she was in the bathroom, prosecutors said.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7710511386394501}]}, {"text": "During the investigation, detectives with the Metro Nashville Police Department in Tennessee also found that the agent, 33-year-old Daniel Boykin, entered the woman's home multiple times, where he took videos, photos and other data.", "labels": [], "entities": []}, {"text": "Police found more than 90 videos and 1,500 photos of the victim on Boykin's phone and computer . The victim filed a complaint after seeing images of herself on his phone last year.", "labels": [], "entities": []}, {"text": "Comprehension Questions (Human Abstract):", "labels": [], "entities": []}], "datasetContent": [{"text": "We proceed by discussing the dataset and settings, comparison systems, and experimental results obtained through both automatic metrics and human evaluation in a reading comprehension setting.", "labels": [], "entities": []}, {"text": "Our goal is to build an extractive summarizer identifying important textual segments from source articles.", "labels": [], "entities": [{"text": "summarizer identifying important textual segments from source articles", "start_pos": 35, "end_pos": 105, "type": "TASK", "confidence": 0.6840165480971336}]}, {"text": "To investigate the effectiveness of the proposed approach, we conduct experiments on the CNN/Daily Mail dataset using aversion provided by.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.9323663711547852}]}, {"text": "The reference summaries of this dataset were created by human editors exhibiting a moderate degree of extractiveness.", "labels": [], "entities": []}, {"text": "E.g., 83% of summary unigrams and 45% of bigrams appear in source articles ( is thus a 512-dimensional vector using either CNN or LSTM encoder.", "labels": [], "entities": []}, {"text": "We set the hidden state dimension of st to be 128.", "labels": [], "entities": []}, {"text": "We also use 100-dimensional word embeddings () and sinusoidal positional encodings () of 30 dimensions.", "labels": [], "entities": []}, {"text": "The maximum article length is set to 400 words.", "labels": [], "entities": []}, {"text": "Compared to the study of Arumae and Liu (2018), we expand the search space dramatically from 100 to 400 words, which poses a challenge to the RLbased summarizers.", "labels": [], "entities": []}, {"text": "We associate each article with at most 10 QA pairs (K=10) and use them to guide the extraction of summary segments.", "labels": [], "entities": []}, {"text": "We apply mini-batch training with Adam optimizer, where a mini-batch contains 128   articles and their QA pairs.", "labels": [], "entities": []}, {"text": "The summary ratio \u03b4 is set to 0.15, yielding extractive summaries of about 60 words.", "labels": [], "entities": [{"text": "summary ratio \u03b4", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8996513684590658}]}, {"text": "Following Arumae and Liu (2018), we set hyperparameters \u03b2 = 2\u03b1; \u03b1 and \u03b3 are tuned on the dev set using grid search.", "labels": [], "entities": []}, {"text": "Comparison systems We compare our method with a number of extractive and abstractive systems that have reported results on the CNN/DM datasets.", "labels": [], "entities": [{"text": "CNN/DM datasets", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.9019483625888824}]}, {"text": "We consider non-neural approaches that extract sentences from the source article to form a summary.", "labels": [], "entities": []}, {"text": "Such methods treat sentences as bags of words, and then select sentences containing topically important words.", "labels": [], "entities": []}, {"text": "We further include the Lead-3 baseline that extracts the first 3 sentences from any given article.", "labels": [], "entities": []}, {"text": "The method has been shown to be a strong baseline for summarizing news articles.", "labels": [], "entities": [{"text": "summarizing news articles", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.9423291087150574}]}, {"text": "Neural extractive approaches focus on learning vector representations for sentences and words, then performing extraction based on the learned representations.", "labels": [], "entities": []}, {"text": "describe a neural network method composed of a hierarchical document encoder and an attention-based extractor.", "labels": [], "entities": []}, {"text": "The system has two variants: NN-WE extracts words from the source article and NN-SE extracts sentences.", "labels": [], "entities": []}, {"text": "SummaRuNNer () presents an autoregressive sequence labeling method based on recurrent neural networks.", "labels": [], "entities": []}, {"text": "It selects summary sentences based on their content, salience, position, and novelty representations.", "labels": [], "entities": []}, {"text": "Abstractive summarization methods are not directly comparable to our approach, but we choose to include three systems that report results respectively for CNN and DM datasets.", "labels": [], "entities": [{"text": "DM datasets", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.7540479004383087}]}, {"text": "Distraction-M3 () trains the summarization system to distract its attention to traverse different regions of the source article.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9678992033004761}]}, {"text": "Graph attention () introduces a graph-based attention mechanism to enhance the encoderdecoder framework.", "labels": [], "entities": []}, {"text": "allows the system to not only copy words from the source text but also generate summary words by selecting them from a vocabulary.", "labels": [], "entities": []}, {"text": "Abstractive methods can thus introduce new words to the summary that are not present in the source article.", "labels": [], "entities": []}, {"text": "However, system summaries may change the meaning of the original texts due to this flexibility.", "labels": [], "entities": []}, {"text": "Results We present summarization results of various systems in, evaluated on the standard CNN/DM test sets by R-1, R-2, and R-L metrics), which respectively measure the overlap of unigrams, bigrams, and longest common subsequences between system and reference summaries.", "labels": [], "entities": [{"text": "CNN/DM test sets", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.8598398089408874}]}, {"text": "We investigate four variants of our method: QASumm+NoQ does not utilize any question-answer pairs during training.", "labels": [], "entities": []}, {"text": "It extracts summary text chunks by learning from groundtruth labels ( \u00a73.2) and the chunks are encoded by f Bi-LSTM  Testing the usefulness of an extractive system driven by reading comprehension is not inherently measured by automatic metrics (i.e. ROUGE).", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9533262252807617}, {"text": "ROUGE", "start_pos": 250, "end_pos": 255, "type": "METRIC", "confidence": 0.9444371461868286}]}, {"text": "We conducted a human evaluation to assess whether the highlighted summaries contribute to document understanding.", "labels": [], "entities": [{"text": "document understanding", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6759093701839447}]}, {"text": "Similar to our training paradigm we presented each participant with the document and three fill-in-the-blank questions created from the human abstracts.", "labels": [], "entities": []}, {"text": "It was guaranteed that each question was from a unique human abstract to avoid seeing the answer adjacent to the same template.", "labels": [], "entities": []}, {"text": "The missing section was randomly generated to be either the root word, the subject or ob-.", "labels": [], "entities": []}, {"text": "Our systems tested were the supervised extractor, and our full model (NER).", "labels": [], "entities": []}, {"text": "ject of the sentence, or a named entity.", "labels": [], "entities": []}, {"text": "We compare our reinforced extracted summary (presented as a bold overlay to the document), against our supervised method (section 3.2), abstractive summaries generated by, and the human abstracts in full.", "labels": [], "entities": []}, {"text": "Additionally we asked the participants to rate the quality of the summary presented (1-5, with 5 being most informative).", "labels": [], "entities": []}, {"text": "We utilized Amazon Mechanical Turk, and conducted an experiment where we sampled 80 documents from the CNN test set.", "labels": [], "entities": [{"text": "CNN test set", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.9753264387448629}]}, {"text": "The articles were evenly split across the four competing systems, and each HIT was completed by 5 turkers.", "labels": [], "entities": []}, {"text": "Upon completion the data was analyzed manually for accuracy since turkers entered each answer as free text, and to remove any meaningless datapoints.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9988501071929932}]}, {"text": "shows the average time (in seconds) to complete a single question, the overall accuracy of the participants, and the informativeness of a given summary type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9990211725234985}]}, {"text": "Excluding the use of human abstracts, all systems resulted in similar performance times.", "labels": [], "entities": []}, {"text": "However we observe a large margin in QA accuracy in our full system compared to the abstractive and our supervised approach.", "labels": [], "entities": [{"text": "QA", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.6023477911949158}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.896765947341919}]}, {"text": "Although participants rated the informativeness of the summaries to be the same our systems yielded a higher performance.", "labels": [], "entities": []}, {"text": "This strongly indicates that having a system which makes using of document comprehension has a tangible effect when applied towards a real-world task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summarization results on CNN test set. Summaries", "labels": [], "entities": [{"text": "CNN test set", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.991492509841919}, {"text": "Summaries", "start_pos": 49, "end_pos": 58, "type": "TASK", "confidence": 0.5931071043014526}]}, {"text": " Table 3: Summarization results on DM test set. To ensure a", "labels": [], "entities": [{"text": "DM test set", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8256853620211283}]}, {"text": " Table 4: Question-answering accuracies using different types of QA pairs (ROOT, SUBJ/OBJ, NER) and different source input", "labels": [], "entities": [{"text": "ROOT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9649185538291931}]}, {"text": " Table 5: Amazon mechanical turk experiments. Human ab-", "labels": [], "entities": []}]}