{"title": [{"text": "Incorporating Emoji Descriptions Improves Tweet Classification", "labels": [], "entities": [{"text": "Incorporating Emoji Descriptions Improves Tweet Classification", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.776909758647283}]}], "abstractContent": [{"text": "Tweets are short messages that often include specialized language such as hashtags and emojis.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple strategy to process emojis: replace them with their natural language description and use pretrained word embeddings as normally done with standard words.", "labels": [], "entities": []}, {"text": "We show that this strategy is more effective than using pretrained emoji embeddings for tweet classification.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7776874601840973}]}, {"text": "Specifically, we obtain new state-of-the-art results in irony detection and sentiment analysis despite our neural network is simpler than previous proposals.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.953107476234436}, {"text": "sentiment analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.968565970659256}]}], "introductionContent": [{"text": "Tweets are short messages shared on Twitter, one of the most popular social networking services with 326 million monthly active users word wide.", "labels": [], "entities": []}, {"text": "Tweets often use specialized language such as abbreviations (e.g., TBH: To be honest), hashtags (e.g., #NBAFinals), emoticons and emojis.", "labels": [], "entities": []}, {"text": "The Oxford Dictionary defines an emoticon as \"a facial expression such as a smile or frown, formed by various combinations of keyboard characters\" (e.g., \":)\", \":-(\"), and an emoji as \"a small digital image or icon used to express an idea or emotion\" (e.g., , , ).", "labels": [], "entities": [{"text": "Oxford Dictionary", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9694364070892334}]}, {"text": "While the number of emoticons is relatively small, the Unicode Standard includes over 2,800 emojis.", "labels": [], "entities": [{"text": "Unicode Standard", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.8838126957416534}]}, {"text": "Emojis are interesting because they succinctly encode meaning that otherwise would require more than one word to convey (e.g., grinning face, clapping hands and face with medical mask for the emojis above).", "labels": [], "entities": []}, {"text": "Additionally, emojis have become popular in social media.", "labels": [], "entities": []}, {"text": "5 billion emojis are sent daily on Facebook.", "labels": [], "entities": []}, {"text": "While only 6% of the top-100 Facebook headlines used emojis in 2015, 52% did so in 2017 14% of tweets and 50% of Instagram posts contain at least one emoji.", "labels": [], "entities": []}, {"text": "Irony detection and sentiment analysis in tweets are two popular tasks.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7608612179756165}, {"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9270457029342651}]}, {"text": "Sentiment analysis has received substantially more attention than irony detection.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9653150141239166}, {"text": "irony detection", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.9688473641872406}]}, {"text": "Irony, however, is a major error source in sentiment analysis (0.71 F1 overall but 0.29 F1 with ironic tweets), and natural language understanding in general does not generalize well with ironic texts (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9435368776321411}]}, {"text": "In this paper, we tackle both irony and sentiment analysis in tweets-two classification tasks.", "labels": [], "entities": [{"text": "sentiment analysis in tweets-two classification", "start_pos": 40, "end_pos": 87, "type": "TASK", "confidence": 0.7608325242996216}]}, {"text": "In particular, we focus on modeling emojis.", "labels": [], "entities": []}, {"text": "Understanding the emojis is critical to making irony and sentiment judgements.", "labels": [], "entities": [{"text": "making irony and sentiment judgements", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.5698857843875885}]}, {"text": "In the first example, the contrast between the emojis helps determining that irony is present (the hashtag #not also helps).", "labels": [], "entities": []}, {"text": "In the second tweet, the OK hand sign and face blowing a kiss emojis help reinforcing that the author is praising somebody and not being ironic.", "labels": [], "entities": []}, {"text": "Similarly, the smiling and sad emojis in the last two examples area clear sign of the author's sentiment towards the movie Ted 2 and the incompatibility issue.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we present a simple strategy to model emojis: replace them with their textual description.", "labels": [], "entities": []}, {"text": "Second, we show that this strategy outperforms previous methods and yields anew stateof-the-art in two tweet classification tasks: irony detection and sentiment analysis.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.749068021774292}, {"text": "irony detection", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.9183239936828613}, {"text": "sentiment analysis", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.9556699991226196}]}], "datasetContent": [{"text": "We experiment with two tweet classification tasks: irony detection and sentiment analysis.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7464154958724976}, {"text": "irony detection", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8758502304553986}, {"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9501773118972778}]}, {"text": "We use standard corpora and compare with previous work using the same setup (i.e., we train and test with exactly the same instances they did).", "labels": [], "entities": []}, {"text": "For sentiment analysis, we use the corpus released by, which has 62,617 tweets (positive: 22,277, neutral: 28,528, negative: 8,982).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9675621688365936}]}, {"text": "shows examples of positive and negative sentiment, and here is an example of neutral sentiment: I'm switching to TMobile tomorrow and I'm getting anew number.", "labels": [], "entities": [{"text": "TMobile", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.8598080277442932}]}, {"text": "We follow the metrics used by previous work.", "labels": [], "entities": []}, {"text": "Regarding irony detection, we report accuracy and macro-average F1 (all labels weighted equal regardless of frequency).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9384696781635284}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996170997619629}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9244194030761719}]}, {"text": "Regarding sentiment analysis, we report accuracy, average recall and F1.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9568738043308258}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9996374845504761}, {"text": "average", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9462772607803345}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9090030193328857}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9998207688331604}]}, {"text": "Following previous work, we calculate accuracy and average recall using all labels (positive, negative and neutral) but F1 using only positive and negative instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9995666146278381}, {"text": "average", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9454241991043091}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.904830813407898}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9935983419418335}]}, {"text": "We preprocess the input text following standard steps.", "labels": [], "entities": []}, {"text": "Specifically, we tokenize with the NLTK's TweetTokenizer, lowercase all text, and use regular expressions to remove stop words, numbers, urls, consecutive repeated words and Twitter users (i.e., tokens whose first character is '@').", "labels": [], "entities": []}, {"text": "We also expand hashtags (e.g., #PickANewSong: Pick anew song) with ekphrasis (.", "labels": [], "entities": []}, {"text": "Regarding emojis, we either (a) do nothing special and use pretrained emoji embeddings (EMJ-EMBED strategy), or (b) replace emojis with their textual description and use pretrained word embeddings for the words in their descriptions (EMJ-DESC strategy).", "labels": [], "entities": [{"text": "EMJ-DESC", "start_pos": 234, "end_pos": 242, "type": "DATASET", "confidence": 0.8947374224662781}]}, {"text": "Let us consider the following tweet: \"@Paul OConnor187 hi we going to see ted 2 at the Odeon cinemas at Glasgow on Wednesday \".", "labels": [], "entities": [{"text": "Odeon cinemas at Glasgow", "start_pos": 87, "end_pos": 111, "type": "DATASET", "confidence": 0.8877513259649277}]}, {"text": "After preprocessing, we transform it into \"hi we going see ted odeon cinemas glasgow wednesday \" or \"hi we going see ted odeon cinemas glasgow wednesday smiling face\" (EMJ-EMBED and EMJ-DESC strategies respectively).", "labels": [], "entities": [{"text": "EMJ-EMBED", "start_pos": 168, "end_pos": 177, "type": "DATASET", "confidence": 0.8265764117240906}, {"text": "EMJ-DESC", "start_pos": 182, "end_pos": 190, "type": "DATASET", "confidence": 0.8380935192108154}]}, {"text": "We experiment with a stack of two BiLSTMs () with attention () to generate distributed representations of the input, and a softmax layer as the output layer.", "labels": [], "entities": []}, {"text": "This architecture is simpler than previous proposals, but as we shall see,: Results on irony detection (Accuracy and Macro F1).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.9046158790588379}, {"text": "Accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.99566650390625}, {"text": "Macro F1", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.6202521324157715}]}, {"text": "Task A is a binary classification (yes / no) and Task B is a four-way classification (verbal irony with polarity contrast, other verbal irony, situational irony, non-irony).", "labels": [], "entities": []}, {"text": "Note that EMJ-EMBED uses both word and emoji embeddings whereas EMJ-DESC only uses word embeddings.", "labels": [], "entities": [{"text": "EMJ-EMBED", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9246347546577454}]}], "tableCaptions": [{"text": " Table 3: Results on irony detection (Accuracy and Macro F1). Task A is a binary classification (yes / no) and Task  B is a four-way classification (verbal irony with polarity contrast, other verbal irony, situational irony, non-irony).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8476466834545135}, {"text": "Accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9982256293296814}, {"text": "Macro F1", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7561286091804504}]}, {"text": " Table 4: Results on sentiment analysis (three-way classification: positive, neural or negative).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9698226451873779}]}]}