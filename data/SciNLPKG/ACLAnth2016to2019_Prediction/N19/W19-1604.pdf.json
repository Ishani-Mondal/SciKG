{"title": [{"text": "Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations", "labels": [], "entities": []}], "abstractContent": [{"text": "Human-robot interaction often occurs in the form of instructions given from a human to a robot.", "labels": [], "entities": []}, {"text": "For a robot to successfully follow instructions, a common representation of the world and objects in it should be shared between humans and the robot so that the instructions can be grounded.", "labels": [], "entities": []}, {"text": "Achieving this representation can be done via learning, where both the world representation and the language grounding are learned simultaneously.", "labels": [], "entities": []}, {"text": "However , in robotics this can be a difficult task due to the cost and scarcity of data.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the problem by separately learning the world representation of the robot and the language grounding.", "labels": [], "entities": []}, {"text": "While this approach can address the challenges in getting sufficient data, it may give rise to inconsistencies between both learned components.", "labels": [], "entities": []}, {"text": "Therefore , we further propose Bayesian learning to resolve such inconsistencies between the natural language grounding and a robot's world representation by exploiting spatio-relational information that is implicitly present in instructions given by a human.", "labels": [], "entities": []}, {"text": "Moreover, we demonstrate the feasibility of our approach on a scenario involving a robotic arm in the physical world.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consider yourself standing in your kitchen and having your robot assist you in preparing tonight's meal.", "labels": [], "entities": []}, {"text": "You then give it the instruction: 'fetch the bowl next to the bread knife!'.", "labels": [], "entities": []}, {"text": "For the robot to correctly perform your intended instruction, which is grounded in your world representation, it must * Equal contribution \u2020 ocan13@ku.edu.tr \u2021 pedro.zuidbergdosmartires@cs.kuleuven.be correctly ground your natural language instruction into its own world representation.", "labels": [], "entities": []}, {"text": "This small scenario already introduces the two key components of language grounding in robotics: the construction of a world representation from sensor data and the grounding of natural language into the constructed representation.", "labels": [], "entities": []}, {"text": "Ideally these two components would be learned in a joint fashion (.", "labels": [], "entities": []}, {"text": "However, the scarcity of data makes this approach impractical.", "labels": [], "entities": []}, {"text": "The millions of data points necessary for state-of-the-art joint computer vision and natural language processing are simply nonexisting.", "labels": [], "entities": []}, {"text": "We opt, therefore, to separately learn the world representation component and the language grounding component.", "labels": [], "entities": []}, {"text": "One approach for constructing a world representation of a robot is through so-called perceptual anchoring.", "labels": [], "entities": []}, {"text": "Perceptual anchoring handles the problem of creating and maintaining, overtime, the correspondence between symbols in a constructed world model and perceptual data that refer to the same physical object.", "labels": [], "entities": [{"text": "Perceptual anchoring", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.867422342300415}]}, {"text": "In this work, we use sensor driven bottom-up anchoring (), whereby anchors (symbolic representations of objects) can be created by perceptual observations derived directly from the input sensory data.", "labels": [], "entities": []}, {"text": "When modeling a scene, based on visual sensor data, through object anchoring, noise and uncertainties will inevitably be present.", "labels": [], "entities": [{"text": "object anchoring", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7363095879554749}]}, {"text": "This leads, for example, to a green 'apple' object being incorrectly anchored as a 'pear'.", "labels": [], "entities": []}, {"text": "For the language grounding, we opt to perform the learning on synthetic data that simulates the world represented as anchors.", "labels": [], "entities": []}, {"text": "This means that we do not ground the language using sensor data as signal but a symbolic representation of the world.", "labels": [], "entities": []}, {"text": "During training these symbols are synthetic and simulated, and during the deployment of the language grounding these are anchors provided by an anchoring system.", "labels": [], "entities": []}, {"text": "As the real world is inherently relational and as natural language instructions are often given in terms of spatial relations as well, the learned language grounder must also be able to ground spatial language such as 'next to'.", "labels": [], "entities": []}, {"text": "As a result of learning the construction of a world model and the language grounding separately, contradictions arise between the world representations of a human and a robot.", "labels": [], "entities": []}, {"text": "The supervision that an instruction would give to a robot is not present when learning the representation of the world of a robot.", "labels": [], "entities": []}, {"text": "These inconsistencies then propagate through to inconsistencies between the instructions a human gives to a robot and the robot's world model.", "labels": [], "entities": []}, {"text": "To ensure that a robot is able to correctly carryout an instruction, such inconsistencies must be resolved and the world model of the robot be matched to the world model of the human.", "labels": [], "entities": []}, {"text": "This is not the first paper that tackles the problem of belief revision in robotics.", "labels": [], "entities": [{"text": "belief revision", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7220450043678284}]}, {"text": "However, prior work (, with the notable exception of, relied on explicit information transfer between humans and robots when inconsistencies arose in grounded language and the robot's world representation.", "labels": [], "entities": []}, {"text": "An example would be a robot asking clarification questions until it is clear what the human meant ( . We propose an approach that probabilistically reasons over the grounding of an instruction and a robot's world representation in order to perform Bayesian learning to update the world representation given the grounding.", "labels": [], "entities": []}, {"text": "This is closely related to the work of Mast et al. who also deploy a Bayesian learning approach.", "labels": [], "entities": []}, {"text": "The key difference, however, is that they do not learn the language component but ground a description of a scene by relying on a predefined model to ground language.", "labels": [], "entities": []}, {"text": "We demonstrate the validity of our approach for reconciling instructions and world representations on a showcase scenario involving a camera, a robot arm and a natural language interface.", "labels": [], "entities": [{"text": "reconciling instructions and world representations", "start_pos": 48, "end_pos": 98, "type": "TASK", "confidence": 0.81186763048172}]}], "datasetContent": [], "tableCaptions": []}