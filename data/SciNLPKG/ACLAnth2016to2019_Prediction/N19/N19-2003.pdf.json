{"title": [{"text": "Practical Semantic Parsing for Spoken Language Understanding", "labels": [], "entities": [{"text": "Practical Semantic Parsing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6012682020664215}, {"text": "Spoken Language Understanding", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8333836595217387}]}], "abstractContent": [{"text": "Executable semantic parsing is the task of converting natural language utterances into logical forms that can be directly used as queries to get a response.", "labels": [], "entities": [{"text": "Executable semantic parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8133514920870463}]}, {"text": "We build a transfer learning framework for executable semantic parsing.", "labels": [], "entities": [{"text": "executable semantic parsing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6841718355814616}]}, {"text": "We show that the framework is effective for Question Answering (Q&A) as well as for Spoken Language Understanding (SLU).", "labels": [], "entities": [{"text": "Question Answering (Q&A)", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8941859688077655}, {"text": "Spoken Language Understanding (SLU)", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.837957481543223}]}, {"text": "We further investigate the case where a parser on anew domain can be learned by exploiting data on other domains, either via multi-task learning between the target domain and an auxiliary domain or via pre-training on the auxiliary domain and fine-tuning on the target domain.", "labels": [], "entities": []}, {"text": "With either flavor of transfer learning , we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9026012718677521}, {"text": "Overnight and NLmaps", "start_pos": 138, "end_pos": 158, "type": "DATASET", "confidence": 0.8238473335901896}]}, {"text": "The experiments carried out on data sets that are different in nature show how executable semantic parsing can unify different areas of NLP such as Q&A and SLU.", "labels": [], "entities": [{"text": "executable semantic parsing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6367457310358683}]}], "introductionContent": [{"text": "Due to recent advances in speech recognition and language understanding, conversational interfaces such as Alexa, Cortana, and Siri are becoming more common.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7384610176086426}, {"text": "language understanding", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7340039312839508}]}, {"text": "They currently have two large uses cases.", "labels": [], "entities": []}, {"text": "First, a user can use them to complete a specific task, such as playing music.", "labels": [], "entities": []}, {"text": "Second, a user can use them to ask questions where the questions are answered by querying knowledge graph or database back-end.", "labels": [], "entities": []}, {"text": "Typically, under a common interface, there exist two disparate systems that can handle each use cases.", "labels": [], "entities": []}, {"text": "The system underlying the first use case is known as a spoken language understanding (SLU) system.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.7991606791814169}]}, {"text": "Typical commercial SLU systems rely on predicting a coarse user intent and then tagging each word in the utterance to * Work conducted while interning at Amazon Alexa AI.", "labels": [], "entities": []}, {"text": "This architecture is popular due to its simplicity and robustness.", "labels": [], "entities": []}, {"text": "On the other hand, Q&A, which need systems to produce more complex structures such as trees and graphs, requires a more comprehensive understanding of human language.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9321106870969137}]}, {"text": "One possible system that can handle such a task is an executable semantic parser).", "labels": [], "entities": []}, {"text": "Given a user utterance, an executable semantic parser can generate tree or graph structures that represent logical forms that can be used to query a knowledge base or database.", "labels": [], "entities": []}, {"text": "In this work, we propose executable semantic parsing as a common framework for both uses cases by framing SLU as executable semantic parsing that unifies the two use cases.", "labels": [], "entities": [{"text": "executable semantic parsing", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7206921776135763}, {"text": "executable semantic parsing", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7345636487007141}]}, {"text": "For Q&A, the input utterances are parsed into logical forms that represent the machine-readable representation of the question, while in SLU, they represent the machine-readable representation of the user intent and slots.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7565223773320516}]}, {"text": "One added advantage of using parsing for SLU is the ability to handle more complex linguistic phenomena such as coordinated intents that traditional SLU systems struggle to handle).", "labels": [], "entities": []}, {"text": "Our parsing model is an extension of the neural transition-based parser of.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9662041068077087}]}, {"text": "A major issue with semantic parsing is the availability of the annotated logical forms to train the parsers, which are expensive to obtain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6899818032979965}]}, {"text": "A solution is to rely more on distant supervisions such as by using question-answer pairs).", "labels": [], "entities": []}, {"text": "Alternatively, it is possible to exploit annotated logical forms from a different domain or related data set.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the scenario where data sets for several domains exist but only very little data fora new one is available and apply transfer learning techniques to it.", "labels": [], "entities": []}, {"text": "A common way to implement transfer learning is by first pre-training the model on a domain on which a large data set is available and subsequently fine-tuning the model on the target domain.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9653968513011932}]}, {"text": "We also consider a multi-task learning (MTL) approach.", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6982409834861756}]}, {"text": "MTL refers to machine learning models that improve generalization by training on more than one task.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.667452871799469}, {"text": "generalization", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9644043445587158}]}, {"text": "MTL has been used fora number of NLP problems such as tagging, syntactic parsing (, machine translation ( and semantic parsing.", "labels": [], "entities": [{"text": "tagging", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9679325819015503}, {"text": "syntactic parsing", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7015409022569656}, {"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7866446077823639}, {"text": "semantic parsing", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.7221951484680176}]}, {"text": "See and for an overview of MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8897944092750549}]}, {"text": "A good Q&A data set for our domain adaptation scenario is the Overnight data set (), which contains sentences annotated with Lambda Dependency-Based Compositional Semantics (Lambda DCS; Liang 2013) for eight different domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7209662199020386}, {"text": "Overnight data set", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.9185949166615804}, {"text": "Lambda DCS; Liang 2013)", "start_pos": 174, "end_pos": 197, "type": "DATASET", "confidence": 0.8320915798346201}]}, {"text": "However, it includes only a few hundred sentences for each domain, and its vocabularies are relatively small.", "labels": [], "entities": []}, {"text": "We also experiment with a larger semantic parsing data set (NLmaps;.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7678721249103546}]}, {"text": "For SLU, we work with data from a commercial conversational assistant that has a much larger vocabulary size.", "labels": [], "entities": [{"text": "SLU", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9429106116294861}]}, {"text": "One common issue in parsing is how to deal with rare or unknown words, which is usually addressed by either delexicalization or by implementing a copy mechanism (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.972162127494812}]}, {"text": "We show clear differences in the outcome of these and other techniques when applied to data sets of varying sizes.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We propose a common semantic parsing framework for Q&A and SLU and demonstrate its broad applicability and effectiveness.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.775399774312973}]}, {"text": "\u2022 We report parsing baselines for Overnight for which exact match parsing scores have not been yet published.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9722254276275635}, {"text": "Overnight", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9768514037132263}]}, {"text": "\u2022 We show that SLU greatly benefits from a copy mechanism, which is also beneficial for NLmaps but not Overnight.", "labels": [], "entities": [{"text": "Overnight", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.8608670830726624}]}, {"text": "\u2022 We investigate the use of transfer learning and show that it can facilitate parsing on lowresource domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first run experiments on single-task semantic parsing to observe the differences among the three different data sources discussed in Section 4.", "labels": [], "entities": [{"text": "single-task semantic parsing", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6796021660168966}]}, {"text": "Specifically, we explore the impact of an attention mechanism on the performance as well as the comparison between delexicalization and a copy mechanism for dealing with data sparsity.", "labels": [], "entities": []}, {"text": "The metric used to evaluate parsers is the exact match accuracy, defined as the ratio of sentences cor-  rectly parsed.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.8400402863820394}]}], "tableCaptions": [{"text": " Table 1: Details of training data. # is the number of sen-", "labels": [], "entities": []}, {"text": " Table 2: Left side: Ablation experiments on attention mech-", "labels": [], "entities": [{"text": "Ablation", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9763521552085876}]}, {"text": " Table 3: Transfer learning results for the Overnight domains.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8393464386463165}, {"text": "Overnight domains", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9529491662979126}]}, {"text": " Table 4: Transfer learning results for SLU domains. BL +", "labels": [], "entities": [{"text": "BL", "start_pos": 53, "end_pos": 55, "type": "DATASET", "confidence": 0.5536060333251953}]}]}