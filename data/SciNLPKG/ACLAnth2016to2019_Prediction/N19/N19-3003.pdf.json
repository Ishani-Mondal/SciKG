{"title": [], "abstractContent": [{"text": "It has been established that the performance of speech recognition systems depends on multiple factors including the lexical content, speaker identity and dialect.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7116090804338455}]}, {"text": "Here we use three English datasets of acted emotion to demonstrate that emotional content also impacts the performance of commercial systems.", "labels": [], "entities": []}, {"text": "On two of the corpora, emotion is a bigger contributor to recognition errors than speaker identity and on two, neutral speech is recognized considerably better than emotional speech.", "labels": [], "entities": []}, {"text": "We further evaluate the commercial systems on spontaneous interactions that contain portions of emotional speech.", "labels": [], "entities": []}, {"text": "We propose and validate on the acted datasets, a method that allows us to evaluate the overall impact of emotion on recognition even when manual transcripts are not available.", "labels": [], "entities": []}, {"text": "Using this method, we show that emotion in natural spontaneous dialogue is a less prominent but still significant factor in recognition accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8800332546234131}]}], "introductionContent": [{"text": "Alexa and Google Home are becoming increasingly popular, their use spanning a range of applications from reducing loneliness in the elderly () to child entertainment and education (.", "labels": [], "entities": [{"text": "Alexa", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9170491695404053}]}, {"text": "As these conversational agents become commonplace, people are likely to express emotion during their interactions, either because of their perception of the agent or because of the emotioneliciting situations in which the agent is deployed.", "labels": [], "entities": []}, {"text": "In this paper, we set out to study the extent to which emotional content in speech impacts speech recognition performance of commercial systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7058765292167664}]}, {"text": "Similar studies have been conducted in the past to study how recognition varies with gender and dialect, lexical content (, topical domain () and delivery style.", "labels": [], "entities": []}, {"text": "A number of studies have studied the impact of stress and emotional factors) on speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9189943373203278}]}, {"text": "Multiple studies () tried to improve upon speech recognition accuracies for emotional speech.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7074031680822372}]}, {"text": "However, these studies were carried outwith older recognition systems.", "labels": [], "entities": []}, {"text": "Recently automatic speech recognition has seen unprecedented gains inaccuracy.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.637934813896815}]}, {"text": "Yet our work shows that emotional content still poses problems to speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7805844843387604}]}, {"text": "In our work we seek to quantify the influence of emotion on recognition accuracy for three commercial systems, on several datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9416987299919128}]}, {"text": "We start outwith two datasets of acted emotion, which are in some respects ideal for the task because the spoken content is constrained to pre-selected utterances and thus manual transcription is not required.", "labels": [], "entities": []}, {"text": "In addition, the lexical content for each emotion is identical, so no special adjustment for that confounding factor is needed in the acted corpora.", "labels": [], "entities": []}, {"text": "At the same time, it is important to validate these results on spontaneous, more natural exchanges, so we also present results on such a corpus of emotion in spontaneous speech.", "labels": [], "entities": []}, {"text": "As the speech becomes more natural, it becomes harder to obtain large manual transcripts fora large portion of the data to carryout the studies that we present, so we also validate an alternative method for finding factors that influence the performance of commercial systems, relying on agreement between systems rather than manual transcripts.", "labels": [], "entities": []}, {"text": "We present convincing evidence that the approach is a reasonable approximation and it can be used for broader studies on factors influencing automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.6019356846809387}]}, {"text": "Here, we apply the method to analyze data from a spontaneous emotional speech corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three acted Emotion datasets: CREMA-D (), RAVDESS (Steven R. Livingstone1, 2018) and MSP-IMPROV (.", "labels": [], "entities": [{"text": "RAVDESS", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.8249289989471436}]}, {"text": "CREMA-D has 12 sentences recorded by 91 actors in 6 different emotions (Anger, Disgust, Fear, Happy, Neutral and Sad), fora total of 7,442 utterances in the dataset.", "labels": [], "entities": [{"text": "CREMA-D", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8994041681289673}]}, {"text": "RAVDESS has just two sentences, which are very similar to each other (Kids are talking by the door and Dogs are sitting by the door), recorded by 24 actors in 8 emotions with the addition of Surprised and Calm.", "labels": [], "entities": [{"text": "RAVDESS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7094681262969971}]}, {"text": "RAVDESS has a total of 1,440 utterances, and each sentence is recorded in two intensities with two repetitions of each.", "labels": [], "entities": [{"text": "RAVDESS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7263185381889343}]}, {"text": "In addition to actors recording sentences in a pre-specified emotion, the MSP-IMPROV dataset contains 'improvised recordings', where actors converse to induce the desired emotion.", "labels": [], "entities": [{"text": "MSP-IMPROV dataset", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.7941164672374725}]}, {"text": "In these interactions, there is at least one emotional rendition of the target utterance but other utterances maybe emotionally neutral.", "labels": [], "entities": []}, {"text": "MSP-IMPROV is comprised of 1,272 utterances distributed over 20 target sentences and four emotions (Neutral, Anger, Happy, Fear).", "labels": [], "entities": []}, {"text": "We refer to this part of the corpus as MSP-IMPROV Target, where we only concern ourselves with recognizing the target sentence.", "labels": [], "entities": []}, {"text": "We refer to the set of complete conversations as the MSP-IMPROV Dialogue Corpus.", "labels": [], "entities": [{"text": "MSP-IMPROV Dialogue Corpus", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.8504709402720133}]}, {"text": "Manual transcripts are not available for this part.", "labels": [], "entities": []}, {"text": "MSP-IMPROV has 1,085 complete conversations.", "labels": [], "entities": [{"text": "MSP-IMPROV", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8890969157218933}]}, {"text": "Commercial systems used for speech recognition are IBM Watson Speech-to-Text, Google Cloud Speech-to-Text and Amazon Transcribe.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8256123960018158}]}, {"text": "2 . We will denote the APIs simply by IBM, GCP and AWS respectively.", "labels": [], "entities": [{"text": "AWS", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8909100890159607}]}, {"text": "We report two measures of automatic speech recognition performance: the Word Error Rate (WER) and the percentage of completely recognized sentences (CR).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.5893147985140482}, {"text": "Word Error Rate (WER)", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.9240825275580088}, {"text": "percentage of completely recognized sentences (CR)", "start_pos": 102, "end_pos": 152, "type": "METRIC", "confidence": 0.6537697687745094}]}, {"text": "Minor semantic and grammatical errors are ignored by manually listing semantically equivalent sentences for computing CR.", "labels": [], "entities": []}, {"text": "We report 1 \u2212 CR instead of CR to maintain consistency with W ER interpretation, lower the better.", "labels": [], "entities": [{"text": "CR", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9739820957183838}, {"text": "CR", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9946314096450806}, {"text": "consistency", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9752189517021179}, {"text": "W ER interpretation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.39665624499320984}]}, {"text": "We then perform an ANOVA analysis to determine the statistical significance of each factor in determining the WER.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9892224073410034}, {"text": "WER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.7211411595344543}]}, {"text": "The dialogues in the MSP-IMPROV corpus do not contain manual transcripts.", "labels": [], "entities": [{"text": "MSP-IMPROV corpus", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9060562551021576}]}, {"text": "We propose a metric to analyze the relative performance of different systems with varying emotions when manual transcripts are not available.", "labels": [], "entities": []}, {"text": "We calculate the performance of every system relative to other systems and then report the average of these cross-comparisons.", "labels": [], "entities": []}, {"text": "This method accurately predicts the relative performance on emotional and neutral speech consistent with WER/CR results on the other corpora for which transcripts are available.", "labels": [], "entities": [{"text": "WER/CR", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.6721636454264323}]}], "tableCaptions": [{"text": " Table 1: Overall Performance of IBM, AWS and GCP", "labels": [], "entities": [{"text": "AWS", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.9040878415107727}, {"text": "GCP", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.5972447991371155}]}, {"text": " Table 2: Spearman and Pearson Correlation between  the cross-comparison WER and the observed WER", "labels": [], "entities": []}, {"text": " Table 3: Spearman Correlation between two-system  cross comparison WER and the observed WER", "labels": [], "entities": []}, {"text": " Table 4: Statistical significance of various factors on speech recognition performance. For entries where the P- value is not mentioned, it is almost zero.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8766795098781586}, {"text": "P- value", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9872660438219706}]}, {"text": " Table 5: Cross-comparison WER for MSP-IMRPOV  Dialogue", "labels": [], "entities": [{"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9828132390975952}]}]}