{"title": [{"text": "Neural Constituency Parsing of Speech Transcripts", "labels": [], "entities": [{"text": "Neural Constituency Parsing of Speech Transcripts", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8513517379760742}]}], "abstractContent": [{"text": "This paper studies the performance of a neu-ral self-attentive parser on transcribed speech.", "labels": [], "entities": []}, {"text": "Speech presents parsing challenges that do not appear in written text, such as the lack of punctuation and the presence of speech dis-fluencies (including filled pauses, repetitions, corrections, etc.).", "labels": [], "entities": []}, {"text": "Disfluencies are especially problematic for conventional syntactic parsers, which typically fail to find any EDITED dis-fluency nodes at all.", "labels": [], "entities": []}, {"text": "This motivated the development of special disfluency detection systems , and special mechanisms added to parsers specifically to handle disfluencies.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7504023313522339}]}, {"text": "However, we show here that neural parsers can find EDITED disfluency nodes, and the best neu-ral parsers find them with an accuracy surpassing that of specialized disfluency detection systems, thus making these specialized mechanisms unnecessary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9990621209144592}]}, {"text": "This paper also investigates a modified loss function that puts more weight on EDITED nodes.", "labels": [], "entities": []}, {"text": "It also describes tree-transformations that simplify the disflu-ency detection task by providing alternative encodings of disfluencies and syntactic information .", "labels": [], "entities": [{"text": "disflu-ency detection", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8454161286354065}]}], "introductionContent": [{"text": "While a great deal of effort has been expended on parsing written text, parsing speech (either transcribed or ASR output) has received less attention.", "labels": [], "entities": [{"text": "parsing written text", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.8825774192810059}, {"text": "parsing speech", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.902487725019455}]}, {"text": "Parsing speech is important because speech is the easiest and most natural means of communication, it is increasingly used as an input modality in human-computer interactions.", "labels": [], "entities": [{"text": "Parsing speech", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8668873906135559}]}, {"text": "Speech presents parsing challenges that do not appear in written text, such as the lack of punctuation and sentence boundaries, speech recognition errors and the presence of speech disfluencies (including filled pauses, repetitions, corrections, etc.)", "labels": [], "entities": []}, {"text": "Of the major challenges associated with transcribed speech, we focus hereon speech disfluencies, which are frequent in spontaneous speech.", "labels": [], "entities": []}, {"text": "Disfluencies include filled pauses (\"um\", \"uh\"), parenthetical asides (\"you know\", \"I mean\"), interjections (\"well\", \"like\") and partial words (\"wou-\", \"oper-\").", "labels": [], "entities": []}, {"text": "One type of disfluency which is especially problematic for conventional syntactic parsers are speech repairs.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7025342583656311}]}, {"text": "Following the analysis of, a speech repair consists of three main parts; the reparandum, the interregnum and the repair.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7313728928565979}, {"text": "interregnum", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9828121066093445}]}, {"text": "As illustrated in the following example, the reparandum we don't is the part of the utterance that is replaced or repaired, the interregnum uh I mean (which consists of a filled pause uh and a discourse marker I mean) is an optional part of the disfluency, and the repair a lot of states don't replaces the reparandum.", "labels": [], "entities": []}, {"text": "The fluent version is obtained by removing the reparandum and the interregnum.", "labels": [], "entities": [{"text": "interregnum", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9510027766227722}]}, {"text": "We don't interregnum uh I mean repair a lot of states don't have capital punishment.", "labels": [], "entities": []}, {"text": "(1) In the Switchboard treebank corpus () the reparanda, filled pauses and discourse markers are dominated by EDITED, INTJ and PRN nodes, respectively (see).", "labels": [], "entities": [{"text": "Switchboard treebank corpus", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.8381706873575846}]}, {"text": "Of these disfluency nodes, EDITED nodes pose a major problem for conventional syntactic parsers, as the parsers typically fail to find any EDITED nodes at all.", "labels": [], "entities": []}, {"text": "Conventional parsers mainly capture tree-structured dependencies between words, while the relation between reparandum and repair is quite different: the repair is often a \"rough copy\" of the reparandum, using the same or very similar words in roughly the same order).", "labels": [], "entities": []}, {"text": "The \"rough copy\" dependencies are strong evidence of a disfluency, but conventional syntac-tic parsers cannot capture them.", "labels": [], "entities": []}, {"text": "Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in, which is an additional difficulty when integrating disfluency detection with syntactic parsing.", "labels": [], "entities": [{"text": "repair", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9777685403823853}, {"text": "disfluency detection", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.7440210580825806}, {"text": "syntactic parsing", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7179713547229767}]}, {"text": "This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing), and special mechanisms added to parsers specifically to handle disfluencies (.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7435463666915894}]}, {"text": "In this paper, we investigate the performance of a neural self-attentive constituency parser on speech transcripts.", "labels": [], "entities": []}, {"text": "We show that an \"off-theshelf\" self-attentive parser, unlike conventional parsers, can detect disfluent words with a performance which is competitive to or better than specialized disfluency detection systems.", "labels": [], "entities": []}, {"text": "In summary, the main contributions of this paper are: \u2022 We show that the self-attentive constituency parser sets anew state-of-the-art for syntactic parsing of transcribed speech, \u2022 A neural constituency parser can detect EDITED words with an accuracy surpassing that of specialized disfluency detection models, \u2022 We demonstrate that syntactic information helps the neural syntactic parsing detect disfluent words more accurately, \u2022 Replacing the constituent-based representation of disfluencies with a word-based representation of disfluencies improves the detection of disfluent words, \u2022 Modifying the training loss function to put more weight on EDITED nodes during training also improves disfluency detection.", "labels": [], "entities": [{"text": "syntactic parsing of transcribed speech", "start_pos": 139, "end_pos": 178, "type": "TASK", "confidence": 0.8496482372283936}, {"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9988958835601807}, {"text": "disfluency detection", "start_pos": 283, "end_pos": 303, "type": "TASK", "confidence": 0.7541959881782532}, {"text": "disfluency detection", "start_pos": 692, "end_pos": 712, "type": "TASK", "confidence": 0.7961316704750061}]}], "datasetContent": [{"text": "We evaluate the self-attentive parser on the Penn Treebank-3 Switchboard corpus ( Except as explicitly noted below, we remove all partial words (words tagged XX and words ending in \"-\") and punctuation from data, as they are not available in realistic ASR applications ().", "labels": [], "entities": [{"text": "Penn Treebank-3 Switchboard corpus", "start_pos": 45, "end_pos": 79, "type": "DATASET", "confidence": 0.9908298999071121}, {"text": "ASR", "start_pos": 252, "end_pos": 255, "type": "TASK", "confidence": 0.968814492225647}]}, {"text": "We evaluate the self-attentive parser in terms of parsing accuracy and disfluency detection performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9595082998275757}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.936823308467865}]}, {"text": "We report precision (P), recall (R) and fscore (F) for both constituent spans (S) and word positions (W), treating each word position as labeled by all the constituents that contain that word.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9527911841869354}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9512296319007874}, {"text": "fscore (F)", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9619894623756409}]}, {"text": "We also consider subsets of constituent spans and word positions; specifically: (i) SE , the set of constituent spans labeled EDITED, (ii) W E , the set of word positions dominated by one or more EDITED nodes, and (iii) W EIP , the set of word positions dominated by one or more EDITED, INTJ or PRN nodes.", "labels": [], "entities": [{"text": "SE", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9975702166557312}]}, {"text": "We demonstrate the evaluation metrics with an example here.", "labels": [], "entities": []}, {"text": "Consider the gold and predicted parse trees illustrated in.", "labels": [], "entities": []}, {"text": "The constituency trees are viewed as a set of labeled spans over the words of the sentence, where constituent spans are pairs of string positions.", "labels": [], "entities": []}, {"text": "As explained earlier, we ignore punctuation and partial words when calculating evaluation scores.", "labels": [], "entities": []}, {"text": "To calculate fscore fora span, i.e., F(S), the gold, predicted and correct labeled spans are counted.", "labels": [], "entities": [{"text": "fscore fora span", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.845320463180542}, {"text": "F", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9951670169830322}]}, {"text": "In this case, the number of predicted, gold and correctly predicted spans is 13, 14 and 12.", "labels": [], "entities": []}, {"text": "Since a parse tree with EDITED nodes identifies certain words as EDITED, we can evaluate how accurately a parser classifies words as EDITED (i.e. F(W E )).", "labels": [], "entities": []}, {"text": "Continuing with the example in, the number of predicted, gold and correctly predicted EDITED words is 1, 3 and 1.", "labels": [], "entities": []}, {"text": "Similarly, we can also measure how well the parser can identify all disfluency words, i.e., the words dominated by EDITED, INTJ or PRN nodes.", "labels": [], "entities": []}, {"text": "Continuing with the example in, the number of predicted, gold and correctly pre-", "labels": [], "entities": [{"text": "correctly pre-", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.8115290999412537}]}], "tableCaptions": [{"text": " Table 2: Parsing precision P(S E ), recall R(S E ) and f- score F(S E ) of EDITED nodes, parsing f-score F(S) and  EDITED word f-score F(W E ) on the Switchboard dev  set for the equally and differentially weighted loss.", "labels": [], "entities": [{"text": "Parsing precision P", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.6723684767882029}, {"text": "recall R", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9612858593463898}, {"text": "f- score F(S E )", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.8690528273582458}, {"text": "Switchboard dev  set", "start_pos": 151, "end_pos": 171, "type": "DATASET", "confidence": 0.7738736669222513}]}, {"text": " Table 3: EDITED word precision P(W E ), recall R(W E )  and f-score F(W E ) as well as EDITED, INTJ and PRN  word f-score F(W EIP ) on the Switchboard dev set for  different encodings of disfluency nodes in data. The  best f-scores are shown in bold.", "labels": [], "entities": [{"text": "EDITED word precision P(W E )", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.756498783826828}, {"text": "recall R(W E )", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.9151166478792826}, {"text": "EDITED", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.8732730150222778}, {"text": "INTJ", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.8835588693618774}, {"text": "PRN  word f-score F(W EIP )", "start_pos": 105, "end_pos": 132, "type": "METRIC", "confidence": 0.6200308036059141}, {"text": "Switchboard dev set", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.7456265091896057}]}, {"text": " Table 4: EDITED word F(W E ) and EDITED, INTJ and  PRN word f-score F(W EIP ) on the Switchboard dev set  for three versions of the training data.", "labels": [], "entities": [{"text": "EDITED word F(W E )", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.89497138772692}, {"text": "EDITED", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9680690765380859}, {"text": "INTJ", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9767167568206787}, {"text": "PRN word f-score F(W EIP )", "start_pos": 52, "end_pos": 78, "type": "METRIC", "confidence": 0.7723676972091198}, {"text": "Switchboard dev set", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.8355502486228943}]}]}