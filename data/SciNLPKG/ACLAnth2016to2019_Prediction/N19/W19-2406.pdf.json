{"title": [{"text": "Identifying Sensible Lexical Relations in Generated Stories", "labels": [], "entities": [{"text": "Identifying Sensible Lexical Relations", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9223488420248032}]}], "abstractContent": [{"text": "As with many text generation tasks, the focus of recent progress on story generation has been in producing texts that are perceived to \"make sense\" as a whole.", "labels": [], "entities": [{"text": "text generation", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7486790120601654}, {"text": "story generation", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7515099942684174}]}, {"text": "There are few automated metrics that address this dimension of story quality even on a shallow lexical level.", "labels": [], "entities": []}, {"text": "To initiate investigation into such metrics, we apply a simple approach to identifying word relations that contribute to the 'narrative sense' of a story.", "labels": [], "entities": []}, {"text": "We use this approach to comparatively analyze the output of a few notable story generation systems in terms of these relations.", "labels": [], "entities": [{"text": "story generation", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7294664084911346}]}, {"text": "We characterize differences in the distributions of relations according to their strength within each story.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current text generation systems are frequently able to produce output that is linguistically wellformed with regard to sentence-level syntactic and lexical dependencies.", "labels": [], "entities": []}, {"text": "Still, when people perceive the generated text as a whole, it often doesn't appear to \"make sense\".", "labels": [], "entities": []}, {"text": "There are many dimensions to what qualifies a text as sensible.", "labels": [], "entities": []}, {"text": "Recent work has focused on trying to model commonsense knowledge and reasoning via the domain of narrative.", "labels": [], "entities": [{"text": "commonsense knowledge and reasoning", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6971006393432617}]}, {"text": "From the perspective of this work, stories encode the rich set of coherence relations between entities and events by which people interpret their experiences.", "labels": [], "entities": []}, {"text": "This has led to frameworks that evaluate automated commonsense reasoning through story modeling tasks like predicting what happens next in a story ( . Accordingly, the challenge of story generation systems is to express the same commonsense relations that establish the coherence of human-authored stories.", "labels": [], "entities": [{"text": "predicting what happens next in a story", "start_pos": 107, "end_pos": 146, "type": "TASK", "confidence": 0.7923657298088074}]}, {"text": "One barrier to addressing this challenge is how to quantify the presence of these relations in a text.", "labels": [], "entities": []}, {"text": "People can readily provide intuitive judgments about whether a story makes sense, but there has been little exploration of cues for these judgments that can be modeled by current NLP analyses, even relatively shallow ones.", "labels": [], "entities": []}, {"text": "We address this in this work by examining a simple approach to detecting lexical relations that contribute to the coherence (or what we call 'narrative sense') of a story.", "labels": [], "entities": []}, {"text": "We apply this approach to compare the output of a few different story generation systems according to these relations.", "labels": [], "entities": []}, {"text": "Evaluation in general is an ongoing challenge in text generation research, and particularly for open-ended content like stories.", "labels": [], "entities": [{"text": "text generation", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7991994917392731}]}, {"text": "Some work has borrowed automated metrics used for evaluation in other generation tasks (e.g. BLEU for machine translation).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9911590218544006}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.762861967086792}]}, {"text": "However, such metrics expect that there is a fixed set of gold standard references to which output should be compared, which is not a fitting assumption for many story generation frameworks.", "labels": [], "entities": []}, {"text": "If the task is to generate a story about a particular topic or to generate a story given an opening sentence, there is no finite set of \"correct\" stories that meet the objective.", "labels": [], "entities": []}, {"text": "For this reason, most work relies on human judgment for evaluation (e.g., often through a quantitative rating or ranking scheme for selected quality dimensions (e.g. asking \"how coherent is this story?\" or \"which story is more coherent?\" among a set of candidates).", "labels": [], "entities": []}, {"text": "While these judgments area reliable indicator of the relative impact of different generation models, they are costly in that they must be repeated for each new set of generated output.", "labels": [], "entities": []}, {"text": "Moreover, relying on holistic ratings/rankings of quality does not provide insight into the text-level features that influence these judgments.", "labels": [], "entities": []}, {"text": "Qualitative feedback is useful for this, but it can be difficult for people to precisely verbalize their intuition about what makes a generated text sound good or bad.", "labels": [], "entities": []}, {"text": "Fully modeling this judgment may require sophisticated nat-ural language understanding capabilities, but we can still investigate whether shallow indicators of this judgment are available.", "labels": [], "entities": []}, {"text": "As more text generation systems are being deployed, comparative evaluations between them are becoming increasingly important.", "labels": [], "entities": [{"text": "text generation", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7581161558628082}]}, {"text": "Many researchers have released story generation models trained through their own particular experiments, including those described in Section 2.", "labels": [], "entities": [{"text": "story generation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7789107263088226}]}, {"text": "These already-trained models can be readily used by other NLP practitioners, but re-training them can often require significant time and resources due to their complexity.", "labels": [], "entities": []}, {"text": "In some cases (e.g. the GPT-2 system described below), the procedure for training the model is not publicly available.", "labels": [], "entities": [{"text": "GPT-2", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.7923749089241028}]}, {"text": "Still, this does not mean any comparative evaluation between systems trained on different datasets is fruitless.", "labels": [], "entities": []}, {"text": "Such evaluations may not be able to completely disentangle the contribution of a particular algorithmic approach versus that of the dataset itself, but they can still illuminate the relative impact of each model in the stories it produces.", "labels": [], "entities": []}, {"text": "Moreover, they can also help scrutinize any qualitative claims made about the performance of a particular system, since sometimes such claims are based on a handful of carefully selected examples.", "labels": [], "entities": []}, {"text": "Our vision is to move towards frameworks that can analyze the characteristics of story generation models even when they are presented as black boxes, simply by observing the stories they generate.", "labels": [], "entities": []}, {"text": "In this work, we analyze stories in terms of word relations in order to investigate whether such relations can be examined as an indicator of narrative sense.", "labels": [], "entities": []}, {"text": "As outlined in Section 3, to capture word relations we use a generic NLP technique of calculating statistical word co-occurences in a corpus of stories, in particular by using the Pointwise Mutual Information (PMI) statistic.", "labels": [], "entities": []}, {"text": "The use of statistical word association measures in narrative modeling tasks is familiar.", "labels": [], "entities": [{"text": "narrative modeling tasks", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.8217899004618326}]}, {"text": "There is work on using these measures to evaluate coherence in news stories.", "labels": [], "entities": []}, {"text": "Other work has used word co-occurrence statistics to predict commonsense cause-effect relations between sentences (.", "labels": [], "entities": []}, {"text": "A related line of research has focused on modeling pairs of verb-argument units in narrative text in order to induce story event sequences.", "labels": [], "entities": []}, {"text": "Other relevant tasks like emotional framing of narrative), sentence completion based on reading comprehension, and creative language generation () have also been addressed using lexical association measures.", "labels": [], "entities": [{"text": "emotional framing of narrative", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8406438082456589}, {"text": "sentence completion", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7809706032276154}, {"text": "creative language generation", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6711204548676809}]}, {"text": "Most relevant to generation evaluation, demonstrated that quality ratings of generated stories correlated significantly with the average PMI score of their component word pairs found in a story corpus.", "labels": [], "entities": [{"text": "PMI score", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9783967435359955}]}, {"text": "Our work takes a look at the distribution of word pair PMI scores in stories generated by alternative approaches that have not yet been directly compared.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics for the number of unique words and word pairs across all 13,453 evaluated stories", "labels": [], "entities": []}]}