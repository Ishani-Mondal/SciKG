{"title": [{"text": "ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters", "labels": [], "entities": [{"text": "Complex Factoid Question Answering", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6109587773680687}]}], "abstractContent": [{"text": "To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated.", "labels": [], "entities": [{"text": "factoid question answering (QA)", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.806563730041186}]}, {"text": "We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons.", "labels": [], "entities": []}, {"text": "ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology.", "labels": [], "entities": [{"text": "WikiAnswers community QA platform", "start_pos": 30, "end_pos": 63, "type": "DATASET", "confidence": 0.8139647096395493}]}, {"text": "Through a large crowd-sourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers.", "labels": [], "entities": []}, {"text": "ComQA contains 11, 214 questions grouped into 4,834 paraphrase clusters.", "labels": [], "entities": []}, {"text": "We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of crowdsourcing.", "labels": [], "entities": []}, {"text": "We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9260048866271973}]}], "introductionContent": [{"text": "Factoid QA is the task of answering natural language questions whose answer is one or a small number of entities).", "labels": [], "entities": [{"text": "Factoid QA", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6718328148126602}]}, {"text": "To advance research in QA in a manner consistent with the needs of end users, it is important to have access to datasets that reflect real user information needs by covering various question phenomena and the wide lexical and syntactic variety in expressing these information needs.", "labels": [], "entities": []}, {"text": "The benchmarks should be large enough to facilitate the use of data-hungry machine learning methods.", "labels": [], "entities": []}, {"text": "In this paper, we present ComQA, a large dataset of 11,214 real user questions collected from the WikiAnswers community QA website.", "labels": [], "entities": [{"text": "WikiAnswers community QA website", "start_pos": 98, "end_pos": 130, "type": "DATASET", "confidence": 0.8180335760116577}]}, {"text": "As shown in, the dataset contains various question phenomena.", "labels": [], "entities": []}, {"text": "ComQA questions are grouped into 4,834 paraphrase clusters through a large-scale crowdsourcing effort, which capture lexical and syntactic variety.", "labels": [], "entities": []}, {"text": "Crowdsourcing is also used to pair paraphrase clusters with answers to serve as a supervision signal for training and as a basis for evaluation.", "labels": [], "entities": []}, {"text": "contrasts ComQA with publicly available QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.7918211221694946}]}, {"text": "The foremost issue that ComQA tackles is ensuring research is driven by information needs formulated by real users.", "labels": [], "entities": []}, {"text": "Most largescale datasets resort to highly-templatic synthetically generated natural language questions 2016;).", "labels": [], "entities": []}, {"text": "Other datasets utilize search engine logs to collect their questions, which creates a bias towards simpler questions that search engines can already answer reasonably well.", "labels": [], "entities": []}, {"text": "In contrast, ComQA questions come from WikiAnswers, a community QA website where users pose questions to be answered by other users.", "labels": [], "entities": []}, {"text": "This is often a reflection of the fact that such questions are beyond the capabilities of commercial search engines and QA systems.", "labels": [], "entities": []}, {"text": "Questions in our dataset exhibit a wide range of interesting aspects such as the need for temporal reasoning ( ComQA is the result of a carefully designed large-scale crowdsourcing effort to group questions into paraphrase clusters and pair them with answers.", "labels": [], "entities": []}, {"text": "Past work has demonstrated the benefits of paraphrasing for QA (.", "labels": [], "entities": []}, {"text": "Motivated by this, we judiciously use crowdsourcing to obtain clean paraphrase clusters from WikiAnswers' noisy ones, resulting in ones like those shown in, with both lexical and syntactic variations.", "labels": [], "entities": []}, {"text": "The only other dataset to provide such clusters is that of, but that is based on synthetic information needs.", "labels": [], "entities": []}, {"text": "For answering, recent research has shown that combining various resources for answering significantly improves performance.", "labels": [], "entities": [{"text": "answering", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9852086305618286}]}, {"text": "Therefore, we do not pair ComQA with a specific knowledge base (KB) or text corpus for answering.", "labels": [], "entities": [{"text": "answering", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.9703251123428345}]}, {"text": "We call on the research community to innovate in combining different answering sources to tackle ComQA and advance research in QA.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to pair paraphrase clusters with answers.", "labels": [], "entities": []}, {"text": "ComQA answers are primarily Wikipedia entity URLs.", "labels": [], "entities": []}, {"text": "This has two motivations: (i) it builds on the example of search engines that use Wikipedia entities as answers for entitycentric queries (e.g., through knowledge cards), and (ii) most modern KBs ground their entities in Wikipedia.", "labels": [], "entities": []}, {"text": "Wherever the answers are temporal or measurable quantities, we use TIMEX3 1 and the International System of Units 2 for normalization.", "labels": [], "entities": [{"text": "TIMEX3 1", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9586480259895325}, {"text": "International System of Units 2", "start_pos": 84, "end_pos": 115, "type": "DATASET", "confidence": 0.9055160880088806}]}, {"text": "Providing canonical answers allows for better comparison of different systems.", "labels": [], "entities": []}, {"text": "We present an extensive analysis of ComQA, where we introduce the various question aspects of the dataset.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.7379668354988098}]}, {"text": "We also analyze the results of running state-of-the-art QA systems on ComQA.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9583615660667419}]}, {"text": "ComQA exposes major shortcomings in these systems, mainly related to their inability to handle compositionality, time, and comparison.", "labels": [], "entities": []}, {"text": "Our detailed error analysis provides inspiration for avenues of future work to ensure that QA systems meet the expectations of real users.", "labels": [], "entities": []}, {"text": "To summarize, in this paper we make the following contributions: \u2022 We present a dataset of 11,214 real user questions collected from a community QA website.", "labels": [], "entities": []}, {"text": "The questions exhibit a range of aspects that are important for users and challenging for existing QA systems.", "labels": [], "entities": []}, {"text": "Using crowdsourcing, questions are grouped into 4,834 paraphrase clusters that are annotated with answers.", "labels": [], "entities": []}, {"text": "ComQA is available at: http://qa. mpi-inf.mpg.de/comqa.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9068285822868347}]}, {"text": "\u2022 We present an extensive analysis and quantify the various difficulties in ComQA.", "labels": [], "entities": []}, {"text": "We also present the results of state-of-the art QA systems on ComQA, and a detailed error analysis.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9649049043655396}]}], "datasetContent": [{"text": "Our goal is to collect factoid questions that represent real information needs and cover a range of question aspects.", "labels": [], "entities": []}, {"text": "Moreover, we want to have different paraphrases for each question.", "labels": [], "entities": []}, {"text": "To this end, we tap into the potential of community QA platforms.", "labels": [], "entities": []}, {"text": "Questions posed there represent real information needs.", "labels": [], "entities": []}, {"text": "Moreover, users of those platforms provide (noisy) annotations around questions e.g., paraphrase clusters.", "labels": [], "entities": []}, {"text": "In this work, we exploit the annotations where users mark questions as duplicates as a basis for paraphrase clusters, and clean those.", "labels": [], "entities": []}, {"text": "Concretely, we started with the WikiAnswers crawl by.", "labels": [], "entities": [{"text": "WikiAnswers crawl by", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9114842613538107}]}, {"text": "We obtained ComQA from this crawl primarily through a largescale crowdsourcing effort, which we describe in what follows.", "labels": [], "entities": []}, {"text": "The original resource curated by Fader et al. contains 763M questions.", "labels": [], "entities": []}, {"text": "Questions in the crawl are grouped into 30M paraphrase clusters based on feedback from WikiAnswers users.", "labels": [], "entities": []}, {"text": "This clustering has a low accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993616938591003}]}, {"text": "Extracting factoid questions and cleaning the clusters are thus essential fora high-quality dataset.", "labels": [], "entities": [{"text": "Extracting factoid questions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8632050553957621}]}, {"text": "In this section, we present a manual analysis of 300 questions sampled at random from the ComQA dataset.", "labels": [], "entities": [{"text": "ComQA dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.962548553943634}]}, {"text": "This analysis helps understand the different aspects of our dataset.", "labels": [], "entities": []}, {"text": "A summary of the analysis is presented in.", "labels": [], "entities": []}, {"text": "We categorized each question as either simple or complex.", "labels": [], "entities": []}, {"text": "A question is complex if it belongs to one or more of the compositional, temporal, or comparison classes.", "labels": [], "entities": []}, {"text": "56.33% of the questions were complex; 32% compositional, 23.67% temporal, and 29.33% contain comparison conditions.", "labels": [], "entities": []}, {"text": "A question may contain multiple conditions (\"What country has the highest population in the year 2008?\" with comparison and temporal conditions).", "labels": [], "entities": []}, {"text": "We also identified questions of telegraphic nature e.g., \"Julia Alvarez's parents?\", with 8% of our questions being telegraphic.", "labels": [], "entities": []}, {"text": "Such questions pose a challenge for systems that rely on linguistic analysis of questions ().", "labels": [], "entities": []}, {"text": "We counted the number of named entities in questions: 23.67% contain two or more entities, reflecting their compositional nature, and 2.67% contain no entities e.g., \"What public company has the most employees in the world?\".", "labels": [], "entities": []}, {"text": "Such questions can be hard as many methods assume the existence of a pivot entity in a question.", "labels": [], "entities": []}, {"text": "Finally, 3.67% of the questions are unanswerable, e.g., \"Who was the first human being on Mars?\".", "labels": [], "entities": []}, {"text": "Such questions incentivise QA systems to return non-empty answers only when suitable.", "labels": [], "entities": []}, {"text": "In we compare ComQA with other current datasets based on real user information needs over different question categories.", "labels": [], "entities": []}, {"text": "We annotated each question with the most fine-grained context-specific answer type.", "labels": [], "entities": []}, {"text": "Answers in ComQA belong to a diverse set of types that range from coarse (e.g., person) to fine (e.g., sports manager).", "labels": [], "entities": []}, {"text": "Types also include literals such as number and date.", "labels": [], "entities": []}, {"text": "shows answer types of the 300 annotated examples as a word cloud.", "labels": [], "entities": []}, {"text": "We annotated questions with topics to which they belong (e.g., geography, movies, sports).", "labels": [], "entities": []}, {"text": "These are shown in, and demonstrate the topical diversity of ComQA.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8466337323188782}]}, {"text": "Questions in ComQA are fairly long, with a mean length of 7.73 words, indicating the compositional nature of questions.", "labels": [], "entities": []}, {"text": "In this section we present experimental results for running ComQA through state-of-the-art QA systems.", "labels": [], "entities": []}, {"text": "Our experiments show that these systems achieve humble performance on ComQA.", "labels": [], "entities": [{"text": "ComQA", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.909929096698761}]}, {"text": "Through a detailed analysis, this performance can be attributed to systematic shortcomings in handling various question aspects in ComQA.", "labels": [], "entities": []}, {"text": "We partition ComQA into a random train/dev/test split of 70/10/20% with 7,850, 1,121 and 2,243 questions, respectively.", "labels": [], "entities": []}, {"text": "We follow the community's standard evaluation metrics: we compute average precision, recall, and F1 scores across all test questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9217911958694458}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9985347986221313}, {"text": "F1 scores", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9820688366889954}]}, {"text": "For unanswerable questions whose correct answer is the empty set, we define precision and recall to be 1 fora system that returns an empty set, and 0 otherwise (Rajpurkar et al., 2018).", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9991402626037598}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9981164932250977}]}], "tableCaptions": [{"text": " Table 3: Comparison of ComQA with existing datasets over various phenomena. We manually annotated 100  random questions from each dataset.", "labels": [], "entities": []}, {"text": " Table 4: Results of baselines on ComQA test set.", "labels": [], "entities": [{"text": "ComQA test set", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9238804578781128}]}, {"text": " Table 5: Results of baselines on different datasets.", "labels": [], "entities": []}, {"text": " Table 6: Distribution of failure sources on ComQA  questions on which QUINT and AQQU failed.", "labels": [], "entities": [{"text": "AQQU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.8037617802619934}]}]}