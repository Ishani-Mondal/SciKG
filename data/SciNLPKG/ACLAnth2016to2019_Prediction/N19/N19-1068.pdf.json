{"title": [], "abstractContent": [{"text": "Authorship verification is the problem of inferring whether two texts were written by the same author.", "labels": [], "entities": [{"text": "Authorship verification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8037649393081665}]}, {"text": "For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts.", "labels": [], "entities": [{"text": "unmasking", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9555293321609497}]}, {"text": "In this paper , we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff.", "labels": [], "entities": [{"text": "authorship verification of texts as short", "start_pos": 77, "end_pos": 118, "type": "TASK", "confidence": 0.8465157846609751}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9970229268074036}, {"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9351381063461304}]}, {"text": "Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions.", "labels": [], "entities": []}, {"text": "The new approach is on par with other state-of-the-art techniques that are optimized for texts of this length: it achieves accuracies of 75-80 %, while also allowing for easy adjustment to forensic scenarios that require higher levels of confidence in the classification.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9971484541893005}]}], "introductionContent": [{"text": "With advances in computational stylometry, determining the original authorship of unknown literary publications can be accomplished with near certainty by state-of-the-art authorship verification.", "labels": [], "entities": [{"text": "determining the original authorship of unknown literary publications", "start_pos": 43, "end_pos": 111, "type": "TASK", "confidence": 0.8125930652022362}]}, {"text": "If the source material is abundant and sufficiently many known publications exist, linking them together is hardly a challenge.", "labels": [], "entities": []}, {"text": "But the playing field changes entirely if only fragments are available for verification, either because few known works of an author exist or because the text to be verified is only a few pages long.", "labels": [], "entities": []}, {"text": "With classification results significantly above chance, yet far below certainty, verification approaches struggle to produce reliable results in short-text scenarios and thus lack real-world practicality for material far below book length.", "labels": [], "entities": [{"text": "certainty", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9905812740325928}]}, {"text": "Even the unmasking approach by, which otherwise proved to be one of the most ingenious and robust verification approaches, fails in this setting and can only deliver below-average performance compared to more specialized verification systems, which still display high uncertainty themselves with an error of up to 25 % (.", "labels": [], "entities": []}, {"text": "At PAN 2015, individual texts of the English-language dataset had an average size of about 1.5 kB (less than 400 words), so it does not come as a surprise that none of the participants employed unmasking.", "labels": [], "entities": [{"text": "PAN 2015", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8642051815986633}]}, {"text": "To tackle the uncertainty problem of authorship verification on short texts, we propose a generalized unmasking approach which prioritizes precision so as to verify authorship with reliable results while rejecting cases of low certainty.", "labels": [], "entities": [{"text": "authorship verification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.917127937078476}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9974684715270996}]}, {"text": "We also present anew open-source general-purpose unmasking framework as a highly-customizable implementation of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data we use for our experiments is a collection of 180 text pairs (consisting of 90 same-author and 90 different-authors cases) for training and a similar set of 80 text pairs (consisting of 40 sameauthor and 40 different-authors cases) for testing.", "labels": [], "entities": []}, {"text": "The texts were obtained from Project Gutenberg, comprise about 4,000 words each (23,000 characters), and were written by a total of 390 unique English-language authors.", "labels": [], "entities": []}, {"text": "We took special care to select texts of similar genre and publication period to avoid accidental topic classification.", "labels": [], "entities": [{"text": "accidental topic classification", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7049081325531006}]}, {"text": "Since short-text authorship verification presents itself as such a difficult problem, it becomes all the more important to find a good measure for assessing the quality of a verifier.", "labels": [], "entities": [{"text": "short-text authorship verification", "start_pos": 6, "end_pos": 40, "type": "TASK", "confidence": 0.6301265060901642}]}, {"text": "Due to the high uncertainty of many results, a standard two-class accuracy measure is not an optimal choice.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9820855855941772}]}, {"text": "Instead of trying to develop perfect verifiers, we can already build more useful tools today by optimizing for precision and sacrificing recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9986844658851624}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9956152439117432}]}, {"text": "Unfortunately, this approach-although more useful in generaldoes not perform well in a setting where accuracy is measured.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9988754391670227}]}, {"text": "In order to provide a more suitable evaluation quantity, PAN adopted the c@1 measure by Pe\u00f1as and Rodrigo (2011): where n denotes the number of problems, n ac the number of correct answers and nu the number of non-answers.", "labels": [], "entities": [{"text": "PAN", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.5602996945381165}]}, {"text": "The c@1 measure solves the uncertainty problem by rewarding non-answers in that it assigns them the same accuracy as the rest of the problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9961453676223755}]}, {"text": "All-correct answers still yield a score of 1, all-wrong or completely unanswered problem sets a score of 0.", "labels": [], "entities": []}, {"text": "Hence, with this measure, a verifier is at liberty not to answer a given problem in case of doubt and still receive a reasonably good score, even if its overall sensitivity (or recall) is low.", "labels": [], "entities": [{"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9877995252609253}]}, {"text": "Most PAN participants did not exploit the c@1 measure to a larger extent, so c@1 scores of their submitted approaches are roughly the same as their accuracy (within margin of a few percent).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9993796348571777}]}, {"text": "A problem with c@1 is that it is still designed for binary classification with equal weights for both classes.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7021367251873016}]}, {"text": "If we are primarily interested in whether two texts were written by the same author, but do not need a reliable decision in the other case, c@1 does not serve us well.", "labels": [], "entities": []}, {"text": "For that reason, we propose as an alternative the F 0.5 measure where we treat non-answers as false negatives: (1 + 0.5 2 ) \u00b7 n tp (1 + 0.5 2 ) \u00b7 n tp + 0.5 2 \u00b7 (n fn + nu ) + n fp , with n tp denoting the number of true positives, n fn the number of false negatives, and n fp the number of false positives.", "labels": [], "entities": [{"text": "F 0.5 measure", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9549828171730042}]}, {"text": "As before, nu is the number of unanswered problems.", "labels": [], "entities": []}, {"text": "The parameter \u03b2 = 0.5 of the F measure was chosen so as to weigh precision substantially higher than recall without diminishing its contribution entirely.", "labels": [], "entities": [{"text": "F measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9848756194114685}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9990015625953674}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9986988306045532}]}, {"text": "Other values can be used depending on individual use cases.", "labels": [], "entities": []}, {"text": "We call this specialized F 0.5 measure F 0.5u .", "labels": [], "entities": [{"text": "F 0.5 measure F 0.5u", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.9269452810287475}]}], "tableCaptions": [{"text": " Table 1: Unmasking performance on our test data at  various confidence thresholds. Recall was calculated  after assigning all non-decisions the negative class.  F 0.5u and c@1 diverge significantly at high thresholds  with an increasing class balance skew.", "labels": [], "entities": [{"text": "Recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9994198083877563}, {"text": "F", "start_pos": 162, "end_pos": 163, "type": "METRIC", "confidence": 0.9850857257843018}]}, {"text": " Table 2: Performance comparison with the state of the  art in short-text authorship verification (c = 0.1 in  generalized unmasking). Differences between the first  three are non-significant.", "labels": [], "entities": [{"text": "authorship verification", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.7910679578781128}]}]}