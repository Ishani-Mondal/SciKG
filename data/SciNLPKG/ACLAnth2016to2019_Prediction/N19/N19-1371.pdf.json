{"title": [{"text": "Inferring Which Medical Treatments Work from Reports of Clinical Trials", "labels": [], "entities": [{"text": "Inferring", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9795240759849548}]}], "abstractContent": [{"text": "How do we know if a particular medical treatment actually works?", "labels": [], "entities": []}, {"text": "Ideally one would consult all available evidence from relevant clinical trials.", "labels": [], "entities": []}, {"text": "Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them.", "labels": [], "entities": []}, {"text": "In this paper , we present anew task and corpus for making this unstructured evidence actionable.", "labels": [], "entities": []}, {"text": "The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention , comparator, and outcome of interest , e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo.", "labels": [], "entities": []}, {"text": "We present anew corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs.", "labels": [], "entities": []}, {"text": "Results using a suite of models-ranging from heuris-tic (rule-based) approaches to attentive neu-ral architectures-demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts.", "labels": [], "entities": []}, {"text": "To facilitate further work on this important, challenging problem we make the corpus, documentation , a website and leaderboard, and code for baselines and evaluation available at http: //evidence-inference.ebm-nlp.com/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Biomedical evidence is predominantly disseminated in unstructured, natural language scientific manuscripts that describe the conduct and results of randomized control trials (RCTs).", "labels": [], "entities": []}, {"text": "The published evidence base is vast and expanding: at present more than 100 reports of RCTs are published everyday, on average.", "labels": [], "entities": [{"text": "RCTs", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.8772960305213928}]}, {"text": "It is thus time-consuming, and often practically impossible, to sort through all of the relevant published literature to robustly answer questions such article and prompt answer and rationale: The task.", "labels": [], "entities": []}, {"text": "Given a treatment A, a comparator B, and an outcome, infer the reported relationship between A and B with respect to outcome, and provide evidence supporting this from the text.", "labels": [], "entities": []}, {"text": "as: Does infliximab reduce dysmenorrhea (pain) scores, relative to placebo?", "labels": [], "entities": []}, {"text": "Given the critical role published reports of trials play in informing evidence-based care, organizations such as the Cochrane collaboration and groups at evidence-based practice centers (EPCs) are dedicated to manually synthesizing findings, but struggle to keep up with the literature.", "labels": [], "entities": []}, {"text": "NLP can play a key role in automating this process, thereby mitigating costs and keeping treatment recommendations up-todate with the evidence as it is published.", "labels": [], "entities": []}, {"text": "In this paper, we consider the task of inferring whether a given treatment is effective with respect to a specified outcome.", "labels": [], "entities": []}, {"text": "Typically, this assessment is done relative to other treatment options (i.e., comparators).", "labels": [], "entities": []}, {"text": "We assume the model is provided with a prompt that specifies an intervention, a comparator, and an outcome, along with a fulltext article.", "labels": [], "entities": []}, {"text": "The model is then to infer the reported findings with respect to this prompt ().", "labels": [], "entities": []}, {"text": "From a healthcare perspective, this inference task is an essential step for automating extraction of actionable evidence from trial reports.", "labels": [], "entities": [{"text": "automating extraction of actionable evidence from trial reports", "start_pos": 76, "end_pos": 139, "type": "TASK", "confidence": 0.8341577872633934}]}, {"text": "From an NLP standpoint, the proposed task can be seen as an instance of natural language inference (, viewing the article and prompt as the premise and hypothesis, respectively.", "labels": [], "entities": []}, {"text": "However, the problem differs in a few important ways from existing NLP formulations.", "labels": [], "entities": []}, {"text": "First, the inputs: prompts are brief (\u223c13.5 words on average), but articles are long (\u223c4200 words).", "labels": [], "entities": []}, {"text": "Further, only a few snippets of the article will be relevant to the label fora given prompt.", "labels": [], "entities": []}, {"text": "Second, prompts in this domain are structured, and include only a few types of key information: interventions, comparators, and outcomes.", "labels": [], "entities": []}, {"text": "Methods that exploit this regularity are likely to be more accurate than generic inference algorithms.", "labels": [], "entities": []}, {"text": "Another interesting property of this task is that the target for an article depends on the interventions and outcome specified by a given prompt.", "labels": [], "entities": []}, {"text": "Most articles report results for multiple interventions and outcomes: 67% of articles in our corpus are associated with two or more prompts that have different labels, e.g., indicating that a specific treatment was comparatively effective for one outcome but not for another.", "labels": [], "entities": []}, {"text": "As a concrete example from our corpus, infliximab was reported as realizing no significant difference with respect to dysmenorrhea, compared to a placebo.", "labels": [], "entities": []}, {"text": "But infliximab was associated with a significant increase in painkiller intake, again compared to placebo.", "labels": [], "entities": []}, {"text": "Generally positive words in an article (e.g., \"improved\") will confuse inference models that fail to account for this.", "labels": [], "entities": []}, {"text": "One may view these as built-in \"adversarial\" examples (Jia and Liang, 2016) for the task.", "labels": [], "entities": []}, {"text": "A key sub-problem is thus identifying snippet(s) of evidence in an article relevant to a given input prompt.", "labels": [], "entities": []}, {"text": "Attention mechanisms () conditioned on prompts would seem a natural means to achieve this, and we do find that these achieve predictive gains, but they are modest.", "labels": [], "entities": []}, {"text": "Existing attention variants seem to struggle to consistently attend to relevant evidence, even when explicitly pretrained using marked rationales.", "labels": [], "entities": []}, {"text": "This corpus can facilitate further research in attention variants designed for lengthy inputs).", "labels": [], "entities": []}, {"text": "In sum, our contributions are threefold.", "labels": [], "entities": []}, {"text": "We: (1) formulate a novel task (evidence inference) that is both practically important and technically challenging; (2) Provide anew publicly-available corpus comprising 10,000+ evidence \"prompts\", answers, supporting evidence spans, and associated full-text articles (http:// evidence-inference.ebm-nlp.com) all manually annotated by medical doctors; (3) Develop baseline algorithms to establish state-ofthe-art performance and highlight modeling challenges posed by this new task.", "labels": [], "entities": [{"text": "evidence inference)", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7610061168670654}]}], "datasetContent": [{"text": "We hired 16 doctors from Upwork and split them at random into groups: 10 for prompt generation, 3 for annotation, and 3 for verification.", "labels": [], "entities": [{"text": "Upwork", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9714540839195251}, {"text": "prompt generation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7973860800266266}]}, {"text": "In total, we have acquired 10,137 annotated prompts for 2,419 unique articles.", "labels": [], "entities": []}, {"text": "For each of these prompts, we have at least two independent sets of labels and associated rationales (supporting snippets).", "labels": [], "entities": []}, {"text": "We additionally calculated agreement between prompt generators, annotators and verifiers using Krippendorf's \u03b1.", "labels": [], "entities": []}, {"text": "To calculate this, we converted the verifier's binary labels of valid or not to the label with which they agreed.", "labels": [], "entities": []}, {"text": "This yields \u03b1 = 0.88.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.97771817445755}]}, {"text": "Removing the verifier annotations from the calculation results in \u03b1 = 0.86.", "labels": [], "entities": []}, {"text": "Intervention, outcome, and comparator strings contain on average 5.1, 5.3, and 3.4 tokens, respectively.", "labels": [], "entities": []}, {"text": "Articles comprise a mean of 4.2k tokens.", "labels": [], "entities": []}, {"text": "We provide additional details concerning the dataset in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8605525493621826}]}, {"text": "One doctor from verification was moved to annotation in order to increase productivity.", "labels": [], "entities": []}, {"text": "Each prompt has at least two rows, one of which corresponds to the prompt generator's answer, and the other for an annotator's.", "labels": [], "entities": []}, {"text": "These rows can be distinguished using the UserID column, in which prompt generator answers are marked with a value of '0', whereas annotator UserIDs are values greater than '0'.", "labels": [], "entities": []}, {"text": "Each row also includes the verifier's response, which denotes the validity of that row's answer and rationale.", "labels": [], "entities": []}, {"text": "We additionally include the offsets of where the rationale occurs in the text . These offsets are calculated through FuzzyWuzzy, due to the frequent differences in encoding between rationales and extracted XML text.", "labels": [], "entities": [{"text": "FuzzyWuzzy", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.758821427822113}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics. Labels -1, 0, 1 indicate significantly decreased, no significant difference and significantly  increased, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Summary results on the evidence inference task test set, averaged over five independent runs (with inde- pendent random initialization values). Metrics are macro-averages over classes. Evidence token AUC and mass  (last column) quantify identification of relevant supporting tokens. Models in the top two rows perform no learning;  the second two correspond to linear models; the rest are neural model variants.  \u2020 indicates 'token-wise' attention  pretraining; we report results for alternative attention losses in the Appendix.", "labels": [], "entities": [{"text": "Evidence token AUC", "start_pos": 195, "end_pos": 213, "type": "METRIC", "confidence": 0.5144015749295553}]}, {"text": " Table 3: Average per-class test performance of best  overall model (pretrain conditional attention).", "labels": [], "entities": []}, {"text": " Table 5: Average results achieved (macro-averages  over five runs) by the neural model when it is provided  only the article or only the prompt. We reproduce re- sults for the best model from Table 2 and the vanilla  (no attention) end-to-end neural network for context.", "labels": [], "entities": []}, {"text": " Table 6: Average results achieved when models are  provided directly with the reference evidence spans.", "labels": [], "entities": []}]}