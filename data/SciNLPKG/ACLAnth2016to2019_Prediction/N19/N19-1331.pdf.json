{"title": [{"text": "Continual Learning for Sentence Representations Using Conceptors", "labels": [], "entities": [{"text": "Sentence Representations", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8327593803405762}]}], "abstractContent": [{"text": "Distributed representations of sentences have become ubiquitous in natural language processing tasks.", "labels": [], "entities": [{"text": "Distributed representations of sentences", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8318628817796707}]}, {"text": "In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9987723231315613}]}, {"text": "To address this problem, we propose to initialize sentence en-coders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of concep-tor matrices to learn corpus-dependent features.", "labels": [], "entities": []}, {"text": "We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora .", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of sentences are essential fora wide variety of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Although recently proposed sentence encoders have achieved remarkable results (e.g.,), most, if not all, of them are trained on a priori fixed corpora.", "labels": [], "entities": []}, {"text": "However, in open-domain NLP systems such as conversational agents, oftentimes we are facing a dynamic environment, where training data are accumulated sequentially overtime and the distributions of training data vary with respect to external input.", "labels": [], "entities": []}, {"text": "To effectively use sentence encoders in such systems, we propose to consider the following continual sentence representation learning task: Given a sequence of corpora, we aim to train sentence encoders such that they can continually learn features from new corpora while retaining strong performance on previously encountered corpora.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.8030200997988383}]}, {"text": "Toward addressing the continual sentence representation learning task, we propose a simple sentence encoder that is based on the summation and linear transform of a sequence of word vectors aided by matrix conceptors.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.8114636937777201}]}, {"text": "Conceptors have their origin in reservoir computing and recently have been used to perform continual learning in deep neural networks.", "labels": [], "entities": []}, {"text": "Here we employ Boolean operations of conceptor matrices to update sentence encoders overtime to meet the following desiderata: 1.", "labels": [], "entities": []}, {"text": "The initialized sentence encoder (no training corpus used) can effectively produce sentence embeddings.", "labels": [], "entities": []}, {"text": "2. Resistant to catastrophic forgetting.", "labels": [], "entities": [{"text": "Resistant to catastrophic forgetting", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.7527900487184525}]}, {"text": "When the sentence encoder is adapted on anew training corpus, it retains strong performances on old ones.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly review a family of linear sentence encoders.", "labels": [], "entities": []}, {"text": "Then we explain how to build upon such sentence encoders for continual sentence representation learning tasks, which lead to our proposed algorithm.", "labels": [], "entities": [{"text": "sentence representation learning tasks", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.814604714512825}]}, {"text": "Finally, we demonstrate the effectiveness of the proposed method using semantic textual similarity tasks.", "labels": [], "entities": []}, {"text": "Notation We assume each word w from a vocabulary set V has a real-valued word vector v w \u2208 Rn . Let p(w) be the monogram probability of a word w.", "labels": [], "entities": []}, {"text": "A corpus Dis a collection of sentences, where each sentence s \u2208 Dis a multiset of words (word order is ignored here).", "labels": [], "entities": []}, {"text": "For a collection of vectors Y = {y i } i\u2208I , where y i \u2208 R l for i in an index set I with cardinality |I|, we let [y i ] i\u2208I \u2208 R l\u00d7|I| be a matrix whose columns are vectors y 1 , \u00b7 \u00b7 \u00b7 , y |I| . An identity matrix is denoted by I.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach for continual sentence representation learning using semantic textual similarity (STS) datasets.", "labels": [], "entities": [{"text": "continual sentence representation learning", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.7418427914381027}]}, {"text": "The evaluation criterion for such datasets is the Pearson correlation coefficient (PCC) between the predicted sentence similarities and the ground-truth sentence similarities.", "labels": [], "entities": [{"text": "Pearson correlation coefficient (PCC)", "start_pos": 50, "end_pos": 87, "type": "METRIC", "confidence": 0.9722786247730255}]}, {"text": "We split these datasets into five corpora by their genre: news, captions, wordnet, forums, tweets (for details see appendix).", "labels": [], "entities": []}, {"text": "Throughout this section, we use publicly available 300-    We use a standard continual learning experiment setup (cf. (, section 5.1)) as follows.", "labels": [], "entities": []}, {"text": "We sequentially present the five training datasets in the order 2 of news, captions, wordnet, forums, and tweets, to train sentence encoders.", "labels": [], "entities": []}, {"text": "Whenever anew training corpus is presented, we train a SIF encoder from scratch 3 (by combining all available training corpora which have been already presented) and then test it on each corpus.", "labels": [], "entities": []}, {"text": "At the same time, we incrementally adapt a CA encoder 4 using the newly presented corpus and test it on each corpus.", "labels": [], "entities": []}, {"text": "The lines of each panel of show the test results of SIF and CA on each testing corpus (specified as the panel subtitle) as a function of the number of training corpora used (the first n corpora of news, captions, wordnet, forums, and tweets for this experiment).", "labels": [], "entities": []}, {"text": "To give a concrete example, consider the blue line in the first panel of.", "labels": [], "entities": []}, {"text": "This line shows the test PCC scores (y-axis) of SIF encoder on the news corpus when the number of training corpora increases (xaxis).", "labels": [], "entities": [{"text": "news corpus", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.7801461815834045}]}, {"text": "Specifically, the left-most blue dot indicates the test result of SIF encoder on news corpus when trained on news corpus itself (that is, the first training corpus is used); the second point indicates the test results of SIF encoder on news corpus when trained on news and captions corpora (i.e., the first two training corpora are used); the third point indicates the test results of SIF encoder on news corpus when trained on news, captions, and wordnet corpora (that is, the first three training corpora are used), soon and so forth.", "labels": [], "entities": []}, {"text": "The dash-lines in panels show the results of a corpus-specialized SIF, which is trained and tested on the same corpus, i.e., as done in (Arora et al., 2017, section 4.1).", "labels": [], "entities": []}, {"text": "We see that the PCC results of CA are better and more \"forgetting-resistant\" than train-from-scratch SIF throughout the time course where more training data are incorporated.", "labels": [], "entities": []}, {"text": "Consider, for example, the test result of news corpus (first panel) again.", "labels": [], "entities": []}, {"text": "As more and more training corpora are used, the performance of train-from-scratch SIF drops with a noticeable slope; by contrast, the performance CA drops only slightly.", "labels": [], "entities": [{"text": "CA", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9753347039222717}]}, {"text": "As remarked in the section 3.2, with a simple modification of CA, we can perform zero-shot sentence representation learning without using any training corpus.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.7759222090244293}]}, {"text": "The zero-shot learning results are presented in, together with the time-course averaged results of CA and train-from-scratch SIF (i.e., the averaged values of those CA or SIF scores in each panel of).", "labels": [], "entities": [{"text": "CA", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9725304245948792}]}, {"text": "We see that the averaged results of our CA method performs the best among these three methods.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, the results yielded by zero-shot CA are better than the averaged results of train-from-scratch SIF inmost of the cases.", "labels": [], "entities": [{"text": "SIF", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.8447398543357849}]}, {"text": "We defer additional experiments to the appendix, where we compared CA against more baseline methods and use different word vectors other than GloVe to carryout the experiments.", "labels": [], "entities": []}], "tableCaptions": []}