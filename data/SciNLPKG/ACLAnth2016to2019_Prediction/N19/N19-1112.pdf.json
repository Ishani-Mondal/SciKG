{"title": [{"text": "Linguistic Knowledge and Transferability of Contextual Representations", "labels": [], "entities": [{"text": "Transferability of Contextual Representations", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.8544739931821823}]}], "abstractContent": [{"text": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and trans-ferable features of language.", "labels": [], "entities": []}, {"text": "To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9810958504676819}]}, {"text": "We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification).", "labels": [], "entities": [{"text": "conjunct identification", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.811293214559555}]}, {"text": "To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within con-textualizers, especially between recurrent neu-ral networks (RNNs) and transformers.", "labels": [], "entities": []}, {"text": "For instance , higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend.", "labels": [], "entities": []}, {"text": "In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks.", "labels": [], "entities": []}, {"text": "For any given task, pretraining on a closely related task yields better performance than language model pretrain-ing (which is better on average) when the pre-training dataset is fixed.", "labels": [], "entities": []}, {"text": "However, language model pretraining on more data gives the best results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pretrained word representations () area key component of state-of-the-art neural NLP models.", "labels": [], "entities": []}, {"text": "Traditionally, these word vectors are static-a single * Work done while at the Allen Institute for Artificial Intelligence.", "labels": [], "entities": []}, {"text": "vector is assigned to each word.", "labels": [], "entities": []}, {"text": "Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context.", "labels": [], "entities": []}, {"text": "CWRs are typically the outputs of a neural network (which we calla contextualizer) trained on tasks with large datasets, such as machine translation ( and language modeling ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8319889903068542}, {"text": "language modeling", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7164485603570938}]}, {"text": "CWRs are extraordinarily effective-using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "The broad success of CWRs indicates that they encode useful, transferable features of language.", "labels": [], "entities": []}, {"text": "However, their linguistic knowledge and transferability are not yet well understood.", "labels": [], "entities": []}, {"text": "Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax or morphology ().", "labels": [], "entities": []}, {"text": "We extend prior work by studying CWRs with a diverse set of sixteen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information, among others.", "labels": [], "entities": []}, {"text": "The result is a broader view of the linguistic knowledge encoded within With respect to transferability, pretraining contextualizers on the language modeling task has had the most empirical success, but we can also consider pretraining contextualizers with other supervised objectives and probing their linguistic knowledge.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7943051854769388}]}, {"text": "We examine how the pretraining task affects the linguistic knowledge learned, considering twelve pretraining tasks and assessing transferability to nine target tasks.", "labels": [], "entities": []}, {"text": "Better understanding the linguistic knowledge and transferability of CWRs is necessary for their principled enhancement through new encoder architectures and pretraining tasks that build upon their strengths or alleviate their weaknesses.", "labels": [], "entities": []}, {"text": "This paper asks and answers:", "labels": [], "entities": []}], "datasetContent": [{"text": "To ensure a controlled comparison of different pretraining tasks, we fix the contextualizer's architecture and pretraining dataset.", "labels": [], "entities": []}, {"text": "All of our contextualizers use the ELMo (original) architecture, and the training data from each of the pretraining tasks is taken from the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 140, "end_pos": 143, "type": "DATASET", "confidence": 0.9407284259796143}]}, {"text": "Each of the (identical) models thus seethe same tokens, but the supervision signal differs.", "labels": [], "entities": []}, {"text": "We compare to (1) a noncontextual baseline (GloVe) to assess the effect of contextualization, (2) a randomly-initialized, untrained ELMo (original) baseline to measure the effect of pretraining, and (3) the ELMo (original) model pretrained on the Billion Word Benchmark to examine the effect of training the bidirectional language model on more data.", "labels": [], "entities": []}, {"text": "the syntactic dependency arc classification (EWT) task, we seethe largest gains from pretraining on the task itself, but with a different dataset (PTB).", "labels": [], "entities": [{"text": "syntactic dependency arc classification (EWT) task", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.7591386660933495}]}, {"text": "However, pretraining on syntactic dependency arc prediction (PTB), CCG supertagging, chunking, the ancestor prediction tasks, and semantic dependency arc classification all give better performance than bidirectional language model pretraining.", "labels": [], "entities": [{"text": "syntactic dependency arc prediction (PTB)", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.7794577990259443}, {"text": "ancestor prediction tasks", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.7774500846862793}, {"text": "semantic dependency arc classification", "start_pos": 130, "end_pos": 168, "type": "TASK", "confidence": 0.6663663014769554}]}], "tableCaptions": [{"text": " Table 1: Performance of the best layerwise linear probing model for each contextualizer compared against a  GloVe-based linear probing baseline and the previous state of the art. The best contextualizer for each task is  bolded. Results for all layers on all tasks, and papers describing the prior state of the art, are given in Appendix D.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 330, "end_pos": 338, "type": "METRIC", "confidence": 0.9219783544540405}]}, {"text": " Table 2: Comparison of different probing models  trained on ELMo (original); best-performing probing  model is bolded. Results for each probing model are  from the highest-performing contextualizer layer. En- abling probing models to learn task-specific contextual  features (with LSTMs) yields outsized benefits in tasks  requiring highly specific information.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8789501786231995}]}, {"text": " Table 5: Token labeling task performance of a linear probing model trained on top of the ELMo and OpenAI  contextualizers, compared against a GloVe-based probing baseline and the previous state of the art.", "labels": [], "entities": [{"text": "Token labeling task", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9473367134730021}]}, {"text": " Table 9: Pairwise relation task performance of a linear probing model trained on top of the ELMo and OpenAI  contextualizers, compared against a GloVe-based probing baseline.", "labels": [], "entities": []}, {"text": " Table 10: Pairwise relation task performance of a linear probing model trained on top of the BERT contextualizers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.7759532332420349}]}]}