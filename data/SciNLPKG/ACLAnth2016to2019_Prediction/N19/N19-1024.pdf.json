{"title": [{"text": "Neural Finite State Transducers: Beyond Rational Relations", "labels": [], "entities": [{"text": "Neural Finite State Transducers", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8043907135725021}]}], "abstractContent": [{"text": "We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings.", "labels": [], "entities": [{"text": "neural finite state transducers (NFSTs)", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.7641450337001255}]}, {"text": "The probability of a string pair is obtained by marginalizing overall its accepting paths in a finite state transducer.", "labels": [], "entities": []}, {"text": "In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property).", "labels": [], "entities": []}, {"text": "NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016).", "labels": [], "entities": []}, {"text": "We present training and inference algorithms for locally and globally normalized variants of NFSTs.", "labels": [], "entities": []}, {"text": "In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Weighted finite state transducers (WFSTs) have been used for decades to analyze, align, and transduce strings in language and speech processing.", "labels": [], "entities": []}, {"text": "They form a family of efficient, interpretable models with wellstudied theory.", "labels": [], "entities": []}, {"text": "A WFST describes a function that maps each string pair (x, y) to a weight-often areal number representing p(x, y) or p(y | x).", "labels": [], "entities": []}, {"text": "The WFST is a labeled graph, in which each path a represents a sequence of operations that describes how some x and some y could be jointly generated, or how x could be edited into y.", "labels": [], "entities": [{"text": "WFST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9124512672424316}]}, {"text": "Multiple paths for the same (x, y) pair correspond to different analyses (labeled alignments) of that pair.", "labels": [], "entities": []}, {"text": "However, WFSTs can only model certain functions, known as the rational relations.The weight of a path is simply the product of the weights on its arcs.", "labels": [], "entities": []}, {"text": "This means  that in a random path of the form ab c, the two subpaths are conditionally independent given their common state b: a Markov property.", "labels": [], "entities": []}, {"text": "In this paper, we propose neural finite state transducers (NFSTs), in which the weight of each path is instead given by some sort of neural network, such as an RNN.", "labels": [], "entities": []}, {"text": "Thus, the weight of an arc can depend on the context in which the arc is used.", "labels": [], "entities": []}, {"text": "By abandoning the Markov property, we lose exact dynamic programming algorithms, but we gain expressivity: the neural network can capture dependencies among the operations along a path.", "labels": [], "entities": []}, {"text": "For example, the RNN might give higher weight to a path if it is \"internally consistent\": it might thus prefer to transcribe a speaker's utterance with a path that maps similar sounds in similar contexts to similar phonemes, thereby adapting to the speaker's accent.", "labels": [], "entities": []}, {"text": "Consider a finite-state transducer T as in (see Appendix A for background).", "labels": [], "entities": []}, {"text": "Using the composition operator \u2022, we can obtain anew FST, x \u2022 T , whose accepting paths correspond to the accepting paths of T that have input string x.", "labels": [], "entities": [{"text": "FST", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.5842806696891785}]}, {"text": "Similarly, the accepting paths of T \u2022 y correspond to the accepting paths of T that have output stringy.", "labels": [], "entities": []}, {"text": "Finally, x \u2022 T \u2022 y extracts the paths that have both properties.", "labels": [], "entities": []}, {"text": "We define a joint probability distribution over (x, y) pairs by marginalizing over those paths: where\u02dcpwhere\u02dc where\u02dcp(a) is the weight of path a and Z(T ) = a\u2208T\u02dcpa\u2208T\u02dc a\u2208T\u02dcp(a) is a normalization constant.", "labels": [], "entities": []}, {"text": "We define\u02dcpdefine\u02dc define\u02dcp(a) exp G \u03b8 (a) with G \u03b8 (a) being some parametric scoring function.", "labels": [], "entities": []}, {"text": "In our experiments, we will adopt a fairly simple left-to-right RNN architecture ( \u00a72.2), but one could easily substitute fancier architectures.", "labels": [], "entities": []}, {"text": "We will also consider defining G \u03b8 by a locally normalized RNN that ensures Z(T ) = 1.", "labels": [], "entities": []}, {"text": "In short, we use the finite-state transducer T to compactly define a set of possible paths a.", "labels": [], "entities": []}, {"text": "The number of paths maybe exponential in the size of T , or infinite if T is cyclic.", "labels": [], "entities": []}, {"text": "However, in contrast to WFSTs, we abandon this combinatorial structure in favor of neural nets when defining the probability distribution over a.", "labels": [], "entities": []}, {"text": "In the resulting marginal distribution p(x, y) given in equation (1), the path a that aligns x and y is a latent variable.", "labels": [], "entities": []}, {"text": "This is also true of the resulting conditional distribution p(y | x).", "labels": [], "entities": []}, {"text": "We explore training and inference algorithms for various classes of NFST models ( \u00a73).", "labels": [], "entities": []}, {"text": "Classical WFSTs ( and) use restricted scoring functions and so admit exact dynamic programming algorithms.", "labels": [], "entities": []}, {"text": "For general NFSTs, however, we must resort to approximate computation of the model's training gradient, marginal probabilities, and predictions.", "labels": [], "entities": []}, {"text": "In this paper, we will use sequential importance sampling methods (, leaving variational approximation methods to future work.", "labels": [], "entities": []}, {"text": "Defining models using FSTs has several benefits: Output-sensitive encoding Currently popular models of p(y | x) used in machine translation and morphology include seq2seq), seq2seq with attention (, the Transformer ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7580093443393707}]}, {"text": "These models first encode x as a vector or sequence of vectors, and then condition the generation of yon this encoding.", "labels": [], "entities": []}, {"text": "The vector is determined from x only.", "labels": [], "entities": []}, {"text": "This is also the casein the BiRNN-WFST (), a previous finite-state model to which we compare.", "labels": [], "entities": [{"text": "BiRNN-WFST", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.7968329787254333}]}, {"text": "By contrast, in our NFST, the state of the RNN as it reads and transduces the second half of x is influenced by the first halves of both x and y and their alignment.", "labels": [], "entities": []}, {"text": "Inductive bias Typically, a FST is constructed with domain knowledge (possibly by compiling a regular expression), so that its states reflect interpretable properties such as syllable boundaries or linguistic features.", "labels": [], "entities": [{"text": "FST", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9488993287086487}]}, {"text": "Indeed, we will show below how to make these properties explicit by \"marking\" the FST arcs.", "labels": [], "entities": [{"text": "FST", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.6129683256149292}]}, {"text": "The NFST's path scoring function then sees these marks and can learn to take them into account.", "labels": [], "entities": [{"text": "NFST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8945183753967285}]}, {"text": "The NFST also inherits any hard constraints from the FST: if the FST omits all (x, y) paths for some \"illegal\" x, y, then p(x, y) = 0 for any parameter vector \u03b8 (a \"structural zero\").", "labels": [], "entities": [{"text": "NFST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9028226137161255}, {"text": "FST", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8228728771209717}]}, {"text": "Interpretability Like a WFST, an NFST can \"explain\" why it mapped x toy in terms of a latent path a, which specifies a hard monotonic labeled alignment.", "labels": [], "entities": []}, {"text": "The posterior distribution p(a | x, y) specifies which paths a are the best explanations (e.g.,).", "labels": [], "entities": []}, {"text": "We conduct experiments on three tasks: grapheme-to-phoneme, phoneme-to-grapheme, and action-to-command ().", "labels": [], "entities": []}, {"text": "Our results on these datasets show that our best models can improve over neural seq2seq and previously proposed hard alignment models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments mainly aim to: (1) show the effectiveness of NFSTs on transduction tasks; (2) illustrate that how prior knowledge can be introduced into NFSTs and improve the performance; (3) demonstrate the interpretability of our model.", "labels": [], "entities": []}, {"text": "Throughout, we experiment on three tasks: (i) graphemeto-phoneme, (ii) phoneme-to-grapheme, and (iii) actions-to-commands.", "labels": [], "entities": []}, {"text": "We compare with competitive string transduction baseline models in these tasks.", "labels": [], "entities": []}, {"text": "We carryout experiments on three string transduction tasks: Grapheme-to-phoneme and phoneme-tographeme (G2P/P2G) refer to the transduction between words' spelling and phonemic transcription.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme", "start_pos": 60, "end_pos": 79, "type": "METRIC", "confidence": 0.9636275172233582}]}, {"text": "English has a highly irregular orthography, which necessitates the use of rich models for this task.", "labels": [], "entities": []}, {"text": "We use a portion of the standard CMUDict dataset: the Sphinx-compatible version of CMUDict (.", "labels": [], "entities": [{"text": "CMUDict dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9617052674293518}, {"text": "CMUDict", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9194936752319336}]}, {"text": "As for metrics, we choose widely used exact match accuracy and edit distance.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.6925522089004517}, {"text": "edit distance", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.834166407585144}]}, {"text": "Action-to-command (A2C) refers to the transduction between an action sequence and imperative commands.", "labels": [], "entities": [{"text": "Action-to-command (A2C)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6537221446633339}]}, {"text": "We use NACS () in our experiment.", "labels": [], "entities": []}, {"text": "As for metrics, we use exact match accuracy (EM).", "labels": [], "entities": [{"text": "exact match accuracy (EM)", "start_pos": 23, "end_pos": 48, "type": "METRIC", "confidence": 0.9162477652231852}]}, {"text": "Note that the in A2C setting, a given input can yield different outputs, e.g. I_JUMP I_WALK I_WALK corresponds to both \"jump and walk twice\" and \"walk twice after jump\".", "labels": [], "entities": [{"text": "I_JUMP I_WALK I_WALK", "start_pos": 78, "end_pos": 98, "type": "METRIC", "confidence": 0.8594344721900092}]}, {"text": "NACS is a finite set of action-command pairs; we consider a predicted command to be correct if it is in the finite set and its corresponding actions is exactly the input.", "labels": [], "entities": [{"text": "NACS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9226342439651489}]}, {"text": "We evaluate on the length setting proposed by, where we train on shorter sequences and evaluate on longer sequences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average exact match accuracy (%, higher the  better) and edit distance (lower the better) on G2P  and P2G as well as exact match accuracy on NACS.", "labels": [], "entities": [{"text": "exact match", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.8282939195632935}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6502545475959778}, {"text": "edit distance", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.9513622224330902}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.6046655774116516}, {"text": "NACS", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.9601948857307434}]}, {"text": " Table 3: Average exact match accuracy (%, higher the  better) and edit distance (lower the better) on G2P and  P2G. The effectiveness of different decoding methods.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 18, "end_pos": 38, "type": "METRIC", "confidence": 0.7684985796610514}, {"text": "edit distance", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.9569173455238342}]}, {"text": " Table 4: Average exact match accuracy (%, higher the  better) and edit distance (lower the better) on G2P and  P2G. The effectiveness of different FST designs.", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 18, "end_pos": 38, "type": "METRIC", "confidence": 0.7556043465932211}, {"text": "edit distance", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.953219085931778}, {"text": "FST", "start_pos": 148, "end_pos": 151, "type": "TASK", "confidence": 0.9348751902580261}]}, {"text": " Table 5: Most probable paths from x \u2022 T \u2022 y under the approximate posterior distribution.", "labels": [], "entities": []}]}