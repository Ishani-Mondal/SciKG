{"title": [{"text": "GumDrop at the DISRPT2019 Shared Task: A Model Stacking Approach to Discourse Unit Segmentation and Connective Detection", "labels": [], "entities": [{"text": "Discourse Unit Segmentation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6159964998563131}, {"text": "Connective Detection", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.6989643275737762}]}], "abstractContent": [{"text": "In this paper we present GumDrop, George-town University's entry at the DISRPT 2019 Shared Task on automatic discourse unit seg-mentation and connective detection.", "labels": [], "entities": [{"text": "DISRPT 2019 Shared Task", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.7370072156190872}, {"text": "connective detection", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.8171965181827545}]}, {"text": "Our approach relies on model stacking, creating a heterogeneous ensemble of classifiers, which feed into a metalearner for each final task.", "labels": [], "entities": [{"text": "model stacking", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.752898782491684}]}, {"text": "The system encompasses three trainable component stacks: one for sentence splitting, one for discourse unit segmentation and one for connective detection.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7315293252468109}, {"text": "discourse unit segmentation", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6707643270492554}, {"text": "connective detection", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.7815583050251007}]}, {"text": "The flexibility of each ensemble allows the system to generalize well to datasets of different sizes and with varying levels of homogeneity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although discourse unit segmentation and connective detection are crucial for higher level shallow and deep discourse parsing tasks, recent years have seen more progress in work on the latter tasks than on predicting underlying segments, such as Elementary Discourse Units (EDUs).", "labels": [], "entities": [{"text": "discourse unit segmentation", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.6520223716894785}, {"text": "connective detection", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8104229271411896}, {"text": "discourse parsing tasks", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.8400667508443197}]}, {"text": "As the most recent overview on parsing in the framework of Rhetorical Structure Theory (RST,) points out () \"all the parsers in our sample except predict binary trees over manually segmented EDUs\".", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9702478051185608}, {"text": "Rhetorical Structure Theory (RST", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7568507075309754}]}, {"text": "Recent discourse parsing papers (e.g.) have focused on complex discourse unit span accuracy above the level of EDUs, attachment accuracy, and relation classification accuracy.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7248404771089554}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9290130734443665}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.5110554695129395}, {"text": "relation classification", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.8623637855052948}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.520169198513031}]}, {"text": "This is due in part to the difficulty in comparing systems when the underlying segmentation is not identical (see), but also because of a relatively stable SOA accuracy of EDU segmentation as evaluated on the largest RST corpus, the English RST Discourse Treebank (RST-DT,, which already exceeded 90% accuracy in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.8794824481010437}, {"text": "EDU segmentation", "start_pos": 172, "end_pos": 188, "type": "TASK", "confidence": 0.6577931046485901}, {"text": "English RST Discourse Treebank (RST-DT", "start_pos": 233, "end_pos": 271, "type": "DATASET", "confidence": 0.7906432350476583}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9982859492301941}]}, {"text": "However, as recent work () has shown, performance on smaller or less homogeneous corpora than RST-DT, and especially in the absence of gold syntax trees (which are realistically unavailable attest time for practical applications), hovers around the mid 80s, making it problematic for full discourse parsing in practice.", "labels": [], "entities": [{"text": "full discourse parsing", "start_pos": 284, "end_pos": 306, "type": "TASK", "confidence": 0.6103340983390808}]}, {"text": "This is more critical for languages and domains in which relatively small datasets are available, making the application of generic neural models less promising.", "labels": [], "entities": []}, {"text": "The DISRPT 2019 Shared Task aims to identify spans associated with discourse relations in data from three formalisms: RST (, SDRT and PDTB ().", "labels": [], "entities": []}, {"text": "The targeted task varies actoss frameworks: Since RST and SDRT segment texts into spans covering the entire document, the corresponding task is to predict the starting point of new discourse units.", "labels": [], "entities": []}, {"text": "In the PDTB framework, the basic locus identifying explicit discourse relations is the spans of discourse connectives which need to be identified among other words.", "labels": [], "entities": []}, {"text": "In total, 15 corpora (10 from RST data, 3 from PDTB-style data, and 2 from SDRT) in 10 languages (Basque, Chinese, Dutch, English, French, German, Portuguese, Russian, Spanish, and Turkish) are used as the input data for the task.", "labels": [], "entities": []}, {"text": "The heterogeneity of the frameworks, languages and even the size of the training datasets all render the shared task challenging: training datasets range from the smallest Chinese RST corpus of 8,960 tokens to the largest English PDTB dataset of 1,061,222 tokens, and all datasets have some different guidelines.", "labels": [], "entities": []}, {"text": "In this paper, we therefore focus on creating an architecture that is not only tailored to resources like RST-DT, and takes into account the crucial importance of high accuracy sentence splitting for real-world data, generalizing well to different guidelines and datasets.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.676738366484642}]}, {"text": "Our system, called GumDrop, relies on model stacking, which has been successfully applied to a number of complex NLP problems (e.g.).", "labels": [], "entities": [{"text": "model stacking", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7478395700454712}]}, {"text": "The system uses a range of different rule-based and machine learning approaches whose predictions are all fed to a 'metalearner' or blender classifier, thus benefiting from both neural models where appropriate, and strong rule-based baselines coupled with simpler classifiers for smaller datasets.", "labels": [], "entities": []}, {"text": "A further motivation for our model stacking approach is curricular: the system was developed as a graduate seminar project in the course LING-765 (Computational Discourse Modeling), and separating work into many sub-modules allowed each contributor to work on a separate sub-project, all of which are combined in the complete system as an ensemble.", "labels": [], "entities": [{"text": "model stacking", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7344276607036591}, {"text": "Computational Discourse Modeling)", "start_pos": 147, "end_pos": 180, "type": "TASK", "confidence": 0.5633993223309517}]}, {"text": "The system was built by six graduate students and the instructor, with each student focusing on one module (notwithstanding occasional collaborations) in two phases: work on a high-accuracy ensemble sentence splitter for the automatic parsing scenario (see Section 3.2), followed by the development of a discourse unit segmenter or connective detection module (Sections 3.3 and 3.4).", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.7679160237312317}, {"text": "discourse unit segmenter or connective detection", "start_pos": 304, "end_pos": 352, "type": "TASK", "confidence": 0.6307713935772578}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: GumDrop sentence splitting performance.", "labels": [], "entities": [{"text": "GumDrop sentence splitting", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6210974951585134}]}, {"text": " Table 3: Subtree, RNN and full GumDrop discourse unit segmentation performance.", "labels": [], "entities": [{"text": "GumDrop discourse unit segmentation", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.5582656636834145}]}, {"text": " Table 4: Connective detection performance.", "labels": [], "entities": [{"text": "Connective detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9014151096343994}]}]}