{"title": [{"text": "LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization", "labels": [], "entities": [{"text": "LeafNATS", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8773499727249146}, {"text": "Neural Abstractive Text Summarization", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.6623638942837715}]}], "abstractContent": [{"text": "Neural abstractive text summarization (NATS) has received a lot of attention in the past few years from both industry and academia.", "labels": [], "entities": [{"text": "Neural abstractive text summarization (NATS)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.9084626521383014}]}, {"text": "In this paper, we introduce an open-source toolkit, namely LeafNATS, for training and evaluation of different sequence-to-sequence based models for the NATS task, and for deploying the pre-trained models to real-world applications.", "labels": [], "entities": [{"text": "NATS task", "start_pos": 152, "end_pos": 161, "type": "TASK", "confidence": 0.6141124367713928}]}, {"text": "The toolkit is modularized and ex-tensible in addition to maintaining competitive performance in the NATS task.", "labels": [], "entities": [{"text": "NATS task", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.6569547355175018}]}, {"text": "A live news blogging system has also been implemented to demonstrate how these models can aid blog/news editors by providing them suggestions of headlines and summaries of their articles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Being one of the prominent natural language generation tasks, neural abstractive text summarization (NATS) has gained a lot of popularity (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7321117321650187}, {"text": "neural abstractive text summarization", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.6511906608939171}]}, {"text": "Different from extractive text summarization (, NATS relies on modern deep learning models, particularly sequence-to-sequence (Seq2Seq) models, to generate words from a vocabulary based on the representations/features of source documents, so that it has the ability to generate high-quality summaries that are verbally innovative and can also easily incorporate external knowledge (.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6031519174575806}]}, {"text": "Many NATS models have achieved better performance in terms of the commonly used evaluation measures (such as ROUGE) score) compared to extractive text summarization approaches (.", "labels": [], "entities": [{"text": "ROUGE) score", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.9536757270495096}, {"text": "extractive text summarization", "start_pos": 135, "end_pos": 164, "type": "TASK", "confidence": 0.6033580104509989}]}, {"text": "We recently provided a comprehensive survey of the Seq2Seq models ( , including their network structures, parameter inference methods, and decoding/generation approaches, for the task of abstractive text summarization.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 187, "end_pos": 217, "type": "TASK", "confidence": 0.6583322187264761}]}, {"text": "A variety of NATS models share many common properties and some of the key techniques are widely used to produce well-formed and human-readable summaries that are inferred from source articles, such as encoder-decoder framework), word embeddings (, attention mechanism (), pointing mechanism ( and beam-search algorithm).", "labels": [], "entities": []}, {"text": "Many of these features have also found applications in other language generation tasks, such as machine translation () and dialog systems).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8239388763904572}]}, {"text": "In addition, other techniques that can also be shared across different tasks include training strategies, data pre-processing, results postprocessing and model evaluation.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.7502128183841705}]}, {"text": "Therefore, having an open-source toolbox that modularizes different network components and unifies the learning framework for each training strategy can benefit researchers in language generation from various aspects, including efficiently implementing new models and generalizing existing models to different tasks.", "labels": [], "entities": [{"text": "language generation", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.7560237050056458}]}, {"text": "In the past few years, different toolkits have been developed to achieve this goal.", "labels": [], "entities": []}, {"text": "Some of them were designed specifically fora single task, such as ParlAI () for dialog research, and some have been further extended to other tasks.", "labels": [], "entities": [{"text": "dialog research", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.9082249999046326}]}, {"text": "For example, OpenNMT () and XNMT () are primarily for neural machine translation (NMT), but have been applied to other areas.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8209547102451324}]}, {"text": "The bottom-up attention model (, which has achieved state-of-the-art performance for abstractive text summarization, is implemented in OpenNMT.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.5960407356421152}]}, {"text": "There are also several other general purpose language generation packages, such as Texar ().", "labels": [], "entities": [{"text": "general purpose language generation", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.596186950802803}, {"text": "Texar", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9554430246353149}]}, {"text": "Compared with these toolkits, LeafNATS is specifically designed for NATS research, but can also be adapted to other tasks.", "labels": [], "entities": []}, {"text": "In this toolkit, we implement an end-toend training framework that can minimize the effort in writing codes for training/evaluation procedures, so that users can focus on building models and pipelines.", "labels": [], "entities": []}, {"text": "This framework also makes it easier for the users to transfer pre-trained parameters of user-specified modules to newly built models.", "labels": [], "entities": []}, {"text": "In addition to the learning framework, we have also developed a web application, which is driven by databases, web services and NATS models, to show a demo of deploying anew NATS idea to a real-life application using LeafNATS.", "labels": [], "entities": []}, {"text": "Such an application can help front-end users (e.g., blog/news authors and editors) by providing suggestions of headlines and summaries for their articles.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 introduces the structure and design of LeafNATS learning framework.", "labels": [], "entities": [{"text": "LeafNATS learning framework", "start_pos": 97, "end_pos": 124, "type": "DATASET", "confidence": 0.8529350161552429}]}, {"text": "In Section 3, we describe the architecture of the live system demo.", "labels": [], "entities": []}, {"text": "Based on the request of the system, we propose and implement anew model using LeafNATS for headline and summary generation.", "labels": [], "entities": [{"text": "LeafNATS", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9206201434135437}, {"text": "headline and summary generation", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.6295940279960632}]}, {"text": "We conclude this paper in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Basic statistics of the datasets used.", "labels": [], "entities": []}, {"text": " Table 2: Performance of our implemented pointer- generator network on different datasets. Newsroom- S and -H represent Newsroom summary and headline  datasets, respectively.", "labels": [], "entities": [{"text": "Newsroom summary and headline  datasets", "start_pos": 120, "end_pos": 159, "type": "DATASET", "confidence": 0.7339871764183045}]}, {"text": " Table 3: Performance of our model.", "labels": [], "entities": []}]}