{"title": [{"text": "Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning", "labels": [], "entities": [{"text": "Character-Level Neural Machine Translation", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.5766825079917908}]}], "abstractContent": [{"text": "We investigate the integration of a planning mechanism into an encoder-decoder architecture with attention.", "labels": [], "entities": []}, {"text": "We develop a model that can plan ahead when it computes alignments between the source and target sequences not only fora single time-step, but for the next k timesteps as well by constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan.", "labels": [], "entities": []}, {"text": "This mechanism is inspired by strategic attentive reader and writer (STRAW) model, a recent neural architecture for planning with hierarchical reinforcement learning that can also learn higher level temporal abstractions.", "labels": [], "entities": []}, {"text": "Our proposed model is end-to-end trainable with differentiable operations.", "labels": [], "entities": []}, {"text": "We show that our model outperforms strong baselines on character-level translation task from WMT'15 with less parameters and computes alignments that are qualitatively intuitive.", "labels": [], "entities": [{"text": "character-level translation task", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.8080289165178934}, {"text": "WMT'15", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.926700234413147}]}], "introductionContent": [{"text": "Character-level neural machine translation (NMT) is an attractive research problem ( because it addresses important issues encountered in word-level NMT.", "labels": [], "entities": [{"text": "Character-level neural machine translation (NMT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7264869553702218}]}, {"text": "Word-level NMT systems can suffer from problems with rare words( or data sparsity, and the existence of compound words without explicit segmentation in certain language pairs can make learning alignments and translations more difficult.", "labels": [], "entities": []}, {"text": "Character-level neural machine translation mitigates these issues.", "labels": [], "entities": [{"text": "Character-level neural machine translation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.662790909409523}]}, {"text": "In this work we propose integrating a planning algorithm with the standard encoder-decoder architecture for character-level NMT, using planning * Equal Contribution specifically to improve the alignment between source and target sequences.", "labels": [], "entities": []}, {"text": "We cast alignment (also called attention) as a planning problem, whereas it has traditionally been treated as a search problem.", "labels": [], "entities": []}, {"text": "The model we propose creates an explicit plan of source-target alignments to use at future time-steps, based on its current observation and a summary of its past actions; it may modify this plan as needed.", "labels": [], "entities": []}, {"text": "The planning mechanism itself is inspired by the strategic attentive reader and writer (STRAW) of.", "labels": [], "entities": [{"text": "STRAW", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.8313649892807007}]}, {"text": "Our work is motivated by the intuition that, although natural language (speech and writing) is generated sequentially because of human physiological constraints, it is almost certainly not conceived word-by-word.", "labels": [], "entities": []}, {"text": "Planning, i.e., choosing some goal along with candidate macro-actions to arrive at it, is one way to induce coherence in natural language.", "labels": [], "entities": []}, {"text": "Learning to generate long coherent sequences or how to form alignments overlong source contexts is difficult for existing models.", "labels": [], "entities": []}, {"text": "In the case of machine translation, performance of encoder-decoder models with attention deteriorates as sequence length increases ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.806518018245697}]}, {"text": "This effect can be more pronounced in character-level NMT, because the length of sequences in character-level translation can be much longer than word-level translation.", "labels": [], "entities": []}, {"text": "A planning mechanism could make the decoder's search for alignments more tractable and scalable.", "labels": [], "entities": []}, {"text": "Our model is based on the well-known encoderdecoder framework for NMT.", "labels": [], "entities": []}, {"text": "Its encoder is a recurrent neural network (RNN) that reads the source (a sequence of byte pairs representing text in some language) and encodes it as a sequence of vector representations; the decoder is a second RNN that generates the target translation character-by-character in the target language.", "labels": [], "entities": []}, {"text": "The decoder uses an attention mechanism to align its internal state to vectors in the source encoding that are relevant to the current generation step (see for the original description).", "labels": [], "entities": []}, {"text": "To plan ahead explicitly rather than focusing primarily on what is relevant at the present time, our model's internal state is augmented with (i) an action plan matrix and (ii) a commitment plan vector.", "labels": [], "entities": []}, {"text": "The action plan matrix is a template of alignments that the model intends to follow at future time-steps, specifically a sequence of probability distributions over source tokens.", "labels": [], "entities": []}, {"text": "The commitment plan vector governs whether to recompute the action plan or to continue following it, and as such models discrete decisions.", "labels": [], "entities": []}, {"text": "Because of computational constraints we here apply planning only on the input sequence, via searching for alignments.", "labels": [], "entities": []}, {"text": "We find this alignment-based planning to be helpful in the translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9916867613792419}]}, {"text": "For other NLP tasks, however, planning could be applied explicitly for generation as well.", "labels": [], "entities": []}, {"text": "Recent work by on actor-critic methods for sequence prediction, for example, can be seen as this kind of generative planning.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.835138350725174}, {"text": "generative planning", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.9797435402870178}]}, {"text": "We evaluate our model and report results on character-level translation tasks from WMT'15 for English to German, English to Finnish, and English to Czech language pairs.", "labels": [], "entities": [{"text": "character-level translation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6708139777183533}, {"text": "WMT'15", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8654440641403198}]}, {"text": "On almost all pairs we observe improvements over a baseline that represents the state-of-the-art in neural character-level translation.", "labels": [], "entities": [{"text": "neural character-level translation", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.6229334870974222}]}, {"text": "In our NMT experiments, our model outperforms the baseline despite using significantly fewer parameters and converges faster in training.", "labels": [], "entities": [{"text": "NMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9038872718811035}]}], "datasetContent": [{"text": "Character-level neural machine translation (NMT) is an attractive research problem () because it addresses important issues encountered in word-level NMT.", "labels": [], "entities": [{"text": "Character-level neural machine translation (NMT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7280645753656115}]}, {"text": "Word-level NMT systems can suffer from problems with rare words ( or data sparsity, and the existence of compound words without explicit segmentation in some language pairs can make learning alignments between different languages and translations to be more difficult.", "labels": [], "entities": []}, {"text": "Characterlevel neural machine translation mitigates these issues.", "labels": [], "entities": [{"text": "Characterlevel neural machine translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7133692651987076}]}, {"text": "In our NMT experiments we use byte pair encoding (BPE) ( for the source sequence and characters at the target, the same setup described in.", "labels": [], "entities": [{"text": "byte pair encoding (BPE)", "start_pos": 30, "end_pos": 54, "type": "METRIC", "confidence": 0.5862007389465967}]}, {"text": "We also use the same preprocessing as in that work.", "labels": [], "entities": []}, {"text": "We present our experimental results in.", "labels": [], "entities": []}, {"text": "Models were tested on the WMT'15 tasks for English to German (En\u2192De), English to Czech (En\u2192Cs), and English to Finnish (En\u2192Fi) language pairs.", "labels": [], "entities": [{"text": "WMT'15 tasks", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.7068336308002472}]}, {"text": "The table shows that our planning mechanism improves translation performance over our baseline (which reproduces the results reported in () to within a small margin).", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9489732980728149}]}, {"text": "It does this with fewer updates and fewer parameters.", "labels": [], "entities": []}, {"text": "We trained (r)PAG for 350K updates on the training set, while the baseline was trained for 680K updates.", "labels": [], "entities": [{"text": "PAG", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.886182427406311}]}, {"text": "We used 600 units in (r)PAG's encoder and decoder, while the baseline used 512 in the encoder and 1024 units in the decoder.", "labels": [], "entities": [{"text": "PAG", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8774394392967224}]}, {"text": "In total our model has about 4M fewer parameters than the baseline.", "labels": [], "entities": []}, {"text": "We tested all models with abeam size of 15.", "labels": [], "entities": []}, {"text": "As can be seen from, layer normalization () improves the performance of PAG model significantly.", "labels": [], "entities": []}, {"text": "However, according to our results on En\u2192De, layer norm affects the performance of our rPAG only marginally.", "labels": [], "entities": []}, {"text": "Thus, we decided not to train rPAG with layer norm on other language pairs.", "labels": [], "entities": []}, {"text": "In, we present the results for PAG using the biscale decoder.", "labels": [], "entities": [{"text": "PAG", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.6979671716690063}]}, {"text": "In, we show qualitatively that our model constructs smoother alignments.", "labels": [], "entities": []}, {"text": "At each word that the baseline decoder generates, it aligns the first few characters to a word in the source sequence, but for the remaining characters places the largest alignment weight on the last, empty token of the source sequence.", "labels": [], "entities": []}, {"text": "This is because the baseline becomes confident of which word to generate after the first few in e in em J ah r z eh n tn u r 3 0 0 F \u00e4 ll e v on W ah lb e tr u g in d en US A . (c): We visualize the alignments learned by PAG in (a) and the biscale baseline model in (b).", "labels": [], "entities": []}, {"text": "As depicted, the alignments learned by PAG look more accurate intuitively and appear smoother than those of the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WMT'15 En\u2192De Results", "labels": [], "entities": [{"text": "WMT'15 En\u2192De Results", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.9132873296737671}]}, {"text": " Table 2: The results of different models on WMT'15 task on English to German, English to Czech and English  to Finnish language pairs. We report BLEU scores of each model computed via the multi-blue.perl script. The  best-score of each model for each language pair appears in bold-face. We use newstest2013 as our development  set, newstest2014 as our \"Test 2014\" and newstest2015 as our \"Test 2015\" set.   \u2020 denotes the results of the", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8181255459785461}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9991880059242249}]}]}