{"title": [{"text": "Deep learning for extracting protein-protein interactions from biomedical literature", "labels": [], "entities": []}], "abstractContent": [{"text": "State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information.", "labels": [], "entities": [{"text": "protein-protein interaction (PPI) extraction", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.6805398613214493}]}, {"text": "But how to incorporate such knowledge in the recent deep learning methods remains an open question.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multichannel dependency-based convolutional neu-ral network model (McDepCNN).", "labels": [], "entities": []}, {"text": "It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word.", "labels": [], "entities": []}, {"text": "Therefore, the model can use richer information obtained from different channels.", "labels": [], "entities": []}, {"text": "Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods.", "labels": [], "entities": []}, {"text": "In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on \"difficult\" instances.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9995506405830383}, {"text": "F1-score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9986341595649719}]}, {"text": "These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.", "labels": [], "entities": [{"text": "McDepCNN generalizes", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.6425939947366714}]}], "introductionContent": [{"text": "With the growing amount of biomedical information available in the textual form, there has been considerable interest in applying natural language processing (NLP) techniques and machine learning (ML) methods to the biomedical literature (.", "labels": [], "entities": []}, {"text": "One of the most important tasks is to extract protein-protein interaction relations (.", "labels": [], "entities": []}, {"text": "Protein-protein interaction (PPI) extraction is a task to identify interaction relations between protein entities mentioned within a document.", "labels": [], "entities": [{"text": "Protein-protein interaction (PPI) extraction", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7470746537049612}]}, {"text": "While PPI relations can span over sentences and even cross documents, current works mostly focus on PPI in individual sentences (.", "labels": [], "entities": []}, {"text": "For example, \"ARFTS\" and \"XIAP-BIR3\" are in a PPI relation in the sentence \"ARFTS PROT1 specifically binds to a distinct domain in XIAP-BIR3 PROT2 \".", "labels": [], "entities": []}, {"text": "Recently, deep learning methods have achieved notable results in various NLP tasks.", "labels": [], "entities": []}, {"text": "For PPI extraction, convolutional neural networks (CNN) have been adopted and applied effectively (.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9585760533809662}]}, {"text": "Compared with traditional supervised ML methods, the CNN model is more generalizable and does not require tedious feature engineering efforts.", "labels": [], "entities": [{"text": "ML", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9574654698371887}]}, {"text": "However, how to incorporate linguistic and semantic information into the CNN model remains an open question.", "labels": [], "entities": []}, {"text": "Thus previous CNN-based methods have not achieved state-ofthe-art performance in the PPI task (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multichannel dependency-based convolutional neural network, McDepCNN, to provide anew way to model the syntactic sentence structure in CNN models.", "labels": [], "entities": [{"text": "McDepCNN", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.7192100882530212}]}, {"text": "Compared with the widely-used one-hot CNN model (e.g., the shortest-path information is firstly transformed into a binary vector which is zero in all positions except at this shortest-path's index, and then applied to CNN), McDepCNN utilizes a separate channel to capture the dependencies of the sentence syntactic structure.", "labels": [], "entities": []}, {"text": "To assess McDepCNN, we evaluated our model on two benchmarking PPI corpora, AIMed () and BioInfer ().", "labels": [], "entities": [{"text": "McDepCNN", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.5393785238265991}]}, {"text": "Our results show that McDepCNN performs better than the state-of-theart feature-and kernel-based methods.", "labels": [], "entities": []}, {"text": "We further examined McDepCNN in two experimental settings: a cross-corpus evaluation and an evaluation on a subset of \"difficult\" PPI instances previously reported.", "labels": [], "entities": []}, {"text": "Our results suggest that McDepCNN is more generalizable and capable of capturing long distance information than kernel methods.", "labels": [], "entities": []}, {"text": "The rest of the manuscript is organized as follows.", "labels": [], "entities": []}, {"text": "We first present related work.", "labels": [], "entities": []}, {"text": "Then, we describe our model in Section 3, followed by an extensive evaluation and discussion in Section 4.", "labels": [], "entities": []}, {"text": "We conclude in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we used the Genia Tagger to obtain the part-of-speech, chunk tags, and named entities of each word).", "labels": [], "entities": []}, {"text": "We parsed each sentence using the Bllip parser with the biomedical model).", "labels": [], "entities": []}, {"text": "The universal dependencies were then obtained by applying the Stanford dependencies converter on the parse tree with the CCProcessed and Universal options.", "labels": [], "entities": []}, {"text": "We implemented the model using TensorFlow ().", "labels": [], "entities": []}, {"text": "All trainable variables were initialized using the Xavier algorithm.", "labels": [], "entities": []}, {"text": "We set the maximum sentence length to 160.", "labels": [], "entities": []}, {"text": "That is, longer sentences were pruned, and shorter sentences were padded with zeros.", "labels": [], "entities": []}, {"text": "We set the learning rate to be 0.0007 and the dropping probability 0.5.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9433476626873016}, {"text": "dropping probability", "start_pos": 46, "end_pos": 66, "type": "METRIC", "confidence": 0.9698303043842316}]}, {"text": "During the training, we ran 250 epochs of all the training examples.", "labels": [], "entities": []}, {"text": "For each epoch, we randomized the training examples and conducted a mini-batch training with a batch size of 128 (m = 128).", "labels": [], "entities": []}, {"text": "In this paper, we experimented with three window sizes: 3, 5 and 7, each of which has 400 filters.", "labels": [], "entities": []}, {"text": "Every filter performs convolution on the sentence matrix and generates variable-length feature maps.", "labels": [], "entities": []}, {"text": "We got the best results using the single window of size 3 (see Section 4.2)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora.  Corpus Sentences # Positives # Negatives  AIMed  1,955  1,000  4,834  BioInfer  1,100  2,534  7,132", "labels": [], "entities": [{"text": "AIMed  1,955  1,000  4,834  BioInfer  1,100  2,534  7,132", "start_pos": 80, "end_pos": 137, "type": "DATASET", "confidence": 0.7340296357870102}]}, {"text": " Table 2: Evaluation results. Performance is reported in terms of Precision, Recall, and F1-score.  AIMed  BioInfer  Method  P  R  F  P  R  F", "labels": [], "entities": [{"text": "Precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9995244741439819}, {"text": "Recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9923271536827087}, {"text": "F1-score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9995185136795044}, {"text": "AIMed  BioInfer  Method  P  R  F  P  R  F", "start_pos": 100, "end_pos": 141, "type": "METRIC", "confidence": 0.7857540912098355}]}, {"text": " Table 3: Cross-corpus results. Performance is reported in terms of Precision, Recall, and F1-score.  AIMed  BioInfer  Method  Training corpus  P  R  F  P  R  F  McDepCNN  AIMed  - - - 39.5 61.4 48.0  BioInfer  40.1 65.9 49.9  - - - Shallow linguistic (Tikk et al., 2010) AIMed  - - - 29.2 66.8 40.6  BioInfer  76.8 27.2 41.5  - - -", "labels": [], "entities": [{"text": "Precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.998561441898346}, {"text": "Recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9901779294013977}, {"text": "F1-score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9988790154457092}, {"text": "AIMed  BioInfer  Method  Training corpus  P  R  F  P  R  F  McDepCNN  AIMed", "start_pos": 102, "end_pos": 177, "type": "DATASET", "confidence": 0.7733898598414201}]}, {"text": " Table 4: Instances that are the most difficult to  classify correctly by the collection of kernels us- ing cross-validation (Tikk et al., 2013).  Corpus Positive difficult Negative difficult  AIMed  61  184  BioInfer  111  295", "labels": [], "entities": []}, {"text": " Table 5: Comparisons on the difficult instances  with CV evaluation. Performance is reported in  terms of Precision, Recall, and F1-score  *  .  Method  P  R  F  McDepCNN  14.0 22.7 17.3  All-path graph kernel  4.3  7.9  5.5  Edit kernel  4.8  5.8  5.3  Shallow linguistic  3.6  7.9  4.9", "labels": [], "entities": [{"text": "Precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9993051290512085}, {"text": "Recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.993800699710846}, {"text": "F1-score", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9994184970855713}]}, {"text": " Table 6: Contributions of different parts in McDe- pCNN. Performance is reported in terms of Preci- sion, Recall, and F1-score.  Method  P  R  F  \u2206  window = 3  67.3 60.1 63.5  window = [3,5]  60.9 62.4 61.6 (1.9)  window = [3,5,7] 61.7 61.9 61.8 (1.7)  Single channel  62.8 62.3 62.6 (1.1)", "labels": [], "entities": [{"text": "McDe- pCNN", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8647168676058451}, {"text": "Preci- sion", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9600138266881307}, {"text": "Recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9855149984359741}, {"text": "F1-score", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9992431402206421}]}]}