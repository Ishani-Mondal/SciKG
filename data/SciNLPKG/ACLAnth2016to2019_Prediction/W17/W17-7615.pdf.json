{"title": [{"text": "Error Analysis of Cross-lingual Tagging and Parsing", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8082607090473175}, {"text": "Cross-lingual Tagging", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.735239714384079}, {"text": "Parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.5558531880378723}]}], "abstractContent": [{"text": "We thoroughly analyse the performance of cross-lingual tagger and parser transfer from English into 32 languages.", "labels": [], "entities": []}, {"text": "We suggest potential remedies for identified issues and evaluate some of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this case study, we try to answer several questions one might have about the performance of crosslingual tagging and parsing.", "labels": [], "entities": [{"text": "crosslingual tagging and parsing", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.6315004676580429}]}, {"text": "We do that by extensively evaluating a state-of-the-art cross-lingual setup, with a single source language (English) and 32 target languages.", "labels": [], "entities": []}, {"text": "A researcher in cross-lingual parsing might ask what the strengths and weaknesses of the system are, which information is transferred well from the input knowledge, which information is lost in the transfer, and which information is already missing or confusing on the input -and why that probably is and how this might potentially be addressed.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7704454660415649}]}, {"text": "Furthermore, a user of the cross-lingual parsing, such as a computational linguist interested in utilising the outputs of the cross-lingual parsing in subsequent automatic processing, or a formal linguist interested in the syntax of low-resource languages, may still ask a somewhat different set of questions, such as how trustworthy the outputs of the system are, and how likely to be correct which parts of the outputs are.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.645567387342453}]}, {"text": "We try to answer questions of both of these kinds, analysing errors in cross-lingual parsing along various dimensions.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.6720902919769287}]}, {"text": "We focus on a state-of-the-art cross-lingual parsing setup, based on translating training data with a 1:1 machine translation (MT) system -this is the approach used in SFNW ( ), the winning system of the VarDial cross-lingual parsing shared task (.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6185748279094696}, {"text": "machine translation (MT)", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.7589082598686219}, {"text": "SFNW", "start_pos": 168, "end_pos": 172, "type": "DATASET", "confidence": 0.8483503460884094}, {"text": "VarDial cross-lingual parsing shared task", "start_pos": 204, "end_pos": 245, "type": "TASK", "confidence": 0.7172006249427796}]}, {"text": "We make sure our setup is realistic for the supposed low-resource scenario, by only requiring a dependency treebank fora source language (we use English) and source-target parallel data to perform the cross-lingual parser transfer; in particular, we do not assume the availability of a target language tagger (or data to train one), contrary to a lot of previous work in the field.", "labels": [], "entities": [{"text": "cross-lingual parser transfer", "start_pos": 201, "end_pos": 230, "type": "TASK", "confidence": 0.660662829875946}]}, {"text": "In practice, significantly better results can be achieved by carefully selecting one or more appropriate source languages for each target language, but this would add too much complexity to our analysis, and we thus leave this for future work.", "labels": [], "entities": []}, {"text": "Using a fixed source language makes it easier to generalise in our observations over some or all of the target languages.", "labels": [], "entities": []}, {"text": "Moreover, choosing English specifically, which we understand well both theoretically and practically, allows us to perform a more in-depth analysis than with a source language we do only have a limited knowledge of.", "labels": [], "entities": []}, {"text": "Note that we do require supervised target language treebanks to be able to perform the error analysis.", "labels": [], "entities": []}, {"text": "However, we hope that our observations can be used to provide a more general insight into the mechanisms of cross-lingual processing, driving intuitions and seeding expectations valid even for languages that we did not cover, thus facilitating a researcher to informedly choose a particular setup for this scenario, knowing what to be careful about and what to expect.", "labels": [], "entities": []}, {"text": "We hope this to be especially useful with truly under-resourced target languages, where performing an error analysis of the outputs is costly.", "labels": [], "entities": []}, {"text": "We review previous work in Section 2 and describe our setup in Section 3.", "labels": [], "entities": []}, {"text": "We then proceed with error analysis of cross-lingual tagging (Section 4) and parsing (Section 5), evaluate some of our suggested remedies in Section 6, and conclude with Section 7.", "labels": [], "entities": [{"text": "cross-lingual tagging", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.709881991147995}, {"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9673569798469543}]}], "datasetContent": [{"text": "We used the Universal Dependencies v1.4 treebanks1 (Nivre et al., 2016) -train for training and dev for evaluation -and parallel OpenSubtitles2016 data from the Opus collection2.", "labels": [], "entities": [{"text": "Universal Dependencies v1.4 treebanks1", "start_pos": 12, "end_pos": 50, "type": "DATASET", "confidence": 0.5636106953024864}, {"text": "Opus collection2", "start_pos": 161, "end_pos": 177, "type": "DATASET", "confidence": 0.9116816520690918}]}, {"text": "We used all UD 1.4 languages except for those that had no or too small parallel data (cop, cu, ga, got, grc, kk, 1http://universaldependencies.org/docsv1/index.html 2http://opus.lingfil.uu.se/ la, sa, swl, ta, ug) and those that do not use spaces to separate words (ja, zh), thus limiting ourselves to 32 target languages.3 For the analysis, we sorted and grouped the languages into three groups according to cross-lingual tagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 431, "end_pos": 439, "type": "METRIC", "confidence": 0.91973876953125}]}, {"text": "A detailed overview of the languages and datasets can be found in in the Appendix; a brief overview of the emergent language groups follows: High pt, no, it, fr, da, de, sv European languages closely related to English, from the Germanic and Romance language families, with sufficient parallel data to provide high-quality machine translation, and thus high accuracy in cross-lingual tagging and parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 323, "end_pos": 342, "type": "TASK", "confidence": 0.70463627576828}, {"text": "accuracy", "start_pos": 358, "end_pos": 366, "type": "METRIC", "confidence": 0.9983716607093811}, {"text": "cross-lingual tagging", "start_pos": 370, "end_pos": 391, "type": "TASK", "confidence": 0.6834053695201874}]}, {"text": "Med bg, ca, gl, nl, sk, cs, ru, id, el, hr, ro, pl, et, lv, sl Mostly European languages from the Indo-European family (with the exception of id and et) which are more distant from English and/or lower on parallel data, but still achieving competitive translation quality and mediocre accuracy of cross-lingual methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9977926015853882}]}, {"text": "Low fi, he, hi, uk, tr, ar, fa, vi, eu, hi Distant non-European or non-Indo-European languages (with the exception of uk, which is extremely low on parallel data), achieving very low quality of both MT and cross-lingual methods.", "labels": [], "entities": [{"text": "MT", "start_pos": 199, "end_pos": 201, "type": "TASK", "confidence": 0.9528024196624756}]}, {"text": "Implementing, fine-tuning and evaluating all of the modifications of the base approach that we suggest would clearly be beyond the scope of this work.", "labels": [], "entities": []}, {"text": "Nevertheless, we include at least a brief experimental part, evaluating the effects of several of the suggested modifications -merging a pair of UPOS labels (NOUN+PROPN, VERB+AUX, PRON+DET), merging a pair of dependency relation labels (nmod and compound),11 and allowing reordering in Moses.12 Note that these are rather preliminary results, without the usual several iterations of experimentation and evaluation.", "labels": [], "entities": [{"text": "VERB+AUX, PRON+DET)", "start_pos": 170, "end_pos": 189, "type": "METRIC", "confidence": 0.7693824991583824}, {"text": "Moses.12", "start_pos": 286, "end_pos": 294, "type": "DATASET", "confidence": 0.9433810114860535}]}, {"text": "shows the number of languages for which LAS improved when the modifications were applied, and the average improvement/deterioration in LAS for each language group.", "labels": [], "entities": [{"text": "LAS", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9370629191398621}]}, {"text": "We see that even the very noisy PROPN signal from the tagger is useful for the parser, probably because the main distinguishing feature (capitalization) is not directly available to the parser, and it thus cannot easily make the distinction itself.", "labels": [], "entities": []}, {"text": "We thus believe that other approaches are to be tried out, such as truecasing the data and/or explicitly including information about the casing into the parser input.", "labels": [], "entities": []}, {"text": "Merging the other label pairs usually behaved quite expectedly, slightly improving the results for the low and med groups, but not for the high group.", "labels": [], "entities": []}, {"text": "The results for merging of DET and PRON are rather mixed, as the language groups do not sufficiently differentiate the usage of determiners in the target language; one should be more careful when deciding whether to merge these labels or not.", "labels": [], "entities": []}, {"text": "The very frequent compound label, on the other hand, is something very specific for English, while inmost target languages it is rare or non-existent; thus, removing it helped even for many languages in the high group.", "labels": [], "entities": []}, {"text": "Surprisingly, enabling reordering in Moses led to deteriorations (often large) in LAS for all languages, except fora few of the most dissimilar ones (8/32), even though the BLEU score actually improved inmost cases.", "labels": [], "entities": [{"text": "LAS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.914934515953064}, {"text": "BLEU score", "start_pos": 173, "end_pos": 183, "type": "METRIC", "confidence": 0.9847255647182465}]}, {"text": "This clearly requires a thorough further investigation, as our previous experiments (unpublished) indicated a positive correlation between BLEU and LAS.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9987459182739258}, {"text": "LAS", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9628674387931824}]}, {"text": "Based on a quick inspection of the data, we currently hypothesise that disallowing reordering forces the MT system to produce more literal translations, which better preserve the sentence structure (POS and dependency relations).", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9451557397842407}, {"text": "POS", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.907575786113739}]}], "tableCaptions": [{"text": " Table 2: Macro-averaged LAS of the cross-lingual parsing, factored along gold-standard dependency  relations (only several most frequent shown) and language groups; also including LAS for the fully  supervised English parser.", "labels": [], "entities": [{"text": "LAS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.7826246619224548}, {"text": "cross-lingual parsing", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.5984973013401031}, {"text": "LAS", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9831279516220093}]}, {"text": " Table 3: Number of target languages for which improvement was observed and absolute improvement in  macro-averaged LAS when various modifications are applied, as compared to Base", "labels": [], "entities": []}]}