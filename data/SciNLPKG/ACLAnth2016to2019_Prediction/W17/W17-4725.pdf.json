{"title": [], "abstractContent": [{"text": "In this paper, we describe the TALP-UPC participation in the News Task for German-English and Finish-English.", "labels": [], "entities": []}, {"text": "Our primary submission implements a fully character to character neural machine translation architecture with an additional rescoring of a n-best list of hypothesis using a forced back-translation to the source sentence.", "labels": [], "entities": [{"text": "character to character neural machine translation", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.747254898150762}]}, {"text": "This model gives consistent improvements on different pairs of languages for the language direction with the lowest performance while keeping the quality in the direction with the highest performance.", "labels": [], "entities": []}, {"text": "Additional experiments are reported for multilingual character to character neural machine translation, phrase-based translation and the additional Turkish-English language pair.", "labels": [], "entities": [{"text": "multilingual character to character neural machine translation", "start_pos": 40, "end_pos": 102, "type": "TASK", "confidence": 0.5961619232382093}, {"text": "phrase-based translation", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.8030388355255127}]}], "introductionContent": [{"text": "Neural Machine Translation (MT) has been proven to reach state-of-the-art results in the last couple of years.", "labels": [], "entities": [{"text": "Neural Machine Translation (MT)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8452684978644053}]}, {"text": "The baseline encoder-decoder architecture has been improved by an attentionbased mechanism citebahdanau:2015, subword units (), character-based encoders or even with generative adversarial nets (), among many others.", "labels": [], "entities": []}, {"text": "Despite its successful beginnings, the neural MT approach still has many challenges to solve and improvements to incorporate into the system.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.748163640499115}]}, {"text": "However, since the system is computationally expensive and training models may last for several weeks, it is not feasible to conduct multiple experiments fora mid-sized laboratory.", "labels": [], "entities": []}, {"text": "For the same reason, it is also relevant to report negative results on NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.644618034362793}]}, {"text": "In this system description, we describe our participation on German-English and Finnish-English for the News Task.", "labels": [], "entities": []}, {"text": "Our system is a fully characterto-character neural MT () system with additional rescoring from the inverse direction model.", "labels": [], "entities": []}, {"text": "In parallel to our final system, we also experimented with multilingual character-tocharacter system using German, Finnish and Turkish on the source side and English on the target side.", "labels": [], "entities": []}, {"text": "Unfortunately, these last experiments did notwork.", "labels": [], "entities": []}, {"text": "All our systems are contrasted with a standard phrase-based system built with Moses ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus Statistics. Number of sentences  (S),words (W), vocabulary (V). M stands for mil- lions and K stands for thousands.", "labels": [], "entities": []}, {"text": " Table 2: Characters, vocabulary size and coverage for each language.", "labels": [], "entities": [{"text": "coverage", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9727658033370972}]}, {"text": " Table 3: BLEU results. In bold, best results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9952132701873779}]}]}