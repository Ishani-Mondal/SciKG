{"title": [{"text": "Learning how to learn: an adaptive dialogue agent for incrementally learning visually grounded word meanings", "labels": [], "entities": [{"text": "learning visually grounded word meanings", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.6452856063842773}]}], "abstractContent": [{"text": "We present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor, trained on real human-human tutoring data.", "labels": [], "entities": [{"text": "interactive learning of visually grounded word meanings", "start_pos": 55, "end_pos": 110, "type": "TASK", "confidence": 0.6336335071495601}]}, {"text": "Within a lifelong interactive learning period, the agent, trained using Reinforcement Learning (RL), must be able to handle natural conversations with human users, and achieve good learning performance (i.e. accuracy) while min-imising human effort in the learning process.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.996380627155304}]}, {"text": "We train and evaluate this system in interaction with a simulated human tutor, which is built on the BURCHAK corpus-a Human-Human Dialogue dataset for the visual learning task.", "labels": [], "entities": [{"text": "BURCHAK corpus-a Human-Human Dialogue dataset", "start_pos": 101, "end_pos": 146, "type": "DATASET", "confidence": 0.9393229365348816}]}, {"text": "The results show that: 1) The learned policy can coherently interact with the simulated user to achieve the goal of the task (i.e. learning visual attributes of objects, e.g. colour and shape); and 2) it finds a better trade-off between classifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9515771865844727}]}], "introductionContent": [{"text": "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings.", "labels": [], "entities": []}, {"text": "Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem.", "labels": [], "entities": []}, {"text": "The grounding problem can be categorised into two distinct, but interdependent types of problem: 1) agent as a second-language learner: the  agent needs to learn to ground (map) NL symbols onto their existing perceptual and lexical knowledge (e.g. a dictionary of pre-trained classifiers) as in e.g.;;;; and 2) the agent as a child: without any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these).", "labels": [], "entities": []}, {"text": "Here, we concentrate on the latter scenario, where a system learns to identify and describe visual attributes (colour and shape in this case) through interaction with human tutors, incrementally, overtime.", "labels": [], "entities": []}, {"text": "Previous work has approached the grounding problem using a variety of resources and approaches, for instance, either using annotated visual datasets, or through interactions with other agents or real humans (, where feedback from other agents is used to learn new concepts.", "labels": [], "entities": []}, {"text": "However, most of these systems, which ground NL symbols through interaction have two common, important drawbacks: 1) in order to achieve better performance (i.e. high accuracy), these systems require a high level of human involvementthey always request feedback from human users, which might affect the quality of human answers and decrease the overall user experience in a lifelong learning task; 2) Most of these approaches are not built/trained based on real human-human conversations, and therefore can't handle them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9952625036239624}]}, {"text": "Natural human dialogue is generally more messy than either machine-machine or human-machine dialogue, containing natural dialogue phenomena that are notoriously difficult to capture, e.g. self-corrections, repetitions and restarts, pauses, fillers, interruptions, and continuations.", "labels": [], "entities": []}, {"text": "Furthermore, they often exhibit much more variation than in their synthetic counterparts (see dialogue examples in).", "labels": [], "entities": []}, {"text": "In order to cope with the first problem, recent prior work (,c) has built multimodal dialogue systems to investigate the effects of different dialogue strategies and capabilities on the overall learning performance.", "labels": [], "entities": []}, {"text": "Their results have shown that, in order to achieve a good tradeoff between learning performance and human involvement, the agent must be able to take initiative in dialogues, take into account uncertainty of its predictions, as well as cope with natural human conversation in the learning process.", "labels": [], "entities": []}, {"text": "However, their systems are built based on hand-crafted, synthetic dialogue examples rather than real humanhuman dialogues.", "labels": [], "entities": []}, {"text": "In this paper, we extend this work to introduce an adaptive visual-attribute learning agent trained using Reinforcement Learning (RL).", "labels": [], "entities": []}, {"text": "The agent, trained with a multi-objective policy, is capable not only of properly learning novel visual objects/attributes through interaction with human tutors, but also of efficiently minimising human involvement in the learning process.", "labels": [], "entities": []}, {"text": "It can achieve equivalent/comparable learning performance (i.e. accuracy) to a fully-supervised system, but with less tutoring effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9977667331695557}]}, {"text": "The dialogue control policy is trained on the BURCHAK Human-Human Dialogue dataset (, consisting of conversations between a human 'tutor' and a human 'learner' on a visual attribute learning task.", "labels": [], "entities": [{"text": "BURCHAK Human-Human Dialogue dataset", "start_pos": 46, "end_pos": 82, "type": "DATASET", "confidence": 0.7564155757427216}]}, {"text": "The dataset includes a wide range of natural, incremental dialogue phenomena (such as overlapping turns, self-correction, repetition, fillers, and continuations), as well as considerable variation in the dialogue strategies used by the tutors and the learners.", "labels": [], "entities": []}, {"text": "Here we compare the new optimised learning agent to rule-based agents with and without adaptive confidence thresholds (see section 3.2.1).", "labels": [], "entities": []}, {"text": "The results show that the RL-based learning agent outperforms the rule-based systems by finding a better trade-off between learning performance and the tutoring effort/cost.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare the optimised and the rule-based learning agents, and also further investigate how the adaptive threshold affect the learning process, we follows the evaluate metrics from the previous work (see ()) considering both the cost to the tutor and the accuracy of the learned meanings, i.e. the classifiers that ground our colour and shape concepts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 257, "end_pos": 265, "type": "METRIC", "confidence": 0.9975455403327942}]}, {"text": "Cost The cost measure reflects the effort needed by a human tutor in interacting with the system.", "labels": [], "entities": []}, {"text": "point out that a comprehensive teachable system should learn as autonomously as possible, rather than involving the human tutor too frequently.", "labels": [], "entities": []}, {"text": "We associate a higher cost (i.e. 5) with correction of statements than that of polar questions.", "labels": [], "entities": []}, {"text": "This is to penalise the learning agent when it confidently makes a false statement -thereby incorporating an aspect of trust in the metric (humans will not trust systems which confidently make false statements).", "labels": [], "entities": []}, {"text": "i.e. differently to the previous evaluation metrics, we do not take into account the costs of parsing and producing utterances Learning Performance As mentioned above, an efficient learner dialogue policy should consider both classification accuracy and tutor effort (Cost).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9516981244087219}, {"text": "tutor effort (Cost)", "start_pos": 254, "end_pos": 273, "type": "METRIC", "confidence": 0.8702794313430786}]}, {"text": "We thus define an integrated measurethe Overall Performance Ratio (R perf ) -that we use to compare the learner's overall performance across the different conditions: i.e. the increase inaccuracy per unit of the cost, or equivalently the gradient of the curve in.", "labels": [], "entities": [{"text": "Overall Performance Ratio (R perf )", "start_pos": 40, "end_pos": 75, "type": "METRIC", "confidence": 0.8150186283247811}]}, {"text": "We seek dialogue strategies that maximise this.", "labels": [], "entities": []}], "tableCaptions": []}