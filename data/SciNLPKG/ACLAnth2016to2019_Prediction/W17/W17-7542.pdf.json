{"title": [{"text": "A Modified Cosine-Similarity based log Kernel for Support Vector Machines in the Domain of Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7008086293935776}]}], "abstractContent": [{"text": "The popularity of the internet is increasing day-by-day, which makes tough for the end-user to get desired pages from the web in a short time.", "labels": [], "entities": []}, {"text": "Text classification, a branch of machine learning can shed light on this problem.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8346303105354309}]}, {"text": "State-of-the-art classi-fier like Support Vector Machines (SVM) has become very popular in the domain of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7476744949817657}]}, {"text": "This paper studies the effect of SVM using different kernel methods and proposes a modified cosine distance based log kernel (log cosine) for text classification which is proved as Conditional Positive Definite (CPD).", "labels": [], "entities": [{"text": "text classification", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7937991321086884}]}, {"text": "Its classification performance is compared with other CPDs and Positive Definite (PD) kernels.", "labels": [], "entities": []}, {"text": "A novel feature selection technique is proposed which improves the effectiveness of the classification and gathers the crux of the terms in the corpus without deteriorating the outcome in the construction process.", "labels": [], "entities": []}, {"text": "From the experimental results, it is observed that CPD kernels provide better results for text classification when used with SVMs compared to PD kernels, and the performance of the proposed log-cosine is better than the existing kernel methods.", "labels": [], "entities": [{"text": "text classification", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7828682065010071}]}], "introductionContent": [{"text": "Text classification plays a vital role in the domain of machine learning where the text data is categorized into different groups of similar data items.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8321093618869781}]}, {"text": "Many times the present search engine retrieves invalid links and irrelevant web pages fora submitted user query.", "labels": [], "entities": []}, {"text": "This weakens the trust of the user on the search engine and thereby degrade its performance.", "labels": [], "entities": []}, {"text": "Text classification, a powerful machine learning technique which categorizes an unseen document into its respective predefined class can help in this direction.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.820904016494751}]}, {"text": "Two basic classifications of web pages are there: subject-based and genre-based (.", "labels": [], "entities": []}, {"text": "In subject-based classification, web pages are classified based on their subject or content.", "labels": [], "entities": [{"text": "subject-based classification", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6883812546730042}]}, {"text": "Topic hierarchies of web pages are built by this approach.", "labels": [], "entities": []}, {"text": "Web pages in genre-based classification are classified into genre or functional related factors, for example, some web pages genres are \"multimedia\", \"home page\", \"online transaction\", and \"news headlines\".", "labels": [], "entities": []}, {"text": "This classification helps users to find their immediate interest from the web without waiting fora longtime.", "labels": [], "entities": []}, {"text": "There are many classification techniques that exist in real and can be divided into two broad categories: eager learner and lazy learner.", "labels": [], "entities": []}, {"text": "According to eager learner classification technique, the learner built a classification model when the training dataset is given before it receives the test dataset.", "labels": [], "entities": [{"text": "eager learner classification", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6574751933415731}]}, {"text": "It can bethought as if the learning model is ready and eager to classify the new test dataset.", "labels": [], "entities": []}, {"text": "Examples of this are decision tree, Bayesian network, support vector machine, rule and association based classifier etc.", "labels": [], "entities": []}, {"text": "But in lazy learner classification technique, the things are different.", "labels": [], "entities": [{"text": "lazy learner classification", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.604981263478597}]}, {"text": "Here, instead of building a classification model, it simply stores the training dataset, hence consumes extra space and after seeing the test dataset, it does the classification based on the similarity to the stored training dataset.", "labels": [], "entities": []}, {"text": "Examples include k-nearest neighbors (k-NN) and Cased-based reasoning.", "labels": [], "entities": []}, {"text": "Content and Context of the web page play major role during the classification process.", "labels": [], "entities": []}, {"text": "The sole content of the page including HTML tags, images, text, videos help for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.9762469530105591}]}, {"text": "Similarly, the hyperlink present in a web pagealso decides the page classification.", "labels": [], "entities": [{"text": "page classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7231077253818512}]}, {"text": "Binary and multi-class are the two basic types of classification exist for classifying the text documents.", "labels": [], "entities": []}, {"text": "Binary classification generally categorizes the documents into one of two pre-defined classes whereas multi-class classification handles more than two classes.", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8720426857471466}]}, {"text": "Classification again can be either single-label or multi-label which is decided depending on the number of labels that is going to be assigned to a document.", "labels": [], "entities": []}, {"text": "Exactly one class label is to be assigned to a document in single-label whereas more than one class label is assigned to a document in multi-label classification.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 135, "end_pos": 161, "type": "TASK", "confidence": 0.6855334043502808}]}, {"text": "For instance, three-class classification means the classification problem consists of three classes say 'Business', 'Sports' and 'Movies'.", "labels": [], "entities": [{"text": "three-class classification", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6866603493690491}]}, {"text": "Many research works has done in the field of web document classification.", "labels": [], "entities": [{"text": "web document classification", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6303847034772238}]}, {"text": "Feature selection plays a major role in text classification because selection of important features not only reduces the training time, but also increases the performance of the classifier by reducing the irrelevant features from the corpus.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7677726447582245}, {"text": "text classification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8576687872409821}]}, {"text": "Further, the algorithms used for feature selection are classified into the following three categories: i.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7795914709568024}]}, {"text": "Filter methods do not use any classifiers for feature selection instead features are selected on the basis of statistical properties.", "labels": [], "entities": []}, {"text": "Hence, these methods are fast to compute and capture the usefulness of the feature set which makes them more practical. ii.", "labels": [], "entities": []}, {"text": "Wrapper methods generate different subsets of features based on some algorithms and test each subset using a classifier.", "labels": [], "entities": []}, {"text": "To find the score of feature subsets, wrapper methods use a predictive model, whereas filter methods use a proxy measure. iii.", "labels": [], "entities": []}, {"text": "Embedded methods () combines the advantages of both the above two methods and their computational complexity lies between these two methods.", "labels": [], "entities": []}, {"text": "To make the classification process more efficient, a good classifier is required.", "labels": [], "entities": [{"text": "classification", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.970740795135498}]}, {"text": "From the research, it has been observed that the usage of SVM) in text classification has been largely accurate.", "labels": [], "entities": [{"text": "text classification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8368982374668121}]}, {"text": "Many research works on text classification using SVM kernel has been done in the past () iii.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8165010809898376}]}, {"text": "Sparse Document Vectors: It has been shown that SVMs are well suited for classification problems with dense concepts and sparse instances.", "labels": [], "entities": [{"text": "Sparse Document Vectors", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7254350384076437}]}, {"text": "In this paper, we studied different existing kernels such as RBF, Linear, Polynomial etc. and compare the classification accuracy of SVMs using those kernels techniques.) in their work have shown that using SVMs, Conditionally Positive Definite (CPD) kernels provide more accurateresults than Positive Definite (PD) kernels while classifying images.", "labels": [], "entities": []}, {"text": "Knowing that SVMs perform well in text classification, here we aim to show that the performance of SVMs using CPD kernels in classifying text document is better than using normal PD kernels.", "labels": [], "entities": [{"text": "text classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7780469954013824}]}, {"text": "We propose anew kernel based on cosine distances (log cosine), and shown that it is indeed CPD.", "labels": [], "entities": []}, {"text": "A novel feature selection technique is proposed in order to reduce the size of the training feature vector which in turn enhances the performance of the classification process.", "labels": [], "entities": []}, {"text": "Experimental results on different benchmark datasets show that the usage of log cosine as a kernel in SVM for text classification is better than the existing kernel methods.", "labels": [], "entities": [{"text": "text classification", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8061857521533966}]}, {"text": "The rest of the paper is organized on the following lines: In Section 2, we have discussed the definitions and background details of the proposed approach.", "labels": [], "entities": []}, {"text": "Section 3 discusses the proposed approach followed by the experimental analysis discussed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we concluded the work with some future enhancement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Four benchmark datasets are used for experimental work (DMOZ 2 , 20-Newsgroups 3 , Reuters, Classic3 and WebKB 5 ).", "labels": [], "entities": [{"text": "Reuters", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9494066834449768}, {"text": "Classic3", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8360950946807861}]}, {"text": "The details of these datasets are discussed below:  The following parameters are used to measure the performance of the classifier. i. Accuracy (acc) is the ratio between the sum of true positive cases, TP (number of documents that are that are classified correctly) and true negative cases, TN (number of documents that are not classified correctly and are not retrieved by the approach) with the total number of documents, N = TP + FP + TN + FN.", "labels": [], "entities": [{"text": "Accuracy (acc)", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.7650425061583519}, {"text": "FP", "start_pos": 434, "end_pos": 436, "type": "METRIC", "confidence": 0.7772471904754639}, {"text": "FN", "start_pos": 444, "end_pos": 446, "type": "METRIC", "confidence": 0.5856374502182007}]}, {"text": "It can be represented as follows: where, FP: number of documents that are not classified correctly and are retrieved by the approach and FN: number of documents that are classified correctly and are not retrieved by the approach. ii.", "labels": [], "entities": [{"text": "FP", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9928227663040161}, {"text": "FN", "start_pos": 137, "end_pos": 139, "type": "METRIC", "confidence": 0.9698497653007507}]}, {"text": "Precision (pr) is the fraction of the retrieved documents by the classifier that are relevant. iii.", "labels": [], "entities": [{"text": "Precision (pr)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9549039900302887}]}, {"text": "Recall(re) is the fraction of the relevant documents that are retrieved by the classifiers. is the harmonic mean of pr and re.", "labels": [], "entities": [{"text": "Recall(re)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9418958276510239}]}, {"text": "F = 2 * pr * re pr + re", "labels": [], "entities": [{"text": "F", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9606090784072876}]}], "tableCaptions": [{"text": " Table 4: SVM Classification on 20-NG dataset", "labels": [], "entities": [{"text": "SVM Classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.740682989358902}, {"text": "20-NG dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.707788422703743}]}, {"text": " Table 5: SVM Classification on DMOZ dataset", "labels": [], "entities": [{"text": "SVM Classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.824913889169693}, {"text": "DMOZ dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9287084937095642}]}, {"text": " Table 6: SVM Classification on Reuters dataset", "labels": [], "entities": [{"text": "SVM Classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7095476388931274}, {"text": "Reuters dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8733214735984802}]}, {"text": " Table 7: SVM Classification on Classic3 dataset", "labels": [], "entities": [{"text": "SVM Classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5560280382633209}, {"text": "Classic3 dataset", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9597703516483307}]}, {"text": " Table 8: SVM Classification on WebKB dataset", "labels": [], "entities": [{"text": "SVM Classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8714879155158997}, {"text": "WebKB dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9765872359275818}]}]}