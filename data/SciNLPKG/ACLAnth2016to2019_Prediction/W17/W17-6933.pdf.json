{"title": [{"text": "There's no 'Count or Predict' but task-based selection for distributional models", "labels": [], "entities": [{"text": "Count", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9963536262512207}, {"text": "Predict", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.7348089218139648}]}], "abstractContent": [{"text": "In this paper, we investigate the differences between prediction-based (word2vec), dense count-based (GloVe) and sparse count-based (JoBimText) semantic models.", "labels": [], "entities": []}, {"text": "We evaluate the models, which were selected because they can all be computed efficiently on large data, based on word similarity tasks and a semantic ranking task both for verbs and nouns.", "labels": [], "entities": []}, {"text": "We demonstrate that prediction-based models yield higher scores than the other two models at determining a similarity score between two words.", "labels": [], "entities": []}, {"text": "To the contrary, sparse count-based methods perform best in the ranking task.", "labels": [], "entities": []}, {"text": "Further, sparse count-based methods benefit more from linguistically informed contexts, such as dependency relations.", "labels": [], "entities": []}, {"text": "In summary, we highlight differences of popular distributional semantic representations and derive recommendations for their usage.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the steady growth of textual data, NLP methods are required that are able to process the data efficiently.", "labels": [], "entities": []}, {"text": "In this paper, we focus on efficient methods that are targeted to compute distributional models that are based on the distributional hypothesis of.", "labels": [], "entities": []}, {"text": "This hypothesis claims that words occurring in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "In order to implement this hypothesis, early approaches) represented words using count-based vectors of the context.", "labels": [], "entities": []}, {"text": "However, such representations are very sparse, require a lot of memory and are not very efficient.", "labels": [], "entities": []}, {"text": "In the last decades, methods have been developed that transform such sparse representations into dense representations mainly using matrix factorization.", "labels": [], "entities": []}, {"text": "With, an efficient prediction-based method was introduced, which also represents words with a dense vector.", "labels": [], "entities": []}, {"text": "However, also sparse and count-based methods have been proposed that allow an efficient computation, e.g. (. A more detailed overview of semantic representations can be found in (.", "labels": [], "entities": []}, {"text": "In this work, we explore different aspects between three different methods for computing similarities: similarity computations that use sparse symbolic vectors for similarity computations, dense vector based methods that are based on co-occurrences and prediction-based methods.", "labels": [], "entities": []}, {"text": "For this, we aim to focus on efficiently computable methods and selected SKIP and CBOW from word2vec, GloVe and JoBimText.", "labels": [], "entities": []}, {"text": "Based on these methods we want to explore different aspects: 1) which method performs the best global similarity scoring using word pair similarity datasets 2) which method performs the best local ranking of most similar terms fora query term 3) which context works best for the different methods and 4) are there differences in the performance when evaluating on verbs and nouns.", "labels": [], "entities": []}, {"text": "embeddings outperform sparse count-based methods and dense count-based methods used for computing distributional semantic models.", "labels": [], "entities": []}, {"text": "The evaluation is performed on datasets for relatedness, analogy, concept categorization and selectional preferences.", "labels": [], "entities": []}, {"text": "The majority of word pairs considered for the evaluation consists of noun pairs.", "labels": [], "entities": []}, {"text": "However, showed that dense count-based methods, using PPMI weighted co-occurrences and SVD, approximates neural word embeddings.", "labels": [], "entities": []}, {"text": "showed in an extensive study the impact of various parameters and show the best performing parameters for these methods.", "labels": [], "entities": []}, {"text": "The study reports results for various datasets for word similarity and analogy.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.73515585064888}]}, {"text": "However, they do not evaluate the performance on local similarity ranking tasks and omit results for pure count-based semantic methods.", "labels": [], "entities": []}, {"text": "performed another comparison of various semantic representation using both intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "They compare the performance of their count-based method to dense representations and prediction-based methods using a manually crafted lexicon, SimLex and an information retrieval task.", "labels": [], "entities": []}, {"text": "They show that their method performs better on the manually crafted lexicon than using word2vec.", "labels": [], "entities": []}, {"text": "For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (;.", "labels": [], "entities": []}, {"text": "Based on the SimLex task and the extrinsic evaluation they show comparable performance to the word2vec model computed on a larger corpus.", "labels": [], "entities": []}, {"text": "In this work, we do not focus on the best performing systems for each dataset, like e.g. retrofitting embeddings (, but want to carve out the difference of existing methods for computing distributional similarities.", "labels": [], "entities": []}], "datasetContent": [{"text": "For performing the studies, we rely on two different evaluation methods.", "labels": [], "entities": []}, {"text": "First, we show results based on datasets that contain averaged similarity scores for word pairs annotated by humans.", "labels": [], "entities": []}, {"text": "We use SimLex-999 (, which consists of 999 word pairs, formed by 666 noun, 222 verb and 111 adjective pairs and the SimVerb-3500 dataset) which comprises of 3500 verb pairs.", "labels": [], "entities": [{"text": "SimVerb-3500 dataset", "start_pos": 116, "end_pos": 136, "type": "DATASET", "confidence": 0.8752570152282715}]}, {"text": "The evaluation scores are computed using the Spearman rank correlation coefficient between the gold standard scores and the similarity scores obtained with the semantic methods.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 45, "end_pos": 82, "type": "METRIC", "confidence": 0.6910865530371666}]}, {"text": "These evaluations validate the ability of semantic methods to provide similarity scores that demonstrate the performance fora global ranking between word pairs scores.", "labels": [], "entities": []}, {"text": "We name this task as a 'global ranking task' as the semantic models have to provide a score between two given word pairs and the evaluation score is computed by the correlation between similarity scores given by the model and averaged similarity scores given by humans.", "labels": [], "entities": []}, {"text": "Ina so-called local ranking task, we will evaluate how well semantic models can retrieve the most similar words fora given term.", "labels": [], "entities": []}, {"text": "For this, we sample 1000 low-, middle-and high frequent nouns and verbs.", "labels": [], "entities": []}, {"text": "In order to compute the semantic similarities between the most similar terms, we use the WordNet Path measure) and perform an evaluation that is similar to the one used by.", "labels": [], "entities": [{"text": "WordNet Path measure", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.7422421375910441}]}, {"text": "This Path measure is the shortest reciprocal distance + 1 between two words based on the IS-A path.", "labels": [], "entities": []}, {"text": "The computation of the various models is performed using a dump of English Wikipedia that comprises of 35 million sentences.", "labels": [], "entities": []}, {"text": "The similarities are computed on raw tokenized text, then on lemmatized and POS-tagged tagged text and finally using dependency parses 6 as context representation, which has been shown to work well for computing similarities.", "labels": [], "entities": []}, {"text": "Whereas the tokens and lemmas can be processed with all methods, the dependency parses can only be used with a modification of word2vec () and JBT.", "labels": [], "entities": [{"text": "JBT", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.9253994226455688}]}, {"text": "In this section, we show the Spearman correlations for the different models using SimLex and SimVerb 7 . First, we perform the computation of the models on raw text (see).", "labels": [], "entities": []}, {"text": "Using various parameters for both word2vec models 8 , we observe the best results for the SimLex dataset when computing both SKIP and CBOW with 500 dimensions, using random sampling (s = 1E \u22125 ), 10 negative examples and a word window size of 1 (W1).", "labels": [], "entities": [{"text": "SimLex dataset", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.8847571611404419}]}, {"text": "This is inline with, who mostly obtain the highest scores for word similarity tasks when using a comparably high number of dimensions.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.8006065487861633}]}, {"text": "For GloVe we obtain the best results with the same parameters as for word2vec: we use a window size of 1 and 500 dimensions.", "labels": [], "entities": []}, {"text": "The CBOW model performs best on the SimVerb dataset but does not yield the best scores for the verbs in SimLex.", "labels": [], "entities": [{"text": "SimVerb dataset", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9195074141025543}]}, {"text": "However, we could not detect much differences between the two sets, as we observe a correlation of 0.9177 for 90 verb pairs that are shared in both datasets.", "labels": [], "entities": []}, {"text": "GloVe performs best on  adjectives and verbs for the SimLex dataset, but cannot reach the highest scores on the SimVerb dataset.", "labels": [], "entities": [{"text": "SimLex dataset", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9405969679355621}, {"text": "SimVerb dataset", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9462049603462219}]}, {"text": "Although, JBT is not optimized for global similarity scoring, as it does not compute normalized similarity scores between two terms, the correlation scores are are highest for the SimLex's nouns.", "labels": [], "entities": [{"text": "JBT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8235111236572266}, {"text": "global similarity scoring", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7439901431401571}]}, {"text": "In contrast to, computing JBT by using the frequency (JBT freq log and JBT freq one) for ranking relevant contexts does not yield the best performance.", "labels": [], "entities": []}, {"text": "Here the highest scores are achieved using LMI with a logarithmic scoring, which confirms the findings by.", "labels": [], "entities": [{"text": "LMI", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.8119933009147644}]}, {"text": "As the selected parameters also performed best for the lemmatized and dependency-parsed data, we restrict the presentation of results to this setting in the remainder.", "labels": [], "entities": []}, {"text": "Inspecting the correlation scores on the lemmatization-based models equipped with POS-tags we observe a similar trend.", "labels": [], "entities": [{"text": "correlation", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9814028739929199}]}, {"text": "In general, we examine higher scores than with raw text.", "labels": [], "entities": []}, {"text": "For the entire SimLex and SimVerb dataset we again observe the best performance with the prediction-based models.", "labels": [], "entities": [{"text": "SimVerb dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9246605038642883}]}, {"text": "In contrast to the previous evaluations, the scores from the JBT LMI log are closer to the highest correlation scores of CBOW.", "labels": [], "entities": [{"text": "JBT LMI log", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9163928429285685}, {"text": "CBOW", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.8662176132202148}]}, {"text": "Again the best scores for verbs and adjectives are retrieved using GloVe.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.7685167193412781}]}, {"text": "Using dependency parses as context, we spot the best performance with JBT LMI log.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7769508063793182}, {"text": "JBT LMI log", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.7738799850145975}]}, {"text": "For the SimVerb dataset, we get even higher results than using the best performing CBOW model using lemmas.", "labels": [], "entities": [{"text": "SimVerb dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9017736315727234}]}, {"text": "Using the dependency-based SKIP model performs well for the SimLex verbs, but apart from that cannot even outperform the word2vec models computed on raw text.", "labels": [], "entities": []}, {"text": "In this section, we use the WordNet-based evaluation in order to show the performance of the methods based on a local similarity ranking.", "labels": [], "entities": []}, {"text": "Here, we focus on the methods with its best performing parameters and show results for lemma and POS-based models and dependency-based models.", "labels": [], "entities": []}, {"text": "shows results for nouns and verbs for different frequent bands for the top N = {1, 5, 10, 50, 100} highest ranked words.", "labels": [], "entities": []}, {"text": "For low-and mid-frequent nouns the best scores up to the top 10 most similar nouns are achieved with the SKIP model.", "labels": [], "entities": []}, {"text": "Beyond considering more than the 10 most similar terms the JBT model performs best.", "labels": [], "entities": [{"text": "JBT", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7023966908454895}]}, {"text": "Whereas, up to the 50 most similar nouns, the performance of the different models is comparable, we observe performance drops for the top 100 ranked words for GloVe and SKIP in comparison to JBT.", "labels": [], "entities": [{"text": "JBT", "start_pos": 191, "end_pos": 194, "type": "DATASET", "confidence": 0.9595209360122681}]}, {"text": "Considering the high frequent nouns the best performance is always obtained with the JBT model.", "labels": [], "entities": [{"text": "JBT", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.8200682997703552}]}, {"text": "For verbs Glove achieves the highest scores for when using the top 1 to 5 most similar terms for high frequent verbs.", "labels": [], "entities": [{"text": "Glove", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8294568061828613}]}, {"text": "However, similar to the results based on nouns the best performance for the 10, 50 and 100 most similar terms ist gained using the JBT model.", "labels": [], "entities": [{"text": "JBT", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8844588994979858}]}, {"text": "Using dependency parses as context, we obtain the overall highest scores using JBT (see).", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7828224003314972}, {"text": "JBT", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7901361584663391}]}, {"text": "Again, the modified SKIP model cannot compete with the count-based method and performs even inferior to the lemma and POS-tag based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation with SimLex and SimVerb for models computed on tokenized text.", "labels": [], "entities": []}, {"text": " Table 2: Results of the lemma-based models for the WordNet-based evaluation showing results for the  top N most similar words.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9185598492622375}]}, {"text": " Table 3: WordNet Path scores for semantic models that use dependency parses as context", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7236088216304779}]}, {"text": " Table 4: Most similar words for the noun \"access\".", "labels": [], "entities": []}, {"text": " Table 5: Percentage of the top N most similar terms for nouns that keep the same POS-tag", "labels": [], "entities": []}]}