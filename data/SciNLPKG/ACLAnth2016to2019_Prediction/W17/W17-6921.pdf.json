{"title": [{"text": "The Pragmatics of Indirect Commands in Collaborative Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "Today's artificial assistants are typically prompted to perform tasks through direct, imperative commands such as Set a timer or Pick up the box.", "labels": [], "entities": []}, {"text": "However, to progress toward more natural exchanges between humans and these assistants, it is important to understand the way non-imperative utterances can indirectly elicit action of an addressee.", "labels": [], "entities": []}, {"text": "In this paper, we investigate command types in the setting of a grounded, collaborative game.", "labels": [], "entities": []}, {"text": "We focus on a less understood family of utterances for eliciting agent action, locatives like The chair is in the other room, and demonstrate how these utterances indirectly command in specific game state contexts.", "labels": [], "entities": []}, {"text": "Our work shows that models with domain-specific grounding can effectively realize the pragmatic reasoning that is necessary for more robust natural language interaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major goal of computational linguistics research is to enable organic, language-mediated interaction between humans and artificial agents.", "labels": [], "entities": []}, {"text": "Ina common scenario of such interaction, a human issues a command in the imperative mood-e.g. Put that there or Pick up the box-and a robot acts in turn.", "labels": [], "entities": []}, {"text": "While this utterance-action paradigm presents its own set of challenges (, it greatly simplifies the diversity of ways in which natural language can be used to elicit action of an agent, be it human or artificial.", "labels": [], "entities": []}, {"text": "Most clause types, even vanilla declaratives, instantiate as performative requests in certain contexts.", "labels": [], "entities": []}, {"text": "In this work, we employ machine learning to study the use of performative commands in the Cards corpus, a set of transcripts from a web-based game that is designed to elicit a high degree of linguistic and strategic collaboration.", "labels": [], "entities": []}, {"text": "For example, players are tasked with navigating a maze-like gameboard in search of six cards of the same suit, but since a player can hold at most three cards at a time, they must coordinate their efforts to win the game.", "labels": [], "entities": []}, {"text": "We focus on a subclass of performative commands that are ubiquitous in the Cards corpus: Nonagentive declaratives about the locations of objects, e.g. \"The five of hearts is in the top left corner,\" hereafter referred to as locatives.", "labels": [], "entities": []}, {"text": "Despite that their semantics makes no reference to either an agent or an action-thus distinguishing them from conventional imperatives)-locatives can be interpreted as commands when embedded in particular discourse contexts.", "labels": [], "entities": []}, {"text": "In the Cards game, it is frequently the case that an addressee will respond to such an utterance by fetching the card mentioned.", "labels": [], "entities": []}, {"text": "Following work on the context-driven interpretation of declaratives as questions), we hypothesize that the illocutionary effect of a locative utterance is a function of contextual features that variably constrain the actions of discourse participants.", "labels": [], "entities": []}, {"text": "To test this idea, we identify a set of 94 locative utterances in the Cards transcripts that we deem to be truly ambiguous, out of context, between informative and command readings.", "labels": [], "entities": []}, {"text": "We then annotate their respective transcripts fora simplified representation of the tabular common ground model of.", "labels": [], "entities": []}, {"text": "Here, we identify the common ground with the state of a game as reflected by the utterances made by both players up to a specific point in time.", "labels": [], "entities": []}, {"text": "Finally, we train machine learning classifiers on features of the common ground to predict whether or not the addressee will act in response to the utterance in question.", "labels": [], "entities": []}, {"text": "Through these experiments we discover a few very powerful contextual features that predict when a locative utterance will be interpreted as a command.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our aim in devising this task is to investigate connection between common ground knowledge and the illocutionary effects of locative utterances.", "labels": [], "entities": []}, {"text": "We therefore train a standard logistic regression classifier and experiment with a few carefully designed features that encode constraints on player action, and which should hypothetically trigger the interpretation of locative utterances as indirect commands.", "labels": [], "entities": []}, {"text": "We experiment with the following features:: F 1 performance as reported on the test set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9809102416038513}]}, {"text": "Note our baselines are italicized.", "labels": [], "entities": []}, {"text": "\u2022 Edit Distance: We use the minimal number of edits for an optimal solution as a feature.", "labels": [], "entities": [{"text": "Edit Distance", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.7176213562488556}]}, {"text": "Given the cards in the players' hands at a given point in the game, we can define an optimal solution based on the number of edits that must be made to the hands to achieve that optimal solution.", "labels": [], "entities": []}, {"text": "An edit is defined as either picking up or dropping a card, and each such action has a cost of 1.", "labels": [], "entities": []}, {"text": "An optimal solution is defined as the one that requires the minimal number of edits given the current hands.", "labels": [], "entities": []}, {"text": "For example, if player 1 has a 2H, 3H, and 4H and player 2 has a 6H and a 7H, the optimal solution is the 2H, 3H, 4H, 5H, 6H, and 7H.", "labels": [], "entities": []}, {"text": "Such a solution requires a single edit because player 2 simply has to pickup a 5H.", "labels": [], "entities": []}, {"text": "This feature seeks to capture the intuition that an addressee should tend to act with respect to a card when the edit distance is not particularly high and hence the game is near a winning state.", "labels": [], "entities": []}, {"text": "\u2022 Explicit Goal: This binary feature is triggered in two cases: 1) When the suit of card mentioned matches the agreed-upon suit strategy in the common ground and 2) When the card mentioned appears in the set of cards the addressee claims to need.", "labels": [], "entities": []}, {"text": "This models the prediction that locative utterances are more likely to be indirect commands when they are relevant to a well-defined goal.", "labels": [], "entities": []}, {"text": "\u2022 Full Hands: This binary feature is triggered when the speaker has three cards of the same suit as the card mentioned, and which are associated with some winning six-card straight, but the addressee does not.", "labels": [], "entities": []}, {"text": "This models the prediction that locative utterances are likely to be indirect commands when they provide information relevant to winning, but only the addressee can act as such.", "labels": [], "entities": []}, {"text": "Single-feature classifiers are compared against a number of baselines to help benchmark our predictive task.", "labels": [], "entities": []}, {"text": "Our first baseline, which is context-agnostic, seeks to capture the intuition that the role of a locative utterance is entirely ambiguous when considered in isolation.", "labels": [], "entities": []}, {"text": "This baseline predicts the agent follow-up using a Bernoulli distribution weighted according to the class priors of the training data.", "labels": [], "entities": []}, {"text": "The second baseline incorporates surface-level dialogue context via bigram features of all the utterance exchanged between players up to and including the locative utterance.", "labels": [], "entities": []}, {"text": "We also experimented with a unigram baseline but found that its performance was inferior to that of the bigram.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F 1 performance as reported on the test set. Note our baselines are italicized.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.977018415927887}]}]}