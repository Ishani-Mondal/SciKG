{"title": [{"text": "Improving Language Modeling using Densely Connected Recurrent Neural Networks", "labels": [], "entities": [{"text": "Improving Language Modeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9198752045631409}]}], "abstractContent": [{"text": "In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks.", "labels": [], "entities": []}, {"text": "We evaluate our proposed architecture on the Penn Treebank language modeling task.", "labels": [], "entities": [{"text": "Penn Treebank language modeling task", "start_pos": 45, "end_pos": 81, "type": "DATASET", "confidence": 0.9070751905441284}]}, {"text": "We show that we can obtain similar perplex-ity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al., 2014).", "labels": [], "entities": []}, {"text": "In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions .", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is a key task in Natural Language Processing (NLP), lying at the root of many NLP applications such as syntactic parsing (, machine translation ( ) and speech processing (.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7400958985090256}, {"text": "Natural Language Processing (NLP)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7199663917223612}, {"text": "syntactic parsing", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7435399889945984}, {"text": "machine translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.813434362411499}]}, {"text": "In, recurrent neural networks were first introduced for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.79838427901268}]}, {"text": "Since then, a number of improvements have been proposed.", "labels": [], "entities": []}, {"text": "used a stack of Long Short-Term Memory (LSTM) layers trained with dropout applied on the outputs of every layer, while and further improved the perplexity score using variational dropout.", "labels": [], "entities": []}, {"text": "Other improvements are more specific to language modeling, such as adding an extra memory component ( or tying the input and output embeddings.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.696095883846283}]}, {"text": "To be able to train larger stacks of LSTM layers, typically four layers or more (), skip or residual connections are needed.", "labels": [], "entities": []}, {"text": "used residual connections to train a machine translation model with eight LSTM layers, while Van Den Oord et al.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7204642593860626}]}, {"text": "(2016) used both residual and skip connections to train a pixel recurrent neural network with twelve LSTM layers.", "labels": [], "entities": []}, {"text": "In both cases, a limited amount of skip/residual connections was introduced to improve gradient flow.", "labels": [], "entities": []}, {"text": "In contrast, showed that densely connecting more than 50 convolutional layers substantially improves the image classification accuracy over regular convolutional and residual neural networks.", "labels": [], "entities": [{"text": "image classification", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.732740044593811}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9484936594963074}]}, {"text": "More specifically, they introduced skip connections between every input and every output of every layer.", "labels": [], "entities": []}, {"text": "Hence, this motivates us to densely connect all layers within a stacked LSTM model using skip connections between every pair of layers.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the usage of skip connections when stacking multiple LSTM layers in the context of language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.6931203156709671}]}, {"text": "When every input of every layer is connected with every output of every other layer, we get a densely connected recurrent neural network.", "labels": [], "entities": []}, {"text": "In contrast with the current usage of skip connections, we demonstrate that skip connections significantly improve performance when stacking only a few layers.", "labels": [], "entities": []}, {"text": "Moreover, we show that densely connected LSTMs need fewer parameters than stacked LSTMs to achieve similar perplexity scores in language modeling.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed architecture on the Penn Treebank (PTB) corpus.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) corpus", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.9784703155358633}]}, {"text": "We adopt the standard train, validation and test splits as described in, containing 930k training, 74k validation, and 82k test words.", "labels": [], "entities": []}, {"text": "The vocabulary is limited to 10,000 words.", "labels": [], "entities": []}, {"text": "Out-ofvocabulary words are replaced with an UNK token.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.8770232498645782}]}, {"text": "Our baseline is a stacked Long Short-Term Memory (LSTM) network, trained with regular dropout as introduced by.", "labels": [], "entities": []}, {"text": "Both the stacked and densely connected LSTM models consist of an embedding layer followed by a variable number of LSTM layers and a single fully connected output layer.", "labels": [], "entities": []}, {"text": "LSTM layers, we also evaluate a model with three stacked LSTM layers, and experiment with two, three, four and five densely connected LSTM layers.", "labels": [], "entities": []}, {"text": "The hidden state size of the densely connected LSTM layers is either 200 or 650.", "labels": [], "entities": []}, {"text": "The size of the embedding layer is always 200.", "labels": [], "entities": []}, {"text": "We applied standard dropout on the output of every layer.", "labels": [], "entities": []}, {"text": "We used a dropout probability of 0.6 for models with size 200 and 0.75 for models with hidden state size 650 to avoid overfitting.", "labels": [], "entities": []}, {"text": "Additionally, we also experimented with Variational Dropout (VD) as implemented in.", "labels": [], "entities": []}, {"text": "We initialized all our weights uniformly in the interval [-0.05;0.05].", "labels": [], "entities": []}, {"text": "In addition, we used a batch size of 20 and a sequence length of 35 during training.", "labels": [], "entities": [{"text": "sequence length", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.9286308884620667}]}, {"text": "We trained the weights using standard Stochastic Gradient Descent (SGD) with the following learning rate scheme: training for six epochs with a learning rate of one and then applying a decay factor of 0.95 every epoch.", "labels": [], "entities": []}, {"text": "We constrained the norm of the gradient to three.", "labels": [], "entities": []}, {"text": "We trained for 100 epochs and used early stopping.", "labels": [], "entities": []}, {"text": "The evaluation metric reported is perplexity as defined in Equation 3.", "labels": [], "entities": []}, {"text": "The number of parameters reported is calculated as the sum of the total amount of weights that reside in every layer.", "labels": [], "entities": []}, {"text": "Note that apart from the exact values of some hyperparameters, the experimental setup is identical to.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of densely connected recurrent neural networks for the PTB language modeling task.", "labels": [], "entities": [{"text": "PTB language modeling task", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7955781370401382}]}]}