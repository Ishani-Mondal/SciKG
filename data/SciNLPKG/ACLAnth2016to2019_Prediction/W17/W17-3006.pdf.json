{"title": [{"text": "One-step and Two-step Classification for Abusive Language Detection on Twitter", "labels": [], "entities": [{"text": "Abusive Language Detection", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7499444882074991}]}], "abstractContent": [{"text": "Automatic abusive language detection is a difficult but important task for online social media.", "labels": [], "entities": [{"text": "Automatic abusive language detection", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6664173901081085}]}, {"text": "Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages.", "labels": [], "entities": []}, {"text": "With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.", "labels": [], "entities": [{"text": "English Twitter corpus", "start_pos": 14, "end_pos": 36, "type": "DATASET", "confidence": 0.6691663960615793}, {"text": "F-measure", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9979637861251831}, {"text": "F-measure", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9976958632469177}]}], "introductionContent": [{"text": "Fighting abusive language online is becoming more and more important in a world where online social media plays a significant role in shaping people's minds.", "labels": [], "entities": []}, {"text": "Nevertheless, major social media companies like Twitter find it difficult to tackle this problem, as the huge number of posts cannot be mediated with only human resources. and are one of the early researches to use machine learning based classifiers for detecting abusive language.", "labels": [], "entities": []}, {"text": "incorporated representation word embeddings (.", "labels": [], "entities": []}, {"text": "combined pre-defined language elements and word embedding to train a regression model.", "labels": [], "entities": []}, {"text": "used logistic regression with n-grams and user-specific features such as gender and location.", "labels": [], "entities": []}, {"text": "conducted a deeper investigation on different types of abusive language.", "labels": [], "entities": []}, {"text": "experimented with deep learning-based models using ensemble gradient boost classifiers to perform multi-class classification on sexist and racist language.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.7024787962436676}]}, {"text": "All approaches have been on one step.", "labels": [], "entities": []}, {"text": "Many have addressed the difficulty of the definition of abusive language while annotating the data, because they are often subjective to individuals () and lack of context (.", "labels": [], "entities": []}, {"text": "This makes it harder for non-experts to annotate without having a certain amount of domain knowledge.", "labels": [], "entities": []}, {"text": "In this research, we aim to experiment a twostep approach of detecting abusive language first and then classifying into specific types and compare with a one-step approach of doing one multiclass classification on sexist and racist language.", "labels": [], "entities": []}, {"text": "Moreover, we explore applying a convolutional neural network (CNN) to tackle the task of abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.6277520755926768}]}, {"text": "We use three kinds of CNN models that use both character-level and word-level inputs to perform classification on different dataset segmentations.", "labels": [], "entities": []}, {"text": "We measure the performance and ability of each model to capture characteristics of abusive language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the two English Twitter Datasets ( published as unshared tasks for the 1 st Workshop on Abusive Language Online(ALW1).", "labels": [], "entities": [{"text": "English Twitter Datasets", "start_pos": 16, "end_pos": 40, "type": "DATASET", "confidence": 0.8726271192232767}, {"text": "1 st Workshop on Abusive Language Online(ALW1)", "start_pos": 79, "end_pos": 125, "type": "TASK", "confidence": 0.6492076784372329}]}, {"text": "It contains tweets with sexist and racist comments.", "labels": [], "entities": []}, {"text": "created a list of criteria based on a critical race theory and let an expert annotate the corpus.", "labels": [], "entities": []}, {"text": "First, we concatenated the two datasets into one and then divided that into three datasets for one-step and two-step classification.", "labels": [], "entities": []}, {"text": "One-step dataset is a segmentation for multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.7432656586170197}]}, {"text": "For two-step classification, we merged the sexism and racism labels into one abusive label.", "labels": [], "entities": []}, {"text": "Finally, we created another dataset with abusive languages to experiment a second classifier to distinguish \"sexism\" and \"racism\", given that the instance is classified as \"abusive\".", "labels": [], "entities": []}, {"text": "We performed two classification experiments: 1.", "labels": [], "entities": []}, {"text": "Detecting \"none\", \"sexist\", and \"racist\" language (one-step) Architecture of HybridCNN 2.", "labels": [], "entities": []}, {"text": "Detecting \"abusive language\", then further classifying into \"sexist\" or \"racist\" (twostep) The purpose of these experiments was to see whether dividing the problem space into two steps makes the detection more effective.", "labels": [], "entities": []}, {"text": "We trained the models using mini-batch stochastic gradient descent with AdamOptimizer ().", "labels": [], "entities": [{"text": "AdamOptimizer", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.849077582359314}]}, {"text": "For more efficient training in an unbalanced dataset, the mini-batch with a size of 32 had been sampled with equal distribution for all labels.", "labels": [], "entities": []}, {"text": "The training continued until the evaluation set loss did not decrease any longer.", "labels": [], "entities": [{"text": "evaluation set loss", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.7590537667274475}]}, {"text": "All the results are average results of 10-fold cross validation.", "labels": [], "entities": []}, {"text": "As evaluation metric, we used F1 scores with precision and recall score and weighted averaged the scores to consider the imbalance of the labels.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.999108612537384}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9991806149482727}, {"text": "recall score", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.9821921288967133}]}, {"text": "For this reason, total average F1 might not between average precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9901890158653259}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9675955176353455}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9970111846923828}]}, {"text": "As baseline, we used the character n-gram logistic regression classifier (indicated as LR on  One-step: Dataset Segmentation and racism) of an abusive language.", "labels": [], "entities": [{"text": "LR", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9623743295669556}]}, {"text": "We can deduce that sexist and racist comments have obvious discriminating features that are easy for all classifiers to capture.", "labels": [], "entities": []}, {"text": "Since the precision and recall scores of the \"abusive\" label is higher than those of \"racism\" and \"sexism\" in the one-step approach, the twostep approach can perform as well as the one-step approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995191097259521}, {"text": "recall scores", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9786348342895508}]}], "tableCaptions": [{"text": " Table 2. Experiment Results: upper part is the one-step methods that perform multi-class classifi- cation and lower methods with (two) indicate two-step that combines two binary classifiers. Hy- bridCNN is our newly created model.", "labels": [], "entities": []}, {"text": " Table 3. Results on Abusive Language Detec- tion", "labels": [], "entities": [{"text": "Abusive Language Detec- tion", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.897027337551117}]}]}