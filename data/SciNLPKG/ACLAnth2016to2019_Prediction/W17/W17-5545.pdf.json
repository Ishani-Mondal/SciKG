{"title": [{"text": "How Would You Say It? Eliciting Lexically Diverse Data for Supervised Semantic Parsing", "labels": [], "entities": [{"text": "Eliciting Lexically Diverse Data", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.8437434881925583}, {"text": "Semantic Parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6625937521457672}]}], "abstractContent": [{"text": "Building dialogue interfaces for real-world scenarios often entails training semantic parsers starting from zero examples.", "labels": [], "entities": []}, {"text": "How can we build datasets that better capture the variety of ways users might phrase their queries, and what queries are actually realistic?", "labels": [], "entities": []}, {"text": "(2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a grammar and having crowdworkers paraphrase them into natural wording.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7514950633049011}]}, {"text": "A limitation of this approach is that it induces bias towards using similar language as the canonical utterances.", "labels": [], "entities": []}, {"text": "In this work, we present a methodology that elicits meaningful and lexically diverse queries from users for semantic parsing tasks.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7895027101039886}]}, {"text": "Starting from a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated.", "labels": [], "entities": []}, {"text": "We use this method to build a semantic parsing dataset from scratch fora dialog agent in a smart-home simulation.", "labels": [], "entities": []}, {"text": "We find evidence that this dataset, which we have named SMARTHOME, is demon-strably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets.", "labels": [], "entities": [{"text": "SMARTHOME", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.7256348729133606}, {"text": "domain-specific semantic parsing", "start_pos": 144, "end_pos": 176, "type": "TASK", "confidence": 0.7418809533119202}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping natural language utterances to their underlying meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8288814723491669}]}, {"text": "This is an essential component for many tasks that require understanding natural language dialogue; Zelle and * *The indicated authors contributed equally to this work..", "labels": [], "entities": [{"text": "understanding natural language dialogue", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.6155908778309822}]}, {"text": "Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain.", "labels": [], "entities": []}, {"text": "An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue.", "labels": [], "entities": []}, {"text": "With the advent of datadriven methods for semantic parsing, constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semantic parsers to new tasks.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7265419661998749}]}, {"text": "propose a methodology for efficient creation of semantic parsing data that starts with the set of target logical forms, and generates example natural language utterances for these logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7477441132068634}]}, {"text": "Specifically, the authors of the parser specify a seed lexicon with canonical phrase/predicate pairs fora particular domain, and subsequently a generic grammar constructs canonical utterances paired with logical forms.", "labels": [], "entities": []}, {"text": "Because the canonical utterances maybe ungrammatical or stilted, they are then paraphrased by crowd workers to be more natural queries in the target language.", "labels": [], "entities": []}, {"text": "We argue that this approach has three limitations when constructing semantic parsers for new domains: (1) the seed utterances may induce bias towards the language of the canonical utterance, specifically with regards to lexical choice, (2) the generic grammar suggested cannot be used to generate all the queries we may want to support in anew domain, and (3) there is no check on the correctness or naturalness of the canonical utterances themselves, which may not be logically plausible.", "labels": [], "entities": []}, {"text": "This is problematic as even unlikely canonical utterances can be paraphrased fluently.", "labels": [], "entities": []}, {"text": "In this paper, we propose and evaluate anew approach for creating lexically diverse and plausible utterances for semantic parsing).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7284507006406784}]}, {"text": "Firstly, inspired by the use of images in the creation of datasets for paraphrasing ( or for natural language generation (, we seek to reduce this linguistic bias by using a lexicon consisting of images.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.822051207224528}]}, {"text": "Secondly, a generative grammar, which is tailored to the domain, combines these images to form mixed text-image representations.", "labels": [], "entities": [{"text": "generative grammar", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8968724608421326}]}, {"text": "Using these two approaches, we retain many of the advantages of existing approaches such as ease of supervision and completeness of the dataset, with the added bonus of promoting lexical diversity in the natural language utterances, and supporting queries relevant to our domain.", "labels": [], "entities": []}, {"text": "Finally, we add a simple step within the crowdsourcing experiment where crowd-workers evaluate the plausibility of the generated canonical utterances.", "labels": [], "entities": []}, {"text": "At training time, we conjecture that optionally adding a term to upweight plausible queries might be useful to deploy a semantic parser in real world settings.", "labels": [], "entities": []}, {"text": "Encouraging the parser to focus on queries that make sense reduces emphasis on things that a user is unlikely to ask.", "labels": [], "entities": []}, {"text": "We evaluate our method by building a semantic parser from scratch fora dialogue agent in a smart home simulation.", "labels": [], "entities": []}, {"text": "The dialogue agent will be capable of answering questions about various sensor activations, and higher-level concepts which map to these activations.", "labels": [], "entities": []}, {"text": "Such a task requires understanding the natural language queries of the user, which could be varied and even indirect.", "labels": [], "entities": []}, {"text": "For example, in SMARTHOME, 'where can I go to cool off?'", "labels": [], "entities": [{"text": "SMARTHOME", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.6580740809440613}]}, {"text": "corresponds to the canonical utterance 'which room contains the AC that is in the house?'.", "labels": [], "entities": []}, {"text": "Similarly, 'is the temp in the chillspace broke?'", "labels": [], "entities": []}, {"text": "corresponds to 'are the thermometers in the living room malfunctioning?'.", "labels": [], "entities": []}, {"text": "As a result of our analysis, we find that the proposed method of eliciting utterances using image-based representations results in considerably more diverse utterances than in the previous text-based approach.", "labels": [], "entities": []}, {"text": "We also find evidence that the SMARTHOME dataset, constructed using this approach, is more diverse than other domainspecific datasets for semantic parsing, such as GEOQUERY or ATIS.", "labels": [], "entities": [{"text": "SMARTHOME dataset", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.8486842811107635}, {"text": "semantic parsing", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7122815847396851}, {"text": "GEOQUERY", "start_pos": 164, "end_pos": 172, "type": "DATASET", "confidence": 0.9364823698997498}]}, {"text": "We release this dataset to the community 1 as anew benchmark.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, it is of interest how the data collection methodology influences the realism and difficulty of the semantic parsing task.", "labels": [], "entities": [{"text": "realism", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9776630401611328}, {"text": "semantic parsing task", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7942576706409454}]}, {"text": "In this section, we run several baseline models to measure this effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of data creation methodol- ogy of (", "labels": [], "entities": []}, {"text": " Table 2: Example from datasets GEO, JOBS, ATIS, OVERNIGHT and SMARTHOME", "labels": [], "entities": [{"text": "GEO", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9261300563812256}, {"text": "JOBS", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.5736339092254639}, {"text": "ATIS", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9811326861381531}, {"text": "OVERNIGHT", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8743875026702881}, {"text": "SMARTHOME", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.5406447649002075}]}, {"text": " Table 3: Number of word types in the language  compared to number of word types in the logical  form. Larger ratio indicates more lexical diversity  for the same complexity of the logical form", "labels": [], "entities": []}, {"text": " Table 4: Test accuracy results of different systems on the SMARTHOME dataset as compared to  OVERNIGHT and GEO", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9435641765594482}, {"text": "SMARTHOME dataset", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9549646973609924}, {"text": "OVERNIGHT", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.6835475564002991}, {"text": "GEO", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.8871449828147888}]}]}