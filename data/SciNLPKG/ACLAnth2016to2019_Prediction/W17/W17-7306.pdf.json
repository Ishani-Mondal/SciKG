{"title": [{"text": "Dealing with Co-reference in Neural Semantic Parsing", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6662964026133219}]}], "abstractContent": [{"text": "Linguistic phenomena like pronouns, control constructions, or co-reference give rise to co-indexed variables in meaning representations.", "labels": [], "entities": []}, {"text": "We review three different methods for dealing with co-indexed variables in the output of neural semantic parsing of abstract meaning representations: (a) copying concepts during training and restoring co-indexation in a post-processing step; (b) explicit indexing of co-indexation; and (c) using absolute paths to designate co-indexing.", "labels": [], "entities": [{"text": "neural semantic parsing of abstract meaning representations", "start_pos": 89, "end_pos": 148, "type": "TASK", "confidence": 0.8575532521520343}]}, {"text": "The second method gives the best results and outperforms the baseline by 2.9 F-score points.", "labels": [], "entities": [{"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9990934133529663}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language sentence into a meaning representation (a logical form).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8295674920082092}]}, {"text": "One of the problems a semantic parser has to deal with is co-indexed variables, which arise in antecedent-anaphor relations, proper name co-reference, control constructions and other linguistic phenomena.", "labels": [], "entities": []}, {"text": "Examples of such constructions are given in (1)-(4): In the context of this paper, we represent meanings using the formalism of Abstract Meaning Representation (AMR), as introduced by.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 128, "end_pos": 165, "type": "TASK", "confidence": 0.7410098860661188}]}, {"text": "AMRs can be seen as graphs connecting concepts by relations.", "labels": [], "entities": []}, {"text": "Each concept is represented by a named instance.", "labels": [], "entities": []}, {"text": "Co-reference is established by re-using these instances.", "labels": [], "entities": []}, {"text": "For example, the AMRs corresponding to examples (1) and (2) above are given in.", "labels": [], "entities": []}, {"text": "Note that, due to the bracketing, the variable b encapsulates the whole entity person :name \"Bob\" and not just person, i.e. b stands fora person with the name Bob.", "labels": [], "entities": []}, {"text": "That there is a lotto gain in this area can be seen by applying the AMR evaluation suite of, which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them.", "labels": [], "entities": [{"text": "AMR evaluation suite", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.702197790145874}, {"text": "AMR parsing", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.8534877300262451}, {"text": "reentrancy", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.9539486765861511}]}, {"text": "Out of the four systems that made these scores available (all scores reported in van), the reentrancy metric obtained the lowest F-score for three of them.", "labels": [], "entities": [{"text": "F-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9989194869995117}]}, {"text": "Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g.;;;) to the more recent neural approaches (;;;).", "labels": [], "entities": [{"text": "parse AMRs", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.7052956223487854}]}, {"text": "Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. and use a special character to indicate reentrancy and restore co-referring variables in a post-processing step.", "labels": [], "entities": []}, {"text": "simply replace reentrancy variables by their co-referring concept in the input and never outputs co-referring nodes. and van use the same input transformation as, but do try to restore co-referring nodes by merging all equal concepts into a single concept in a post-processing step.", "labels": [], "entities": []}, {"text": "All these methods have in common that they are not very sophisticated, but more importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best implementation is for future neural AMR parsers.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 263, "end_pos": 274, "type": "TASK", "confidence": 0.8311093747615814}]}, {"text": "In this paper we present three methods to handle reentrancy for AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9573142528533936}]}, {"text": "The first two methods are based on the previous work described above, while the third is anew, more principled method.", "labels": [], "entities": []}, {"text": "These methods are applied on the model that reported the best results in the literature, the character-level neural semantic parsing method of van.", "labels": [], "entities": [{"text": "character-level neural semantic parsing", "start_pos": 93, "end_pos": 132, "type": "TASK", "confidence": 0.6828360036015511}]}, {"text": "Ina nutshell, this method uses a character-based sequence-to-sequence model to translate sentences to AMRs.", "labels": [], "entities": []}, {"text": "To enable this process, pre-processing and post-processing steps are needed.", "labels": [], "entities": []}, {"text": "The aim of this paper is to find the best method to handle reentrancy in neural semantic parsing and to show the specific impact that each of the methods have on general performance.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.6642220218976339}]}], "datasetContent": [{"text": "We test the impact of the different methods on two of our earlier models, described in van.", "labels": [], "entities": [{"text": "van", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.94988614320755}]}, {"text": "The first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.", "labels": [], "entities": []}, {"text": "This model is referred to as the char-only model.", "labels": [], "entities": []}, {"text": "The second is the approach that produced one of the best results so far in the literature.", "labels": [], "entities": []}, {"text": "This model uses POS-tagged input, clusters together groups of characters (super characters) and exploits 100,000 \"silver\" AMRs that were obtained by using the off-the-shelf AMR parsers CAMR ( and JAMR ( . The added AMRs are all CAMR-produced.", "labels": [], "entities": [{"text": "AMR parsers CAMR", "start_pos": 173, "end_pos": 189, "type": "TASK", "confidence": 0.5179265439510345}, {"text": "JAMR", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.6842955350875854}]}, {"text": "We must note that CAMR is not particularly keen on outputting coreference, as the 100,000 silver AMRs only produced 18,865 new reentrancy nodes.", "labels": [], "entities": [{"text": "CAMR", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.6929157376289368}, {"text": "outputting coreference", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7320891916751862}]}, {"text": "The second approach also employs the postprocessing methods Wikification and pruning, as explained in van.", "labels": [], "entities": [{"text": "Wikification", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8638440370559692}]}, {"text": "The Wikification step simply adds wiki links to :name nodes, since those links were removed in the input.", "labels": [], "entities": []}, {"text": "Pruning is used to remove erroneously produced duplicate output.", "labels": [], "entities": []}, {"text": "This is a common problem for sequence-to-sequence models, since the model does not keep track of what it has already output.", "labels": [], "entities": []}, {"text": "No pre-training or ensemble methods are used for both approaches.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the co-indexed variables in the AMR data set.", "labels": [], "entities": [{"text": "AMR data set", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.9254976908365885}]}, {"text": " Table 2. It is trained for 20 epochs, after which the model  that performs best on the development set is used to decode the test set.", "labels": [], "entities": []}, {"text": " Table 2: Parameter settings of the seq2seq model, as in van Noord and Bos (2017).", "labels": [], "entities": []}, {"text": " Table 3: Results of the different methods in comparison to the char-only and best model.", "labels": [], "entities": []}, {"text": " Table 4. We see that, in general,  we indeed only improve on both baselines for AMRs that have co-indexed variables. This is the case for  both the dev and the test set. This is the desired scenario: we improve on AMRs with reentrancy, while", "labels": [], "entities": [{"text": "reentrancy", "start_pos": 225, "end_pos": 235, "type": "METRIC", "confidence": 0.9630045890808105}]}, {"text": " Table 5: Number of possible (pos) and impossible (imp) paths in the output, for the dev and test set for  both our models, when using the Absolute Paths method.", "labels": [], "entities": []}, {"text": " Table 6: Number of times each step in Algorithm 1 was responsible for replacing an index in the output  when using the Indexing method.", "labels": [], "entities": []}]}