{"title": [], "abstractContent": [{"text": "Encoder-decoder neural networks have been used for many NLP tasks, such as neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.6119545300801595}]}, {"text": "They have also been applied to constituent parsing by using bracketed tree structures as a target language, translating input sentences into syntactic trees.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7655393481254578}]}, {"text": "A more commonly used method to linearize syntactic trees is the shift-reduce system, which uses a sequence of transition-actions to build trees.", "labels": [], "entities": []}, {"text": "We empirically investigate the effectiveness of applying the encoder-decoder network to transition-based parsing.", "labels": [], "entities": []}, {"text": "On standard benchmarks, our system gives comparable results to the stack LSTM parser for dependency parsing, and significantly better results compared to the afore-mentioned parser for constituent parsing, which uses bracketed tree formats.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8333427608013153}, {"text": "constituent parsing", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7222975492477417}]}], "introductionContent": [{"text": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency grammar, constituent grammar () and CCG ().", "labels": [], "entities": []}, {"text": "For transitionbased parsing, employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser.", "labels": [], "entities": [{"text": "transitionbased parsing", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6197675466537476}]}, {"text": "Subsequent work extend this method by investigating more complex representations of configurations () and global training with beam search (.", "labels": [], "entities": []}, {"text": "Borrowing ideas from neural machine translation (NMT) (, a line of work utilizes a bidirectional RNN to encode input sentences, using it for feature extraction, and observing improved performances for both transition-based) and graph-based parsers.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8751798967520396}, {"text": "feature extraction", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7485759556293488}]}, {"text": "In particular, using such encoder structure, the graph-based parser of achieves the state-of-the-art results for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8202257454395294}]}, {"text": "The success of the encoder structure can be attributed to the use of multilayer bidirectional LSTMs to induce non-local representations of sentences.", "labels": [], "entities": []}, {"text": "Without manual feature engineering, such architecture automatically extracts complex features for syntactic representation.", "labels": [], "entities": [{"text": "syntactic representation", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7149412631988525}]}, {"text": "For neural machine translation, such encoder structure has been connected to a corresponding LSTM decoder, giving the state-of-the-art for sequenceto-sequence learning.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7039204637209574}]}, {"text": "Compared to carefully designed feature representations, such as the parser of and the stack-LSTM structure of , the encoderdecoder structure is conceptually simpler, and more general, which can be used across different grammar formalisms without redesigning the stack representation.", "labels": [], "entities": []}, {"text": "applied the encoder-decoder structure to constituent parsing, generating the bracketed syntactic trees as the output token sequence without model combination.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6726770401000977}]}, {"text": "However, their model achieves relatively low accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9806273579597473}]}, {"text": "The advantage of using a decoder LSTM is that it leverages a recurrent structure for capturing full sequence information in the output.", "labels": [], "entities": []}, {"text": "Unlike greedy or CRF decoders, which capture only local label dependencies, LSTM decoder models global label sequence relations.", "labels": [], "entities": []}, {"text": "use bracketed syntactic trees as the output token sequence, which requires strong constraints to ensure that the output strings correspond to valid dependency trees.", "labels": [], "entities": []}, {"text": "On the other hand, a more commonly used sequential representation of syntactic structures is the transition-action sequences in shift reduce parsers.", "labels": [], "entities": []}, {"text": "For both constituent) and dependency parsing, output syntactic structures can be built using a sequence of inter-dependent shift-reduce actions, which convey incremental structural information.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6980138272047043}]}, {"text": "Motivated by the above, we study the effectiveness of a highly simple encode-decoder structure for shift-reduce parsing.", "labels": [], "entities": []}, {"text": "In particular, the encoder is used to represent the input sentence and the decoder is used to generate a sequence of transition actions for constructing the syntactic structure.", "labels": [], "entities": []}, {"text": "We additionally use the attention over the input sequence , but with a slight modification, taking separate attentions to represent the stack and queue, respectively.", "labels": [], "entities": []}, {"text": "On standard PTB evaluation, our final model achieves 93.1% UAS for dependency parsing, which is comparable to the model of , and 90.5% on constituent parsing, which is 2.2% higher compared to . We release our source code at https://github.com/LeonCrashCode/ Encoder-Decoder-Parser.", "labels": [], "entities": [{"text": "UAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9993177652359009}, {"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7899027168750763}, {"text": "constituent parsing", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.6581042408943176}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Dependency parsing dev results.", "labels": [], "entities": [{"text": "Dependency parsing dev", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8535963098208109}]}, {"text": " Table 3: Results for dependency parsing, where *  use global training,  \u2020 use dynamic oracle.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8703646063804626}]}, {"text": " Table 4: Results for constituent parsing.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8538209497928619}]}]}