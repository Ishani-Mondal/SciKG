{"title": [{"text": "A Multi-task Approach for Named Entity Recognition in Social Media Data", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8877825935681661}]}], "abstractContent": [{"text": "Named Entity Recognition for social media data is challenging because of its inherent noisiness.", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7085275948047638}]}, {"text": "In addition to improper grammatical structures, it contains spelling inconsistencies and numerous informal abbreviations.", "labels": [], "entities": []}, {"text": "We propose a novel multi-task approach by employing a more general secondary task of Named Entity (NE) segmentation together with the primary task of fine-grained NE categoriza-tion.", "labels": [], "entities": [{"text": "Named Entity (NE) segmentation", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6586995224157969}]}, {"text": "The multi-task neural network architecture learns higher order feature representations from word and character sequences along with basic Part-of-Speech tags and gazetteer information.", "labels": [], "entities": []}, {"text": "This neu-ral network acts as a feature extractor to feed a Conditional Random Fields clas-sifier.", "labels": [], "entities": []}, {"text": "We were able to obtain the first position in the 3rd Workshop on Noisy User-generated Text (WNUT-2017) with a 41.86% entity F1-score and a 40.24% surface F1-score.", "labels": [], "entities": [{"text": "Noisy User-generated Text (WNUT-2017)", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.4942276080449422}, {"text": "F1-score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8851613998413086}, {"text": "F1-score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.8200083374977112}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) aims at identifying different types of entities, such as people names, companies, location, etc., within a given text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8035151312748591}]}, {"text": "This information is useful for higher-level Natural Language Processing (NLP) applications such as information extraction, summarization, and data mining).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.8239081501960754}, {"text": "summarization", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.8718005418777466}, {"text": "data mining", "start_pos": 142, "end_pos": 153, "type": "TASK", "confidence": 0.7245941460132599}]}, {"text": "Learning Named Entities (NEs) from social media is a challenging task mainly because (i) entities usually represent a small part of limited annotated data which makes the task hard to generalize, and (ii) they do not follow strict rules (.", "labels": [], "entities": [{"text": "Learning Named Entities (NEs) from social media", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7806224524974823}]}, {"text": "This paper describes a multi-task neural network that aims at generalizing the underneath rules of emerging NEs in user-generated text.", "labels": [], "entities": []}, {"text": "In addition to the main category classification task, we employ an auxiliary but related secondary task called NE segmentation (i.e. a binary classification of whether a given token is a NE or not).", "labels": [], "entities": [{"text": "category classification task", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7909564971923828}, {"text": "NE segmentation", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7902567386627197}]}, {"text": "We use both tasks to jointly train the network.", "labels": [], "entities": []}, {"text": "More specifically, the model captures word shapes and some orthographic features at the character level by using a Convolutional Neural Network (CNN).", "labels": [], "entities": []}, {"text": "For contextual and syntactical information at the word level, such as word and Partof-Speech (POS) embeddings, the model implements a Bidirectional Long-Short Term Memory (BLSTM) architecture.", "labels": [], "entities": []}, {"text": "Finally, to cover wellknown entities, the model uses a dense representation of gazetteers.", "labels": [], "entities": []}, {"text": "Once the network is trained, we use it as a feature extractor to feed a Conditional Random Fields (CRF) classifier.", "labels": [], "entities": []}, {"text": "The CRF classifier jointly predicts the most likely sequence of labels giving better results than the network itself.", "labels": [], "entities": []}, {"text": "With respect to the participants of the shared task, our approach achieved the best results in both categories: 41.86% F1-score for entities, and 40.24% F1-score for surface forms.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9994422793388367}, {"text": "F1-score", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9991061091423035}]}, {"text": "The data for this shared task is provided by.", "labels": [], "entities": []}], "datasetContent": [{"text": "We preprocess all the datasets by replacing the URLs with the token <URL> before performing any experiment.", "labels": [], "entities": []}, {"text": "Additionally, we use half of development set as validation and the other half as evaluation.", "labels": [], "entities": []}, {"text": "Figure 3: Overall system design.", "labels": [], "entities": []}, {"text": "First, the system embeds a sentence into a high-dimensional space and uses CNN, BLSTM, and dense encoders to extract features.", "labels": [], "entities": [{"text": "CNN", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8832926154136658}, {"text": "BLSTM", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.5090727806091309}]}, {"text": "Then, it concatenates the resulting vectors of each encoder and performs multi-task.", "labels": [], "entities": []}, {"text": "The top left single-node layer represents segmentation (red) while the top right three-node layer represents categorization (blue).", "labels": [], "entities": []}, {"text": "Finally, a CRF classifier uses the weights of the common dense layer to perform a sequential classification.", "labels": [], "entities": []}, {"text": "Regarding the network hyper-parameters, in the case of the CNN, we set the kernel size to 3 on both convolutional layers.", "labels": [], "entities": []}, {"text": "We also use the same number of filters on both layers: 64.", "labels": [], "entities": []}, {"text": "Increasing the number of filters and the number of convolutional layers yields worse results, and it takes significantly more time.", "labels": [], "entities": []}, {"text": "In the case of the BLSTM architecture, we add dropout layers before and after the Bidirectional LSTM layers with dropout rates of 0.5.", "labels": [], "entities": []}, {"text": "The dropout layers allow the network to reduce overfitting ().", "labels": [], "entities": []}, {"text": "We also tried using a batch normalization layer instead of dropouts, but the experiment yielded worse results.", "labels": [], "entities": []}, {"text": "The training of the whole neural network is conducted using a batch size of 500 samples, and 150 epochs.", "labels": [], "entities": []}, {"text": "Additionally, we compile the model using the AdaMax optimizer).", "labels": [], "entities": []}, {"text": "Accuracy and F1-score are used as evaluation metrics.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9971867203712463}, {"text": "F1-score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994387030601501}]}, {"text": "For sequential inference, the CRF classifier uses L-BFGS as a training algorithm with L1 and L2 regularization.", "labels": [], "entities": []}, {"text": "The penalties for L1 and L2 are 1.0 and 1.0e \u22123 , respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: This table shows the results from the CRF clas-", "labels": [], "entities": [{"text": "CRF clas", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9138525128364563}]}, {"text": " Table 2: This table shows the final results of our submis-", "labels": [], "entities": []}, {"text": " Table 3: The scores of all the participants in the WNUT-", "labels": [], "entities": [{"text": "WNUT", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.5766522288322449}]}]}