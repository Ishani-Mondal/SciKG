{"title": [{"text": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "labels": [], "entities": [{"text": "Word Representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6591692417860031}, {"text": "Neural Machine Translation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.6767079333464304}]}], "abstractContent": [{"text": "Out-of-vocabulary words present a great challenge for Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.885670930147171}]}, {"text": "Recently various character-level composi-tional models were proposed to address this issue.", "labels": [], "entities": []}, {"text": "In current research we incorporate two most popular neural archi-tectures, namely LSTM and CNN, into hard-and soft-attentional models of translation for character-level representation of the source.", "labels": [], "entities": [{"text": "character-level representation of the source", "start_pos": 153, "end_pos": 197, "type": "TASK", "confidence": 0.8075867176055909}]}, {"text": "We propose semantic and morphological intrinsic evaluation of encoder-level representations.", "labels": [], "entities": []}, {"text": "Our analysis of the learned representations reveals that character-based LSTM seems to be better at capturing morphological aspects compared to character-based CNN.", "labels": [], "entities": []}, {"text": "We also show that a hard-attentional model provides better character-level representations compared to standard 'soft' attention .", "labels": [], "entities": []}], "introductionContent": [{"text": "Models of end-to-end machine translation based on neural networks can produce excellent translations, rivalling or surpassing traditional statistical machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7155633419752121}, {"text": "statistical machine translation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.7299439708391825}]}, {"text": "A central challenge in neural MT is handling rare and uncommon words.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.7921930551528931}]}, {"text": "Conventional neural MT models use a fixed modest-size vocabulary, such that the identity of rare words are lost, which makes their translation exceedingly difficult.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9205793142318726}]}, {"text": "Accordingly, sentences containing rare words tend to be translated much more poorly than those containing only common words.", "labels": [], "entities": []}, {"text": "The rare word problem is exacerbated when translating from morphologically rich languages, where the several morphological variants of words result in a huge vocabulary with a heavy tail.", "labels": [], "entities": []}, {"text": "For example in Russian, there are at least 70 word forms for dog, encoding case, gender, age, number, sentiment and other semantic connotations.", "labels": [], "entities": []}, {"text": "Many of them share a common lemma, and contain regular morphological affixation; consequently much of the information required for translation is present, but not in an accessible form for models of neural MT.", "labels": [], "entities": [{"text": "translation", "start_pos": 131, "end_pos": 142, "type": "TASK", "confidence": 0.9622442722320557}, {"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.849631667137146}]}, {"text": "In many cases the OOV problem is addressed by incorporating character-level word representations largely belonging to one of two classes, namely convolutional neural networks (CNNs) and recurrent neural networks based on long-short term memory (LSTM) units).", "labels": [], "entities": [{"text": "OOV", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9217275977134705}]}, {"text": "But there was no investigation of what each of the models captures and how well they can model morphology in particular.", "labels": [], "entities": []}, {"text": "In this paper, we fill this gap by evaluating of encoderlevel representations of OOV words.", "labels": [], "entities": []}, {"text": "To get the representations, we incorporate LSTM and CNN word representation models into two types of attentional machine translation models.", "labels": [], "entities": [{"text": "attentional machine translation", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.6740013758341471}]}, {"text": "Our evaluation includes both intrinsic and extrinsic metrics, where we compare these approaches based on their translation performance as well as their ability to recover synonyms for the rare words.", "labels": [], "entities": []}, {"text": "Intrinsic analysis shows that there is only minor differences in end translation performance, although detailed analysis shows that character-based LSTM is overally best at capturing morphological regularities.", "labels": [], "entities": [{"text": "end translation", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.6308889985084534}]}], "datasetContent": [{"text": "We use parallel bilingual data from Europarl for Estonian-English (, and web-crawled parallel data for Russian-English (Antonova and Misyurev, 2011).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9827605485916138}]}, {"text": "For preprocessing, we tokenize, lower-case, and filter out sentences longer than 30 words.", "labels": [], "entities": []}, {"text": "We apply a frequency threshold of 5, replacing low-frequency words with a special UNK token.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.8790881335735321}]}, {"text": "We apply the character level models in the encoder of the neural attentional () (AM, soft-attentional) and neural operation sequence (Vylomova et al., 2016) (OSM, hardattentional) models, replacing the source word embedding component with a BiLSTM or CNN over characters.", "labels": [], "entities": []}, {"text": "To evaluate translations, we re-ranked moses 3 100-best output translations using the attentional models.", "labels": [], "entities": []}, {"text": "The re-ranker includes standard features from moses plus an extra feature(s) for each of the models.", "labels": [], "entities": []}, {"text": "For the AM we supply the log probability of the candidate translation, and for the OSM we add two extra features corresponding to the generated alignment and the translation probabilities.", "labels": [], "entities": [{"text": "OSM", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8542695045471191}]}, {"text": "The weights of the re-ranker are then trained using MERT with 100 restarts to optimise BLEU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9763892292976379}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.997092604637146}]}, {"text": "As seen, re-ranking based on neural models' scores outperforms the phrase-based baseline.", "labels": [], "entities": []}, {"text": "However, the translation quality of the neural models are not significantly different.", "labels": [], "entities": []}, {"text": "We assume that this is due to re-ranking of moses translations rather than decoding.", "labels": [], "entities": []}, {"text": "Also note that here we do not address the problem of OOV on the decoding side.", "labels": [], "entities": [{"text": "OOV", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8832149505615234}]}, {"text": "We now take a closer look at the embeddings learned by the models, based on how well they: Corpus statistics for parallel data between Russian/Estonian and English.", "labels": [], "entities": []}, {"text": "The OOV rate are the fraction of word types in the source language that are in the test set but are below the frequency cut-off or unseen in training.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9884446561336517}]}, {"text": "capture the semantic and morphological information in the nearest neighbour words.", "labels": [], "entities": []}, {"text": "Learning representations for low frequency words is harder than that for high-frequency words, since low frequency words cannot capitalise as reliably on their contexts.", "labels": [], "entities": []}, {"text": "Therefore, we split the test lexicon into 6 parts according to their frequency in the training set.", "labels": [], "entities": []}, {"text": "Since we set out word frequency threshold to 5 for the training set, all words appearing in the lowest frequency band are OOVs for the test set.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9839218854904175}]}, {"text": "For each word of the test set, we take its top-20 nearest neighbours from the whole training lexicon using cosine similarity.", "labels": [], "entities": []}, {"text": "We investigate how well the nearest neighbours are interchangable with a query word in the translation process.", "labels": [], "entities": []}, {"text": "So we formalise the notion of semantics of the source words based on their translations in the target language.", "labels": [], "entities": []}, {"text": "We use pivoting to define the probability of a candidate word e to be the synonym of the query word e, p(e |e) = f p(f |e)p(e |f ), where f is a target language word, and the translation probabilities inside the summation are estimated using a word-based translation model trained on the entire initial bilingual corpora.", "labels": [], "entities": []}, {"text": "We then take the top-5 most probable words as the gold synonyms for each query word of the test set.", "labels": [], "entities": []}, {"text": "We measure the quality of predicted nearest neighbours using the multi-label accuracy 5 , where G(w) and N (w) are the sets of gold standard synonyms and nearest neighbors for w respectively; the function 1 is one if the condition C is true, and zero otherwise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.7264644503593445}]}, {"text": "In other words, it is the fraction of words in S whose nearest neighbours and gold standard synonyms have non-empty overlap.", "labels": [], "entities": []}, {"text": "In case of the hard attentional model we OSM CNN char outperforms other representations by a large margin.", "labels": [], "entities": []}, {"text": "We now turn to evaluating the morphological component.", "labels": [], "entities": []}, {"text": "We only focus on Russian since it has a notoriously hard morphology.", "labels": [], "entities": []}, {"text": "We run another morphological analyser, mystem, to generate linguistically tagged morphological analyses fora word, e.g. POS tags, case, person, plurality, etc.", "labels": [], "entities": []}, {"text": "We represent each morphological analysis with a bit vector, where each 1 bit indicates the presence of a specific grammatical feature.", "labels": [], "entities": []}, {"text": "Each word is then assigned a set of bit vectors corresponding to the set of its morphological analyses.", "labels": [], "entities": []}, {"text": "As the morphology similarity between two words, we take the minimum of Hamming similarity 6 between the corresponding two sets of bit vectors.    is due to the fact that they are naturally biased towards most recent inputs.", "labels": [], "entities": []}, {"text": "CNNs, on the other hand, are more invariant of character positions and provide whole-word similarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores for re-ranking the test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988288283348083}]}, {"text": " Table 1: Corpus statistics for parallel data between Russian/Estonian and English. The OOV rate are the fraction of word types  in the source language that are in the test set but are below the frequency cut-off or unseen in training.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9896692037582397}]}, {"text": " Table 4: Morphology analysis for nearest neighbours based  on (a) Grammar tag features, and (b) Lemma features, evalu- ated on Russian.", "labels": [], "entities": [{"text": "Morphology analysis", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8867147862911224}]}]}