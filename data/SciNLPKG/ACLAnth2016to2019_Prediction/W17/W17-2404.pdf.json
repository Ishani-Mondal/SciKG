{"title": [{"text": "Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are high-dimensional vector representations of words and are thus difficult to interpret.", "labels": [], "entities": []}, {"text": "In order to deal with this, we introduce an unsupervised parameter free method for creating a hierarchical graphical clustering of the full ensemble of word vectors and show that this structure is a geometrically meaningful representation of the original relations between the words.", "labels": [], "entities": []}, {"text": "This newly obtained representation can be used for better understanding and thus improving the embedding algorithm and exhibits semantic meaning, so it can also be utilized in a variety of language processing tasks like cat-egorization or measuring similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are different ways to assess word embeddings.", "labels": [], "entities": []}, {"text": "While some authors focus on general properties, as for example or, most evaluations are with respect to specific tasks.", "labels": [], "entities": []}, {"text": "Examples of the latter include the works by,, or, to name but a few.", "labels": [], "entities": []}, {"text": "The objective of this paper is to introduce a method forgetting a grasp of the global structure of embeddings, which is different from general schemes for dimensionality reduction like t-SNE), the methods summarized by, or visualization interfaces such as Roleo ( and).", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.7543716430664062}]}, {"text": "The method presented here is a specific way of clustering (a field nicely reviewed by) that works particularly well for the current objective.", "labels": [], "entities": []}, {"text": "We present a global analysis of the statistical properties of the embedding space.", "labels": [], "entities": []}, {"text": "This is based on the output of the well-known word2vec program ( , using the example of the dataset published alongside the source code on the web 1 , which was generated with the skipgram model with negative sampling.", "labels": [], "entities": []}, {"text": "This dataset was trained on parts of the English Google news corpus and consists of 3,000,000 words with 300-dimensional embedding vectors.", "labels": [], "entities": [{"text": "English Google news corpus", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.8882334977388382}]}, {"text": "First, densities in the embedding space will be explored.", "labels": [], "entities": []}, {"text": "Based on that a parameter free hierarchical graph-based clustering approach is developed that is the basis of a tool that allows to explore the neighborhood of a term of interest.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: After a quick discussion of statistical properties of the dataset, the concept of the graphical neighborhood hierarchy is explained.", "labels": [], "entities": []}, {"text": "Specific properties of the resulting graphs are brought into the context of peculiarities of the dataset for showing that this representation is particularly well-suited.", "labels": [], "entities": []}, {"text": "Finally, the semantic properties of the graphs are briefly evaluated.", "labels": [], "entities": []}], "datasetContent": [{"text": "The focus of this paper is on the analysis of embeddings.", "labels": [], "entities": []}, {"text": "Nevertheless, as already mentioned above, the findings presented in the previous sections indicate that the NH might be used for NLP tasks beyond visualization of word embeddings or other large high-dimensional datasets, because the neighborhood and macro vertex relations appear to be connected to semantical relations between the words, particularly on the lower levels.", "labels": [], "entities": []}, {"text": "Possible tasks that directly come to mind are measuring relatedness or similarity, various kinds of tagging, and classification.", "labels": [], "entities": []}, {"text": "In contrast to typical semantical frameworks like WordNet or FrameNet () whose creation requires extensive human resources, the NH can be created without expert knowledge in a very short time and has the capability of including much more words.", "labels": [], "entities": []}, {"text": "analyze graphs extracted from Wikipedia 3 and summarize a variety of methods for evaluating semantical relations.", "labels": [], "entities": []}, {"text": "In this spirit and fora first and quick quantitative view at the NH, similarity between neighbors in the graph and between words and their macro vertex are tested by calculating the respective WuPalmer similarity scores (.", "labels": [], "entities": []}, {"text": "Other scores basically lead to similar results and are thus not discussed in more detail.", "labels": [], "entities": []}, {"text": "Because the number of words in WordNet is much smaller than that in the dataset under consideration, the analysis is limited to those words that can be found in both datasets, which amounts to 54,586 words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9631828665733337}]}, {"text": "For that to be possible, a NH of these words alone is used, which is distinct from the full hierarchy discussed above.", "labels": [], "entities": []}, {"text": "The usefulness of these results fora much smaller dataset can be justified by envisioning that the sparser NNG must roughly be a skeleton of the full graph for geometrical reasons and must thus be related to the latter.", "labels": [], "entities": []}, {"text": "Besides that, quantifying similarity on the smaller graph is interesting in its own right.", "labels": [], "entities": []}, {"text": "3 http://www.wikipedia.org The results for the first four levels of the NH are shown in.", "labels": [], "entities": [{"text": "the NH", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.5876255333423615}]}, {"text": "Intuitively, the semantic relations between neighbors or words and macro vertices are expected to be stronger, if more \"probability mass\" can be found on the right side of the plot, because then more relations correspond to a higher similarity.", "labels": [], "entities": []}, {"text": "In order to clarify the meaning of the curves, a baseline curve is added that corresponds to an equivalent evaluation of random word pairs.", "labels": [], "entities": []}, {"text": "Both the neighborhood relation and the macro vertex assignment yield noticeably better results than the baseline.", "labels": [], "entities": []}, {"text": "In accordance with earlier remarks, the curves confirm that the semantical significance of the hierarchy is much higher on the lower levels.", "labels": [], "entities": []}, {"text": "While the first and the second level appear to exhibit a large amount of meaningful relations, the higher levels are not much better than the baseline.: Evaluation of similarity.", "labels": [], "entities": []}, {"text": "The curves represent the probability density of finding a certain Wu-Palmer similarity between the respective words.", "labels": [], "entities": []}, {"text": "The baseline peaks at (0,6.8) but is cutoff for clarity of the other curves.", "labels": [], "entities": [{"text": "clarity", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9817754030227661}]}], "tableCaptions": [{"text": " Table 1: General properties of the NH of the  word2vec dataset. In the third row, the aver- age number of words per cluster is given. See sec- tion 5.1 for definitions of the other quantities.", "labels": [], "entities": [{"text": "word2vec dataset", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.7969828248023987}]}]}