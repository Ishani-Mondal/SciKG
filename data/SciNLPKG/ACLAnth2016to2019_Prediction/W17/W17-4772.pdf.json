{"title": [{"text": "LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task", "labels": [], "entities": [{"text": "WMT17 Automatic Post-Editing Task", "start_pos": 27, "end_pos": 60, "type": "DATASET", "confidence": 0.7598557323217392}]}], "abstractContent": [{"text": "This paper presents the LIG-CRIStAL submission to the shared Automatic Post-Editing task of WMT 2017.", "labels": [], "entities": [{"text": "LIG-CRIStAL", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.8563023209571838}, {"text": "WMT 2017", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.7689570486545563}]}, {"text": "We propose two neural post-editing models: a mono-source model with a task-specific attention mechanism, which performs particularly well in a low-resource scenario; and a chained architecture which makes use of the source sentence to provide extra context.", "labels": [], "entities": []}, {"text": "This latter architecture manages to slightly improve our results when more training data is available.", "labels": [], "entities": []}, {"text": "We present and discuss our results on two datasets (en-de and de-en) that are made available for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "It has become quite common for human translators to use machine translation (MT) as a first step, and then to manually post-edit the translation hypothesis.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8500093698501587}]}, {"text": "This can result in a significant gain of time, compared to translating from scratch (.", "labels": [], "entities": []}, {"text": "Such translation workflows can result in the production of new training data, that maybe re-injected into the system in order to improve it.", "labels": [], "entities": []}, {"text": "Common ways to do so are retraining, incremental training, translation memories, or automatic postediting (.", "labels": [], "entities": [{"text": "translation memories", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.8490866124629974}]}, {"text": "In Automatic Post-Editing (APE), the MT system is usually considered as a blackbox: a separate APE system takes as input the outputs of this MT system, and tries to improve them.", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.62980055809021}, {"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9335998892784119}]}, {"text": "Statistical PostEditing (SPE) was first proposed by.", "labels": [], "entities": [{"text": "Statistical PostEditing (SPE)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7253139317035675}]}, {"text": "It consists in training a Statistical Machine Translation (SMT) system (, to translate from translation hypotheses to a human post-edited version of those.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.808904747168223}]}, {"text": "then proposed away to integrate both the translation hypothesis and the original (source language) sentence.", "labels": [], "entities": []}, {"text": "More recent contributions in the same vein are (.", "labels": [], "entities": []}, {"text": "When too little training data is available, one may resort to using synthetic corpora: with simulated PE, or round-trip translation.", "labels": [], "entities": [{"text": "PE", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.925840437412262}]}, {"text": "Recently, with the success of Neural Machine Translation (NMT) models, new kinds of APE methods have been proposed that use encoder-decoder approaches, in which a Recurrent Neural Network (RNN) encodes the source sequence into a fixed size representation (encoder), and another RNN uses this representation to output anew sequence.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.860991487900416}]}, {"text": "These encoder-decoder models are generally enhanced with an attention mechanism, which learns to look at the entire sequence of encoder states (.", "labels": [], "entities": []}, {"text": "We present novel neural architectures for automatic post-editing.", "labels": [], "entities": []}, {"text": "Our models learn to generate sequences of edit operations, and use a taskspecific attention mechanism which gives information about the word being post-edited.", "labels": [], "entities": []}], "datasetContent": [{"text": "This year's APE task consists in two sub-tasks: a task on English to German post-editing in the IT domain (en-de), and a task on German to English post-editing in the medical domain (de-en).", "labels": [], "entities": [{"text": "APE task", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.8863404989242554}]}, {"text": "gives the size of each of the corpora available.", "labels": [], "entities": []}, {"text": "The goal of both tasks is to minimize the HTER () between our automatic post-editing output, and the human post-editing output.", "labels": [], "entities": [{"text": "HTER", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9974337220191956}]}, {"text": "The en-de 23k training set is a concatenation of last year's 12k dataset, and a newly released 11k dataset.", "labels": [], "entities": [{"text": "en-de 23k training set", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.861626461148262}]}, {"text": "A synthetic corpus was built and used by the winner of last year's edition, and is available this year as additional data (500k and 4M corpora).", "labels": [], "entities": []}, {"text": "For the en-de task, we limit our use of external data to the 500k corpus.", "labels": [], "entities": []}, {"text": "For the de-en task, we built our own synthetic corpus, using a technique similar to).", "labels": [], "entities": []}, {"text": "We trained mono-source forced models, as well as chained models for both APE tasks.", "labels": [], "entities": [{"text": "APE tasks", "start_pos": 73, "end_pos": 82, "type": "TASK", "confidence": 0.7960287928581238}]}, {"text": "We also trained mono-source models with a global attention mechanism, similar to (Libovick\u00b4yLibovick\u00b4y et al., 2016) as a measure of comparison to our forced models.", "labels": [], "entities": []}, {"text": "For en-de, we trained two sets of models (with the same configuration) on the 12k train set (to compare with 2016 competitors), and on the new (23k) train set.", "labels": [], "entities": []}, {"text": "The encoders are bidirectional LSTMs of size 128.", "labels": [], "entities": []}, {"text": "The embeddings have a size of 128.", "labels": [], "entities": []}, {"text": "The first state of the decoder is initialized with the last state of the forward encoder (after a non-linear transformation with dropout).", "labels": [], "entities": []}, {"text": "Teacher forcing is used during training (instead of feeding the previous generated output to the decoder, we feed the ground truth).", "labels": [], "entities": []}, {"text": "Like, there is a maxout layer before the final projection.", "labels": [], "entities": []}, {"text": "We train our models with pure SGD with a batch size of 32, and an initial learning rate of 1.0.", "labels": [], "entities": [{"text": "SGD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9244896173477173}]}, {"text": "We decay the learning rate by 0.8 every epoch for the models trained with real PE data, and by 0.5 every half epoch for the models that use additional synthetic data.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9545778334140778}]}, {"text": "The models are evaluated periodically on a dev set, and we save checkpoints for the best TER scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9952729344367981}]}, {"text": "We manually stop training when TER scores on the dev set stop decreasing, and use the best checkpoint for evaluation on the test set (after about 50k steps for the small training sets, and 120k steps for the larger ones).", "labels": [], "entities": [{"text": "TER", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9944500923156738}]}, {"text": "Unlike Junczys-Dowmunt and Grundkiewicz (2016), we do not use subword units, as we found them not to be beneficial when predicting edit operations.", "labels": [], "entities": []}, {"text": "For the larger datasets, our vocabularies are limited to the 30,000 most frequent symbols.", "labels": [], "entities": []}, {"text": "Our implementation uses, and runs on a single GPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of each available corpus (number of  SRC, MT, PE sentence tuples).", "labels": [], "entities": []}, {"text": " Table 2: Top 8 edit ops in the target side of the  training set for en-de (top), and most generated  edit ops by our primary (500k + 23k) system on  dev set (bottom).", "labels": [], "entities": []}, {"text": " Table 3: Results on the en-de task. The SPE results are those provided by the organizers of the task (SMT  system). The AMU system is the winner of the 2016 APE task", "labels": [], "entities": [{"text": "AMU", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.8514499068260193}, {"text": "APE task", "start_pos": 158, "end_pos": 166, "type": "TASK", "confidence": 0.7262537777423859}]}, {"text": " Table 4: Results on the de-en task. Because the test set was not available before submission, we used  a small part (1000 tuples) of the training set as a train-dev set. This set was used for selecting the best  models, while the provided dev set was used for final evaluation of our models. The 500k + 24k corpus  is a concatenation of our synthetic corpus with the 24k corpus oversampled 10 times.", "labels": [], "entities": []}, {"text": " Table 5: Top 8 edit ops in the target side of the training set for de-en (left), and most generated edit ops  by our primary system on train-dev (right).", "labels": [], "entities": []}]}