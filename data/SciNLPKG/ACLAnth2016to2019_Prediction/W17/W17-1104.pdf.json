{"title": [{"text": "Aligning Entity Names with Online Aliases on Twitter", "labels": [], "entities": [{"text": "Aligning Entity Names", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9207642078399658}]}], "abstractContent": [{"text": "This paper presents new models that automatically align online aliases with their real entity names.", "labels": [], "entities": []}, {"text": "Many research applications rely on identifying entity names in text, but people often refer to entities with unexpected nicknames and aliases.", "labels": [], "entities": []}, {"text": "For example, The King and King James are aliases for Lebron James, a professional basketball player.", "labels": [], "entities": [{"text": "The King and King James", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.6602051615715027}]}, {"text": "Recent work on entity linking attempts to resolve mentions to knowledge base entries, like a wikipedia page, but linking is unfortunately limited to well-known entities with pre-built pages.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.724960133433342}]}, {"text": "This paper asks a more basic question: can aliases be aligned without background knowledge of the entity?", "labels": [], "entities": []}, {"text": "Further, can the semantics surrounding alias mentions be used to inform alignments?", "labels": [], "entities": []}, {"text": "We describe statistical models that make decisions based on the lexicographic properties of the aliases with their semantic context in a large corpus of tweets.", "labels": [], "entities": []}, {"text": "We experiment on a database of Twitter users and their usernames, and present the first human evaluation for this task.", "labels": [], "entities": []}, {"text": "Alignment accuracy approaches human performance at 81%, and we show that while lexico-graphic features are most important, the semantic context of an alias further improves classification accuracy.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9154664874076843}, {"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8544518947601318}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.6989765167236328}]}], "introductionContent": [{"text": "A wide range of research in natural language processing focuses on entities.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6750419934590658}]}, {"text": "These range from basic language tasks like coreference resolution to broader aggregation applications like sentiment analysis and information extraction.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.9672656655311584}, {"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9611943960189819}, {"text": "information extraction", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8503987789154053}]}, {"text": "Building an accurate picture of an entity (e.g., aggregate sentiment toward the entity, entity tracking across websites, database population) requires an understanding of all the varying ways people refer to that entity.", "labels": [], "entities": [{"text": "entity tracking across websites", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7911361530423164}]}, {"text": "Tracking \"facebook\" is not enough to know how people feel about it, as mentions of \"fbook\", \"FB\", and \"the book\" also need to be understood.", "labels": [], "entities": []}, {"text": "Although many applications exist for tracking known mentions of entities, less research exists for detecting nicknames and aliases.", "labels": [], "entities": []}, {"text": "This paper presents new models to align an entity's name (e.g., \"Bank of America\") with its online aliases (\"BAmerica\") and nicknames (\"BofA\").", "labels": [], "entities": [{"text": "Bank of America\")", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.8279570043087006}]}, {"text": "Unlike the traditional entity linking task that relies on known knowledge base (KB) entries, our task is unique by removing the assumption that a KB is available for each entity.", "labels": [], "entities": [{"text": "entity linking task", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8080018758773804}]}, {"text": "Instead, we simply begin with an entity name and an alias, and ask if the two are likely to refer to the same real-world entity.", "labels": [], "entities": []}, {"text": "By asking this more basic question first, several research threads will benefit.", "labels": [], "entities": []}, {"text": "For instance, aligning entity names is important to sentiment analysis, but typically ignored for simplification reasons.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9500308632850647}]}, {"text": "Companies track social media for mentions of their company in hopes of identifying the public sentiment toward them.", "labels": [], "entities": []}, {"text": "Political races rely on similar models, tracking mentions of politicians (\"Trump\" might be negatively referred to as \"Frump\").", "labels": [], "entities": []}, {"text": "Research on contextual sentiment analysis has exploded as a result, but the vast majority assumes a single known entity name.", "labels": [], "entities": [{"text": "contextual sentiment analysis", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7965515851974487}]}, {"text": "In fact, this paper's work originally came about because the authors wanted to track events surrounding 'Bank of America', but we kept coming across unexpected new aliases that referred to the company.", "labels": [], "entities": [{"text": "Bank of America'", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.8207649141550064}]}, {"text": "Another application is user profiling across websites.", "labels": [], "entities": [{"text": "user profiling across websites", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.8117787837982178}]}, {"text": "User accounts that span multiple websites often use different usernames.", "labels": [], "entities": []}, {"text": "Most research in this area has focused on aligning account attributes and graph structure.", "labels": [], "entities": []}, {"text": "This paper con-tributes by first addressing the more basic challenge of username alignment.", "labels": [], "entities": [{"text": "username alignment", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8201068341732025}]}, {"text": "Finally, this paper also furthers research in event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.9477459490299225}]}, {"text": "If a subset of users on Twitter are talking about a Katy Perry concert next week, the task is to extract the date and artist.", "labels": [], "entities": []}, {"text": "However, are they referring to the same concert when other users mention Fruit Sister?", "labels": [], "entities": []}, {"text": "Still others might discuss Katey Parry (a misspelling) and Katherine Hudson (previous name)?", "labels": [], "entities": []}, {"text": "Despite the popularity of this artist, some of these names don't exist in preconstructed KBs.", "labels": [], "entities": []}, {"text": "The challenge is exacerbated when the target artist is relatively unknown.", "labels": [], "entities": []}, {"text": "This paper experiments with new learning models to align examples like these using only the corpus in which they appear.", "labels": [], "entities": []}, {"text": "The first set of models rely on purely lexicographic characteristics.", "labels": [], "entities": []}, {"text": "We propose a series of character and word-based features, trained with discriminative classifiers.", "labels": [], "entities": []}, {"text": "Many aliases share obvious characteristics, such as acronym usage and word shortening.", "labels": [], "entities": [{"text": "acronym usage", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8155086934566498}, {"text": "word shortening", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.6708991825580597}]}, {"text": "These models learn the patterns used when people create nicknames.", "labels": [], "entities": []}, {"text": "The second set of models take a distributional semantics approach.", "labels": [], "entities": []}, {"text": "Names like fruit sister and katy perry have no obvious lexical overlap, so the task of aligning the aliases is impossible without understanding their usage in language.", "labels": [], "entities": []}, {"text": "We first present experiments with distributional word vectors to represent the context of aliases, and then measure vector similarity to inform the alignment decision.", "labels": [], "entities": []}, {"text": "We then round off the contextual approach with word embeddings from recent neural network research in NLP.", "labels": [], "entities": []}, {"text": "To our knowledge, these are the first machine learned models that align entity names without prior knowledge of the entities.", "labels": [], "entities": []}, {"text": "Further, we describe the first human study to measure task difficulty and compare model performance.", "labels": [], "entities": []}, {"text": "The lexical and semantic models approach human performance on the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main experiment dataset is a list of name/alias pairs.", "labels": [], "entities": []}, {"text": "shows a few examples of these pairs.", "labels": [], "entities": []}, {"text": "The list is comprised of approximately 110k pairs of names and their actual twitter handles extracted from a single day of tweets in November 2015.", "labels": [], "entities": []}, {"text": "We selected 110k tweets, and paired the name listed on the profile of the user who wrote the tweet with the same user's twitter handle.", "labels": [], "entities": []}, {"text": "This name/handle pair is a single datum in the dataset.", "labels": [], "entities": []}, {"text": "We then generated another 110k false pairs by randomly selecting twitter handles and matching them with incorrect profile names.", "labels": [], "entities": []}, {"text": "Combined with the correct 110k pairs, the resulting dataset is 220k name/alias pairs, half of which are correct and half incorrect.", "labels": [], "entities": []}, {"text": "This list is then broken up into 160k pairs for training, 40k fora held-out test set, and 20k fora development set.", "labels": [], "entities": []}, {"text": "Finally, we remove all pairs in the test set that contain a username or a handle that also appears in the training set.", "labels": [], "entities": []}, {"text": "This avoids all overlap between train and test.", "labels": [], "entities": []}, {"text": "Avery minor reduction in test set size resulted from this.", "labels": [], "entities": []}, {"text": "Since our experiments rely on corpus-based features, we use one year of tweets from the freely available Twitter streaming API from Aug 2014 to Aug 2015.", "labels": [], "entities": []}, {"text": "We refer to this corpus later as our \"one-year tweet corpus\".", "labels": [], "entities": []}, {"text": "We focus on the basic task of predicting whether an alias belongs to a name, given only a corpus of tweets and no other entity knowledge.", "labels": [], "entities": [{"text": "predicting whether an alias", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.8464713990688324}]}, {"text": "Experiments use the name/alias pairs as described above in Datasets.", "labels": [], "entities": []}, {"text": "Given a name/alias pair, the task is to predict \"yes\" or \"no\" to whether the two mentions likely refer to the same entity.", "labels": [], "entities": []}, {"text": "As a binary prediction task, the random baseline is 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9986085295677185}]}, {"text": "Each name in the dataset appears in both one correct pair with its true twitter handle, and one incorrect pair with a randomly selected twitter handle.", "labels": [], "entities": []}, {"text": "We use accuracy as the evaluation metric with its normal definition: where N is the size of the evaluation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9992448091506958}]}, {"text": "We report accuracy on the entire evaluation set (Accuracy: all) as well as a subset of the evaluation that includes only entity pairs such that the entity name and the twitter handle were each seen at least 100 times in the one-year twitter corpus (Accuracy: 100).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994447827339172}, {"text": "Accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9929112792015076}, {"text": "Accuracy", "start_pos": 249, "end_pos": 257, "type": "METRIC", "confidence": 0.9863942265510559}]}, {"text": "This second set serves the purpose of distinguishing the importance of frequency when using semantic vector features.", "labels": [], "entities": []}, {"text": "Entity mentions that rarely occur have sparse vectors, and a prediction relies solely on the lexicographic features.", "labels": [], "entities": [{"text": "Entity", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9249615669250488}]}, {"text": "The features in the models were developed on the training and development sets only.", "labels": [], "entities": []}, {"text": "We report on several feature ablation tests on the development set.", "labels": [], "entities": []}, {"text": "Feature ablation was not conducted on the test set.", "labels": [], "entities": [{"text": "Feature ablation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6153160035610199}]}, {"text": "The test set was only used to generate the final results table.", "labels": [], "entities": []}, {"text": "Both SVM and MaxEnt models used the default settings in CoreNLP, but we only report MaxEnt results as neither significantly outperformed the other.", "labels": [], "entities": []}, {"text": "Four baseline models are included to illustrate the non-trivial nature of this task.", "labels": [], "entities": []}, {"text": "At first blush, it may appear that this paper's focus is a trivial string match.", "labels": [], "entities": []}, {"text": "Part of the motivation for this paper's focus is to illustrate how even the most basic of username mapping tasks is non-trivial.", "labels": [], "entities": [{"text": "username mapping tasks", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7974215149879456}]}, {"text": "The first baseline, Simple-Match, simply lowercases and removes white space from both the name and alias.", "labels": [], "entities": []}, {"text": "If the two changed strings now match exactly, then the baseline predicts match.", "labels": [], "entities": [{"text": "predicts match", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.6969017386436462}]}, {"text": "The second baseline, Alpha-Match, is a variation of Simple-Match, but also removes all characters not in the a-z alphabet (e.g., 'david' and '88david-2' match).", "labels": [], "entities": []}, {"text": "The third baseline, Alpha-RelaxMatch relaxes AlphaMatch by only requiring the first 5 characters in both name and alias to match.", "labels": [], "entities": [{"text": "AlphaMatch", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.786605954170227}]}, {"text": "Finally, the fourth baseline is a machine learned model using only the edit distance feature (Edit-Dist) on lowercased and white-space condensed strings.", "labels": [], "entities": []}, {"text": "Finally, we ran a human evaluation to measure ideal performance on this task.", "labels": [], "entities": []}, {"text": "We randomly selected 2,010 pairs from the bigger test set, and several undergraduates were asked to judge whether each name/alias pair was likely or not to be the same entity.", "labels": [], "entities": []}, {"text": "We ran our best models on this same smaller test set and report accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9483126997947693}]}, {"text": "shows results on the development set for the basic model with additional characterbased features.", "labels": [], "entities": []}, {"text": "The Simple-Match baseline performs surprisingly low at 57.66%, Alpha-Match slightly better, and Alpha-RelaxMatch the best baseline at 69.57%.", "labels": [], "entities": []}, {"text": "Entity names and their twitter handles are not often clear matches.", "labels": [], "entities": []}, {"text": "The machine learned baseline that uses only edit distance somewhat surprising barely performs better than random chance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Development set results and feature com- parison. Numbers are % accuracy: 81.6 and 70.9", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9976058006286621}]}, {"text": " Table 3: Word vector accuracy on the development  set. Each row is the feature by itself without the  other vector features. The final row is the inclusion  of all three features in one learned model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9684106111526489}]}, {"text": " Table 4: Word vector accuracy results on the Test  set. All features are included.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8639267683029175}, {"text": "Test  set", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.8442953526973724}]}, {"text": " Table 5: Accuracy on the Test set when adding  word embedding features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983105659484863}]}, {"text": " Table 6: Precision and Recall on the Test set for  correctly identifying alignment pairs.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9980417490005493}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9989215135574341}]}, {"text": " Table 7: Human evaluation comparison on a sepa- rate test set of approximately 2000 pairs.", "labels": [], "entities": []}]}