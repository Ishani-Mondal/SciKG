{"title": [{"text": "Enhancing Automatic ICD-9-CM Code Assignment for Medical Texts with PubMed", "labels": [], "entities": [{"text": "Enhancing Automatic ICD-9-CM Code Assignment", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6189933538436889}, {"text": "PubMed", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.7874670028686523}]}], "abstractContent": [{"text": "Assigning a standard ICD-9-CM code to disease symptoms in medical texts is an important task in the medical domain.", "labels": [], "entities": []}, {"text": "Automating this process could greatly reduce the costs.", "labels": [], "entities": []}, {"text": "However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by unbalanced training data.", "labels": [], "entities": [{"text": "ICD-9-CM code classifier", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6865857640902201}]}, {"text": "Frequent diseases often have more training data, which helps its classification to perform better than that of an infrequent disease.", "labels": [], "entities": []}, {"text": "However , a diseases frequency does not necessarily reflect its importance.", "labels": [], "entities": []}, {"text": "To resolve this training data shortage problem, we propose to strategically draw data from PubMed to enrich the training data when there is such need.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.9799550175666809}]}, {"text": "We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classi-fiers' performance at the macro-averaging level.", "labels": [], "entities": [{"text": "CMC dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9647260308265686}]}], "introductionContent": [], "datasetContent": [{"text": "We validate our methods on the official dataset of the 2007 Computational Medicine Challenge (CMC dataset), collected by Cincinnati Children's Hospital Medical Center (, which is frequently used by researchers working on the ICD-9-CM code assignment task.", "labels": [], "entities": [{"text": "official dataset of the 2007 Computational Medicine Challenge (CMC dataset)", "start_pos": 31, "end_pos": 106, "type": "DATASET", "confidence": 0.8083750903606415}, {"text": "Cincinnati Children's Hospital Medical Center", "start_pos": 121, "end_pos": 166, "type": "DATASET", "confidence": 0.8934657474358877}, {"text": "ICD-9-CM code assignment task", "start_pos": 225, "end_pos": 254, "type": "TASK", "confidence": 0.7725732922554016}]}, {"text": "The CMC dataset consists of training and testing dataset, but only training dataset is accessible for us.", "labels": [], "entities": [{"text": "CMC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9752916693687439}, {"text": "training dataset", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.7632315754890442}]}, {"text": "Fortunately, most studies publish their system performance on both training and testing dataset, and then we can compare our methods with stateof-art methods.", "labels": [], "entities": []}, {"text": "This corpus consists of 978 radiological reports taken from real medical records, and each report has been manually labeled with ICD-9-CM codes by professional companies.", "labels": [], "entities": [{"text": "ICD-9-CM", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.6763508319854736}]}, {"text": "The example in comes from this dataset.", "labels": [], "entities": []}, {"text": "In total, there are 45 ICD-9-CM codes appearing in the CMC dataset, and each report is labeled with one or more ICD-9-CM codes.", "labels": [], "entities": [{"text": "CMC dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9690312147140503}]}, {"text": "This is a very imbalanced collection, with around half codes having less than 10 training data (see).", "labels": [], "entities": []}, {"text": "Following the past studies (, we evaluate the classification performance through a micro F1 score (i.e., sum of the individual classification performance and divided by the individual amount) and a macro F1 score (i.e., sum of the classifiers performance and divided by the classifiers amount).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.945960521697998}, {"text": "F1 score", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9203133285045624}]}, {"text": "We expect that by alleviating the data imbalance problem, macro F1 scores can increase significantly.", "labels": [], "entities": [{"text": "macro F1 scores", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.7032579878966013}]}, {"text": "All experiments in this study have gone through 10-fold cross validation, because it can provide a reliable result when data is limited (Witten et al., 2016).  with ICD code official description In the first experiment, supplementary data is collected based on ICD-9-CM code official description, as described in method I.", "labels": [], "entities": [{"text": "ICD-9-CM code official description", "start_pos": 261, "end_pos": 295, "type": "DATASET", "confidence": 0.9046496897935867}]}, {"text": "The supplementing document size is set to be 10, 20, 40 and 60.", "labels": [], "entities": []}, {"text": "Supplementary training data is added to 24 classifiers in Group 1.", "labels": [], "entities": [{"text": "Supplementary", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9536097049713135}]}, {"text": "We name these two new runs as Group 1 Description LR (G1 desc LR) and Group 1 Description SVM (G1 desc SVM), appended with supplementary data size.", "labels": [], "entities": []}, {"text": "The results in also show that supplementing 10 documents can generate best performance, and with more documents added, both macro and micro F1 will decrease.: Enhance classifiers in Group 1 with supplementary data collected with method I, while the evaluation is performed on all classes.", "labels": [], "entities": [{"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.6401910185813904}]}, {"text": "In this second experiment, we collect PubMed data through the ICD-9-CM code's both official and synonyms description that appears in CMC dataset.", "labels": [], "entities": [{"text": "PubMed data", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9120372235774994}, {"text": "ICD-9-CM code", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9573943614959717}, {"text": "CMC dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9382418394088745}]}, {"text": "We name these two runs as Group 1 Synonym LR (G1 syn LR) and Group 1 Synonym SVM (G1 syn SVM).", "labels": [], "entities": []}, {"text": "Due to the paper size limitation, here we only show the best results with supplementary document size being 10 in.", "labels": [], "entities": []}, {"text": "Through Wilcoxon Signed Ranks test, G1 syn SVM 10 significantly outperforms baseline (p \u2212 value < 0.01), but has no significant difference compared with G1 desc SVM 10.", "labels": [], "entities": []}, {"text": "However, if only classifiers in Group 1 are considered, G1 syn SVM 10 significantly outperforms G1 desc SVM 10 (p \u2212 value < 0.01).", "labels": [], "entities": []}, {"text": "This indicates that our propose method II can generate effective supplementary training data.", "labels": [], "entities": []}, {"text": "On the other hand, G1 syn LR 10 is found to outperform Baseline LR significantly only on Group 1 classes (p \u2212 value < 0.01).", "labels": [], "entities": []}, {"text": "In the third experiment, we add supplementary data to all 45 classifiers to explore whether adding supplementary data to the classifiers that originally have sufficient training data still can gain performance improvement.", "labels": [], "entities": []}, {"text": "We name these two runs as All Synonym LR (All syn LR) and All Synonym SVM (All syn SVM).", "labels": [], "entities": []}, {"text": "Also, only the best results with supplementary document size being 10 is shown in.", "labels": [], "entities": []}, {"text": "Through Wilcoxon Signed Ranks test, All syn SVM 10 significantly outperforms the baseline, and All syn LR 10 significantly outperforms the baseline only on Group 1 (p \u2212 value < 0.01), but both have no significant difference with G1 syn and G1 desc.", "labels": [], "entities": []}, {"text": "These means that adding supplementary training data is effective on solving data imbalance problem, but for the classifiers that originally have sufficient training data, extra training data seems have no significant effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Enhance classifiers in Group 1 with sup- plementary data collected with method I, while the  evaluation is performed on all classes.", "labels": [], "entities": []}, {"text": " Table 3: Experiment results.  \u2020means significantly  outperform baseline.  \u2021means significant outper- form baseline on Group 1.", "labels": [], "entities": []}, {"text": " Table 3. Through Wilcoxon  Signed Ranks test, All syn SVM 10 significantly  outperforms the baseline, and All syn LR 10 sig- nificantly outperforms the baseline only on Group  1 (p \u2212 value < 0.01), but both have no signifi- cant difference with G1 syn and G1 desc. These  means that adding supplementary training data is  effective on solving data imbalance problem, but  for the classifiers that originally have sufficient  training data, extra training data seems have no", "labels": [], "entities": []}]}