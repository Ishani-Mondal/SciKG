{"title": [{"text": "Evaluation of word embeddings against cognitive processes: primed reaction times in lexical decision and naming tasks", "labels": [], "entities": [{"text": "lexical decision and naming tasks", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.6637543201446533}]}], "abstractContent": [{"text": "This work presents a framework for word similarity evaluation grounded on cogni-tive sciences experimental data.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8362585703531901}]}, {"text": "Word pair similarities are compared to reaction times of subjects in large scale lexical decision and naming tasks under semantic priming.", "labels": [], "entities": [{"text": "large scale lexical decision and naming tasks", "start_pos": 69, "end_pos": 114, "type": "TASK", "confidence": 0.7619972569601876}]}, {"text": "Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a training corpus is less important than that of the algorithm.", "labels": [], "entities": []}, {"text": "Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply word related-ness or similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representations have attracted a lot of interest in the community and led to very useful applications in a range of domains of natural language processing.", "labels": [], "entities": [{"text": "Word representations", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6760658472776413}]}, {"text": "Such representations are typically evaluated intrinsically on word similarity tasks and extrinsically on their impact on NLP systems performance (.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.7664580146471659}]}, {"text": "A recent trend towards building more general representations has looked at how similarities in the representation space can predict the outcome of cognitive experiments, such as human reaction time in semantic priming experiments or relying on eye tracking and brain imaging data).", "labels": [], "entities": []}, {"text": "The idea is that ground truth from unconscious phenomena might be less prone to subjective factors of more traditional word similarity and relatedness datasets.", "labels": [], "entities": []}, {"text": "In this paper, we describe an evaluation framework based on comparing word embedding similarity against reaction times from the Semantic Priming Project (.", "labels": [], "entities": []}, {"text": "A set of word embeddings is evaluated by computing its Spearman rank correlation with average reaction times obtained by submitting a set of subjects to a prime (one word from the pair) and then perform one of two tasks: lexical decision (decide whether the second word is an existing word or not), and naming (read aloud the second word).", "labels": [], "entities": [{"text": "naming", "start_pos": 303, "end_pos": 309, "type": "TASK", "confidence": 0.9576566815376282}]}, {"text": "Extending the ideas developed in, this paper describes the following contributions: \u2022 we create and distribute a package 1 for word embedding evaluation based on the SPP primed reaction time data; \u2022 in order to calibrate results from that evaluation framework, we look at the effect of training corpus on a set of word embeddings; \u2022 we also look at the correlation between SPP reaction times and subjective similarity and relatedness ratings from existing datasets.", "labels": [], "entities": [{"text": "word embedding evaluation", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.6902073125044504}, {"text": "SPP primed reaction time data", "start_pos": 166, "end_pos": 195, "type": "DATASET", "confidence": 0.5112886130809784}]}], "datasetContent": [{"text": "Embeddings are evaluated by computing the cosine similarity between word pairs from the SPP project, and look at their Spearman rank correlation with the RT data.", "labels": [], "entities": [{"text": "SPP project", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.7136470377445221}, {"text": "Spearman rank correlation", "start_pos": 119, "end_pos": 144, "type": "METRIC", "confidence": 0.6654919882615408}, {"text": "RT data", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.7208317369222641}]}, {"text": "The results are given in term of negative correlation.", "labels": [], "entities": []}, {"text": "Significance of the difference between correlations is calculated with the Steiger test.", "labels": [], "entities": []}, {"text": "In this evaluation framework, the word pairs for the Lexical Decision Task (LDT) and Naming Task (NT) are split according to two partitions.", "labels": [], "entities": []}, {"text": "The first one (P1), used here, consists of a development set of 1,328 pairs and a test set of 5,309 pairs.", "labels": [], "entities": []}, {"text": "Parameters of the proposed approaches can be tuned on the development set and performance must be reported on the test set.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9715331792831421}]}, {"text": "Another partition (P2) is made available to also include data for training algorithms.", "labels": [], "entities": []}, {"text": "The Train set consists of 3,981 pairs, the Dev has 1,328 pairs and the Test has 1,328 pairs.", "labels": [], "entities": [{"text": "Train set", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8372739255428314}]}, {"text": "These partitions were obtained by using 10 folds which are also available.", "labels": [], "entities": []}, {"text": "We didn't create them with a particular goal in mind but we made sure that the mean word frequency and the standard deviation (SD) of each fold was close to the mean word frequency and SD of the full dataset.", "labels": [], "entities": [{"text": "standard deviation (SD)", "start_pos": 107, "end_pos": 130, "type": "METRIC", "confidence": 0.9531752467155457}, {"text": "SD", "start_pos": 185, "end_pos": 187, "type": "METRIC", "confidence": 0.9214513301849365}]}, {"text": "By standardizing the data splits, we ensure that results presented in future work will be comparable.", "labels": [], "entities": []}, {"text": "Experiments Ina first set of experiments, we benchmark a range of embeddings on the SPP data.", "labels": [], "entities": [{"text": "SPP data", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.8394865691661835}]}, {"text": "Four conditions are considered: LDT-200, LDT-1200, NT-200, NT-1200 (for lexical decision task, and naming task, both with an onset of 200 ms and 1,200 ms).", "labels": [], "entities": [{"text": "naming task", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.8912384808063507}, {"text": "onset", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9547628164291382}]}, {"text": "We compare two categories of embeddings: a controlled setting for which algorithms are trained on the same dataset, and a selection of pretrained embeddings available to the community.", "labels": [], "entities": []}, {"text": "Due to space constraints, the pretrained embeddings considered 2 are limited to: W2V Skip-gram (,), Multilingual), Dependencybased ( , and FastText (.", "labels": [], "entities": [{"text": "W2V Skip-gram", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.7884481251239777}]}, {"text": "For the embeddings with controlled settings, we used two algorithms: W2V Skip-grams and GloVe.", "labels": [], "entities": [{"text": "W2V Skip-grams", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.7588083446025848}, {"text": "GloVe", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9505782723426819}]}, {"text": "We used three different corpora to train these embeddings: Wikipedia 2013 (Wiki), Gigaword 3 (GW) and OpenSubtitles 2016 (OS).", "labels": [], "entities": [{"text": "Wikipedia 2013 (Wiki)", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.9368620395660401}]}, {"text": "We used a centered window of size 10 and generated vectors with 100 dimensions for all 6 models.", "labels": [], "entities": []}, {"text": "From the experiment detailed in, it appears that GloVe leads to significantly larger negative correlation compared to other approaches, both on the controlled and pretrained settings.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9907822012901306}]}, {"text": "On the controlled setting, we notice that the choice of the corpora doesn't significantly affect the correlation.", "labels": [], "entities": []}, {"text": "Even if the correlation seems to be higher with the NT-200 and NT-1200 datasets when using the OpenSubtitles corpus, the Steiger test shows that the difference in correlation when using the other corpora is not significant inmost cases.", "labels": [], "entities": [{"text": "NT-200", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9741239547729492}, {"text": "NT-1200 datasets", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9175237119197845}, {"text": "OpenSubtitles corpus", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.9260141253471375}]}, {"text": "However, the algorithms used do have a significant impact on the correlations.", "labels": [], "entities": []}, {"text": "In the future, it would be interesting to look at the impact of various other settings such as the size and position of the window, or the dimension of the word vectors.", "labels": [], "entities": []}, {"text": "It can also noted that the correlations on the lexical decision tasks are higher than on the naming tasks which supports the idea that lexical decision: Spearman's correlation between reaction times from the SPP datasets and relationship scores from other datasets (with more than 16 overlapping pairs).", "labels": [], "entities": [{"text": "SPP datasets", "start_pos": 208, "end_pos": 220, "type": "DATASET", "confidence": 0.7280887365341187}]}, {"text": "seems to be a task less subject to variability from production of the response (pressing a button vs saying a word).", "labels": [], "entities": []}, {"text": "Better correlations at an onset of 200 ms can be explained by the fact that subjects are allowed more time to build an intent, leading to more factors being involved.", "labels": [], "entities": []}, {"text": "This also probably means that most word embedding models are better at capturing automatic priming mechanisms.", "labels": [], "entities": []}, {"text": "The second experiment detailed in looks at the characteristics of the SPP data in regard to other available datasets.", "labels": [], "entities": [{"text": "SPP data", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8970630764961243}]}, {"text": "We calculated the correlation between the reaction times in the SPP datasets and the relationship scores in a set of existing datasets, using the pairs of words that are available in the two compared datasets (different pairs are compared for each dataset).", "labels": [], "entities": [{"text": "SPP datasets", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.7951997816562653}]}, {"text": "We only show the results for datasets that have 16 or more pairs in common with the SPP dataset.", "labels": [], "entities": [{"text": "SPP dataset", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8554364740848541}]}, {"text": "The SimVerb dataset had 208 pairs in common but since the words used were only verbs whereas in the SPP dataset the part-of-speech wasn't specified, we couldn't really compare the two datasets.", "labels": [], "entities": [{"text": "SimVerb dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9339406788349152}, {"text": "SPP dataset", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9046030640602112}]}, {"text": "It can be observed that the correlations are low which probably means that evaluating on the SPP data could outline different phenomena than what is already covered by relatedness and similarity oriented datasets.", "labels": [], "entities": [{"text": "SPP data", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.8270123898983002}]}, {"text": "Additional work has to be done to fully understand what factors are taken into ac-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's correlation 4 between the test SPP datasets and word embedding models. Highest  results are in bold. Significativity of the figures compared to the best results according to Steiger test is  indicated by  *  (pval < 0.01) and  \u2020 (pval \u2208 [0.01, 0.05[).", "labels": [], "entities": []}, {"text": " Table 2: Spearman's correlation between reac- tion times from the SPP datasets and relation- ship scores from other datasets (with more than 16  overlapping pairs).", "labels": [], "entities": [{"text": "reac- tion times", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.8654971420764923}, {"text": "SPP datasets", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.7534727454185486}]}]}