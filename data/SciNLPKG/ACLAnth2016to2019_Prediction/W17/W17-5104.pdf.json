{"title": [{"text": "Improving Claim Stance Classification with Lexical Knowledge Expansion and Context Utilization", "labels": [], "entities": [{"text": "Improving Claim Stance Classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8918372541666031}]}], "abstractContent": [{"text": "Stance classification is a core component in on-demand argument construction pipelines.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8994503915309906}, {"text": "on-demand argument construction pipelines", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.7544503509998322}]}, {"text": "Previous work on claim stance classification relied on background knowledge such as manually-composed sentiment lexicons.", "labels": [], "entities": [{"text": "claim stance classification", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.9033249417940775}]}, {"text": "We show that both accuracy and coverage can be significantly improved through automatic expansion of the initial lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.99910968542099}, {"text": "coverage", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9558472037315369}]}, {"text": "We also developed a set of contextual features that further improves the state-of-the-art for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Debating technologies aim to help humans debate and make better decisions.", "labels": [], "entities": []}, {"text": "A core capability for these technologies is the on-demand construction of pro and con arguments fora given controversial topic.", "labels": [], "entities": []}, {"text": "Most previous work was aimed at detecting topic-dependent argument components, such as claims and evidence (.", "labels": [], "entities": []}, {"text": "Recently, introduced the related task of claim stance classification.", "labels": [], "entities": [{"text": "claim stance classification", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8583303689956665}]}, {"text": "For example, given the topic (1) The monarchy should be abolished. and the following two claims (2) Social traditions or hierarchies are essential for social order.", "labels": [], "entities": []}, {"text": "\u2295 \u21d4 (3) People feel greater dignity when choosing their head of state.", "labels": [], "entities": []}, {"text": "\u2295 \u21d4 the goal is to classify (2) as Con and as Pro with respect to (1).", "labels": [], "entities": []}, {"text": "Bar-Haim et al. proposed a model that breaks this task into several sub-tasks: (a) Identify the sentiment targets of the topic and the claim (b) Determine the sentiment of the topic and the claim towards their sentiment targets, and (c) Determine the relation between the targets.", "labels": [], "entities": []}, {"text": "Target A is consistent/contrastive with target B if the stance towards A implies the same/opposite stance towards B, respectively.", "labels": [], "entities": []}, {"text": "In (1)-(3), targets are marked in bold, positive/negative sentiment is indicated as \u2295/ and consistent/contrastive relation is marked as \u21d4/ \u21d4.", "labels": [], "entities": []}, {"text": "For instance, (3) has positive sentiment towards its target, choosing their head of state, which implies negative sentiment towards the monarchy, since the targets are contrastive.", "labels": [], "entities": []}, {"text": "The topic's sentiment towards the monarchy is also negative, hence it is a Pro claim.", "labels": [], "entities": []}, {"text": "On-demand argument generation is inherently an open-domain task, so one cannot learn topicspecific features for stance classification from the training data.", "labels": [], "entities": [{"text": "On-demand argument generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6120663384596506}, {"text": "stance classification", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.9168638586997986}]}, {"text": "Furthermore, claims are short sentences, and the number of claims in the training data is relatively small as compared to common sentiment analysis and stance classification benchmarks.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.8625860512256622}]}, {"text": "Consequently, external knowledge such as sentiment lexicons is crucial for this task.", "labels": [], "entities": []}, {"text": "However, the coverage of manually-constructed sentiment lexicons is often incomplete.", "labels": [], "entities": []}, {"text": "As reported by Bar-Haim et al., the sentiment lexicon they used was able to match sentiment terms in fewer than 80% of the claims.", "labels": [], "entities": []}, {"text": "Moreover, manually composed sentiment lexicons lack the notion of (numeric) sentiment strength.", "labels": [], "entities": []}, {"text": "A more general limitation of sentiment-based approaches is that some claims express stance but do not convey explicit sentiment.", "labels": [], "entities": []}, {"text": "As an example, consider the following Pro claim for (1): (4) The people, not the members of one family, should be sovereign.", "labels": [], "entities": []}, {"text": "In this work we present several improvements to the system of (henceforth, the baseline system), which address the above limitations.", "labels": [], "entities": []}, {"text": "First, we present a method for automatic expansion of a given sentiment lexicon, which leads to a substantial performance increase.", "labels": [], "entities": [{"text": "automatic expansion of a given sentiment lexicon", "start_pos": 31, "end_pos": 79, "type": "TASK", "confidence": 0.7108979736055646}]}, {"text": "Second, while the baseline system only considers the claim itself, we developed a set of contextual features that further boosts the performance of the system.", "labels": [], "entities": []}, {"text": "In particular, these contextual features allow classification of claims with no explicit sentiment.", "labels": [], "entities": [{"text": "classification of claims with no explicit sentiment", "start_pos": 47, "end_pos": 98, "type": "TASK", "confidence": 0.7560268640518188}]}, {"text": "Overall, we outperformed the best published results for this task by a large margin.", "labels": [], "entities": []}], "datasetContent": [{"text": "We followed the experimental setup of Bar-Haim et al., including the train/test split of the dataset and the evaluation measures, and predicted the majority class in the train set with a constant, very low confidence when the classifier's output was zero.", "labels": [], "entities": []}, {"text": "The training set contained 25 topics (1,039 claims), and the test set contained 30 topics (1,355 claims).", "labels": [], "entities": []}, {"text": "The evaluation explored the trade-off between accuracy (fraction of correct stance predictions) and coverage (fraction of claims for which we make a non-zero prediction).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.999243974685669}, {"text": "coverage", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9979138970375061}]}, {"text": "This tradeoff was controlled by setting a minimum confidence threshold for making a prediction.", "labels": [], "entities": []}, {"text": "Given a coverage level \u03b2, Accuracy@\u03b2 is defined as the maximal accuracy such that the corresponding coverage is at least \u03b2, found by exhaustive search over the threshold values.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999583899974823}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9267944097518921}, {"text": "coverage", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.964870810508728}]}, {"text": "Coverage and accuracy for each threshold are macro-averaged over the tested topics.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9896577000617981}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9983522891998291}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Rows (1-2) quote the two best-performing configurations reported by Bar-Haim et al.", "labels": [], "entities": []}, {"text": "The first is the baseline configuration used in this work, which performed best on lower coverage rates.", "labels": [], "entities": []}, {"text": "The second is a combination of the baseline system and an SVM with unigram features, which was the best performer on higher coverage rates.", "labels": [], "entities": []}, {"text": "Row 3 is our rerun of the baseline system.", "labels": [], "entities": []}, {"text": "The results are close to the EACL '17 results (row 1) but not identical.", "labels": [], "entities": [{"text": "EACL '17 results", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9125421941280365}]}, {"text": "This is due to some changes in low-level tools used by the system, such as the wikifier.", "labels": [], "entities": []}, {"text": "The configurations in rows 4-6 are the contributions of this work.", "labels": [], "entities": []}, {"text": "Row 4 reports the results for the baseline system with the expanded lexicon (Section 3).", "labels": [], "entities": []}, {"text": "Like the baseline system, this configuration only considers the claim itself.", "labels": [], "entities": []}, {"text": "The results show substantial improvements over the baseline (row 3), as well as the best previously reported results (rows 1-2).", "labels": [], "entities": []}, {"text": "The expanded lexicon increased the (macro-averaged) coverage of the system from 78.2% to 98.1%.", "labels": [], "entities": []}, {"text": "The next two configurations use increasingly richer contexts, in addition to using the expanded lexicon.", "labels": [], "entities": []}, {"text": "Row 5 shows the results for the classifier described in Section 4, using all the contextual features except for the neighboring claims feature.", "labels": [], "entities": []}, {"text": "We refer to this feature set as local contextual features.", "labels": [], "entities": []}, {"text": "The results show that these features achieve further improvement.", "labels": [], "entities": []}, {"text": "Last, row 6 shows the results for adding the neighboring claims feature, which achieves the best results.", "labels": [], "entities": []}, {"text": "This configuration requires additional knowledge about other claims in the proximity of the given claim.", "labels": [], "entities": []}, {"text": "While in this experiment the labeled data provides perfect knowledge about neighboring claims, in actual implementations of argument construction pipelines this information is obtained from the imperfect output of a claim detection module.", "labels": [], "entities": []}, {"text": "Overall, our results represent significant advancement of the state-of-the-art for this task, both for lower coverage rates (top predictions) and over the whole dataset (Accuracy@1.0).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9969134330749512}]}], "tableCaptions": [{"text": " Table 2: Stance classification results. Majority baseline Accuracy@1.0=51.9%", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.935380607843399}, {"text": "Majority baseline", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.9011725783348083}, {"text": "Accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8242619037628174}]}]}