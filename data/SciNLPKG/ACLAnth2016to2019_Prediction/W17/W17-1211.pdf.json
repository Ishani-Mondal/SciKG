{"title": [{"text": "Identifying dialects with textual and acoustic cues", "labels": [], "entities": [{"text": "Identifying dialects", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9217816591262817}]}], "abstractContent": [{"text": "We describe several systems for identifying short samples of Arabic or Swiss-German dialects, which were prepared for the shared task of the 2017 DSL Workshop (Zampieri et al., 2017).", "labels": [], "entities": []}, {"text": "The Arabic data comprises both text and acoustic files, and our best run combined both.", "labels": [], "entities": []}, {"text": "The Swiss-German data is text-only.", "labels": [], "entities": [{"text": "Swiss-German data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9233419597148895}]}, {"text": "Coincidently, our best runs achieved a accuracy of nearly 63% on both the Swiss-German and Ara-bic dialects tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9996683597564697}, {"text": "Swiss-German and Ara-bic dialects tasks", "start_pos": 74, "end_pos": 113, "type": "DATASET", "confidence": 0.8201067686080933}]}], "introductionContent": [{"text": "The 2017 Distinguishing Similar Languages Workshop sponsored four shared tasks, and our team participated in two of them: Arabic dialect identification, and Swiss-German dialect identification.", "labels": [], "entities": [{"text": "Distinguishing Similar Languages Workshop", "start_pos": 9, "end_pos": 50, "type": "TASK", "confidence": 0.8451575934886932}, {"text": "Arabic dialect identification", "start_pos": 122, "end_pos": 151, "type": "TASK", "confidence": 0.5932360291481018}, {"text": "Swiss-German dialect identification", "start_pos": 157, "end_pos": 192, "type": "TASK", "confidence": 0.5536997020244598}]}, {"text": "The Arabic dialect data includes Automatic Speech Recognition transcripts of broadcasts, as well as the most helpful audio features, which were provided as 400-dimensional I-vector files.", "labels": [], "entities": [{"text": "Automatic Speech Recognition transcripts", "start_pos": 33, "end_pos": 73, "type": "TASK", "confidence": 0.6968983113765717}]}, {"text": "The raw audio data was also available for download.", "labels": [], "entities": []}, {"text": "The Swiss-German data consists of transcripts only, transcribed to indicate pronunciation by human linguists.", "labels": [], "entities": [{"text": "Swiss-German data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8777753412723541}]}, {"text": "The training set for Arabic comprises 14000 lines, totaling 1.7MB, each line labeled for one of five dialect groups.", "labels": [], "entities": []}, {"text": "In addition, 1524 lines totaling 318KB of development data were also provided.", "labels": [], "entities": []}, {"text": "The test set is 1492 lines.", "labels": [], "entities": []}, {"text": "We did not use the IS2016 data or the varDial3 shared task data, which have similar characteristics, and might have improved the efficacy of training.", "labels": [], "entities": [{"text": "IS2016 data", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9726907312870026}, {"text": "varDial3 shared task data", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8612870424985886}]}, {"text": "For the three Arabic runs, we prepared six different text-based classifiers, and five wave-filebased classifiers, in addition to the two baseline word and I-vector systems, and combined them in two groups of four and one group of five classifiers.", "labels": [], "entities": []}, {"text": "Our best run on the Arabic test data has a weighted F1 score of 0.628; this run combined some of our classifiers with the provided svm multiclass baseline classifiers.", "labels": [], "entities": [{"text": "Arabic test data", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9111674626668295}, {"text": "F1 score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9823433458805084}]}, {"text": "The Swiss-German data consists of 14478 lines of data, totalling 700KB, labeled with one of four dialects.", "labels": [], "entities": [{"text": "Swiss-German data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8996705710887909}]}, {"text": "We divided this into a 13032 line training set, and two 723-line files for development.", "labels": [], "entities": []}, {"text": "The test set is 3638 lines.", "labels": [], "entities": []}, {"text": "Only two of the classifiers prepared for Arabic were deployed on the Swiss-German test data.", "labels": [], "entities": [{"text": "Swiss-German test data", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.922951320807139}]}, {"text": "Our best run on this data has an accuracy of 0.63 and a weighted F1 score of 0.61.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.999661922454834}, {"text": "F1 score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9854491651058197}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Classifier Accuracy on ADI Development  Data, Test Sest  Section Described  Dev. Set Test Set  3.1  0.48  3.2  0.57  3.4  0.44  3.3  0.52  3.5  0.63  3.6  0.52  0.32  3.7.2  0.40  3.7.3 (256 bigrams)  0.45  3.7.3 (2048 unigrams) 0.47  3.7.4  0.58  3.7.5  0.61  0.59", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9748753309249878}, {"text": "ADI Development  Data", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.7167807022730509}]}, {"text": " Table 2: 2017 versus 2016 ADI results  Team  F1 2017 F1 2016  unibuckernel  0.763  0.5131 6  MAZA  0.717  0.5132 7  tubasfs  0.697  0.472 8  ahaqst  0.628  0.426 9  qcri mit  0.616  -new- deepCybErNet 0.574  -new-", "labels": [], "entities": [{"text": "ADI results  Team  F1 2017 F1 2016  unibuckernel  0.763", "start_pos": 27, "end_pos": 82, "type": "DATASET", "confidence": 0.7958167195320129}]}, {"text": " Table 3: Performance of our merged classifiers  Run (Data) Accuracy F1 (mic) F1 (wt'd)  2 (Text)  0.3231  0.3231  0.3137  3 (Acoust.)  0.5932  0.5932  0.5861  1 (both)  0.6287  0.6287  0.628", "labels": [], "entities": [{"text": "Accuracy F1 (mic) F1", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.8465760946273804}]}, {"text": " Table 4: Participation of Swiss-German teams in  other tasks  Team  GDI DSL ADI  MAZA  1  2  CECL  2  1  CLUZH  3  qcri mit  4  5  unibuckernel  5  1  tubasfs  6  4  3  ahaqst  7  4  Citius Ixa Imaxin  8  9  XAC Bayesline  9  3  deepCybErNet  10  11  6", "labels": [], "entities": [{"text": "deepCybErNet", "start_pos": 230, "end_pos": 242, "type": "DATASET", "confidence": 0.8541198372840881}]}, {"text": " Table 5: AHAQST results on GDI task", "labels": [], "entities": [{"text": "AHAQST", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9606091976165771}]}]}