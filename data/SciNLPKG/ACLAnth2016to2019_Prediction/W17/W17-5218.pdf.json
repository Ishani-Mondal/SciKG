{"title": [{"text": "Towards an integrated pipeline for aspect-based sentiment analysis in various domains", "labels": [], "entities": [{"text": "aspect-based sentiment analysis", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.7240739365418752}]}], "abstractContent": [{"text": "This paper presents an integrated ABSA pipeline for Dutch that has been developed and tested on qualitative user feedback coming from three domains: retail, banking and human resources.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.9635894298553467}]}, {"text": "The two latter domains provide service-oriented data, which has not been investigated before in ABSA.", "labels": [], "entities": []}, {"text": "By performing in-domain and cross-domain experiments the validity of our approach was investigated.", "labels": [], "entities": [{"text": "validity", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9579336643218994}]}, {"text": "We show promising results for the three ABSA sub-tasks, aspect term extraction, aspect category classification and aspect polarity classification.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.8666032552719116}, {"text": "aspect term extraction", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7788238922754923}, {"text": "aspect category classification", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.7355698148409525}, {"text": "aspect polarity classification", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.7841114600499471}]}], "introductionContent": [{"text": "With the rise of web 2.0 applications, customers have been given anew platform to express their opinions in the form of reviews on designated websites.", "labels": [], "entities": []}, {"text": "At the same time many companies proactively collect direct customer feedback after an interaction, such as a store visit, a client meeting or online purchase.", "labels": [], "entities": []}, {"text": "Both information types have in common that besides quantitative data (\"How would you rate the overall shopping experience on a scale from one to ten\") also qualitative data (\"Why did you assign this score\") is being collected.", "labels": [], "entities": []}, {"text": "A fine-grained analysis of this qualitative textual feedback offers companies valuable detailed insights into the strong and weak aspects of their products and services and allows them to strengthen their offer.", "labels": [], "entities": []}, {"text": "Extracting this information automatically is known as the task of aspect-based sentiment analysis (ABSA).", "labels": [], "entities": [{"text": "aspect-based sentiment analysis (ABSA)", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.7960549692312876}]}, {"text": "ABSA systems () focus on the detection of all sentiment expressions within a given document and the concepts and aspects (or features) to which they refer.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8719902634620667}, {"text": "detection of all sentiment expressions within a given document", "start_pos": 29, "end_pos": 91, "type": "TASK", "confidence": 0.7609446081850264}]}, {"text": "Such systems do not only try to distinguish the positive from the negative utterances, but also strive to detect the target of the opinion, which comes down to a very fine-grained sentiment analysis task and \"almost all real-life sentiment analysis systems in industry should be based on this level of analysis\".", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.7872254252433777}, {"text": "sentiment analysis", "start_pos": 230, "end_pos": 248, "type": "TASK", "confidence": 0.7005500197410583}]}, {"text": "This fine-grained sentiment analysis task received special attention in the framework of three SemEval shared tasks: and, which focussed on English customer reviews, and) where seven other languages were also included.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8725157777468363}, {"text": "SemEval shared tasks", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.8608711163202921}]}, {"text": "Each time the idea was to perform three subtasks: (i) extract all aspect expressions of the entities, (ii) categorize these aspect expressions into predefined categories and (iii) determine whether an opinion on an aspect is positive, negative or neutral.", "labels": [], "entities": []}, {"text": "In this paper, we discuss a fine-grained sentiment analysis pipeline to deal with qualitative Dutch feedback data coming from three different domains: banking, retail, and human resources.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9004283845424652}]}, {"text": "This paper presents a collaboration between academia and industry to create a proofof-concept, the pipeline is currently in production at Hello Customer.", "labels": [], "entities": []}, {"text": "In the framework of the SemEval shared tasks, similar methodologies have been investigated, but the research presented here differs in two ways.", "labels": [], "entities": [{"text": "SemEval shared tasks", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8672978083292643}]}, {"text": "First, the main focus has always been on customer reviews of experiences (restaurants, hotels, movies) or tangible products (laptops, smartphones).", "labels": [], "entities": []}, {"text": "Besides product-oriented data, we move towards more service-oriented data coming from financial institutions and human resources agencies.", "labels": [], "entities": []}, {"text": "Second, the various ABSA subtasks have always been tackled and evaluated separately in the framework of SemEval.", "labels": [], "entities": []}, {"text": "In reality, however, all steps have to be performed sequen-tially, entailing error percolation from one step to the other.", "labels": [], "entities": []}, {"text": "In this paper we present such an integrated pipeline for each domain and also perform cross-domain experiments.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the data we have collected and annotated.", "labels": [], "entities": []}, {"text": "Next, in Section 3 we present the pipeline that has been developed for performing this task and in Section 4 we discuss the results.", "labels": [], "entities": []}, {"text": "We end this paper with a conclusion and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the past, ABSA datasets have been annotated comprising movie reviews, reviews for electronic products(, and restaurant reviews ().", "labels": [], "entities": [{"text": "ABSA datasets", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.7658368647098541}]}, {"text": "As mentioned above, in the framework of three SemEval shared tasks (, several benchmark review datasets coming from various domains (electronics, hotels, restaurants, and telecom) and languages (English, Dutch, French, Arabic, Chinese, Spanish, Turkish and Russian) have been made publicly available.", "labels": [], "entities": [{"text": "SemEval shared tasks", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8605380853017172}]}, {"text": "For the work presented here, direct customer feedback data written in Dutch was collected in three domains: banking, retail and human resources (HR).", "labels": [], "entities": []}, {"text": "The data provider for the first domain, banking, is a large Belgian financial institution offering basic financial products (e.g. loans, insurances) and services (e.g. investing or financial advice).", "labels": [], "entities": []}, {"text": "The second domain, retail, comprises data coming from a large clothing company with offline stores allover Belgium and an online webshop.", "labels": [], "entities": []}, {"text": "Data for the third domain, HR, comes from two data providers who are active in the recruiting sector, namely employment agencies.", "labels": [], "entities": []}, {"text": "For all domains, data was collected by asking customers two things: (i) assign a NPS score 1 to the company and (ii) provide textual feedback for this score.", "labels": [], "entities": [{"text": "NPS score 1", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.8327928384145101}]}, {"text": "This feedback is referred to as a verbatim, which can vary from one short sentence to various sentences discussing various aspects.", "labels": [], "entities": []}, {"text": "Table 1 presents an overview of all data that has been collected and annotated in the three domains, expressed in number of verbatims and tokens.", "labels": [], "entities": []}, {"text": "For the actual annotations, see fora visualization, we annotated each aspect term and assigned it to a predefined aspect category.", "labels": [], "entities": []}, {"text": "These aspect categories are domain-dependent and consist of a main category (e.g. Personnel) and subcategory (e.g. quality) 2 . For banking there are 22 such possible combinations, for retail 24 and for HR 23.", "labels": [], "entities": []}, {"text": "gives an overview of the three largest main categories per domain.", "labels": [], "entities": []}, {"text": "Ina next step, sentiment bearing words were selected, assigned a polarity: positive, negative or neutral, and linked to the appropriate aspect term (is about arrow).", "labels": [], "entities": [{"text": "sentiment bearing words", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8618589639663696}]}, {"text": "All annotations were carried outwith the BRAT rapid annotation tool ().", "labels": [], "entities": [{"text": "BRAT rapid annotation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.39489378531773883}]}, {"text": "For all three domains, we went through the same annotation process to ensure consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9584842324256897}]}, {"text": "First, a preliminary aspect category typology was devised after which 50 verbatims were annotated by two annotators independently from each other.", "labels": [], "entities": []}, {"text": "These annotations were discussed, inconsistencies were resolved and the typology was altered, if necessary.", "labels": [], "entities": []}, {"text": "Next, an inter-annotator agreement study was conducted on 50 new verbatims, which were again annotated by two independent annotators.", "labels": [], "entities": []}, {"text": "The annotations were compared to the annotations of a third, more experienced annotator who also received more time to complete the task.", "labels": [], "entities": []}, {"text": "Accuracy was calculated on two levels: the consistency of the annotated category expressions (cat) and the consistency of the annotated polarity expressions (pol).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9949749708175659}, {"text": "consistency", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9815257787704468}, {"text": "consistency", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.963683545589447}]}, {"text": "As can be observed in, the IAA was high for all three domains.", "labels": [], "entities": [{"text": "IAA", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9955012202262878}]}, {"text": "For the remainder of the annotation work, the same two annotators performed all annotations and frequently checked and discussed their work to ensure consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 150, "end_pos": 161, "type": "METRIC", "confidence": 0.9513934254646301}]}], "tableCaptions": [{"text": " Table 1: Verbatims and tokens in each domain.", "labels": [], "entities": []}, {"text": " Table 2: Typology of the three main aspect categories and occurrences per domain.", "labels": [], "entities": [{"text": "Typology", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9645835757255554}]}, {"text": " Table 3: IAA, expressed in accuracy (%).", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.996324360370636}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996992349624634}]}, {"text": " Table 4: Precision, recall, and F-1 scores for aspect term extraction on held-out test sets.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990358352661133}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9914495348930359}, {"text": "F-1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9967560172080994}, {"text": "aspect term extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6781126856803894}]}, {"text": " Table 5: Aspect category classification results.", "labels": [], "entities": [{"text": "Aspect category classification", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7791348298390707}]}, {"text": " Table 6: Aspect polarity classification results.", "labels": [], "entities": [{"text": "Aspect polarity classification", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8450969060262045}]}]}