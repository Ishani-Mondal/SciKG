{"title": [{"text": "Learning to Embed Words in Context for Syntactic Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "We present models for embedding words in the context of surrounding words.", "labels": [], "entities": []}, {"text": "Such models, which we refer to as token em-beddings, represent the characteristics of a word that are specific to a given context , such as word sense, syntactic category , and semantic role.", "labels": [], "entities": []}, {"text": "We explore simple, efficient token embedding models based on standard neural network archi-tectures.", "labels": [], "entities": []}, {"text": "We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7310611605644226}]}, {"text": "We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.7268438140551249}]}, {"text": "Many embedding models have been developed) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.6528844118118287}, {"text": "named entity recognition", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.6166096131006876}, {"text": "semantic role labeling", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.6786693334579468}, {"text": "dependency parsing", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8443967401981354}, {"text": "machine translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.8020307421684265}]}, {"text": "The majority of this work has focused on a single embedding for each word type in a vocabulary.", "labels": [], "entities": []}, {"text": "We will refer to these as type embed-dings.", "labels": [], "entities": []}, {"text": "However, the same word type can exhibit a range of linguistic behaviors in different contexts.", "labels": [], "entities": []}, {"text": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type.", "labels": [], "entities": []}, {"text": "But token-level linguistic phenomena go beyond word sense, and these approaches are only reliable for frequent words.", "labels": [], "entities": []}, {"text": "Several kinds of token-level phenomena relate directly to NLP tasks.", "labels": [], "entities": []}, {"text": "Word sense disambiguation relies on context to determine which sense is intended.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7048787573973337}]}, {"text": "POS tagging, dependency parsing, and semantic role labeling identify syntactic categories and semantic roles for each token.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7627785205841064}, {"text": "dependency parsing", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7976956367492676}, {"text": "semantic role labeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6412936846415201}]}, {"text": "Sentiment analysis and related tasks like opinion mining seek to understand word connotations in context.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9527713656425476}, {"text": "opinion mining", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8079650402069092}]}, {"text": "In this paper, we develop and evaluate models for embedding word tokens.", "labels": [], "entities": []}, {"text": "Our token embeddings capture linguistic characteristics expressed in the context of a token.", "labels": [], "entities": []}, {"text": "Unlike type embeddings, it is infeasible to precompute and store all possible (or even a significant fraction of) token embeddings.", "labels": [], "entities": []}, {"text": "Instead, our token embedding models are parametric, so they can be applied on the fly to embed any word in its context.", "labels": [], "entities": []}, {"text": "We focus on simple and efficient token embedding models based on local context and standard neural network architectures.", "labels": [], "entities": []}, {"text": "We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.6903137564659119}, {"text": "dependency parsing", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7559078633785248}]}, {"text": "We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of.", "labels": [], "entities": []}, {"text": "We add our token embeddings to Tweeboparser (), improving its performance and establishing anew state of the art for Twitter dependency parsing.", "labels": [], "entities": [{"text": "Twitter dependency parsing", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.624748557806015}]}], "datasetContent": [{"text": "For training the token embedding models, we mostly use the same settings as in Section 4.1 for the qualitative analysis.", "labels": [], "entities": []}, {"text": "The only difference is that we train the token embedding models for 5 epochs, again saving the model that reaches the best objective value on a held-out set of 3,000 unlabeled tweets.", "labels": [], "entities": []}, {"text": "We also experiment with several values for the context window size wand the hidden layer size, reported below.", "labels": [], "entities": []}, {"text": "in j n \u2206 = 1 \u2206 = 2 3 \u2264 \u2206 \u2264 5 6 \u2264 \u2206 \u2264 10 \u2206 \u2265 11 i < j i > j x j is wall symbol: Dependency pair features for arc with child xi and parent x j in an n-word sentence and where \u2206 = |i \u2212 j|.", "labels": [], "entities": []}, {"text": "The final feature is 1 if x j is the wall symbol ($), indicating a root attachment for xi . In that case, all features are zero except for the first and last.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Dependency pair features for arc with child x i and parent x j in an n-word sentence and where  \u2206 = |i \u2212 j|. The final feature is 1 if x j is the wall symbol ($), indicating a root attachment for x i . In that  case, all features are zero except for the first and last.", "labels": [], "entities": []}, {"text": " Table 5: Tagging accuracies (%) on validation  (OCT27TEST) and test (DAILY547) sets. Accu- racy deltas are always relative to the respective  baseline in each section of the table. \"updating\"  = updates type embeddings during training, \"fea- tures\" = uses binary feature vector for center word,  * = omits center word type embedding.", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.854416012763977}, {"text": "Accu- racy deltas", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9396683871746063}]}, {"text": " Table 6: Tagging accuracies (%) on validation  (OCT27TEST) and test (DAILY547) sets using all  features: Brown clusters, tag dictionaries, name  lists, and character n-grams. Last row is best re- sult from", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8608524203300476}]}, {"text": " Table 7: Attachment F 1 (%) on validation set us- ing different models and window sizes. For TE  columns, the input does not include any type em- beddings at all, only token embeddings. Best re- sult in each column is in boldface.", "labels": [], "entities": [{"text": "Attachment F 1", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8902915914853414}]}, {"text": " Table 8: Dependency parsing unlabeled attach- ment F 1 (%) on test (TEST-NEW) sets for baseline  parser and results when augmented with token em- bedding features. Following Kong et al., we report  three significant digits.", "labels": [], "entities": [{"text": "Dependency parsing unlabeled attach- ment F 1", "start_pos": 10, "end_pos": 55, "type": "METRIC", "confidence": 0.7838766723871231}]}]}