{"title": [{"text": "Replacing OOV Words For Dependency Parsing With Distributional Semantics", "labels": [], "entities": [{"text": "Replacing OOV Words", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7950490315755209}]}], "abstractContent": [{"text": "Lexical information is an important feature in syntactic processing like part-of-speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6599667549133301}, {"text": "dependency parsing", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7900956869125366}]}, {"text": "However, there is no such information available for out-of-vocabulary (OOV) words, which causes many classification errors.", "labels": [], "entities": []}, {"text": "We propose to replace OOV words with in-vocabulary words that are semantically similar according to dis-tributional similar words computed from a large background corpus, as well as morphologically similar according to common suffixes.", "labels": [], "entities": []}, {"text": "We show performance differences both for count-based and dense neu-ral vector-based semantic models.", "labels": [], "entities": []}, {"text": "Further , we discuss the interplay of POS and lexical information for dependency parsing and provide a detailed analysis and a discussion of results: while we observe significant improvements for count-based methods, neural vectors do not increase the overall accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8388904333114624}, {"text": "accuracy", "start_pos": 260, "end_pos": 268, "type": "METRIC", "confidence": 0.9975705742835999}]}], "introductionContent": [{"text": "Due to the high expense of creating treebanks, there is a notorious scarcity of training data for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8402955532073975}]}, {"text": "The quality of dependency parsing crucially hinges on the quality of partof-speech (POS) tagging as a preprocessing step; many dependency parsers also utilize lexicalized information, which is only available for the training vocabulary.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7913666069507599}, {"text": "partof-speech (POS) tagging", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6638387322425843}, {"text": "dependency parsers", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.712361752986908}]}, {"text": "Thus errors in dependency parsers often relate to OOV (out of vocabulary, i.e. not seen in the training data) words.", "labels": [], "entities": [{"text": "OOV", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.990063488483429}]}, {"text": "While there has been a considerable amount of work to address the OOV problem with continuous word representations (see Section 2), this requires a more complex model and hence, increases training and execution complexity.", "labels": [], "entities": []}, {"text": "In this paper, we present a very simple yet effective way of alleviating the OOV problem to some extent: we use two flavors of distributional similarity, computed on a large background corpus, to replace OOV words in the input with semantically or morphologically similar words that have been seen in the training, and project parse labels back to the original sequence.", "labels": [], "entities": [{"text": "OOV problem", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.877385675907135}]}, {"text": "If we succeed in replacing OOV words with in-vocabulary words of the same syntactic behavior, we expect the tagging and parsing process to be less prone to errors caused by the absence of lexical information.", "labels": [], "entities": [{"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.6445902585983276}]}, {"text": "We show consistent significant improvements both for POS tagging accuracy as well as for Labeled Attachment Scores (LAS) for graph-based semantic similarities.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8610466718673706}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9267402291297913}, {"text": "Labeled Attachment Scores (LAS)", "start_pos": 89, "end_pos": 120, "type": "METRIC", "confidence": 0.7461273272832235}]}, {"text": "The successful strategies mostly improve POS accuracy on open class words, which results in better dependency parses.", "labels": [], "entities": [{"text": "POS", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.877004086971283}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9009873867034912}, {"text": "dependency parses", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.6771033108234406}]}, {"text": "Beyond improving POS tagging, the strategy also contributes to parsing accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8403845429420471}, {"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9849768877029419}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9461929798126221}]}, {"text": "Through extensive experiments -we show results for seven different languages -we are able to recommend one particular strategy in the conclusion and show the impact of using different similarity sources.", "labels": [], "entities": []}, {"text": "Since our method manipulates the input data rather than the model, it can be used with any existing dependency parser without re-training, which makes it very applicable in existing environments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we describe the methods, background corpora used for computing similarities and all further tools used for the experiments.", "labels": [], "entities": []}, {"text": "With our experiments, we target to address the following research questions: \u2022 Can syntactic processing benefit from OOV replacement, and if so, under what strategies and conditions?", "labels": [], "entities": [{"text": "OOV replacement", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7604357004165649}]}, {"text": "\u2022 Is there a qualitative difference between similarity sources with respect to tagger/parser performance?", "labels": [], "entities": []}, {"text": "1 Translations: Nachtzeit = nighttime; tags\u00fcber = during the day; Pachtzeit = length of lease; Ruhezeit = downtime; Echtzeit = real time; Jahreswende = turn of the year \u2022 Are there differences in the sensitivity of parsing inference methods to OOV replacement?", "labels": [], "entities": [{"text": "tags\u00fcber", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9599080681800842}, {"text": "parsing inference", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.8919971883296967}, {"text": "OOV replacement", "start_pos": 244, "end_pos": 259, "type": "TASK", "confidence": 0.8640955686569214}]}], "tableCaptions": [{"text": " Table 2: Test set overall OOV rates, POS accuracy  in % for baseline, suffix-only baseline, DT simi- larity and suffix replacement strategies for seven  languages.", "labels": [], "entities": [{"text": "OOV rates", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9696117043495178}, {"text": "POS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9979890584945679}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8361684083938599}, {"text": "suffix replacement", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7141674160957336}]}, {"text": " Table 3: Test set POS accuracies for w2v-based  model's similarity and suffix replacement strate- gies for three languages.", "labels": [], "entities": []}, {"text": " Table 4: LAS scores for the parsing performance on the test sets when replacing OOV words with a DT.  Additionally, we present \u2206 values for all languages.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9933116436004639}]}, {"text": " Table 5: LAS scores for the parsing performance replacing OOV words with", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9637715816497803}, {"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9654017090797424}]}]}