{"title": [{"text": "Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambiguous Synonyms", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6690725882848104}]}], "abstractContent": [{"text": "This paper compares two approaches to word sense disambiguation using word embeddings trained on unambiguous synonyms.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7620605230331421}]}, {"text": "The first one is an unsupervised method based on computing log probability from sequences of word embedding vectors, taking into account ambiguous word senses and guessing correct sense from context.", "labels": [], "entities": []}, {"text": "The second method is supervised.", "labels": [], "entities": []}, {"text": "We use a multilayer neural network model to learn a context-sensitive transformation that maps an input vector of ambiguous word into an output vector representing its sense.", "labels": [], "entities": []}, {"text": "We evaluate both methods on corpora with manual annotations of word senses from the Polish wordnet.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity is one of the fundamental features of natural language, so every attempt to understand NL utterances has to include a disambiguation step.", "labels": [], "entities": []}, {"text": "People usually do not even notice ambiguity because of the clarifying role of the context.", "labels": [], "entities": []}, {"text": "A word market is ambiguous, and it is still such in the phrase the fish market while in a longer phrase like the global fish market it is unequivocal because of the word global, which cannot be used to describe physical place.", "labels": [], "entities": []}, {"text": "Thus, distributional semantics methods seem to be a natural way to solve the word sense discrimination/disambiguation task (WSD).", "labels": [], "entities": [{"text": "word sense discrimination/disambiguation task (WSD)", "start_pos": 77, "end_pos": 128, "type": "TASK", "confidence": 0.8306445280710856}]}, {"text": "One of the first approaches to WSD was context-group sense discrimination) in which sense representations were computed as groups of similar contexts.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9923737645149231}, {"text": "context-group sense discrimination", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.5894809563954672}]}, {"text": "Since then, distributional semantic methods were utilized in very many ways in supervised, weekly supervised and unsupervised approaches.", "labels": [], "entities": []}, {"text": "Unsupervised WSD algorithms aim at resolving word ambiguity without the use of annotated corpora.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.968825101852417}]}, {"text": "There are two popular categories of knowledge-based algorithms.", "labels": [], "entities": []}, {"text": "The first one originates from the algorithm, and exploit the number of common words in two sense definitions (glosses) to select the proper meaning in a context.", "labels": [], "entities": []}, {"text": "Lesk algorithm relies on the set of dictionary entries and the information about the context in which the word occurs.", "labels": [], "entities": []}, {"text": "In () the concept of overlap is replaced by similarity represented by a DSM model.", "labels": [], "entities": []}, {"text": "The authors compute the overlap between the gloss of the meaning and the context as a similarity measure between their corresponding vector representations in a semantic space.", "labels": [], "entities": []}, {"text": "A semantic space is a co-occurrences matrix M build by analysing the distribution of words in a large corpus, later reduced using Latent Semantic Analysis).", "labels": [], "entities": []}, {"text": "The second group of algorithms comprises graph-based methods which use structure of semantic nets in which different types of word sense relations are represented and linked (e.g. WordNet, BabelNet).", "labels": [], "entities": []}, {"text": "They used various graph-induced information, e.g. Page Rank algorithm (.", "labels": [], "entities": []}, {"text": "In this paper we present a method of word sense disambiguation, i.e. inferring an appropriate word sense from those listed in Polish wordnet, using word embeddings in both supervised and unsupervised approaches.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7134920159975687}]}, {"text": "The main tested idea is to calculate sense embeddings using unambiguous synonyms (elements of the same synsets) fora particular word sense.", "labels": [], "entities": []}, {"text": "In section 2 we shortly present existing results for WSD for Polish as well as other works related to word embeddings for other languages, while section 3 presents annotated data we use for evaluation and supervised model training.", "labels": [], "entities": []}, {"text": "Next sections describe the chosen method of calculating word sense embeddings, our unsuper-vised and supervised WSD experiments and some comments on the results.", "labels": [], "entities": [{"text": "calculating word sense embeddings", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7730880379676819}, {"text": "WSD", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.8970723748207092}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision of word-sense disambiguation  methods for Polish.", "labels": [], "entities": [{"text": "Precision of word-sense disambiguation", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5904593542218208}]}]}