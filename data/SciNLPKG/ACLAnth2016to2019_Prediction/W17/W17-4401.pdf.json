{"title": [{"text": "Boundary-Based MWE Segmentation With Text Partitioning", "labels": [], "entities": [{"text": "MWE Segmentation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8384322226047516}, {"text": "Text Partitioning", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7156896740198135}]}], "abstractContent": [{"text": "This work presents a fine-grained, text-chunking algorithm designed for the task of multiword expressions (MWEs) segmentation.", "labels": [], "entities": [{"text": "multiword expressions (MWEs) segmentation", "start_pos": 84, "end_pos": 125, "type": "TASK", "confidence": 0.6841744482517242}]}, {"text": "As a lexical class, MWEs include a wide variety of idioms, whose automatic identification area necessity for the handling of colloquial language.", "labels": [], "entities": []}, {"text": "This algorithm's core novelty is its use of non-word tokens, i.e., boundaries, in a bottom-up strategy.", "labels": [], "entities": []}, {"text": "Leveraging boundaries refines token-level information, forging high-level performance from relatively basic data.", "labels": [], "entities": []}, {"text": "The generality of this model's feature space allows for its application across languages and domains.", "labels": [], "entities": []}, {"text": "Experiments spanning 19 different languages exhibit a broadly-applicable, state-of-the-art model.", "labels": [], "entities": []}, {"text": "Evaluation against recent shared-task data places text partitioning as the overall, best performing MWE segmen-tation algorithm, covering all MWE classes and multiple English domains (including user-generated text).", "labels": [], "entities": [{"text": "text partitioning", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7618017792701721}, {"text": "MWE segmen-tation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7304490506649017}]}, {"text": "This performance, coupled with a non-combinatorial, fast-running design, produces an ideal combination for implementations at scale, which are facilitated through the release of open-source software.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword expressions (MWEs) constitute a mixed class of complex lexical objects that often behave in syntactically unruly ways.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7187979221343994}]}, {"text": "A unifying property that ties this class together is the lexicalization of multiple words into a single unit.", "labels": [], "entities": []}, {"text": "MWEs are generally difficult to understand through grammatical decomposition, casting them as types of minimal semantic units.", "labels": [], "entities": []}, {"text": "There is variation in this non-compositionality property (, which in part maybe attributed to differences in MWE types.", "labels": [], "entities": []}, {"text": "These range from multiword named entities, such as Long Beach, California, to proverbs, such as it takes one to know one, to idiomatic verbal expressions, like cut it out (which often contain flexible gaps).", "labels": [], "entities": []}, {"text": "For all of their strangeness they appear across natural languages), though generally not for common meanings, and frequently with opaque etymologies that confound non-native speakers.", "labels": [], "entities": []}], "datasetContent": [{"text": "It is reasonably straightforward to measure precision, recall, and F 1 for exact matches of MWEs.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9997584223747253}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9997809529304504}, {"text": "F 1", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9881793856620789}]}, {"text": "However, this strategy is unreasonably coarse, failing to represent partial credit when algorithms get only portions of MWEs correct.", "labels": [], "entities": []}, {"text": "Thus, the developers of the different gold standard data sets have established other evaluation metrics that are more flexible.", "labels": [], "entities": [{"text": "gold standard data sets", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.7807058691978455}]}, {"text": "Utilizing these partial credit MWE evaluation metrics provides refined detail into the performance of algorithms.", "labels": [], "entities": [{"text": "MWE evaluation", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.8365339636802673}]}, {"text": "However, these are not the same across the gold standard data sets.", "labels": [], "entities": [{"text": "gold standard data sets", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.8277044743299484}]}, {"text": "So, to maintain comparability of the present results, this work uses the specific strategies associated to each shared task.", "labels": [], "entities": []}, {"text": "In application to the PARSEME data sets, precision, recall, and F 1 describe tokens' presence in MWEs.", "labels": [], "entities": [{"text": "PARSEME data sets", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.8988128304481506}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9996894598007202}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9994557499885559}, {"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9936108589172363}]}, {"text": "Alternatively, DIMSUM-style metrics measure link/boundary-based evaluations.", "labels": [], "entities": []}, {"text": "Specifically, this strategy checks if the links between tokens are correct.", "labels": [], "entities": []}, {"text": "Note that this latter (DIM-SUM) evaluation is better aligned to the formulation of text partitioning, but leaves the number evaluation points atone fewer per MWE than the PARSEME scheme.", "labels": [], "entities": [{"text": "text partitioning", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7673921287059784}]}, {"text": "Thus, PARSEME evaluations favor longer MWEs more heavily.", "labels": [], "entities": [{"text": "PARSEME", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.602885901927948}]}, {"text": "The basic text partitioning model relies on the single threshold parameter, q, and integration of POS tags relies on a second.", "labels": [], "entities": [{"text": "text partitioning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7630840241909027}]}, {"text": "So, optimization ultimately entails the determination of parameters for both tokens, q tok , and POS tags q POS . To balance both precision and recall, these parameters are determined through optimization of the F 1 measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9988831877708435}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.997423529624939}, {"text": "F 1 measure", "start_pos": 212, "end_pos": 223, "type": "METRIC", "confidence": 0.9095378120740255}]}, {"text": "In the absence of the LFD, F 1 -optimal pairs, (q tok , q POS ), are first determined via a full parameter scan over For a given threshold pair, LFD-enhancement can then only increase precision, while decreasing recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9991137385368347}, {"text": "recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.997835099697113}]}, {"text": "So, subsequent optimization with the LFD is accomplished through scanning values of q tok and q POS in the parameter space no less than those previously determined for basic, non-LFD model.", "labels": [], "entities": []}, {"text": "The different experiments were conducted in accordance with the protocols established by the designers of data sets and shared tasks, and in all cases, an eight-fold cross-validation was conducted for optimization.", "labels": [], "entities": []}, {"text": "Exact comparability was achieved for the DIMSUM and PARSEME experiments as a result of the precise configurations of training and testing data from the shared tasks.", "labels": [], "entities": [{"text": "DIMSUM", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.8711892366409302}, {"text": "PARSEME", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.5965666174888611}]}, {"text": "Moreover, since an evaluation script was provided for each, metrics reported for DIMSUM and PARSEME experiments are incomplete accord with the results of the shared tasks.", "labels": [], "entities": [{"text": "DIMSUM", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.8499722480773926}, {"text": "PARSEME", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.47477200627326965}]}, {"text": "For the DIMSUM experiments, results should be compared to the open track (external data was utilized), and for the PARSEME experiments, results should be compared to the closed track (no external data was utilized).", "labels": [], "entities": [{"text": "PARSEME", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.7972562909126282}]}], "tableCaptions": [{"text": " Table 1: Evaluation results, including data sets (Experiment); the LFD's application (LFD); token (qtok) and POS (qPOS)", "labels": [], "entities": []}]}