{"title": [{"text": "Incorporating visual features into word embeddings: A bimodal autoencoder-based approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Multimodal semantic representation is an evolving area of research in natural language processing as well as computer vision.", "labels": [], "entities": [{"text": "Multimodal semantic representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8179671168327332}]}, {"text": "Combining or integrating perceptual information, such as visual features, with linguistic features is recently being actively studied.", "labels": [], "entities": []}, {"text": "This paper presents a novel bi-modal autoencoder model for multimodal representation learning: the autoencoder learns in order to enhance linguistic feature vectors by incorporating the corresponding visual features.", "labels": [], "entities": [{"text": "multimodal representation learning", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.7183497150739034}]}, {"text": "During the runtime, owing to the trained neural network, visually enhanced multimodal representations can be achieved even for words for which direct visual-linguistic correspondences are not learned.", "labels": [], "entities": []}, {"text": "The empirical results obtained with standard semantic relatedness tasks demonstrate that our approach is generally promising.", "labels": [], "entities": []}, {"text": "We further investigate the potential efficacy of the enhanced word embeddings in discriminating antonyms and synonyms from vaguely related words.", "labels": [], "entities": []}], "introductionContent": [{"text": "The efficient learning and the effective exploitation of a distributed representation of words, phrases, and sentences are active research topics in NLP (.", "labels": [], "entities": []}, {"text": "Theoretically supported by the concept of grounded cognition ( and technically endorsed by the progress of deep learning techniques, this line of research has been further pursued in order to incorporate perceptual information, such as visual features, into linguistic embeddings).", "labels": [], "entities": []}, {"text": "The resulting semantic representation is often referred to as multimodal semantic representation.", "labels": [], "entities": []}, {"text": "Two fundamental requirements, however, may not have been fulfilled simultaneously: (1) the zeroshot representation learning of words, and (2) the exploitation of existing useful resources.", "labels": [], "entities": [{"text": "zeroshot representation learning of words", "start_pos": 91, "end_pos": 132, "type": "TASK", "confidence": 0.7702376008033752}]}, {"text": "It should be noted that zero-shot representation learning in the context of the present research means a computational process for obtaining an appropriate multimodal representation even fora word for which direct visuallinguistic correspondences have not been learned.", "labels": [], "entities": [{"text": "zero-shot representation learning", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6994821031888326}]}, {"text": "In this paper, a bimodal autoencoder 1 model, named ViEW (visually enhanced word embeddings), is proposed for incorporating visual features into existing word embeddings.", "labels": [], "entities": []}, {"text": "This architecture facilitates bimodal representation learning from the given visual-linguistic correspondences.", "labels": [], "entities": []}, {"text": "In the training, the autoencoder learns to reproduce a linguistic word embedding vector, while having an additional input vector (of the same dimensionality) that represents the corresponding visual features.", "labels": [], "entities": []}, {"text": "During the runtime, by exploiting the trained neural network parameters, the autoencoder can construct a visually enhanced word embedding even fora word for which direct visual-linguistic correspondences have not been learned.", "labels": [], "entities": []}, {"text": "It should be noted here that these visual and linguistic features could be drawn from independently developed existing resources.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that our model exhibits state-of-the-art performances in standard semantic relatedness tasks, some of which innately contain zero-shot instances.", "labels": [], "entities": []}, {"text": "We further discuss the potential efficacy of the enhanced word embeddings in discriminating antonyms and synonyms from vaguely related words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the achieved multimodal representations using standard semantic relatedness tasks (), which would enable us to compare our results with that of previous works.", "labels": [], "entities": []}, {"text": "As semantic relatedness covers a wider range of lexical or semantic relationships between words than semantic similarity, the relatedness tasks maybe more suitable for assessing the performance of multimodal semantic representations, which would encode our implicit perceptual knowledge.", "labels": [], "entities": []}, {"text": "We mainly employ the MEN () dataset and assess the performance by measuring the Spearman's rank correlation coefficients between the MEN's gold ratings and the predicted relatedness.", "labels": [], "entities": [{"text": "MEN () dataset", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.802998681863149}, {"text": "Spearman's rank correlation", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.589497484266758}]}, {"text": "The MEN dataset was specifically developed for evaluating multimodal semantic models ().", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8909064531326294}]}, {"text": "In addition, we used the SimLex-999 () and the SemSim/VisSim) datasets to further investigate the applicability of the proposed model in other datasets having different characteristics.", "labels": [], "entities": [{"text": "SemSim/VisSim) datasets", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.5742309987545013}]}, {"text": "\u2022 MEN: This dataset includes 3,000 word pairs created from 751 distinct words.", "labels": [], "entities": []}, {"text": "Each pair in the dataset was given a semantic relatedness score in the range of.", "labels": [], "entities": []}, {"text": "It contains highly semantically related pairs (e.g., beach/sand rated as 0.96) as well as low-scored pairs (e.g., bakery/zebra rated as 0).", "labels": [], "entities": []}, {"text": "Each word in the dataset was assigned apart of speech (POS) tag: verb, adjective, or noun.", "labels": [], "entities": []}, {"text": "\u2022 SimLex-999: This is a dataset of 999 word pairs that is used for assessing the ability of a semantic model in capturing semantic similarity, rather than semantic relatedness or association.", "labels": [], "entities": []}, {"text": "As the authors argue and several empirical results suggest, this dataset poses challenges to the most models based on the distributional hypothesis.", "labels": [], "entities": []}, {"text": "\u2022 SemSim/VisSim: This is a dataset of 7,576 word pairs, each of which is annotated using not only semantic similarities (SemSim) but also visual similarities (VisSim), so that the user can compare the performances of her/his model in predicting different types of similarities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ViEW model results for the non-ZS setting (MEN dataset).", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.80112224817276}]}, {"text": " Table 2: Comparison of the non-ZS results (MEN dataset).", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.8239692747592926}]}, {"text": " Table 3: Comparison of the ZS results with four datasets.", "labels": [], "entities": []}, {"text": " Table 4: POS breakdown of the MEN dataset results (Image source: ESP-Game).", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.8478640615940094}]}, {"text": " Table 6: Changes in the similarity ranks of antonyms/synonyms (ImageNet).", "labels": [], "entities": []}]}