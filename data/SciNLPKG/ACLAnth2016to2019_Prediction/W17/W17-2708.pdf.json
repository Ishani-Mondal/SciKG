{"title": [{"text": "Inference of Fine-Grained Event Causality from Blogs and Films", "labels": [], "entities": []}], "abstractContent": [{"text": "Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7303507924079895}]}, {"text": "Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline.", "labels": [], "entities": []}, {"text": "In this position paper , we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8373245000839233}]}, {"text": "We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions.", "labels": [], "entities": []}, {"text": "We show that our method learns fine-grained causal relations , judged by humans as likely to be causal over 80% of the time.", "labels": [], "entities": []}, {"text": "We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational models of language understanding must recognize narrative structure because many types of natural language texts are narratively structured, e.g. news, reviews, film scripts, conversations, and personal blogs.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7762199342250824}]}, {"text": "Human understanding of narrative is driven by reasoning about causal relations between the events and states in the story (GerWe packed all our things on the night before Thu (24 Jul) except for frozen food.", "labels": [], "entities": []}, {"text": "We brought a lot of things along.", "labels": [], "entities": []}, {"text": "We woke up early on Thu and JS started packing the frozen marinatinated food inside the small cooler...", "labels": [], "entities": []}, {"text": "In the end, we decided the best place to setup the tent was the squarish ground that's located on the right.", "labels": [], "entities": []}, {"text": "Prior to setting up our tent, we placed a tarpon the ground.", "labels": [], "entities": []}, {"text": "In this way, the underneaths of the tent would be kept clean.", "labels": [], "entities": []}, {"text": "After that, we set the tent up.).", "labels": [], "entities": []}, {"text": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (, with the long-term aim of using this knowledge for understanding.", "labels": [], "entities": []}, {"text": "Some of this work explicitly models causality; other work characterizes the semantic relations more loosely as \"events that tend to cooccur\".", "labels": [], "entities": []}, {"text": "Related work points out that causality is granular in nature, and that humans flexibly move back and forth between different levels of granularity of causal knowledge.", "labels": [], "entities": []}, {"text": "Thus methods are needed to learn causal relations and reason about them at different levels of granularity).", "labels": [], "entities": []}, {"text": "One limitation of prior work is that it has primarily focused on newswire, thus have only learned relations about newsworthy topics, and likely the most frequent, highly common (coarsegrained) news events.", "labels": [], "entities": []}, {"text": "But news articles are not the only resource for learning about relations between events.", "labels": [], "entities": []}, {"text": "Much of the content on social media in personal blogs is written by ordinary people about their daily lives (, and these blogs contain a large variety of everyday events.", "labels": [], "entities": []}, {"text": "Film scene descriptions are also action-rich and told in fine-grained detail.", "labels": [], "entities": []}, {"text": "Moreover, both of these genres typically report events in temporal order, which is a primary cue to causality.", "labels": [], "entities": []}, {"text": "In this position paper, we claim that knowledge about fine-grained causal relations between everyday events is often not available in news, and can be better learned from other narrative genres.", "labels": [], "entities": []}, {"text": "For example, shows apart of a personal narrative written in a blog about a camping trip ().", "labels": [], "entities": []}, {"text": "The major event in this story is camping, which is contingent upon several finer-grained events, such as packing things the night before, waking up in the morning, packing frozen food, and later on at the campground, placing a tarp and setting up the tent.", "labels": [], "entities": []}, {"text": "Similarly film scene descriptions, such as the one shown in, typically contain fine-grained causality.", "labels": [], "entities": []}, {"text": "In this scene from Lord of the Rings, grabbing leads to spilling, and pushing leads to stumbling and falling.", "labels": [], "entities": [{"text": "Lord of the Rings", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6465782821178436}]}, {"text": "We show that unsupervised methods for modeling causality can learn fine-grained event relations from personal narratives and film scenes, even when the corpus is relatively small compared to those that have been used for newswire.", "labels": [], "entities": []}, {"text": "We learn high-quality causal relations, with over 80% judged as causal by humans.", "labels": [], "entities": []}, {"text": "We claim that these fine-grained causal relations are much closer in spirit to those motivating earlier work on scripts, and we show that the causal knowledge we learn is not found in causal knowledge bases learned from news.", "labels": [], "entities": []}, {"text": "Section 2 first summarizes previous work on learning causal knowledge.", "labels": [], "entities": []}, {"text": "We then present our experiments and results on modeling event causality in blogs and film scenes in Section 3.", "labels": [], "entities": []}, {"text": "Conclusions and future directions are discussed in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary goal is simply to show that finegrained causal relations can be learned from film scripts and blogs, and that these are not found in causal knowledge bases learned from newswire.", "labels": [], "entities": []}, {"text": "In this section we describe our datasets and methods, and the present two evaluations.", "labels": [], "entities": []}, {"text": "First, we evaluate whether the relations learned are causal  Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (  We process the data in each dataset and calculate causal potential score for each extracted event pair, resulting in a rank-ordered list of causal event pairs.", "labels": [], "entities": []}, {"text": "We evaluate the top 100 event pairs for camping, and the top 684 event pairs for films.", "labels": [], "entities": []}, {"text": "We take a number of event pairs from each film genre (proportional to the number of films in that genre, see, then remove duplicate event pairs, which result in the 684 event pairs from film.", "labels": [], "entities": []}, {"text": "CP event pairs from each corpus.", "labels": [], "entities": []}, {"text": "In our following Mechanical Turk experiments, Turkers have to pass qualification tests similar to the actual HITs to be able to participate in our task.", "labels": [], "entities": []}, {"text": "Ina study on each genre of films, we compare high-CP pairs to a random sample of low-CP pairs on Mechanical Turk to see if pairs with high CP score more strongly encode causal relations that ones with low CP.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9443377256393433}]}, {"text": "For every event pair in the 684 high pairs, we randomly select a low pair in order to collect human judgments on Mechanical Turk.", "labels": [], "entities": []}, {"text": "The task first defines events and event pairs, then gives examples of event pairs with causal relations.", "labels": [], "entities": []}, {"text": "Turkers are asked to select the event pair that is more likely to manifest a causal relation.", "labels": [], "entities": []}, {"text": "The results, summarized in, show that humans judge a large majority of the high-CP pairs to have a causal relation and the results vary by genre.", "labels": [], "entities": []}, {"text": "The causality rate is achieved for more focused genres, Fantasy (90.7%) and Mystery (87.7%), despite their smaller size, and the lowest for Drama (82.6%).", "labels": [], "entities": []}, {"text": "We believe this result is further evidence that topical coherence improves causal relation learning.", "labels": [], "entities": [{"text": "causal relation learning", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7070716420809428}]}, {"text": "In our second evaluation method, we compare the learned CP event pairs to the existing causal knowledge collections.", "labels": [], "entities": []}, {"text": "First, we compare our results to the Rel-grams data (learned from newswire) ().", "labels": [], "entities": [{"text": "Rel-grams data", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8161647021770477}]}, {"text": "For event pairs from films, we randomly sample 100 high-CP event pairs ensuring that each of the first events of the pairs are distinct.", "labels": [], "entities": []}, {"text": "We use the publicly available search interface for Rel-grams to find tuples with the same first event for direct comparison of content of the learned knowledge.", "labels": [], "entities": [{"text": "Rel-grams", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.903205931186676}]}, {"text": "We set the co-occurrence window to 5, and select the Rel-gram tuples with the highest # 50 (FS) (frequency of first statement occurring before second statement within a window of 50) to choose high-quality tuples.", "labels": [], "entities": [{"text": "FS)", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9758079946041107}]}, {"text": "We evaluate the extracted Rel-gram tuples using the same Mechani-cal Turk HIT described above.", "labels": [], "entities": [{"text": "Mechani-cal Turk HIT", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.7082341909408569}]}, {"text": "shows Mechanical Turk evaluation results for our method on films vs. Rel-grams: in 81% questions, humans judge the high-CP pairs to be more likely to manifest a causal relation.", "labels": [], "entities": []}, {"text": "We believe this is because the fine-grained event pairs we learn do not exist in the Rel-gram collections and thus the Rel-gram tuples that matched our first events are not highly coherent, despite the filtering we applied.", "labels": [], "entities": [{"text": "Rel-gram collections", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.865007609128952}]}, {"text": "Percentage of causal relation 81 % 19 % For event pairs from camping blogs, we evaluate all 100 high-CP pairs in a Mechanical Turk study where Turkers are asked to choose whether an event pair has causal relation or not.", "labels": [], "entities": []}, {"text": "We also evaluate Rel-gram tuples using the same task.", "labels": [], "entities": []}, {"text": "However, Rel-grams are not sorted by topic.", "labels": [], "entities": []}, {"text": "To find tuples relevant to Camping Trip, we use our top 10 indicative events and extracted all the Relgram tuples that included at least one event corresponding to one of the Camping indicative events, e.g. go camp.", "labels": [], "entities": []}, {"text": "We remove any tuple with frequency less than 25 and sort the rest by the total symmetrical conditional probability.", "labels": [], "entities": []}, {"text": "The evaluation results presented in show that 82% of the blog paurs were labeled as causal, whereas only 42% of the Rel-gram pairs were labeled as causal.", "labels": [], "entities": []}, {"text": "We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.", "labels": [], "entities": [{"text": "newswire data", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.8970633447170258}]}, {"text": "Camping Rel-gram Tuples Percentage of causal relation 82 % 42 % Next, we compare our results to the event pairs in VerbOcean (learned from newswire) with the HAPPENS-BEFORE relation).", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.8766120076179504}]}, {"text": "We use all 6497 event pairs from VerbOcean, comparing with our 684 event pairs from films and 100 event pairs from camping blogs with high CP scores.", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9264453649520874}, {"text": "CP", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9585029482841492}]}, {"text": "Our result shows that there are 12 event pairs that exist in both VerbOcean and films, e.g. turn -leave and slow -stop, and there is only one event pair that exist in both VerbOcean and camping blogs: pack -leave.", "labels": [], "entities": []}, {"text": "This confirms that most causal relations learned from other narrative genres do not exist in the currently available knowledge bases extracted from newswire.", "labels": [], "entities": []}, {"text": "A number of event pairs from these collections share the first event, e.g. dig -find and scan -spot from films vs. dig -repair and scan -upload from VerbOcean; drive -park and pick -eat from blogs vs. drive -drag and pick -plunk from VerbOcean.", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 149, "end_pos": 158, "type": "DATASET", "confidence": 0.9503818154335022}, {"text": "VerbOcean", "start_pos": 234, "end_pos": 243, "type": "DATASET", "confidence": 0.9646093845367432}]}, {"text": "Finally, we compare our high-CP pairs learned from film to the high-CP event pairs from Beamer and, learned from only 173 films.", "labels": [], "entities": [{"text": "Beamer", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8866031169891357}]}, {"text": "There is no public release of Beamer and Girju's event pairs, thus we take the 29 event pairs with high CP score presented in the paper.", "labels": [], "entities": [{"text": "CP score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9769260883331299}]}, {"text": "A total of 14 of their 29 pairs are also in our top 684 film pairs.", "labels": [], "entities": []}, {"text": "These include pairs such as swerve -avoid, leave -stand and unlock -open.", "labels": [], "entities": []}, {"text": "However on our larger genre-sorted corpus we also learn pairs such as grab -haul, scratch -claw and saddle-mount that do not exist in their collection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents and word count for  each dataset", "labels": [], "entities": []}, {"text": " Table 3: Percentage of high-CP pairs labeled as  causal by AMT worker, comparing with low-PC  pairs, in film genres Drama, Fantasy and Mystery.", "labels": [], "entities": []}]}