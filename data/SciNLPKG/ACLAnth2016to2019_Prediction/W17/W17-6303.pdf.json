{"title": [{"text": "Lexicalized vs. Delexicalized Parsing in Low-Resource Scenarios", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a systematic analysis of lexi-calized vs. delexicalized parsing in low-resource scenarios, and propose a methodology to choose one method over another under certain conditions.", "labels": [], "entities": []}, {"text": "We create a set of simulation experiments on 41 languages and apply our findings to 9 low-resource languages.", "labels": [], "entities": []}, {"text": "Experimental results show that our methodology chooses the best approach in 8 out of 9 cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent CoNLL Shared Task on Parsing Universal Dependencies (CoNLL-ST) (  gave researchers the opportunity to study dependency parsing on a wide selection of treebanks.", "labels": [], "entities": [{"text": "CoNLL Shared Task on Parsing Universal Dependencies (CoNLL-ST)", "start_pos": 11, "end_pos": 73, "type": "TASK", "confidence": 0.5640808075666428}, {"text": "dependency parsing", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.766586422920227}]}, {"text": "While the ultimate goal remained the same, i.e., achieving the best accuracy in predicting the head and dependency label of a token, the starting point changed from one group of languages to another, depending on the available resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9986271858215332}]}, {"text": "In the surprise languages scenario, participants were given a very small training treebank, no development set, relatively accurate POS tags for the test set, and little or no parallel data.", "labels": [], "entities": []}, {"text": "When parallel data is not available, many of the standard cross-lingual parsing techniques (e.g. annotation projection (); treebank translation (; utilizing cross-lingual word clusters) or word embeddings) become impossible to apply.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.6711460053920746}, {"text": "annotation projection", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.671285405755043}, {"text": "treebank translation", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.7615329921245575}]}, {"text": "Delexicalized parsing) provides a suitable alternative, while it does not require parallel data.", "labels": [], "entities": []}, {"text": "The central idea is to train a source-side parser without any lexical features, i.e., typically using only POS tags, and then use this trained parser to parse a target, lowresource language that shares the same POS tag set.", "labels": [], "entities": []}, {"text": "No gold trees are required on the target side, and only POS tags have to be predicted prior to parsing.", "labels": [], "entities": []}, {"text": "Given the simplicity of this method, several CoNLL-ST participants have chosen delexicalized approaches, not only for surprise languages but also for the other CoNLL-ST scenario -small languages 2 . In this scenario, as opposed to the surprise languages, the small training treebanks were the only source of gold POS tags.", "labels": [], "entities": []}, {"text": "When the data for training POS taggers is small -as for the small languages scenario as well as for many upcoming UD treebanks 3 -the delexicalized methods might be affected by poor POS accuracy.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8621441423892975}, {"text": "UD treebanks", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.7780696153640747}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9339048862457275}]}, {"text": "On the other hand, there are some gold trees for those scenarios and it is possible to train lexicalized parsers on them.", "labels": [], "entities": []}, {"text": "Could there be cases in which such a low-resource lexicalized parser is preferred over a delexicalized one?", "labels": [], "entities": []}, {"text": "The central question we examine is whether we can find cases where a low-resource lexicalized parser achieves better accuracy than a delexicalized one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9921272993087769}]}, {"text": "As a related problem, we investigate the following case: when there is anew language to parse, with no treebank but with the chance to predict POS tags, should one pursue a delexicalized parsing or invest in some tree annotation?", "labels": [], "entities": []}, {"text": "Our goal is to investigate those questions sys-tematically in low-resource settings and to find the conditions under which one strategy leads to better results than the other.", "labels": [], "entities": []}, {"text": "While our scenarios originate from the CoNLL-ST, our approach should be applicable to other settings.", "labels": [], "entities": [{"text": "CoNLL-ST", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9336997866630554}]}, {"text": "For example, our conclusions might prove helpful in developing early parsing models of anew treebank or in deciding how to proceed when there is a large gold POS tagged corpus but no trees (e.g. present a Middle High German corpus with 20,000 tokens of gold POS, no trees, and no apparent parallel data).", "labels": [], "entities": []}, {"text": "They can also help plan resource creation.", "labels": [], "entities": []}, {"text": "While POS annotation can be relatively fast (, creating treebanks is costly).", "labels": [], "entities": []}, {"text": "The decision of building a large POS annotated corpus vs. a small treebank in a limited time, could depend on whether delexicalized models would work well fora target language.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We use the Universal Dependencies v2.0 treebanks () released for the CoNLL-ST (.", "labels": [], "entities": [{"text": "Universal Dependencies v2.0 treebanks", "start_pos": 16, "end_pos": 53, "type": "DATASET", "confidence": 0.6364554017782211}, {"text": "CoNLL-ST", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9577636122703552}]}, {"text": "We use all the treebanks except domain-specific 4 ones as sources (46 languages).", "labels": [], "entities": []}, {"text": "As targets we take two groups of languages from the CoNLL-ST that correspond to the two settings we experiment with: Surprise languages: Kurmanji (kmr), Upper Sorbian (hsb), North Sami (sme), Buryat (bxr).", "labels": [], "entities": [{"text": "CoNLL-ST", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.956405520439148}]}, {"text": "Each language contains a small sample of gold training data (see) and its test set is provided with POS tags predicted by a system trained on a data set much larger than the training data.", "labels": [], "entities": []}, {"text": "Those languages represent the EXTPOS setting.", "labels": [], "entities": []}, {"text": "Small languages: Latin (la), Irish (ga), Ukrainian (uk), Kazakh (kk), Uyghur (ug).", "labels": [], "entities": []}, {"text": "They have small treebanks (especially Kazakh and   Uyghur), with no additional POS data.", "labels": [], "entities": [{"text": "POS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8454338908195496}]}, {"text": "They correspond to the TBPOS setting.", "labels": [], "entities": [{"text": "TBPOS setting", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.716318815946579}]}, {"text": "The 9 target treebanks do not contain development sets, so as to not compromise our test sets, we set those languages aside as test cases.", "labels": [], "entities": []}, {"text": "We perform analysis on simulated low-resource languages instead.", "labels": [], "entities": []}, {"text": "For this purpose we use the subset of source treebanks that has a development set (41 languages).", "labels": [], "entities": []}, {"text": "For each of them we sample small training treebanks starting with only 100 tokens.", "labels": [], "entities": []}, {"text": "Tools To train POS taggers, we employ MarMoT ().", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.867388904094696}, {"text": "MarMoT", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.5856566429138184}]}, {"text": "In EXTPOS, POS taggers are trained on the whole treebanks, wherein TBPOS, training is done only on small samples.", "labels": [], "entities": [{"text": "EXTPOS", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.8905787467956543}, {"text": "POS taggers", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.774637371301651}]}, {"text": "We use Universal Part of Speech tags (UPOS) in all experiments.", "labels": [], "entities": []}, {"text": "For parsing, we use a beam-search transitionbased parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9865545034408569}]}, {"text": "Delexicalized parsers (DELEX), and lexicalized parsers (LEX) for TBPOS are trained on gold POS tags.", "labels": [], "entities": []}, {"text": "For EXTPOS lexicalized parsers are trained on 5-fold jackknifed POS tags for better performance (we sample the small treebanks after performing jackknifing).", "labels": [], "entities": [{"text": "EXTPOS lexicalized parsers", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6050945421059927}]}, {"text": "We blend delexicalized models' outputs via methods described in Section 2.2.", "labels": [], "entities": []}, {"text": "In presenting the experimental results, we refer to these DELEX models with their weighting scheme, namely R&Z15, A17, and LAS tgt . Evaluation We use labeled attachment score (LAS) as the evaluation metric and evaluate using the script provided by the CoNLL-ST organizers.", "labels": [], "entities": [{"text": "R&Z15", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.8716733853022257}, {"text": "A17", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.7155984044075012}, {"text": "LAS", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9523340463638306}, {"text": "labeled attachment score (LAS)", "start_pos": 151, "end_pos": 181, "type": "METRIC", "confidence": 0.8150150378545126}, {"text": "CoNLL-ST organizers", "start_pos": 253, "end_pos": 272, "type": "DATASET", "confidence": 0.9634435772895813}]}, {"text": "We also experimented with the graph-based parser Mate to test our hypotheses on a different parsing architecture.", "labels": [], "entities": []}, {"text": "We achieved results parallel to the transitionbased parser, thus we only present one set of results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test languages.", "labels": [], "entities": []}]}