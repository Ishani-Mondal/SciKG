{"title": [{"text": "Complex Verbs are Different: Exploring the Visual Modality in Multi-Modal Models to Predict Compositionality", "labels": [], "entities": [{"text": "Compositionality", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7867568731307983}]}], "abstractContent": [{"text": "This paper compares a neural network DSM relying on textual co-occurrences with a multi-modal model integrating visual information.", "labels": [], "entities": []}, {"text": "We focus on nominal vs. verbal compounds, and zoom into lexical , empirical and perceptual target properties to explore the contribution of the visual modality.", "labels": [], "entities": []}, {"text": "Our experiments show that (i) visual features contribute differently for verbs than for nouns, and (ii) images complement textual information, if (a) the tex-tual modality by itself is poor and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantic models (DSMs) rely on the distributional hypothesis, that words with similar distributions have related meanings.", "labels": [], "entities": [{"text": "Distributional semantic models (DSMs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6672476589679718}]}, {"text": "They represent a well-established tool for modelling semantic relatedness between words and phrases).", "labels": [], "entities": []}, {"text": "In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (, or by integrating perceptual information.", "labels": [], "entities": []}, {"text": "While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by,), etc.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7011252045631409}, {"text": "relation distinction", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7878258526325226}]}, {"text": "In this paper, we compare a neural network DSM relying on textual co-occurrences with a multi-modal model extension integrating visual information.", "labels": [], "entities": []}, {"text": "We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs.", "labels": [], "entities": [{"text": "prediction of compositionality", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8453560471534729}]}, {"text": "Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German ().", "labels": [], "entities": []}, {"text": "Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (, and a novel clustering filter.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions.", "labels": [], "entities": []}, {"text": "The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the most imaginable targets are added, or (b) the textual modality performs well, and all available -potentially noisy-images are added.", "labels": [], "entities": []}, {"text": "In addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi-modality than for nouns, presumably because they are more difficult to grasp.", "labels": [], "entities": []}], "datasetContent": [{"text": "Predicting Compositionality For the prediction of compositionality, we represented the meanings of the multi-word expressions and their constituent words by textual, visual and textual+visual (i.e., multi-modal) vectors.", "labels": [], "entities": []}, {"text": "The similarity of a compound-constituent vector pair as measured by the cosine was taken as the predicted degree of compound-constituent compositionality, and the overall ranking of pair similarities was compared to the gold standard compositionality ratings using Spearman's Rank-Order Correlation Coefficient \u03c1 (.", "labels": [], "entities": []}, {"text": "Lexical, Empirical and Visual Filters The experiments compare the predictions of compositionality across all targets in the gold standards.", "labels": [], "entities": []}, {"text": "Furthermore, we zoom into factors that might influence the quality of predictions: (A) the impact of lexical and empirical target properties, i.e., ambiguity (relying on the DUDEN dictionary 3 , frequency (as provided by the gold standards), abstractness and imageability (as taken from K\u00f6per and Schulte im Walde (2016)); (B) optimisation of the visual space: (i) In accordance with human concept processing), including image representations should be more useful for words which are visual.", "labels": [], "entities": []}, {"text": "We therefore apply the dispersion-based filter suggested by.", "labels": [], "entities": []}, {"text": "The filter decides whether to include perceptual information fora specific word or not, relying on a pairwise similarity between all images of a concept.", "labels": [], "entities": []}, {"text": "The underlying idea is that highly visual concepts are visualised by similar pictures and thus trigger a high average similarity between the word's images.", "labels": [], "entities": []}, {"text": "Abstract concepts, on the other hand, are expected to provide a lower dispersion.", "labels": [], "entities": []}, {"text": "For a given word, the filter decides about using only the textual representation, or both the textual and visual representations, depending on the dispersion value and a predefined threshold (set to the median of all the dispersion values).", "labels": [], "entities": []}, {"text": "(ii) We apply an imageability filter based on external imageability norms, to successively include only images for the most imaginable target words.", "labels": [], "entities": []}, {"text": "This filter is applied in the same way as dispersion.", "labels": [], "entities": []}, {"text": "(iii) We suggest a novel clustering filter, that performs a clustering of the 25 images fora given concept, using the algo- present the prediction results for the two gold standards, GS-NN and GS-PV.", "labels": [], "entities": [{"text": "GS-NN", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.9505003094673157}, {"text": "GS-PV", "start_pos": 193, "end_pos": 198, "type": "DATASET", "confidence": 0.9494406580924988}]}, {"text": "For GS-NN, we focus on predicting the compositionality for compound-head pairs (ignoring compound-modifier pairs), in order to have a more parallel setup to GS-PV, where the particle verb compositionality focuses on the contribution of the base verb.", "labels": [], "entities": []}, {"text": "The figures show the results across all targets.", "labels": [], "entities": []}, {"text": "Note that the vertical axis, showing the range of Spearman's \u03c1 are different for both results.", "labels": [], "entities": []}, {"text": "zoom into target subsets regarding target ambiguity (one sense vs. multiple senses), frequency, abstractness vs. concreteness, imageability, and compositionality.", "labels": [], "entities": []}, {"text": "The bars refer to the textual model, the multi-modal model (including all images for all targets), and the best results obtained when using the dispersion/imageability/clustering 4 filters.", "labels": [], "entities": []}], "tableCaptions": []}