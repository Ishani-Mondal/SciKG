{"title": [{"text": "End to End Dialog System for Telugu", "labels": [], "entities": [{"text": "Telugu", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.47729235887527466}]}], "abstractContent": [{"text": "This paper describes an end to end dialog system created using sequence to sequence learning and memory networks for Telugu, a low-resource language.", "labels": [], "entities": []}, {"text": "We automatically generate dialog data for Tel-ugu in the tourist domain, using a knowledge base that provides tourist place, type, tour time, etc.", "labels": [], "entities": []}, {"text": "Using this data, we train a sequence to sequence model to learn system responses in the dialog.", "labels": [], "entities": []}, {"text": "In order to add the query prediction for information retrieval (through API calls), we train a memory network.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7593328356742859}]}, {"text": "We also handle cases requiring updation of API calls and querying for additional information.", "labels": [], "entities": []}, {"text": "Using the combination of sequence to sequence learning and memory network, we successfully create an end to end dialog system for Telugu.", "labels": [], "entities": []}], "introductionContent": [{"text": "There have been few attempts to create dialog systems for Telugu, which are mostly rule-based systems using ad-hoc user interactions to test the system rather than over a set of prepared test dialogs ().", "labels": [], "entities": []}, {"text": "This is primarily due to alack of dialog data as Telugu is a low-resource language.", "labels": [], "entities": []}, {"text": "proclaim that the greatest bottleneck for statistical approaches to dialog system development is the collection of appropriate data which is especially true for task oriented dialog systems; that for task-oriented dialog systems, indomain data is essential.", "labels": [], "entities": [{"text": "dialog system development", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.8036161859830221}]}, {"text": "Dialog models using neural networks are able to leverage the large amounts of data to learn meaningful representations for natural language and generation strategies, and require only a minimal amount of domain knowledge and handcrafting (.", "labels": [], "entities": []}, {"text": "The neural networks are used to represent both dialog histories and to produce output either through a generative model that generates responses word-by-word conditioned on a dialog context (which is the model this paper uses) or through a discriminative model that is trained to select an appropriate response from a set of candidate responses.", "labels": [], "entities": []}, {"text": "We use both the models for generating system responses in our dialog system.", "labels": [], "entities": []}, {"text": "Sequence to Sequence learning () has been used to build end-to-end trainable non-task-oriented conversational dialog systems (.", "labels": [], "entities": []}, {"text": "This approach models dialog as a source to target sequence transduction problem, applying an encoder network ( ) to encode a user query into a distributed vector representation of its semantics, which conditions a decoder network to generate each system response.", "labels": [], "entities": []}, {"text": "This has been extended to a taskoriented system that interacts with a knowledge base by.", "labels": [], "entities": []}, {"text": "End-to-end dialog systems are trained on past dialogs directly, with no assumptions made on the basis of the domain or on the structure of the dialog, which makes scaling up automatically to new domains easy.", "labels": [], "entities": []}, {"text": "As an end-to-end neural model, Memory Networks, with an attention based architecture, showed promising results for non goaloriented dialog (, and have also been applied to question answering ( and language modelling (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.885834664106369}, {"text": "language modelling", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7443473935127258}]}, {"text": "However, goaloriented dialog requires the system to ask questions to clearly define a user request, query knowledge bases, etc., as extended by.", "labels": [], "entities": []}, {"text": "We first create a corpus of Telugu dialog data in the Tourist domain, which we then use to train our sequence to sequence and memory network models.", "labels": [], "entities": []}, {"text": "We report our results for system response generation through the sequence to sequence model, and our results for API call generation, for retrieving information from knowledge base, through the memory network model.", "labels": [], "entities": [{"text": "system response generation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7057387828826904}, {"text": "API call generation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7620252370834351}]}, {"text": "Through this combination of sequence to sequence learning and memory network, we successfully create an end-to-end dialog system for the tourist domain in Telugu.", "labels": [], "entities": []}, {"text": "After discussing Related Work in Section 2, we outline the tasks our system must perform in Section 3, then we discuss the motivation behind our system pipeline in Section 4, Section 5 describes dialog data creation strategy, followed by sequence-to-sequence model for producing system responses in Section 6, Section 7 deals with the memory network layer for generating API calls, finally followed by the conclusion and future work in Sections 8 and 9 respectively.", "labels": [], "entities": [{"text": "dialog data creation", "start_pos": 195, "end_pos": 215, "type": "TASK", "confidence": 0.7000717222690582}]}], "datasetContent": [{"text": "We conduct five experiments with the sequence to sequence model:  In the third and fifth experiments, we replace the full API call (api call kanchanbagh historical 15) with just api call, which the sequence to sequence model learns as a placeholder.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sequence to Sequence Experiment Re- sults", "labels": [], "entities": []}, {"text": " Table 2: Memory Network Experiment Results", "labels": [], "entities": [{"text": "Memory Network Experiment", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6539444526036581}]}]}