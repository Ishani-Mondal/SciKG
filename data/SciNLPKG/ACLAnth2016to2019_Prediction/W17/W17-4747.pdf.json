{"title": [{"text": "DCU System Report on the WMT 2017 Multi-modal Machine Translation Task", "labels": [], "entities": [{"text": "DCU System Report", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9021727641423544}, {"text": "WMT 2017 Multi-modal Machine Translation", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.6683346331119537}]}], "abstractContent": [{"text": "We report experiments with multi-modal neural machine translation models that incorporate global visual features in different parts of the encoder and decoder, and use the VGG19 network to extract features for all images.", "labels": [], "entities": [{"text": "multi-modal neural machine translation", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.7811911106109619}, {"text": "VGG19 network", "start_pos": 172, "end_pos": 185, "type": "DATASET", "confidence": 0.9721238613128662}]}, {"text": "In our experiments, we explore both different strategies to include global image features and also how ensembling different models at inference time impact translations.", "labels": [], "entities": []}, {"text": "Our submissions ranked 3rd best for translating from En-glish into French, always improving considerably over an neural machine translation baseline across all language pair evaluated , e.g. an increase of 7.0-9.2 METEOR points.", "labels": [], "entities": [{"text": "translating from En-glish", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8682076334953308}, {"text": "METEOR", "start_pos": 214, "end_pos": 220, "type": "METRIC", "confidence": 0.9960702657699585}]}], "introductionContent": [{"text": "In this paper we report on our application of three different multi-modal neural machine translation (NMT) systems to translate image descriptions.", "labels": [], "entities": [{"text": "multi-modal neural machine translation (NMT)", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.7967312591416496}, {"text": "translate image descriptions", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.8310882051785787}]}, {"text": "We use encoder-decoder attentive multi-modal NMT models where each training example consists of one source variable-length sequence, one image, and one target variable-length sequence, and a model is trained to translate sequences in the source language into corresponding sequences in the target language while taking the image into consideration.", "labels": [], "entities": []}, {"text": "We use the three models introduced in, which integrate global image features extracted using a pre-trained convolutional neural network into NMT (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state.", "labels": [], "entities": []}, {"text": "We are inspired by the recent success of multimodal NMT models applied to the translation of image descriptions).", "labels": [], "entities": [{"text": "translation of image descriptions", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.844259724020958}]}, {"text": "incorporate global visual features into NMT with some success, and propose to use local visual features instead, achieving better results.", "labels": [], "entities": []}, {"text": "We follow and investigate whether we can achieve better results while still using global visual features, which are considerably smaller and simpler to integrate when compared to local features.", "labels": [], "entities": []}, {"text": "We expect that, by integrating visual information when translating image descriptions, we are able to exploit valuable information from both modalities when generating the target description, effectively grounding machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.7352175712585449}]}], "datasetContent": [{"text": "We report results for Task 1, specifically when translating from English into German (en-de) and French (en-fr).", "labels": [], "entities": []}, {"text": "We conducted experiments on the constrained version of the shared task, which means that the only training data we used is the data released by the shared task organisers, i.e. the translated Multi30k (M30k T ) data set () with the additional French image descriptions, included for the 2017 run of the shared task.", "labels": [], "entities": [{"text": "Multi30k (M30k T ) data set", "start_pos": 192, "end_pos": 219, "type": "DATASET", "confidence": 0.6444003496851239}]}, {"text": "Our encoder is a bi-directional RNN with GRU, one 1024D single-layer forward RNN and one 1024D single-layer backward RNN.", "labels": [], "entities": [{"text": "GRU", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.979947566986084}]}, {"text": "Throughout, we parameterise our models using 620D source and target word embeddings, and both are trained jointly with our model.", "labels": [], "entities": []}, {"text": "All non-recurrent matrices are initialised by sampling from a Gaussian distribution (\u00b5 = 0, \u03c3 = 0.01), recurrent matrices are random orthogonal and bias vectors are all initialised to #\u00bb 0 . We apply dropout () with a probability of 0.3 in source and target word embeddings, in the image features, in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN.", "labels": [], "entities": []}, {"text": "We follow and apply dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps.", "labels": [], "entities": []}, {"text": "The translated Multi30k training and validation sets contain 29k and 1014 images respectively, each accompanied by a sentence triple, the original English sentence and its gold-standard translations into German and into French.", "labels": [], "entities": []}, {"text": "We use the scripts in the Moses SMT Toolkit () to normalise, lowercase and tokenize English, German and French descriptions and we also convert space-separated tokens into subwords (.", "labels": [], "entities": [{"text": "Moses SMT Toolkit", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.7584854364395142}, {"text": "tokenize English, German and French descriptions", "start_pos": 75, "end_pos": 123, "type": "TASK", "confidence": 0.8281333105904716}]}, {"text": "The subword models are trained jointly for English-German descriptions and separately for English-French descriptions using the English-German and EnglishFrench WMT 2015 data (.", "labels": [], "entities": [{"text": "EnglishFrench WMT 2015 data", "start_pos": 147, "end_pos": 174, "type": "DATASET", "confidence": 0.8918170481920242}]}, {"text": "English-German models have a final vocabulary of 74K English and 81K German subword tokens, and English-French models 82K English and 82K French subword tokens.", "labels": [], "entities": []}, {"text": "If sentences in English, German or French are longer than 80 tokens, they are discarded.", "labels": [], "entities": []}, {"text": "Finally, we use the 29K entries in the M30k T training set for training our models, and the 1, 014 entries in the M30k T development set for model selection, early stopping the training procedure in case the model stops improving BLEU scores on this development set.", "labels": [], "entities": [{"text": "M30k T training set", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9202011376619339}, {"text": "M30k T development set", "start_pos": 114, "end_pos": 136, "type": "DATASET", "confidence": 0.9215361773967743}, {"text": "model selection", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.6906045228242874}, {"text": "BLEU", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9993836879730225}]}, {"text": "We evaluate our English-German models on three held-out test sets, the Multi30k 2016/2017 and the MSCOCO 2017 test sets, and our English-French models on the Multi30k 2017 and the MSCOCO 2017 test sets.", "labels": [], "entities": [{"text": "Multi30k 2016/2017", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.9340761303901672}, {"text": "MSCOCO 2017 test sets", "start_pos": 98, "end_pos": 119, "type": "DATASET", "confidence": 0.9043851643800735}, {"text": "MSCOCO 2017 test sets", "start_pos": 180, "end_pos": 201, "type": "DATASET", "confidence": 0.9499114602804184}]}, {"text": "We evaluate translation quality quantitatively in terms of BLEU4 (), METEOR, and TER ().", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9546282887458801}, {"text": "BLEU4", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9991233944892883}, {"text": "METEOR", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9970965385437012}, {"text": "TER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9987471103668213}]}], "tableCaptions": [{"text": " Table 1: Results for the M30k T 2017 English- German and English-French test sets. All models  are trained on the original M30k T training data.  Our ensemble uses four multi-modal models, all  independently trained: two models IMG D , one  model IMG E , and one model IMG 2W .", "labels": [], "entities": [{"text": "M30k T 2017 English- German and English-French test sets", "start_pos": 26, "end_pos": 82, "type": "DATASET", "confidence": 0.9146968364715576}, {"text": "M30k T training data", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.9253129959106445}]}, {"text": " Table 2: Results for the MSCOCO 2017 English- German and English-French test sets. All mod- els are trained on the original M30k T training  data. Ensemble uses four multi-modal models,  all trained independently: two models IMG D , one  model IMG E , and one model IMG 2W .", "labels": [], "entities": [{"text": "MSCOCO 2017 English- German and English-French test sets", "start_pos": 26, "end_pos": 82, "type": "DATASET", "confidence": 0.8680437670813667}, {"text": "M30k T training  data", "start_pos": 125, "end_pos": 146, "type": "DATASET", "confidence": 0.8743749260902405}]}, {"text": " Table 3: Results for the best model of Calixto et al. (2017a), which is pre-trained on the English-German  WMT 2015 (Bojar et al., 2015), and different combinations of multi-modal models, all trained on the  original M30k T training data only, evaluated on the M30k T 2016 test set.", "labels": [], "entities": [{"text": "WMT 2015", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.7768278419971466}, {"text": "M30k T training data", "start_pos": 218, "end_pos": 238, "type": "DATASET", "confidence": 0.8917016983032227}, {"text": "M30k T 2016 test set", "start_pos": 262, "end_pos": 282, "type": "DATASET", "confidence": 0.9542030811309814}]}]}