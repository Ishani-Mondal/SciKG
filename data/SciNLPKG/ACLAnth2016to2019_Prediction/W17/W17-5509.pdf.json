{"title": [{"text": "Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning", "labels": [], "entities": [{"text": "Reward-Balancing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.931944727897644}]}], "abstractContent": [{"text": "Reinforcement learning is widely used for dialogue policy optimization where the reward function often consists of more than one component, e.g., the dialogue success and the dialogue length.", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8651828467845917}, {"text": "dialogue policy optimization", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8540215293566386}]}, {"text": "In this work, we propose a structured method for finding a good balance between these components by searching for the optimal reward component weighting.", "labels": [], "entities": []}, {"text": "To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required.", "labels": [], "entities": []}, {"text": "We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina Spoken Dialogue System (SDS), one of the main problems is to find appropriate system behaviour for any given situation.", "labels": [], "entities": [{"text": "Spoken Dialogue System (SDS)", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6889257381359736}]}, {"text": "This problem is often modelled using reinforcement learning (RL) where the task is to find an optimal policy \u03c0(b) = a which maps the current belief state b-an estimate of the user goal-to the next system action a.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7042950093746185}]}, {"text": "To do this, RL algorithms seek to optimize an objective function, the reward r, using sample dialogues.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9634228944778442}]}, {"text": "In contrast to other RL tasks (like AlphaGo (), the reward used in goal-oriented dialogue systems usually consists of more than one objective (e.g., task success and dialogue length ().", "labels": [], "entities": [{"text": "RL tasks", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9189604818820953}]}, {"text": "However, balancing these rewards is rarely considered and the goal of this paper is to propose a structured method for finding the optimal weights fora multiple objective reward function.", "labels": [], "entities": []}, {"text": "Finding a good balance between multiple objectives is usually domain-specific and not straight-forward.", "labels": [], "entities": []}, {"text": "For example, in the case of task success and dialogue length, if the reward for success is too high, the learning algorithm is insensitive to potentially irritating actions such as repeat provided that the dialogue is ultimately successful.", "labels": [], "entities": []}, {"text": "Conversely, if the reward for success is too small, the resulting policy may irritate users by offering inappropriate solutions before fully illiciting the user's requirements.", "labels": [], "entities": []}, {"text": "In this paper, we propose to find a suitable reward balance by searching through the space of reward component weights.", "labels": [], "entities": []}, {"text": "Doing this with conventional RL techniques is infeasible as a policy must be trained for each candidate balance and this requires an enormous number of training dialogues.", "labels": [], "entities": [{"text": "RL", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9543105363845825}]}, {"text": "To alleviate this, we propose to use multi-objective RL (MORL) which is specifically designed for this task (among others ().", "labels": [], "entities": [{"text": "multi-objective RL (MORL)", "start_pos": 37, "end_pos": 62, "type": "METRIC", "confidence": 0.6025708913803101}]}, {"text": "Then, only one policy needs to be trained which maybe evaluated with several candidate balances.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first time MORL has been applied to dialogue policy optimization.", "labels": [], "entities": [{"text": "MORL", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.4409387707710266}, {"text": "dialogue policy optimization", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.8446051875750223}]}, {"text": "In contrast to previous work which explicitly selects component weights to maximize user satisfaction) explicitly, the proposed method enables optimisation of an implicit goal by allowing the interplay each reward component to be explored at low computational cost.", "labels": [], "entities": []}, {"text": "Several different algorithms have previously been used for MORL.", "labels": [], "entities": [{"text": "MORL", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.9063235521316528}]}, {"text": "In this work, we propose a novel MORL algorithm based on Gaussian processes.", "labels": [], "entities": []}, {"text": "This is described in Section 2 along with a brief introduction to MORL.", "labels": [], "entities": [{"text": "MORL", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.70611172914505}]}, {"text": "In Section 3, the proposed method for finding a good reward balance with MORL is presented.", "labels": [], "entities": [{"text": "MORL", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9710429310798645}]}, {"text": "Section 4 describes the application and evaluation of the balancing method on six different domains.", "labels": [], "entities": []}, {"text": "Finally conclusions are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The reward balancing method described in the previous section is applied to six domains: finding TVs, laptops, restaurants or hotels (the latter two in Cambridge and San Francisco For consistency with previous work) the rewards r w s = 20 and r w l = \u22121 are used representing the weight configuration w = (0.5, 0.5).", "labels": [], "entities": []}, {"text": "This results in r s = 40 and r l = \u22122.", "labels": [], "entities": []}, {"text": "For the evaluation, simulated dialogues were created using the statistical spoken dialogue toolkit PyDial ( . It contains an agenda-based user simulator () with an error model to simulate the semantic error rate (SER) encountered in real systems due to the noisy speech channel.", "labels": [], "entities": [{"text": "semantic error rate (SER)", "start_pos": 192, "end_pos": 217, "type": "METRIC", "confidence": 0.8107104549805323}]}, {"text": "A policy has been trained for each domain using multi-objective GPSARSA with 3,000 dialogues and an SER of 15%.", "labels": [], "entities": [{"text": "SER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9995957016944885}]}, {"text": "Each policy was evaluated with 300 dialogues for each weight configuration in {(0.1, 0.9), (0.2, 0.8), . .", "labels": [], "entities": []}, {"text": ", (0.9, 0.1)}.", "labels": [], "entities": []}, {"text": "The results in are the averages of five trained policies with different random seeds.", "labels": [], "entities": []}, {"text": "All curves follow a similar pattern: at some point, the success curve reaches a plateau where the performance does not increase any further with higher w s . The following weights were selected: CamRestaurants w s = 0.4; CamHotels w s = 0.6; SFRestaurants w s = 0.6; SFHotels w s = 0.7; TV w s = 0.6; Laptops w s = 0.7.", "labels": [], "entities": []}, {"text": "These weights were selected by hand according to the success rate 2 as well as the average dialogue length.", "labels": [], "entities": []}, {"text": "The selected weights were scaled to keep the T (s): The MORL success-weight and length-weight curves (m, task success rate (TSR) on left, number of turns T on right vertical axes; success weights w son horizontal axes) after 3,000 training dialogues.", "labels": [], "entities": [{"text": "MORL", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9363489747047424}, {"text": "length-weight", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.9820536375045776}, {"text": "task success rate (TSR)", "start_pos": 105, "end_pos": 128, "type": "METRIC", "confidence": 0.7655762583017349}]}, {"text": "Each data point is the average over five policies with different seeds where each policy/weight configuration is evaluated with 300 dialogues.", "labels": [], "entities": []}, {"text": "As a comparison, the same curves using single-objective RL (s, separate policies trained for each balance) have been created after selecting the weights.: Task success rates (TSRs) and number of turns after 4,000 training dialogues using a success reward of 20 (baseline) compared to the optimised success reward r w s . All TSR differences are statistically significant (t-test, p < 0.05).", "labels": [], "entities": [{"text": "Task success rates (TSRs)", "start_pos": 155, "end_pos": 180, "type": "METRIC", "confidence": 0.7337842583656311}]}, {"text": "turn penalty w w l constant at \u22121.", "labels": [], "entities": [{"text": "turn penalty w w l", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9274008393287658}]}, {"text": "Using these reward settings, each domain was evaluated with 4,000 dialogues in 10 batches.", "labels": [], "entities": []}, {"text": "After each batch, the policies were evaluated with 300 dialogues.", "labels": [], "entities": []}, {"text": "The final results shown in (selection of learning curves in) are compared to the baseline of w = (0.5, 0.5) (i.e. standard unoptimised reward component weight balance).", "labels": [], "entities": []}, {"text": "Evidently, optimising the balance has a significant impact on the performance of the trained polices.", "labels": [], "entities": []}, {"text": "To analyse the performance of multi-objective GPSARSA, policies were trained and evaluated for each reward balance with single-objective (SO) GPSARSA (see after the weights had been selected.", "labels": [], "entities": []}, {"text": "Each SO policy was trained with 1,000 dialogues and evaluated with 300 dialogues, all averaged over five runs.", "labels": [], "entities": [{"text": "SO policy", "start_pos": 5, "end_pos": 14, "type": "TASK", "confidence": 0.9249061048030853}]}, {"text": "The success-weight curves for SORL clearly resemble the MORL curves for almost all domains except for CamRestaurants where it leads to an incorrect selection of weights.", "labels": [], "entities": [{"text": "MORL", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7313005924224854}, {"text": "CamRestaurants", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9490386843681335}]}, {"text": "This maybe attributed to the kernel used for multi-objective GPSARSA.", "labels": [], "entities": []}, {"text": "It is worth noting that for the presented full MORL analysis, 3,000 training dialogues were necessary for each domain to find a good balance.", "labels": [], "entities": []}, {"text": "This is significantly less than the 9,000 dialogues needed for the SORL analysis and this difference would increase further fora finer grain search grid.", "labels": [], "entities": [{"text": "SORL analysis", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.5900993943214417}]}], "tableCaptions": [{"text": " Table 1: Task success rates (TSRs) and number of  turns after 4,000 training dialogues using a success  reward of 20 (baseline) compared to the optimised  success reward r w  s . All TSR differences are statis- tically significant (t-test, p < 0.05).", "labels": [], "entities": [{"text": "Task success rates (TSRs)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6780161956946055}]}]}