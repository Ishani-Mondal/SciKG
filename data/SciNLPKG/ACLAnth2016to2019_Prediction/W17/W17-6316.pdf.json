{"title": [], "abstractContent": [{"text": "We present a neural transition-based parser for spinal trees, a dependency representation of constituent trees.", "labels": [], "entities": []}, {"text": "The parser uses Stack-LSTMs that compose constituent nodes with dependency-based derivations.", "labels": [], "entities": []}, {"text": "In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure , suggesting that LSTMs induce useful states by themselves.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a clear trend in neural transition systems for parsing sentences into dependency trees and constituent trees).", "labels": [], "entities": []}, {"text": "These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation.", "labels": [], "entities": []}, {"text": "This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation.", "labels": [], "entities": []}, {"text": "In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure.", "labels": [], "entities": []}, {"text": "This representation is inherent in head-driven models and was used by with a higher-order factored model.", "labels": [], "entities": []}, {"text": "We extend the Stack-LSTMs by from dependency to spinal parsing, by augmenting the composition operations to include constituent information in the form of spines.", "labels": [], "entities": [{"text": "spinal parsing", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.6260924637317657}]}, {"text": "To parse sentences, we use the extension by of the arc-standard system for dependency parsing).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8094377219676971}]}, {"text": "This parsing system generalizes shift-reduce methods to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.", "labels": [], "entities": []}, {"text": "In experiments on the Penn Treebank, we look at how sensitive our method is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies) or those by.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.9947030544281006}]}, {"text": "This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents.", "labels": [], "entities": []}, {"text": "We also show that turning a dependency Stack-LSTM into spinal results in some improvements.", "labels": [], "entities": [{"text": "spinal", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.989147961139679}]}], "datasetContent": [{"text": "We experiment with stack-LSTM spinal models trained with different types of head rules.", "labels": [], "entities": []}, {"text": "Our goal is to check how the head identities, which define the derivation sequence, interact with the ability of Stack-LSTMs to propagate latent information beyond the local scope of each action.", "labels": [], "entities": []}, {"text": "We use the Penn Treebank () with standard splits.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9947828352451324}]}, {"text": "We start training four spinal models, varying the head rules that define the spinal derivations: 4 \u2022 Leftmost heads as in.", "labels": [], "entities": []}, {"text": "\u2022 Stanford Dependencies (SD)), as in.", "labels": [], "entities": [{"text": "Stanford Dependencies (SD))", "start_pos": 2, "end_pos": 29, "type": "DATASET", "confidence": 0.8571958661079406}]}, {"text": "\u2022 Yamada and Matsumoto heads (YM).", "labels": [], "entities": []}, {"text": "presents constituency and dependency metrics on the development set.", "labels": [], "entities": []}, {"text": "The model using rightmost heads works the best at 91.11 F1, followed by the one using leftmost heads.", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9362463355064392}]}, {"text": "It is worth to note that the two models using structural head identities (right or left) work better than those using linguistic ones.", "labels": [], "entities": []}, {"text": "This suggests that the Stack-LSTM model already finds useful head-child relations in a constituent by parsing from the left (or right) even if there are non-local interactions.", "labels": [], "entities": []}, {"text": "In this case, head rules are not useful.", "labels": [], "entities": []}, {"text": "The same shows two ablation studies.", "labels": [], "entities": [{"text": "ablation", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9517713189125061}]}, {"text": "First, we turnoff the composition of constituent nodes into the latent derivations (Eq 1).", "labels": [], "entities": []}, {"text": "The ablated models, tagged with \"no n-comp\", perform from 0.5 to 1 points F1 worse, showing the benefit of adding constituent structure.", "labels": [], "entities": []}, {"text": "Then, we check if constituent structure is any useful for dependency parsing metrics.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8376753032207489}]}, {"text": "To this end, we emulate a dependency parser using a spinal model by taking standard Stanford dependency trees and adding a dummy constituent for every head with all its children.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7990774214267731}]}, {"text": "This model, tagged \"SD heads, dummy spines\", is slightly outperformed by the \"SD heads\" model using true spines, even though the margin is small.", "labels": [], "entities": []}, {"text": "present results on the test, for constituent and dependency parsing respectively.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.6531011909246445}]}, {"text": "As shown in our model is competitive compared to the best parsers; the generative parsers by,  and are better than the rest, but compared to the rest our parser is at the same level or better.", "labels": [], "entities": []}, {"text": "The most similar system is by and our parser significantly improves the performance.", "labels": [], "entities": []}, {"text": "Considering dependency parsing, our model is worse than the ones that train with exploration as and , but it slightly improves the parser by with static training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8120269477367401}]}, {"text": "The systems that calculate dependencies by transforming phrase-structures with conversion rules and that use generative training are ahead compared to the rest.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development results for spinal models,  in terms of labeled precision (LP), recall (LR)  and F1 for constituents, and unlabeled attach- ment score (UAS) against Stanford dependencies.  Spinal models are trained using different head an- notations (see text). Models labeled with \"no n- comp\" do not use node compositions. The model  labeled with \"dummy spines\" corresponds to a  standard dependency model.", "labels": [], "entities": [{"text": "labeled precision (LP)", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.8067729234695434}, {"text": "recall (LR)", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9690197557210922}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9682564735412598}, {"text": "unlabeled attach- ment score (UAS)", "start_pos": 128, "end_pos": 162, "type": "METRIC", "confidence": 0.8699594587087631}]}, {"text": " Table 2: Constituency results on the PTB test set.", "labels": [], "entities": [{"text": "Constituency", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9287960529327393}, {"text": "PTB test set", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9833494226137797}]}, {"text": " Table 3: Stanford Dependency results (UAS) on  PTB test set. Parsers marked with * calculate de- pendencies by transforming phrase-structures with  conversion rules.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9530752698580424}]}]}