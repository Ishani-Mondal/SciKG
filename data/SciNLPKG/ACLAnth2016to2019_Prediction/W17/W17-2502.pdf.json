{"title": [{"text": "Deep Investigation of Cross-Language Plagiarism Detection Methods", "labels": [], "entities": [{"text": "Deep Investigation of Cross-Language Plagiarism Detection", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6116622140010198}]}], "abstractContent": [{"text": "This paper is a deep investigation of cross-language plagiarism detection methods on anew recently introduced open dataset, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts).", "labels": [], "entities": [{"text": "cross-language plagiarism detection", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.6733328302701315}]}, {"text": "We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granu-larities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages.", "labels": [], "entities": [{"text": "cross-language plagiarism detection", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.7178109089533488}]}], "introductionContent": [{"text": "Plagiarism is a very significant problem nowadays, specifically in higher education institutions.", "labels": [], "entities": []}, {"text": "In monolingual context, this problem is rather well treated by several recent researches).", "labels": [], "entities": []}, {"text": "Nevertheless, the expansion of the Internet, which facilitates access to documents throughout the world and to increasingly efficient (freely available) machine translation tools, helps to spread cross-language plagiarism.", "labels": [], "entities": []}, {"text": "Crosslanguage plagiarism means plagiarism by translation, i.e. a text has been plagiarized while being translated (manually or automatically).", "labels": [], "entities": []}, {"text": "The challenge in detecting this kind of plagiarism is that the suspicious document is no longer in the same language of its source.", "labels": [], "entities": []}, {"text": "In this relatively new field of research, no systematic evaluation of the main methods, on several language pairs, for different text granularities and for different text genres, has been proposed yet.", "labels": [], "entities": []}, {"text": "This is what we propose in this paper.", "labels": [], "entities": []}, {"text": "The paper focus is on crosslanguage semantic textual similarity detection which is the main part (with source retrieval) in cross-language plagiarism detection.", "labels": [], "entities": [{"text": "crosslanguage semantic textual similarity detection", "start_pos": 22, "end_pos": 73, "type": "TASK", "confidence": 0.8221408486366272}, {"text": "cross-language plagiarism detection", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.7561674118041992}]}, {"text": "The evaluation dataset used) allows us to run a large amount of experiments and analyses.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first time that full potential of such a diverse dataset is used for benchmarking.", "labels": [], "entities": [{"text": "benchmarking", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.9586287140846252}]}, {"text": "So, the paper main contribution is a systematic evaluation of cross-language similarity detection methods (using in plagiarism detection) on different languages, sizes and genres of texts through a reproducible evaluation protocol.", "labels": [], "entities": [{"text": "cross-language similarity detection", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.7323491970698038}]}, {"text": "Robust conclusions are derived on the best methods while deeply analyzing correlations across document styles and languages.", "labels": [], "entities": []}, {"text": "Due to space limitations, we only provide a subset of our experiments in the paper while more result tables and correlation analyses are provided as supplementary material on a Web link . Outline.", "labels": [], "entities": [{"text": "Outline", "start_pos": 188, "end_pos": 195, "type": "DATASET", "confidence": 0.973328709602356}]}, {"text": "After presenting the dataset used for our study in section 2, and reviewing the stateof-the-art methods of cross-language plagiarism detection that we evaluate in section 3, we describe the evaluation protocol employed in section 4.", "labels": [], "entities": [{"text": "cross-language plagiarism detection", "start_pos": 107, "end_pos": 142, "type": "TASK", "confidence": 0.6860350171724955}]}, {"text": "Then, section 5.1 presents the correla-tion of the methods across language pairs, while section 5.2 presents a detailed analysis on only English-French pair.", "labels": [], "entities": []}, {"text": "Finally, section 6 concludes this work and gives a few perspectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "The reference dataset used during our study is the new dataset 2 recently introduced by.", "labels": [], "entities": []}, {"text": "The dataset was specially designed fora rigorous evaluation of cross-language textual similarity detection.", "labels": [], "entities": [{"text": "cross-language textual similarity detection", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.7362652346491814}]}, {"text": "The different characteristics of the dataset are synthesized in, while Table 2 presents the number of aligned units by subcorpus and by granularity.", "labels": [], "entities": []}, {"text": "We apply the same evaluation protocol as in's paper.", "labels": [], "entities": []}, {"text": "We build a distance matrix of size N x M , with M = 1,000 and N = |S| where S is the evaluated sub-corpus.", "labels": [], "entities": []}, {"text": "Each textual unit of S is compared to itself (actually, since this is cross-lingual similarity detection, each source language unit is compared to its corresponding unit in the target language) and to M -1 other units randomly selected from S.", "labels": [], "entities": [{"text": "cross-lingual similarity detection", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.6245449980099996}]}, {"text": "The same unit maybe selected several times.", "labels": [], "entities": []}, {"text": "Then, a matching score for each comparison performed is obtained, leading to the distance matrix.", "labels": [], "entities": [{"text": "distance", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9754951000213623}]}, {"text": "Thresholding on the matrix is applied to find the threshold giving the best F 1 score.", "labels": [], "entities": [{"text": "Thresholding", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9851791262626648}, {"text": "F 1 score", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9767206311225891}]}, {"text": "The F 1 score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9833449323972067}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995266199111938}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9971224665641785}]}, {"text": "Precision is defined as the proportion of relevant matches (similar crosslanguage units) retrieved among all the matches retrieved.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9897910952568054}]}, {"text": "Recall is the proportion of relevant matches retrieved among all the relevant matches to retrieve.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9915812611579895}]}, {"text": "Each method is applied on each subcorpus for chunk and sentence granularities.", "labels": [], "entities": []}, {"text": "For each configuration (i.e. a particular method applied on a particular sub-corpus considering a particular granularity), 10 folds are carried out by changing the M selected units.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the dataset (Ferrero et al., 2016) for each sub-corpus. The percentages of  named entities (NE) present in the last column are estimated with Stanford Named Entity Recognizer 3 .", "labels": [], "entities": [{"text": "Stanford Named Entity Recognizer", "start_pos": 171, "end_pos": 203, "type": "DATASET", "confidence": 0.8561936914920807}]}, {"text": " Table 2: Number of aligned documents, sentences and noun chunks by sub-corpus.", "labels": [], "entities": []}, {"text": " Table 3: Overall F 1 score over all sub-corpora of the state-of-the-art methods for each language pair  (EN: English; FR: French; ES: Spanish).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9849418799082438}]}, {"text": " Table 4: Pearson correlations of the overall F 1 score over all sub-corpora of all methods between the  different language pairs (EN: English; FR: French; ES: Spanish).", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9223611056804657}, {"text": "F 1 score", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9748754700024923}]}, {"text": " Table 8: Average F 1 scores and confidence intervals of methods applied on EN\u2192FR sub-corpora at  chunk and sentence level -10 folds validation.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.930571973323822}]}, {"text": " Table 6: Pearson correlations of the results of all  methods on all sub-corpora, between the chunk  and the sentence granularity, by language pair  (EN: English; FR: French; ES: Spanish) (calcu- lated from Table 3).", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9285700619220734}, {"text": "calcu- lated", "start_pos": 189, "end_pos": 201, "type": "METRIC", "confidence": 0.7155479987462362}]}, {"text": " Table 7: Pearson correlations of the results on  all sub-corpora on all language pairs, between the  chunk and the sentence granularity, by methods  (calculated from", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.902446985244751}]}, {"text": " Table 9: Precision (P), Recall (R) and F 1 score,  reached at a certain threshold (T), of some state- of-the-art methods for a data subset made with  1000 positives and 1000 negatives (mis)matches  -10 folds validation.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9349859356880188}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9528573155403137}, {"text": "F 1 score", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9788979689280192}]}]}