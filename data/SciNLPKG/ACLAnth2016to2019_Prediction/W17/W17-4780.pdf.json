{"title": [], "abstractContent": [{"text": "This paper describes our submission to the WMT 2017 Neural MT Training Task.", "labels": [], "entities": [{"text": "WMT 2017 Neural MT Training Task", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.7308158576488495}]}, {"text": "We modified the provided NMT system in order to allow for interrupting and continuing the training of models.", "labels": [], "entities": []}, {"text": "This allowed mid-training batch size decremen-tation and incrementation at variable rates.", "labels": [], "entities": []}, {"text": "In addition to the models with variable batch size, we tried different setups with pre-trained word2vec embeddings.", "labels": [], "entities": []}, {"text": "Aside from batch size incrementation, all our experiments performed below the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in the WMT 2017 NMT Training Task, experimenting with pre-trained word embeddings and mini-batch sizing.", "labels": [], "entities": [{"text": "WMT 2017 NMT Training Task", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.5812865674495697}]}, {"text": "The underlying NMT system (Neural Monkey, Helcl and Libovick\u00b4ybovick\u00b4y, 2017) was provided by the task organizers (, including the training data for English to Czech translation.", "labels": [], "entities": [{"text": "Neural Monkey", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.5301135927438736}]}, {"text": "The goal of the task was to find training criteria and training data layout which leads to the best translation quality.", "labels": [], "entities": []}, {"text": "The provided NMT system is based on an attentional encoder-decoder () and utilizes BPE for vocabulary size reduction to allow handling open vocabulary.", "labels": [], "entities": [{"text": "BPE", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.969487190246582}, {"text": "vocabulary size reduction", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6083089013894399}]}, {"text": "We modified the provided NMT system in order to allow for interruption and continuation of the training process by saving and reloading variable files.", "labels": [], "entities": []}, {"text": "This did not result in any noticeable change in the learning.", "labels": [], "entities": []}, {"text": "Furthermore, it allowed for midtraining mini-batch size decrementation and incrementation at variable rates.", "labels": [], "entities": []}, {"text": "As our main experiment, we tried to employ pre-trained word embeddings to initialize embeddings in the model on the source side (monolingually trained embeddings) and on both source and target sides (bilingually trained embeddings).", "labels": [], "entities": []}, {"text": "Section 1.1 describes our baseline system.", "labels": [], "entities": []}, {"text": "Section 2 examines the pre-trained embeddings and Section 3 the effect of batch size modifications.", "labels": [], "entities": []}, {"text": "Further work and conclusion (Sections 4 and 5) close the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The tested setups are summarized in and the learning curves are plotted in.", "labels": [], "entities": []}, {"text": "The line \"Config for\" indicates which of the provided model sizes was used (the 4GB and 8GB setups differ in embeddings and RNN sizes, otherwise, the network and training are the same).: Pairwise cosine distances between embeddings correlated with standard human judgments for the common subset of the vocabularies.", "labels": [], "entities": []}, {"text": "Best result in each row in bold.", "labels": [], "entities": []}, {"text": "We used uniform distribution from 0 to 1 in the first experiment with embeddings and returned to the baseline normal distribution in subsequent experiments.", "labels": [], "entities": []}, {"text": "The best results we were able to obtain are from a third experiment \"Larger Source-Only\" with batch size increased to 150 but also with differences in other model parameters.", "labels": [], "entities": []}, {"text": "(We ran this setup on a K80 card at Amazon EC2.)", "labels": [], "entities": []}, {"text": "This run is therefore not comparable with any of the remaining runs, but we nevertheless submitted it as our secondary submission for the WMT 2017 training task (i.e. not to be evaluated manually).", "labels": [], "entities": [{"text": "WMT 2017 training task", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.5756964087486267}]}], "tableCaptions": [{"text": " Table 1: The different setups of models initialized with pre-trained embeddings.", "labels": [], "entities": []}, {"text": " Table 2: Pairwise cosine distances between embeddings correlated with standard human judgments for  the common subset of the vocabularies. Best result in each row in bold.", "labels": [], "entities": []}, {"text": " Table 3: The different setups with mini-batch size decrementation. The run reducing every 48h was our  primary submission (*).", "labels": [], "entities": []}]}