{"title": [{"text": "Combining Graph Degeneracy and Submodularity for Unsupervised Extractive Summarization", "labels": [], "entities": [{"text": "Unsupervised Extractive Summarization", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.6521806220213572}]}], "abstractContent": [{"text": "We present a fully unsupervised, extrac-tive text summarization system that leverages a submodularity framework introduced by past research.", "labels": [], "entities": [{"text": "extrac-tive text summarization", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.5891851286093394}]}, {"text": "The framework allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees.", "labels": [], "entities": [{"text": "summaries", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9666714072227478}]}, {"text": "Our main contribution is the novel coverage reward term of the objective function optimized by the greedy algorithm.", "labels": [], "entities": [{"text": "coverage reward term", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.8457498749097189}]}, {"text": "This component builds on the graph-of-words representation of text and the k-core decomposition algorithm to assign meaningful scores to words.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the AMI and ICSI meeting speech corpora, and on the DUC2001 news corpus.", "labels": [], "entities": [{"text": "AMI", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8914855718612671}, {"text": "ICSI meeting speech corpora", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.8854944407939911}, {"text": "DUC2001 news corpus", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.9823908011118571}]}, {"text": "We reach state-of-the-art performance on all datasets.", "labels": [], "entities": []}, {"text": "Results indicate that our method is particularly well-suited to the meeting domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present an extractive text summarization system and test it on automatic meeting speech transcriptions and news articles.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.5879289209842682}, {"text": "meeting speech transcriptions", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.6468950808048248}]}, {"text": "Summarizing spontaneous multiparty meeting speech text is a difficult task fraught with many unique challenges ().", "labels": [], "entities": []}, {"text": "Rather than the wellformed grammatical sentences found in traditional documents, the input data consist of utterances, or fragments of speech transcripts.", "labels": [], "entities": []}, {"text": "Information is diluted across utterances due to speakers frequently hesitating and interrupting each other, and noise abounds in the form of disfluencies (often expressed with filler words such as \"um\", \"uh-huh\", etc.) and unrelated chit-chat.", "labels": [], "entities": []}, {"text": "Since human transcriptions are very costly, the only transcriptions available in practice are often Automatic Speech Recognition (ASR) output.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.7250038782755533}]}, {"text": "Recognition errors introduce much additional noise, making the task of summarization even more difficult.", "labels": [], "entities": [{"text": "summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9923238754272461}]}, {"text": "In this paper, we use ASR output as our sole input, and do not make use of additional data such as prosodic features ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9347886443138123}]}], "datasetContent": [{"text": "We tested our approach on ASR output and regular text.", "labels": [], "entities": [{"text": "ASR", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.901032030582428}]}, {"text": "The lists of meetings/documents IDs we used for development and testing are available on the project online repository 4 .  To align with previous efforts, the extractive summaries generated by our system and the baselines (that will be presented subsequently) were compared against the human abstractive summaries.", "labels": [], "entities": []}, {"text": "We used the ROUGE-1 evaluation metric).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9745492339134216}]}, {"text": "ROUGE, based on n-gram overlap, is the standard way of evaluating performance in the field of textual summarization.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9760090112686157}, {"text": "textual summarization", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.5355528593063354}]}, {"text": "In particular, ROUGE-1, which works at the unigram level, was shown to significantly correlate with human evaluations.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9952744245529175}]}, {"text": "While it has been suggested than correlation maybe weaker in the meeting domain (Liu and Liu, 2008), we stuck to ROUGE because For each dataset, and fora given summarization method, ROUGE scores were computed for each meeting in the test set and then averaged to obtain an overall score for the method (macro-averaging).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9628036618232727}, {"text": "ROUGE", "start_pos": 182, "end_pos": 187, "type": "METRIC", "confidence": 0.9420075416564941}]}, {"text": "For the ICSI corpus, 3 human abstractive summaries are available for each meeting in the test set, so an average score was first computed.", "labels": [], "entities": [{"text": "ICSI corpus", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8796906471252441}]}], "tableCaptions": [{"text": " Table 2: Macro-averaged ROUGE-1 scores on the AMI test  set (20 meetings) for summaries of 350 words. Statistically  significant difference (p < 0.03) w.r.t. all baselines except  PRsub.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.874096691608429}, {"text": "AMI test  set", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9654484987258911}, {"text": "Statistically  significant difference", "start_pos": 103, "end_pos": 140, "type": "METRIC", "confidence": 0.922977348168691}]}, {"text": " Table 3: Macro-averaged ROUGE scores on the ICSI test set  (6 meetings) for summaries of 450 words. Statistically  significant difference (p < 0.05) w.r.t. all baselines except  the oracle and PRsub.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.8633807897567749}, {"text": "ICSI test set", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9783811370531718}]}, {"text": " Table 4: Macro-averaged ROUGE scores on the DUC2001  test set (207 documents) for summaries of 125 words.  Statistically significant difference (p < 0.03) w.r.t. the  Longest greedy and Random baselines.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.8885164260864258}, {"text": "DUC2001  test set (207 documents)", "start_pos": 45, "end_pos": 78, "type": "DATASET", "confidence": 0.9623039535113743}, {"text": "Statistically significant difference", "start_pos": 108, "end_pos": 144, "type": "METRIC", "confidence": 0.8895348310470581}]}]}