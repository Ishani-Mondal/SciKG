{"title": [{"text": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "labels": [], "entities": [{"text": "Explaining Recurrent Neural Network Predictions", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8511985182762146}, {"text": "Sentiment Analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9110280871391296}]}], "abstractContent": [{"text": "Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions.", "labels": [], "entities": [{"text": "understanding feed-forward neural network classification decisions", "start_pos": 155, "end_pos": 221, "type": "TASK", "confidence": 0.7506905347108841}]}, {"text": "In the present work, we extend the usage of LRP to recurrent neural networks.", "labels": [], "entities": []}, {"text": "We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs.", "labels": [], "entities": []}, {"text": "We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.", "labels": [], "entities": [{"text": "sentiment prediction task", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.8364446957906088}]}], "introductionContent": [{"text": "Semantic composition plays an important role in sentiment analysis of phrases and sentences.", "labels": [], "entities": [{"text": "Semantic composition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7504803240299225}, {"text": "sentiment analysis of phrases and sentences", "start_pos": 48, "end_pos": 91, "type": "TASK", "confidence": 0.9207183519999186}]}, {"text": "This includes detecting the scope and impact of negation in reversing a sentiment's polarity, as well as quantifying the influence of modifiers, such as degree adverbs and intensifiers, in rescaling the sentiment's intensity.", "labels": [], "entities": []}, {"text": "Recently, a trend emerged for tackling these challenges via deep learning models such as convolutional and recurrent neural networks, as observed e.g. on the SemEval-2016 Task for Sentiment Analysis in Twitter (.", "labels": [], "entities": [{"text": "SemEval-2016 Task for Sentiment Analysis in Twitter", "start_pos": 158, "end_pos": 209, "type": "TASK", "confidence": 0.663990203823362}]}, {"text": "As these models become increasingly predictive, one also needs to make sure that they work as intended, in particular, their decisions should be made as transparent as possible.", "labels": [], "entities": []}, {"text": "Some forms of transparency are readily obtained from the structure of the model, e.g. recursive nets, where sentiment can be probed at each node of a parsing tree.", "labels": [], "entities": []}, {"text": "Another type of analysis seeks to determine what input features were important for reaching the final top-layer prediction.", "labels": [], "entities": []}, {"text": "Recent work in this direction has focused on bringing measures of feature importance to state-of-the-art models such as deep convolutional neural networks for vision, or to general deep neural networks for text.", "labels": [], "entities": []}, {"text": "Some of these techniques are based on the model's local gradient information while other methods seek to redistribute the function's value on the input variables, typically by reverse propagation in the neural network graph).", "labels": [], "entities": []}, {"text": "We refer the reader to () for an overview on methods for understanding and interpreting deep neural network predictions.", "labels": [], "entities": [{"text": "interpreting deep neural network predictions", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.8324360489845276}]}, {"text": "proposed specific propagation rules for neural networks (LRP rules).", "labels": [], "entities": []}, {"text": "These rules were shown to produce better explanations than e.g. gradient-based techniques, and were also successfully transferred to neural networks for text data (.", "labels": [], "entities": []}, {"text": "In this paper, we extend LRP with a rule that handles multiplicative interactions in the LSTM model, a particularly suitable model for modeling long-range interactions in texts such as those occurring in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.933461606502533}]}, {"text": "We then apply the extended LRP method to a bidirectional LSTM trained on a five-class sentiment prediction task.", "labels": [], "entities": [{"text": "sentiment prediction task", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.8372564911842346}]}, {"text": "It allows us to produce reliable explanations of which words are responsible for attributing sentiment in individual texts, compared to the explanations obtained by using a gradientbased approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}