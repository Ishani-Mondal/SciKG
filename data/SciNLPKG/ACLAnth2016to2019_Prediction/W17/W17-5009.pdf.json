{"title": [{"text": "Predicting Audience's Laughter During Presentations Using Convolutional Neural Network", "labels": [], "entities": [{"text": "Predicting Audience's Laughter", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8941438496112823}]}], "abstractContent": [{"text": "Public speakings play important roles in schools and work places and properly using humor contributes to effective presentations.", "labels": [], "entities": []}, {"text": "For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks.", "labels": [], "entities": []}, {"text": "Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.847667932510376}]}, {"text": "Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8550064563751221}, {"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7107493132352829}]}, {"text": "The advantages of the CNN method are both getting higher detection accuracies and being able to learn essential features automatically.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to make effective presentations has been found to be linked with success at school and in the workplace (.", "labels": [], "entities": []}, {"text": "Humor plays an important role in successful public speaking, e.g., helping to reduce public speaking anxiety often regarded as the most prevalent type of social phobia, generating shared amusement to boost persuasive power, and serving as a means to attract attention and reduce tension.", "labels": [], "entities": []}, {"text": "Automatically simulating an audience's reactions to humor will not only be useful for presentation training, but also improve conversational systems by giving machines more empathetic power.", "labels": [], "entities": [{"text": "presentation training", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8963046371936798}]}, {"text": "The present study reports our efforts in recognizing utterances that cause laughter in presentations.", "labels": [], "entities": []}, {"text": "These include building a corpus from TED talks and using Convolutional Neural Networks (CNNs) in the recognition.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 briefly reviews the previous related research; Section 3 describes the corpus we collected from TED talks; Section 4 describes the text classification methods; Section 5 reports on our experiments; finally, Section 6 discusses the findings of our study and plans for future work.", "labels": [], "entities": [{"text": "text classification", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7543965280056}]}], "datasetContent": [{"text": "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 5 (denoted as Pun).", "labels": [], "entities": [{"text": "TED Talk corpus", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8411306540171305}, {"text": "Pun of the Day corpus 5", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.5653917193412781}]}, {"text": "Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters.", "labels": [], "entities": [{"text": "Pun data", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9076163470745087}]}, {"text": "The Pun data allows us to verify that our implementation of the conventional model is consistent with the work reported in.", "labels": [], "entities": [{"text": "Pun data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8629853129386902}]}, {"text": "In our experiment, we firstly divided each corpus into two parts.", "labels": [], "entities": []}, {"text": "The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers.", "labels": [], "entities": []}, {"text": "The larger portion (the CV set) was then formulated as a 10-fold crossvalidation setup for obtaining a stable and comprehensive model evaluation result.", "labels": [], "entities": []}, {"text": "For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences.", "labels": [], "entities": [{"text": "PUN data", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8160861730575562}, {"text": "CV set", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.8648340106010437}]}, {"text": "For the TED data, the Dev set contains 1046 utterances, while the CV set contains 8406 utterances.", "labels": [], "entities": [{"text": "TED data", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8123972415924072}, {"text": "Dev set", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7960668206214905}, {"text": "CV set", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9224536120891571}]}, {"text": "Note that, with a goal of building a speaker-independent humor detector, when partitioning our TED data set, we always kept all utterances of a single talk within the same partition.", "labels": [], "entities": [{"text": "TED data set", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8099103569984436}]}, {"text": "When building conventional models, we developed our own feature extraction scripts and used the SKLL 6 python package for building Random Forest models.", "labels": [], "entities": []}, {"text": "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), n f is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (.", "labels": [], "entities": []}, {"text": "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid over-fitting.", "labels": [], "entities": []}, {"text": "On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in.", "labels": [], "entities": [{"text": "Pun data", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9473269581794739}]}, {"text": "In particular, precision has been greatly increased from 0.762 to 0.864.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9997672438621521}]}, {"text": "On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%).", "labels": [], "entities": [{"text": "TED data", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.8793024718761444}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9995673298835754}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9994495511054993}]}, {"text": "The empirical evaluation results suggest that the CNN-based model has an advantage on the humor recognition task.", "labels": [], "entities": [{"text": "humor recognition task", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8734490275382996}]}, {"text": "In addition, focusing on the system development time, gener-7 Our code implementation was based on https://github.com/shagunsodhani/ CNN-Sentence-Classifier ating and implementing those features in the conventional model would take days or even weeks.", "labels": [], "entities": []}, {"text": "However, the CNN model automatically learns its optimal feature representation and can adjust the features automatically across data sets.", "labels": [], "entities": []}, {"text": "This makes the CNN model quite versatile for supporting different tasks and data domains.", "labels": [], "entities": []}, {"text": "Compared with the humor recognition results on the Pun data, the results on the TED data are still quite low, and more research is needed to fully handle humor in authentic presentations.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6721124798059464}, {"text": "Pun data", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8702047765254974}, {"text": "TED data", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8669078648090363}]}], "tableCaptions": [{"text": " Table 1: Humor recognition on both Pun and TED  data sets by using (a) random prediction (Chance),  conventional method (Base) and CNN method", "labels": [], "entities": [{"text": "Humor recognition", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9775390326976776}, {"text": "TED  data sets", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.80024982492129}]}]}